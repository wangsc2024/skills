# Vllm - Other

**Pages:** 20

---

## Automatic Prefix Caching - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/prefix_caching/

**Contents:**
- Automatic Prefix Caching¶
- Data Structure¶
- Operations¶
  - Block Allocation¶
  - Free¶
  - Eviction (LRU)¶
- Example¶

Prefix caching kv-cache blocks is a popular optimization in LLM inference to avoid redundant prompt computations. The core idea is simple – we cache the kv-cache blocks of processed requests, and reuse these blocks when a new request comes in with the same prefix as previous requests. Since prefix caching is almost a free lunch and won’t change model outputs, it has been widely used by many public endpoints (e.g., OpenAI, Anthropic, etc.) and most open source LLM inference frameworks (e.g., SGLang).

While there are many ways to implement prefix caching, vLLM chooses a hash-based approach. Specifically, we hash each kv-cache block by the tokens in the block and the tokens in the prefix before the block:

In the example above, the KV cache in the first block can be uniquely identified with the token “A gentle breeze stirred”. The third block can be uniquely identified with the tokens in the block “laughed in the distance”, along with the prefix tokens “A gentle breeze stirred the leaves as children”. Therefore, we can build the block hash of hash(tuple[components]), where components are:

We only cache full blocks.

The above hash key structure is not 100% collision free. Theoretically it’s still possible for the different prefix tokens to have the same hash value. To avoid any hash collisions in a multi-tenant setup, we use SHA256 as hash function instead of the builtin hash. SHA256 is supported since vLLM v0.8.3 and the default since v0.10.2. It comes with a negligible performance impact of about 75ns per token (<4ms for 50k tokens of context).

A hashing example with multi-modality inputs In this example, we illustrate how prefix caching works with multi-modality inputs (e.g., images). Assuming we have a request with the following messages:

It will become the following prompt:

As we can see, after the tokenization, the [IMG] will be replaced by a sequence of placeholder tokens, and these placeholders will be replaced by image embeddings during prefill. The challenge for prefix caching to support this case is we need to differentiate images from the placeholders. To address this problem, we encode the image hash generated by the frontend image processor. For example, the hash of the blocks in the above prompt would be (assuming block size 16, and we have 41 placeholder tokens):

In the rest of this document, we first introduce the data structure used for prefix caching in vLLM v1, followed by the prefix caching workflow of major KV cache operators (e.g., allocate, append, free, eviction). Finally, we use an example to illustrate the end to end prefix caching workflow.

Cache Isolation for Security To improve privacy in shared environments, vLLM supports isolating prefix cache reuse through optional per-request salting. By including a cache_salt in the request, this value is injected into the hash of the first block, ensuring that only requests with the same salt can reuse cached KV blocks. This prevents timing-based attacks where an adversary could infer cached content by observing latency differences. This offers protection without compromising performance.

With this setup, cache sharing is limited to users or requests that explicitly agree on a common salt, enabling cache reuse within a trust group while isolating others.

The prefix caching in vLLM v1 is implemented in the KV cache manager. The basic building block is the “Block” data class (simplified):

There are two design points to highlight:

As a result, we will have the following components when the KV cache manager is initialized:

New request: Workflow for the scheduler to schedule a new request with KV cache block allocation:

Running request: Workflow for the scheduler to schedule a running request with KV cache block allocation:

Duplicated blocks Assuming block size is 4 and you send a request (Request 1) with prompt ABCDEF and decoding length 3:

Now block 0 and block 1 are cached, and we send the same request again (Request 2) with greedy sampling, so that it will produce exactly the same outputs as the Request 1:

As can be seen, block 3 is a new full block and is cached. However, it is redundant as block 1, meaning that we cached the same block twice. In v0, when detecting block 3 is duplicated, we free block 3 and let Request 2 use block 1 instead, so its block table becomes [0, 1] in Time 1. However, the block table in vLLM v1 is append-only, meaning that changing the block table from [0, 3] to [0, 1] is not allowed. As a result, we will have duplicated blocks for the hash key E-H. This duplication will be eliminated when the request is freed.

When a request is finished, we free all its blocks if no other requests are using them (reference count = 0). In this example, we free request 1 and block 2, 3, 4, 8 associated with it. We can see that the freed blocks are added to the tail of the free queue in the reverse order. This is because the last block of a request must hash more tokens and is less likely to be reused by other requests. As a result, it should be evicted first.

When the head block (least recently used block) of the free queue is cached, we have to evict the block to prevent it from being used by other requests. Specifically, eviction involves the following steps:

In this example, we assume the block size is 4 (each block can cache 4 tokens), and we have 10 blocks in the KV-cache manager in total.

Time 1: The cache is empty and a new request comes in. We allocate 4 blocks. 3 of them are already full and cached. The fourth block is partially full with 3 of 4 tokens.

Time 2: Request 0 makes the block 3 full and asks for a new block to keep decoding. We cache block 3 and allocate block 4.

Time 3: Request 1 comes in with the 14 prompt tokens, where the first 10 tokens are the same as request 0. We can see that only the first 2 blocks (8 tokens) hit the cache, because the 3rd block only matches 2 of 4 tokens.

Time 4: Request 0 is finished and free. Blocks 2, 3 and 4 are added to the free queue in the reverse order (but block 2 and 3 are still cached). Block 0 and 1 are not added to the free queue because they are being used by Request 1.

Time 5: Request 1 is finished and free.

Time 6: Request 2 comes in with the 29 prompt tokens, where the first 12 tokens are the same as request 0. Note that even the block order in the free queue was 7 - 8 - 9 - 4 - 3 - 2 - 6 - 5 - 1 - 0, the cache hit blocks (i.e., 0, 1, 2) are touched and removed from the queue before allocation, so the free queue becomes 7 - 8 - 9 - 4 - 3 - 6 - 5. As a result, the allocated blocks are 0 (cached), 1 (cached), 2 (cached), 7, 8, 9, 4, 3 (evicted).

**Examples:**

Example 1 (r):
```r
Block 1                  Block 2                  Block 3
         [A gentle breeze stirred] [the leaves as children] [laughed in the distance]
Block 1: |<--- block tokens ---->|
Block 2: |<------- prefix ------>| |<--- block tokens --->|
Block 3: |<------------------ prefix -------------------->| |<--- block tokens ---->|
```

Example 2 (r):
```r
Block 1                  Block 2                  Block 3
         [A gentle breeze stirred] [the leaves as children] [laughed in the distance]
Block 1: |<--- block tokens ---->|
Block 2: |<------- prefix ------>| |<--- block tokens --->|
Block 3: |<------------------ prefix -------------------->| |<--- block tokens ---->|
```

Example 3 (json):
```json
messages = [
    {"role": "user",
     "content": [
         {"type": "text",
          "text": "What's in this image?"
         },
         {"type": "image_url",
          "image_url": {"url": image_url},
         },
    ]},
]
```

Example 4 (json):
```json
messages = [
    {"role": "user",
     "content": [
         {"type": "text",
          "text": "What's in this image?"
         },
         {"type": "image_url",
          "image_url": {"url": image_url},
         },
    ]},
]
```

---

## Contact Us - vLLM

**URL:** https://docs.vllm.ai/en/latest/community/contact_us/

**Contents:**
- Contact Us¶

---

## CUDA Graphs - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/cuda_graphs/

**Contents:**
- CUDA Graphs¶
- Motivation¶
- CudagraphModes¶
- Detailed Design¶
  - Overview¶
  - BatchDescriptor¶
  - CudagraphDispatcher¶
  - CUDAGraphWrapper¶
    - Nested Wrapper design¶
  - Full CUDA Graph capturing & warm-up¶

This write-up introduces the new CUDA Graphs modes in vLLM v1 beyond previous torch.compile integration. To summarize, we:

In this document we will discuss the:

In this document, we refer to pure decode (max_query_len=1) or speculative decode (max_query_len =1+num_spec_tokens) as uniform decode batches, and the opposite would be non-uniform batches (i.e., prefill or mixed prefill-decode batches).

The following contents are mostly based on the last commit of Pull Request #20059.

Initial piecewise compilation was built to allow piecewise cudagraph capture, excluding cudagraph-unsupported operations (mainly attention). This allowed some speedup from cudagraphs while maintaining compatibility with all attention backends. We later added support for "full cudagraphs" by not compiling piecewise, so that we could further reduce the latency in cases where attention supported cudagraphs. However, this tight coupling between compilation and cudagraph capture led to an all-or-nothing experience with little flexibility. Many attention backends also weren’t ready for unified "full" CUDA Graphs capture (e.g., only FlashAttention 3 supports it currently) or only support CUDA Graphs for pure decode batches (e.g., Flashinfer, FlashMLA, and Mamba, etc.). That led to confusing performance/compatibility tradeoffs, inconsistent CUDA Graphs support, and increasingly complex code structure.

This led us to seek a more fine-grained CUDA Graphs solution with the following features:

These features allow the most flexibility for cudagraph capture and compilation for all kinds of startup/performance tradeoffs and feature support.

CUDAGraphMode is the single knob you tune in CompilationConfig.cudagraph_mode:

Defaults: If you’re on v1 with piecewise compilation, we default to FULL_AND_PIECEWISE for better performance, (for pooling models, it's still PIECEWISE). Otherwise, e.g. if piecewise compilation unavailable, we default to NONE.

While NONE , PIECEWISE, and FULL are single-mode configurations and simply equivalent to past implementations of eager execution, piecewise CUDA Graphs, and full CUDA Graphs respectively, FULL_DECODE_ONLY and FULL_AND_PIECEWISE are newly appended dual-mode configurations, which require dispatching to switch between concrete runtime modes according to runtime batches dynamically.

Here, the single-modes NONE, PIECEWISE, and FULL are treated as the runtime modes for CUDA Graphs dispatching. If using a dual-mode, the dispatcher will always dispatch to one of its member modes (plus a potential NONE if no suitable CUDA Graph available), depending on the batch composition.

While cascade attention is not cudagraph compatible, it is now compatible with all possible cudagraph mode configurations. If a batch uses cascade attention, it always gets dispatched to PIECEWISE mode if available (otherwise NONE).

Not all CUDA Graph modes are compatible with every attention backend. We automatically "downgrade" modes to the closest supported mode. For example, if a backend only supports CUDA Graphs for pure decode/uniform batches, we convert FULL to FULL_AND_PIECEWISE if piecewise compilation is enabled, and FULL_DECODE_ONLY otherwise.

The new CUDA Graphs logic is built on top of piecewise compilation and supports dual CUDA Graphs runtime mode switching. The system contains the following core components:

See the following figures for a quick comparison between the previous and current design patterns of CUDA Graphs with inductor compilation. We can see that previously the CUDA Graphs logic and compilation logic were tightly coupled into the vllm PiecewiseBackend, and CUDA Graphs was implicitly dispatched by batch_size idly. Now the CUDA Graphs logic is separated into the CUDAGraphWrapper class, responsible for both full and piecewise CUDA Graphs abilities, and dispatching is explicitly done via runtime mode plus the BatchDescriptor as the dispatch key via CudagraphDispatcher.

BatchDescriptor is a component within ForwardContext, alongside the CUDA Graphs runtime modes, serving as the core structure for dispatching keys at runtime. The prototype is:

where num_tokens can be the padded token length, and uniform indicates if all the requests have the same query lengths. Many attention backends only support full cudagraphs when the batches are uniform; pure decode batches are uniform but may not be query length 1 (i.e. num_tokens == num_reqs), this occurs in the validation pass of spec-decode where "decode" batches will have a query length of 1+num_spec_tokens.

The goal of this structure is to uniquely identify a (padded) batch with minimal possible items corresponding to a CUDA Graphs item.

The prototype of BatchDescriptor may be extended for more general situations in the future, e.g., include more items, like uniform_query_len to support multiple different uniform decode lengths settings ( Pull Request #23679), or other modifications needed to support CUDA Graphs for models whose inputs are not necessarily token length aware (for example, some multi-modal inputs).

The CudagraphDispatcher takes responsibility for maintaining two sets of valid dispatching keys, one set for FULL runtime mode and one set for PIECEWISE runtime mode, and dispatches the correct runtime mode and the dispatching keys before executing the model's forwards. It will take in the initial key (a rough batch_descriptor for the padded input) and return the selected runtime mode and the final batch_descriptor, then tell the CUDAGraphWarpper instances that decision through forward contexts. Notice that CudagraphDispatcher is the only source of truth for available CUDA Graph keys and CUDAGraphWrapper instances can blindly trust the forward context on what CUDA Graphs to dispatch to. This lets us simplify the wrapper code and centralize the logic in the dispatcher.

The dispatching keys are initialized through the dispatcher's initialize_cudagraph_keys method, which is called by the gpu_model_runner after all possible attention backends are initialized. This is where we can get much fancier in the future and “prepare” all kinds of CUDA Graphs combinations. For now, we just append available keys based on the valid combos of decode_mode/mixed_mode of cudagraph_mode and cudagraph_capture_sizes in the compilation config.

The dispatch code looks like:

Inside the dispatch() method, the dispatcher will search the proper CUDA Graphs runtime mode and existing dispatching keys for a return. We basically search the existing keys following the priority: FULL>PIECEWISE>None. If the dispatching key does not exist, default to return NONE mode for eager execution. The implementations can be found here.

Here is a simplified illustration of the workflow at runtime in the model executor:

A CUDAGraphWrapper instance wraps a runnable and simply mimics the runnable with appended CUDA Graphs abilities. Each wrapper instance is bound to a specific runtime_mode, which is restricted to PIECEWISE and FULL mode, and takes responsibility for capturing/replaying and passing through (directly calling) the runnable. At runtime, each wrapper would:

The above steps are based on the assumption that the CUDA Graphs wrapper would directly trust what’s in the forward context (controlled by the dispatcher). This lets us simplify and centralize the logic, reducing the complexity as well as the risk of mismatched state between the wrappers and the dispatcher. It also allows reusing the wrapper class for both FULL and PIECEWISE runtime modes. See the implementation here.

The core mechanism of making a full CUDA Graphs and piecewise CUDA Graphs coexist and compatible is the nested CUDA Graphs wrapper design, building on top of piecewise compilation with only a single piecewise FX graph. We wrap a FULL mode wrapper outside the entire model for the full CUDA Graphs functionality; meanwhile, each piecewise backend is wrapped via a PIECEWISE mode wrapper inside the compilation.

The flow chart below should clearly describe how it works.

Therefore, for a FULL runtime mode, it is safe to capture/replay a full CUDA Graph since the piecewise wrapper is not activated. The situation is similar for PIECEWISE mode, as there are no conflicts between the FULL mode wrapper and PIECEWISE mode wrappers. For the NONE runtime mode, both FULL and PIECEWISE wrappers would not be activated, so we simply fall through to eager execution.

The CUDA Graphs capturing happens when the runner first calls the model forward (using _dummy_run) with a non-NONE runtime mode. For full CUDA Graph capture, we explicitly capture different cases (i.e., prefill/mixed batch or uniform_decode batch) by properly setting attention metadata to make sure the underlying attention backends launch the desired kernel routines. To distinguish prefill/mixed batch or uniform_decode batch, the most important property is the max_query_len in attn_metadata (true for most attention backends). We set it to the desired uniform_query_len for uniform_decode otherwise we make it just the num_tokens for a non-uniform_decode batch.

The CUDA Graphs wrapper no longer manages the warm-up logic. The warm-up process is now controlled directly by the GPU model runner, where the NONE runtime mode is assigned to play an eager execution for warm-up. When warming up for a full CUDA Graph, it is also important to explicitly run attention during the warmup dummy_run call.

To signal the CUDA Graphs compatibility of the attention backends, we introduce a new enum type AttentionCGSupport, which is an enum type that tracks the capability of the attention backend to support CUDA Graphs. The value is sorted in the order of the capability, i.e., ALWAYS> UNIFORM_BATCH> UNIFORM_SINGLE_TOKEN_DECODE> NEVER.

Suppose we have hybrid attention backends (e.g., in mamba mixer models). In that case, we seek the minimum capability of all backends to determine the final capability of the model, and we might resolve the incompatible CUDA Graphs mode by downgrading the mode to the best fit one. For example, downgrading FULL mode to FULL_AND_PIECEWISE mode if the minimum capability is UNIFORM_BATCH, or PIECEWISE mode if the minimum capability is NEVER for -O3 compilation mode. For the complete fallback policy, please see the code for this.

The following table lists backends that support full CUDA Graphs at the time of writing.

Unlisted backends are all declared as NEVER.

Now the CLI is directly using the uppercase string of cudagraph_mode for compilation_config: --compilation-config '{"cudagraph_mode": "..."}', where ... should be one of NONE, PIECEWISE, FULL, FULL_DECODE_ONLY, and FULL_AND_PIECEWISE. Note that all PIECEWISE related modes require piecewise compilation, and all FULL related modes need CUDA Graphs support of attention backends. For example:

Unfortunately, some custom compile passes have to see the whole graph to be effective and hence aren't compatible with piecewise compilation. This includes AttnFusionPass and SequenceParallelismPass. As a short-term solution, we automatically disable piecewise compilation (by setting splitting_ops=[]) when attention fusion is enabled. We use CUDA Graph modes FULL or FULL_DECODE_ONLY (depending on backend support). However, this leads to another optimization incompatibility and confusing performance tradeoffs.

Long term, we've added the ability to partition the graph in Inductor instead of right after Dynamo. It can be enabled with CompilationConfig.use_inductor_graph_partition=True but is currently experimental and only available with torch>=2.9. This also increases compilation time as it has to compile the whole graph and cannot reuse piecewise compilation artifacts. Once vLLM supports 2.9, we plan to make this the default approach as it will also speed up piecewise cudagraph capture.

See the following links for examples:

**Examples:**

Example 1 (typescript):
```typescript
class BatchDescriptor(NamedTuple):
    num_tokens: int
    num_reqs: int
    uniform: bool = False
    has_lora: bool = False
```

Example 2 (typescript):
```typescript
class BatchDescriptor(NamedTuple):
    num_tokens: int
    num_reqs: int
    uniform: bool = False
    has_lora: bool = False
```

Example 3 (typescript):
```typescript
batch_descriptor=BatchDescriptor(num_tokens=num_input_tokens, uniform_decode=...)
runtime_mode, batch_descriptor = cudagraphdispatcher.dispatch(batch_descriptor)
# execution
with set_forward_context(
    ..., 
    cudagraph_runtime_mode=runtime_mode, 
    batch_descriptor=batch_descriptor,
):
     output = self.model(...)
```

Example 4 (typescript):
```typescript
batch_descriptor=BatchDescriptor(num_tokens=num_input_tokens, uniform_decode=...)
runtime_mode, batch_descriptor = cudagraphdispatcher.dispatch(batch_descriptor)
# execution
with set_forward_context(
    ..., 
    cudagraph_runtime_mode=runtime_mode, 
    batch_descriptor=batch_descriptor,
):
     output = self.model(...)
```

---

## Dual Batch Overlap - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/dbo/

**Contents:**
- Dual Batch Overlap¶
- Motivation¶
- Introduction¶
- Running with DBO¶
- DBO Components¶
  - GPU Model Runner¶
  - UBatchWrapper¶
    - Interfaces¶
  - UBatchContext¶
    - Interfaces¶

The core motivation of the DBO system in vLLM is to overlap the sparse all-to-all communication in the MoE layer with the surrounding computation. This system currently only targets DP+EP deployments.

The Dual Batch Overlap system works by splitting the batch in the model runner, creating two worker threads, and then running the model on each of these worker threads. When DBO is enabled, yield points within the FusedMoEModularKernel allow the two CPU worker threads (also called UBatch threads) to ping-pong between each other so that when one is running compute, the other is waiting on communication. Throughout the code, ubatch may be used as a short form of microbatch; this is an ASCII-friendly version of the short form µ-batch.

The DBO system includes modifications to GpuModelRunner and ModularKernel, and defines two utility classes: UBatchWrapper and UBatchContext. UBatchWrapper manages thread lifecycle and CUDA graph execution of the model. UBatchContext wraps ForwardContext to coordinate synchronization between the two UBatch threads.

Below is the overlap schedule that is currently implemented in vLLM.

To enable the DBO system pass in the --enable-dbo argument to your vllm serve command. This must be run in conjunction with --data-parallel-size N where N is greater than 1 and --enable-expert-parallel. Additionally, there are two configuration knobs.

Currently, DBO is only supported with DeepEP, so DeepEP must be installed and the --all2all-backend argument must be set to deepep_low_latency if your workload is primarily decode requests, or deepep_high_throughput if your workload is primarily prefill requests.

Below is a command that will spin up a two DP rank server with expert parallelism and DBO enabled. EX: vllm serve deepseek-ai/DeepSeek-V2-Lite --trust-remote-code --data-parallel-size 2 --enable-expert-parallel --enable-dbo --all2all-backend deepep_low_latency

Note that there must be at least two GPUs visible in CUDA_VISIBLE_DEVICES

The batch is split into microbatches by the GPUModelRunner class. This is accomplished in two steps. First, coordination across all DP ranks is performed to determine whether microbatching will be applied. Microbatching must be uniform across all DP ranks. If microbatching is not feasible for any DP rank, it is disabled for all ranks. If all DP ranks are going to microbatch, the total number of tokens is padded up to the max number of tokens amongst all ranks. If any rank would end up with an empty second microbatch after the padding is applied, microbatching will be aborted and no ranks will microbatch. Once microbatching has been initiated by all ranks, the second step is performed. The CommonAttentionMetadata is sliced in half by the GPUModelRunner so that there is one attention metadata per-microbatch.

The UBatchWrapper class is a model wrapper that's responsible for all of the thread, UBatchContext, and CUDA graph management for DBO. It's designed to be relatively transparent to the GPU Model Runner.

The implementation runs the model twice, once for each microbatch. Each model invocation occurs within a UBatch thread. These threads are launched in parallel and are synchronized using the UBatchContext. Each thread is provided with a sliced version of the attention metadata that is used to run its half of the batch.

CUDA graphs for DBO are entirely managed by the UBatchWrapper. Because of this, DBO only supports running with Full CUDA graphs. However, once a DBO CUDA graph has been captured, it can be replayed without any multithreading or CPU synchronization.

The __init__ method takes in the model, VllmConfig, CUDAGraphMode, and device.

The forward method exclusively takes in model arguments. It determines whether or not to run with DBO based on whether a ubatch_slices object is present in the forward_context. Otherwise, the model is run without DBO.

The UBatchContext class is a ForwardContext wrapper class that is used by the UBatchWrapper class to synchronize the two UBatch threads. It should only be instantiated by using make_ubatch_contexts.

When one of the UBatch threads reaches a dbo_yield call, it pauses, and starts the other thread which will run until it reaches the same dbo_yield call. This "ping-pong" dynamic continues, with threads swapping at each dbo_yield call, until the model's execution is complete.

The current implementation has all dbo_yield and dbo_maybe_run_recv_hook calls in the FusedMoEModularKernel.forward method.

The make_ubatch_context function initializes two UBatchContexts, one for each UBatch thread. It takes two CUDA streams, the preexisting ForwardContexts and a CPU thread barrier. This function should be used exclusively to instantiate UBatchContexts. It will handle all of the event initialization.

The dbo_register_recv_hook method registers a callback that can be returned by the FusedMoEPrepareAndFinalize class in the other UBatch thread’s UBatchContext. The callback will be run when the other thread calls dbo_maybe_run_recv_hook. This is typically used to wait on an all-to-all kernel.

The dbo_maybe_run_recv_hook method runs a callback that’s set by the dbo_register_recv_hook function if that callback exists.

The dbo_yield method puts the current thread to sleep and wakes up the other UBatch thread.

**Examples:**

Example 1 (markdown):
```markdown
# Schedule notation legend:
#    S = Shared expert
#    A0 = MLA qkv proj,
#    A1 = Core attn + out proj + MoE gate
#    D = Dispatch
#    C = Combine

# Comp: |-A0₀-A1₀-||-MLP₁-||-S₁-MLP₀-||-S₀-A0₁-A1₁-|
# Comm: |----D₁---||--D₀--||----C₁---||-----C₀-----|
# Order: D₁ send, A0₀, A1₀, D₁ recv, D₀ send, MLP₁, D₀ recv,
#        C₁ send, S₁, MLP₀, C₁ recv, C₀ send, S₀, A0₁, A1₁, C₀ recv.
# MLP_SHARED_OVERLAP = "mlp_shared_overlap"
```

Example 2 (markdown):
```markdown
# Schedule notation legend:
#    S = Shared expert
#    A0 = MLA qkv proj,
#    A1 = Core attn + out proj + MoE gate
#    D = Dispatch
#    C = Combine

# Comp: |-A0₀-A1₀-||-MLP₁-||-S₁-MLP₀-||-S₀-A0₁-A1₁-|
# Comm: |----D₁---||--D₀--||----C₁---||-----C₀-----|
# Order: D₁ send, A0₀, A1₀, D₁ recv, D₀ send, MLP₁, D₀ recv,
#        C₁ send, S₁, MLP₀, C₁ recv, C₀ send, S₀, A0₁, A1₁, C₀ recv.
# MLP_SHARED_OVERLAP = "mlp_shared_overlap"
```

---

## Fused MoE Modular Kernel - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/

**Contents:**
- Fused MoE Modular Kernel¶
- Introduction¶
- Motivation¶
- ModularKernel Components¶
  - TopKWeightAndReduce¶
  - FusedMoEPrepareAndFinalize¶
  - FusedMoEPermuteExpertsUnpermute¶
    - apply()¶
    - workspace_shapes()¶
    - finalize_weight_and_reduce_impl()¶

FusedMoEModularKernel is implemented here

Based on the format of the input activations, FusedMoE implementations are broadly classified into 2 types.

The terms Contiguous, Standard, and Non-Batched are used interchangeably throughout the document.

The input activation format completely depends on the All2All Dispatch being used.

The FusedMoE operation is generally made of multiple operations, in both the Contiguous and Batched variants, as described in the diagrams below

The main difference, in terms of operations, between the Batched and Non-Batched cases is the Permute / Unpermute operations. All other operations remain.

As can be seen from the diagrams, there are a lot of operations and there can be a variety of implementations for each operation. The set of ways the operations can be put together to make a valid FusedMoE implementation quickly becomes intractable. The Modular Kernel framework addresses this issue, by grouping the operations into logical components. This broad categorization makes the combinations manageable and prevents code-duplication. This also decouples the All2All Dispatch & Combine implementations from the FusedMoE implementations and allows for their independent development and testing. Furthermore, the Modular Kernel framework introduces Abstract classes for the different components thus providing a well-defined skeleton for future implementations.

The rest of the document will focus on the Contiguous / Non-Batched case. Extrapolating to the Batched case should be straight-forward.

FusedMoEModularKernel splits the FusedMoE operation into 3 parts,

The TopK Weight Application and Reduction components happen right after the Unpermute operation and before the All2All Combine. Note that the FusedMoEPermuteExpertsUnpermute is responsible for the Unpermute and FusedMoEPrepareAndFinalize is responsible for the All2All Combine. There is value in doing the TopK Weight Application and Reduction in the FusedMoEPermuteExpertsUnpermute. But some implementations choose to do it FusedMoEPrepareAndFinalize. In order to enable this flexibility, we have a TopKWeightAndReduce abstract class.

Please find the implementations of TopKWeightAndReduce here.

FusedMoEPrepareAndFinalize::finalize() method accepts a TopKWeightAndReduce argument that is invoked inside the method. The FusedMoEModularKernel acts as a bridge between the FusedMoEPermuteExpertsUnpermute and FusedMoEPerpareAndFinalize implementations to determine where the TopK Weight Application and Reduction happens.

The FusedMoEPrepareAndFinalize abstract class exposes prepare, prepare_no_receive and finalize functions. The prepare function is responsible for input activation Quantization and All2All Dispatch. If implemented, The prepare_no_receive is like prepare except it does not wait to receive results from other workers. Instead it returns a "receiver" callback that must be invoked to wait for the final results of worker. It is not required that this method is supported by all FusedMoEPrepareAndFinalize classes, but if it is available, it can be used to interleave work with the initial all to all communication, e.g. interleaving shared experts with fused experts. The finalize function is responsible for invoking the All2All Combine. Additionally the finalize function may or may not do the TopK weight application and reduction (Please refer to the TopKWeightAndReduce section)

The FusedMoEPermuteExpertsUnpermute class is where the crux of the MoE operations happen. The FusedMoEPermuteExpertsUnpermute abstract class exposes a few important functions,

The apply method is where the implementations perform

The core FusedMoE implementation performs a series of operations. It would be inefficient to create output memory for each of these operations separately. To that effect, implementations are required to declare 2 workspace shapes, the workspace datatype and the FusedMoE output shape as outputs of the workspace_shapes() method. This information is used to allocate the workspace tensors and the output tensor in FusedMoEModularKernel::forward() and passed on to the FusedMoEPermuteExpertsUnpermute::apply() method. The workspaces could then be used as intermediate buffers in the FusedMoE implementation.

It is sometimes efficient to perform TopK weight application and Reduction inside the FusedMoEPermuteExpertsUnpermute::apply(). Find an example here. We have a TopKWeightAndReduce abstract class to facilitate such implementations. Please refer to the TopKWeightAndReduce section. FusedMoEPermuteExpertsUnpermute::finalize_weight_and_reduce_impl() returns the TopKWeightAndReduce object that the implementation wants the FusedMoEPrepareAndFinalize::finalize() to use.

FusedMoEModularKernel is composed of the FusedMoEPrepareAndFinalize and FusedMoEPermuteExpertsUnpermute objects. FusedMoEModularKernel pseudocode/sketch,

Typically a FusedMoEPrepareAndFinalize type is backed by an All2All Dispatch & Combine implementation / kernel. For example,

The purpose of the All2All Manager is to set up the All2All kernel implementations. The FusedMoEPrepareAndFinalize implementations typically fetch a kernel-implementation "handle" from the All2All Manager to invoke the Dispatch and Combine functions. Please look at the All2All Manager implementations here.

This section describes the significance of the various functions exposed by the FusedMoEPrepareAndFinalize abstract class.

FusedMoEPrepareAndFinalize::prepare(): The prepare method implements the Quantization and All2All Dispatch. Typically the Dispatch function from the relevant All2All Manager is invoked.

FusedMoEPrepareAndFinalize::has_prepare_no_receive(): Indicates whether or not this subclass implements prepare_no_receive. Defaults to False.

FusedMoEPrepareAndFinalize::prepare_no_receive(): The prepare_no_receive method implements the Quantization and All2All Dispatch. It does not wait for the result of the dispatch operation but instead returns a thunk that can be invoked to wait for the final results. Typically the Dispatch function from the relevant All2All Manager is invoked.

FusedMoEPrepareAndFinalize::finalize(): Maybe perform TopK Weight Application and Reduction and All2All Combine. Typically the Combine function from the relevant All2AllManager is invoked.

FusedMoEPrepareAndFinalize::activation_format(): Return FusedMoEActivationFormat.BatchedExperts if the output of the prepare method (i.e. the All2All dispatch) is Batched. Return FusedMoEActivationFormat.Standard otherwise.

FusedMoEPrepareAndFinalize::topk_indices_dtype(): Data type of the TopK ids. Some All2All kernels have strict requirements pertaining to the data type of the TopK ids. This requirement is passed on to the FusedMoe::select_experts function so it could be respected. If there are no strict requirements return None.

FusedMoEPrepareAndFinalize::max_num_tokens_per_rank(): This is the maximum number of tokens that would be submitted to the All2All Dispatch at once.

FusedMoEPrepareAndFinalize::num_dispatchers(): Total number of dispatching units. This value determines the size of the Dispatch output. The Dispatch output is of shape (num_local_experts, max_num_tokens, K). Here max_num_tokens = num_dispatchers() * max_num_tokens_per_rank().

We suggest picking an already existing FusedMoEPrepareAndFinalize implementation that matches your All2All implementation closely and using it as a reference.

FusedMoEPermuteExpertsUnpermute performs the core of the FusedMoE operations. The various functions exposed by the abstract class and their significance is as follows,

FusedMoEPermuteExpertsUnpermute::activation_formats(): Return the supported Input and Output activation formats. i.e. Contiguous / Batched format.

FusedMoEPermuteExpertsUnpermute::supports_chunking(): Return True if the implementation supports chunking. Typically implementations that input FusedMoEActivationFormat.Standard support chunking and FusedMoEActivationFormat.BatchedExperts do not.

FusedMoEPermuteExpertsUnpermute::supports_expert_map(): Return True if the implementation supports expert map.

FusedMoEPermuteExpertsUnpermute::workspace_shapes() / FusedMoEPermuteExpertsUnpermute::finalize_weight_and_reduce_impl / FusedMoEPermuteExpertsUnpermute::apply: Refer to FusedMoEPermuteExpertsUnpermute section above.

FusedMoEMethodBase class has 3 methods that are collectively responsible in creating the FusedMoEModularKernel object. They are,

The maybe_make_prepare_finalize method is responsible for constructing an instance of FusedMoEPrepareAndFinalize when appropriate based on the current all2all backend, e.g. when EP + DP is enabled. The base class method currently constructs all the FusedMoEPrepareAndFinalize objects for the EP+DP case. Derived classes can override this method to construct prepare/finalize objects for different scenarios, e.g. ModelOptNvFp4FusedMoE can construct a FlashInferCutlassMoEPrepareAndFinalize for the EP+TP case. Please refer to the implementations in,

The select_gemm_impl method is undefined in the base class. It is the responsibility of the derived class to implement a method that constructs a valid/appropriate FusedMoEPermuteExpertsUnpermute object. Please refer to the implementations in,

Based on the input and env settings, the init_prepare_finalize method creates the appropriate FusedMoEPrepareAndFinalize object. The method then queries select_gemm_impl for the appropriate FusedMoEPermuteExpertsUnpermute object and builds the FusedMoEModularKernel object

Please take a look at init_prepare_finalize. Important: The FusedMoEMethodBase derived classes use the FusedMoEMethodBase::fused_experts object in their apply methods. When settings permit the construction of a valid FusedMoEModularKernel object, we override FusedMoEMethodBase::fused_experts with it. This essentially makes the derived classes agnostic to what FusedMoE implementation is used.

We have FusedMoEModularKernel unit tests at test_modular_kernel_combinations.py.

The unit test iterates through all combinations of FusedMoEPrepareAndFinalize and FusedMoEPremuteExpertsUnpermute types and if they are compatible, runs some correctness tests. If you are adding some FusedMoEPrepareAndFinalize / FusedMoEPermuteExpertsUnpermute implementations,

Doing this will add the new implementation to the test suite.

The unit test file test_modular_kernel_combinations.py can also be executed as a standalone script. Example: python3 -m tests.kernels.moe.test_modular_kernel_combinations --pf-type PplxPrepareAndFinalize --experts-type BatchedTritonExperts As a side effect, this script can be used to test FusedMoEPrepareAndFinalize & FusedMoEPermuteExpertsUnpermute compatibility. When invoked with incompatible types, the script will error.

Please take a look at profile_modular_kernel.py The script can be used to generate Torch traces for a single FusedMoEModularKernel::forward() call for any compatible FusedMoEPrepareAndFinalize and FusedMoEPermuteExpertsUnpermute types. Example: python3 -m tests.kernels.moe.modular_kernel_tools.profile_modular_kernel --pf-type PplxPrepareAndFinalize --experts-type BatchedTritonExperts

See Fused MoE Kernel features for a list of all the available modular prepare and finalize subclasses.

See Fused MoE Kernel features for a list of all the available modular experts.

**Examples:**

Example 1 (python):
```python
class FusedMoEModularKernel:
    def __init__(self,
                 prepare_finalize: FusedMoEPrepareAndFinalize,
                 fused_experts: FusedMoEPermuteExpertsUnpermute):

        self.prepare_finalize = prepare_finalize
        self.fused_experts = fused_experts

    def forward(self, DP_A):

        Aq, A_scale, _, _, _ = self.prepare_finalize.prepare(DP_A, ...)

        workspace13_shape, workspace2_shape, _, _ = self.fused_experts.workspace_shapes(...)

        # allocate workspaces
        workspace_13 = torch.empty(workspace13_shape, ...)
        workspace_2 = torch.empty(workspace2_shape, ...)

        # execute fused_experts
        fe_out = self.fused_experts.apply(Aq, A_scale, workspace13, workspace2, ...)

        # war_impl is an object of type TopKWeightAndReduceNoOp if the fused_experts implementations
        # performs the TopK Weight Application and Reduction.
        war_impl = self.fused_experts.finalize_weight_and_reduce_impl()

        output = self.prepare_finalize.finalize(fe_out, war_impl,...)

        return output
```

Example 2 (python):
```python
class FusedMoEModularKernel:
    def __init__(self,
                 prepare_finalize: FusedMoEPrepareAndFinalize,
                 fused_experts: FusedMoEPermuteExpertsUnpermute):

        self.prepare_finalize = prepare_finalize
        self.fused_experts = fused_experts

    def forward(self, DP_A):

        Aq, A_scale, _, _, _ = self.prepare_finalize.prepare(DP_A, ...)

        workspace13_shape, workspace2_shape, _, _ = self.fused_experts.workspace_shapes(...)

        # allocate workspaces
        workspace_13 = torch.empty(workspace13_shape, ...)
        workspace_2 = torch.empty(workspace2_shape, ...)

        # execute fused_experts
        fe_out = self.fused_experts.apply(Aq, A_scale, workspace13, workspace2, ...)

        # war_impl is an object of type TopKWeightAndReduceNoOp if the fused_experts implementations
        # performs the TopK Weight Application and Reduction.
        war_impl = self.fused_experts.finalize_weight_and_reduce_impl()

        output = self.prepare_finalize.finalize(fe_out, war_impl,...)

        return output
```

---

## How to debug the vLLM-torch.compile integration - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/debug_vllm_compile/

**Contents:**
- How to debug the vLLM-torch.compile integration¶
- vLLM-torch.compile overview¶
- Use tlparse¶
- Turn off vLLM-torch.compile integration¶
- Debugging TorchDynamo¶
- Debugging Dynamic Shape full graph capture¶
- Debugging constraint violations and dynamic shapes guards issues¶
  - Printing guards¶
- Debugging TorchInductor¶
  - Editable TorchInductor code¶

To improve performance, vLLM leverages torch.compile and CUDAGraphs to speed things up. torch.compile generates optimized kernels for PyTorch code while CUDAGraphs eliminates overhead. Most notably, vLLM-compile is NOT torch.compile, it is a custom compiler built using internal PyTorch Compile APIs.

Things can go wrong in each of the four steps. When something does go wrong, please try to isolate the subsystem that went wrong -- this will allow you to turn off the minimal number of things to keep reliability goals while minimizing impact to performance and also helps us (vLLM) when you open a bug report.

For more details on the design, please see the following resources:

Use tlparse to acquire torch.compile logs. These logs show all stages of the compilation process, including the fused kernels that torch.compile produces. If you can, we recommend sending these or pieces of these along with any bug reports -- they are very helpful.

Usage (offline inference)

The tlparse command outputs some HTML files (perhaps into e.g. ./tl_out/index.html). Open it to see the logs. It'll look something like the following:

Pass --enforce-eager to turn off the vLLM-torch.compile integration and run entirely in eager mode. This includes turning off CUDAGraphs.

To turn off just torch.compile, pass mode = NONE to the compilation config. (-cc is short for --compilation_config):

To turn off just CUDAGraphs, pass cudagraph_mode = NONE:

vLLM requires model code be capturable into a full graph via TorchDynamo (torch.compile's frontend). TorchDynamo does not support all of Python. It will error (in fullgraph mode) if it cannot support a feature (this is sometimes known as a graph break).

If you encounter a graph break, please open an issue to pytorch/pytorch so the PyTorch devs can prioritize. Then, try your best to rewrite the code to avoid the graph break. For more information, see this Dynamo guide.

vLLM requires that the model's forward pass be capturable into a full graph that is dynamic on the batch size (i.e. the number of tokens). It (by default) compiles this one graph into one artifact and uses this artifact for all batch sizes.

If your code cannot be captured with Dynamic Shapes, you may see silent incorrectness, loud errors, or CUDA illegal memory accesses. For example, the following is not capturable into a single graph:

This problem is easy to diagnose. Use tlparse and click on compilation_metrics: it will tell you symbolic constraints on the batch size. If there is any constraint that restricts the batch sizes, then we've got a problem.

To avoid this, please either:

Dynamic-shape guards are a specific category of Dynamo guards. They are constraints that torch.compile attaches to dynamic dimensions (e.g., seq_len) to ensure the compiled artifact remains valid. These guards typically appear when framework code, custom passes, or user code branches based on dynamic shape values.

This creates a guard x > 10 or x <= 10 depending on which path was traced.

vLLM's Assumption: vLLM assumes that all guards added by torch.compile are safe to drop and will not constrain the compiled graph to specific input shapes. When this assumption is violated, it can cause issues that users need to debug. Some side effects that indicates this assumption is violated are runtime errors or ConstraintViolationErrors.

A ConstraintViolationErrors will be thrown if a dynamic shape gets constrained to a single value. If you encounter a constraint violation error or suspect that a dynamic shapes guard is being added incorrectly, you can use stricter dynamic shape modes to help debug the issue:

These modes are stricter and reduce or eliminate the need of dynamic shapes guarding, which can help isolate issues:

For more details on dynamic shapes modes, see Dynamic shapes and vLLM guard dropping.

To see all guards that are being added during compilation, you can use TORCH_LOGS=+dynamic:

Look for [guard added] in the logs to see where guards are being added. This can help you identify which operations are causing guards to be added incorrectly.

TorchInductor takes a captured graph and then compiles it down to some Python code that may call 1+ triton kernels. On rare (but unfortunate) occasions, it may produce an incorrect triton kernel. This may manifest as silent incorrectness, CUDA illegal memory accesses, or loud errors.

To debug if TorchInductor is at fault, you can disable it by passing backend='eager' to the compilation config:

If Inductor is at fault, file a bug to PyTorch. If you're feeling adventurous, you can debug the triton kernels in the Inductor output code (that you can locate via using tlparse).

You can also use TORCH_LOGS=output_code <command> to print the Inductor output code.

You can edit the TorchInductor code that gets run by setting VLLM_COMPILE_CACHE_SAVE_FORMAT=unpacked or passing -cc.compile_cache_save_format=unpacked. The default is binary, which means it is not editable.

This is a useful technique: you can put breakpoints (e.g. torch.distributed.breakpoint()) and print statements in the output code.

vLLM built its own cache for torch.compile artifacts. The idea is that the artifacts can be compiled once and then reused after they have been compiled. This is a layer on top of torch.compile's compiler cache.

While torch.compile's compiler cache is rock-stable, vLLM's compiler cache is unfortunately not always correct. You can disable it via setting VLLM_DISABLE_COMPILE_CACHE=1.

You can also manually remove this cache.

vLLM's cache is a mapping from cache key to a compiled artifact. vLLM computes the cache key via combining multiple factors (e.g. config flags and model name). If vLLM's compile cache is wrong, this usually means that a factor is missing. Please see this example of how vLLM computes part of the cache key.

CUDAGraphs is a feature that allows one to:

The captured CUDAGraph contains all of the memory used during the capture process. The replay of the CUDAGraph reads and writes to exactly the same regions of memory.

This leads to some restrictions:

vLLM uses the raw CUDAGraphs API, which is unsafe when used incorrectly.

To turn off just CUDAGraphs, pass cudagraph_mode = NONE:

**Examples:**

Example 1 (unknown):
```unknown
pip install tlparse
```

Example 2 (unknown):
```unknown
pip install tlparse
```

Example 3 (typescript):
```typescript
TORCH_TRACE=~/trace_dir python my_script.py
tlparse ~/trace_dir/<the_first_log_file>
```

Example 4 (typescript):
```typescript
TORCH_TRACE=~/trace_dir python my_script.py
tlparse ~/trace_dir/<the_first_log_file>
```

---

## Hybrid KV Cache Manager - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/

**Contents:**
- Hybrid KV Cache Manager¶
- What is a hybrid model?¶
- Definitions¶
- Allocation¶
  - High level idea¶
  - Case 1: toy model¶
  - Case 2: same kv_hidden_size and a regular pattern¶
  - Case 3: same kv_hidden_size and no regular pattern¶
  - Case 4: different kv_hidden_size (mainly hybrid mamba models)¶
  - Case 5: KV sharing¶

This document was written based on commit 458e74. This feature is still in its early stage and things may change.

Many recent "hybrid" LLMs combine multiple attention types within one model. For example:

To serve these models efficiently, our KVCacheManager must:

page size: the physical memory size of a block, defined as:

num_layers doesn't mean the total number of layers in the model. The exact number depends on the context in this doc.

This is different from KVCacheSpec.page_size_bytes in the code, which is defined as:

We use a single memory pool for all layer types. The memory pool is split into multiple blocks with the same page size. KVCacheManager allocates different numbers of blocks to different layers according to its attention type.

The core challenge is ensuring every layer type uses the same page size. For full-attention-only models, the page size is straightforward, defined as:

However, in hybrid models, num_hidden_layers varies by attention type, which would normally produce mismatched page sizes. The cases below show how we unify them.

Let's start with a toy example: a model has 1 full attention layer and 3 sliding window attention layers. All layers have the same kv_hidden_size.

We let each block to hold block_size tokens for one layer, so:

KVCacheManager allocates a different number of blocks to each layer.

This case is only a toy example. For real models, please refer to the following cases.

When the model has more layers, e.g., 20 sliding window attention layers and 10 full attention layers with the same kv_hidden_size. Calling the allocator once per layer (30 calls) is OK but becomes inefficient. As a solution, we group the allocation of layers that need the same number of blocks to reduce the number of calls.

The grouping is feasible because there is usually a beautiful ratio between the number of different types of layers. For example:

Our example can be regarded as 2 sw : 1 full. We can allocate blocks as if there are 2 sw and 1 full in the model, and repeat the result by 10 times to generate the block_ids for the 30 layers. The page size becomes:

Assume block_size 16, sliding window size 32, request length 112, then for the above example model, we need to allocate 11 blocks (0-6 for full, 7-8 for sw group 1, 9-10 for sw group 2).

Here, "/" denotes no block needed (sliding‑window layers don't need slots for early tokens).

See the formal definition below. The layers are divided into multiple KV Cache Groups so that there is:

Our example model is divided into 3 KV cache groups:

Obviously, it satisfies rule 1. For rule 2, all 3 groups have

Unfortunately, not all models have such a beautiful ratio, and approach in Case 2 will produce too many small groups. For example, Gemma-3-27b has 52 sliding window attention layers and 10 full attention layers. With the constraints in case 2, it would be 26 sliding window groups and 5 full attention groups, each contains 2 layers. The allocation is still inefficient. To reduce the number of kv cache groups, we group layers using the smallest layer count among all attention types. For example, min(52, 10)=10 layers per group in Gemma-3-27b. Then the grouping result is:

We will update this algorithm if this heuristic leads to a bad result when a new model comes out (e.g., 20 full + 30 sw, the group size should be 10 instead of 20).

This case happens in Gemma-3 series models, and models in case 2 but with eagle speculative decoding which introduce one full attention layer. The solution has some memory waste and is not perfect. Please report any cases where padding overhead becomes unacceptable so we can refine the algorithm.

Some architectures (e.g., Bamba, Jamba, Minimax) interleave standard attention layers with Mamba layers, where each Mamba layer's state size per token can be much larger than the attention layers' kv_hidden_size. Because we only support a single page size across all groups, we must reconcile these differing hidden sizes.

The current algorithm is:

This can lead to more than 400 block_size for attention layers, which is too large. Another padding strategy is to increase block_size until

This padding strategy is still a work in progress.

KV sharing refers to a layer using the KV cache of another layer, e.g., gemma-3n. In these models, KVCacheManager ignores all layers with kv sharing and only allocates KV cache for layers that need kv cache, and some patches are made in model runner to apply the allocation result to kv sharing layers.

For simplicity, we assume block_size=1 in this section.

The block pool uses a dict similar to tuple(block_hash, group_id) -> block to catch the full blocks. That means the same tokens of different groups are cached and evicted independently.

When a new request comes in, we check the cache hit prefix of each group, and return the intersection of these groups as the cached prefix of the request. See below for the detailed algorithm for checking the cache hit of one group & performing the intersection.

For full attention layers, blocks are allocated for all tokens in the request. For details on the underlying design, see Prefix Caching

To find the longest cache hit prefix of a request, we enumerate from left (the first block) to right (the last block), checking whether the block is cached, and exit when cache misses. For example, we will return the first 7 tokens (0-6) as the cache hit prefix in the below example (blue blocks are cached):

For sliding window attention layers, a naive implementation for memory allocation is to allocate sliding_window_size blocks and fill in the blocks in a round-robin way. But this naive implementation is not compatible with prefix caching so we didn't pick this design. In vLLM, we allocate different blocks for different tokens and free blocks that are outside the sliding window.

For a new request, the cache hit prefix only requires the last sliding_window_size - 1 tokens being cached. Let's say sliding_window_size = 4 and block_size = 1, and the request is a 15-token prompt (blue blocks are cached):

There are 3 possible cache hit prefixes:

We can check the cache hit from right to left, and early exit when we find a match.This is opposite from full attention, where we check from left to right and early exit when the match fails. One potential cons (compared to full attention) is that we end up iterating over the entire list of tokens when there's no match, which is often a common case. This could potentially cause non-negligible overheads, but fine with full + swa, as discussed below.

The first problem is how to find the cache hit prefix. We need to "intersect" the cache hits of global and sliding window attention layers by:

It can be ensured that the resulting cache hit of sliding window attention layers is also a cache hit of full attention layers. This is more efficient than finding all possible prefixes of each group and doing the intersection, because our approach can exit early if there is no cache hit.

The algorithm applies to models with exactly two attention types full attention + X, where X can be an arbitrary efficient attention algorithm like sliding window, llama 4 local attention, and mamba. It doesn't support models without full attention layers, and models with more than 2 types of attention. This is enough for most hybrid models at the moment of writing this doc.

The second question is the cache eviction policy. For now, we use one LRU queue for all kv cache groups. The blocks are added to the LRU queue when freed, either because the request is finished or the block is out of the sliding window.

The prefix caching support of the mamba model is work in progress. Once implemented, models with mamba layer + full attention layer can be supported via the full attention + X algorithm in case 2.

The KVCacheManager is organized into 3 layers:

The blue box in the above figure shows the case with 10 full attention layers and 20 sliding window attention layers, thus:

For a model with n KVCacheGroups, each with m layers, we allocate m buffers. Each buffer is shared by n layers, one from each group.

The following figure is for a model with 10 full attention layers (full.0 - full.9) and 20 sliding window attention layers (sw.0-sw.19). It follows "case 2" in "Allocation" section and is divided into 3 groups:

And for a request, we allocate 11 blocks with block_id 0-6 to group 0, 7-8 to group 1, and 9-10 to group 2.

With such an example, the physical memory is divided into 10 buffers (KVCacheTensor 0 - KVCacheTensor 9). Each buffer is shared by 3 layers (e.g., KVCacheTensor 0 is shared by full.0 from group 0, sw.0 from group 1, and sw.10 from group 2) and is divided into pieces with size block_size * kv_hidden_size. The KV cache of these 3 attention layers are saved to different pieces of the buffer based on the allocated block_ids:

One logic "block" is mapped to 10 pieces in the 10 buffers of the physical memory.

---

## Integration with Hugging Face - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/huggingface_integration/

**Contents:**
- Integration with Hugging Face¶

This document describes how vLLM integrates with Hugging Face libraries. We will explain step by step what happens under the hood when we run vllm serve.

Let's say we want to serve the popular Qwen model by running vllm serve Qwen/Qwen2-7B.

The model argument is Qwen/Qwen2-7B. vLLM determines whether this model exists by checking for the corresponding config file config.json. See this code snippet for the implementation. Within this process:

After confirming the existence of the model, vLLM loads its config file and converts it into a dictionary. See this code snippet for the implementation.

Next, vLLM inspects the model_type field in the config dictionary to generate the config object to use. There are some model_type values that vLLM directly supports; see here for the list. If the model_type is not in the list, vLLM will use AutoConfig.from_pretrained to load the config class, with model, --revision, and --trust_remote_code as the arguments. Please note that:

Subsequently, vLLM applies some historical patches to the config object. These are mostly related to RoPE configuration; see here for the implementation.

Finally, vLLM can reach the model class we want to initialize. vLLM uses the architectures field in the config object to determine the model class to initialize, as it maintains the mapping from architecture name to model class in its registry. If the architecture name is not found in the registry, it means this model architecture is not supported by vLLM. For Qwen/Qwen2-7B, the architectures field is ["Qwen2ForCausalLM"], which corresponds to the Qwen2ForCausalLM class in vLLM's code. This class will initialize itself depending on various configs.

Beyond that, there are two more things vLLM depends on Hugging Face for.

Tokenizer: vLLM uses the tokenizer from Hugging Face to tokenize the input text. The tokenizer is loaded using AutoTokenizer.from_pretrained with the model argument as the model name and the --revision argument as the revision. It is also possible to use a tokenizer from another model by specifying the --tokenizer argument in the vllm serve command. Other relevant arguments are --tokenizer-revision and --tokenizer-mode. Please check Hugging Face's documentation for the meaning of these arguments. This part of the logic can be found in the get_tokenizer function. After obtaining the tokenizer, notably, vLLM will cache some expensive attributes of the tokenizer in vllm.tokenizers.hf.get_cached_tokenizer.

Model weight: vLLM downloads the model weight from the Hugging Face model hub using the model argument as the model name and the --revision argument as the revision. vLLM provides the argument --load-format to control what files to download from the model hub. By default, it will try to load the weights in the safetensors format and fall back to the PyTorch bin format if the safetensors format is not available. We can also pass --load-format dummy to skip downloading the weights.

This completes the integration between vLLM and Hugging Face.

In summary, vLLM reads the config file config.json, tokenizer, and model weight from the Hugging Face model hub or a local directory. It uses the config class from either vLLM, Hugging Face transformers, or loads the config class from the model's repository.

---

## P2P NCCL Connector - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/

**Contents:**
- P2P NCCL Connector¶
- Detailed Design¶
  - Overall Process¶
  - Proxy/Router (Demo)¶
  - KV Cache Transfer Methods¶
  - P2P Communication via ZMQ & NCCL¶
  - NCCL Group Topology¶
  - GPU Memory Buffer and Tensor Memory Pool¶
- Install vLLM¶
- Run xPyD¶

An implementation of xPyD with dynamic scaling based on point-to-point communication, partly inspired by Dynamo.

As shown in Figure 1, the overall process of this PD disaggregation solution is described through a request flow:

A simple HTTP service acts as the entry point for client requests and starts a background thread to listen for P/D instances reporting their HTTP IP and PORT, as well as ZMQ IP and PORT. It maintains a dictionary of http_addr -> zmq_addr. The http_addr is the IP:PORT for the vLLM instance's request, while the zmq_addr is the address for KV cache handshake and metadata reception.

The Proxy/Router is responsible for selecting 1P1D based on the characteristics of the client request, such as the prompt, and generating a corresponding request_id, for example:

Currently, to quickly verify whether xPyD can work, a round-robin selection of 1P1D is used. In the future, it is planned to use a trie combined with the load status of instances to select appropriate P and D.

Each P/D instance periodically sends a heartbeat packet to the Proxy/Router (currently every 3 seconds) to register (i.e., report http_addr -> zmq_addr) and keep the connection alive. If an instance crashes and fails to send a ping for a certain period of time, the Proxy/Router will remove the timed-out instance (this feature has not yet been developed).

There are three methods for KVCache transfer: PUT, GET, and PUT_ASYNC. These methods can be specified using the --kv-transfer-config and kv_connector_extra_config parameters, specifically through the send_type field. Both PUT and PUT_ASYNC involve the P instance actively sending KVCache to the D instance. The difference is that PUT is a synchronous transfer method that blocks the main process, while PUT_ASYNC is an asynchronous transfer method. PUT_ASYNC uses a dedicated thread for sending KVCache, which means it does not block the main process. In contrast, the GET method involves the P instance saving the KVCache to the memory buffer after computing the prefill. The D instance then actively retrieves the computed KVCache from the P instance once it has allocated space for the KVCache.

Experimental results have shown that the performance of these methods, from highest to lowest, is as follows: PUT_ASYNC → GET → PUT.

As long as the address of the counterpart is known, point-to-point KV cache transfer (using NCCL) can be performed, without being constrained by rank and world size. To support dynamic scaling (expansion and contraction) of instances with PD disaggregation. This means that adding or removing P/D instances does not require a full system restart.

Each P/D instance only needs to create a single P2pNcclEngine instance. This instance maintains a ZMQ Server, which runs a dedicated thread to listen on the zmq_addr address and receive control flow requests from other instances. These requests include requests to establish an NCCL connection and requests to send KVCache metadata (such as tensor shapes and data types). However, it does not actually transmit the KVCache data itself.

When a P instance and a D instance transmit KVCache for the first time, they need to establish a ZMQ connection and an NCCL group. For subsequent KVCache transmissions, this ZMQ connection and NCCL group are reused. The NCCL group consists of only two ranks, meaning the world size is equal to 2. This design is intended to support dynamic scaling, which means that adding or removing P/D instances does not require a full system restart. As long as the address of the counterpart is known, point-to-point KVCache transmission can be performed, without being restricted by rank or world size.

Currently, only symmetric TP (Tensor Parallelism) methods are supported for KVCache transmission. Asymmetric TP and PP (Pipeline Parallelism) methods will be supported in the future. Figure 2 illustrates the 1P2D setup, where each instance has a TP (Tensor Parallelism) degree of 2. There are a total of 7 NCCL groups: three vLLM instances each have one NCCL group with TP=2. Additionally, the 0th GPU card of the P instance establishes an NCCL group with the 0th GPU card of each D instance. Similarly, the 1st GPU card of the P instance establishes an NCCL group with the 1st GPU card of each D instance.

Each NCCL group occupies a certain amount of GPU memory buffer for communication, the size of which is primarily influenced by the NCCL_MAX_NCHANNELS environment variable. When NCCL_MAX_NCHANNELS=16, an NCCL group typically occupies 100MB, while when NCCL_MAX_NCHANNELS=8, it usually takes up 52MB. For large-scale xPyD configurations—such as DeepSeek's 96P144D—this implementation is currently not feasible. Moving forward, we are considering using RDMA for point-to-point communication and are also keeping an eye on UCCL.

The trade-off in the size of the memory buffer is as follows: For P instances, the memory buffer is not required in PUT and PUT_ASYNC modes, but it is necessary in GET mode. For D instances, a memory buffer is needed in all three modes. The memory buffer for D instances should not be too large. Similarly, for P instances in GET mode, the memory buffer should also not be too large. The memory buffer of D instances is used to temporarily store KVCache sent by P instances. If it is too large, it will reduce the KVCache space available for normal inference by D instances, thereby decreasing the inference batch size and ultimately leading to a reduction in output throughput. The size of the memory buffer is configured by the parameter kv_buffer_size, measured in bytes, and is typically set to 5%～10% of the memory size.

If the --max-num-seqs parameter for P instances is set to a large value, due to the large batch size, P instances will generate a large amount of KVCache simultaneously. This may exceed the capacity of the memory buffer of D instances, resulting in KVCache loss. Once KVCache is lost, D instances need to recompute Prefill, which is equivalent to performing Prefill twice. Consequently, the time-to-first-token (TTFT) will significantly increase, leading to degraded performance.

To address the above issues, I have designed and developed a local Tensor memory pool for storing KVCache, inspired by the buddy system used in Linux memory modules. Since the memory is sufficiently large, typically in the TB range on servers, there is no need to consider prefix caching or using block-based designs to reuse memory, thereby saving space. When the memory buffer is insufficient, KVCache can be directly stored in the Tensor memory pool, and D instances can subsequently retrieve KVCache from it. The read and write speed is that of PCIe, with PCIe 4.0 having a speed of approximately 21 GB/s, which is usually faster than the Prefill speed. Otherwise, solutions like Mooncake and lmcache would not be necessary. The Tensor memory pool acts as a flood diversion area, typically unused except during sudden traffic surges. In the worst-case scenario, my solution performs no worse than the normal situation with a Cache store.

**Examples:**

Example 1 (json):
```json
cmpl-___prefill_addr_10.0.1.2:21001___decode_addr_10.0.1.3:22001_93923d63113b4b338973f24d19d4bf11-0
```

Example 2 (json):
```json
cmpl-___prefill_addr_10.0.1.2:21001___decode_addr_10.0.1.3:22001_93923d63113b4b338973f24d19d4bf11-0
```

Example 3 (unknown):
```unknown
pip install "vllm>=0.9.2"
```

Example 4 (unknown):
```unknown
pip install "vllm>=0.9.2"
```

---

## Paged Attention - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/paged_attention/

**Contents:**
- Paged Attention¶
- Inputs¶
- Concepts¶
- Query¶
- Key¶
- QK¶
- Softmax¶
  - qk_max and logits¶
  - exp_sum¶
- Value¶

This is a historical document based on the original paper for vLLM. It no longer describes the code used in vLLM today.

Currently, vLLM utilizes its own implementation of a multi-head query attention kernel (csrc/attention/attention_kernels.cu). This kernel is designed to be compatible with vLLM's paged KV caches, where the key and value cache are stored in separate blocks (note that this block concept differs from the GPU thread block. So in a later document, I will refer to vLLM paged attention block as "block", while refer to GPU thread block as "thread block").

To achieve high performance, this kernel relies on a specially designed memory layout and access method, specifically when threads read data from global memory to shared memory. The purpose of this document is to provide a high-level explanation of the kernel implementation step by step, aiding those who wish to learn about the vLLM multi-head query attention kernel. After going through this document, users will likely have a better understanding and feel easier to follow the actual implementation.

Please note that this document may not cover all details, such as how to calculate the correct index for the corresponding data or the dot multiplication implementation. However, after reading this document and becoming familiar with the high-level logic flow, it should be easier for you to read the actual code and understand the details.

The kernel function takes a list of arguments for the current thread to perform its assigned work. The three most important arguments are the input pointers q, k_cache, and v_cache, which point to query, key, and value data on global memory that need to be read and processed. The output pointer out points to global memory where the result should be written. These four pointers actually refer to multidimensional arrays, but each thread only accesses the portion of data assigned to it. I have omitted all other runtime parameters here for simplicity.

There are also a list of template arguments above the function signature that are determined during compilation time. scalar_t represents the data type of the query, key, and value data elements, such as FP16. HEAD_SIZE indicates the number of elements in each head. BLOCK_SIZE refers to the number of tokens in each block. NUM_THREADS denotes the number of threads in each thread block. PARTITION_SIZE represents the number of tensor parallel GPUs (For simplicity, we assume this is 0 and tensor parallel is disabled).

With these arguments, we need to perform a sequence of preparations. This includes calculating the current head index, block index, and other necessary variables. However, for now, we can ignore these preparations and proceed directly to the actual calculations. It will be easier to understand them once we grasp the entire flow.

Just before we dive into the calculation flow, I want to describe a few concepts that are needed for later sections. However, you may skip this section and return later if you encounter any confusing terminologies.

This section will introduce how query data is stored in memory and fetched by each thread. As mentioned above, each thread group fetches one query token data, while each thread itself only handles a part of one query token data. Within each warp, every thread group will fetch the same query token data, but will multiply it with different key token data.

Each thread defines its own q_ptr which points to the assigned query token data on global memory. For example, if VEC_SIZE is 4 and HEAD_SIZE is 128, the q_ptr points to data that contains total of 128 elements divided into 128 / 4 = 32 vecs.

Next, we need to read the global memory data pointed to by q_ptr into shared memory as q_vecs. It is important to note that each vecs is assigned to a different row. For example, if the THREAD_GROUP_SIZE is 2, thread 0 will handle the 0th row vecs, while thread 1 handles the 1st row vecs. By reading the query data in this way, neighboring threads like thread 0 and thread 1 can read neighbor memory, achieving the memory coalescing to improve performance.

Similar to the "Query" section, this section introduces memory layout and assignment for keys. While each thread group only handle one query token one kernel run, it may handle multiple key tokens across multiple iterations. Meanwhile, each warp will process multiple blocks of key tokens in multiple iterations, ensuring that all context tokens are processed by the entire thread group after the kernel run. In this context, "handle" refers to performing the dot multiplication between query data and key data.

Unlike to q_ptr, k_ptr in each thread will point to different key token at different iterations. As shown above, that k_ptr points to key token data based on k_cache at assigned block, assigned head and assigned token.

The diagram above illustrates the memory layout for key data. It assumes that the BLOCK_SIZE is 16, HEAD_SIZE is 128, x is 8, THREAD_GROUP_SIZE is 2, and there are a total of 4 warps. Each rectangle represents all the elements for one key token at one head, which will be processed by one thread group. The left half shows the total 16 blocks of key token data for warp 0, while the right half represents the remaining key token data for other warps or iterations. Inside each rectangle, there are a total 32 vecs (128 elements for one token) that will be processed by 2 threads (one thread group) separately.

Next, we need to read the key token data from k_ptr and store them on register memory as k_vecs. We use register memory for k_vecs because it will only be accessed by one thread once, whereas q_vecs will be accessed by multiple threads multiple times. Each k_vecs will contain multiple vectors for later calculation. Each vec will be set at each inner iteration. The assignment of vecs allows neighboring threads in a warp to read neighboring memory together, which again promotes the memory coalescing. For instance, thread 0 will read vec 0, while thread 1 will read vec 1. In the next inner loop, thread 0 will read vec 2, while thread 1 will read vec 3, and so on.

You may still be a little confused about the overall flow. Don't worry, please keep reading the next "QK" section. It will illustrate the query and key calculation flow in a clearer and higher-level manner.

As shown the pseudocode below, before the entire for loop block, we fetch the query data for one token and store it in q_vecs. Then, in the outer for loop, we iterate through different k_ptrs that point to different tokens and prepare the k_vecs in the inner for loop. Finally, we perform the dot multiplication between the q_vecs and each k_vecs.

As mentioned before, for each thread, it only fetches part of the query and key token data at a time. However, there will be a cross thread group reduction happen in the Qk_dot<>::dot . So qk returned here is not just between part of the query and key token dot multiplication, but actually a full result between entire query and key token data.

For example, if the value of HEAD_SIZE is 128 and THREAD_GROUP_SIZE is 2, each thread's k_vecs will contain total 64 elements. However, the returned qk is actually the result of dot multiplication between 128 query elements and 128 key elements. If you want to learn more about the details of the dot multiplication and reduction, you may refer to the implementation of Qk_dot<>::dot. However, for the sake of simplicity, I will not cover it in this document.

Next, we need to calculate the normalized softmax for all qks, as shown above, where each \(x\) represents a qk. To do this, we must obtain the reduced value of qk_max(\(m(x)\)) and the exp_sum(\(\ell(x)\)) of all qks. The reduction should be performed across the entire thread block, encompassing results between the query token and all context key tokens.

Just right after we get the qk result, we can set the temporary logits result with qk (In the end, the logits should store the normalized softmax result). Also we can compare and collect the qk_max for all qks that are calculated by current thread group.

Please note that the logits here is on shared memory, so each thread group will set the fields for its own assigned context tokens. Overall, the size of logits should be number of context tokens.

Then we need to get the reduced qk_max across each warp. The main idea is to make threads in warp to communicate with each other and get the final max qk .

Finally, we can get the reduced qk_max from whole thread block by compare the qk_max from all warps in this thread block. Then we need to broadcast the final result to each thread.

Similar to qk_max, we need to get the reduced sum value from the entire thread block too.

Firstly, sum all exp values from each thread group, and meanwhile, convert each entry of logits from qk to exp(qk - qk_max). Please note, the qk_max here is already the max qk across the whole thread block. And then we can do reduction for exp_sum across whole thread block just like the qk_max.

Finally, with the reduced qk_max and exp_sum, we can obtain the final normalized softmax result as logits. This logits variable will be used for dot multiplication with the value data in later steps. Now, it should store the normalized softmax result of qk for all assigned context tokens.

Now we need to retrieve the value data and perform dot multiplication with logits. Unlike query and key, there is no thread group concept for value data. As shown in diagram, different from key token memory layout, elements from the same column correspond to the same value token. For one block of value data, there are HEAD_SIZE of rows and BLOCK_SIZE of columns that are split into multiple v_vecs.

Each thread always fetches V_VEC_SIZE elements from the same V_VEC_SIZE of tokens at a time. As a result, a single thread retrieves multiple v_vecs from different rows and the same columns through multiple inner iterations. For each v_vec, it needs to be dot multiplied with the corresponding logits_vec, which is also V_VEC_SIZE elements from logits. Overall, with multiple inner iterations, each warp will process one block of value tokens. And with multiple outer iterations, the whole context value tokens are processed

As shown in the above pseudocode, in the outer loop, similar to k_ptr, logits_vec iterates over different blocks and reads V_VEC_SIZE elements from logits. In the inner loop, each thread reads V_VEC_SIZE elements from the same tokens as a v_vec and performs dot multiplication. It is important to note that in each inner iteration, the thread fetches different head position elements for the same tokens. The dot result is then accumulated in accs. Therefore, each entry of accs is mapped to a head position assigned to the current thread.

For example, if BLOCK_SIZE is 16 and V_VEC_SIZE is 8, each thread fetches 8 value elements for 8 tokens at a time. Each element is from different tokens at the same head position. If HEAD_SIZE is 128 and WARP_SIZE is 32, for each inner loop, a warp needs to fetch WARP_SIZE * V_VEC_SIZE = 256 elements. This means there are a total of 128 * 16 / 256 = 8 inner iterations for a warp to handle a whole block of value tokens. And each accs in each thread contains 8 elements that accumulated at 8 different head positions. For the thread 0, the accs variable will have 8 elements, which are 0th, 32nd … 224th elements of a value head that are accumulated from all assigned 8 tokens.

Now, we need to perform reduction for accs within each warp. This process allows each thread to accumulate the accs for the assigned head positions of all tokens in one block.

Next, we perform reduction for accs across all warps, allowing each thread to have the accumulation of accs for the assigned head positions of all context tokens. Please note that each accs in every thread only stores the accumulation for a portion of elements of the entire head for all context tokens. However, overall, all results for output have been calculated but are just stored in different thread register memory.

Now we can write all of calculated result from local register memory to final output global memory.

First, we need to define the out_ptr variable, which points to the start address of the assigned sequence and assigned head.

Finally, we need to iterate over different assigned head positions and write out the corresponding accumulated result based on the out_ptr.

**Examples:**

Example 1 (jsx):
```jsx
template<typename scalar_t, int HEAD_SIZE, int BLOCK_SIZE, int NUM_THREADS, int PARTITION_SIZE = 0>
__device__ void paged_attention_kernel(
    ... // Other side args.
    const scalar_t* __restrict__ out,       // [num_seqs, num_heads, max_num_partitions, head_size]
    const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]
    const scalar_t* __restrict__ k_cache,   // [num_blocks, num_kv_heads, head_size/x, block_size, x]
    const scalar_t* __restrict__ v_cache,   // [num_blocks, num_kv_heads, head_size, block_size]
    ... // Other side args.
)
```

Example 2 (jsx):
```jsx
template<typename scalar_t, int HEAD_SIZE, int BLOCK_SIZE, int NUM_THREADS, int PARTITION_SIZE = 0>
__device__ void paged_attention_kernel(
    ... // Other side args.
    const scalar_t* __restrict__ out,       // [num_seqs, num_heads, max_num_partitions, head_size]
    const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]
    const scalar_t* __restrict__ k_cache,   // [num_blocks, num_kv_heads, head_size/x, block_size, x]
    const scalar_t* __restrict__ v_cache,   // [num_blocks, num_kv_heads, head_size, block_size]
    ... // Other side args.
)
```

Example 3 (unknown):
```unknown
const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
```

Example 4 (unknown):
```unknown
const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
```

---

## Plugin System - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/plugin_system/

**Contents:**
- Plugin System¶
- How Plugins Work in vLLM¶
- How vLLM Discovers Plugins¶
- Types of supported plugins¶
- Guidelines for Writing Plugins¶
  - Platform plugins guidelines¶
- Compatibility Guarantee¶
- Deprecation announcement¶

The community frequently requests the ability to extend vLLM with custom features. To facilitate this, vLLM includes a plugin system that allows users to add custom features without modifying the vLLM codebase. This document explains how plugins work in vLLM and how to create a plugin for vLLM.

Plugins are user-registered code that vLLM executes. Given vLLM's architecture (see Arch Overview), multiple processes may be involved, especially when using distributed inference with various parallelism techniques. To enable plugins successfully, every process created by vLLM needs to load the plugin. This is done by the load_plugins_by_group function in the vllm.plugins module.

vLLM's plugin system uses the standard Python entry_points mechanism. This mechanism allows developers to register functions in their Python packages for use by other packages. An example of a plugin:

For more information on adding entry points to your package, please check the official documentation.

Every plugin has three parts:

General plugins (with group name vllm.general_plugins): The primary use case for these plugins is to register custom, out-of-the-tree models into vLLM. This is done by calling ModelRegistry.register_model to register the model inside the plugin function.

Platform plugins (with group name vllm.platform_plugins): The primary use case for these plugins is to register custom, out-of-the-tree platforms into vLLM. The plugin function should return None when the platform is not supported in the current environment, or the platform class's fully qualified name when the platform is supported.

IO Processor plugins (with group name vllm.io_processor_plugins): The primary use case for these plugins is to register custom pre-/post-processing of the model prompt and model output for pooling models. The plugin function returns the IOProcessor's class fully qualified name.

Stat logger plugins (with group name vllm.stat_logger_plugins): The primary use case for these plugins is to register custom, out-of-the-tree loggers into vLLM. The entry point should be a class that subclasses StatLoggerBase.

Create a platform plugin project, for example, vllm_add_dummy_platform. The project structure should look like this:

In the setup.py file, add the following entry point:

Please make sure vllm_add_dummy_platform:register is a callable function and returns the platform class's fully qualified name. for example:

Implement the platform class MyDummyPlatform in my_dummy_platform.py. The platform class should inherit from vllm.platforms.interface.Platform. Please follow the interface to implement the functions one by one. There are some important functions and properties that should be implemented at least:

Implement the worker class MyDummyWorker in my_dummy_worker.py. The worker class should inherit from WorkerBase. Please follow the interface to implement the functions one by one. Basically, all interfaces in the base class should be implemented, since they are called here and there in vLLM. To make sure a model can be executed, the basic functions should be implemented are:

Additional functions that can be implemented are:

Please look at the worker base class WorkerBase for more functions that can be implemented.

Implement the attention backend class MyDummyAttention in my_dummy_attention.py. The attention backend class should inherit from AttentionBackend. It's used to calculate attentions with your device. Take vllm.v1.attention.backends as examples, it contains many attention backend implementations.

Implement custom ops for high performance. Most ops can be ran by pytorch native implementation, while the performance may not be good. In this case, you can implement specific custom ops for your plugins. Currently, there are kinds of custom ops vLLM supports:

pytorch ops there are 3 kinds of pytorch ops:

triton ops Custom way doesn't work for triton ops now.

(optional) Implement other plugable modules, such as lora, graph backend, quantization, mamba attention backend, etc.

vLLM guarantees the interface of documented plugins, such as ModelRegistry.register_model, will always be available for plugins to register models. However, it is the responsibility of plugin developers to ensure their plugins are compatible with the version of vLLM they are targeting. For example, "vllm_add_dummy_model.my_llava:MyLlava" should be compatible with the version of vLLM that the plugin targets.

The interface for the model/module may change during vLLM's development. If you see any deprecation log info, please upgrade your plugin to the latest version.

**Examples:**

Example 1 (python):
```python
# inside `setup.py` file
from setuptools import setup

setup(name='vllm_add_dummy_model',
    version='0.1',
    packages=['vllm_add_dummy_model'],
    entry_points={
        'vllm.general_plugins':
        ["register_dummy_model = vllm_add_dummy_model:register"]
    })

# inside `vllm_add_dummy_model.py` file
def register():
    from vllm import ModelRegistry

    if "MyLlava" not in ModelRegistry.get_supported_archs():
        ModelRegistry.register_model(
            "MyLlava",
            "vllm_add_dummy_model.my_llava:MyLlava",
        )
```

Example 2 (python):
```python
# inside `setup.py` file
from setuptools import setup

setup(name='vllm_add_dummy_model',
    version='0.1',
    packages=['vllm_add_dummy_model'],
    entry_points={
        'vllm.general_plugins':
        ["register_dummy_model = vllm_add_dummy_model:register"]
    })

# inside `vllm_add_dummy_model.py` file
def register():
    from vllm import ModelRegistry

    if "MyLlava" not in ModelRegistry.get_supported_archs():
        ModelRegistry.register_model(
            "MyLlava",
            "vllm_add_dummy_model.my_llava:MyLlava",
        )
```

Example 3 (unknown):
```unknown
vllm_add_dummy_platform/
├── vllm_add_dummy_platform/
│   ├── __init__.py
│   ├── my_dummy_platform.py
│   ├── my_dummy_worker.py
│   ├── my_dummy_attention.py
│   ├── my_dummy_device_communicator.py
│   ├── my_dummy_custom_ops.py
├── setup.py
```

Example 4 (unknown):
```unknown
vllm_add_dummy_platform/
├── vllm_add_dummy_platform/
│   ├── __init__.py
│   ├── my_dummy_platform.py
│   ├── my_dummy_worker.py
│   ├── my_dummy_attention.py
│   ├── my_dummy_device_communicator.py
│   ├── my_dummy_custom_ops.py
├── setup.py
```

---

## Python Multiprocessing - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/multiprocessing/

**Contents:**
- Python Multiprocessing¶
- Debugging¶
- Introduction¶
- Multiprocessing Methods¶
  - Tradeoffs¶
- Compatibility with Dependencies¶
- Current State (v0)¶
- Prior State in v1¶
  - Changes Made in v1¶
- Alternatives Considered¶

Please see the Troubleshooting page for information on known issues and how to solve them.

The source code references are to the state of the code at the time of writing in December 2024.

The use of Python multiprocessing in vLLM is complicated by:

This document describes how vLLM deals with these challenges.

Python multiprocessing methods include:

spawn - spawn a new Python process. The default on Windows and macOS.

fork - Use os.fork() to fork the Python interpreter. The default on Linux for Python versions prior to 3.14.

forkserver - Spawn a server process that will fork a new process on request. The default on Linux for Python version 3.14 and newer.

fork is the fastest method, but is incompatible with dependencies that use threads. If you are under macOS, using fork may cause the process to crash.

spawn is more compatible with dependencies, but can be problematic when vLLM is used as a library. If the consuming code does not use a __main__ guard (if __name__ == "__main__":), the code will be inadvertently re-executed when vLLM spawns a new process. This can lead to infinite recursion, among other problems.

forkserver will spawn a new server process that will fork new processes on demand. This unfortunately has the same problem as spawn when vLLM is used as a library. The server process is created as a spawned new process, which will re-execute code not protected by a __main__ guard.

For both spawn and forkserver, the process must not depend on inheriting any global state as would be the case with fork.

Multiple vLLM dependencies indicate either a preference or requirement for using spawn:

It is perhaps more accurate to say that there are known problems with using fork after initializing these dependencies.

The environment variable VLLM_WORKER_MULTIPROC_METHOD can be used to control which method is used by vLLM. The current default is fork.

When we know we own the process because the vllm command was used, we use spawn because it's the most widely compatible.

The multiproc_xpu_executor forces the use of spawn.

There are other miscellaneous places hard-coding the use of spawn:

There was an environment variable to control whether multiprocessing is used in the v1 engine core, VLLM_ENABLE_V1_MULTIPROCESSING. This defaulted to off.

When it was enabled, the v1 LLMEngine would create a new process to run the engine core.

It was off by default for all the reasons mentioned above - compatibility with dependencies and code using vLLM as a library.

There is not an easy solution with Python's multiprocessing that will work everywhere. As a first step, we can get v1 into a state where it does "best effort" choice of multiprocessing method to maximize compatibility.

The case that is known to still break in this scenario is code using vLLM as a library that initializes cuda before calling vLLM. The warning we emit should instruct users to either add a __main__ guard or to disable multiprocessing.

If that known-failure case occurs, the user will see two messages that explain what is happening. First, a log message from vLLM:

Second, Python itself will raise an exception with a nice explanation:

It has been suggested that we could behave better if we could detect whether code using vLLM as a library has a __main__ guard in place. This post on stackoverflow was from a library author facing the same question.

It is possible to detect whether we are in the original, __main__ process, or a subsequent spawned process. However, it does not appear to be straight forward to detect whether a __main__ guard is present in the code.

This option has been discarded as impractical.

At first it appears that forkserver is a nice solution to the problem. However, the way it works presents the same challenges that spawn does when vLLM is used as a library.

One way to clean this up is to just force the use of spawn all the time and document that the use of a __main__ guard is required when using vLLM as a library. This would unfortunately break existing code and make vLLM harder to use, violating the desire to make the LLM class as easy as possible to use.

Instead of pushing this on our users, we will retain the complexity to do our best to make things work.

We may want to consider a different worker management approach in the future that works around these challenges.

We could implement something forkserver-like, but have the process manager be something we initially launch by running our own subprocess and a custom entrypoint for worker management (launch a vllm-manager process).

We can explore other libraries that may better suit our needs. Examples to consider:

https://github.com/joblib/loky

**Examples:**

Example 1 (json):
```json
WARNING 12-11 14:50:37 multiproc_worker_utils.py:281] CUDA was previously
    initialized. We must use the `spawn` multiprocessing start method. Setting
    VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See
    https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing
    for more information.
```

Example 2 (json):
```json
WARNING 12-11 14:50:37 multiproc_worker_utils.py:281] CUDA was previously
    initialized. We must use the `spawn` multiprocessing start method. Setting
    VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See
    https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing
    for more information.
```

Example 3 (julia):
```julia
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
```

Example 4 (julia):
```julia
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The "freeze_support()" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the "Safe importing of main module"
        section in https://docs.python.org/3/library/multiprocessing.html
```

---

## Reinforcement Learning from Human Feedback - vLLM

**URL:** https://docs.vllm.ai/en/latest/training/rlhf/

**Contents:**
- Reinforcement Learning from Human Feedback¶

Reinforcement Learning from Human Feedback (RLHF) is a technique that fine-tunes language models using human-generated preference data to align model outputs with desired behaviors. vLLM can be used to generate the completions for RLHF.

The following open-source RL libraries use vLLM for fast rollouts (sorted alphabetically and non-exhaustive):

See the following basic examples to get started if you don't want to use an existing library:

See the following notebooks showing how to use vLLM for GRPO:

---

## Security - vLLM

**URL:** https://docs.vllm.ai/en/latest/usage/security/

**Contents:**
- Security¶
- Inter-Node Communication¶
  - Configuration Options for Inter-Node Communications¶
    - 1. Environment Variables:¶
    - 2. KV Cache Transfer Configuration:¶
    - 3. Data Parallel Configuration:¶
  - Notes on PyTorch Distributed¶
  - Security Recommendations¶
    - 1. Network Isolation:¶
    - 2. Configuration Best Practices:¶

All communications between nodes in a multi-node vLLM deployment are insecure by default and must be protected by placing the nodes on an isolated network. This includes:

The following options control internode communications in vLLM:

vLLM uses PyTorch's distributed features for some internode communication. For detailed information about PyTorch Distributed security considerations, please refer to the PyTorch Security Guide.

Key points from the PyTorch security guide:

Restrict domains that vLLM can access for media URLs by setting --allowed-media-domains to prevent Server-Side Request Forgery (SSRF) attacks. (e.g. --allowed-media-domains upload.wikimedia.org github.com www.bogotobogo.com)

Also, consider setting VLLM_MEDIA_URL_ALLOW_REDIRECTS=0 to prevent HTTP redirects from being followed to bypass domain restrictions.

While vLLM is designed to allow unsafe network services to be isolated to private networks, there are components—such as dependencies and underlying frameworks—that may open insecure services listening on all network interfaces, sometimes outside of vLLM's direct control.

A major concern is the use of torch.distributed, which vLLM leverages for distributed communication, including when using vLLM on a single host. When vLLM uses TCP initialization (see PyTorch TCP Initialization documentation), PyTorch creates a TCPStore that, by default, listens on all network interfaces. This means that unless additional protections are put in place, these services may be accessible to any host that can reach your machine via any network interface.

From a PyTorch perspective, any use of torch.distributed should be considered insecure by default. This is a known and intentional behavior from the PyTorch team.

The best way to protect your vLLM system is to carefully configure a firewall to expose only the minimum network surface area necessary. In most cases, this means:

Block all incoming connections except to the TCP port the API server is listening on.

Ensure that ports used for internal communication (such as those for torch.distributed and KV cache transfer) are only accessible from trusted hosts or networks.

Never expose these internal ports to the public internet or untrusted networks.

Consult your operating system or application platform documentation for specific firewall configuration instructions.

The --api-key flag (or VLLM_API_KEY environment variable) provides authentication for vLLM's HTTP server, but only for OpenAI-compatible API endpoints under the /v1 path prefix. Many other sensitive endpoints are exposed on the same HTTP server without any authentication enforcement.

Important: Do not rely exclusively on --api-key for securing access to vLLM. Additional security measures are required for production deployments.

When --api-key is configured, the following /v1 endpoints require Bearer token authentication:

The following endpoints do not require authentication even when --api-key is configured:

Operational control endpoints (always enabled):

Tokenizer information endpoint (only when --enable-tokenizer-info-endpoint is set):

This endpoint is only available when the --enable-tokenizer-info-endpoint flag is set. It may expose sensitive information such as chat templates and tokenizer configuration:

Development endpoints (only when VLLM_SERVER_DEV_MODE=1):

These endpoints are only available when the environment variable VLLM_SERVER_DEV_MODE is set to 1. They are intended for development and debugging purposes and should never be enabled in production:

Profiler endpoints (only when VLLM_TORCH_PROFILER_DIR or VLLM_TORCH_CUDA_PROFILE are set):

These endpoints are only available when profiling is enabled and should only be used for local development:

Note: The /invocations endpoint is particularly concerning as it provides unauthenticated access to the same inference capabilities as the protected /v1 endpoints.

An attacker who can reach the vLLM HTTP server can:

CRITICAL: Never set VLLM_SERVER_DEV_MODE=1 in production environments. Development endpoints expose extremely dangerous functionality including:

Similarly, never enable profiler endpoints (VLLM_TORCH_PROFILER_DIR or VLLM_TORCH_CUDA_PROFILE) in production.

Be cautious with --enable-tokenizer-info-endpoint: Only enable the /tokenizer_info endpoint if you need to expose tokenizer configuration information. This endpoint reveals chat templates and tokenizer settings that may contain sensitive implementation details or prompt engineering strategies.

The most effective approach is to deploy vLLM behind a reverse proxy (such as nginx, Envoy, or a Kubernetes Gateway) that:

If you believe you have found a security vulnerability in vLLM, please report it following the project's security policy. For more information on how to report security issues and the project's security policy, please see the vLLM Security Policy.

---

## torch.compile integration - vLLM

**URL:** https://docs.vllm.ai/en/latest/design/torch_compile/

**Contents:**
- torch.compile integration¶
- Compilation Cache¶
- Dynamic shapes and vllm guard dropping¶
  - Configuring Dynamic Shapes¶
    - Offline Inference Example (Using LLM class)¶
    - Online Serving Example (Using vllm serve)¶
    - Choosing the Right Mode¶
- Python Code Compilation¶
- Computation Graph Processing¶
- Computation Graph Compilation¶

In vLLM's V1 architecture, torch.compile is enabled by default and is a critical part of the framework. This document gives a simple walk-through example to show how to understand the torch.compile usage.

Throughout the example, we will run a common Llama model, and turn on debug level logging to show all the details. The command to be used is VLLM_LOGGING_LEVEL=DEBUG vllm serve meta-llama/Llama-3.2-1B.

For more information and the latest progress of torch.compile integration, see this Blog Post.

In the very verbose logs, we can see:

vLLM will take all the available factors into consideration, and decide a directory to store all the compilation artifact. This means, you can directly copy the whole ~/.cache/vllm/torch_compile_cache directory in your deployment scenario to save a great amount of compilation time, and hence accelerating the starting time of the vLLM instance.

The factors considered include:

With all these factors taken into consideration, usually we can guarantee that the cache is safe to use, and will not cause any unexpected behavior. Therefore, the cache is enabled by default. If you want to debug the compilation process, or if you suspect the cache is causing some issues, you can disable it by setting the environment variable VLLM_DISABLE_COMPILE_CACHE=1.

A unique aspect of vLLM's torch.compile integration, is that we guarantee all the compilation finishes before we serve any requests. No requests will trigger new compilations. Otherwise, the engine would be blocked on that request, and the response time will have unexpected spikes.

By default, the cache saves compiled artifacts as binary files. If you would like to interact with the generated code for debugging purposes, set the field compile_cache_save_format=unpacked in the compilation config, or omit this and set the env variable VLLM_COMPILE_CACHE_SAVE_FORMAT=unpacked.

torch.compile is designed to guard on dynamic shapes with no hesitation when needed. This contradicts with vLLM's torch.compile approach of dropping the guards since many of those guards could be material.

torch.compile provides two kinds of dynamic shapes: backed and unbacked. torch.compile guards on backed dynamic shapes and does not provide a guarantee that no guards will be added to them. User code, dynamo, inductor, and autograd all can add guards. Moreover, for 0/1 specializations, backed symbols are specialized unconditionally to 0, 1, or >=2 even without encountering a branching on those ranges.

On the contrary, unbacked dynamic shapes are guaranteed not to be guarded on and are not 0/1 specialized. However, there is a possibility of throwing a data dependent error when a branch that requires their value is encountered and no explicit unbacked handling is defined. The framework is converging to a state where it won't throw DDE but rather pick general paths. One downside of using unbacked is missed optimization opportunities due to either perf bugs or picking general paths, also using a fixed non-example input-based hint (this will be fixed soon with override_hint API). An example of picking general paths is assuming input not contiguous in functions call contiguous() and reshape() when can't be symbolically proven with a change of introducing a clone.

backed_size_oblivious is a flag that enables treating backed symbols as unbacked wherever explicit handling for unbacked is defined. With this mode, 0/1 specializations are mostly avoided in framework code and the default 0/1 specialization does not happen. However, there is still no guarantee that torch.compile won't guard, especially due to user code or custom passes. backed_size_oblivious is experimental in PyTorch compile and could be deprecated. That said, it's a safer option to use than backed and the probability of reducing performance is lower than unbacked.

The DynamicShapesConfig allows you to control the dynamic shapes behavior by setting the type field. You can choose between three modes: BACKED(default), UNBACKED , and BACKED_SIZE_OBLIVIOUS.

When using the LLM class for offline inference, you can configure dynamic shapes through the compilation_config parameter:

When using vllm serve for online serving, you can configure dynamic shapes through the --compilation-config flag:

BACKED (default): Use when you're willing to accept potential unsafe dropping of guards for maximal performance. Guard could be unsoundly added and then ignored.

UNBACKED Use when you need the strongest guarantee against guards. This is the most conservative option but may miss some optimization opportunities.

BACKED_SIZE_OBLIVIOUS: Use when you want a balance between avoiding guards and performance. This experimental mode is safer than BACKED but still not as conservative as UNBACKED.

In the very verbose logs, we can see:

This is about the Python code compilation, i.e. graph capture by Dynamo. It tries to trace the function with code xxx/vllm/model_executor/models/llama.py:339, which is the forward function of the model we compile. During the forward pass, there are also other functions called and inlined by Dynamo, as shown by the logs, including some PyTorch functions from xxx/torch/nn/modules/module.py (used by PyTorch nn.Module, because module attribute access will trigger a function call), some communication / attention / activation functions from vLLM. All the traced files will be considered when we decide the cache directory to use. This way, any code change in the above files will trigger compilation cache miss, and therefore recompilation.

The result of the Dynamo compilation, is a new function stored in ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/transformed_code.py. Usually, this function unpacks tensors from the module, and then pass it to the traced computation graph. The computation graph is stored in ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/computation_graph.py.

The computation graph has shape annotations for every tensor. The inputs are input ids, position ids, weights and buffers from the model, and the outputs are the final hidden states. Note that lm head projection and sampling operations are not considered in the graph.

Most of the inputs to the computation graph has static shape, since they are model weights and buffers, and will not change during the lifetime of the model. Only the input ids and position ids have symbolic shapes, i.e. the shape can change from batch to batch. However, they will share the same symbolic shapes. That is to say, the only changing size to the computation graph, is the batch size (number of tokens processed in the current forward pass).

The attention operation is complicated, and it needs to interact with kv caches, with complicated shapes. Fortunately, the output of the attention operation just share the same shape as the input query of the attention operation. Therefore, we wrap the whole attention operation into a PyTorch custom op torch.ops.vllm.unified_attention_with_output, so that Dynamo will not try to inspect any of the internal operations. This way, although attention operation is complicated, we can still capture the model's computation graph as a full-graph, from Dynamo's perspective.

The computation graph is further split into pieces, by the splitting_ops (usually this is the attention operation). Therefore, in the ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/computation_graph.py file, we can see lots of submodules, each submodule is a piece of graph after splitting:

Every submodule can be identified by its index, and will be processed individually.

In the very verbose logs, we can also see:

This means the first piece of computation graph (with shape None for symbolic shape) is compiled by Inductor (with a key fpegyiq3v3wzjzphd45wkflpabggdbjpylgr7tta4hj6uplstsiw). The compiled kernel is stored in ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0/inductor_cache/iw/ciwzrk3ittdqatuzwonnajywvno3llvjcs2vfdldzwzozn3zi3iy.py. You can open the file to see what is the code Inductor finally runs.

One more detail: you can see that the 1-th graph and the 15-th graph have the same key, while the 0-th graph and the 16-th graph are different. This is expected, since we split the graph by the attention op, we get 3 unique subgraphs:

If we already have the cache directory (e.g. run the same code for the second time), we will see the following logs:

This time, Inductor compilation is completely bypassed, and we will load from disk to read the compilation artifact we get from the last time.

The above example just uses Inductor to compile for a general shape (i.e. symbolic shape). We can also use Inductor to compile for some of the specific shapes, for example:

Then it will also compile a specific kernel just for batch size 1, 2, 4, 8. At this time, all of the shapes in the computation graph are static and known, and we will turn on auto-tuning to tune for max performance. This can be slow when you run it for the first time, but the next time you run it, we can directly bypass the tuning and run the tuned kernel.

When all the shapes are known, torch.compile can compare different configs, and often find some better configs to run the kernel. For example, we can see the following log:

It means, for a matrix multiplication with shape 8x2048x3072, torch.compile tries triton template with various configs, and it is much faster than the default code (which dispatches to cublas library).

Unfortunately, because auto-tuning takes quite a long time (from seconds to minutes, depending on the model size and the batch size), even though it can be cached for later use, for the sake of user-friendliness, we turn it off by default. If you want to have max performance, it is recommended to try it, by compiling specific shapes.

vLLM's V1 architecture uses piecewise cudagraph that aligns with the piecewise compilation. The full computation graph is split as mentioned above, and we only capture the cudagraph for the piece of graph between attention operations (including the first graph before any attention operation, and the last graph after all the attention operation). This is based on a common observation: computation between attentions are usually token-wise and easy to deal with for cudagraph; while the attention operation is non-trivial to be cudagraph compatible. Thus, by running the attention operation in eager mode while the rest operations in cudagraph, we keep the flexibility of the attention operation.

The piecewise cudagraph also has fine-grained memory management. The purpose is to only exclude the attention kernel from cudagraph, while keeping all the rest modules and the memory allocation operations in the cudagraph. This is why the attention operation in V1 has the output tensor as the input of the attention.

The cudagraphs are captured and managed by the compiler backend, and replayed when the batch size has corresponding cudagraph captured. The caller of the model (model runner) only needs to make sure it manages the input buffers correctly. All of the intermediate buffers are managed automatically by the compiler backend.

By default, vLLM will try to determine a set of sizes to capture cudagraph. You can also override it using the config cudagraph_capture_sizes:

Then it will only capture cudagraph for the specified sizes. It can be useful to have fine-grained control over the cudagraph capture.

It is possible to include attention as part of the cudagraph if using an attention backend that is cudagraph compatible. This can improve performance in some cases such as decode speed for smaller models or MOEs. See CUDA Graphs for more details.

**Examples:**

Example 1 (julia):
```julia
INFO 03-07 03:06:55 [backends.py:409] Using cache directory: ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0 for vLLM's torch.compile
```

Example 2 (julia):
```julia
INFO 03-07 03:06:55 [backends.py:409] Using cache directory: ~/.cache/vllm/torch_compile_cache/1517964802/rank_0_0 for vLLM's torch.compile
```

Example 3 (python):
```python
from vllm import LLM, SamplingParams
from vllm.config.compilation import CompilationConfig, DynamicShapesConfig, DynamicShapesType

# Example: Using backed_size_oblivious (experimental, safer than backed)
llm = LLM(
    model="meta-llama/Llama-3.2-1B",
    compilation_config=CompilationConfig(
        dynamic_shapes_config=DynamicShapesConfig(
            type=DynamicShapesType.BACKED_SIZE_OBLIVIOUS
        )
    )
)

# Example: Using unbacked (strongest guarantee against guards)
llm = LLM(
    model="meta-llama/Llama-3.2-1B",
    compilation_config=CompilationConfig(
        dynamic_shapes_config=DynamicShapesConfig(
            type=DynamicShapesType.UNBACKED
        )
    )
)

# Generate outputs
prompts = ["Hello, my name is", "The future of AI is"]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
outputs = llm.generate(prompts, sampling_params)
```

Example 4 (python):
```python
from vllm import LLM, SamplingParams
from vllm.config.compilation import CompilationConfig, DynamicShapesConfig, DynamicShapesType

# Example: Using backed_size_oblivious (experimental, safer than backed)
llm = LLM(
    model="meta-llama/Llama-3.2-1B",
    compilation_config=CompilationConfig(
        dynamic_shapes_config=DynamicShapesConfig(
            type=DynamicShapesType.BACKED_SIZE_OBLIVIOUS
        )
    )
)

# Example: Using unbacked (strongest guarantee against guards)
llm = LLM(
    model="meta-llama/Llama-3.2-1B",
    compilation_config=CompilationConfig(
        dynamic_shapes_config=DynamicShapesConfig(
            type=DynamicShapesType.UNBACKED
        )
    )
)

# Generate outputs
prompts = ["Hello, my name is", "The future of AI is"]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
outputs = llm.generate(prompts, sampling_params)
```

---

## Transformers Reinforcement Learning - vLLM

**URL:** https://docs.vllm.ai/en/latest/training/trl/

**Contents:**
- Transformers Reinforcement Learning¶
- Modes of Using vLLM During Training¶
  - Server mode¶
  - Colocate mode¶

Transformers Reinforcement Learning (TRL) is a full stack library that provides a set of tools to train transformer language models with methods like Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), Reward Modeling, and more. The library is integrated with 🤗 transformers.

Online methods such as GRPO or Online DPO require the model to generate completions. vLLM can be used to generate these completions!

See the vLLM integration guide in the TRL documentation for more information.

TRL currently supports the following online trainers with vLLM:

To enable vLLM in TRL, set the use_vllm flag in the trainer configuration to True.

TRL supports two modes for integrating vLLM during training: server mode and colocate mode. You can control how vLLM operates during training with the vllm_mode parameter.

In server mode, vLLM runs as an independent process on dedicated GPUs and communicates with the trainer through HTTP requests. This configuration is ideal when you have separate GPUs for inference, as it isolates generation workloads from training, ensuring stable performance and easier scaling.

In colocate mode, vLLM runs inside the trainer process and shares GPU memory with the training model. This avoids launching a separate server and can improve GPU utilization, but may lead to memory contention on the training GPUs.

Some trainers also support vLLM sleep mode, which offloads parameters and caches to GPU RAM during training, helping reduce memory usage. Learn more in the memory optimization docs.

For detailed configuration options and flags, refer to the documentation of the specific trainer you are using.

**Examples:**

Example 1 (python):
```python
from trl import GRPOConfig

training_args = GRPOConfig(
    ...,
    use_vllm=True,
    vllm_mode="server",  # default value, can be omitted
)
```

Example 2 (python):
```python
from trl import GRPOConfig

training_args = GRPOConfig(
    ...,
    use_vllm=True,
    vllm_mode="server",  # default value, can be omitted
)
```

Example 3 (python):
```python
from trl import GRPOConfig

training_args = GRPOConfig(
    ...,
    use_vllm=True,
    vllm_mode="colocate",
)
```

Example 4 (python):
```python
from trl import GRPOConfig

training_args = GRPOConfig(
    ...,
    use_vllm=True,
    vllm_mode="colocate",
)
```

---

## Troubleshooting - vLLM

**URL:** https://docs.vllm.ai/en/latest/usage/troubleshooting/

**Contents:**
- Troubleshooting¶
- Hangs downloading a model¶
- Hangs loading a model from disk¶
- Out of memory¶
- Generation quality changed¶
- Enable more logging¶
- Breakpoints¶
- Incorrect network setup¶
- Error near self.graph.replay()¶
- Incorrect hardware/driver¶

This document outlines some troubleshooting strategies you can consider. If you think you've discovered a bug, please search existing issues first to see if it has already been reported. If not, please file a new issue, providing as much relevant information as possible.

Once you've debugged a problem, remember to turn off any debugging environment variables defined, or simply start a new shell to avoid being affected by lingering debugging settings. Otherwise, the system might be slow with debugging functionalities left activated.

If the model isn't already downloaded to disk, vLLM will download it from the internet which can take time and depend on your internet connection. It's recommended to download the model first using the huggingface-cli and passing the local path to the model to vLLM. This way, you can isolate the issue.

If the model is large, it can take a long time to load it from disk. Pay attention to where you store the model. Some clusters have shared filesystems across nodes, e.g. a distributed filesystem or a network filesystem, which can be slow. It'd be better to store the model in a local disk. Additionally, have a look at the CPU memory usage, when the model is too large it might take a lot of CPU memory, slowing down the operating system because it needs to frequently swap between disk and memory.

To isolate the model downloading and loading issue, you can use the --load-format dummy argument to skip loading the model weights. This way, you can check if the model downloading and loading is the bottleneck.

If the model is too large to fit in a single GPU, you will get an out-of-memory (OOM) error. Consider adopting these options to reduce the memory consumption.

In v0.8.0, the source of default sampling parameters was changed in Pull Request #12622. Prior to v0.8.0, the default sampling parameters came from vLLM's set of neutral defaults. From v0.8.0 onwards, the default sampling parameters come from the generation_config.json provided by the model creator.

In most cases, this should lead to higher quality responses, because the model creator is likely to know which sampling parameters are best for their model. However, in some cases the defaults provided by the model creator can lead to degraded performance.

You can check if this is happening by trying the old defaults with --generation-config vllm for online and generation_config="vllm" for offline. If, after trying this, your generation quality improves we would recommend continuing to use the vLLM defaults and petition the model creator on https://huggingface.co to update their default generation_config.json so that it produces better quality generations.

If other strategies don't solve the problem, it's likely that the vLLM instance is stuck somewhere. You can use the following environment variables to help debug the issue:

Setting normal pdb breakpoints may not work in vLLM's codebase if they are executed in a subprocess. You will experience something like:

One solution is using forked-pdb. Install with pip install fpdb and set a breakpoint with something like:

Another option is to disable multiprocessing entirely, with the VLLM_ENABLE_V1_MULTIPROCESSING environment variable. This keeps the scheduler in the same process, so you can use stock pdb breakpoints:

The vLLM instance cannot get the correct IP address if you have a complicated network config. You can find a log such as DEBUG 06-10 21:32:17 parallel_state.py:88] world_size=8 rank=0 local_rank=0 distributed_init_method=tcp://xxx.xxx.xxx.xxx:54641 backend=nccl and the IP address should be the correct one. If it's not, override the IP address using the environment variable export VLLM_HOST_IP=<your_ip_address>.

You might also need to set export NCCL_SOCKET_IFNAME=<your_network_interface> and export GLOO_SOCKET_IFNAME=<your_network_interface> to specify the network interface for the IP address.

If vLLM crashes and the error trace captures it somewhere around self.graph.replay() in vllm/worker/model_runner.py, it is a CUDA error inside CUDAGraph. To identify the particular CUDA operation that causes the error, you can add --enforce-eager to the command line, or enforce_eager=True to the LLM class to disable the CUDAGraph optimization and isolate the exact CUDA operation that causes the error.

If GPU/CPU communication cannot be established, you can use the following Python script and follow the instructions below to confirm whether the GPU/CPU communication is working correctly.

If you are testing with a single node, adjust --nproc-per-node to the number of GPUs you want to use:

If you are testing with multi-nodes, adjust --nproc-per-node and --nnodes according to your setup and set MASTER_ADDR to the correct IP address of the master node, reachable from all nodes. Then, run:

If the script runs successfully, you should see the message sanity check is successful!.

If the test script hangs or crashes, usually it means the hardware/drivers are broken in some sense. You should try to contact your system administrator or hardware vendor for further assistance. As a common workaround, you can try to tune some NCCL environment variables, such as export NCCL_P2P_DISABLE=1 to see if it helps. Please check their documentation for more information. Please only use these environment variables as a temporary workaround, as they might affect the performance of the system. The best solution is still to fix the hardware/drivers so that the test script can run successfully.

A multi-node environment is more complicated than a single-node one. If you see errors such as torch.distributed.DistNetworkError, it is likely that the network/DNS setup is incorrect. In that case, you can manually assign node rank and specify the IP via command line arguments:

Adjust --nproc-per-node, --nnodes, and --node-rank according to your setup, being sure to execute different commands (with different --node-rank) on different nodes.

If you have seen a warning in your logs like this:

or an error from Python that looks like this:

then you must update your Python code to guard usage of vllm behind a if __name__ == '__main__': block. For example, instead of this:

vLLM heavily depends on torch.compile to optimize the model for better performance, which introduces the dependency on the torch.compile functionality and the triton library. By default, we use torch.compile to optimize some functions in the model. Before running vLLM, you can check if torch.compile is working as expected by running the following script:

If it raises errors from torch/_inductor directory, usually it means you have a custom triton library that is not compatible with the version of PyTorch you are using. See Issue #12219 for example.

If you see an error like:

It means that vLLM failed to import the model file. Usually, it is related to missing dependencies or outdated binaries in the vLLM build. Please read the logs carefully to determine the root cause of the error.

If you see an error like:

But you are sure that the model is in the list of supported models, there may be some issue with vLLM's model resolution. In that case, please follow these steps to explicitly specify the vLLM implementation for the model.

If you see an error like RuntimeError: Failed to infer device type, it means that vLLM failed to infer the device type of the runtime environment. You can check the code to see how vLLM infers the device type and why it is not working as expected. After this PR, you can also set the environment variable VLLM_LOGGING_LEVEL=DEBUG to see more detailed logs to help debug the issue.

If your serving workload uses GPUDirect RDMA for distributed serving across multiple nodes and encounters an error during ncclCommInitRank, with no clear error message even with NCCL_DEBUG=INFO set, it might look like this:

This indicates vLLM failed to initialize the NCCL communicator, possibly due to a missing IPC_LOCK linux capability or an unmounted /dev/shm. Refer to Enabling GPUDirect RDMA for guidance on properly configuring the environment for GPUDirect RDMA.

If you see an error like RuntimeError: CUDA error: the provided PTX was compiled with an unsupported toolchain., it means that the CUDA PTX in vLLM's wheels was compiled with a toolchain unsupported by your system. The released vLLM wheels have to be compiled with a specific version of CUDA toolkit, and the compiled code might fail to run on lower versions of CUDA drivers. Read cuda compatibility for more details. The solution is to install cuda-compat package from your package manager. For example, on Ubuntu, you can run sudo apt-get install cuda-compat-12-9, and then add export LD_LIBRARY_PATH=/usr/local/cuda-12.9/compat:$LD_LIBRARY_PATH to your .bashrc file. When successfully installed, you should see that the output of nvidia-smi will show CUDA Version: 12.9. Note that we use CUDA 12.9 as an example here, you may want to install a higher version of cuda-compat package in case vLLM's default CUDA version goes higher.

If you use triton kernels with cuda 13, you might see an error like ptxas fatal: Value 'sm_110a' is not defined for option 'gpu-name':

It means that the ptxas in triton bundle not compatible with your device. You need to set TRITON_PTXAS_PATH environment variable to use cuda toolkit's ptxas manually instead:

**Examples:**

Example 1 (python):
```python
File "/usr/local/uv/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/bdb.py", line 100, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/uv/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/bdb.py", line 125, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
```

Example 2 (python):
```python
File "/usr/local/uv/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/bdb.py", line 100, in trace_dispatch
    return self.dispatch_line(frame)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/uv/cpython-3.12.11-linux-x86_64-gnu/lib/python3.12/bdb.py", line 125, in dispatch_line
    if self.quitting: raise BdbQuit
                      ^^^^^^^^^^^^^
bdb.BdbQuit
```

Example 3 (unknown):
```unknown
__import__('fpdb').ForkedPdb().set_trace()
```

Example 4 (unknown):
```unknown
__import__('fpdb').ForkedPdb().set_trace()
```

---

## Usage Stats Collection - vLLM

**URL:** https://docs.vllm.ai/en/latest/usage/usage_stats/

**Contents:**
- Usage Stats Collection¶
- What data is collected?¶
- Opting out¶

vLLM collects anonymous usage data by default to help the engineering team better understand which hardware and model configurations are widely used. This data allows them to prioritize their efforts on the most common workloads. The collected data is transparent, does not contain any sensitive information.

A subset of the data, after cleaning and aggregation, will be publicly released for the community's benefit. For example, you can see the 2024 usage report here.

The list of data collected by the latest version of vLLM can be found here: vllm/usage/usage_lib.py

Here is an example as of v0.4.0:

You can preview the collected data by running the following command:

You can opt out of usage stats collection by setting the VLLM_NO_USAGE_STATS or DO_NOT_TRACK environment variable, or by creating a ~/.config/vllm/do_not_track file:

**Examples:**

Example 1 (json):
```json
{
  "uuid": "fbe880e9-084d-4cab-a395-8984c50f1109",
  "provider": "GCP",
  "num_cpu": 24,
  "cpu_type": "Intel(R) Xeon(R) CPU @ 2.20GHz",
  "cpu_family_model_stepping": "6,85,7",
  "total_memory": 101261135872,
  "architecture": "x86_64",
  "platform": "Linux-5.10.0-28-cloud-amd64-x86_64-with-glibc2.31",
  "gpu_count": 2,
  "gpu_type": "NVIDIA L4",
  "gpu_memory_per_device": 23580639232,
  "model_architecture": "OPTForCausalLM",
  "vllm_version": "0.3.2+cu123",
  "context": "LLM_CLASS",
  "log_time": 1711663373492490000,
  "source": "production",
  "dtype": "torch.float16",
  "tensor_parallel_size": 1,
  "block_size": 16,
  "gpu_memory_utilization": 0.9,
  "quantization": null,
  "kv_cache_dtype": "auto",
  "enable_lora": false,
  "enable_prefix_caching": false,
  "enforce_eager": false,
  "disable_custom_all_reduce": true
}
```

Example 2 (json):
```json
{
  "uuid": "fbe880e9-084d-4cab-a395-8984c50f1109",
  "provider": "GCP",
  "num_cpu": 24,
  "cpu_type": "Intel(R) Xeon(R) CPU @ 2.20GHz",
  "cpu_family_model_stepping": "6,85,7",
  "total_memory": 101261135872,
  "architecture": "x86_64",
  "platform": "Linux-5.10.0-28-cloud-amd64-x86_64-with-glibc2.31",
  "gpu_count": 2,
  "gpu_type": "NVIDIA L4",
  "gpu_memory_per_device": 23580639232,
  "model_architecture": "OPTForCausalLM",
  "vllm_version": "0.3.2+cu123",
  "context": "LLM_CLASS",
  "log_time": 1711663373492490000,
  "source": "production",
  "dtype": "torch.float16",
  "tensor_parallel_size": 1,
  "block_size": 16,
  "gpu_memory_utilization": 0.9,
  "quantization": null,
  "kv_cache_dtype": "auto",
  "enable_lora": false,
  "enable_prefix_caching": false,
  "enforce_eager": false,
  "disable_custom_all_reduce": true
}
```

Example 3 (unknown):
```unknown
tail ~/.config/vllm/usage_stats.json
```

Example 4 (unknown):
```unknown
tail ~/.config/vllm/usage_stats.json
```

---

## Using vLLM - vLLM

**URL:** https://docs.vllm.ai/en/latest/usage/

**Contents:**
- Using vLLM¶

First, vLLM must be installed for your chosen device in either a Python or Docker environment.

Then, vLLM supports the following usage patterns:

---

## vLLM

**URL:** https://docs.vllm.ai/en/latest/

**Contents:**
- Welcome to vLLM¶

Easy, fast, and cheap LLM serving for everyone

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the Sky Computing Lab at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

Where to get started with vLLM depends on the type of user. If you are looking to:

For information about the development of vLLM, see:

vLLM is flexible and easy to use with:

For more information, check out the following:

---
