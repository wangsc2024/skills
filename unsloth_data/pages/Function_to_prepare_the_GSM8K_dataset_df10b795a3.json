{
  "title": "Function to prepare the GSM8K dataset",
  "content": "def get_gsm8k_questions(split=\"train\") -> Dataset:\n    data = load_dataset(\"openai/gsm8k\", \"main\")[split]\n    data = data.map(\n        lambda x: {\n            \"prompt\": [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": x[\"question\"]},\n            ],\n            \"answer\": extract_hash_answer(x[\"answer\"]),\n        }\n    )\n    return data\n\ndataset = get_gsm8k_questions()\npython\nepsilon=0.2,\nepsilon_high=0.28, # one sided\ndelta=1.5 # two sided",
  "code_samples": [
    {
      "code": "The dataset is prepared by extracting the answers and formatting them as structured strings.\n{% endstep %}\n\n{% step %}\n\n#### Reward Functions/Verifier\n\n[Reward Functions/Verifiers](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/..#reward-functions-verifier) lets us know if the model is doing well or not according to the dataset you have provided. Each generation run will be assessed on how it performs to the score of the average of the rest of generations. You can create your own reward functions however we have already pre-selected them for you with [Will's GSM8K](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/..#gsm8k-reward-functions) reward functions. With this, we have 5 different ways which we can reward each generation.\n\nYou can input your generations into an LLM like ChatGPT 4o or Llama 3.1 (8B) and design a reward function and verifier to evaluate it. For example, feed your generations into a LLM of your choice and set a rule: \"If the answer sounds too robotic, deduct 3 points.\" This helps refine outputs based on quality criteria. **See examples** of what they can look like [here](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/..#reward-function-examples).\n\n**Example Reward Function for an Email Automation Task:**\n\n* **Question:** Inbound email\n* **Answer:** Outbound email\n* **Reward Functions:**\n  * If the answer contains a required keyword → **+1**\n  * If the answer exactly matches the ideal response → **+1**\n  * If the response is too long → **-1**\n  * If the recipient's name is included → **+1**\n  * If a signature block (phone, email, address) is present → **+1**\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-95cd00b6a52b8161b31a2399e25863ee0349920e%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n{% step %}\n\n#### Train your model\n\nWe have pre-selected hyperparameters for the most optimal results however you could change them. Read all about [parameters here](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide). For **advanced GRPO** documentation on batching, generation and training parameters, [read our guide!](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/advanced-rl-documentation)\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-a22d3475d925d2d858c9fcc228f0e13893eff0f9%2Fimage.png?alt=media\" alt=\"\" width=\"563\"><figcaption></figcaption></figure>\n\nThe **GRPOConfig** defines key hyperparameters for training:\n\n* `use_vllm`: Activates fast inference using vLLM.\n* `learning_rate`: Determines the model's learning speed.\n* `num_generations`: Specifies the number of completions generated per prompt.\n* `max_steps`: Sets the total number of training steps.\n\n{% hint style=\"success\" %}\n**NEW!** We now support DAPO, Dr. GRPO and most other new GRPO techniques. You can play with the following arguments in GRPOConfig to enable:",
      "language": "unknown"
    }
  ],
  "headings": [],
  "url": "llms-txt#function-to-prepare-the-gsm8k-dataset",
  "links": []
}