{
  "title": "Multi-GPU Fine-tuning with Unsloth",
  "content": "Learn how to fine-tune LLMs on multiple GPUs and parallelism with Unsloth.\n\nUnsloth currently supports multi-GPU setups through libraries like Accelerate and DeepSpeed. This means you can already leverage parallelism methods such as **FSDP** and **DDP** with Unsloth.\n\n#### **See our new Distributed Data Parallel** [**(DDP) multi-GPU Guide here**](https://docs.unsloth.ai/basics/multi-gpu-training-with-unsloth/ddp)**.**\n\nWe know that the process can be complex and requires manual setup. We’re working hard to make multi-GPU support much simpler and more user-friendly, and we’ll be announcing official multi-GPU support for Unsloth soon.\n\nFor now, you can use our [Magistral-2509 Kaggle notebook](https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms/magistral-how-to-run-and-fine-tune#fine-tuning-magistral-with-unsloth) as an example which utilizes multi-GPU Unsloth to fit the 24B parameter model or our [DDP guide](https://docs.unsloth.ai/basics/multi-gpu-training-with-unsloth/ddp).\n\n**In the meantime**, to enable multi GPU for DDP, do the following:\n\n1. Create your training script as `train.py` (or similar). For example, you can use one of our [training scripts](https://github.com/unslothai/notebooks/tree/main/python_scripts) created from our various notebooks!\n2. Run `accelerate launch train.py` or `torchrun --nproc_per_node N_GPUS train.py` where `N_GPUS` is the number of GPUs you have.\n\n**Pipeline / model splitting loading** is also allowed, so if you do not have enough VRAM for 1 GPU to load say Llama 70B, no worries - we will split the model for you on each GPU! To enable this, use the `device_map = \"balanced\"` flag:\n\n**Stay tuned for our official announcement!**\\\nFor more details, check out our ongoing [Pull Request](https://github.com/unslothai/unsloth/issues/2435) discussing multi-GPU support.",
  "code_samples": [
    {
      "code": "from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/Llama-3.3-70B-Instruct\",\n    load_in_4bit = True,\n    device_map = \"balanced\",\n)",
      "language": "python"
    }
  ],
  "headings": [],
  "url": "llms-txt#multi-gpu-fine-tuning-with-unsloth",
  "links": []
}