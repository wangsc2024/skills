{
  "title": "Text-to-Speech (TTS) Fine-tuning",
  "content": "Learn how to to fine-tune TTS & STT voice models with Unsloth.\n\nFine-tuning TTS models allows them to adapt to your specific dataset, use case, or desired style and tone. The goal is to customize these models to clone voices, adapt speaking styles and tones, support new languages, handle specific tasks and more. We also support **Speech-to-Text (STT)** models like OpenAI's Whisper.\n\nWith [Unsloth](https://github.com/unslothai/unsloth), you can fine-tune **any** TTS model (`transformers` compatible) 1.5x faster with 50% less memory than other implementations with Flash Attention 2.\n\n⭐ **Unsloth supports any `transformers` compatible TTS model.** Even if we don’t have a notebook or upload for it yet, it’s still supported e.g., try fine-tuning Dia-TTS or Moshi.\n\n{% hint style=\"info\" %}\nZero-shot cloning captures tone but misses pacing and expression, often sounding robotic and unnatural. Fine-tuning delivers far more accurate and realistic voice replication. [Read more here](#fine-tuning-voice-models-vs.-zero-shot-voice-cloning).\n{% endhint %}\n\n### Fine-tuning Notebooks:\n\nWe've also uploaded TTS models (original and quantized) to our [Hugging Face page](https://huggingface.co/collections/unsloth/text-to-speech-tts-models-68007ab12522e96be1e02155).\n\n| [Sesame-CSM (1B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Sesame_CSM_\\(1B\\)-TTS.ipynb) | [Orpheus-TTS (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_\\(3B\\)-TTS.ipynb) | [Whisper Large V3](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Whisper.ipynb) (STT) |\n| ------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |\n| [Spark-TTS (0.5B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Spark_TTS_\\(0_5B\\).ipynb)   | [Llasa-TTS (1B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llasa_TTS_\\(1B\\).ipynb)     | [Oute-TTS (1B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Oute_TTS_\\(1B\\).ipynb)  |\n\n{% hint style=\"success\" %}\nIf you notice that the output duration reaches a maximum of 10 seconds, increase`max_new_tokens = 125` from its default value of 125. Since 125 tokens corresponds to 10 seconds of audio, you'll need to set a higher value for longer outputs.\n{% endhint %}\n\n### Choosing and Loading a TTS Model\n\nFor TTS, smaller models are often preferred due to lower latency and faster inference for end users. Fine-tuning a model under 3B parameters is often ideal, and our primary examples uses Sesame-CSM (1B) and Orpheus-TTS (3B), a Llama-based speech model.\n\n#### Sesame-CSM (1B) Details\n\n**CSM-1B** is a base model, while **Orpheus-ft** is fine-tuned on 8 professional voice actors, making voice consistency the key difference. CSM requires audio context for each speaker to perform well, whereas Orpheus-ft has this consistency built in.\n\nFine-tuning from a base model like CSM generally needs more compute, while starting from a fine-tuned model like Orpheus-ft offers better results out of the box.\n\nTo help with CSM, we’ve added new sampling options and an example showing how to use audio context for improved voice consistency.\n\n#### Orpheus-TTS (3B) Details\n\nOrpheus is pre-trained on a large speech corpus and excels at generating realistic speech with built-in support for emotional cues like laughs and sighs. Its architecture makes it one of the easiest TTS models to utilize and train as it can be exported via llama.cpp meaning it has great compatibility across all inference engines. For unsupported models, you'll only be able to save the LoRA adapter safetensors.\n\n#### Loading the models\n\nBecause voice models are usually small in size, you can train the models using LoRA 16-bit or full fine-tuning FFT which may provide higher quality results. To load it in LoRA 16-bit:\n\nWhen this runs, Unsloth will download the model weights if you prefer 8-bit, you could use `load_in_8bit = True`, or for full fine-tuning set `full_finetuning = True` (ensure you have enough VRAM). You can also replace the model name with other TTS models.\n\n{% hint style=\"info\" %}\n**Note:** Orpheus’s tokenizer already includes special tokens for audio output (more on this later). You do *not* need a separate vocoder – Orpheus will output audio tokens directly, which can be decoded to a waveform.\n{% endhint %}\n\n### Preparing Your Dataset\n\nAt minimum, a TTS fine-tuning dataset consists of **audio clips and their corresponding transcripts** (text). Let’s use the [*Elise* dataset](https://huggingface.co/datasets/MrDragonFox/Elise) which is \\~3 hour single-speaker English speech corpus. There are two variants:\n\n* [`MrDragonFox/Elise`](https://huggingface.co/datasets/MrDragonFox/Elise) – an augmented version with **emotion tags** (e.g. \\<sigh>, \\<laughs>) embedded in the transcripts. These tags in angle brackets indicate expressions (laughter, sighs, etc.) and are treated as special tokens by Orpheus’s tokenizer\n* [`Jinsaryko/Elise`](https://huggingface.co/datasets/Jinsaryko/Elise) – base version with transcripts without special tags.\n\nThe dataset is organized with one audio and transcript per entry. On Hugging Face, these datasets have fields such as `audio` (the waveform), `text` (the transcription), and some metadata (speaker name, pitch stats, etc.). We need to feed Unsloth a dataset of audio-text pairs.\n\n{% hint style=\"success\" %}\nInstead of solely focusing on tone, cadence, and pitch, the priority should be ensuring your dataset is fully annotated and properly normalized.\n{% endhint %}\n\n{% hint style=\"info\" %}\nWith some models like **Sesame-CSM-1B**, you might notice voice variation across generations using speaker ID 0 because it's a **base model**—it doesn’t have fixed voice identities. Speaker ID tokens mainly help maintain **consistency within a conversation**, not across separate generations.\n\nTo get a consistent voice, provide **contextual examples**, like a few reference audio clips or prior utterances. This helps the model mimic the desired voice more reliably. Without this, variation is expected, even with the same speaker ID.\n{% endhint %}\n\n**Option 1: Using Hugging Face Datasets library** – We can load the Elise dataset using Hugging Face’s `datasets` library:\n\n```python\nfrom datasets import load_dataset, Audio",
  "code_samples": [
    {
      "code": "from unsloth import FastModel\n\nmodel_name = \"unsloth/orpheus-3b-0.1-pretrained\"\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name,\n    load_in_4bit=False  # use 4-bit precision (QLoRA)\n)",
      "language": "python"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Fine-tuning Notebooks:",
      "id": "fine-tuning-notebooks:"
    },
    {
      "level": "h3",
      "text": "Choosing and Loading a TTS Model",
      "id": "choosing-and-loading-a-tts-model"
    },
    {
      "level": "h3",
      "text": "Preparing Your Dataset",
      "id": "preparing-your-dataset"
    }
  ],
  "url": "llms-txt#text-to-speech-(tts)-fine-tuning",
  "links": []
}