{
  "title": "Inference & Deployment",
  "content": "Learn how to save your finetuned model so you can run it in your favorite inference engine.\n\nYou can also run your fine-tuned models by using [Unsloth's 2x faster inference](https://docs.unsloth.ai/basics/inference-and-deployment/unsloth-inference).\n\n<table data-card-size=\"large\" data-view=\"cards\"><thead><tr><th></th><th data-hidden data-card-target data-type=\"content-ref\"></th><th data-hidden data-type=\"content-ref\"></th></tr></thead><tbody><tr><td><a href=\"inference-and-deployment/saving-to-gguf\">llama.cpp - Saving to GGUF</a></td><td><a href=\"inference-and-deployment/saving-to-gguf\">saving-to-gguf</a></td><td><a href=\"inference-and-deployment/saving-to-gguf\">saving-to-gguf</a></td></tr><tr><td><a href=\"inference-and-deployment/saving-to-ollama\">Ollama</a></td><td><a href=\"inference-and-deployment/saving-to-ollama\">saving-to-ollama</a></td><td><a href=\"inference-and-deployment/saving-to-ollama\">saving-to-ollama</a></td></tr><tr><td><a href=\"inference-and-deployment/vllm-guide\">vLLM</a></td><td><a href=\"inference-and-deployment/vllm-guide\">vllm-guide</a></td><td><a href=\"inference-and-deployment/vllm-guide\">vllm-guide</a></td></tr><tr><td><a href=\"inference-and-deployment/sglang-guide\">SGLang</a></td><td><a href=\"inference-and-deployment/sglang-guide\">sglang-guide</a></td><td><a href=\"inference-and-deployment/vllm-guide/vllm-engine-arguments\">vllm-engine-arguments</a></td></tr><tr><td><a href=\"inference-and-deployment/unsloth-inference\">Unsloth Inference</a></td><td><a href=\"inference-and-deployment/unsloth-inference\">unsloth-inference</a></td><td><a href=\"inference-and-deployment/unsloth-inference\">unsloth-inference</a></td></tr><tr><td><a href=\"inference-and-deployment/troubleshooting-inference\">Troubleshooting</a></td><td><a href=\"inference-and-deployment/troubleshooting-inference\">troubleshooting-inference</a></td><td><a href=\"inference-and-deployment/troubleshooting-inference\">troubleshooting-inference</a></td></tr><tr><td><a href=\"inference-and-deployment/llama-server-and-openai-endpoint\">llama-server &#x26; OpenAI endpoint</a></td><td><a href=\"inference-and-deployment/llama-server-and-openai-endpoint\">llama-server-and-openai-endpoint</a></td><td></td></tr><tr><td><a href=\"inference-and-deployment/vllm-guide/vllm-engine-arguments\">vLLM Engine Arguments</a></td><td><a href=\"inference-and-deployment/vllm-guide/vllm-engine-arguments\">vllm-engine-arguments</a></td><td><a href=\"inference-and-deployment/sglang-guide\">sglang-guide</a></td></tr><tr><td><a href=\"inference-and-deployment/vllm-guide/lora-hot-swapping-guide\">LoRA Hotswapping</a></td><td><a href=\"inference-and-deployment/vllm-guide/lora-hot-swapping-guide\">lora-hot-swapping-guide</a></td><td></td></tr></tbody></table>",
  "code_samples": [],
  "headings": [],
  "url": "llms-txt#inference-&-deployment",
  "links": []
}