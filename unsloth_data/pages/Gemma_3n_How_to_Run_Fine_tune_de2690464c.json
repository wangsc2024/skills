{
  "title": "Gemma 3n: How to Run & Fine-tune",
  "content": "Run Google's new Gemma 3n locally with Dynamic GGUFs on llama.cpp, Ollama, Open WebUI and fine-tune with Unsloth!\n\nGoogle‚Äôs Gemma 3n multimodal model handles image, audio, video, and text inputs. Available in 2B and 4B sizes, it supports 140 languages for text and multimodal tasks. You can now run and fine-tune **Gemma-3n-E4B** and **E2B** locally using [Unsloth](https://github.com/unslothai/unsloth).\n\n> **Fine-tune Gemma 3n with our** [**free Colab notebook**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3N_\\(4B\\)-Conversational.ipynb)\n\nGemma 3n has **32K context length**, 30s audio input, OCR, auto speech recognition (ASR), and speech translation via prompts.\n\n<a href=\"#running-gemma-3n\" class=\"button primary\">Running Tutorial</a><a href=\"#fine-tuning-gemma-3n-with-unsloth\" class=\"button secondary\">Fine-tuning Tutorial</a><a href=\"#fixes-for-gemma-3n\" class=\"button secondary\">Fixes + Technical Analysis</a>\n\n**Unsloth Gemma 3n (Instruct) uploads with optimal configs:**\n\n<table><thead><tr><th width=\"249\">Dynamic 2.0 GGUF (text only)</th><th width=\"285\">Dynamic 4-bit Instruct (to fine-tune)</th><th>16-bit Instruct</th></tr></thead><tbody><tr><td><ul><li><a href=\"https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF\">2B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF\">4B</a></li></ul></td><td><ul><li><a href=\"https://huggingface.co/unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\">2B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\">4B</a></li></ul></td><td><ul><li><a href=\"https://huggingface.co/unsloth/gemma-3n-E2B-it\">2B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3n-E4B-it\">4B</a></li></ul></td></tr></tbody></table>\n\n**See all our Gemma 3n uploads including base and more formats in** [**our collection here**](https://huggingface.co/collections/unsloth/gemma-3n-685d3874830e49e1c93f9339)**.**\n\n## üñ•Ô∏è Running Gemma 3n\n\nCurrently Gemma 3n is only supported in **text format** for inference.\n\n{% hint style=\"info\" %}\nWe‚Äôve [fixed issues](#fixes-for-gemma-3n) with GGUFs not working properly in Ollama only. Please redownload if using Ollama.\n{% endhint %}\n\n### :gear: Official Recommended Settings\n\nAccording to the Gemma team, the official recommended settings for inference:\n\n`temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0`\n\n* Temperature of 1.0\n* Top\\_K of 64\n* Min\\_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)\n* Top\\_P of 0.95\n* Repetition Penalty of 1.0. (1.0 means disabled in llama.cpp and transformers)\n* Chat template:\n\n<pre data-overflow=\"wrap\"><code><strong>&#x3C;bos>&#x3C;start_of_turn>user\\nHello!&#x3C;end_of_turn>\\n&#x3C;start_of_turn>model\\nHey there!&#x3C;end_of_turn>\\n&#x3C;start_of_turn>user\\nWhat is 1+1?&#x3C;end_of_turn>\\n&#x3C;start_of_turn>model\\n\n  </strong></code></pre>\n* Chat template with `\\n`newlines rendered (except for the last)\n\n{% code overflow=\"wrap\" %}\n\n{% hint style=\"danger\" %}\nllama.cpp an other inference engines auto add a \\<bos> - DO NOT add TWO \\<bos> tokens! You should ignore the \\<bos> when prompting the model!\n{% endhint %}\n\n### :llama: Tutorial: How to Run Gemma 3n in Ollama\n\n{% hint style=\"success\" %}\nPlease re download Gemma 3N quants or remove the old ones via Ollama since there are some bug fixes. You can do the below to delete the old file and refresh it:\n\n1. Install `ollama` if you haven't already!\n\n2. Run the model! Note you can call `ollama serve`in another terminal if it fails! We include all our fixes and suggested parameters (temperature etc) in `params` in our Hugging Face upload!\n\n### üìñ Tutorial: How to Run Gemma 3n in llama.cpp\n\n{% hint style=\"info\" %}\nWe would first like to thank [Xuan-Son Nguyen](https://x.com/ngxson) from Hugging Face, [Georgi Gerganov](https://x.com/ggerganov) from the llama.cpp team on making Gemma 3N work in llama.cpp!\n{% endhint %}\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. If you want to use `llama.cpp` directly to load models, you can do the below: (:Q4\\_K\\_XL) is the quantization type. You can also download via Hugging Face (point 3). This is similar to `ollama run`\n\n3. **OR** download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose Q4\\_K\\_M, or other quantized versions (like BF16 full precision).",
  "code_samples": [
    {
      "code": "<bos><start_of_turn>user\nHello!<end_of_turn>\n<start_of_turn>model\nHey there!<end_of_turn>\n<start_of_turn>user\nWhat is 1+1?<end_of_turn>\n<start_of_turn>model\\n",
      "language": "unknown"
    },
    {
      "code": "ollama rm hf.co/unsloth/gemma-3n-E4B-it-GGUF:UD-Q4_K_XL\n\nollama run hf.co/unsloth/gemma-3n-E4B-it-GGUF:UD-Q4_K_XL",
      "language": "unknown"
    },
    {
      "code": "apt-get update\napt-get install pciutils -y\ncurl -fsSL https://ollama.com/install.sh | sh",
      "language": "bash"
    },
    {
      "code": "ollama run hf.co/unsloth/gemma-3n-E4B-it-GGUF:UD-Q4_K_XL",
      "language": "bash"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "./llama.cpp/llama-cli -hf unsloth/gemma-3n-E4B-it-GGUF:UD-Q4_K_XL -ngl 99 --jinja",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "üñ•Ô∏è Running Gemma 3n",
      "id": "üñ•Ô∏è-running-gemma-3n"
    },
    {
      "level": "h3",
      "text": ":gear: Official Recommended Settings",
      "id": ":gear:-official-recommended-settings"
    },
    {
      "level": "h3",
      "text": ":llama: Tutorial: How to Run Gemma 3n in Ollama",
      "id": ":llama:-tutorial:-how-to-run-gemma-3n-in-ollama"
    },
    {
      "level": "h3",
      "text": "üìñ Tutorial: How to Run Gemma 3n in llama.cpp",
      "id": "üìñ-tutorial:-how-to-run-gemma-3n-in-llama.cpp"
    }
  ],
  "url": "llms-txt#gemma-3n:-how-to-run-&-fine-tune",
  "links": []
}