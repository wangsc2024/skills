{
  "title": "Tokenize the text transcripts",
  "content": "def preprocess_function(example):\n    # Tokenize the text (keep the special tokens like <laugh> intact)\n    tokens = tokenizer(example[\"text\"], return_tensors=\"pt\")\n    # Flatten to list of token IDs\n    input_ids = tokens[\"input_ids\"].squeeze(0)\n    # The model will generate audio tokens after these text tokens.\n    # For training, we can set labels equal to input_ids (so it learns to predict next token).\n    # But that only covers text tokens predicting the next text token (which might be an audio token or end).\n    # A more sophisticated approach: append a special token indicating start of audio, and let the model generate the rest.\n    # For simplicity, use the same input as labels (the model will learn to output the sequence given itself).\n    return {\"input_ids\": input_ids, \"labels\": input_ids}\n\ntrain_data = dataset.map(preprocess_function, remove_columns=dataset.column_names)\npython\nfrom transformers import TrainingArguments,Trainer,DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\ntrainer = Trainer(\n    model = model,\n    train_dataset = dataset,\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)\npython\nmodel.save_pretrained(\"lora_model\")  # Local saving\ntokenizer.save_pretrained(\"lora_model\")",
  "code_samples": [
    {
      "code": "{% hint style=\"info\" %}\nThe above is a simplification. In reality, to fine-tune Orpheus properly, you would need the *audio tokens as part of the training labels*. Orpheus’s pre-training likely involved converting audio to discrete tokens (via an audio codec) and training the model to predict those given the preceding text. For fine-tuning on new voice data, you would similarly need to obtain the audio tokens for each clip (using Orpheus’s audio codec). The Orpheus GitHub provides a script for data processing – it encodes audio into sequences of `<custom_token_x>` tokens.\n{% endhint %}\n\nHowever, **Unsloth may abstract this away**: if the model is a FastModel with an associated processor that knows how to handle audio, it might automatically encode the audio in the dataset to tokens. If not, you’d have to manually encode each audio clip to token IDs (using Orpheus’s codebook). This is an advanced step beyond this guide, but keep in mind that simply using text tokens won’t teach the model the actual audio – it needs to match the audio patterns.\n\nLet's assume Unsloth provides a way to feed audio directly (for example, by setting `processor` and passing the audio array). If Unsloth does not yet support automatic audio tokenization, you might need to use the Orpheus repository’s `encode_audio` function to get token sequences for the audio, then use those as labels. (The dataset entries do have `phonemes` and some acoustic features which suggests a pipeline.)\n\n**Step 3: Set up training arguments and Trainer**",
      "language": "unknown"
    },
    {
      "code": "We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. Using a per\\_device\\_train\\_batch\\_size >1 may lead to errors if multi-GPU setup to avoid issues, ensure CUDA\\_VISIBLE\\_DEVICES is set to a single GPU (e.g., CUDA\\_VISIBLE\\_DEVICES=0). Adjust as needed.\n\n**Step 4: Begin fine-tuning**\n\nThis will start the training loop. You should see logs of loss every 50 steps (as set by `logging_steps`). The training might take some time depending on GPU – for example, on a Colab T4 GPU, a few epochs on 3h of data may take 1-2 hours. Unsloth’s optimizations will make it faster than standard HF training.\n\n**Step 5: Save the fine-tuned model**\n\nAfter training completes (or if you stop it mid-way when you feel it’s sufficient), save the model. This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!",
      "language": "unknown"
    }
  ],
  "headings": [],
  "url": "llms-txt#tokenize-the-text-transcripts",
  "links": []
}