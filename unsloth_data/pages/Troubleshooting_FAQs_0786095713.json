{
  "title": "Troubleshooting & FAQs",
  "content": "Tips to solve issues, and frequently asked questions.\n\nIf you're still encountering any issues with versions or depencies, please use our [Docker image](https://docs.unsloth.ai/get-started/install-and-update/docker) which will have everything pre-installed.\n\n{% hint style=\"success\" %}\n**Try always to update Unsloth if you find any issues.**\n\n`pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`\n{% endhint %}\n\n### Running in Unsloth works well, but after exporting & running on other platforms, the results are poor\n\nYou might sometimes encounter an issue where your model runs and produces good results on Unsloth, but when you use it on another platform like Ollama or vLLM, the results are poor or you might get gibberish, endless/infinite generations *or* repeated output&#x73;**.**\n\n* The most common cause of this error is using an <mark style=\"background-color:blue;\">**incorrect chat template**</mark>**.** Itâ€™s essential to use the SAME chat template that was used when training the model in Unsloth and later when you run it in another framework, such as llama.cpp or Ollama. When inferencing from a saved model, it's crucial to apply the correct template.\n* It might also be because your inference engine adds an unnecessary \"start of sequence\" token (or the lack of thereof on the contrary) so ensure you check both hypotheses!\n* <mark style=\"background-color:green;\">**Use our conversational notebooks to force the chat template - this will fix most issues.**</mark>\n  * Qwen-3 14B Conversational notebook [**Open in Colab**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_\\(14B\\)-Reasoning-Conversational.ipynb)\n  * Gemma-3 4B Conversational notebook [**Open in Colab**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_\\(4B\\).ipynb)\n  * Llama-3.2 3B Conversational notebook [**Open in Colab**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_\\(1B_and_3B\\)-Conversational.ipynb)\n  * Phi-4 14B Conversational notebook [**Open in Colab**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)\n  * Mistral v0.3 7B Conversational notebook [**Open in Colab**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\\(7B\\)-Conversational.ipynb)\n  * **More notebooks in our** [**notebooks docs**](https://docs.unsloth.ai/get-started/unsloth-notebooks)\n\n### Saving to GGUF / vLLM 16bit crashes\n\nYou can try reducing the maximum GPU usage during saving by changing `maximum_memory_usage`.\n\nThe default is `model.save_pretrained(..., maximum_memory_usage = 0.75)`. Reduce it to say 0.5 to use 50% of GPU peak memory or lower. This can reduce OOM crashes during saving.\n\n### How do I manually save to GGUF?\n\nFirst save your model to 16bit via:\n\nCompile llama.cpp from source like below:\n\nThen, save the model to F16:",
  "code_samples": [
    {
      "code": "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = \"merged_16bit\",)",
      "language": "python"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "python llama.cpp/convert_hf_to_gguf.py merged_model \\\n    --outfile model-F16.gguf --outtype f16 \\\n    --split-max-size 50G",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Running in Unsloth works well, but after exporting & running on other platforms, the results are poor",
      "id": "running-in-unsloth-works-well,-but-after-exporting-&-running-on-other-platforms,-the-results-are-poor"
    },
    {
      "level": "h3",
      "text": "Saving to GGUF / vLLM 16bit crashes",
      "id": "saving-to-gguf-/-vllm-16bit-crashes"
    },
    {
      "level": "h3",
      "text": "How do I manually save to GGUF?",
      "id": "how-do-i-manually-save-to-gguf?"
    }
  ],
  "url": "llms-txt#troubleshooting-&-faqs",
  "links": []
}