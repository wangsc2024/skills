{
  "title": "Phi-4 Reasoning: How to Run & Fine-tune",
  "content": "Learn to run & fine-tune Phi-4 reasoning models locally with Unsloth + our Dynamic 2.0 quants\n\nMicrosoft's new Phi-4 reasoning models are now supported in Unsloth. The 'plus' variant performs on par with OpenAI's o1-mini, o3-mini and Sonnet 3.7. The 'plus' and standard reasoning models are 14B parameters while the 'mini' has 4B parameters.\\\n\\\nAll Phi-4 reasoning uploads use our [Unsloth Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs) methodology.\n\n#### **Phi-4 reasoning - Unsloth Dynamic 2.0 uploads:**\n\n| Dynamic 2.0 GGUF (to run)                                                                                                                                                                                                                                                                                    | Dynamic 4-bit Safetensor (to finetune/deploy)                                                                                                                                                                                                                                                                                 |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <ul><li><a href=\"https://huggingface.co/unsloth/Phi-4-reasoning-plus-GGUF/\">Reasoning-plus</a> (14B)</li><li><a href=\"https://huggingface.co/unsloth/Phi-4-reasoning-GGUF\">Reasoning</a> (14B)</li><li><a href=\"https://huggingface.co/unsloth/Phi-4-mini-reasoning-GGUF/\">Mini-reasoning</a> (4B)</li></ul> | <ul><li><a href=\"https://huggingface.co/unsloth/Phi-4-reasoning-plus-unsloth-bnb-4bit\">Reasoning-plus</a></li><li><a href=\"https://huggingface.co/unsloth/phi-4-reasoning-unsloth-bnb-4bit\">Reasoning</a></li><li><a href=\"https://huggingface.co/unsloth/Phi-4-mini-reasoning-unsloth-bnb-4bit\">Mini-reasoning</a></li></ul> |\n\n## üñ•Ô∏è **Running Phi-4 reasoning**\n\n### :gear: Official Recommended Settings\n\nAccording to Microsoft, these are the recommended settings for inference:\n\n* <mark style=\"background-color:blue;\">**Temperature = 0.8**</mark>\n* Top\\_P = 0.95\n\n### **Phi-4 reasoning Chat templates**\n\nPlease ensure you use the correct chat template as the 'mini' variant has a different one.\n\n{% code overflow=\"wrap\" %}\n\n#### **Phi-4-reasoning and Phi-4-reasoning-plus:**\n\nThis format is used for general conversation and instructions:\n\n{% code overflow=\"wrap\" %}\n\n{% hint style=\"info\" %}\nYes, the chat template/prompt format is this long!\n{% endhint %}\n\n### ü¶ô Ollama: Run Phi-4 reasoning Tutorial\n\n1. Install `ollama` if you haven't already!\n\n2. Run the model! Note you can call `ollama serve`in another terminal if it fails. We include all our fixes and suggested parameters (temperature etc) in `params` in our Hugging Face upload.\n\n### üìñ Llama.cpp: Run Phi-4 reasoning Tutorial\n\n{% hint style=\"warning\" %}\nYou must use `--jinja` in llama.cpp to enable reasoning for the models, expect for the 'mini' variant. Otherwise no token will be provided.\n{% endhint %}\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. Download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose Q4\\_K\\_M, or other quantized versions.",
  "code_samples": [
    {
      "code": "<|system|>Your name is Phi, an AI math expert developed by Microsoft.<|end|><|user|>How to solve 3*x^2+4*x+5=1?<|end|><|assistant|>",
      "language": "unknown"
    },
    {
      "code": "<|im_start|>system<|im_sep|>You are Phi, a language model trained by Microsoft to help users. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> {Thought section} </think> {Solution section}. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion. Now, try to solve the following question through the above guidelines:<|im_end|><|im_start|>user<|im_sep|>What is 1+1?<|im_end|><|im_start|>assistant<|im_sep|>",
      "language": "unknown"
    },
    {
      "code": "apt-get update\napt-get install pciutils -y\ncurl -fsSL https://ollama.com/install.sh | sh",
      "language": "bash"
    },
    {
      "code": "ollama run hf.co/unsloth/Phi-4-mini-reasoning-GGUF:Q4_K_XL",
      "language": "bash"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "üñ•Ô∏è **Running Phi-4 reasoning**",
      "id": "üñ•Ô∏è-**running-phi-4-reasoning**"
    },
    {
      "level": "h3",
      "text": ":gear: Official Recommended Settings",
      "id": ":gear:-official-recommended-settings"
    },
    {
      "level": "h3",
      "text": "**Phi-4 reasoning Chat templates**",
      "id": "**phi-4-reasoning-chat-templates**"
    },
    {
      "level": "h3",
      "text": "ü¶ô Ollama: Run Phi-4 reasoning Tutorial",
      "id": "ü¶ô-ollama:-run-phi-4-reasoning-tutorial"
    },
    {
      "level": "h3",
      "text": "üìñ Llama.cpp: Run Phi-4 reasoning Tutorial",
      "id": "üìñ-llama.cpp:-run-phi-4-reasoning-tutorial"
    }
  ],
  "url": "llms-txt#phi-4-reasoning:-how-to-run-&-fine-tune",
  "links": []
}