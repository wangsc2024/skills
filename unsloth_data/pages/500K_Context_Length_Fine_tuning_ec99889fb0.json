{
  "title": "500K Context Length Fine-tuning",
  "content": "Learn how to enable >500K token context window fine-tuning with Unsloth.\n\nWeâ€™re introducing new algorithms in Unsloth that push the limits of long-context training for **any LLM and VLM**. Training LLMs like gpt-oss-20b can now reach **500K+ context lengths** on a single 80GB H100 GPU, compared to 80K previously with no accuracy degradation.\n\nYou can reach >**750K context windows** on a B200 192GB GPU.\n\n> **Try 500K-context gpt-oss-20b fine-tuning on our** [**80GB A100 Colab notebook**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt_oss_\\(20B\\)_500K_Context_Fine_tuning.ipynb)**.**\n\nWeâ€™ve significantly improved how Unsloth handles memory usage patterns, speed, and context lengths:\n\n* **60% lower VRAM use** with **3.2x longer context** via Unslothâ€™s new [fused and chunked cross-entropy](#unsloth-loss-refactoring-chunk-and-fuse) loss, with no degradation in speed or accuracy\n* Enhanced activation offloading in Unslothâ€™s [**Gradient Checkpointing**](#unsloth-gradient-checkpointing-enhanced)\n* Collabing with Stas Bekman from Snowflake on [Tiled MLP](#tiled-mlp-unlocking-500k), enabling 2Ã— more contexts\n\nUnslothâ€™s algorithms allows gpt-oss-20b QLoRA (4bit) with 290K context possible on a H100 with no accuracy loss, and 500K+ with Tiled MLP enabled, altogether delivering >**6.4x longer context lengths.**\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F8Ha930qR5XXBOK7M7oiy%2Fline_chart_light_tiled.png?alt=media&#x26;token=51467f68-a77b-4037-b9d9-e668223868c5\" alt=\"\" width=\"563\"><figcaption></figcaption></figure>\n\n### ğŸ“ Unsloth Loss Refactoring: Chunk & Fuse\n\nOur new fused loss implementation adds **dynamic sequence chunking**: instead of computing language model head logits and cross-entropies over the entire sequence at once, we process manageable slices along the flattened sequence dimension. This cuts peak memory from GBs to a smaller chunk sizes. Each chunk still runs a fully fused forward + backward pass via `torch.func.grad_and_value` , and retains mixed precision accuracy by upcasting to float32 if necessary. **These changes do not degrade training speed or accuracy.**\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FFF43WA1X8Y4vADBrCi8T%2Fline_chart_light.png?alt=media&#x26;token=7afc7f73-bc54-403a-9674-8a16841ec659\" alt=\"\" width=\"563\"><figcaption></figcaption></figure>\n\nThe key innovation is that the **chunk size is chosen automatically at runtime** based on available VRAM.\n\n* If you have more free VRAM, larger chunks are used for faster runs\n* If you have less VRAM, it increases the number of chunks to avoid memory blowouts.\n\nThis **removes manual tuning** and keeps our algorithm robust across old and new GPUs, workloads and different sequence lengths.\n\n{% hint style=\"success\" %}\nDue to automatic tuning, **smaller contexts will use more VRAM** (fewer chunks) to **avoid unnecessary overhead**. For the plots above, we adjust the number of loss chunks to reflect realistic VRAM tiers. With 80GB VRAM, this yields >3.2Ã— longer contexts.\n{% endhint %}\n\n### ğŸ Unsloth Gradient Checkpointing Enhancements\n\nOur [Unsloth Gradient Checkpointing](https://unsloth.ai/blog/long-context) algorithm, **introduced in April 2024**, quickly became popular and the standard across the industry, having been integrated into most training packages nowadays. It offloads activations to CPU RAM which allowed 10x longer context lengths. Our new enhancements uses CUDA Streams and other tricks to add at most **0.1%** training overhead with no impact on accuracy. Previously it added 1 to 3% training overhead.\n\n{% code expandable=\"true\" %}",
  "code_samples": [],
  "headings": [
    {
      "level": "h3",
      "text": "ğŸ“ Unsloth Loss Refactoring: Chunk & Fuse",
      "id": "ğŸ“-unsloth-loss-refactoring:-chunk-&-fuse"
    },
    {
      "level": "h3",
      "text": "ğŸ Unsloth Gradient Checkpointing Enhancements",
      "id": "ğŸ-unsloth-gradient-checkpointing-enhancements"
    }
  ],
  "url": "llms-txt#500k-context-length-fine-tuning",
  "links": []
}