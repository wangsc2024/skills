{
  "title": "Quantization-Aware Training (QAT)",
  "content": "Quantize models to 4-bit with Unsloth and PyTorch to recover accuracy.\n\nIn collaboration with PyTorch, we're introducing QAT (Quantization-Aware Training) in Unsloth to enable **trainable quantization** that recovers as much accuracy as possible. This results in significantly better model quality compared to standard 4-bit naive quantization. QAT can recover up to **70% of the lost accuracy** and achieve a **1–3%** model performance improvement on benchmarks such as GPQA and MMLU Pro.\n\n> **Try QAT with our free** [**Qwen3 (4B) notebook**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_\\(4B\\)_Instruct-QAT.ipynb)\n\n### :books:Quantization\n\n{% columns %}\n{% column width=\"50%\" %}\nNaively quantizing a model is called **post-training quantization** (PTQ). For example, assume we want to quantize to 8bit integers:\n\n1. Find `max(abs(W))`\n2. Find `a = 127/max(abs(W))` where a is int8's maximum range which is 127\n3. Quantize via `qW = int8(round(W * a))`\n   {% endcolumn %}\n\n{% column width=\"50%\" %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-f3e1cee8e4047dcbbbace7548694ad63af9869de%2Fquant-freeze.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n{% endcolumns %}\n\nDequantizing back to 16bits simply does the reverse operation by `float16(qW) / a` . Post-training quantization (PTQ) can greatly reduce storage and inference costs, but quite often degrades accuracy when representing high-precision values with fewer bits - especially at 4-bit or lower. One way to solve this to utilize our [**dynamic GGUF quants**](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs), which uses a calibration dataset to change the quantization procedure to allocate more importance to important weights. The other way is to make **quantization smarter, by making it trainable or learnable**!\n\n### :fire:Smarter Quantization\n\n<div><figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-1f6260ef5c041ada2f8b1fb4c6aad114f61061d4%2F4bit_QAT_recovery_sideways_clipped75_bigtext_all(1).png?alt=media\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-ad1ac9d29482ea07cbabb6efa18a0d1f06b297e9%2FQLoRA_QAT_Accuracy_Boosts_v7_bigaxes_nogrid_600dpi.png?alt=media\" alt=\"\"><figcaption></figcaption></figure></div>\n\nTo enable smarter quantization, we collaborated with the [TorchAO](https://github.com/pytorch/ao) team to add **Quantization-Aware Training (QAT)** directly inside of Unsloth - so now you can fine-tune models in Unsloth and then export them to 4-bit QAT format directly with accuracy improvements!\n\nIn fact, **QAT recovers 66.9%** of Gemma3-4B on GPQA, and increasing the raw accuracy by +1.0%. Gemma3-12B on BBH recovers 45.5%, and **increased the raw accuracy by +2.1%**. QAT has no extra overhead during inference, and uses the same disk and memory usage as normal naive quantization! So you get all the benefits of low-bit quantization, but with much increased accuracy!\n\n### :mag:Quantization-Aware Training\n\nQAT simulates the true quantization procedure by \"**fake quantizing**\" weights and optionally activations during training, which typically means rounding high precision values to quantized ones (while staying in high precision dtype, e.g. bfloat16) and then immediately dequantizing them.\n\nTorchAO enables QAT by first (1) inserting fake quantize operations into linear layers, and (2) transforms the fake quantize operations to actual quantize and dequantize operations after training to make it inference ready. Step 1 enables us to train a more accurate quantization representation.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-3d990e2bf19ef1aa7e65a8dd07e4b71cf8882a2a%2Fqat_diagram.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n### :sparkles:QAT + LoRA finetuning\n\nQAT in Unsloth can additionally be combined with LoRA fine-tuning to enable the benefits of both worlds: significantly reducing storage and compute requirements during training while mitigating quantization degradation! We support multiple methods via `qat_scheme` including `fp8-int4`, `fp8-fp8`, `int8-int4`, `int4` . We also plan to add custom definitions for QAT in a follow up release!\n\n{% code overflow=\"wrap\" %}\n\n### :teapot:Exporting QAT models\n\nAfter fine-tuning in Unsloth, you can call `model.save_pretrained_torchao` to save your trained model using TorchAO’s PTQ format. You can also upload these to the HuggingFace hub! We support any config, and we plan to make text based methods as well, and to make the process more simpler for everyone! But first, we have to prepare the QAT model for the final conversion step via:\n\n{% code overflow=\"wrap\" %}\n\nAnd now we can select which QAT style you want:\n\n{% code overflow=\"wrap\" %}",
  "code_samples": [
    {
      "code": "from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-4B-Instruct-2507\",\n    max_seq_length = 2048,\n    load_in_16bit = True,\n)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 32,\n    \n    # We support fp8-int4, fp8-fp8, int8-int4, int4\n    qat_scheme = \"int4\",\n)",
      "language": "python"
    },
    {
      "code": "from torchao.quantization import quantize_\nfrom torchao.quantization.qat import QATConfig\nquantize_(model, QATConfig(step = \"convert\"))",
      "language": "python"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": ":books:Quantization",
      "id": ":books:quantization"
    },
    {
      "level": "h3",
      "text": ":fire:Smarter Quantization",
      "id": ":fire:smarter-quantization"
    },
    {
      "level": "h3",
      "text": ":mag:Quantization-Aware Training",
      "id": ":mag:quantization-aware-training"
    },
    {
      "level": "h3",
      "text": ":sparkles:QAT + LoRA finetuning",
      "id": ":sparkles:qat-+-lora-finetuning"
    },
    {
      "level": "h3",
      "text": ":teapot:Exporting QAT models",
      "id": ":teapot:exporting-qat-models"
    }
  ],
  "url": "llms-txt#quantization-aware-training-(qat)",
  "links": []
}