{
  "title": "--save_model",
  "content": "torchrun --nproc_per_node=2 unsloth-cli.py \\\n  --model_name=Qwen/Qwen3-8B \\\n  --dataset=yahma/alpaca-cleaned \\\n  --learning_rate=2e-5 \\\n  --max_seq_length=2048 \\\n  --per_device_train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --max_steps=1000 \\\n  --save_model\nbash\n$ nvidia-smi\nMon Nov 24 12:58:42 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |\n| N/A   38C    P0            193W /  700W |   18903MiB /  81559MiB |     25%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA H100 80GB HBM3          On  |   00000000:05:00.0 Off |                    0 |\n| N/A   37C    P0            199W /  700W |   18905MiB /  81559MiB |     28%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    0   N/A  N/A            4935      C   ...und/unsloth/.venv/bin/python3      18256MiB |\n|    0   N/A  N/A            4936      C   ...und/unsloth/.venv/bin/python3        630MiB |\n|    1   N/A  N/A            4935      C   ...und/unsloth/.venv/bin/python3        630MiB |\n|    1   N/A  N/A            4936      C   ...und/unsloth/.venv/bin/python3      18258MiB |\n+-----------------------------------------------------------------------------------------+\n```\n\nWe can see that both GPUs are now using \\~19GB of VRAM per H100 GPU!\n\nInspecting the training logs, we see that weâ€™re able to train at a rate of \\~1.1 iterations/s. This training speed is \\~constant even as we add more GPUs, so our training throughput increases \\~linearly with the number of GPUs!\n\nWe ran a few short rank-16 LoRA fine-tunes on [unsloth/Llama-3.2-1B-Instruct](https://huggingface.co/unsloth/Llama-3.2-1B-Instruct) on the [yahma/alpaca-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned) dataset to demonstrate the improved training throughput when using DDP training with multiple GPUs.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FdySJnhNUzVD3gsWmPqHR%2Funknown.png?alt=media&#x26;token=9905cccb-04c8-45b1-bfb1-680823713319\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\nThe above figure compares training loss between two Llama-3.2-1B-Instruct LoRA fine-tunes over 500 training steps, with single GPU training (pink) vs. multi-GPU DDP training (blue).\n\nNotice that the loss curves match in scale and trend, but otherwise are a *bit* different, since *the multi-GPU training processes twice as much training data per step*. This results in a slightly different training curve with less variability on a step-by-step basis.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fz4XgknzMgljaFInMEzHc%2Funknown.png?alt=media&#x26;token=4e28e2b1-8bc8-4049-983d-e4f980f3f4cf\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\nThe above figure plots training progress for the same two fine-tunes.\n\nNotice that the multi-GPU DDP training progresses through an epoch of the training data in half as many steps as single GPU training. This is because each GPU can process a distinct batch (of size `per_device_train_batch_size`) per step. However, the per-step timing for DDP training is slightly slower due to distributed communication for the model weight updates. As you increase the number of GPUs, the training throughput will continue to increase \\~linearly (but with a small, but increasing penalty for the distributed comms).\n\nThese same loss and training epoch progress behaviors hold for QLoRA fine-tunes, in which we loaded the base models in 4-bit precision in order to save additional GPU memory. This is particularly useful for training large models on limited amounts of GPU VRAM:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FUrCEgA7OBVhc8ICkMaP6%2Funknown.png?alt=media&#x26;token=0f5de3df-77df-4ee5-bf7a-68dead857c9a\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\nTraining loss comparison between two Llama-3.2-1B-Instruct QLoRA fine-tunes over 500 training steps, with single GPU training (orange) vs. multi-GPU DDP training (purple).\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F8cG6rjmjeznNfgWrYdnG%2Funknown.png?alt=media&#x26;token=d1c2c1fe-c117-49b5-8e9d-fdc01154cc01\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\nTraining progress comparison for the same two fine-tunes.",
  "code_samples": [
    {
      "code": "{% endcode %}\n\nIf you have more GPUs, you may set `--nproc_per_node` accordingly to utilize them.\n\n**Note:** You can use the `torchrun` launcher with any of your Unsloth training scripts, including the [scripts](https://github.com/unslothai/notebooks/tree/main/python_scripts) converted from our free Colab notebooks, and DDP will be auto-enabled when training with >1 GPU!\n\nTaking a look again at `nvidia-smi` while training is in-flight:\n\n{% code expandable=\"true\" %}",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Training metrics",
      "id": "training-metrics"
    }
  ],
  "url": "llms-txt#--save_model",
  "links": []
}