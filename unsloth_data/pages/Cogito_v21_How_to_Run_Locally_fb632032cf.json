{
  "title": "Cogito v2.1: How to Run Locally",
  "content": "Cogito v2.1 LLMs are one of the strongest open models in the world trained with IDA. Also v1 comes in 4 sizes: 70B, 109B, 405B and 671B, allowing you to select which size best matches your hardware.\n\n{% hint style=\"success\" %}\nDeep Cogito v2.1 is an updated 671B MoE that is the most powerful open weights model as of 19 November 2025.\n{% endhint %}\n\nCogito v2.1 comes in 1 671B MoE size, whilst Cogito v2 Preview is [Deep Cogito](https://www.deepcogito.com/)'s release of models spans 4 model sizes ranging from 70B to 671B. By using **IDA (Iterated‚ÄØDistillation‚ÄØ& Amplification)**, these models are trained with the model internalizing the reasoning process using iterative policy improvement, rather than simply searching longer at inference time (like DeepSeek R1).\n\nDeep Cogito is based in [San Fransisco, USA](https://techcrunch.com/2025/04/08/deep-cogito-emerges-from-stealth-with-hybrid-ai-reasoning-models/) (like Unsloth :flag\\_us:) and we're excited to provide quantized dynamic models for all 4 model sizes! All uploads use Unsloth [Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs) for SOTA 5-shot MMLU and KL Divergence performance, meaning you can run & fine-tune quantized these LLMs with minimal accuracy loss!\n\n**Tutorials navigation:**\n\n<a href=\"https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/cogito-v2-how-to-run-locally#run-cogito-671b-moe-in-llama.cpp\" class=\"button secondary\">Run 671B MoE</a><a href=\"https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/cogito-v2-how-to-run-locally#run-cogito-109b-moe-in-llama.cpp\" class=\"button secondary\">Run 109B MoE</a><a href=\"https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/cogito-v2-how-to-run-locally#run-cogito-405b-dense-in-llama.cpp\" class=\"button secondary\">Run 405B Dense</a><a href=\"https://docs.unsloth.ai/basics/tutorials-how-to-fine-tune-and-run-llms/cogito-v2-how-to-run-locally#run-cogito-70b-dense-in-llama.cpp\" class=\"button secondary\">Run 70B Dense</a>\n\n{% hint style=\"success\" %}\nChoose which model size fits your hardware! We upload 1.58bit to 16bit variants for all 4 model sizes!\n{% endhint %}\n\n## :gem: Model Sizes and Uploads\n\nThere are 4 model sizes:\n\n1. 2 Dense models based off from Llama - 70B and 405B\n2. 2 MoE models based off from Llama 4 Scout (109B) and DeepSeek R1 (671B)\n\n<table data-full-width=\"false\"><thead><tr><th>Model Sizes</th><th width=\"256.9999694824219\">Recommended Quant &#x26; Link</th><th>Disk Size</th><th>Architecture</th></tr></thead><tbody><tr><td>70B Dense</td><td><a href=\"https://huggingface.co/unsloth/cogito-v2-preview-llama-70B-GGUF\">UD-Q4_K_XL</a></td><td><strong>44GB</strong></td><td>Llama 3 70B</td></tr><tr><td>109B MoE</td><td><a href=\"https://huggingface.co/unsloth/cogito-v2-preview-llama-109B-MoE-GGUF\">UD-Q3_K_XL</a></td><td><strong>50GB</strong></td><td>Llama 4 Scout</td></tr><tr><td>405B Dense</td><td><a href=\"https://huggingface.co/unsloth/cogito-v2-preview-llama-405B-GGUF\">UD-Q2_K_XL</a></td><td><strong>152GB</strong></td><td>Llama 3 405B</td></tr><tr><td>671B MoE</td><td><a href=\"https://huggingface.co/unsloth/cogito-v2-preview-deepseek-671B-MoE-GGUF\">UD-Q2_K_XL</a></td><td><strong>251GB</strong></td><td>DeepSeek R1</td></tr></tbody></table>\n\n{% hint style=\"success\" %}\nThough not necessary, for the best performance, have your VRAM + RAM combined = to the size of the quant you're downloading. If you have less VRAM + RAM, then the quant will still function, just be much slower.\n{% endhint %}\n\n## üê≥ Run Cogito 671B MoE in llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n{% code overflow=\"wrap\" %}\n\n2. If you want to use `llama.cpp` directly to load models, you can do the below: (:IQ1\\_S) is the quantization type. You can also download via Hugging Face (point 3). This is similar to `ollama run` . Use `export LLAMA_CACHE=\"folder\"` to force `llama.cpp` to save to a specific location.\n\n{% hint style=\"success\" %}\nPlease try out `-ot \".ffn_.*_exps.=CPU\"` to offload all MoE layers to the CPU! This effectively allows you to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression to fit more layers if you have more GPU capacity.\n\nIf you have a bit more GPU memory, try `-ot \".ffn_(up|down)_exps.=CPU\"` This offloads up and down projection MoE layers.\n\nTry `-ot \".ffn_(up)_exps.=CPU\"` if you have even more GPU memory. This offloads only up projection MoE layers.\n\nAnd finally offload all layers via `-ot \".ffn_.*_exps.=CPU\"` This uses the least VRAM.\n\nYou can also customize the regex, for example `-ot \"\\.(6|7|8|9|[0-9][0-9]|[0-9][0-9][0-9])\\.ffn_(gate|up|down)_exps.=CPU\"` means to offload gate, up and down MoE layers but only from the 6th layer onwards.\n{% endhint %}\n\n3. Download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose `UD-IQ1_S`(dynamic 1.78bit quant) or other quantized versions like `Q4_K_M` . We <mark style=\"background-color:green;\">**recommend using our 2.7bit dynamic quant**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**`UD-Q2_K_XL`**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**to balance size and accuracy**</mark>. More versions at: <https://huggingface.co/unsloth/cogito-671b-v2.1-GGUF>\n\n{% code overflow=\"wrap\" %}",
  "code_samples": [
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "shellscript"
    },
    {
      "code": "export LLAMA_CACHE=\"unsloth/cogito-671b-v2.1-GGUF\"\n./llama.cpp/llama-cli \\\n    -hf unsloth/cogito-671b-v2.1-GGUF:UD-Q2_K_XL \\\n    --n-gpu-layers 99 \\\n    --temp 0.6 \\\n    --top_p 0.95 \\\n    --min_p 0.01 \\\n    --ctx-size 16384 \\\n    --seed 3407 \\\n    --jinja \\\n    -ot \".ffn_.*_exps.=CPU\"",
      "language": "shellscript"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": ":gem: Model Sizes and Uploads",
      "id": ":gem:-model-sizes-and-uploads"
    },
    {
      "level": "h2",
      "text": "üê≥ Run Cogito 671B MoE in llama.cpp",
      "id": "üê≥-run-cogito-671b-moe-in-llama.cpp"
    }
  ],
  "url": "llms-txt#cogito-v2.1:-how-to-run-locally",
  "links": []
}