{
  "title": "FP8 Reinforcement Learning",
  "content": "Train reinforcement learning (RL) and GRPO in FP8 precision with Unsloth.\n\nWe're introducing FP8-precision training for RL, making FP8 GRPO now possible on **consumer GPUs** (RTX 40, 50 etc). DeepSeek-R1 demonstrated how powerful FP8 can be and with Unsloth, Qwen3-1.7B FP8 GRPO now works on just **5GB of VRAM**.\n\nFaster RL inference is critical as it's the most compute-intensive workload in RL. We collabed with [TorchAO](https://github.com/pytorch/ao) from PyTorch to enable performance gains with no loss in accuracy.\n\n* **\\~1.4√ó faster** RL inference via [vLLM](https://github.com/vllm-project/vllm) ‚Ä¢ 2x longer context vs. BF16 and FP16\n* **60% less VRAM** and **10√ó longer** context than other FP8 RL implementations\n* Unsloth is the **only framework** to make FP8 RL LoRA work on consumer GPUs (e.g. NVIDIA GeForce RTX 40 and 50 Series). Also works on H100, H200, B200 etc.\n* Use `load_in_fp8 = True` within `FastLanguageModel` to enable FP8 RL.\n* Though Qwen3-8B fits in 16GB VRAM, free Colab NVIDIA Tesla T4 GPUs **don‚Äôt support FP8**. So our notebooks use **24GB L4 GPUs which fits Qwen3-14B**.\n\n**Notebooks:** [Qwen3-8B FP8 GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb) and [Llama-3.2-1B FP8 GRPO](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb)\n\n{% hint style=\"success\" %}\nBonus: You‚Äôll notice Unsloth now uses much less VRAM. We‚Äôll share details in a new blog soon.\n{% endhint %}\n\nOur FP8 support uses Unsloth‚Äôs [weight-sharing feature](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/memory-efficient-rl), reducing VRAM use by another **50%**, enabling **10√ó more** context with no accuracy loss. We use [vLLM](https://github.com/vllm-project/vllm) for fast inference and, our techniques like Unsloth [Standby](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/memory-efficient-rl) and [Flex Attention](https://docs.unsloth.ai/models/gpt-oss-how-to-run-and-fine-tune/long-context-gpt-oss-training) to further reduce VRAM use. TorchAO enables universal on the fly FP8, so Llama, Gemma, Mistral & more work. We‚Äôve also [uploaded](#unsloth-fp8-uploads) most FP8 models (including Qwen3).\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FNhbi7jRc6zwCAeuddBBk%2Foutput(14).png?alt=media&#x26;token=80ad0712-4626-4536-aa57-29bc53b40540\" alt=\"\" width=\"375\"><figcaption><p>Reward plot shows FP8 following the same trend as BF16</p></figcaption></figure>\n\n### :sunflower:FP8 vs BF16 Training\n\nResearch shows that FP8 training can largely match BF16 accuracy and if you serve models in FP8, **training and serving in the same precision** helps preserve accuracy. Also FP8 vs BF16 yields 1.6x higher throughput on H100s and has 2x lower memory usage.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FApLfXUBVZSbjpPhRJG6z%2Ffp8%20f16%20quant.png?alt=media&#x26;token=77a2917a-f191-44a7-8597-6796fcf24ed7\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\n#### Weight scales & FP8 types\n\nQuantized training stores a low-precision weight (e.g., FP8) plus a higher-precision scale (FP16/BF16/FP32). You approximately recover the original weight via: `original_weight ‚âà quantized_weight * weight_scale`\n\nThe scale maps the weight‚Äôs range into FP8‚Äôs representable range. More scales usually improve accuracy, but scales cost extra high-precision memory, so it‚Äôs a tradeoff. [DeepSeek R1](https://arxiv.org/abs/2501.12948), for instance, mostly favors block quantization.\n\nThere are 3 common FP8 types as defined by vLLM's [llm-compressor](https://github.com/vllm-project/llm-compressor). We benchmarked Qwen3-8B on all 3 types, and also checked throughput, MMLU Pro and GQPA Diamond. We find **FP8 Block-Wise or Per-Channel (-FP8-Dynamic) is the best** in terms of accuracy and throughput.\n\n<table><thead><tr><th width=\"121\">Type</th><th width=\"225.20001220703125\"></th><th width=\"126.4000244140625\">Throughput</th><th width=\"121.60003662109375\">MMLU Pro</th><th>GQPA Diamond</th></tr></thead><tbody><tr><td></td><td>Bfloat16 Baseline</td><td>11,367</td><td><strong>62.04%</strong></td><td>28.79%</td></tr><tr><td>Block-wise</td><td>Scales per block (128X128)</td><td>12,041</td><td><strong>62.37%</strong></td><td><strong>29.29%</strong></td></tr><tr><td>Per-Channel</td><td>1 scale per row or column</td><td>12,963</td><td>61.89%</td><td><strong>31.82%</strong></td></tr><tr><td>Per-Tensor</td><td>1 scale for the whole tensor</td><td><strong>13,681</strong></td><td>61.83%</td><td>27.78%</td></tr></tbody></table>\n\n### :zap:FP8 Performance Benchmarks\n\nUnsloth FP8 RL inference via vLLM is generally 1.4x faster than BF16. You may see even more speed improvements if the model is larger!\n\n#### Accuracy Training loss Benchmarks\n\nWe tested multiple models including Qwen3-4B, 8B, 14B, Llama 3.2 1B, 3B, Qwen3-VL-2B, Qwen3-VL 4B and many more. All were trained both in BF16 and FP8. As seen in the plots, the **loss curves during SFT for BF16 and FP8 closely track each other**. There isn‚Äôt much to choose between the two data types in terms of training loss:\n\n{% columns %}\n{% column %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FR6Hx9RtgqPXnYxvx5BbR%2FW%26B%20Chart%2025_11_2025%2C%208_54_56%20am.png?alt=media&#x26;token=d1d70d59-df00-45bb-8352-e833f9b5f3cd\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FlUzs2uNkCyF1ulNdrVRc%2FW%26B%20Chart%2025_11_2025%2C%208_56_50%20am.png?alt=media&#x26;token=09545235-c9fa-4b76-a834-ffe0ceb8f639\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n{% endcolumns %}\n\nFor GRPO specifically, due to generation differences, the goal is to see if the reward plots at least match up and not diverge (sometimes for eg Qwen3-14B runs might not be exactly similar)\n\n{% columns %}\n{% column width=\"50%\" %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FeLBs5GrQb988GcrYVzpF%2FW%26B%20Chart%2025_11_2025%2C%209_00_50%20am.png?alt=media&#x26;token=59220833-33c6-4c28-abe7-b5d0d93a0a17\" alt=\"\"><figcaption></figcaption></figure>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FPqXVeofauAIr5Qngm9d2%2FW%26B%20Chart%2025_11_2025%2C%209_08_06%20am.png?alt=media&#x26;token=16498cf1-17e1-4984-b933-fe3633e19a6b\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n\n{% column width=\"50%\" %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FC76ql9G59SB0v3nG3pbL%2FW%26B%20Chart%2025_11_2025%2C%209_05_32%20am.png?alt=media&#x26;token=554b6fe8-c121-48a4-8b33-41f28fc38ebb\" alt=\"\"><figcaption></figcaption></figure>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FqM5NKHjOxqJv0hrzmr2B%2FW%26B%20Chart%2025_11_2025%2C%209_07_12%20am.png?alt=media&#x26;token=a7ad9eb0-0ea2-4364-982a-0875ec63459f\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n{% endcolumns %}\n\n### :shinto\\_shrine:Inference = 96% of RL training\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FTvC7GqMM5XAfV8Zv2tpf%2Fimage.avif?alt=media&#x26;token=62b40c34-3111-40a9-b02a-4bfa8826402d\" alt=\"\"><figcaption></figcaption></figure>\n\nIn RL, we have to call the LLM / VLM to generate some possible candidate solutions to some run, then we score each possible solution and **reward good solutions, and penalize bad answers**. To achieve maximum efficiency, we must make inference nearly 100% of the training run. In Unsloth, we **managed to make training take only <4% of the entire RL run, with 96% being purely vLLM inference.**\n\nFor example for Qwen-3-8B, which is 1.15x faster on shorter sequence lengths, vLLM FP8 itself for inference (without training) throughput is also 1.15x faster. We see our RL run in Unsloth attains also 1.15x faster on tokens processed, showing how **training overhead is negligible in Unsloth.**\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2F105iKEPXAor00mdUPTfo%2FTokens%20Processed%20during%20RL.svg?alt=media&#x26;token=ca1c1d76-64b2-4019-91ac-7043f0ab79fd\" alt=\"\"><figcaption></figcaption></figure>\n\n### :1234:60% less memory usage\n\nIn theory, you‚Äôd expect memory savings to roughly **equal to the model‚Äôs weight memory**, because: optimizer states are still stored in high precision and activations are also stored in high precision (for now). Our findings match the theory. For LoRA fine-tuning, we observed: **\\~30 GB saved** for **Qwen3-32B, \\~14 GB saved** for **Qwen2.5-14B** and **\\~8 GB saved** for **Qwen3-8B**\n\nFor **BF16 LoRA fine-tuning on** Qwen3-32B, we were ooming at higher batch sizes and had to shrink the batch. The **FP8 variant had no such issues**, and we could use **larger batch sizes** without OOMing.\n\nAlso reminder in Unsloth we share vLLM's memory space for the weights as introduced in [memory-efficient-rl](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/memory-efficient-rl \"mention\") - we have bought this trick over to the FP8 domain!\n\n| 80GB GPU                                                                                                                                                            | Inference Engine   | Training Engine                          |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------ | ---------------------------------------- |\n| Model Weights                                                                                                                                                       | **8GB SHARED FP8** | **<<< SHARED**                           |\n| <p><mark style=\"background-color:purple;\"><strong>Multi-purpose</strong></mark></p><p><mark style=\"background-color:purple;\"><strong>72GB space</strong></mark></p> | KV Cache           | Activations, Gradients, Optimizer States |\n\nTo enable [Unsloth Standby](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/memory-efficient-rl) for FP8 (or BF16) RL, simply add the below to all RL / GRPO training runs before any Unsloth import:\n\n### :question:How to use FP8 RL / installation\n\nSimply update Unsloth or install Unsloth in a new virtual environment for H100, L4, RTX 50x, RTX 40x, H200s, B200s, and any NVIDIA GPU (consumer or data center grade) released after the RTX 4090.\n\nTo update Unsloth: `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`Or make a new environment:\n\n{% code overflow=\"wrap\" %}\n\nThen use `load_in_fp8 = True` and you're good to go! We'll auto map the model name to the Float8 variant, or we'll on the fly convert the model to Float8!\n\n<pre class=\"language-python\" data-overflow=\"wrap\"><code class=\"lang-python\">import os\nos.environ['UNSLOTH_VLLM_STANDBY'] = \"1\" # Unsloth standby saves 30%+ memory for RL\nfrom unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Can increase for longer reasoning traces\nlora_rank = 32 # Larger rank = smarter, but slower\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Qwen3-8B\",\n    max_seq_length = max_seq_length,\n    load_in_4bit = False, # False for LoRA 16bit\n    fast_inference = True, # Enable vLLM fast inference\n    max_lora_rank = lora_rank,\n<strong>    load_in_fp8 = True, # Float8 RL / GRPO!\n</strong>)\n</code></pre>\n\nFor example on a RTX 5090 (reminder to set `os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"` )\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FlVA3v7E5J8pHb1QKLi2V%2Fimage.png?alt=media&#x26;token=20b5329c-6ac2-479a-a4cc-2a0d74486696\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\nThen use our 2 FP8 notebooks for RL:\n\n{% columns %}\n{% column %}\n**Qwen3-8B FP8 RL Colab**\n\n{% embed url=\"<https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_8B_FP8_GRPO.ipynb>\" %}\n{% endcolumn %}\n\n{% column %}\n**Llama-3.2-1B-FP8 RL Colab**\n\n{% embed url=\"<https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama_FP8_GRPO.ipynb>\" %}\n{% endcolumn %}\n{% endcolumns %}\n\n### :cd:Implementing FP8 Training\n\nOur first reference point was `transformers`, which already supports FP8 in a couple of ways. One of them is a block-quantized matmul implementation: when a layer receives 16‚Äëbit activations, it quantizes them and passes them to a custom FP8 matmul kernel. After wiring this up and benchmarking on an NVIDIA H100, we saw the opposite of what we wanted: fine-tuning became about **4√ó slower** than standard BF16 fine-tuning.\n\nSo we worked with the [TorchAO](https://github.com/pytorch/ao) team (huge thanks to[ Andrew](https://github.com/unslothai/unsloth/pull/3440)) to incorporate TorchAO‚Äôs FP8 support into our RL workloads and saw around **1.4√ó faster throughput** and up to **60% less model memory usage**. At a high level:\n\n* We store the frozen LoRA weights in FP8.\n* During the forward pass, we apply dynamic FP8 quantization to the input activations, while keeping the trainable LoRA adapters in BF16.\n* These FP8 weights share the same buffers as the vLLM model weights, so there‚Äôs only a single FP8 copy of the model in memory at any time (no ‚Äúdouble model‚Äù memory overhead).\n* In the backward pass, we dequantize the LoRA weights so all gradient computation is done in BF16 for better accuracy.\n\nThis general setup works across all supported RL algorithms, including [GSPO](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/gspo-reinforcement-learning), Dr. GRPO, PPO, and DPO.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FUir0hB7T0xBtWUTnK3aG%2Funknown.png?alt=media&#x26;token=d225cd2e-fdf4-4521-8e9f-72bd684eb9e4\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\nTorchAO provides PyTorch-native FP8 support for both training and inference, offering a variety of scaling granularities including tensorwise, row-wise, and 128x128 blockwise (prototype). TorchAO‚Äôs FP8 support can improve inference throughput by up to [1.64x at 27B scale](https://huggingface.co/pytorch/gemma-3-27b-it-FP8/blob/main/README.md#results-h100-machine) with row-wise scaling granularity. For more details, visit the TorchAO [FP8 README](https://github.com/pytorch/ao/blob/main/torchao/float8/README.md).\n\n#### TorchAO‚Äôs block-quantized FP8 matmul\n\nWe used TorchAO‚Äôs block‚Äëquantized FP8 matmul implementation which provided:\n\n* **80% of BF16 throughput**\n* Without degrading loss or training stability\n\nSo for a while, this became our default FP8 matmul backend, until FBGEMM caught up - we know default to using FBGEMM's implementation, if your GPU supports it! The current version of Unsloth can automatically choose the best backend based on what‚Äôs installed. If you have the right packages, you don‚Äôt have to leave performance on the table üôÇ\n\nPS: We also experimented with DeepSeek‚Äôs DeepGEMM, but couldn‚Äôt get it fully integrated end‚Äëto‚Äëend to run clean, apples‚Äëto‚Äëapples comparisons.\n\n### :bird:On the fly TorchAO FP8 quantization\n\nMassive thanks to [Andrew](https://github.com/unslothai/unsloth/pull/3440) from TorchAO, Unsloth FP8 RL also lets you quantize the model on the fly by doing quantization within the model load time and passing that on to vLLM. This way, you need not explicitly quantize the model yourself (we handle it for you). You can do this by setting `load_in_fp8 = True` in the model load arguments, and will do offline FP8 if we don't find a suitable pre-quantized checkpoint.\n\n### :tada:Unsloth FP8 uploads\n\nFor convenience, we uploaded FP8 Dynamic and FP8 Block models on Hugging Face. You can use them for FP8 training or also efficient & fast serving/deployment via [vLLM](https://docs.unsloth.ai/basics/inference-and-deployment/vllm-guide)/[SGLang](https://docs.unsloth.ai/basics/inference-and-deployment/sglang-guide) etc.\n\nFP8 Dynamic offers slightly faster training and lower VRAM usage than FP8 Block, but with a small trade-off in accuracy. [See here](https://docs.unsloth.ai/get-started/unsloth-model-catalog#fp8) for our full list of FP8 quants, but here the most popular ones:\n\n| Model                 | FP8 uploads                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Qwen3 (2507)**      | <p>4B Instruct ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-4B-Instruct-2507-FP8\">FP8</a><br>4B Thinking ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-4B-Thinking-2507-FP8\">FP8</a><br>30B-A3B Instruct ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-30B-A3B-Instruct-2507-FP8\">FP8</a><br>30B-A3B Thinking ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-30B-A3B-Thinking-2507-FP8\">FP8</a></p>                                                                                                                                                                                                                                                                                                                                 |\n| **Qwen3-VL**          | <p>4B Instruct ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-FP8\">FP8</a><br>4B Thinking ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-FP8\">FP8</a><br>8B Instruct ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-FP8\">FP8</a><br>8B Thinking ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-FP8\">FP8</a></p>                                                                                                                                                                                                                                                                                                                                                             |\n| **Llama 3.1**         | <p>8B Instruct ‚Äî <a href=\"https://huggingface.co/unsloth/Llama-3.1-8B-Instruct-FP8-Dynamic\">Dynamic</a> ¬∑ <a href=\"https://huggingface.co/unsloth/Llama-3.1-8B-Instruct-FP8-Block\">Block</a><br>8B Base ‚Äî <a href=\"https://huggingface.co/unsloth/Llama-3.1-8B-FP8-Dynamic\">Dynamic</a> ¬∑ <a href=\"https://huggingface.co/unsloth/Llama-3.1-8B-FP8-Block\">Block</a><br>70B ‚Äî <a href=\"https://huggingface.co/unsloth/Llama-3.1-70B-FP8-Dynamic\">Dynamic</a> ¬∑ <a href=\"https://huggingface.co/unsloth/Llama-3.1-70B-FP8-Block\">Block</a></p>                                                                                                                                                                                                |\n| **Qwen3**             | <p>0.6B ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-0.6B-FP8\">FP8</a><br>1.7B ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-1.7B-FP8\">FP8</a><br>4B ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-4B-FP8\">FP8</a><br>8B ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-8B-FP8\">FP8</a><br>14B ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-14B-FP8\">FP8</a><br>32B ‚Äî <a href=\"https://huggingface.co/unsloth/Qwen3-32B-FP8\">FP8</a></p>                                                                                                                                                                                                                                                                                         |\n| **Llama 3.3**         | 70B ‚Äî [Dynamic](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-FP8-Dynamic) ¬∑ [Block](https://huggingface.co/unsloth/Llama-3.3-70B-Instruct-FP8-Block)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| **Llama 3.2**         | <p>1B Base ‚Äî <a href=\"https://huggingface.co/unsloth/Llama-3.2-1B-FP8-Dynamic\">Dynamic</a> ¬∑ <a href=\"https://huggingface.co/unsloth/Llama-3.2-1B-FP8-Block\">Block</a><br>1B Instruct ‚Äî <a href=\"https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-FP8-Dynamic\">Dynamic</a> ¬∑ <a href=\"https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-FP8-Block\">Block</a><br>3B Base ‚Äî <a href=\"https://huggingface.co/unsloth/Llama-3.2-3B-FP8-Dynamic\">Dynamic</a> ¬∑ <a href=\"https://huggingface.co/unsloth/Llama-3.2-3B-FP8-Block\">Block</a><br>3B Instruct ‚Äî <a href=\"https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-FP8-Dynamic\">Dynamic</a> ¬∑ <a href=\"https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-FP8-Block\">Block</a></p> |\n| **Granite 4.0**       | <p>h-tiny ‚Äî <a href=\"https://huggingface.co/unsloth/granite-4.0-h-tiny-FP8-Dynamic\">FP8 Dynamic</a><br>h-small ‚Äî <a href=\"https://huggingface.co/unsloth/granite-4.0-h-small-FP8-Dynamic\">FP8 Dynamic</a></p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| **Magistral Small**   | [FP8 Dynamic](https://huggingface.co/unsloth/Magistral-Small-2509-FP8-Dynamic) ¬∑ [FP8 torchao](https://huggingface.co/unsloth/Magistral-Small-2509-FP8-torchao)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n| **Mistral Small 3.2** | [FP8](https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-FP8)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n| **Gemma 3**           | <p>270m ‚Äî <a href=\"https://huggingface.co/unsloth/gemma-3-270m-it-FP8-Dynamic\">FP8</a><br>1B ‚Äî <a href=\"https://huggingface.co/unsloth/gemma-3-1b-it-FP8-Dynamic\">FP8</a><br>4B ‚Äî <a href=\"https://huggingface.co/unsloth/gemma-3-4b-it-FP8-Dynamic\">FP8</a><br>12B ‚Äî <a href=\"https://huggingface.co/unsloth/gemma-3-12B-it-FP8-Dynamic\">FP8</a><br>27B ‚Äî <a href=\"https://huggingface.co/unsloth/gemma-3-27b-it-FP8-Dynamic\">FP8</a></p>                                                                                                                                                                                                                                                                                                  |\n\n### :person\\_tipping\\_hand:Acknowledgements\n\nHuge thanks to the entire PyTorch and TorchAO team for their help and collaboration! A huge thank you especially to: Andrew Or, Jerry Zhang, Supriya Rao, Scott Roy and Mergen Nachin for helping on many discussions on FP8 RL, and on helping to integrate it into Unsloth! Also thanks to the Executorch team as well!",
  "code_samples": [
    {
      "code": "import os\nos.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"",
      "language": "python"
    },
    {
      "code": "python -m venv unsloth_env\nsource unsloth_env/bin/activate\n\npip install unsloth vllm\npip install --pre torchao --index-url https://download.pytorch.org/whl/nightly/cu128 --force-reinstall\npip install --pre fbgemm-gpu fbgemm-gpu-genai --index-url https://download.pytorch.org/whl/cu128 --force-reinstall\npip install --upgrade numba numpy",
      "language": "bash"
    },
    {
      "code": "from unsloth import FastLanguageModel\nfp8_model = FastLanguageModel.from_pretrained(\n    \"unsloth/Llama-3.3-70B-Instruct\", # Can be any model name!\n    load_in_fp8 = True, # Can be \"block\" for block FP8, True for row FP8, False\n)",
      "language": "python"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": ":sunflower:FP8 vs BF16 Training",
      "id": ":sunflower:fp8-vs-bf16-training"
    },
    {
      "level": "h3",
      "text": ":zap:FP8 Performance Benchmarks",
      "id": ":zap:fp8-performance-benchmarks"
    },
    {
      "level": "h3",
      "text": ":shinto\\_shrine:Inference = 96% of RL training",
      "id": ":shinto\\_shrine:inference-=-96%-of-rl-training"
    },
    {
      "level": "h3",
      "text": ":1234:60% less memory usage",
      "id": ":1234:60%-less-memory-usage"
    },
    {
      "level": "h3",
      "text": ":question:How to use FP8 RL / installation",
      "id": ":question:how-to-use-fp8-rl-/-installation"
    },
    {
      "level": "h3",
      "text": ":cd:Implementing FP8 Training",
      "id": ":cd:implementing-fp8-training"
    },
    {
      "level": "h3",
      "text": "üî•TorchAO Collab",
      "id": "üî•torchao-collab"
    },
    {
      "level": "h3",
      "text": ":bird:On the fly TorchAO FP8 quantization",
      "id": ":bird:on-the-fly-torchao-fp8-quantization"
    },
    {
      "level": "h3",
      "text": ":tada:Unsloth FP8 uploads",
      "id": ":tada:unsloth-fp8-uploads"
    },
    {
      "level": "h3",
      "text": ":person\\_tipping\\_hand:Acknowledgements",
      "id": ":person\\_tipping\\_hand:acknowledgements"
    }
  ],
  "url": "llms-txt#fp8-reinforcement-learning",
  "links": []
}