{
  "title": "Grok 2",
  "content": "Run xAI's Grok 2 model locally!\n\nYou can now run **Grok 2** (aka Grok 2.5), the 270B parameter model by xAI. Full precision requires **539GB**, while the Unsloth Dynamic 3-bit version shrinks size down to just **118GB** (a 75% reduction). GGUF: [Grok-2-GGUF](https://huggingface.co/unsloth/grok-2-GGUF)\n\nThe **3-bit Q3\\_K\\_XL** model runs on a single **128GB Mac** or **24GB VRAM + 128GB RAM**, achieving **5+ tokens/s** inference. Thanks to the llama.cpp team and community for [supporting Grok 2](https://github.com/ggml-org/llama.cpp/pull/15539) and making this possible. We were also glad to have helped a little along the way!\n\nAll uploads use Unsloth [Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs) for SOTA 5-shot MMLU and KL Divergence performance, meaning you can run quantized Grok LLMs with minimal accuracy loss.\n\n<a href=\"#run-in-llama.cpp\" class=\"button secondary\">Run in llama.cpp Tutorial</a>\n\n## :gear: Recommended Settings\n\nThe 3-bit dynamic quant uses 118GB (126GiB) of disk space - this works well in a 128GB RAM unified memory Mac or on a 1x24GB card and 128GB of RAM. It is recommended to have at least 120GB RAM to run this 3-bit quant.\n\n{% hint style=\"warning\" %}\nYou must use `--jinja` for Grok 2. You might get incorrect results if you do not use `--jinja`\n{% endhint %}\n\nThe 8-bit quant is \\~300GB in size will fit in a 1x 80GB GPU (with MoE layers offloaded to RAM). Expect around 5 tokens/s with this setup if you have bonus 200GB RAM as well. To learn how to increase generation speed and fit longer contexts, [read here](#improving-generation-speed).\n\n{% hint style=\"info\" %}\nThough not a must, for best performance, have your VRAM + RAM combined equal to the size of the quant you're downloading. If not, hard drive / SSD offloading will work with llama.cpp, just inference will be slower.\n{% endhint %}\n\n### Sampling parameters\n\n* Grok 2 has a 128K max context length thus, use `131,072` context or less.\n* Use `--jinja` for llama.cpp variants\n\nThere are no official sampling parameters to run the model, thus you can use standard defaults for most models:\n\n* Set the <mark style=\"background-color:green;\">**temperature = 1.0**</mark>\n* <mark style=\"background-color:green;\">**Min\\_P = 0.01**</mark> (optional, but 0.01 works well, llama.cpp default is 0.1)\n\n## Run Grok 2 Tutorial:\n\nCurrently you can only run Grok 2 in llama.cpp.\n\n### ✨ Run in llama.cpp\n\n{% stepper %}\n{% step %}\nInstall the specific `llama.cpp` PR for Grok 2 on [GitHub here](https://github.com/ggml-org/llama.cpp/pull/15539). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n{% step %}\nIf you want to use `llama.cpp` directly to load models, you can do the below: (:Q3\\_K\\_XL) is the quantization type. You can also download via Hugging Face (point 3). This is similar to `ollama run` . Use `export LLAMA_CACHE=\"folder\"` to force `llama.cpp` to save to a specific location. Remember the model has only a maximum of 128K context length.\n\n{% hint style=\"info\" %}\nPlease try out `-ot \".ffn_.*_exps.=CPU\"` to offload all MoE layers to the CPU! This effectively allows you to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression to fit more layers if you have more GPU capacity.\n\nIf you have a bit more GPU memory, try `-ot \".ffn_(up|down)_exps.=CPU\"` This offloads up and down projection MoE layers.\n\nTry `-ot \".ffn_(up)_exps.=CPU\"` if you have even more GPU memory. This offloads only up projection MoE layers.\n\nAnd finally offload all layers via `-ot \".ffn_.*_exps.=CPU\"` This uses the least VRAM.\n\nYou can also customize the regex, for example `-ot \"\\.(6|7|8|9|[0-9][0-9]|[0-9][0-9][0-9])\\.ffn_(gate|up|down)_exps.=CPU\"` means to offload gate, up and down MoE layers but only from the 6th layer onwards.\n{% endhint %}\n\n{% step %}\nDownload the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose `UD-Q3_K_XL` (dynamic 3-bit quant) or other quantized versions like `Q4_K_M` . We <mark style=\"background-color:green;\">**recommend using our 2.7bit dynamic quant**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**`UD-Q2_K_XL`**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**or above to balance size and accuracy**</mark>.",
  "code_samples": [
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncd llama.cpp && git fetch origin pull/15539/head:MASTER && git checkout MASTER && cd ..\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli llama-server\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "export LLAMA_CACHE=\"unsloth/grok-2-GGUF\"\n./llama.cpp/llama-cli \\\n    -hf unsloth/grok-2-GGUF:Q3_K_XL \\\n    --jinja \\\n    --n-gpu-layers 99 \\\n    --temp 1.0 \\\n    --top-p 0.95 \\\n    --min-p 0.01 \\\n    --ctx-size 16384 \\\n    --seed 3407 \\\n    -ot \".ffn_.*_exps.=CPU\"",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": ":gear: Recommended Settings",
      "id": ":gear:-recommended-settings"
    },
    {
      "level": "h3",
      "text": "Sampling parameters",
      "id": "sampling-parameters"
    },
    {
      "level": "h2",
      "text": "Run Grok 2 Tutorial:",
      "id": "run-grok-2-tutorial:"
    },
    {
      "level": "h3",
      "text": "✨ Run in llama.cpp",
      "id": "✨-run-in-llama.cpp"
    }
  ],
  "url": "llms-txt#grok-2",
  "links": []
}