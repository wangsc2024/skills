{
  "title": "Qwen3-VL: How to Run Guide",
  "content": "Learn to fine-tune and run Qwen3-VL locally with Unsloth.\n\nQwen3-VL is Qwen‚Äôs new vision models with **instruct** and **thinking** versions. The 2B, 4B, 8B and 32B models are dense, while 30B and 235B are MoE. The 235B thinking LLM delivers SOTA vision and coding performance rivaling GPT-5 (high) and Gemini 2.5 Pro.\\\n\\\nQwen3-VL has vision, video and OCR capabilities as well as 256K context (can be extended to 1M).\\\n\\\n[Unsloth](https://github.com/unslothai/unsloth) supports **Qwen3-VL fine-tuning and** [**RL**](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/vision-reinforcement-learning-vlm-rl). Train Qwen3-VL (8B) for free with our [notebooks](#fine-tuning-qwen3-vl).\n\n<a href=\"#running-qwen3-vl\" class=\"button primary\">Running Qwen3-VL</a><a href=\"#fine-tuning-qwen3-vl\" class=\"button primary\">Fine-tuning Qwen3-VL</a>\n\n## üñ•Ô∏è **Running Qwen3-VL**\n\nTo run the model in llama.cpp, vLLM, Ollama etc., here are the recommended settings:\n\n### :gear: Recommended Settings\n\nQwen recommends these settings for both models (they're a bit different for Instruct vs Thinking):\n\n| Instruct Settings:                                                       | Thinking Settings:                                                       |\n| ------------------------------------------------------------------------ | ------------------------------------------------------------------------ |\n| <mark style=\"background-color:blue;\">**Temperature = 0.7**</mark>        | <mark style=\"background-color:blue;\">**Temperature = 1.0**</mark>        |\n| <mark style=\"background-color:yellow;\">**Top\\_P = 0.8**</mark>           | <mark style=\"background-color:yellow;\">**Top\\_P = 0.95**</mark>          |\n| <mark style=\"background-color:green;\">**presence\\_penalty = 1.5**</mark> | <mark style=\"background-color:green;\">**presence\\_penalty = 0.0**</mark> |\n| Output Length = 32768 (up to 256K)                                       | Output Length = 40960 (up to 256K)                                       |\n| Top\\_K = 20                                                              | Top\\_K = 20                                                              |\n\nQwen3-VL also used the below settings for their benchmarking numbers, as mentioned [on GitHub](https://github.com/QwenLM/Qwen3-VL/tree/main?tab=readme-ov-file#generation-hyperparameters).\n\n{% columns %}\n{% column %}\nInstruct Settings:\n\n{% column %}\nThinking Settings:\n\n{% endcolumn %}\n{% endcolumns %}\n\n### :bug:Chat template bug fixes\n\nAt Unsloth, we care about accuracy the most, so we investigated why after the 2nd turn of running the Thinking models, llama.cpp would break, as seen below:\n\n{% columns %}\n{% column %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-37356b40688b10a85c927e1d432739a15bb33682%2Fimage.webp?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n\n{% column %}\nThe error code:\n\n{% endcolumn %}\n{% endcolumns %}\n\nWe have successfully fixed the Thinking chat template for the VL models so we re-uploaded all Thinking quants and Unsloth's quants. They should now all work after the 2nd conversation - **other quants will fail to load after the 2nd conversation.**\n\n### **Qwen3-VL Unsloth uploads**:\n\nQwen3-VL is now supported for GGUFs by llama.cpp as of 30th October 2025, so you can run them locally!\n\n| Dynamic GGUFs (to run)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 4-bit BnB Unsloth Dynamic                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 16-bit full-precision                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <ul><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-GGUF\">2B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-GGUF\">2B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-GGUF\">4B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-GGUF\">4B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-GGUF\">8B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-GGUF\">8B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF\">30B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF\">30B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-GGUF\">32B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-GGUF\">32B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct-GGUF\">235B-A22B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking-GGUF\">235B-A22B-Thinking</a></li></ul> | <ul><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit\">2B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-2B-Thinking-unsloth-bnb-4bit\">2B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct-unsloth-bnb-4bit\">4B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking-unsloth-bnb-4bit\">4B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\">8B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking-unsloth-bnb-4bit\">8B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct-unsloth-bnb-4bit\">32B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking-unsloth-bnb-4bit\">32B-Thinking</a></li></ul> | <ul><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-2B-Instruct\">2B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-4B-Instruct\">4B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-4B-Thinking\">4B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-8B-Instruct\">8B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-8B-Thinking\">8B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Instruct\">30B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-30B-A3B-Thinking\">30B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-32B-Instruct\">32B-Instruct</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-32B-Thinking\">32B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Thinking\">235B-A22B-Thinking</a></li><li><a href=\"https://huggingface.co/unsloth/Qwen3-VL-235B-A22B-Instruct\">235B-A22B-Instruct</a></li></ul> |\n\n### üìñ Llama.cpp: Run Qwen3-VL Tutorial\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. **Let's first get an image!** You can also upload images as well. We shall use <https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/unsloth%20made%20with%20love.png>, which is just our mini logo showing how finetunes are made with Unsloth:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-9bf7ec93680f889d7602e5f56a8d677d6a58ae6a%2Funsloth%20made%20with%20love.png?alt=media\" alt=\"\" width=\"188\"><figcaption></figcaption></figure>\n\n3. Let's download this image\n\n{% code overflow=\"wrap\" %}\n\n4. Let's get the 2nd image at <https://files.worldwildlife.org/wwfcmsprod/images/Sloth_Sitting_iStock_3_12_2014/story_full_width/8l7pbjmj29_iStock_000011145477Large_mini__1_.jpg>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-4b30cc86b2c75edf95ee1ec6fe0c51fb30afd6c0%2F8l7pbjmj29_iStock_000011145477Large_mini__1_.jpg?alt=media\" alt=\"\" width=\"188\"><figcaption></figcaption></figure>\n\n{% code overflow=\"wrap\" %}\n\n5. Then, let's use llama.cpp's auto model downloading feature, try this for the 8B Instruct model:\n\n6. Once in, you will see the below screen:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-636dfd126430a8a8c91ef6d248b007daa34561c5%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n7. Load up the image via `/image PATH` ie `/image unsloth.png` then press ENTER\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-7525265b8ef19c7fd17cca64d1b64ffe1959c2d1%2Fimage.png?alt=media\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\n8. When you hit ENTER, it'll say \"unsloth.png image loaded\"\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-2c996efe3373ae7f05bfec4d214524768624a6a8%2Fimage.png?alt=media\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\n9. Now let's ask a question like \"What is this image?\":\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-62bd79e094c7daad6a8f021194aa0e67ef96f9a5%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n10. Now load in picture 2 via `/image picture.png` then hit ENTER and ask \"What is this image?\"\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-317cc2c7e41765ff466d357d14d506115f3262b6%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n11. And finally let's ask how are both images are related (it works!)\n\n{% code overflow=\"wrap\" %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-e323226293156ac17708836c635c6df3ab2b9ca3%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n12. You can also download the model via (after installing `pip install huggingface_hub hf_transfer` ) HuggingFace's `snapshot_download` which is useful for large model downloads, **since llama.cpp's auto downloader might lag.** You can choose Q4\\_K\\_M, or other quantized versions.",
  "code_samples": [
    {
      "code": "export greedy='false'\nexport seed=3407\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=32768",
      "language": "bash"
    },
    {
      "code": "export greedy='false'\nexport seed=1234\nexport top_p=0.95\nexport top_k=20\nexport temperature=1.0\nexport repetition_penalty=1.0\nexport presence_penalty=0.0\nexport out_seq_length=40960",
      "language": "bash"
    },
    {
      "code": "terminate called after throwing an instance of 'std::runtime_error'\n  what():  Value is not callable: null at row 63, column 78:\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = ((content.split('</think>')|first).rstrip('\\n').split('<think>')|last).lstrip('\\n') %}\n                                                                             ^",
      "language": "unknown"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "wget https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/unsloth%20made%20with%20love.png -O unsloth.png",
      "language": "bash"
    },
    {
      "code": "wget https://files.worldwildlife.org/wwfcmsprod/images/Sloth_Sitting_iStock_3_12_2014/story_full_width/8l7pbjmj29_iStock_000011145477Large_mini__1_.jpg -O picture.png",
      "language": "bash"
    },
    {
      "code": "./llama.cpp/llama-mtmd-cli \\\n    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL \\\n    --n-gpu-layers 99 \\\n    --jinja \\\n    --top-p 0.8 \\\n    --top-k 20 \\\n    --temp 0.7 \\\n    --min-p 0.0 \\\n    --flash-attn on \\\n    --presence-penalty 1.5 \\\n    --ctx-size 8192",
      "language": "bash"
    },
    {
      "code": "The two images are directly related because they both feature the **tree sloth**, which is the central subject of the \"made with unsloth\" project.\n\n- The first image is the **official logo** for the \"made with unsloth\" project. It features a stylized, cartoonish tree sloth character inside a green circle, with the text \"made with unsloth\" next to it. This is the visual identity of the project.\n- The second image is a **photograph** of a real tree sloth in its natural habitat. This photo captures the animal's physical appearance and behavior in the wild.\n\nThe relationship between the two images is that the logo (image 1) is a digital representation or symbol used to promote the \"made with unsloth\" project, while the photograph (image 2) is a real-world depiction of the actual tree sloth. The project likely uses the character from the logo as an icon or mascot, and the photograph serves to illustrate what the tree sloth looks like in its natural environment.",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "üñ•Ô∏è **Running Qwen3-VL**",
      "id": "üñ•Ô∏è-**running-qwen3-vl**"
    },
    {
      "level": "h3",
      "text": ":gear: Recommended Settings",
      "id": ":gear:-recommended-settings"
    },
    {
      "level": "h3",
      "text": ":bug:Chat template bug fixes",
      "id": ":bug:chat-template-bug-fixes"
    },
    {
      "level": "h3",
      "text": "**Qwen3-VL Unsloth uploads**:",
      "id": "**qwen3-vl-unsloth-uploads**:"
    },
    {
      "level": "h3",
      "text": "üìñ Llama.cpp: Run Qwen3-VL Tutorial",
      "id": "üìñ-llama.cpp:-run-qwen3-vl-tutorial"
    }
  ],
  "url": "llms-txt#qwen3-vl:-how-to-run-guide",
  "links": []
}