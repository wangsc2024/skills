{
  "title": "4bit pre quantized models we support for 4x faster downloading + no OOMs.",
  "content": "fourbit_models = [\n    \"unsloth/gpt-oss-20b-unsloth-bnb-4bit\", # 20B model using bitsandbytes 4bit quantization\n<strong>    \"unsloth/gpt-oss-120b-unsloth-bnb-4bit\",\n</strong>    \"unsloth/gpt-oss-20b\", # 20B model using MXFP4 format\n    \"unsloth/gpt-oss-120b\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/gpt-oss-20b\",\n    dtype = dtype, # None for auto detection\n    max_seq_length = max_seq_length, # Choose any for long context!\n    load_in_4bit = True,  # 4 bit quantization to reduce memory\n    full_finetuning = False, # [NEW!] We have full finetuning now!\n    # token = \"hf_...\", # use one if using gated models\n)\n</code></pre>\n\nYou should see output similar to the example below. Note: We explicitly change the `dtype` to `float32` to ensure correct training behavior.\n{% endstep %}\n\n#### Fine-tuning Hyperparameters (LoRA)\n\nNow it's time to adjust your training hyperparameters. For a deeper dive into how, when, and what to tune, check out our [detailed hyperparameters guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide).\n\n{% hint style=\"info\" %}\nTo avoid [overfitting](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#avoiding-overfitting-and-underfitting), monitor your training loss and avoid setting these values too high.\n{% endhint %}\n\nThis step adds LoRA adapters for parameter-efficient fine-tuning. Only about 1% of the model‚Äôs parameters are trained, which makes the process significantly more efficient.\n\n#### Data Preparation\n\nFor this example, we will use the [`HuggingFaceH4/Multilingual-Thinking`](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking). This dataset contains chain-of-thought reasoning examples derived from user questions translated from English into four additional languages.\n\nThis is the same dataset referenced in OpenAI's fine-tuning cookbook. The goal of using a multilingual dataset is to help the model learn and generalize reasoning patterns across multiple languages.\n\ngpt-oss introduces a reasoning effort system that controls how much reasoning the model performs. By default, the reasoning effort is set to `low`, but you can change it by setting the `reasoning_effort` parameter to `low`, `medium` or `high`.\n\nTo format the dataset, we apply a customized version of the gpt-oss prompt:\n\nLet's inspect the dataset by printing the first example:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-348661d8e6a1aa0efeea2b63fb71c2bb6f09109e%2Fimage.png?alt=media\" alt=\"\" width=\"563\"><figcaption></figcaption></figure>\n\nOne unique feature of gpt-oss is its use of the [**OpenAI Harmony format**](https://github.com/openai/harmony)**,** which supports structured conversations, reasoning output, and tool calling. This format includes tags such as `<|start|>` , `<|message|>` , and `<|return|>` .\n\n{% hint style=\"info\" %}\nü¶• Unsloth fixes the chat template to ensure it is correct. See this [tweet](https://x.com/danielhanchen/status/1953901104150065544) for technical details on our template fix.\n{% endhint %}\n\nFeel free to adapt the prompt and structure to suit your own dataset or use-case. For more guidance, refer to our [dataset guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/datasets-guide).\n{% endstep %}\n\nWe've pre-selected training hyperparameters for optimal results. However, you can modify them based on your specific use case. Refer to our [hyperparameters guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide).\n\nIn this example, we train for 60 steps to speed up the process. For a full training run, set `num_train_epochs=1` and disable the step limiting by setting `max_steps=None`.\n\nDuring training, monitor the loss to ensure that it is decreasing over time. This confirms that the training process is functioning correctly.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-5ace71760531cf39f14499baf9ca0f78d8018756%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n#### Inference: Run Your Trained Model\n\nNow it's time to run inference with your fine-tuned model. You can modify the instruction and input, but leave the output blank.\n\nIn this example, we test the model's ability to reason in French by adding a specific instruction to the system prompt, following the same structure used in our dataset.\n\nThis should produce an output similar to:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-31de17223d48ce57d5e178e5901e566c47adf59e%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n#### Save and Export Your Model\n\nTo save your fine-tuned model, it can be exported in the Safetensors format with our new **on-demand dequantization of MXFP4** base models (like gpt-oss) during the LoRA merge process. This makes it possible to **export your fine-tuned model in bf16 format**.\n\n{% hint style=\"success\" %}\nNew: Saving or merging QLoRA fine-tuned models to GGUF is now supported for use in other frameworks (e.g. Hugging Face, llama.cpp with GGUF).\n{% endhint %}\n\nAfter fine-tuning your gpt-oss model, you can merge it into 16-bit format with:\n\nIf you prefer to merge the model and push to the hugging-face hub directly:\n\n#### :sparkles: Saving to Llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. Convert and quantize the merged model:\n\n3. Run inference on the quantized model:\n\n{% endstep %}\n{% endstepper %}\n\nYou've fine-tuned gpt-oss with Unsloth. We're currently working on RL and GRPO implementations, as well as improved model saving and running, so stay tuned.\n\nAs always, feel free to drop by our [Discord](https://discord.com/invite/unsloth) or [Reddit](https://www.reddit.com/r/unsloth/) if you need any help.\n\n## ‚ùìFAQ (Frequently Asked Questions)\n\n#### 1. Can I export my model to use in Hugging Face, llama.cpp GGUF or vLLM later?\n\nYes you can now [save/export your gpt-oss fine-tuned](https://docs.unsloth.ai/models/long-context-gpt-oss-training#new-saving-to-gguf-vllm-after-gpt-oss-training) model using Unsloth's new update!\n\n#### 2. Can I do fp4 or MXFP4 training with gpt-oss?\n\nNo, currently no framework supports fp4 or MXFP4 training. Unsloth however is the only framework to support QLoRA 4-bit fine-tuning for the model, enabling more than 4x less VRAM use.\n\n#### 3. Can I export my model to MXFP4 format after training?\n\nNo, currently no library or framework supports this.\n\n#### 4. Can I do Reinforcement Learning (RL) or GRPO with gpt-oss?\n\nYes! Unsloth now supports RL for gpt-oss with GRPO/GSPO. We made it work on a free Kaggle notebook and achieved the fastest inference for RL. [Read more here](https://docs.unsloth.ai/models/gpt-oss-how-to-run-and-fine-tune/gpt-oss-reinforcement-learning)\n\n***Acknowledgements:** A huge thank you to* [*Eyera*](https://huggingface.co/Orenguteng) *for contributing to this guide!*",
  "code_samples": [
    {
      "code": "model = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)",
      "language": "python"
    },
    {
      "code": "def formatting_prompts_func(examples):\n    convos = examples[\"messages\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\npass\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\ndataset",
      "language": "python"
    },
    {
      "code": "tokenizer.apply_chat_template(\n    text, \n    tokenize = False, \n    add_generation_prompt = False,\n    reasoning_effort = \"medium\",\n)",
      "language": "python"
    },
    {
      "code": "from unsloth.chat_templates import standardize_sharegpt\ndataset = standardize_sharegpt(dataset)\ndataset = dataset.map(formatting_prompts_func, batched = True,)",
      "language": "python"
    },
    {
      "code": "<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-348661d8e6a1aa0efeea2b63fb71c2bb6f09109e%2Fimage.png?alt=media\" alt=\"\" width=\"563\"><figcaption></figcaption></figure>\n\nOne unique feature of gpt-oss is its use of the [**OpenAI Harmony format**](https://github.com/openai/harmony)**,** which supports structured conversations, reasoning output, and tool calling. This format includes tags such as `<|start|>` , `<|message|>` , and `<|return|>` .\n\n{% hint style=\"info\" %}\nü¶• Unsloth fixes the chat template to ensure it is correct. See this [tweet](https://x.com/danielhanchen/status/1953901104150065544) for technical details on our template fix.\n{% endhint %}\n\nFeel free to adapt the prompt and structure to suit your own dataset or use-case. For more guidance, refer to our [dataset guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/datasets-guide).\n{% endstep %}\n\n{% step %}\n\n#### Train the model\n\nWe've pre-selected training hyperparameters for optimal results. However, you can modify them based on your specific use case. Refer to our [hyperparameters guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide).\n\nIn this example, we train for 60 steps to speed up the process. For a full training run, set `num_train_epochs=1` and disable the step limiting by setting `max_steps=None`.",
      "language": "unknown"
    },
    {
      "code": "During training, monitor the loss to ensure that it is decreasing over time. This confirms that the training process is functioning correctly.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-5ace71760531cf39f14499baf9ca0f78d8018756%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n{% step %}\n\n#### Inference: Run Your Trained Model\n\nNow it's time to run inference with your fine-tuned model. You can modify the instruction and input, but leave the output blank.\n\nIn this example, we test the model's ability to reason in French by adding a specific instruction to the system prompt, following the same structure used in our dataset.",
      "language": "unknown"
    },
    {
      "code": "This should produce an output similar to:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-31de17223d48ce57d5e178e5901e566c47adf59e%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n{% step %}\n\n#### Save and Export Your Model\n\nTo save your fine-tuned model, it can be exported in the Safetensors format with our new **on-demand dequantization of MXFP4** base models (like gpt-oss) during the LoRA merge process. This makes it possible to **export your fine-tuned model in bf16 format**.\n\n{% hint style=\"success\" %}\nNew: Saving or merging QLoRA fine-tuned models to GGUF is now supported for use in other frameworks (e.g. Hugging Face, llama.cpp with GGUF).\n{% endhint %}\n\nAfter fine-tuning your gpt-oss model, you can merge it into 16-bit format with:",
      "language": "unknown"
    },
    {
      "code": "If you prefer to merge the model and push to the hugging-face hub directly:",
      "language": "unknown"
    },
    {
      "code": "#### :sparkles: Saving to Llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.",
      "language": "unknown"
    },
    {
      "code": "2. Convert and quantize the merged model:",
      "language": "unknown"
    },
    {
      "code": "3. Run inference on the quantized model:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "üèÅ And that's it!",
      "id": "üèÅ-and-that's-it!"
    },
    {
      "level": "h2",
      "text": "‚ùìFAQ (Frequently Asked Questions)",
      "id": "‚ùìfaq-(frequently-asked-questions)"
    }
  ],
  "url": "llms-txt#4bit-pre-quantized-models-we-support-for-4x-faster-downloading-+-no-ooms.",
  "links": []
}