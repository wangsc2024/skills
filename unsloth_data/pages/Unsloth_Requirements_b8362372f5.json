{
  "title": "Unsloth Requirements",
  "content": "Here are Unsloth's requirements including system and GPU VRAM requirements.\n\n## System Requirements\n\n* **Operating System**: Works on Linux and [Windows](https://docs.unsloth.ai/get-started/install-and-update/windows-installation)\n* Supports NVIDIA GPUs since 2018+ including [Blackwell RTX 50](https://docs.unsloth.ai/basics/fine-tuning-llms-with-blackwell-rtx-50-series-and-unsloth) and [DGX Spark](https://docs.unsloth.ai/basics/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth)\n  * [fine-tuning-llms-with-blackwell-rtx-50-series-and-unsloth](https://docs.unsloth.ai/basics/fine-tuning-llms-with-blackwell-rtx-50-series-and-unsloth \"mention\")\n  * [fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth](https://docs.unsloth.ai/basics/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth \"mention\")\n* Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20 & 50, A100, H100, L40 etc) [Check your GPU!](https://developer.nvidia.com/cuda-gpus) GTX 1070, 1080 works, but is slow.\n* The official [Unsloth Docker image](https://hub.docker.com/r/unsloth/unsloth) `unsloth/unsloth` is available on Docker Hub\n  * [how-to-run-llms-with-docker](https://docs.unsloth.ai/models/how-to-run-llms-with-docker \"mention\")\n* Unsloth works on [AMD](https://docs.unsloth.ai/new/fine-tuning-llms-on-amd-gpus-with-unsloth) and [Intel](https://github.com/unslothai/unsloth/pull/2621) GPUs! Apple/Silicon/MLX is in the works\n* If you have different versions of torch, transformers etc., `pip install unsloth` will automatically install all the latest versions of those libraries so you don't need to worry about version compatibility.\n* Your device should have `xformers`, `torch`, `BitsandBytes` and `triton` support.\n\n{% hint style=\"info\" %}\nPython 3.13 is now supported!\n{% endhint %}\n\n## Fine-tuning VRAM requirements:\n\nHow much GPU memory do I need for LLM fine-tuning using Unsloth?\n\n{% hint style=\"info\" %}\nA common issue when you OOM or run out of memory is because you set your batch size too high. Set it to 1, 2, or 3 to use less VRAM.\n\n**For context length benchmarks, see** [**here**](https://docs.unsloth.ai/basics/unsloth-benchmarks#context-length-benchmarks)**.**\n{% endhint %}\n\nCheck this table for VRAM requirements sorted by model parameters and fine-tuning method. QLoRA uses 4-bit, LoRA uses 16-bit. Keep in mind that sometimes more VRAM is required depending on the model so these numbers are the absolute minimum:\n\n| Model parameters | QLoRA (4-bit) VRAM | LoRA (16-bit) VRAM |\n| ---------------- | ------------------ | ------------------ |\n| 3B               | 3.5 GB             | 8 GB               |\n| 7B               | 5 GB               | 19 GB              |\n| 8B               | 6 GB               | 22 GB              |\n| 9B               | 6.5 GB             | 24 GB              |\n| 11B              | 7.5 GB             | 29 GB              |\n| 14B              | 8.5 GB             | 33 GB              |\n| 27B              | 22GB               | 64GB               |\n| 32B              | 26 GB              | 76 GB              |\n| 40B              | 30GB               | 96GB               |\n| 70B              | 41 GB              | 164 GB             |\n| 81B              | 48GB               | 192GB              |\n| 90B              | 53GB               | 212GB              |\n| 405B             | 237 GB             | 950 GB             |",
  "code_samples": [],
  "headings": [
    {
      "level": "h2",
      "text": "System Requirements",
      "id": "system-requirements"
    },
    {
      "level": "h2",
      "text": "Fine-tuning VRAM requirements:",
      "id": "fine-tuning-vram-requirements:"
    }
  ],
  "url": "llms-txt#unsloth-requirements",
  "links": []
}