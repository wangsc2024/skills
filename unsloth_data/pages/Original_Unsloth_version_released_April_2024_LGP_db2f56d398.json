{
  "title": "Original Unsloth version released April 2024 - LGPLv3 Licensed",
  "content": "class Unsloth_Offloaded_Gradient_Checkpointer(torch.autograd.Function):\n    @staticmethod\n    @torch_amp_custom_fwd\n    def forward(ctx, forward_function, hidden_states, *args):\n        ctx.device = hidden_states.device\n        saved_hidden_states = hidden_states.to(\"cpu\", non_blocking = True)\n        with torch.no_grad():\n            output = forward_function(hidden_states, *args)\n        ctx.save_for_backward(saved_hidden_states)\n        ctx.forward_function, ctx.args = forward_function, args\n        return output\n\n@staticmethod\n    @torch_amp_custom_bwd\n    def backward(ctx, dY):\n        (hidden_states,) = ctx.saved_tensors\n        hidden_states = hidden_states.to(ctx.device, non_blocking = True).detach()\n        hidden_states.requires_grad_(True)\n        with torch.enable_grad():\n            (output,) = ctx.forward_function(hidden_states, *ctx.args)\n        torch.autograd.backward(output, dY)\n        return (None, hidden_states.grad,) + (None,)*len(ctx.args)\npy\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    ...,\n    unsloth_tiled_mlp = True,\n)\n```\n\nJust set `unsloth_tiled_mlp = True` in `from_pretrained` and Tiled MLP is enabled. We follow the same logic as the Arctic paper and choose `num_shards = ceil(seq_len/hidden_size)`. Each tile will operate on sequence lengths which are the same size of the hidden dimension of the model to balance throughput and memory savings.\n\nWe also discussed how Tiled MLP effectively does 3 forward passes and 1 backward, compared to normal gradient checkpointing which does 2 forward passes and 1 backward with Stas Bekman and [DeepSpeed](https://github.com/deepspeedai/DeepSpeed/pull/7664) provided a doc update for Tiled MLP within DeepSpeed.\n\n{% hint style=\"success\" %}\nNext time fine-tuning runs out of memory, try turning on `unsloth_tiled_mlp = True`. This should save some VRAM as long as the context length is longer than the LLM's hidden dimension.\n{% endhint %}\n\n**With our latest update, it is possible to now reach 1M context length with a smaller model on a single GPU!**\n\n**Try 500K-context gpt-oss-20b fine-tuning on our** [**80GB A100 Colab notebook**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt_oss_\\(20B\\)_500K_Context_Fine_tuning.ipynb)**.**\n\nIf you've made it this far, we're releasing a new blog on our latest improvements in training speed this week so stay tuned by joining our [Reddit r/unsloth](https://www.reddit.com/r/unsloth/) or our Docs.",
  "code_samples": [
    {
      "code": "{% endcode %}\n\nBy offloading activations as soon as they are produced, we minimize peak activation footprint and free GPU memory exactly when itâ€™s needed. This sharply reduces memory pressure in long-context or large-batch training, where a single decoder layerâ€™s activations can exceed 2 GB.\n\n> **Thus, Unslothâ€™s new algorithms & Gradient Checkpointing contributes to most improvements (3.2x), enabling 290k-context-length QLoRA GPT-OSS fine-tuning on a single H100.**\n\n### ðŸ”“ Tiled MLP: Unlocking 500K+\n\nWith help from [Stas Bekman](https://x.com/StasBekman) (Snowflake), we integrated Tiled MLP from Snowflakeâ€™s Arctic Long Sequence Training [paper](https://arxiv.org/abs/2506.13996) and blog post. TiledMLP reduces activation memory and enables much longer sequence lengths by tiling hidden states along the sequence dimension before heavy MLP projections.\n\n**We also introduce a few quality-of-life improvements:**\n\nWe preserve RNG state across tiled forward recomputations so dropout and other stochastic ops are consistent between forward and backward replays. This keeps nested checkpointed computations stable and numerically identical.\n\n{% hint style=\"success\" %}\nOur implementation auto patches any module named or typed as `mlp`, so **nearly all models with MLP modules are supported out of the box for Tiled MLP.**\n{% endhint %}\n\n**Tradeoffs to keep in mind**\n\nTiledMLP saves VRAM at the cost of extra forward passes. Because it lives inside a checkpointed transformer block and is itself written in a checkpoint style, it effectively becomes a nested checkpoint: one **MLP now performs \\~3 forward passes and 1 backward pass per step**. In return, we can drop almost all intermediate MLP activations from VRAM while still supporting extremely long sequences.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FdeOJEEqucGYtbXbb7nqB%2Fbaseline_vs_unsloth_spike.png?alt=media&#x26;token=3b1cdfd3-dd24-4c94-b7ec-5d1366464afb\" alt=\"\"><figcaption></figcaption></figure>\n\nThe plots compare active memory timelines for a single decoder layerâ€™s forward and backward during a long-context training step, without Tiled MLP (left) and with it (right). Without Tiled MLP, peak VRAM occurs during the MLP backward; with Tiled MLP, it shifts to the fused loss calculation. We see \\~40% lower VRAM usage, and because the fused loss auto chunks dynamically based on available VRAM, the peak with Tiled MLP would be even smaller on smaller GPUs.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2FUCx0X7S5FvaD3hUsma5j%2Fbaseline_vs_unsloth_nospike.png?alt=media&#x26;token=a81b8639-21d0-43aa-a837-8209949e8742\" alt=\"\"><figcaption></figcaption></figure>\n\nTo show cross-entropy loss is not the new bottleneck, we fix its chunk size instead of choosing it dynamically and then double the number of chunks. This significantly reduces the loss-related memory spikes. The max memory now occurs during backward in both cases, and overall timing is similar, though Tiled MLP adds a small overhead: one large GEMM becomes many sequential matmuls, plus the extra forward pass mentioned above.\n\nOverall, the trade-off is worth it: without Tiled MLP, long-context training can require roughly 2Ã— the memory usage, while with **Tiled MLP a single GPU pays only about a 1.3Ã— increase in step time for the same context length.**\n\n**Enabling Tiled MLP in Unsloth:**",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "ðŸ”“ Tiled MLP: Unlocking 500K+",
      "id": "ðŸ”“-tiled-mlp:-unlocking-500k+"
    }
  ],
  "url": "llms-txt#original-unsloth-version-released-april-2024---lgplv3-licensed",
  "links": []
}