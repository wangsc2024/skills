{
  "title": "vLLM Deployment & Inference Guide",
  "content": "Guide on saving and deploying LLMs to vLLM for serving LLMs in production\n\n### :computer:Installing vLLM\n\nFor NVIDIA GPUs, use uv and run:\n\nFor AMD GPUs, please use the nightly Docker image: `rocm/vllm-dev:nightly`\n\nFor the nightly branch for NVIDIA GPUs, run:\n\n{% code overflow=\"wrap\" %}\n\nSee [vLLM docs](https://docs.vllm.ai/en/stable/getting_started/installation) for more details\n\n### :truck:Deploying vLLM models\n\nAfter saving your fine-tune, you can simply do:\n\n### :fire\\_engine:vLLM Deployment Server Flags, Engine Arguments & Options\n\nSome important server flags to use are at [#vllm-deployment-server-flags-engine-arguments-and-options](#vllm-deployment-server-flags-engine-arguments-and-options \"mention\")\n\n### ðŸ¦¥Deploying Unsloth finetunes in vLLM\n\nAfter fine-tuning [fine-tuning-llms-guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide \"mention\") or using our notebooks at [unsloth-notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks \"mention\"), you can save or deploy your models directly through vLLM within a single workflow. An example Unsloth finetuning script for eg:\n\n**To save to 16-bit for vLLM, use:**\n\n{% code overflow=\"wrap\" %}\n\n**To save just the LoRA adapters**, either use:\n\nOr just use our builtin function to do that:\n\n{% code overflow=\"wrap\" %}\n\nTo merge to 4bit to load on HuggingFace, first call `merged_4bit`. Then use `merged_4bit_forced` if you are certain you want to merge to 4bit. I highly discourage you, unless you know what you are going to do with the 4bit model (ie for DPO training for eg or for HuggingFace's online inference engine)\n\n{% code overflow=\"wrap\" %}\n\nThen to load the finetuned model in vLLM in another terminal:\n\nYou might have to provide the full path if the above doesn't work ie:\n\n### [vllm-engine-arguments](https://docs.unsloth.ai/basics/inference-and-deployment/vllm-guide/vllm-engine-arguments \"mention\")\n\n### [lora-hot-swapping-guide](https://docs.unsloth.ai/basics/inference-and-deployment/vllm-guide/lora-hot-swapping-guide \"mention\")",
  "code_samples": [
    {
      "code": "pip install --upgrade pip\npip install uv\nuv pip install -U vllm --torch-backend=auto",
      "language": "bash"
    },
    {
      "code": "pip install --upgrade pip\npip install uv\nuv pip install -U vllm --torch-backend=auto --extra-index-url https://wheels.vllm.ai/nightly",
      "language": "bash"
    },
    {
      "code": "vllm serve unsloth/gpt-oss-120b",
      "language": "bash"
    },
    {
      "code": "from unsloth import FastLanguageModel\nimport torch\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/gpt-oss-20b\",\n    max_seq_length = 2048,\n    load_in_4bit = True,\n)\nmodel = FastLanguageModel.get_peft_model(model)",
      "language": "python"
    },
    {
      "code": "model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"merged_16bit\")\n## OR to upload to HuggingFace:\nmodel.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")",
      "language": "python"
    },
    {
      "code": "model.save_pretrained(\"finetuned_lora\")\ntokenizer.save_pretrained(\"finetuned_lora\")",
      "language": "python"
    },
    {
      "code": "model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"lora\")\n## OR to upload to HuggingFace\nmodel.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")",
      "language": "python"
    },
    {
      "code": "model.save_pretrained_merged(\"finetuned_model\", tokenizer, save_method = \"merged_4bit\")\n## To upload to HuggingFace:\nmodel.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")",
      "language": "python"
    },
    {
      "code": "vllm serve finetuned_model",
      "language": "bash"
    },
    {
      "code": "vllm serve /mnt/disks/daniel/finetuned_model",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": ":computer:Installing vLLM",
      "id": ":computer:installing-vllm"
    },
    {
      "level": "h3",
      "text": ":truck:Deploying vLLM models",
      "id": ":truck:deploying-vllm-models"
    },
    {
      "level": "h3",
      "text": ":fire\\_engine:vLLM Deployment Server Flags, Engine Arguments & Options",
      "id": ":fire\\_engine:vllm-deployment-server-flags,-engine-arguments-&-options"
    },
    {
      "level": "h3",
      "text": "ðŸ¦¥Deploying Unsloth finetunes in vLLM",
      "id": "ðŸ¦¥deploying-unsloth-finetunes-in-vllm"
    },
    {
      "level": "h2",
      "text": "OR to upload to HuggingFace:",
      "id": "or-to-upload-to-huggingface:"
    },
    {
      "level": "h2",
      "text": "OR to upload to HuggingFace",
      "id": "or-to-upload-to-huggingface"
    },
    {
      "level": "h2",
      "text": "To upload to HuggingFace:",
      "id": "to-upload-to-huggingface:"
    },
    {
      "level": "h3",
      "text": "[vllm-engine-arguments](https://docs.unsloth.ai/basics/inference-and-deployment/vllm-guide/vllm-engine-arguments \"mention\")",
      "id": "[vllm-engine-arguments](https://docs.unsloth.ai/basics/inference-and-deployment/vllm-guide/vllm-engine-arguments-\"mention\")"
    },
    {
      "level": "h3",
      "text": "[lora-hot-swapping-guide](https://docs.unsloth.ai/basics/inference-and-deployment/vllm-guide/lora-hot-swapping-guide \"mention\")",
      "id": "[lora-hot-swapping-guide](https://docs.unsloth.ai/basics/inference-and-deployment/vllm-guide/lora-hot-swapping-guide-\"mention\")"
    }
  ],
  "url": "llms-txt#vllm-deployment-&-inference-guide",
  "links": []
}