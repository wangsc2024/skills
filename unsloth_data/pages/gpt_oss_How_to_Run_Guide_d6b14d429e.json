{
  "title": "gpt-oss: How to Run Guide",
  "content": "Run & fine-tune OpenAI's new open-source models!\n\nOpenAI releases '**gpt-oss-120b'** and '**gpt-oss-20b'**, two SOTA open language models under the Apache 2.0 license. Both 128k context models outperform similarly sized open models in reasoning, tool use, and agentic tasks. You can now run & fine-tune them locally with Unsloth!\n\n<a href=\"#run-gpt-oss-20b\" class=\"button secondary\">Run gpt-oss-20b</a><a href=\"#run-gpt-oss-120b\" class=\"button secondary\">Run gpt-oss-120b</a><a href=\"#fine-tuning-gpt-oss-with-unsloth\" class=\"button primary\">Fine-tune gpt-oss</a>\n\n{% hint style=\"success\" %}\n[**Aug 28 update**](https://docs.unsloth.ai/models/long-context-gpt-oss-training#new-saving-to-gguf-vllm-after-gpt-oss-training)**:** You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, HF etc.\n\nWe also introduced [Unsloth Flex Attention](https://docs.unsloth.ai/models/long-context-gpt-oss-training#introducing-unsloth-flex-attention-support) which enables **>8√ó longer context lengths**, **>50% less VRAM usage** and **>1.5√ó faster training** vs. all implementations. [Read more here](https://docs.unsloth.ai/models/long-context-gpt-oss-training#introducing-unsloth-flex-attention-support)\n{% endhint %}\n\n> [**Fine-tune**](#fine-tuning-gpt-oss-with-unsloth) **gpt-oss-20b for free with our** [**Colab notebook**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-\\(20B\\)-Fine-tuning.ipynb)\n\nTrained with [RL](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide), **gpt-oss-120b** rivals o4-mini and **gpt-oss-20b** rivals o3-mini. Both excel at function calling and CoT reasoning, surpassing o1 and GPT-4o.\n\n#### **gpt-oss - Unsloth GGUFs:**\n\n{% hint style=\"success\" %}\n**Includes Unsloth's** [**chat template fixes**](#unsloth-fixes-for-gpt-oss)**. For best results, use our uploads & train with Unsloth!**\n{% endhint %}\n\n* 20B: [gpt-oss-**20B**](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)\n* 120B: [gpt-oss-**120B**](https://huggingface.co/unsloth/gpt-oss-120b-GGUF)\n\n## :scroll:Unsloth fixes for gpt-oss\n\n{% hint style=\"info\" %}\nSome of our fixes were pushed upstream to OpenAI's official model on Hugging Face. [See](https://huggingface.co/openai/gpt-oss-20b/discussions/94/files)\n{% endhint %}\n\nOpenAI released a standalone parsing and tokenization library called [Harmony](https://github.com/openai/harmony) which allows one to tokenize conversations to OpenAI's preferred format for gpt-oss.\n\nInference engines generally use the jinja chat template instead and not the Harmony package, and we found some issues with them after comparing with Harmony directly. If you see below, the top is the correct rendered form as from Harmony. The below is the one rendered by the current jinja chat template. There are quite a few differences!\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-9b377044965ac55a125d6c703ec1c50555157266%2FScreenshot%202025-08-08%20at%2008-19-49%20Untitled151.ipynb%20-%20Colab.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nWe also made some functions to directly allow you to use OpenAI's Harmony library directly without a jinja chat template if you desire - you can simply parse in normal conversations like below:\n\nThen use the `encode_conversations_with_harmony` function from Unsloth:\n\nThe harmony format includes multiple interesting things:\n\n1. `reasoning_effort = \"medium\"` You can select low, medium or high, and this changes gpt-oss's reasoning budget - generally the higher the better the accuracy of the model.\n2. `developer_instructions` is like a system prompt which you can add.\n3. `model_identity` is best left alone - you can edit it, but we're unsure if custom ones will function.\n\nWe find multiple issues with current jinja chat templates (there exists multiple implementations across the ecosystem):\n\n1. Function and tool calls are rendered with `tojson`, which is fine it's a dict, but if it's a string, speech marks and other **symbols become backslashed**.\n2. There are some **extra new lines** in the jinja template on some boundaries.\n3. Tool calling thoughts from the model should have the **`analysis` tag and not `final` tag**.\n4. Other chat templates seem to not utilize `<|channel|>final` at all - one should use this for the final assistant message. You should not use this for thinking traces or tool calls.\n\nOur chat templates for the GGUF, our BnB and BF16 uploads and all versions are fixed! For example when comparing both ours and Harmony's format, we get no different characters:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-4c42f3d83194ea2fbe436670a550e1b6f148f4cd%2FScreenshot%202025-08-08%20at%2008-20-00%20Untitled151.ipynb%20-%20Colab.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n### :1234: Precision issues\n\nWe found multiple precision issues in Tesla T4 and float16 machines primarily since the model was trained using BF16, and so outliers and overflows existed. MXFP4 is not actually supported on Ampere and older GPUs, so Triton provides `tl.dot_scaled` for MXFP4 matrix multiplication. It upcasts the matrices to BF16 internaly on the fly.\n\nWe made a [MXFP4 inference notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_\\(20B\\)-Inference.ipynb) as well in Tesla T4 Colab!\n\n{% hint style=\"info\" %}\n[Software emulation](https://triton-lang.org/main/python-api/generated/triton.language.dot_scaled.html) enables targeting hardware architectures without native microscaling operation support. Right now for such case, microscaled lhs/rhs are upcasted to `bf16` element type beforehand for dot computation,\n{% endhint %}\n\nWe found if you use float16 as the mixed precision autocast data-type, you will get infinities after some time. To counteract this, we found doing the MoE in bfloat16, then leaving it in either bfloat16 or float32 precision. If older GPUs don't even have bfloat16 support (like T4), then float32 is used.\n\nWe also change all precisions of operations (like the router) to float32 for float16 machines.\n\n## üñ•Ô∏è **Running gpt-oss**\n\nBelow are guides for the [20B](#run-gpt-oss-20b) and [120B](#run-gpt-oss-120b) variants of the model.\n\n{% hint style=\"info\" %}\nAny quant smaller than F16, including 2-bit has minimal accuracy loss, since only some parts (e.g., attention layers) are lower bit while most remain full-precision. That‚Äôs why sizes are close to the F16 model; for example, the 2-bit (11.5 GB) version performs nearly the same as the full 16-bit (14 GB) one. Once llama.cpp supports better quantization for these models, we'll upload them ASAP.\n{% endhint %}\n\nThe `gpt-oss` models from OpenAI include a feature that allows users to adjust the model's \"reasoning effort.\" This gives you control over the trade-off between the model's performance and its response speed (latency) which by the amount of token the model will use to think.\n\nThe `gpt-oss` models offer three distinct levels of reasoning effort you can choose from:\n\n* **Low**: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.\n* **Medium**: A balance between performance and speed.\n* **High**: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency.\n\n### :gear: Recommended Settings\n\nOpenAI recommends these inference settings for both models:\n\n`temperature=1.0`, `top_p=1.0`, `top_k=0`\n\n* <mark style=\"background-color:green;\">**Temperature of 1.0**</mark>\n* Top\\_K = 0 (or experiment with 100 for possible better results)\n* Top\\_P = 1.0\n* Recommended minimum context: 16,384\n* Maximum context length window: 131,072\n\nThe end of sentence/generation token: EOS is `<|return|>`\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-920b641670a166258845bbe8152999983b1e68af%2Fgpt-oss-20b.svg?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nTo achieve inference speeds of 6+ tokens per second for our Dynamic 4-bit quant, have at least **14GB of unified memory** (combined VRAM and RAM) or **14GB of system RAM** alone. As a rule of thumb, your available memory should match or exceed the size of the model you‚Äôre using. GGUF Link: [unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)\n\n**NOTE:** The model can run on less memory than its total size, but this will slow down inference. Maximum memory is only needed for the fastest speeds.\n\n{% hint style=\"info\" %}\nFollow the [**best practices above**](#recommended-settings). They're the same as the 120B model.\n{% endhint %}\n\nYou can run the model on Google Colab, Docker, LM Studio or llama.cpp for now. See below:\n\n> **You can run gpt-oss-20b for free with our** [**Google Colab notebook**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_\\(20B\\)-Inference.ipynb)\n\n#### üêã Docker: Run gpt-oss-20b Tutorial\n\nIf you already have Docker desktop, all you need to do is run the command below and you're done:\n\n#### :sparkles: Llama.cpp: Run gpt-oss-20b Tutorial\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. You can directly pull from Hugging Face via:\n\n3. Download the model via (after installing `pip install huggingface_hub hf_transfer` ).",
  "code_samples": [
    {
      "code": "messages = [\n    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n    {\"role\" : \"assistant\", \"content\" : \"2\"},\n    {\"role\": \"user\",  \"content\": \"What's the temperature in San Francisco now? How about tomorrow? Today's date is 2024-09-30.\"},\n    {\"role\": \"assistant\",  \"content\": \"User asks: 'What is the weather in San Francisco?' We need to use get_current_temperature tool.\", \"thinking\" : \"\"},\n    {\"role\": \"assistant\", \"content\": \"\", \"tool_calls\": [{\"name\": \"get_current_temperature\", \"arguments\": '{\"location\": \"San Francisco, California, United States\", \"unit\": \"celsius\"}'}]},\n    {\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": '{\"temperature\": 19.9, \"location\": \"San Francisco, California, United States\", \"unit\": \"celsius\"}'},\n]",
      "language": "python"
    },
    {
      "code": "from unsloth_zoo import encode_conversations_with_harmony\n\ndef encode_conversations_with_harmony(\n    messages,\n    reasoning_effort = \"medium\",\n    add_generation_prompt = True,\n    tool_calls = None,\n    developer_instructions = None,\n    model_identity = \"You are ChatGPT, a large language model trained by OpenAI.\",\n)",
      "language": "python"
    },
    {
      "code": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-05\\n\\nReasoning: medium\\n\\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there!<|end|><|start|>user<|message|>What is 1+1?<|end|><|start|>assistant",
      "language": "unknown"
    },
    {
      "code": "docker model run hf.co/unsloth/gpt-oss-20b-GGUF:F16",
      "language": "bash"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "./llama.cpp/llama-cli \\\n       -hf unsloth/gpt-oss-20b-GGUF:F16 \\\n       --jinja -ngl 99 --threads -1 --ctx-size 16384 \\\n       --temp 1.0 --top-p 1.0 --top-k 0",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": ":scroll:Unsloth fixes for gpt-oss",
      "id": ":scroll:unsloth-fixes-for-gpt-oss"
    },
    {
      "level": "h3",
      "text": ":1234: Precision issues",
      "id": ":1234:-precision-issues"
    },
    {
      "level": "h2",
      "text": "üñ•Ô∏è **Running gpt-oss**",
      "id": "üñ•Ô∏è-**running-gpt-oss**"
    },
    {
      "level": "h3",
      "text": ":gear: Recommended Settings",
      "id": ":gear:-recommended-settings"
    },
    {
      "level": "h3",
      "text": "Run gpt-oss-20B",
      "id": "run-gpt-oss-20b"
    }
  ],
  "url": "llms-txt#gpt-oss:-how-to-run-guide",
  "links": []
}