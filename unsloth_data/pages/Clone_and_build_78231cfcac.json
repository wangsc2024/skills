{
  "title": "Clone and build",
  "content": "pip install ninja\nexport TORCH_CUDA_ARCH_LIST=\"12.0\"\ngit clone --depth=1 https://github.com/facebookresearch/xformers --recursive\ncd xformers && python setup.py install && cd ..\nbash\nuv pip install unsloth\nbash\n   curl -LsSf https://astral.sh/uv/install.sh | sh && source $HOME/.local/bin/env\n   bash\n   mkdir 'unsloth-blackwell' && cd 'unsloth-blackwell'\n   uv venv .venv --python=3.12 --seed\n   source .venv/bin/activate\n   bash\n   uv pip install -U vllm --torch-backend=cu128\n   bash\n   uv pip install unsloth unsloth_zoo bitsandbytes\n   bash\n   uv pip install -qqq \\\n   \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n   \"unsloth[base] @ git+https://github.com/unslothai/unsloth\"\n   bash\n   # First uninstall xformers installed by previous libraries\n   pip uninstall xformers -y\n\n# Clone and build\n   pip install ninja\n   export TORCH_CUDA_ARCH_LIST=\"12.0\"\n   git clone --depth=1 https://github.com/facebookresearch/xformers --recursive\n   cd xformers && python setup.py install && cd ..\n   bash\n   uv pip install -U transformers\n   bash\n   curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\n   bash\n   bash Miniforge3-$(uname)-$(uname -m).sh\n   bash\n   conda create --name unsloth-blackwell python==3.12 -y\n   bash\n   conda activate unsloth-blackwell\n   bash\n   pip install -U vllm --extra-index-url https://download.pytorch.org/whl/cu128\n   bash\n   pip install unsloth unsloth_zoo bitsandbytes\n   bash\n   # First uninstall xformers installed by previous libraries\n   pip uninstall xformers -y\n\n# Clone and build\n   pip install ninja\n   export TORCH_CUDA_ARCH_LIST=\"12.0\"\n   git clone --depth=1 https://github.com/facebookresearch/xformers --recursive\n   cd xformers && python setup.py install && cd ..\n   bash\n   pip install -U triton>=3.3.1\n   bash\n   uv pip install -U transformers\n   bash\n   # Create or edit .wslconfig in your Windows user directory\n   # (typically C:\\Users\\YourUsername\\.wslconfig)\n\n# Add these lines to the file\n   [wsl2]\n   memory=16GB  # Minimum 16GB recommended for xformers compilation\n   processors=4  # Adjust based on your CPU cores\n   swap=2GB\n   localhostForwarding=true\n   powershell\n   wsl --shutdown\n   bash\n   # Set CUDA architecture for Blackwell GPUs\n   export TORCH_CUDA_ARCH_LIST=\"12.0\"\n\n# Install xformers from source with optimized build flags\n   pip install -v --no-build-isolation -U git+https://github.com/facebookresearch/xformers.git@main#egg=xformers\n   ```\n\nThe `--no-build-isolation` flag helps avoid potential build issues in WSL environments.",
  "code_samples": [
    {
      "code": "{% endcode %}\n\n### Docker\n\n[**`unsloth/unsloth`**](https://hub.docker.com/r/unsloth/unsloth) is Unsloth's only Docker image. For Blackwell and 50-series GPUs, use this same image - no separate image needed.\n\nFor installation instructions, please follow our [Unsloth Docker guide](https://docs.unsloth.ai/new/how-to-fine-tune-llms-with-unsloth-and-docker).\n\n### uv",
      "language": "unknown"
    },
    {
      "code": "#### uv (Advanced)\n\nThe installation order is important, since we want the overwrite bundled dependencies with specific versions (namely, `xformers` and `triton`).\n\n1. I prefer to use `uv` over `pip` as it's faster and better for resolving dependencies, especially for libraries which depend on `torch` but for which a specific `CUDA` version is required per this scenario.\n\n   Install `uv`",
      "language": "unknown"
    },
    {
      "code": "Create a project dir and venv:",
      "language": "unknown"
    },
    {
      "code": "2. Install `vllm`",
      "language": "unknown"
    },
    {
      "code": "Note that we have to specify `cu128`, otherwise `vllm` will install `torch==2.7.0` but with `cu126`.\n3. Install `unsloth` dependencies",
      "language": "unknown"
    },
    {
      "code": "If you notice weird resolving issues due to Xformers, you can also install Unsloth from source without Xformers:",
      "language": "unknown"
    },
    {
      "code": "4. Download and build `xformers` (Optional)\n\n   Xformers is optional, but it is definitely faster and uses less memory. We'll use PyTorch's native SDPA if you do not want Xformers. Building Xformers from source might be slow, so beware!",
      "language": "unknown"
    },
    {
      "code": "Note that we have to explicitly set `TORCH_CUDA_ARCH_LIST=12.0`.\n5. `transformers` Install any transformers version, but best to get the latest.",
      "language": "unknown"
    },
    {
      "code": "### Conda or mamba (Advanced)\n\n1. Install `conda/mamba`",
      "language": "unknown"
    },
    {
      "code": "Run the installation script",
      "language": "unknown"
    },
    {
      "code": "Create a conda or mamba environment",
      "language": "unknown"
    },
    {
      "code": "Activate newly created environment",
      "language": "unknown"
    },
    {
      "code": "2. Install `vllm`\n\n   Make sure you are inside the activated conda/mamba environment. You should see the name of your environment as a prefix to your terminal shell like this your `(unsloth-blackwell)user@machine:`",
      "language": "unknown"
    },
    {
      "code": "Note that we have to specify `cu128`, otherwise `vllm` will install `torch==2.7.0` but with `cu126`.\n3. Install `unsloth` dependencies\n\n   Make sure you are inside the activated conda/mamba environment. You should see the name of your environment as a prefix to your terminal shell like this your `(unsloth-blackwell)user@machine:`",
      "language": "unknown"
    },
    {
      "code": "4. Download and build `xformers` (Optional)\n\n   Xformers is optional, but it is definitely faster and uses less memory. We'll use PyTorch's native SDPA if you do not want Xformers. Building Xformers from source might be slow, so beware!\n\n   You should see the name of your environment as a prefix to your terminal shell like this your `(unsloth-blackwell)user@machine:`",
      "language": "unknown"
    },
    {
      "code": "Note that we have to explicitly set `TORCH_CUDA_ARCH_LIST=12.0`.\n5. Update `triton`\n\n   Make sure you are inside the activated conda/mamba environment. You should see the name of your environment as a prefix to your terminal shell like this your `(unsloth-blackwell)user@machine:`",
      "language": "unknown"
    },
    {
      "code": "`triton>=3.3.1` is required for `Blackwell` support.\n6. `Transformers` Install any transformers version, but best to get the latest.",
      "language": "unknown"
    },
    {
      "code": "If you are using mamba as your package just replace conda with mamba for all commands shown above.\n\n### WSL-Specific Notes\n\nIf you're using WSL (Windows Subsystem for Linux) and encounter issues during xformers compilation (reminder Xformers is optional, but faster for training) follow these additional steps:\n\n1. **Increase WSL Memory Limit** Create or edit the WSL configuration file:",
      "language": "unknown"
    },
    {
      "code": "After making these changes, restart WSL:",
      "language": "unknown"
    },
    {
      "code": "2. **Install xformers** Use the following command to install xformers with optimized compilation for WSL:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Docker",
      "id": "docker"
    },
    {
      "level": "h3",
      "text": "uv",
      "id": "uv"
    },
    {
      "level": "h3",
      "text": "Conda or mamba (Advanced)",
      "id": "conda-or-mamba-(advanced)"
    },
    {
      "level": "h3",
      "text": "WSL-Specific Notes",
      "id": "wsl-specific-notes"
    }
  ],
  "url": "llms-txt#clone-and-build",
  "links": []
}