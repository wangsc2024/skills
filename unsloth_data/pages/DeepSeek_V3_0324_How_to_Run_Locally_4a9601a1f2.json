{
  "title": "DeepSeek-V3-0324: How to Run Locally",
  "content": "How to run DeepSeek-V3-0324 locally using our dynamic quants which recovers accuracy\n\n{% hint style=\"info\" %}\nPlease see <https://docs.unsloth.ai/basics/deepseek-r1-0528-how-to-run-locally> (May 28th 2025 update) to learn on how to run DeepSeek faster and more efficiently!\n{% endhint %}\n\nDeepSeek is at it again! After releasing V3, R1 Zero and R1 back in December 2024 and January 2025, DeepSeek updated their checkpoints / models for V3, and released a March update!\n\nAccording to DeepSeek, MMLU-Pro jumped +5.3% to 81.2%. **GPQA +9.3% points**. AIME + 19.8% and LiveCodeBench + 10.0%! They provided a plot showing how they compared to the previous V3 checkpoint and other models like GPT 4.5 and Claude Sonnet 3.7. <mark style=\"background-color:blue;\">**But how do we run a 671 billion parameter model locally?**</mark>\n\n<table data-full-width=\"true\"><thead><tr><th>MoE Bits</th><th>Type</th><th>Disk Size</th><th>Accuracy</th><th>Link</th><th>Details</th></tr></thead><tbody><tr><td>1.78bit</td><td>IQ1_S</td><td><strong>173GB</strong></td><td>Ok</td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ1_S\">Link</a></td><td>2.06/1.56bit</td></tr><tr><td>1.93bit</td><td>IQ1_M</td><td><strong>183GB</strong></td><td>Fair</td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ1_M\">Link</a></td><td>2.5/2.06/1.56</td></tr><tr><td>2.42bit</td><td>IQ2_XXS</td><td><strong>203GB</strong></td><td><mark style=\"background-color:blue;\"><strong>Suggested</strong></mark></td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-IQ2_XXS\">Link</a></td><td>2.5/2.06bit</td></tr><tr><td>2.71bit</td><td>Q2_K_XL</td><td><strong>231GB</strong></td><td><mark style=\"background-color:purple;\"><strong>Suggested</strong></mark></td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q2_K_XL\">Link</a></td><td>3.5/2.5bit</td></tr><tr><td>3.5bit</td><td>Q3_K_XL</td><td><strong>320GB</strong></td><td>Great</td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q3_K_XL\">Link</a></td><td>4.5/3.5bit</td></tr><tr><td>4.5bit</td><td>Q4_K_XL</td><td><strong>406GB</strong></td><td>Best</td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF/tree/main/UD-Q4_K_XL\">Link</a></td><td>5.5/4.5bit</td></tr></tbody></table>\n\n{% hint style=\"success\" %}\nDeepSeek V3's original upload is in float8, which takes 715GB. Using Q4\\_K\\_M halves the file size to 404GB or so, and our dynamic 1.78bit quant fits in around 151GB. **We suggest using our 2.7bit quant to balance size and accuracy! The 2.4bit one also works well!**\n{% endhint %}\n\n## :gear: Official Recommended Settings\n\nAccording to [DeepSeek](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324), these are the recommended settings for inference:\n\n* <mark style=\"background-color:blue;\">**Temperature of 0.3**</mark> (Maybe 0.0 for coding as [seen here](https://api-docs.deepseek.com/quick_start/parameter_settings))\n* Min\\_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)\n* Chat template: `<ÔΩúUserÔΩú>Create a simple playable Flappy Bird Game in Python. Place the final game inside of a markdown section.<ÔΩúAssistantÔΩú>`\n* A BOS token of `<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>` is auto added during tokenization (do NOT add it manually!)\n* DeepSeek mentioned using a <mark style=\"background-color:green;\">**system prompt**</mark> as well (optional) - it's in Chinese: `ËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\\n‰ªäÂ§©ÊòØ3Êúà24Êó•ÔºåÊòüÊúü‰∏Ä„ÄÇ` which translates to: `The assistant is DeepSeek Chat, created by DeepSeek.\\nToday is Monday, March 24th.`\n* <mark style=\"background-color:orange;\">**For KV cache quantization, use 8bit, NOT 4bit - we found it to do noticeably worse.**</mark>\n\n## üìñ Tutorial: How to Run DeepSeek-V3 in llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n{% hint style=\"warning\" %}\nNOTE using `-DGGML_CUDA=ON` for GPUs might take 5 minutes to compile. CPU only takes 1 minute to compile. You might be interested in llama.cpp's precompiled binaries.\n{% endhint %}\n\n2. Download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose `UD-IQ1_S`(dynamic 1.78bit quant) or other quantized versions like `Q4_K_M` . <mark style=\"background-color:green;\">**I recommend using our 2.7bit dynamic quant**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**`UD-Q2_K_XL`**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**to balance size and accuracy**</mark>. More versions at: <https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF>\n\n{% code overflow=\"wrap\" %}",
  "code_samples": [
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": ":gear: Official Recommended Settings",
      "id": ":gear:-official-recommended-settings"
    },
    {
      "level": "h2",
      "text": "üìñ Tutorial: How to Run DeepSeek-V3 in llama.cpp",
      "id": "üìñ-tutorial:-how-to-run-deepseek-v3-in-llama.cpp"
    }
  ],
  "url": "llms-txt#deepseek-v3-0324:-how-to-run-locally",
  "links": []
}