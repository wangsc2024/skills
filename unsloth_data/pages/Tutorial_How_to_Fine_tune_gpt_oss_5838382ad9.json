{
  "title": "Tutorial: How to Fine-tune gpt-oss",
  "content": "Learn step-by-step how to train OpenAI gpt-oss locally with Unsloth.\n\nIn this guide with screenshots, you'll learn to fine-tune your own custom gpt-oss model either [locally](#local-gpt-oss-fine-tuning) on your machine or for free using [Google Colab](#colab-gpt-oss-fine-tuning). We'll walk you through the entire process, from setup to running and saving your trained model.\n\n{% hint style=\"success\" %}\n[**Aug 28 update**](https://docs.unsloth.ai/models/long-context-gpt-oss-training#introducing-unsloth-flex-attention-support)**:** You can now export/save your QLoRA fine-tuned gpt-oss model to llama.cpp, vLLM, HF etc.\n\nWe also introduced [Unsloth Flex Attention](https://docs.unsloth.ai/models/long-context-gpt-oss-training#introducing-unsloth-flex-attention-support) which enables **>8√ó longer context lengths**, **>50% less VRAM usage** and **>1.5√ó faster training** vs. all implementations. [Read more here](https://docs.unsloth.ai/models/long-context-gpt-oss-training#introducing-unsloth-flex-attention-support)\n{% endhint %}\n\n> **Quickstart:** Fine-tune gpt-oss-20b for free with our: [Colab notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-\\(20B\\)-Fine-tuning.ipynb)\n\nUnsloth gpt-oss fine-tuning, when compared to all other FA2 implementations, achieves 1.5√ó faster training, 70% reduction in VRAM use, and 10x longer context lengths - with no accuracy loss.\n\n* **QLoRA requirements:** gpt-oss-20b = 14GB VRAM ‚Ä¢ gpt-oss-120b = 65GB VRAM.\n* **BF16 LoRA requirements:** gpt-oss-20b = 44GB VRAM ‚Ä¢ gpt-oss-120b = 210GB VRAM.\n\n<a href=\"#local-gpt-oss-fine-tuning\" class=\"button secondary\">Local Guide</a><a href=\"#colab-gpt-oss-fine-tuning\" class=\"button secondary\">Colab Guide</a>\n\n## üåê Colab gpt-oss Fine-tuning\n\nThis section covers fine-tuning gpt-oss using our Google Colab [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks). You can also save and use the gpt-oss notebook into your favorite code editor and follow our [local gpt-oss guide](#local-gpt-oss-fine-tuning).\n\n{% stepper %}\n{% step %}\n\n#### Install Unsloth (in Colab)\n\nIn Colab, run cells **from top to bottom**. Use **Run all** for the first pass. The first cell installs Unsloth (and related dependencies) and prints GPU/memory info. If a cell throws an error, simply re-run it.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-b5e2d89ed2815aa5dd6be7e4d2424df454c46ca0%2Fchrome_wTbzfmSI21.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-bbea9a8316e670247b6e69ff62d45a0dea189f35%2Fchrome_yPnb553OGW.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n#### Configuring gpt-oss and Reasoning Effort\n\nWe‚Äôll load **`gpt-oss-20b`** using Unsloth's [linearized version](https://docs.unsloth.ai/models/gpt-oss-how-to-run-and-fine-tune/..#making-efficient-gpt-oss-fine-tuning-work) (as no other version will work).\n\nConfigure the following parameters:\n\n* `max_seq_length = 1024`\n  * Recommended for quick testing and initial experiments.\n* `load_in_4bit = True`\n  * Use `False` for LoRA training (note: setting this to `False` will need at least 43GB VRAM). You ***MUST*** also set **`model_name = \"unsloth/gpt-oss-20b-BF16\"`**\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-eff24652551c00dccb790fda29fc3d580823cb31%2Fchrome_3qSe2UIFN0.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nYou should see output similar to the example below. Note: We explicitly change the `dtype` to `float32` to ensure correct training behavior.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-6bd982cfb20d01502802a926938b9a62abd9b1e7%2Fchrome_DGMDHldw0J.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n#### Fine-tuning Hyperparameters (LoRA)\n\nNow it's time to adjust your training hyperparameters. For a deeper dive into how, when, and what to tune, check out our [detailed hyperparameters guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide).\n\n{% hint style=\"info\" %}\nTo avoid [overfitting](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide#avoiding-overfitting-and-underfitting), monitor your training loss and avoid setting these values too high.\n{% endhint %}\n\nThis step adds LoRA adapters for parameter-efficient fine-tuning. Only about 1% of the model‚Äôs parameters are trained, which makes the process significantly more efficient.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-83a37bf7602d892fe7b8350e5025b1d5a1ad75b6%2Fchrome_ucj0VKT1lh.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\nIn the notebook, there's a section called *\"Reasoning Effort\"* that demonstrates gpt-oss inference running in Colab. You can skip this step, but you'll still need to run the model later once you've finished fine-tuning it.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-395308c7013021932a20a4eef85e2b17f8b6b029%2Fchrome_o2rLNfES8e.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n#### Data Preparation\n\nFor this example, we will use the [`HuggingFaceH4/Multilingual-Thinking`](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking). This dataset contains chain-of-thought reasoning examples derived from user questions translated from English into four additional languages.\n\nThis is the same dataset referenced in OpenAI's fine-tuning cookbook.\n\nThe goal of using a multilingual dataset is to help the model learn and generalize reasoning patterns across multiple languages.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-a63d7b6555b7ffdccb506ed44a34deb0370e7a90%2Fchrome_rRKmU99f0T.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\ngpt-oss introduces a reasoning effort system that controls how much reasoning the model performs. By default, the reasoning effort is set to `low`, but you can change it by setting the `reasoning_effort` parameter to `low`, `medium` or `high`.\n\nTo format the dataset, we apply a customized version of the gpt-oss prompt:\n\nLet's inspect the dataset by printing the first example:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-999632c15fd6bc73e3f7c1a11b74c8cedf563478%2Fchrome_sjbDtIhP5e.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nOne unique feature of gpt-oss is its use of the [**OpenAI Harmony format**](https://github.com/openai/harmony)**,** which supports structured conversations, reasoning output, and tool calling. This format includes tags such as `<|start|>` , `<|message|>` , and `<|return|>` .\n\n{% hint style=\"info\" %}\nü¶• Unsloth fixes the chat template to ensure it is correct. See this [tweet](https://x.com/danielhanchen/status/1953901104150065544) for technical details on our template fix.\n{% endhint %}\n\nFeel free to adapt the prompt and structure to suit your own dataset or use-case. For more guidance, refer to our [dataset guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/datasets-guide).\n{% endstep %}\n\nWe've pre-selected training hyperparameters for optimal results. However, you can modify them based on your specific use case. Refer to our [hyperparameters guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide).\n\nIn this example, we train for 60 steps to speed up the process. For a full training run, set `num_train_epochs=1` and disable the step limiting by setting `max_steps=None`.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-942bbba058a27b056cab8a21bed15d988e39fafc%2Fchrome_R85PmZRHMQ.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nDuring training, monitor the loss to ensure that it is decreasing over time. This confirms that the training process is functioning correctly.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-5ace71760531cf39f14499baf9ca0f78d8018756%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n#### Inference: Run your trained model\n\nNow it's time to run inference with your fine-tuned model. You can modify the instruction and input, but leave the output blank.\n\nIn this example, we test the model's ability to reason in French by adding a specific instruction to the system prompt, following the same structure used in our dataset.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-85e0e0aac7ae30bf7108470795fbabf815176abe%2Fchrome_jbJmBTaY7B.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nThis should produce an output similar to:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-0cb10ed022a5b451fe0bf4a4b9b35bef94364a5b%2Fchrome_ORco4bpZZ6.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n#### Save/export your model\n\nTo save your fine-tuned model, you can export your fine-tuned model both in **bf16 format ,** with our **on-demand dequantization of MXFP4** base models using `save_method=\"merged_16bit\"`or in native **MXFP4** Safetensors format using `save_method=\"mxfp4\"` .\n\nThe **MXFP4** native merge format offers significant performance improvements compared to the **bf16 format**: it uses up to 75% less disk space, reduces VRAM consumption by 50%, accelerates merging by 5-10x, and enables much faster conversion to **GGUF** format.\n\n{% hint style=\"success\" %}\nNew: Saving or merging QLoRA fine-tuned models to GGUF is now supported for use in other frameworks (e.g. Hugging Face, llama.cpp with GGUF).\n{% endhint %}\n\nAfter fine-tuning your gpt-oss model, you can merge it into **MXFP4** format with:\n\nIf you prefer to merge the model and push to the hugging-face hub directly:\n\n#### :sparkles: Saving to Llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. Convert the **MXFP4** merged model:\n\n3. Run inference on the quantized model:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-4379581da4820a0b717e8ae2456814c6c90c344b%2Fchrome_fKEKXHti5r.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n{% endstepper %}\n\n## üñ•Ô∏è Local gpt-oss Fine-tuning\n\nThis chapter covers fine-tuning gpt-oss on your local device. While **gpt-oss-20b** fine-tuning can operate on just 14GB VRAM, we recommend having at least 16GB VRAM available to ensure stable and reliable training runs.\n\n{% hint style=\"info\" %}\nWe recommend downloading or incorporating elements from our Colab [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) into your local setup for easier use.\n{% endhint %}\n\n{% stepper %}\n{% step %}\n\n#### Install Unsloth Locally\n\nEnsure your device is [Unsloth compatible](https://docs.unsloth.ai/get-started/fine-tuning-for-beginners/unsloth-requirements) and you can read our detailed [installation guide](https://docs.unsloth.ai/get-started/install-and-update).\n\nNote that `pip install unsloth` will not work for this setup, as we need to use the latest PyTorch, Triton and related packages. Install Unsloth using this specific command:",
  "code_samples": [
    {
      "code": "tokenizer.apply_chat_template(\n    text, \n    tokenize = False, \n    add_generation_prompt = False,\n    reasoning_effort = \"medium\",\n)",
      "language": "python"
    },
    {
      "code": "from unsloth.chat_templates import standardize_sharegpt\ndataset = standardize_sharegpt(dataset)\ndataset = dataset.map(formatting_prompts_func, batched = True,)",
      "language": "python"
    },
    {
      "code": "<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-999632c15fd6bc73e3f7c1a11b74c8cedf563478%2Fchrome_sjbDtIhP5e.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nOne unique feature of gpt-oss is its use of the [**OpenAI Harmony format**](https://github.com/openai/harmony)**,** which supports structured conversations, reasoning output, and tool calling. This format includes tags such as `<|start|>` , `<|message|>` , and `<|return|>` .\n\n{% hint style=\"info\" %}\nü¶• Unsloth fixes the chat template to ensure it is correct. See this [tweet](https://x.com/danielhanchen/status/1953901104150065544) for technical details on our template fix.\n{% endhint %}\n\nFeel free to adapt the prompt and structure to suit your own dataset or use-case. For more guidance, refer to our [dataset guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/datasets-guide).\n{% endstep %}\n\n{% step %}\n\n#### Train the model\n\nWe've pre-selected training hyperparameters for optimal results. However, you can modify them based on your specific use case. Refer to our [hyperparameters guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/lora-hyperparameters-guide).\n\nIn this example, we train for 60 steps to speed up the process. For a full training run, set `num_train_epochs=1` and disable the step limiting by setting `max_steps=None`.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-942bbba058a27b056cab8a21bed15d988e39fafc%2Fchrome_R85PmZRHMQ.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nDuring training, monitor the loss to ensure that it is decreasing over time. This confirms that the training process is functioning correctly.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-5ace71760531cf39f14499baf9ca0f78d8018756%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n{% step %}\n\n#### Inference: Run your trained model\n\nNow it's time to run inference with your fine-tuned model. You can modify the instruction and input, but leave the output blank.\n\nIn this example, we test the model's ability to reason in French by adding a specific instruction to the system prompt, following the same structure used in our dataset.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-85e0e0aac7ae30bf7108470795fbabf815176abe%2Fchrome_jbJmBTaY7B.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nThis should produce an output similar to:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-0cb10ed022a5b451fe0bf4a4b9b35bef94364a5b%2Fchrome_ORco4bpZZ6.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n\n{% step %}\n\n#### Save/export your model\n\nTo save your fine-tuned model, you can export your fine-tuned model both in **bf16 format ,** with our **on-demand dequantization of MXFP4** base models using `save_method=\"merged_16bit\"`or in native **MXFP4** Safetensors format using `save_method=\"mxfp4\"` .\n\nThe **MXFP4** native merge format offers significant performance improvements compared to the **bf16 format**: it uses up to 75% less disk space, reduces VRAM consumption by 50%, accelerates merging by 5-10x, and enables much faster conversion to **GGUF** format.\n\n{% hint style=\"success\" %}\nNew: Saving or merging QLoRA fine-tuned models to GGUF is now supported for use in other frameworks (e.g. Hugging Face, llama.cpp with GGUF).\n{% endhint %}\n\nAfter fine-tuning your gpt-oss model, you can merge it into **MXFP4** format with:",
      "language": "unknown"
    },
    {
      "code": "If you prefer to merge the model and push to the hugging-face hub directly:",
      "language": "unknown"
    },
    {
      "code": "#### :sparkles: Saving to Llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.",
      "language": "unknown"
    },
    {
      "code": "2. Convert the **MXFP4** merged model:",
      "language": "unknown"
    },
    {
      "code": "3. Run inference on the quantized model:",
      "language": "unknown"
    },
    {
      "code": "<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-4379581da4820a0b717e8ae2456814c6c90c344b%2Fchrome_fKEKXHti5r.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endstep %}\n{% endstepper %}\n\n## üñ•Ô∏è Local gpt-oss Fine-tuning\n\nThis chapter covers fine-tuning gpt-oss on your local device. While **gpt-oss-20b** fine-tuning can operate on just 14GB VRAM, we recommend having at least 16GB VRAM available to ensure stable and reliable training runs.\n\n{% hint style=\"info\" %}\nWe recommend downloading or incorporating elements from our Colab [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) into your local setup for easier use.\n{% endhint %}\n\n{% stepper %}\n{% step %}\n\n#### Install Unsloth Locally\n\nEnsure your device is [Unsloth compatible](https://docs.unsloth.ai/get-started/fine-tuning-for-beginners/unsloth-requirements) and you can read our detailed [installation guide](https://docs.unsloth.ai/get-started/install-and-update).\n\nNote that `pip install unsloth` will not work for this setup, as we need to use the latest PyTorch, Triton and related packages. Install Unsloth using this specific command:",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": "üåê Colab gpt-oss Fine-tuning",
      "id": "üåê-colab-gpt-oss-fine-tuning"
    },
    {
      "level": "h2",
      "text": "üñ•Ô∏è Local gpt-oss Fine-tuning",
      "id": "üñ•Ô∏è-local-gpt-oss-fine-tuning"
    }
  ],
  "url": "llms-txt#tutorial:-how-to-fine-tune-gpt-oss",
  "links": []
}