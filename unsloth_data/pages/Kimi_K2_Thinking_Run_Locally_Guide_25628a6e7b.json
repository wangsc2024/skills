{
  "title": "Kimi K2 Thinking: Run Locally Guide",
  "content": "Guide on running Kimi-K2-Thinking and Kimi-K2 on your own local device!\n\n{% hint style=\"success\" %}\nKimi-K2-Thinking got released. Read our [Thinking guide](#kimi-k2-thinking-guide) or access [GGUFs here](https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF).\n\nWe also collaborated with the Kimi team on [**system prompt fix**](#tokenizer-quirks-and-bug-fixes) for Kimi-K2-Thinking.\n{% endhint %}\n\nKimi-K2 and **Kimi-K2-Thinking** achieve SOTA performance in knowledge, reasoning, coding, and agentic tasks. The full 1T parameter models from Moonshot AI requires 1.09TB of disk space, while the quantized **Unsloth Dynamic 1.8-bit** version reduces this to just 230GB (-80% size)**:** [**Kimi-K2-GGUF**](https://huggingface.co/unsloth/Kimi-K2-Instruct-GGUF)\n\nYou can also now run our [**Kimi-K2-Thinking** GGUFs](https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF).\n\nAll uploads use Unsloth [Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs) for SOTA [Aider Polyglot](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot) and 5-shot MMLU performance. See how our Dynamic 1â€“2 bit GGUFs perform on [coding benchmarks here](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot).\n\n<a href=\"#kimi-k2-thinking-guide\" class=\"button primary\">Run Thinking</a><a href=\"#kimi-k2-instruct-guide\" class=\"button primary\">Run Instruct</a>\n\n### :gear: Recommended Requirements\n\n{% hint style=\"info\" %}\nYou need **247GB of disk space** to run the 1bit quant!\n\nThe only requirement is **`disk space + RAM + VRAM â‰¥ 247GB`**. That means you do not need to have that much RAM or VRAM (GPU) to run the model, but it will be much slower.\n{% endhint %}\n\nThe 1.8-bit (UD-TQ1\\_0) quant will fit in a 1x 24GB GPU (with all MoE layers offloaded to system RAM or a fast disk). Expect around \\~1-2 tokens/s with this setup if you have bonus 256GB RAM as well. The full Kimi K2 Q8 quant is 1.09TB in size and will need at least 8 x H200 GPUs.\n\nFor optimal performance you will need at least **247GB unified memory or 247GB combined RAM+VRAM** for 5+ tokens/s. If you have less than 247GB combined RAM+VRAM, then the speed of the model will definitely take a hit.\n\n**If you do not have 247GB of RAM+VRAM, no worries!** llama.cpp inherently has **disk offloading**, so through mmaping, it'll still work, just be slower - for example before you might get 5 to 10 tokens / second, now it's under 1 token.\n\nWe suggest using our **UD-Q2\\_K\\_XL (360GB)** quant to balance size and accuracy!\n\n{% hint style=\"success\" %}\nFor the best performance, have your VRAM + RAM combined = the size of the quant you're downloading. If not, it'll still work via disk offloading, just it'll be slower!\n{% endhint %}\n\n## ðŸ’­Kimi-K2-Thinking Guide\n\nKimi-K2-Thinking should generally follow the same instructions as the Instruct model, with a few key differences, particularly in areas such as settings and the chat template.\n\n{% hint style=\"success\" %}\n**To run the model in full precision, you only need to use the 4-bit or 5-bit Dynamic GGUFs (e.g. UD\\_Q4\\_K\\_XL) because the model was originally released in INT4 format.**\n\nYou can choose a higher-bit quantization just to be safe in case of small quantization differences, but in most cases this is unnecessary.\n{% endhint %}\n\n### ðŸŒ™ Official Recommended Settings:\n\nAccording to [Moonshot AI](https://huggingface.co/moonshotai/Kimi-K2-Thinking), these are the recommended settings for Kimi-K2-Thinking inference:\n\n* Set the <mark style=\"background-color:green;\">**temperature 1.0**</mark> to reduce repetition and incoherence.\n* Suggested context length = 98,304 (up to 256K)\n* Note: Using different tools may require different settings\n\n{% hint style=\"info\" %}\nWe recommend setting <mark style=\"background-color:green;\">**min\\_p to 0.01**</mark> to suppress the occurrence of unlikely tokens with low probabilities.\n{% endhint %}\n\nFor example given a user message of \"What is 1+1?\", we get:\n\n{% code overflow=\"wrap\" %}\n\n### âœ¨ Run Kimi K2 Thinking in llama.cpp\n\n{% hint style=\"success\" %}\nYou can now use the latest update of [llama.cpp](https://github.com/ggml-org/llama.cpp) to run the model:\n{% endhint %}\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. If you want to use `llama.cpp` directly to load models, you can do the below: (:UD-TQ1\\_0) is the quantization type. You can also download via Hugging Face (point 3). This is similar to `ollama run` . Use `export LLAMA_CACHE=\"folder\"` to force `llama.cpp` to save to a specific location.\n\n3. The above will use around 8GB of GPU memory. If you have around 360GB of combined GPU memory, remove `-ot \".ffn_.*_exps.=CPU\"` to get maximum speed!\n\n{% hint style=\"info\" %}\nPlease try out `-ot \".ffn_.*_exps.=CPU\"` to offload all MoE layers to the CPU! This effectively allows you to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression to fit more layers if you have more GPU capacity.\n\nIf you have a bit more GPU memory, try `-ot \".ffn_(up|down)_exps.=CPU\"` This offloads up and down projection MoE layers.\n\nTry `-ot \".ffn_(up)_exps.=CPU\"` if you have even more GPU memory. This offloads only up projection MoE layers.\n\nAnd finally offload all layers via `-ot \".ffn_.*_exps.=CPU\"` This uses the least VRAM.\n\nYou can also customize the regex, for example `-ot \"\\.(6|7|8|9|[0-9][0-9]|[0-9][0-9][0-9])\\.ffn_(gate|up|down)_exps.=CPU\"` means to offload gate, up and down MoE layers but only from the 6th layer onwards.\n{% endhint %}\n\n3. Download the model via (after installing `pip install huggingface_hub hf_transfer` ). We recommend using our 2bit dynamic quant UD-Q2\\_K\\_XL to balance size and accuracy. All versions at: [huggingface.co/unsloth/Kimi-K2-Thinking-GGUF](https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF)\n\n{% code overflow=\"wrap\" %}",
  "code_samples": [
    {
      "code": "<|im_system|>system<|im_middle|>You are Kimi, an AI assistant created by Moonshot AI.<|im_end|><|im_user|>user<|im_middle|>What is 1+1?<|im_end|><|im_assistant|>assistant<|im_middle|>",
      "language": "unknown"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "export LLAMA_CACHE=\"unsloth/Kimi-K2-Thinking-GGUF\"\n./llama.cpp/llama-cli \\\n    -hf unsloth/Kimi-K2-Thinking-GGUF:UD-TQ1_0 \\\n    --n-gpu-layers 99 \\\n    --temp 1.0 \\\n    --min-p 0.01 \\\n    --ctx-size 16384 \\\n    --seed 3407 \\\n    -ot \".ffn_.*_exps.=CPU\"",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": ":gear: Recommended Requirements",
      "id": ":gear:-recommended-requirements"
    },
    {
      "level": "h2",
      "text": "ðŸ’­Kimi-K2-Thinking Guide",
      "id": "ðŸ’­kimi-k2-thinking-guide"
    },
    {
      "level": "h3",
      "text": "ðŸŒ™ Official Recommended Settings:",
      "id": "ðŸŒ™-official-recommended-settings:"
    },
    {
      "level": "h3",
      "text": "âœ¨ Run Kimi K2 Thinking in llama.cpp",
      "id": "âœ¨-run-kimi-k2-thinking-in-llama.cpp"
    }
  ],
  "url": "llms-txt#kimi-k2-thinking:-run-locally-guide",
  "links": []
}