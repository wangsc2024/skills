{
  "title": "GLM-4.7: How to Run Locally Guide",
  "content": "A guide on how to run Z.ai GLM-4.7 model on your own local device!\n\nGLM-4.7 is Z.ai’s latest thinking model, delivering stronger coding, agent, and chat performance than [GLM-4.6](https://docs.unsloth.ai/models/glm-4.6-how-to-run-locally). It achieves SOTA performance on on SWE-bench (73.8%, +5.8), SWE-bench Multilingual (66.7%, +12.9), and Terminal Bench 2.0 (41.0%, +16.5).\n\nThe full 355B parameter model requires **400GB** of disk space, while the Unsloth Dynamic 2-bit GGUF reduces the size to **134GB** (-**75%)**. [**GLM-4.7-GGUF**](https://huggingface.co/unsloth/GLM-4.7-GGUF)\n\nAll uploads use Unsloth [Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs) for SOTA 5-shot MMLU and Aider performance, meaning you can run & fine-tune quantized GLM LLMs with minimal accuracy loss.\n\n### :gear: Usage Guide\n\nThe 2-bit dynamic quant UD-Q2\\_K\\_XL uses 135GB of disk space - this works well in a **1x24GB card and 128GB of RAM** with MoE offloading. The 1-bit UD-TQ1 GGUF also **works natively in Ollama**!\n\n{% hint style=\"info\" %}\nYou must use `--jinja` for llama.cpp quants - this uses our [fixed chat templates](#chat-template-bug-fixes) and enables the correct template! You might get incorrect results if you do not use `--jinja`\n{% endhint %}\n\nThe 4-bit quants will fit in a 1x 40GB GPU (with MoE layers offloaded to RAM). Expect around 5 tokens/s with this setup if you have bonus 165GB RAM as well. It is recommended to have at least 205GB RAM to run this 4-bit. For optimal performance you will need at least 205GB unified memory or 205GB combined RAM+VRAM for 5+ tokens/s. To learn how to increase generation speed and fit longer contexts, [read here](#improving-generation-speed).\n\n{% hint style=\"success\" %}\nThough not a must, for best performance, have your VRAM + RAM combined equal to the size of the quant you're downloading. If not, hard drive / SSD offloading will work with llama.cpp, just inference will be slower.\n{% endhint %}\n\n### Recommended Settings\n\nUse distinct settings for different use cases. Recommended settings for default and multi-turn agentic use cases:\n\n| Default Settings (Most Tasks)                                      | Terminal Bench, SWE Bench Verified                                 |\n| ------------------------------------------------------------------ | ------------------------------------------------------------------ |\n| <mark style=\"background-color:green;\">**temperature = 1.0**</mark> | <mark style=\"background-color:green;\">**temperature = 0.7**</mark> |\n| <mark style=\"background-color:green;\">**top\\_p = 0.95**</mark>     | <mark style=\"background-color:green;\">**top\\_p = 1.0**</mark>      |\n| `131072` **max new tokens**                                        | `16384` **max new tokens**                                         |\n\n* Use `--jinja` for llama.cpp variants - we **fixed some chat template issues as well!**\n* **Maximum context window:** `131,072`\n\n## Run GLM-4.7 Tutorials:\n\nSee our step-by-step guides for running GLM-4.7 in [Ollama](#run-in-ollama) and [llama.cpp](#run-in-llama.cpp).\n\n### ✨ Run in llama.cpp\n\n{% stepper %}\n{% step %}\nObtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n{% step %}\nIf you want to use `llama.cpp` directly to load models, you can do the below: (:Q2\\_K\\_XL) is the quantization type. You can also download via Hugging Face (point 3). This is similar to `ollama run` . Use `export LLAMA_CACHE=\"folder\"` to force `llama.cpp` to save to a specific location. Remember the model has only a maximum of 128K context length.\n\n{% hint style=\"info\" %}\nPlease try out `-ot \".ffn_.*_exps.=CPU\"` to offload all MoE layers to the CPU! This effectively allows you to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression to fit more layers if you have more GPU capacity.\n\nIf you have a bit more GPU memory, try `-ot \".ffn_(up|down)_exps.=CPU\"` This offloads up and down projection MoE layers.\n\nTry `-ot \".ffn_(up)_exps.=CPU\"` if you have even more GPU memory. This offloads only up projection MoE layers.\n\nAnd finally offload all layers via `-ot \".ffn_.*_exps.=CPU\"` This uses the least VRAM.\n\nYou can also customize the regex, for example `-ot \"\\.(6|7|8|9|[0-9][0-9]|[0-9][0-9][0-9])\\.ffn_(gate|up|down)_exps.=CPU\"` means to offload gate, up and down MoE layers but only from the 6th layer onwards.\n{% endhint %}\n{% endstep %}\n\n{% step %}\nDownload the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose `UD-`Q2\\_K\\_XL (dynamic 2bit quant) or other quantized versions like `Q4_K_XL` . We <mark style=\"background-color:green;\">**recommend using our 2.7bit dynamic quant**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**`UD-Q2_K_XL`**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**to balance size and accuracy**</mark>.",
  "code_samples": [
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-cli llama-mtmd-cli llama-server llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "export LLAMA_CACHE=\"unsloth/GLM-4.7-GGUF\"\n./llama.cpp/llama-cli \\\n    --model GLM-4.7-GGUF/UD-Q2_K_XL/GLM-4.7-UD-Q2_K_XL-00001-of-00003.gguf \\\n    --n-gpu-layers 99 \\\n    --jinja \\\n    --ctx-size 16384 \\\n    --flash-attn on \\\n    --temp 1.0 \\\n    --top-p 0.95 \\\n    -ot \".ffn_.*_exps.=CPU\"",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": ":gear: Usage Guide",
      "id": ":gear:-usage-guide"
    },
    {
      "level": "h3",
      "text": "Recommended Settings",
      "id": "recommended-settings"
    },
    {
      "level": "h2",
      "text": "Run GLM-4.7 Tutorials:",
      "id": "run-glm-4.7-tutorials:"
    },
    {
      "level": "h3",
      "text": "✨ Run in llama.cpp",
      "id": "✨-run-in-llama.cpp"
    }
  ],
  "url": "llms-txt#glm-4.7:-how-to-run-locally-guide",
  "links": []
}