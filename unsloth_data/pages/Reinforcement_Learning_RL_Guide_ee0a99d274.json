{
  "title": "Reinforcement Learning (RL) Guide",
  "content": "Learn all about Reinforcement Learning (RL) and how to train your own DeepSeek-R1 reasoning model with Unsloth using GRPO. A complete guide from beginner to advanced.\n\nReinforcement Learning is where an \"agent\" learns to make decisions by interacting with an environment and receiving **feedback** in the form of **rewards** or **penalties**.\n\n* **Action:** What the model generates (e.g. a sentence).\n* **Reward:** A signal indicating how good or bad the model's action was (e.g. did the response follow instructions? was it helpful?).\n* **Environment:** The scenario or task the model is working on (e.g. answering a user’s question).\n\n{% hint style=\"success\" %}\n**Nov 26 update:** We're introducing FP8 precision RL and GRPO in Unsloth! [Read blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning)\n{% endhint %}\n\n### :sloth:What you will learn\n\n1. What is RL? RLVR? PPO? GRPO? RLHF? RFT? Is <mark style=\"background-color:green;\">**\"Luck is All You Need?\"**</mark> for RL?\n2. What is an environment? Agent? Action? Reward function? Rewards?\n\nThis article covers everything (from beginner to advanced) you need to know about GRPO, Reinforcement Learning (RL) and reward functions, along with tips, and the basics of using GRPO with [Unsloth](https://github.com/unslothai/unsloth). If you're looking for a step-by-step tutorial for using GRPO, see our guide [here](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo).\n\n{% hint style=\"info\" %}\nFor **advanced GRPO** documentation on batching, generation and training parameters, [read our guide!](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/advanced-rl-documentation)\n{% endhint %}\n\n## :question:What is Reinforcement Learning (RL)?\n\nThe goal of RL is to:\n\n1. **Increase the chance of seeing&#x20;**<mark style=\"background-color:green;\">**\"good\"**</mark>**&#x20;outcomes.**\n2. **Decrease the chance of seeing&#x20;**<mark style=\"background-color:red;\">**\"bad\"**</mark>**&#x20;outcomes.**\n\n**That's it!** There are intricacies on what \"good\" and \"bad\" means, or how do we go about \"increasing\" or \"decreasing\" it, or what even \"outcomes\" means.\n\n{% columns %}\n{% column width=\"50%\" %}\nFor example, in the **Pacman game**:\n\n1. The <mark style=\"background-color:green;\">**environment**</mark> is the game world.\n2. The <mark style=\"background-color:blue;\">**actions**</mark> you can take are UP, LEFT, RIGHT and DOWN.\n3. The <mark style=\"background-color:purple;\">**rewards**</mark> are good if you eat a cookie, or bad if you hit one of the squiggly enemies.\n4. In RL, you can't know the \"best action\" you can take, but you can observe intermediate steps, or the final game state (win or lose)\n   {% endcolumn %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-e853f7e6da505ee587642314b98180ebf840252c%2FRL%20Game.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n{% endcolumns %}\n\n{% columns %}\n{% column width=\"50%\" %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-30bade1550c877bb7f79075c80ac79476b0ecd76%2FMath%20RL.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n\n{% column %}\nAnother example is imagine you are given the question: <mark style=\"background-color:blue;\">**\"What is 2 + 2?\"**</mark> (4) An unaligned language model will spit out 3, 4, C, D, -10, literally anything.\n\n1. Numbers are better than C or D right?\n2. Getting 3 is better than say 8 right?\n3. Getting 4 is definitely correct.\n\nWe just designed a <mark style=\"background-color:orange;\">**reward function**</mark>!\n{% endcolumn %}\n{% endcolumns %}\n\n### :person\\_running:From RLHF, PPO to GRPO and RLVR\n\n{% columns %}\n{% column %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-5d0c90e4b45507d3e12c8b938cbd1679cd38f4f9%2FRLHF.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n\n{% column %}\nOpenAI popularized the concept of [RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback) (Reinforcement Learning from Human Feedback), where we train an <mark style=\"background-color:red;\">**\"agent\"**</mark> to produce outputs to a question (the <mark style=\"background-color:yellow;\">**state**</mark>) that are rated more useful by human beings.\n\nThe thumbs up and down in ChatGPT for example can be used in the RLHF process.\n{% endcolumn %}\n{% endcolumns %}\n\n{% columns %}\n{% column %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-1e1dff9c921e787e669dee79c41a76db89e882e7%2FPPO.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-f6156f2c519baf81e6ef286476f4092037303799%2FPPO%20formula.png?alt=media\" alt=\"\"><figcaption><p>PPO formula</p></figcaption></figure>\n\nThe clip(..., 1-e, 1+e) term is used to force PPO not to take too large changes. There is also a KL term with beta set to > 0 to force the model not to deviate too much away.\n{% endcolumn %}\n\n{% column %}\nIn order to do RLHF, [<mark style=\"background-color:red;\">**PPO**</mark>](https://en.wikipedia.org/wiki/Proximal_policy_optimization) (Proximal policy optimization) was developed. The <mark style=\"background-color:blue;\">**agent**</mark> is the language model in this case. In fact it's composed of 3 systems:\n\n1. The **Generating Policy (current trained model)**\n2. The **Reference Policy (original model)**\n3. The **Value Model (average reward estimator)**\n\nWe use the **Reward Model** to calculate the reward for the current environment, and our goal is to **maximize this**!\n\nThe formula for PPO looks quite complicated because it was designed to be stable. Visit our [AI Engineer talk](https://docs.unsloth.ai/ai-engineers-2025) we gave in 2025 about RL for more in depth maths derivations about PPO.\n{% endcolumn %}\n{% endcolumns %}\n\n{% columns %}\n{% column %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-4f4e188edbcad4f53aaa4a626bc5b2fd01334574%2FGRPO%20%2B%20RLVR.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n\n{% column %}\nDeepSeek developed [<mark style=\"background-color:red;\">**GRPO**</mark>](https://unsloth.ai/blog/grpo) (Group Relative Policy Optimization) to train their R1 reasoning models. The key differences to PPO are:\n\n1. The **Value Model is removed,** replaced with statistics from calling the reward model multiple times.\n2. The **Reward Model is removed** and replaced with just custom reward function which <mark style=\"background-color:blue;\">**RLVR**</mark> can be used.\n   {% endcolumn %}\n   {% endcolumns %}\n\nThis means GRPO is extremely efficient. Previously PPO needed to train multiple models - now with the reward model and value model removed, we can save memory and speed up everything.\n\n<mark style=\"background-color:orange;\">**RLVR (Reinforcement Learning with Verifiable Rewards)**</mark> allows us to reward the model based on tasks with easy to verify solutions. For example:\n\n1. Maths equations can be easily verified. Eg 2+2 = 4.\n2. Code output can be verified as having executed correctly or not.\n3. Designing verifiable reward functions can be tough, and so most examples are math or code.\n4. Use-cases for GRPO isn’t just for code or math—its reasoning process can enhance tasks like email automation, database retrieval, law, and medicine, greatly improving accuracy based on your dataset and reward function - the trick is to define a <mark style=\"background-color:yellow;\">**rubric - ie a list of smaller verifiable rewards, and not a final all consuming singular reward.**</mark> OpenAI popularized this in their [reinforcement learning finetuning (RFT)](https://platform.openai.com/docs/guides/reinforcement-fine-tuning) offering for example.\n\n{% columns %}\n{% column %} <mark style=\"background-color:red;\">**Why \"Group Relative\"?**</mark>\n\nGRPO removes the value model entirely, but we still need to estimate the <mark style=\"background-color:yellow;\">**\"average reward\"**</mark> given the current state.\n\nThe **trick is to sample the LLM**! We then calculate the average reward through statistics of the sampling process across multiple different questions.\n{% endcolumn %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-29e188e5adc6de1e62c841e6cd9e34a2dae4994a%2FGroup%20Relative.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n{% endcolumn %}\n{% endcolumns %}\n\n{% columns %}\n{% column %}\nFor example for \"What is 2+2?\" we sample 4 times. We might get 4, 3, D, C. We then calculate the reward for each of these answers, then calculate the **average reward** and **standard deviation**, then <mark style=\"background-color:red;\">**Z-score standardize**</mark> this!\n\nThis creates the <mark style=\"background-color:blue;\">**advantages A**</mark>, which we will use in replacement of the value model. This saves a lot of memory!\n{% endcolumn %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-d40a73cd48b05b9205810a1946f4fc1dce81ae7d%2FStatistics.png?alt=media\" alt=\"\"><figcaption><p>GRPO advantage calculation</p></figcaption></figure>\n{% endcolumn %}\n{% endcolumns %}\n\n### :fingers\\_crossed:Luck (well Patience) Is All You Need\n\nThe trick of RL is you need 2 things only:\n\n1. A question or instruction eg \"What is 2+2?\" \"Create a Flappy Bird game in Python\"\n2. A reward function and verifier to verify if the output is good or bad.\n\nWith only these 2, we can essentially **call a language model an infinite times** until we get a good answer. For example for \"What is 2+2?\", an untrained bad language model will output:\n\n***0, cat, -10, 1928, 3, A, B, 122, 17, 182, 172, A, C, BAHS, %$, #, 9, -192, 12.31\\*\\*\\*\\*\\*\\*\\*\\*&#x20;**<mark style=\"color:green;\">**then suddenly 4**</mark>**.***\n\n***The reward signal was 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\\*\\*\\*\\*\\*\\*\\*\\*&#x20;**<mark style=\"color:green;\">**then suddenly 1.**</mark>*\n\nSo by luck and by chance, RL managed to find the correct answer across multiple <mark style=\"background-color:yellow;\">**rollouts**</mark>. Our goal is we want to see the good answer 4 more, and the rest (the bad answers) much less.\n\n<mark style=\"color:blue;\">**So the goal of RL is to be patient - in the limit, if the probability of the correct answer is at least a small number (not zero), it's just a waiting game - you will 100% for sure encounter the correct answer in the limit.**</mark>\n\n<mark style=\"background-color:blue;\">**So I like to call it as \"Luck Is All You Need\" for RL.**</mark>\n\n<mark style=\"background-color:orange;\">**Well a better phrase is \"Patience is All You Need\" for RL.**</mark>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-4f0cb4803aa22583e88dfa8de8061b66bbe6a6b1%2FLuck%20is%20all%20you%20need.png?alt=media\" alt=\"\" width=\"375\"><figcaption></figcaption></figure>\n\nRL essentially provides us a trick - instead of simply waiting for infinity, we do get \"bad signals\" ie bad answers, and we can essentially \"guide\" the model to already try not generating bad solutions. This means although you waited very long for a \"good\" answer to pop up, the model already has been changed to try its best not to output bad answers.\n\nIn the \"What is 2+2?\" example - ***0, cat, -10, 1928, 3, A, B, 122, 17, 182, 172, A, C, BAHS, %$, #, 9, -192, 12.31\\*\\*\\*\\*\\*\\*\\*\\*&#x20;**<mark style=\"color:green;\">**then suddenly 4**</mark>**.***\n\nSince we got bad answers, RL will influence the model to try NOT to output bad answers. This means over time, we are carefully \"pruning\" or moving the model's output distribution away from bad answers. This means RL is <mark style=\"color:blue;\">**efficient**</mark>, since we are NOT just waiting for infinity, but we are actively trying to \"push\" the model to go as much as possible to the \"correct answer space\".\n\n{% hint style=\"danger\" %}\n**If the probability is always 0, then RL will never work**. This is also why people like to do RL from an already instruction finetuned model, which can partially follow instructions reasonably well - this boosts the probability most likely above 0.\n{% endhint %}\n\n## :sloth:What Unsloth offers for RL\n\n* With 15GB VRAM, Unsloth allows you to transform any model up to 17B parameters like Llama 3.1 (8B), Phi-4 (14B), Mistral (7B) or Qwen2.5 (7B) into a reasoning model\n* **Unsloth now supports** [**RL for Vision/multimodal**](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/vision-reinforcement-learning-vlm-rl) **models!**\n* **Minimum requirement:** Just  5GB VRAM is enough to train your own reasoning model locally (for any model with 1.5B parameters or less)\n\n{% content-ref url=\"reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo\" %}\n[tutorial-train-your-own-reasoning-model-with-grpo](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo)\n{% endcontent-ref %}\n\n| [**gpt-oss-20b**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-\\(20B\\)-GRPO.ipynb) **GSPO -** new | [**Qwen3-VL-8B**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_VL_\\(8B\\)-Vision-GRPO.ipynb) - Vision **GSPO** - new | [Gemma 3 (4B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_\\(4B\\)-Vision-GRPO.ipynb) - Vision GSPO - new   |\n| -------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------- |\n| [**Qwen3 (4B)**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_\\(4B\\)-GRPO.ipynb) - Advanced         | [**DeepSeek-R1-0528-Qwen3-8B**](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/DeepSeek_R1_0528_Qwen3_\\(8B\\)_GRPO.ipynb)    | [Llama 3.2 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Advanced_Llama3_2_\\(3B\\)_GRPO_LoRA.ipynb) - Advanced |\n| [Gemma 3 (1B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_\\(1B\\)-GRPO.ipynb)                     | [Phi-4 (14B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_\\(14B\\)-GRPO.ipynb)                                      | [Qwen2.5 (3B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_\\(3B\\)-GRPO.ipynb)                             |\n| [Mistral v0.3 (7B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_\\(7B\\)-GRPO.ipynb)          | [Llama 3.1 (8B)](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_\\(8B\\)-GRPO.ipynb)                                 |                                                                                                                                                 |\n\n{% hint style=\"success\" %}\n**NEW!** We now support [**GSPO**](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/gspo-reinforcement-learning) and most other new GRPO techniques. You can play with the following arguments in GRPOConfig to enable:\n\n```python\nepsilon=0.2,\nepsilon_high=0.28, # one sided\ndelta=1.5 # two sided",
  "code_samples": [],
  "headings": [
    {
      "level": "h3",
      "text": ":sloth:What you will learn",
      "id": ":sloth:what-you-will-learn"
    },
    {
      "level": "h2",
      "text": ":question:What is Reinforcement Learning (RL)?",
      "id": ":question:what-is-reinforcement-learning-(rl)?"
    },
    {
      "level": "h3",
      "text": ":person\\_running:From RLHF, PPO to GRPO and RLVR",
      "id": ":person\\_running:from-rlhf,-ppo-to-grpo-and-rlvr"
    },
    {
      "level": "h3",
      "text": ":fingers\\_crossed:Luck (well Patience) Is All You Need",
      "id": ":fingers\\_crossed:luck-(well-patience)-is-all-you-need"
    },
    {
      "level": "h2",
      "text": ":sloth:What Unsloth offers for RL",
      "id": ":sloth:what-unsloth-offers-for-rl"
    },
    {
      "level": "h3",
      "text": "GRPO notebooks:",
      "id": "grpo-notebooks:"
    }
  ],
  "url": "llms-txt#reinforcement-learning-(rl)-guide",
  "links": []
}