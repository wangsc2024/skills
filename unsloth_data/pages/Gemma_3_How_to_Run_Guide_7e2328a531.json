{
  "title": "Gemma 3 - How to Run Guide",
  "content": "How to run Gemma 3 effectively with our GGUFs on llama.cpp, Ollama, Open WebUI and how to fine-tune with Unsloth!\n\nGoogle releases Gemma 3 with a new 270M model and the previous 1B, 4B, 12B, and 27B sizes. The 270M and 1B are text-only, while larger models handle both text and vision. We provide GGUFs, and a guide of how to run it effectively, and how to finetune & do [RL](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide) with Gemma 3!\n\n{% hint style=\"success\" %}\n**NEW Aug 14, 2025 Update:** Try our fine-tuning [Gemma 3 (270M) notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_\\(270M\\).ipynb) and [GGUFs to run](https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b).\n\nAlso see our [Gemma 3n Guide](https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune/gemma-3n-how-to-run-and-fine-tune).\n{% endhint %}\n\n<a href=\"#gmail-running-gemma-3-on-your-phone\" class=\"button secondary\">Running Tutorial</a><a href=\"#fine-tuning-gemma-3-in-unsloth\" class=\"button secondary\">Fine-tuning Tutorial</a>\n\n**Unsloth is the only framework which works in float16 machines for Gemma 3 inference and training.** This means Colab Notebooks with free Tesla T4 GPUs also work!\n\n* Fine-tune Gemma 3 (4B) with vision support using our [free Colab notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_\\(4B\\)-Vision.ipynb)\n\n{% hint style=\"info\" %}\nAccording to the Gemma team, the optimal config for inference is\\\n`temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0`\n{% endhint %}\n\n**Unsloth Gemma 3 uploads with optimal configs:**\n\n| GGUF                                                                                                                                                                                                                                                                                                                                                                                                           | Unsloth Dynamic 4-bit Instruct                                                                                                                                                                                                                                                                                                                                                                                                               | 16-bit Instruct                                                                                                                                                                                                                                                                                                                                                     |\n| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <ul><li><a href=\"https://huggingface.co/unsloth/gemma-3-270m-it-GGUF\">270M</a> - new</li><li><a href=\"https://huggingface.co/unsloth/gemma-3-1b-it-GGUF\">1B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-4b-it-GGUF\">4B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-12b-it-GGUF\">12B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-27b-it-GGUF\">27B</a></li></ul> | <ul><li><a href=\"https://huggingface.co/unsloth/gemma-3-270m-it-unsloth-bnb-4bit\">270M</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-1b-it-bnb-4bit\">1B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-4b-it-bnb-4bit\">4B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-27b-it-unsloth-bnb-4bit\">12B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-27b-it-bnb-4bit\">27B</a></li></ul> | <ul><li><a href=\"https://huggingface.co/unsloth/gemma-3-270m-it\">270M</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-1b\">1B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-4b\">4B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-12b\">12B</a></li><li><a href=\"https://huggingface.co/unsloth/gemma-3-27b\">27B</a></li></ul> |\n\n## :gear: Recommended Inference Settings\n\nAccording to the Gemma team, the official recommended settings for inference is:\n\n* Temperature of 1.0\n* Top\\_K of 64\n* Min\\_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1)\n* Top\\_P of 0.95\n* Repetition Penalty of 1.0. (1.0 means disabled in llama.cpp and transformers)\n* Chat template:\n\n<pre data-overflow=\"wrap\"><code><strong>&#x3C;bos>&#x3C;start_of_turn>user\\nHello!&#x3C;end_of_turn>\\n&#x3C;start_of_turn>model\\nHey there!&#x3C;end_of_turn>\\n&#x3C;start_of_turn>user\\nWhat is 1+1?&#x3C;end_of_turn>\\n&#x3C;start_of_turn>model\\n\n  </strong></code></pre>\n* Chat template with `\\n`newlines rendered (except for the last)\n\n{% code overflow=\"wrap\" %}\n\n{% hint style=\"danger\" %}\nllama.cpp an other inference engines auto add a \\<bos> - DO NOT add TWO \\<bos> tokens! You should ignore the \\<bos> when prompting the model!\n{% endhint %}\n\n### âœ¨Running Gemma 3 on your phone <a href=\"#gmail-running-gemma-3-on-your-phone\" id=\"gmail-running-gemma-3-on-your-phone\"></a>\n\nTo run the models on your phone, we recommend using any mobile app that can run GGUFs locally on edge devices like phones. After fine-tuning you can export it to GGUF then run it locally on your phone. Ensure your phone has enough RAM/power to process the models as it can overheat so we recommend using Gemma 3 270M or the Gemma 3n models for this use-case. You can try the [open-source project AnythingLLM's](https://github.com/Mintplex-Labs/anything-llm) mobile app which you can download on [Android here](https://play.google.com/store/apps/details?id=com.anythingllm) or [ChatterUI](https://github.com/Vali-98/ChatterUI), which are great apps for running GGUFs on your phone.\n\n{% hint style=\"success\" %}\nRemember, you can change the model name 'gemma-3-27b-it-GGUF' to any Gemma model like 'gemma-3-270m-it-GGUF:Q8\\_K\\_XL' for all the tutorials.\n{% endhint %}\n\n## :llama: Tutorial: How to Run Gemma 3 in Ollama\n\n1. Install `ollama` if you haven't already!\n\n2. Run the model! Note you can call `ollama serve`in another terminal if it fails! We include all our fixes and suggested parameters (temperature etc) in `params` in our Hugging Face upload! You can change the model name 'gemma-3-27b-it-GGUF' to any Gemma model like 'gemma-3-270m-it-GGUF:Q8\\_K\\_XL'.\n\n## ðŸ“– Tutorial: How to Run Gemma 3 27B in llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. If you want to use `llama.cpp` directly to load models, you can do the below: (:Q4\\_K\\_XL) is the quantization type. You can also download via Hugging Face (point 3). This is similar to `ollama run`\n\n3. **OR** download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose Q4\\_K\\_M, or other quantized versions (like BF16 full precision). More versions at: <https://huggingface.co/unsloth/gemma-3-27b-it-GGUF>",
  "code_samples": [
    {
      "code": "<bos><start_of_turn>user\nHello!<end_of_turn>\n<start_of_turn>model\nHey there!<end_of_turn>\n<start_of_turn>user\nWhat is 1+1?<end_of_turn>\n<start_of_turn>model\\n",
      "language": "unknown"
    },
    {
      "code": "apt-get update\napt-get install pciutils -y\ncurl -fsSL https://ollama.com/install.sh | sh",
      "language": "bash"
    },
    {
      "code": "ollama run hf.co/unsloth/gemma-3-27b-it-GGUF:Q4_K_XL",
      "language": "bash"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=ON -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "./llama.cpp/llama-mtmd-cli \\\n    -hf unsloth/gemma-3-4b-it-GGUF:Q4_K_XL",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": ":gear: Recommended Inference Settings",
      "id": ":gear:-recommended-inference-settings"
    },
    {
      "level": "h3",
      "text": "âœ¨Running Gemma 3 on your phone <a href=\"#gmail-running-gemma-3-on-your-phone\" id=\"gmail-running-gemma-3-on-your-phone\"></a>",
      "id": "âœ¨running-gemma-3-on-your-phone-<a-href=\"#gmail-running-gemma-3-on-your-phone\"-id=\"gmail-running-gemma-3-on-your-phone\"></a>"
    },
    {
      "level": "h2",
      "text": ":llama: Tutorial: How to Run Gemma 3 in Ollama",
      "id": ":llama:-tutorial:-how-to-run-gemma-3-in-ollama"
    },
    {
      "level": "h2",
      "text": "ðŸ“– Tutorial: How to Run Gemma 3 27B in llama.cpp",
      "id": "ðŸ“–-tutorial:-how-to-run-gemma-3-27b-in-llama.cpp"
    }
  ],
  "url": "llms-txt#gemma-3---how-to-run-guide",
  "links": []
}