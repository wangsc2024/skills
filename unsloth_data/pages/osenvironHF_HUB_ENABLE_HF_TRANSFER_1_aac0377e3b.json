{
  "title": "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"",
  "content": "from huggingface_hub import snapshot_download\nsnapshot_download(\n  repo_id = \"unsloth/DeepSeek-R1-GGUF\",\n  local_dir = \"DeepSeek-R1-GGUF\",\n  allow_patterns = [\"*UD-IQ1_S*\"], # Select quant type UD-IQ1_S for 1.58bit\n)\nbash\n./llama.cpp/llama-cli \\\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --cache-type-k q4_0 \\\n    --threads 12 -no-cnv --prio 2 \\\n    --temp 0.6 \\\n    --ctx-size 8192 \\\n    --seed 3407 \\\n    --prompt \"<｜User｜>What is 1+1?<｜Assistant｜>\"\ntxt\n <think>\n Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\n Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\n Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\n I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\n Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any...\nbash\n./llama.cpp/llama-cli \\\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --cache-type-k q4_0 \\\n    --threads 12 -no-cnv --prio 2 \\\n    --n-gpu-layers 7 \\\n    --temp 0.6 \\\n    --ctx-size 8192 \\\n    --seed 3407 \\\n    --prompt \"<｜User｜>Create a Flappy Bird game in Python.<｜Assistant｜>\"\n\n<｜User｜>Create a Flappy Bird game in Python. You must include these things:\n1. You must use pygame.\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\n3. Pressing SPACE multiple times will accelerate the bird.\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<｜Assistant｜>\n\n./llama.cpp/llama-cli \\\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --cache-type-k q4_0 \\\n    --threads 12 -no-cnv --prio 2 \\\n    --n-gpu-layers 7 \\\n    --temp 0.6 \\\n    --ctx-size 8192 \\\n    --seed 3407 \\\n    --prompt \"<｜User｜>Create a Flappy Bird game in Python. You must include these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressing SPACE multiple times will accelerate the bird.\\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<｜Assistant｜>\"\n\n./llama.cpp/llama-gguf-split --merge \\\n    DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    merged_file.gguf\n\n./llama.cpp/llama-cli \\\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n    --cache-type-k q4_0 \\\n    --threads 16 \\\n    --prio 2 \\\n    --temp 0.6 \\\n    --ctx-size 8192 \\\n    --seed 3407 \\\n    --n-gpu-layers 59 \\\n    -no-cnv \\\n    --prompt \"<｜User｜>Create a Flappy Bird game in Python.<｜Assistant｜>\"\n\n./llama.cpp/llama-gguf-split --merge \\\n  DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\\n\tmerged_file.gguf\n```\n\n## DeepSeek Chat Template\n\nAll distilled versions and the main 671B R1 model use the same chat template:\n\n`<｜begin▁of▁sentence｜><｜User｜>What is 1+1?<｜Assistant｜>It's 2.<｜end▁of▁sentence｜><｜User｜>Explain more!<｜Assistant｜>`\n\nA BOS is forcibly added, and an EOS separates each interaction. To counteract double BOS tokens during inference, you should only call *tokenizer.encode(..., add\\_special\\_tokens = False)* since the chat template auto adds a BOS token as well.\\\nFor llama.cpp / GGUF inference, you should skip the BOS since it’ll auto add it.\n\n`<｜User｜>What is 1+1?<｜Assistant｜>`\n\nThe \\<think> and \\</think> tokens get their own designated tokens. For the distilled versions for Qwen and Llama, some tokens are re-mapped, whilst Qwen for example did not have a BOS token, so <|object\\_ref\\_start|> had to be used instead.\\\n\\\n**Tokenizer ID Mappings:**\n\n| Token                     | R1     | Distill Qwen | Distill Llama |\n| ------------------------- | ------ | ------------ | ------------- |\n| \\<think>                  | 128798 | 151648       | 128013        |\n| \\</think>                 | 128799 | 151649       | 128014        |\n| <\\|begin\\_of\\_sentence\\|> | 0      | 151646       | 128000        |\n| <\\|end\\_of\\_sentence\\|>   | 1      | 151643       | 128001        |\n| <\\|User\\|>                | 128803 | 151644       | 128011        |\n| <\\|Assistant\\|>           | 128804 | 151645       | 128012        |\n| Padding token             | 2      | 151654       | 128004        |\n\nOriginal tokens in models:\n\n| Token                 | Qwen 2.5 32B Base        | Llama 3.3 70B Instruct            |\n| --------------------- | ------------------------ | --------------------------------- |\n| \\<think>              | <\\|box\\_start\\|>         | <\\|reserved\\_special\\_token\\_5\\|> |\n| \\</think>             | <\\|box\\_end\\|>           | <\\|reserved\\_special\\_token\\_6\\|> |\n| <｜begin▁of▁sentence｜> | <\\|object\\_ref\\_start\\|> | <\\|begin\\_of\\_text\\|>             |\n| <｜end▁of▁sentence｜>   | <\\|endoftext\\|>          | <\\|end\\_of\\_text\\|>               |\n| <｜User｜>              | <\\|im\\_start\\|>          | <\\|reserved\\_special\\_token\\_3\\|> |\n| <｜Assistant｜>         | <\\|im\\_end\\|>            | <\\|reserved\\_special\\_token\\_4\\|> |\n| Padding token         | <\\|vision\\_pad\\|>        | <\\|finetune\\_right\\_pad\\_id\\|>    |\n\nAll Distilled and the original R1 versions seem to have accidentally assigned the padding token to <｜end▁of▁sentence｜>, which is mostly not a good idea, especially if you want to further finetune on top of these reasoning models. This will cause endless infinite generations, since most frameworks will mask the EOS token out as -100.\\\n\\\nWe fixed all distilled and the original R1 versions with the correct padding token (Qwen uses <|vision\\_pad|>, Llama uses <|finetune\\_right\\_pad\\_id|>, and R1 uses <｜▁pad▁｜> or our own added <｜PAD▁TOKEN｜>.\n\n<table data-full-width=\"true\"><thead><tr><th>MoE Bits</th><th>Type</th><th>Disk Size</th><th>Accuracy</th><th>Link</th><th>Details</th></tr></thead><tbody><tr><td>1.58bit</td><td>UD-IQ1_S</td><td><strong>131GB</strong></td><td>Fair</td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S\">Link</a></td><td>MoE all 1.56bit. <code>down_proj</code> in MoE mixture of 2.06/1.56bit</td></tr><tr><td>1.73bit</td><td>UD-IQ1_M</td><td><strong>158GB</strong></td><td>Good</td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M\">Link</a></td><td>MoE all 1.56bit. <code>down_proj</code> in MoE left at 2.06bit</td></tr><tr><td>2.22bit</td><td>UD-IQ2_XXS</td><td><strong>183GB</strong></td><td>Better</td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS\">Link</a></td><td>MoE all 2.06bit. <code>down_proj</code> in MoE mixture of 2.5/2.06bit</td></tr><tr><td>2.51bit</td><td>UD-Q2_K_XL</td><td><strong>212GB</strong></td><td>Best</td><td><a href=\"https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL\">Link</a></td><td>MoE all 2.5bit. <code>down_proj</code> in MoE mixture of 3.5/2.5bit</td></tr></tbody></table>",
  "code_samples": [
    {
      "code": "6. Example with Q4\\_0 K quantized cache **Notice -no-cnv disables auto conversation mode**",
      "language": "unknown"
    },
    {
      "code": "Example output:",
      "language": "unknown"
    },
    {
      "code": "4. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.",
      "language": "unknown"
    },
    {
      "code": "5. To test our Flappy Bird example as mentioned in our blog post here: <https://unsloth.ai/blog/deepseekr1-dynamic>, we can produce the 2nd example like below using our 1.58bit dynamic quant:\n\n<table data-column-title-hidden data-view=\"cards\" data-full-width=\"false\"><thead><tr><th></th><th></th><th></th><th data-hidden data-card-cover data-type=\"files\"></th></tr></thead><tbody><tr><td>Original DeepSeek R1</td><td></td><td></td><td><a href=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-3c484081174c631653c8c7bf7e7674f05255f740%2FInShot_20250127_043158375_H8Uu6tyJXYAFwUEIu04Am.gif?alt=media\">InShot_20250127_043158375_H8Uu6tyJXYAFwUEIu04Am.gif</a></td></tr><tr><td>1.58bit Dynamic Quant</td><td></td><td></td><td><a href=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-c41eac0fea9362017e94123ee8f9793df21b8e97%2FInShot_20250127_042648160_lrtL8-eRhl4qtLaUDSU87.gif?alt=media\">InShot_20250127_042648160_lrtL8-eRhl4qtLaUDSU87.gif</a></td></tr></tbody></table>\n\nThe prompt used is as below:\n\n{% code overflow=\"wrap\" %}",
      "language": "unknown"
    },
    {
      "code": "{% endcode %}\n\nTo call llama.cpp using this example, we do:",
      "language": "unknown"
    },
    {
      "code": "5. Also, if you want to merge the weights together for use in Ollama for example, use this script:",
      "language": "unknown"
    },
    {
      "code": "6. DeepSeek R1 has 61 layers. For example with a 24GB GPU or 80GB GPU, you can expect to offload after rounding down (reduce by 1 if it goes out of memory):\n\n| Quant   | File Size | 24GB GPU | 80GB GPU | 2x80GB GPU    |\n| ------- | --------- | -------- | -------- | ------------- |\n| 1.58bit | 131GB     | 7        | 33       | All layers 61 |\n| 1.73bit | 158GB     | 5        | 26       | 57            |\n| 2.22bit | 183GB     | 4        | 22       | 49            |\n| 2.51bit | 212GB     | 2        | 19       | 32            |\n\n### Running on Mac / Apple devices\n\nFor Apple Metal devices, be careful of --n-gpu-layers. If you find the machine going out of memory, reduce it. For a 128GB unified memory machine, you should be able to offload 59 layers or so.",
      "language": "unknown"
    },
    {
      "code": "### Run in Ollama/Open WebUI\n\nOpen WebUI has made an step-by-step tutorial on how to run R1 here: [docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/](https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/)\\\n\\\nIf you want to use Ollama for inference on GGUFs, you need to first merge the 3 GGUF split files into 1 like the code below. Then you will need to run the model locally.",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "Running on Mac / Apple devices",
      "id": "running-on-mac-/-apple-devices"
    },
    {
      "level": "h3",
      "text": "Run in Ollama/Open WebUI",
      "id": "run-in-ollama/open-webui"
    },
    {
      "level": "h2",
      "text": "DeepSeek Chat Template",
      "id": "deepseek-chat-template"
    },
    {
      "level": "h2",
      "text": "GGUF R1 Table",
      "id": "gguf-r1-table"
    }
  ],
  "url": "llms-txt#os.environ[\"hf_hub_enable_hf_transfer\"]-=-\"1\"",
  "links": []
}