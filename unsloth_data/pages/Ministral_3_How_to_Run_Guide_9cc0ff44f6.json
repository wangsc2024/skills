{
  "title": "Ministral 3 - How to Run Guide",
  "content": "Guide for Mistral Ministral 3 models, to run or fine-tune locally on your device\n\nistral releases Ministral 3, their new multimodal models in Base, Instruct, and Reasoning variants, available in **3B**, **8B**, and **14B** sizes. They offer best-in-class performance for their size, and are fine-tuned for instruction and chat use cases. The multimodal models support **256K context** windows, multiple languages, native function calling, and JSON output.\n\nThe full unquantized 14B Ministral-3-Instruct-2512 model fits in **24GB RAM**/VRAM. You can now run, fine-tune and RL on all Ministral 3 models with Unsloth:\n\n<a href=\"#run-ministral-3-tutorials\" class=\"button primary\">Run Ministral 3 Tutorials</a><a href=\"#fine-tuning\" class=\"button primary\">Fine-tuning Ministral 3</a>\n\nWe've also uploaded Mistral Large 3 [GGUFs here](https://huggingface.co/unsloth/Mistral-Large-3-675B-Instruct-2512-GGUF). For all Ministral 3 uploads (BnB, FP8), [see here](https://huggingface.co/collections/unsloth/ministral-3).\n\n| Ministral-3-Instruct GGUFs:                                                                                                                                                                                               | Ministral-3-Reasoning GGUFs:                                                                                                                                                                                                  |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [3B](https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF) • [8B](https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF) • [14B](https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF) | [3B](https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF) • [8B](https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF) • [14B](https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF) |\n\nTo achieve optimal performance for **Instruct**, Mistral recommends using lower temperatures such as `temperature = 0.15` or `0.1`<br>\n\nFor **Reasoning**, Mistral recommends `temperature = 0.7` and `top_p = 0.95`.\n\n| Instruct:                     | Reasoning:          |\n| ----------------------------- | ------------------- |\n| `Temperature = 0.15` or `0.1` | `Temperature = 0.7` |\n| `Top_P = default`             | `Top_P = 0.95`      |\n\n**Adequate Output Length**: Use an output length of `32,768` tokens for most queries for the reasoning variant, and `16,384` for the instruct variant. You can increase the max output size for the reasoning model if necessary.\n\nThe maximum context length Ministral 3 can reach is `262,144`\n\nThe chat template format is found when we use the below:\n\n{% code overflow=\"wrap\" %}\n\n#### Ministral *Reasoning* chat template:\n\n{% code overflow=\"wrap\" lineNumbers=\"true\" %}\n\n#### Ministral *Instruct* chat template:\n\n{% code overflow=\"wrap\" lineNumbers=\"true\" expandable=\"true\" %}\n\n```\n<s>[SYSTEM_PROMPT]You are Ministral-3-3B-Instruct-2512, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYou power an AI assistant called Le Chat.\nYour knowledge base was last updated on 2023-10-01.\nThe current date is {today}.\n\nWhen you're not sure about some information or when the user's request requires up-to-date or specific data, you must use the available tools to fetch the information. Do not hesitate to use tools whenever they can provide a more accurate or complete response. If no relevant tools are available, then clearly state that you don't have the information and avoid making up anything.\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\").\nYou are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is {yesterday}) and when asked about information at specific dates, you discard information that is at another date.\nYou follow these instructions in all languages, and always respond to the user in the language they use or request.\nNext sections describe the capabilities that you have.",
  "code_samples": [
    {
      "code": "tokenizer.apply_chat_template([\n    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n    {\"role\" : \"assistant\", \"content\" : \"2\"},\n    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"}\n    ], add_generation_prompt = True\n)",
      "language": "python"
    },
    {
      "code": "<s>[SYSTEM_PROMPT]# HOW YOU SHOULD THINK AND ANSWER\n\nFirst draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.\n\nYour thinking process must follow the template below:[THINK]Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response to the user.[/THINK]Here, provide a self-contained response.[/SYSTEM_PROMPT][INST]What is 1+1?[/INST]2</s>[INST]What is 2+2?[/INST]",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "⚙️ Usage Guide",
      "id": "⚙️-usage-guide"
    }
  ],
  "url": "llms-txt#ministral-3---how-to-run-guide",
  "links": []
}