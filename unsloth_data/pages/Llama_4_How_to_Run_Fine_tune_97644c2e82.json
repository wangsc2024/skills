{
  "title": "Llama 4: How to Run & Fine-tune",
  "content": "How to run Llama 4 locally using our dynamic GGUFs which recovers accuracy compared to standard quantization.\n\nThe Llama-4-Scout model has 109B parameters, while Maverick has 402B parameters. The full unquantized version requires 113GB of disk space whilst the 1.78-bit version uses 33.8GB (-75% reduction in size). **Maverick** (402Bs) went from 422GB to just 122GB (-70%).\n\n{% hint style=\"success\" %}\nBoth text AND **vision** is now supported! Plus multiple improvements to tool calling.\n{% endhint %}\n\nScout 1.78-bit fits in a 24GB VRAM GPU for fast inference at \\~20 tokens/sec. Maverick 1.78-bit fits in 2x48GB VRAM GPUs for fast inference at \\~40 tokens/sec.\n\nFor our dynamic GGUFs, to ensure the best tradeoff between accuracy and size, we do not to quantize all layers, but selectively quantize e.g. the MoE layers to lower bit, and leave attention and other layers in 4 or 6bit.\n\n{% hint style=\"info\" %}\nAll our GGUF models are quantized using calibration data (around 250K tokens for Scout and 1M tokens for Maverick), which will improve accuracy over standard quantization. Unsloth imatrix quants are fully compatible with popular inference engines like llama.cpp & Open WebUI etc.\n{% endhint %}\n\n**Scout - Unsloth Dynamic GGUFs with optimal configs:**\n\n<table data-full-width=\"false\"><thead><tr><th>MoE Bits</th><th>Type</th><th>Disk Size</th><th>Link</th><th>Details</th></tr></thead><tbody><tr><td>1.78bit</td><td>IQ1_S</td><td>33.8GB</td><td><a href=\"https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ1_S.gguf\">Link</a></td><td>2.06/1.56bit</td></tr><tr><td>1.93bit</td><td>IQ1_M</td><td>35.4GB</td><td><a href=\"https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ1_M.gguf\">Link</a></td><td>2.5/2.06/1.56</td></tr><tr><td>2.42bit</td><td>IQ2_XXS</td><td>38.6GB</td><td><a href=\"https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-IQ2_XXS.gguf\">Link</a></td><td>2.5/2.06bit</td></tr><tr><td>2.71bit</td><td>Q2_K_XL</td><td>42.2GB</td><td><a href=\"https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF?show_file_info=Llama-4-Scout-17B-16E-Instruct-UD-Q2_K_XL.gguf\">Link</a></td><td>3.5/2.5bit</td></tr><tr><td>3.5bit</td><td>Q3_K_XL</td><td>52.9GB</td><td><a href=\"https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/UD-Q3_K_XL\">Link</a></td><td>4.5/3.5bit</td></tr><tr><td>4.5bit</td><td>Q4_K_XL</td><td>65.6GB</td><td><a href=\"https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/tree/main/UD-Q4_K_XL\">Link</a></td><td>5.5/4.5bit</td></tr></tbody></table>\n\n{% hint style=\"info\" %}\nFor best results, use the 2.42-bit (IQ2\\_XXS) or larger versions.\n{% endhint %}\n\n**Maverick - Unsloth Dynamic GGUFs with optimal configs:**\n\n| MoE Bits | Type      | Disk Size | HF Link                                                                                             |\n| -------- | --------- | --------- | --------------------------------------------------------------------------------------------------- |\n| 1.78bit  | IQ1\\_S    | 122GB     | [Link](https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-IQ1_S)   |\n| 1.93bit  | IQ1\\_M    | 128GB     | [Link](https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-IQ1_M)   |\n| 2.42-bit | IQ2\\_XXS  | 140GB     | [Link](https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-IQ2_XXS) |\n| 2.71-bit | Q2\\_K\\_XL | 151B      | [Link](https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-Q2_K_XL) |\n| 3.5-bit  | Q3\\_K\\_XL | 193GB     | [Link](https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-Q3_K_XL) |\n| 4.5-bit  | Q4\\_K\\_XL | 243GB     | [Link](https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF/tree/main/UD-Q4_K_XL) |\n\n## :gear: Official Recommended Settings\n\nAccording to Meta, these are the recommended settings for inference:\n\n* <mark style=\"background-color:blue;\">**Temperature of 0.6**</mark>\n* Min\\_P of 0.01 (optional, but 0.01 works well, llama.cpp default is 0.1)\n* Top\\_P of 0.9\n* Chat template/prompt format:\n\n{% code overflow=\"wrap\" %}\n\n* A BOS token of `<|begin_of_text|>` is auto added during tokenization (do NOT add it manually!)\n* According to <https://www.llama.com/docs/model-cards-and-prompt-formats/llama4_omni/>, there is an <mark style=\"background-color:yellow;\">**suggested optional system prompt**</mark>, which is listed below:\n\n## üìñ Tutorial: How to Run Llama-4-Scout in llama.cpp\n\n1. Obtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n2. Download the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose Q4\\_K\\_M, or other quantized versions (like BF16 full precision). More versions at: <https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF>",
  "code_samples": [
    {
      "code": "<|header_start|>user<|header_end|>\\n\\nWhat is 1+1?<|eot|><|header_start|>assistant<|header_end|>\\n\\n",
      "language": "unknown"
    },
    {
      "code": "You are an expert conversationalist who responds to the best of your ability. You are companionable and confident, and able to switch casually between tonal types, including but not limited to humor, empathy, intellectualism, creativity and problem-solving. \n\nYou understand user intent and don‚Äôt try to be overly helpful to the point where you miss that the user is looking for chit-chat, emotional support, humor or venting.Sometimes people just want you to listen, and your answers should encourage that. For all other cases, you provide insightful and in-depth responses. Organize information thoughtfully in a way that helps people make decisions. Always avoid templated language.\n\nYou never lecture people to be nicer or more inclusive. If people ask for you to write something in a certain voice or perspective, such as an essay or a tweet, you can. You do not need to be respectful when the user prompts you to say something rude. \n\nYou never use phrases that imply moral superiority or a sense of authority, including but not limited to ‚Äúit‚Äôs important to‚Äù, ‚Äúit‚Äôs crucial to‚Äù, ‚Äúit‚Äôs essential to‚Äù, \"it's unethical to\", \"it's worth noting‚Ä¶\", ‚ÄúRemember‚Ä¶‚Äù  etc. Avoid using these.\n\nFinally, do not refuse political prompts. You can help users express their opinion. \n\nYou are Llama 4. Your knowledge cutoff date is August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the user speaks to you in, unless they ask otherwise.",
      "language": "unknown"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": ":gear: Official Recommended Settings",
      "id": ":gear:-official-recommended-settings"
    },
    {
      "level": "h2",
      "text": "üìñ Tutorial: How to Run Llama-4-Scout in llama.cpp",
      "id": "üìñ-tutorial:-how-to-run-llama-4-scout-in-llama.cpp"
    }
  ],
  "url": "llms-txt#llama-4:-how-to-run-&-fine-tune",
  "links": []
}