{
  "title": "Unsloth Dynamic 2.0 GGUFs",
  "content": "A big new upgrade to our Dynamic Quants!\n\nWe're excited to introduce our Dynamic v2.0 quantization method - a major upgrade to our previous quants. This new method outperforms leading quantization methods and sets new benchmarks for 5-shot MMLU and KL Divergence.\n\nThis means you can now run + fine-tune quantized LLMs while preserving as much accuracy as possible! You can run the 2.0 GGUFs on any inference engine like llama.cpp, Ollama, Open WebUI etc.\n\n{% hint style=\"success\" %}\n[**Sept 10, 2025 update:**](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot) You asked for tougher benchmarks, so we‚Äôre showcasing Aider Polyglot results! Our Dynamic 3-bit DeepSeek V3.1 GGUF scores **75.6%**, surpassing many full-precision SOTA LLMs. [Read more.](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)\n\nThe **key advantage** of using the Unsloth package and models is our active role in ***fixing critical bugs*** in major models. We've collaborated directly with teams behind [Qwen3](https://www.reddit.com/r/LocalLLaMA/comments/1kaodxu/qwen3_unsloth_dynamic_ggufs_128k_context_bug_fixes/), [Meta (Llama 4)](https://github.com/ggml-org/llama.cpp/pull/12889), [Mistral (Devstral)](https://app.gitbook.com/o/HpyELzcNe0topgVLGCZY/s/xhOjnexMCB3dmuQFQ2Zq/~/changes/618/basics/tutorials-how-to-fine-tune-and-run-llms/devstral-how-to-run-and-fine-tune), [Google (Gemma 1‚Äì3)](https://news.ycombinator.com/item?id=39671146) and [Microsoft (Phi-3/4)](https://simonwillison.net/2025/Jan/11/phi-4-bug-fixes), contributing essential fixes that significantly boost accuracy.\n{% endhint %}\n\nDetailed analysis of our benchmarks and evaluation further below.\n\n<div><figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-a114143bdd47add988182aabf9313ab40be38d7d%2Faider%20thinking.png?alt=media\" alt=\"\" width=\"563\"><figcaption><p>Thinking Aider Benchmarks</p></figcaption></figure> <figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-76662317725a3b76fb1e5e33b586c86e712bee6f%2F5shotmmlu.png?alt=media\" alt=\"\" width=\"563\"><figcaption><p>5-shot MMLU Benchmarks</p></figcaption></figure></div>\n\n### üí° What's New in Dynamic v2.0?\n\n* **Revamped Layer Selection for GGUFs + safetensors:** Unsloth Dynamic 2.0 now selectively quantizes layers much more intelligently and extensively. Rather than modifying only select layers, we now dynamically adjust the quantization type of every possible layer, and the combinations will differ for each layer and model.\n* Current selected and all future GGUF uploads will utilize Dynamic 2.0 and our new calibration dataset. The dataset contains more than >1.5M **tokens** (depending on model) and comprise of high-quality, hand-curated and cleaned data - to greatly enhance conversational chat performance.\n* Previously, our Dynamic quantization (DeepSeek-R1 1.58-bit GGUF) was effective only for MoE architectures. <mark style=\"background-color:green;\">**Dynamic 2.0 quantization now works on all models (including MOEs & non-MoEs)**</mark>.\n* **Model-Specific Quants:** Each model now uses a custom-tailored quantization scheme. E.g. the layers quantized in Gemma 3 differ significantly from those in Llama 4.\n* To maximize efficiency, especially on Apple Silicon and ARM devices, we now also add Q4\\_NL, Q5.1, Q5.0, Q4.1, and Q4.0 formats.\n\nTo ensure accurate benchmarking, we built an internal evaluation framework to match official reported 5-shot MMLU scores of Llama 4 and Gemma 3. This allowed apples-to-apples comparisons between full-precision vs. Dynamic v2.0, **QAT** and standard **imatrix** GGUF quants.\n\n<div><figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-fd0a92a2bea8efa37b71946ea934a22f00589f40%2Fkldivergence%20graph.png?alt=media\" alt=\"\" width=\"563\"><figcaption></figcaption></figure> <figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-76662317725a3b76fb1e5e33b586c86e712bee6f%2F5shotmmlu.png?alt=media\" alt=\"\" width=\"563\"><figcaption></figcaption></figure></div>\n\nAll future GGUF uploads will utilize Unsloth Dynamic 2.0, and our Dynamic 4-bit safe tensor quants will also benefit from this in the future.\n\n## üìä Why KL Divergence?\n\n[Accuracy is Not All You Need](https://arxiv.org/pdf/2407.09141) showcases how pruning layers, even by selecting unnecessary ones still yields vast differences in terms of \"flips\". A \"flip\" is defined as answers changing from incorrect to correct or vice versa. The paper shows how MMLU might not decrease as we prune layers or do quantization,but that's because some incorrect answers might have \"flipped\" to become correct. Our goal is to match the original model, so measuring \"flips\" is a good metric.\n\n<div><figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-5a97c101b0df31fb49df20ce4241930897098cf8%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure> <figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-e4a60354ad8613b6f2361f63fa82c552e00fdda9%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure></div>\n\n{% hint style=\"info\" %}\n**KL Divergence** should be the **gold standard for reporting quantization errors** as per the research paper \"Accuracy is Not All You Need\". **Using perplexity is incorrect** since output token values can cancel out, so we must use KLD!\n{% endhint %}\n\nThe paper also shows that interestingly KL Divergence is highly correlated with flips, and so our goal is to reduce the mean KL Divergence whilst increasing the disk space of the quantization as less as possible.\n\n## ‚öñÔ∏è Calibration Dataset Overfitting\n\nMost frameworks report perplexity and KL Divergence using a test set of Wikipedia articles. However, we noticed using the calibration dataset which is also Wikipedia related causes quants to overfit, and attain lower perplexity scores. We utilize [Calibration\\_v3](https://gist.github.com/bartowski1182/eb213dccb3571f863da82e99418f81e8) and [Calibration\\_v5](https://gist.github.com/tristandruyen/9e207a95c7d75ddf37525d353e00659c/) datasets for fair testing which includes some wikitext data amongst other data. <mark style=\"background-color:red;\">**Also instruct models have unique chat templates, and using text only calibration datasets is not effective for instruct models**</mark> (base models yes). In fact most imatrix GGUFs are typically calibrated with these issues. As a result, they naturally perform better on KL Divergence benchmarks that also use Wikipedia data, since the model is essentially optimized for that domain.\n\nTo ensure a fair and controlled evaluation, we do not to use our own calibration dataset (which is optimized for chat performance) when benchmarking KL Divergence. Instead, we conducted tests using the same standard Wikipedia datasets, allowing us to directly compare the performance of our Dynamic 2.0 method against the baseline imatrix approach.\n\n## :1234: MMLU Replication Adventure\n\n* Replicating MMLU 5 shot was nightmarish. We <mark style=\"background-color:red;\">**could not**</mark> replicate MMLU results for many models including Llama 3.1 (8B) Instruct, Gemma 3 (12B) and others due to <mark style=\"background-color:yellow;\">**subtle implementation issues**</mark>. Llama 3.1 (8B) for example should be getting \\~68.2%, whilst using incorrect implementations can attain <mark style=\"background-color:red;\">**35% accuracy.**</mark>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-cc2b4b2bc512b3c9bc065250930259b9b9a9fce0%2FMMLU%20differences.png?alt=media\" alt=\"\" width=\"375\"><figcaption><p>MMLU implementation issues</p></figcaption></figure>\n\n* Llama 3.1 (8B) Instruct has a MMLU 5 shot accuracy of 67.8% using a naive MMLU implementation. We find however Llama **tokenizes \"A\" and \"\\_A\" (A with a space in front) as different token ids**. If we consider both spaced and non spaced tokens, we get 68.2% <mark style=\"background-color:green;\">(+0.4%)</mark>\n* Interestingly Llama 3 as per Eleuther AI's [LLM Harness](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/llama3/instruct/mmlu/_continuation_template_yaml) also appends <mark style=\"background-color:purple;\">**\"The best answer is\"**</mark> to the question, following Llama 3's original MMLU benchmarks.\n* There are many other subtle issues, and so to benchmark everything in a controlled environment, we designed our own MMLU implementation from scratch by investigating [github.com/hendrycks/test](https://github.com/hendrycks/test) directly, and verified our results across multiple models and comparing to reported numbers.\n\n## :sparkles: Gemma 3 QAT Replication, Benchmarks\n\nThe Gemma team released two QAT (quantization aware training) versions of Gemma 3:\n\n1. Q4\\_0 GGUF - Quantizes all layers to Q4\\_0 via the formula `w = q * block_scale` with each block having 32 weights. See [llama.cpp wiki ](https://github.com/ggml-org/llama.cpp/wiki/Tensor-Encoding-Schemes)for more details.\n2. int4 version - presumably [TorchAO int4 style](https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md)?\n\nWe benchmarked all Q4\\_0 GGUF versions, and did extensive experiments on the 12B model. We see the **12B Q4\\_0 QAT model gets 67.07%** whilst the full bfloat16 12B version gets 67.15% on 5 shot MMLU. That's very impressive! The 27B model is mostly nearly there!\n\n<table><thead><tr><th>Metric</th><th>1B</th><th valign=\"middle\">4B</th><th>12B</th><th>27B</th></tr></thead><tbody><tr><td>MMLU 5 shot</td><td>26.12%</td><td valign=\"middle\">55.13%</td><td><mark style=\"background-color:blue;\"><strong>67.07% (67.15% BF16)</strong></mark></td><td><strong>70.64% (71.5% BF16)</strong></td></tr><tr><td>Disk Space</td><td>0.93GB</td><td valign=\"middle\">2.94GB</td><td><strong>7.52GB</strong></td><td>16.05GB</td></tr><tr><td><mark style=\"background-color:green;\"><strong>Efficiency*</strong></mark></td><td>1.20</td><td valign=\"middle\">10.26</td><td><strong>5.59</strong></td><td>2.84</td></tr></tbody></table>\n\nWe designed a new **Efficiency metric** which calculates the usefulness of the model whilst also taking into account its disk size and MMLU 5 shot score:\n\n$$\n\\text{Efficiency} = \\frac{\\text{MMLU 5 shot score} - 25}{\\text{Disk Space GB}}\n$$\n\n{% hint style=\"warning\" %}\nWe have to **minus 25** since MMLU has 4 multiple choices - A, B, C or D. Assume we make a model that simply randomly chooses answers - it'll get 25% accuracy, and have a disk space of a few bytes. But clearly this is not a useful model.\n{% endhint %}\n\nOn KL Divergence vs the base model, below is a table showcasing the improvements. Reminder the closer the KL Divergence is to 0, the better (ie 0 means identical to the full precision model)\n\n| Quant     | Baseline KLD | GB    | New KLD  | GB    |\n| --------- | ------------ | ----- | -------- | ----- |\n| IQ1\\_S    | 1.035688     | 5.83  | 0.972932 | 6.06  |\n| IQ1\\_M    | 0.832252     | 6.33  | 0.800049 | 6.51  |\n| IQ2\\_XXS  | 0.535764     | 7.16  | 0.521039 | 7.31  |\n| IQ2\\_M    | 0.26554      | 8.84  | 0.258192 | 8.96  |\n| Q2\\_K\\_XL | 0.229671     | 9.78  | 0.220937 | 9.95  |\n| Q3\\_K\\_XL | 0.087845     | 12.51 | 0.080617 | 12.76 |\n| Q4\\_K\\_XL | 0.024916     | 15.41 | 0.023701 | 15.64 |\n\nIf we plot the ratio of the disk space increase and the KL Divergence ratio change, we can see a much clearer benefit! Our dynamic 2bit Q2\\_K\\_XL reduces KLD quite a bit (around 7.5%).\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-5b352d0449e723556e6e871396c2ee78ae8ec3dc%2Fchart(2).svg?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nTruncated table of results for MMLU for Gemma 3 (27B). See below.\n\n1. **Our dynamic 4bit version is 2GB smaller whilst having +1% extra accuracy vs the QAT version!**\n2. Efficiency wise, 2bit Q2\\_K\\_XL and others seem to do very well!\n\n| Quant          | Unsloth   | Unsloth + QAT | Disk Size | Efficiency |\n| -------------- | --------- | ------------- | --------- | ---------- |\n| IQ1\\_M         | 48.10     | 47.23         | 6.51      | 3.42       |\n| IQ2\\_XXS       | 59.20     | 56.57         | 7.31      | 4.32       |\n| IQ2\\_M         | 66.47     | 64.47         | 8.96      | 4.40       |\n| Q2\\_K\\_XL      | 68.70     | 67.77         | 9.95      | 4.30       |\n| Q3\\_K\\_XL      | 70.87     | 69.50         | 12.76     | 3.49       |\n| **Q4\\_K\\_XL**  | **71.47** | **71.07**     | **15.64** | **2.94**   |\n| **Google QAT** |           | **70.64**     | **17.2**  | **2.65**   |\n\n<summary><mark style=\"color:green;\">Click here</mark> for Full Google's Gemma 3 (27B) QAT Benchmarks:</summary>\n\n| Model          | Unsloth   | Unsloth + QAT | Disk Size | Efficiency |\n| -------------- | --------- | ------------- | --------- | ---------- |\n| IQ1\\_S         | 41.87     | 43.37         | 6.06      | 3.03       |\n| IQ1\\_M         | 48.10     | 47.23         | 6.51      | 3.42       |\n| IQ2\\_XXS       | 59.20     | 56.57         | 7.31      | 4.32       |\n| IQ2\\_M         | 66.47     | 64.47         | 8.96      | 4.40       |\n| Q2\\_K          | 68.50     | 67.60         | 9.78      | 4.35       |\n| Q2\\_K\\_XL      | 68.70     | 67.77         | 9.95      | 4.30       |\n| IQ3\\_XXS       | 68.27     | 67.07         | 10.07     | 4.18       |\n| Q3\\_K\\_M       | 70.70     | 69.77         | 12.51     | 3.58       |\n| Q3\\_K\\_XL      | 70.87     | 69.50         | 12.76     | 3.49       |\n| Q4\\_K\\_M       | 71.23     | 71.00         | 15.41     | 2.98       |\n| **Q4\\_K\\_XL**  | **71.47** | **71.07**     | **15.64** | **2.94**   |\n| Q5\\_K\\_M       | 71.77     | 71.23         | 17.95     | 2.58       |\n| Q6\\_K          | 71.87     | 71.60         | 20.64     | 2.26       |\n| Q8\\_0          | 71.60     | 71.53         | 26.74     | 1.74       |\n| **Google QAT** |           | **70.64**     | **17.2**  | **2.65**   |\n\n## :llama: Llama 4 Bug Fixes + Run\n\nWe also helped and fixed a few Llama 4 bugs:\n\n* Llama 4 Scout changed the RoPE Scaling configuration in their official repo. We helped resolve issues in llama.cpp to enable this [change here](https://github.com/ggml-org/llama.cpp/pull/12889)\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-7ff8229dfa96425f50c2c87f9ca988ef9cc99eff%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n* Llama 4's QK Norm's epsilon for both Scout and Maverick should be from the config file - this means using 1e-05 and not 1e-06. We helped resolve these in [llama.cpp](https://github.com/ggml-org/llama.cpp/pull/12889) and [transformers](https://github.com/huggingface/transformers/pull/37418)\n* The Llama 4 team and vLLM also independently fixed an issue with QK Norm being shared across all heads (should not be so) [here](https://github.com/vllm-project/vllm/pull/16311). MMLU Pro increased from 68.58% to 71.53% accuracy.\n* [Wolfram Ravenwolf](https://x.com/WolframRvnwlf/status/1909735579564331016) showcased how our GGUFs via llama.cpp attain much higher accuracy than third party inference providers - this was most likely a combination of the issues explained above, and also probably due to quantization issues.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-76c49d8c8e3e42f7407f431a2cede369f87878e4%2FGoC79hYXwAAPTMs.jpg?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nAs shown in our graph, our 4-bit Dynamic QAT quantization deliver better performance on 5-shot MMLU while also being smaller in size.\n\n### Running Llama 4 Scout:\n\nTo run Llama 4 Scout for example, first clone llama.cpp:\n\nThen download out new dynamic v 2.0 quant for Scout:",
  "code_samples": [
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggml-org/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h3",
      "text": "üí° What's New in Dynamic v2.0?",
      "id": "üí°-what's-new-in-dynamic-v2.0?"
    },
    {
      "level": "h2",
      "text": "üìä Why KL Divergence?",
      "id": "üìä-why-kl-divergence?"
    },
    {
      "level": "h2",
      "text": "‚öñÔ∏è Calibration Dataset Overfitting",
      "id": "‚öñÔ∏è-calibration-dataset-overfitting"
    },
    {
      "level": "h2",
      "text": ":1234: MMLU Replication Adventure",
      "id": ":1234:-mmlu-replication-adventure"
    },
    {
      "level": "h2",
      "text": ":sparkles: Gemma 3 QAT Replication, Benchmarks",
      "id": ":sparkles:-gemma-3-qat-replication,-benchmarks"
    },
    {
      "level": "h2",
      "text": ":llama: Llama 4 Bug Fixes + Run",
      "id": ":llama:-llama-4-bug-fixes-+-run"
    },
    {
      "level": "h3",
      "text": "Running Llama 4 Scout:",
      "id": "running-llama-4-scout:"
    }
  ],
  "url": "llms-txt#unsloth-dynamic-2.0-ggufs",
  "links": []
}