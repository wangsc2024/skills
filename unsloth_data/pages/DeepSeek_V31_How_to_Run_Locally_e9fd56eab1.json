{
  "title": "DeepSeek-V3.1: How to Run Locally",
  "content": "A guide on how to run DeepSeek-V3.1 and Terminus on your own local device!\n\nDeepSeek‚Äôs V3.1 and **Terminus** update introduces hybrid reasoning inference, combining 'think' and 'non-think' into one model. The full 671B parameter model requires 715GB of disk space. The quantized dynamic 2-bit version uses 245GB (-75% reduction in size). GGUF: [**DeepSeek-V3.1-GGUF**](https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF)\n\n{% hint style=\"success\" %}\n**NEW:** DeepSeek-V3.1-Terminus out now: [DeepSeek-V3.1-Terminus-GGUF](https://huggingface.co/unsloth/DeepSeek-V3.1-Terminus-GGUF)\\\n\\\n[**Sept 10, 2025 update:**](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot) You asked for tougher benchmarks, so we‚Äôre showcasing Aider Polyglot results! Our Dynamic 3-bit DeepSeek V3.1 GGUF scores **75.6%**, surpassing many full-precision SOTA LLMs. [Read more.](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot)\n\nOur DeepSeek-V3.1 GGUFs include Unsloth [chat template fixes](#chat-template-bug-fixes) for llama.cpp supported backends.\n{% endhint %}\n\nAll uploads use Unsloth [Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs) for SOTA 5-shot MMLU and KL Divergence performance, meaning you can run & fine-tune quantized DeepSeek LLMs with minimal accuracy loss.\n\n**Tutorials navigation:**\n\n<a href=\"#run-in-llama.cpp\" class=\"button secondary\">Run in llama.cpp</a><a href=\"#run-in-ollama-open-webui\" class=\"button secondary\">Run in Ollama/Open WebUI</a>\n\n## :gear: Recommended Settings\n\nThe 1-bit dynamic quant TQ1\\_0 (1bit for unimportant MoE layers, 2-4bit for important MoE, and 6-8bit for rest) uses 170GB of disk space - this works well in a **1x24GB card and 128GB of RAM** with MoE offloading - it also **works natively in Ollama**!\n\n{% hint style=\"info\" %}\nYou must use `--jinja` for llama.cpp quants - this uses our [fixed chat templates](#chat-template-bug-fixes) and enables the correct template! You might get incorrect results if you do not use `--jinja`\n{% endhint %}\n\nThe 2-bit quants will fit in a 1x 24GB GPU (with MoE layers offloaded to RAM). Expect around 5 tokens/s with this setup if you have bonus 128GB RAM as well. It is recommended to have at least 226GB RAM to run this 2-bit. For optimal performance you will need at least 226GB unified memory or 226GB combined RAM+VRAM for 5+ tokens/s. To learn how to increase generation speed and fit longer contexts, [read here](#improving-generation-speed).\n\n{% hint style=\"success\" %}\nThough not a must, for best performance, have your VRAM + RAM combined equal to the size of the quant you're downloading. If not, hard drive / SSD offloading will work with llama.cpp, just inference will be slower.\n{% endhint %}\n\n## :butterfly:Chat template bug fixes\n\nWe fixed a few issues with DeepSeek V3.1's chat template since they did not function correctly in llama.cpp and other engines:\n\n1. DeepSeek V3.1 is a hybrid reasoning model, meaning you can change the chat template to enable reasoning. The chat template introduced `thinking = True` , but other models use `enable_thinking = True` . We added the option to use `enable_thinking` as a keyword instead.\n2. llama.cpp's jinja renderer via [minja](https://github.com/google/minja) does not allow the use of extra arguments in the `.split()` command, so using `.split(text, 1)` works in Python, but not in minja. We had to change this to make llama.cpp function correctly without erroring out.\\\n   \\\n   You will get the following error when using other quants:\\\n   `terminate called after throwing an instance of 'std::runtime_error' what(): split method must have between 1 and 1 positional arguments and between 0 and 0 keyword arguments at row 3, column 1908` We fixed it in all our quants!\n\n### üê≥Official Recommended Settings\n\nAccording to [DeepSeek](https://huggingface.co/deepseek-ai/DeepSeek-V3.1), these are the recommended settings for V3.1 inference:\n\n* Set the <mark style=\"background-color:green;\">**temperature 0.6**</mark> to reduce repetition and incoherence.\n* Set <mark style=\"background-color:green;\">**top\\_p to 0.95**</mark> (recommended)\n* **128K context length** or less\n* Use `--jinja` for llama.cpp variants - we **fixed some chat template issues as well!**\n* **Use** `enable_thinking = True` to use reasoning/ thinking mode. By default it's set to non reasoning.\n\n#### :1234: Chat template/prompt format\n\nYou do not need to force `<think>\\n` , but you can still add it in! With the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3, it introduces an additional token `</think>`.\n\nA BOS is forcibly added, and an EOS separates each interaction. To counteract double BOS tokens during inference, you should only call `tokenizer.encode(..., add_special_tokens = False)` since the chat template auto adds a BOS token as well. For llama.cpp / GGUF inference, you should skip the BOS since it‚Äôll auto add it.\n\n#### :notebook\\_with\\_decorative\\_cover: Non-Thinking Mode (use `thinking = False`or `enable_thinking = False` and is by default)\n\nPrefix: `<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>`\n\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3, it introduces an additional token `</think>`.\n\nContext: `<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>`\n\nPrefix: `<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>`\n\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\n\n#### :books: Thinking Mode (use `thinking = True`or `enable_thinking = True` and is by default)\n\nPrefix: `<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>`\n\nThe prefix of thinking mode is similar to DeepSeek-R1.\n\nContext: `<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>`\n\nPrefix: `<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>`\n\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the `</think>` is retained in every turn of context.\n\n#### :bow\\_and\\_arrow: Tool Calling\n\nTool calling is supported in non-thinking mode. The format is:\n\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}{tool_description}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>` where we populate the tool\\_description is area after the system prompt.\n\n## :arrow\\_forward:Run DeepSeek-V3.1 Tutorials:\n\n### :llama: Run in Ollama/Open WebUI\n\n{% stepper %}\n{% step %}\nInstall `ollama` if you haven't already! To run more variants of the model, [see here](#run-in-llama.cpp).\n\n{% step %}\nRun the model! Note you can call `ollama serve`in another terminal if it fails! We include all our fixes and suggested parameters (temperature etc) in `params` in our Hugging Face upload!\\\n\\&#xNAN;**(NEW) To run the full R1-0528 model in Ollama, you can use our TQ1\\_0 (170GB quant):**\n\n{% step %}\nTo run other quants, you need to first merge the GGUF split files into 1 like the code below. Then you will need to run the model locally.\n\n{% step %}\nOpen WebUI also made a [step-by-step tutorial](https://docs.openwebui.com/tutorials/integrations/deepseekr1-dynamic/) on how to run R1 and for V3.1, you will just need to replace R1 with the new V3.1 quant.\n{% endstep %}\n{% endstepper %}\n\n### ‚ú® Run in llama.cpp\n\n{% stepper %}\n{% step %}\nObtain the latest `llama.cpp` on [GitHub here](https://github.com/ggml-org/llama.cpp). You can follow the build instructions below as well. Change `-DGGML_CUDA=ON` to `-DGGML_CUDA=OFF` if you don't have a GPU or just want CPU inference.\n\n{% step %}\nIf you want to use `llama.cpp` directly to load models, you can do the below: (:Q2\\_K\\_XL) is the quantization type. You can also download via Hugging Face (point 3). This is similar to `ollama run` . Use `export LLAMA_CACHE=\"folder\"` to force `llama.cpp` to save to a specific location. Remember the model has only a maximum of 128K context length.\n\n{% hint style=\"success\" %}\nPlease try out `-ot \".ffn_.*_exps.=CPU\"` to offload all MoE layers to the CPU! This effectively allows you to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression to fit more layers if you have more GPU capacity.\n\nIf you have a bit more GPU memory, try `-ot \".ffn_(up|down)_exps.=CPU\"` This offloads up and down projection MoE layers.\n\nTry `-ot \".ffn_(up)_exps.=CPU\"` if you have even more GPU memory. This offloads only up projection MoE layers.\n\nAnd finally offload all layers via `-ot \".ffn_.*_exps.=CPU\"` This uses the least VRAM.\n\nYou can also customize the regex, for example `-ot \"\\.(6|7|8|9|[0-9][0-9]|[0-9][0-9][0-9])\\.ffn_(gate|up|down)_exps.=CPU\"` means to offload gate, up and down MoE layers but only from the 6th layer onwards.\n{% endhint %}\n\n{% step %}\nDownload the model via (after installing `pip install huggingface_hub hf_transfer` ). You can choose `UD-`Q2\\_K\\_XL (dynamic 2bit quant) or other quantized versions like `Q4_K_M` . We <mark style=\"background-color:green;\">**recommend using our 2.7bit dynamic quant**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**`UD-Q2_K_XL`**</mark><mark style=\"background-color:green;\">**&#x20;**</mark><mark style=\"background-color:green;\">**to balance size and accuracy**</mark>.",
  "code_samples": [
    {
      "code": "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>",
      "language": "unknown"
    },
    {
      "code": "apt-get update\napt-get install pciutils -y\ncurl -fsSL https://ollama.com/install.sh | sh",
      "language": "bash"
    },
    {
      "code": "OLLAMA_MODELS=unsloth ollama serve &\n\nOLLAMA_MODELS=unsloth ollama run hf.co/unsloth/DeepSeek-V3.1-Terminus-GGUF:TQ1_0",
      "language": "unknown"
    },
    {
      "code": "./llama.cpp/llama-gguf-split --merge \\\n  DeepSeek-V3.1-Terminus-GGUF/DeepSeek-V3.1-Terminus-UD-Q2_K_XL/DeepSeek-V3.1-Terminus-UD-Q2_K_XL-00001-of-00006.gguf \\\n\tmerged_file.gguf",
      "language": "bash"
    },
    {
      "code": "OLLAMA_MODELS=unsloth ollama serve &\n\nOLLAMA_MODELS=unsloth ollama run merged_file.gguf",
      "language": "bash"
    },
    {
      "code": "apt-get update\napt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\\n    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split llama-mtmd-cli llama-server\ncp llama.cpp/build/bin/llama-* llama.cpp",
      "language": "bash"
    },
    {
      "code": "export LLAMA_CACHE=\"unsloth/DeepSeek-V3.1-GGUF\"\n./llama.cpp/llama-cli \\\n    -hf unsloth/DeepSeek-V3.1-Terminus-GGUF:UD-Q2_K_XL \\\n    --jinja \\\n    --n-gpu-layers 99 \\\n    --temp 0.6 \\\n    --top-p 0.95 \\\n    --min-p 0.01 \\\n    --ctx-size 16384 \\\n    --seed 3407 \\\n    -ot \".ffn_.*_exps.=CPU\"",
      "language": "bash"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": ":gear: Recommended Settings",
      "id": ":gear:-recommended-settings"
    },
    {
      "level": "h2",
      "text": ":butterfly:Chat template bug fixes",
      "id": ":butterfly:chat-template-bug-fixes"
    },
    {
      "level": "h3",
      "text": "üê≥Official Recommended Settings",
      "id": "üê≥official-recommended-settings"
    },
    {
      "level": "h2",
      "text": ":arrow\\_forward:Run DeepSeek-V3.1 Tutorials:",
      "id": ":arrow\\_forward:run-deepseek-v3.1-tutorials:"
    },
    {
      "level": "h3",
      "text": ":llama: Run in Ollama/Open WebUI",
      "id": ":llama:-run-in-ollama/open-webui"
    },
    {
      "level": "h3",
      "text": "‚ú® Run in llama.cpp",
      "id": "‚ú®-run-in-llama.cpp"
    }
  ],
  "url": "llms-txt#deepseek-v3.1:-how-to-run-locally",
  "links": []
}