{
  "title": "Add LoRA adapter to the model for parameter efficient fine tuning",
  "content": "model = FastVisionModel.get_peft_model(\n    model,\n\nfinetune_vision_layers     = False,# fast_inference doesn't support finetune_vision_layers yet :(\n    finetune_language_layers   = True, # False if not finetuning language layers\n    finetune_attention_modules = True, # False if not finetuning attention layers\n    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n\nr = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    lora_alpha = lora_rank*2, # *2 speeds up training\n    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n    random_state = 3407,\n)\n\naddCriterion\n <tool_call>\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n\\n addCriterion\\n\\n 自动生成\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n addCriterion\\n\\n\\n addCriterion\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\n\nFigure is an overhead view of the path taken by a race car driver as his car collides with the racetrack wall. Just before the collision, he is traveling at speed $v_i=70 \\mathrm{~m} / \\mathrm{s}$ along a straight line at $30^{\\circ}$ from the wall. Just after the collision, he is traveling at speed $v_f=50 \\mathrm{~m} / \\mathrm{s}$ along a straight line at $10^{\\circ}$ from the wall. His mass $m$ is $80 \\mathrm{~kg}$. The collision lasts for $14 \\mathrm{~ms}$. What is the magnitude of the average force on the driver during the collision?\npython\ndef formatting_reward_func(completions,**kwargs):\n    import re\n    thinking_pattern = f'{REASONING_START}(.*?){REASONING_END}'\n    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n\nscores = []\n    for completion in completions:\n        score = 0\n        thinking_matches = re.findall(thinking_pattern, completion, re.DOTALL)\n        answer_matches = re.findall(answer_pattern, completion, re.DOTALL)\n        if len(thinking_matches) == 1:\n            score += 1.0\n        if len(answer_matches) == 1:\n            score += 1.0\n\n# Fix up addCriterion issues\n        # See https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl#qwen-2.5-vl-vision-rl-issues-and-quirks\n        # Penalize on excessive addCriterion and newlines\n        if len(completion) != 0:\n            removal = completion.replace(\"addCriterion\", \"\").replace(\"\\n\", \"\")\n            if (len(completion)-len(removal))/len(completion) >= 0.5:\n                score -= 2.0\n\nscores.append(score)\n    return scores\npython\ntraining_args = GRPOConfig(\n    output_dir = \"vlm-grpo-unsloth\",\n    per_device_train_batch_size = 8,\n    gradient_accumulation_steps = 4,\n    learning_rate = 5e-6,\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.99,\n    weight_decay = 0.1,\n    warmup_ratio = 0.1,\n    lr_scheduler_type = \"cosine\",\n    optim = \"adamw_8bit\",\n    # beta = 0.00,\n    epsilon = 3e-4,\n    epsilon_high = 4e-4,\n    num_generations = 8,    \n    max_prompt_length = 1024,\n    max_completion_length = 1024,\n    log_completions = False,\n    max_grad_norm = 0.1,\n    temperature = 0.9,\n    # report_to = \"none\", # Set to \"wandb\" if you want to log to Weights & Biases\n    num_train_epochs = 2, # For a quick test run, increase for full training\n    report_to = \"none\"\n    \n    # GSPO is below:\n    importance_sampling_level = \"sequence\",\n    \n    # Dr GRPO / GAPO etc\n    loss_type = \"dr_grpo\",\n)\n```\n\nOverall, Unsloth now with VLM vLLM fast inference enables for both 90% reduced memory usage but also 1.5-2x faster speed with GRPO and GSPO!\n\nIf you'd like to read more about reinforcement learning, check out out RL guide:\n\n[](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide \"mention\")\n\n***Authors:** A huge thank you to* [*Keith*](https://www.linkedin.com/in/keith-truongcao-7bb84a23b/) *and* [*Datta*](https://www.linkedin.com/in/datta0/) *for contributing to this article!*",
  "code_samples": [
    {
      "code": "## :butterfly:Qwen 2.5 VL Vision RL Issues and Quirks\n\nDuring RL for Qwen 2.5 VL, you might see the following inference output:\n\n{% code overflow=\"wrap\" %}",
      "language": "unknown"
    },
    {
      "code": "{% endcode %}\n\nThis was [reported](https://github.com/QwenLM/Qwen2.5-VL/issues/759) as well in Qwen2.5-VL-7B-Instruct output unexpected results \"addCriterion\". In fact we see this as well! We tried both non Unsloth, bfloat16 and float16 machines and other things, but it appears still. For example item 165 ie `train_dataset[165]` from the [AI4Math/MathVista](https://huggingface.co/datasets/AI4Math/MathVista) dataset is below:\n\n{% code overflow=\"wrap\" %}",
      "language": "unknown"
    },
    {
      "code": "{% endcode %}\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-61a659529171fcc10ed6398a15912b21d6b1a076%2FUntitled.png?alt=media\" alt=\"\" width=\"128\"><figcaption></figcaption></figure>\n\nAnd then we get the above gibberish output. One could add a reward function to penalize the addition of addCriterion, or penalize gibberish outputs. However, the other approach is to train it for longer. For example only after 60 steps ish do we see the model actually learning via RL:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-5f34f66f0ac6508fd28343b16592c59b889ec5ca%2Fimage.webp?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\n{% hint style=\"success\" %}\nForcing `<|assistant|>` during generation will reduce the occurrences of these gibberish results as expected since this is an Instruct model, however it's still best to add a reward function to penalize bad generations, as described in the next section.\n{% endhint %}\n\n## :medal:Reward Functions to reduce gibberish\n\nTo penalize `addCriterion` and gibberish outputs, we edited the reward function to penalize too much of `addCriterion` and newlines.",
      "language": "unknown"
    },
    {
      "code": "## :checkered\\_flag:GSPO Reinforcement Learning\n\nThis update in addition adds GSPO ([Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)) which is a variant of GRPO made by the Qwen team at Alibaba. They noticed that GRPO implicitly results in importance weights for each token, even though explicitly advantages do not scale or change with each token.\n\nThis lead to the creation of GSPO, which now assigns the importance on the sequence likelihood rather than the individual token likelihoods of the tokens. The difference between these two algorithms can be seen below, both from the GSPO paper from Qwen and Alibaba:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-45d743dd5dcd590626777ce09cfab61808aa8c24%2Fimage.png?alt=media\" alt=\"\" width=\"563\"><figcaption><p>GRPO Algorithm, Source: <a href=\"https://arxiv.org/abs/2507.18071\">Qwen</a></p></figcaption></figure>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-ee755850cbe17482ce240dde227d55c62e9a3e64%2Fimage.png?alt=media\" alt=\"\" width=\"563\"><figcaption><p>GSPO algorithm, Source: <a href=\"https://arxiv.org/abs/2507.18071\">Qwen</a></p></figcaption></figure>\n\nIn Equation 1, it can be seen that the advantages scale each of the rows into the token logprobs before that tensor is sumed. Essentially, each token is given the same scaling even though that scaling was given to the entire sequence rather than each individual token. A simple diagram of this can be seen below:\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-b3c944808a15dde0a7ff45782f9f074993304bf1%2FCopy%20of%20GSPO%20diagram%20(1).jpg?alt=media\" alt=\"\" width=\"286\"><figcaption><p>GRPO Logprob Ratio row wise scaled with advantages</p></figcaption></figure>\n\nEquation 2 shows that the logprob ratios for each sequence is summed and exponentiated after the Logprob ratios are computed, and only the resulting now sequence ratios get row wise multiplied by the advantages.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-62fc5b50921e79cce155d2794201c9b96faf941e%2FGSPO%20diagram%20(1).jpg?alt=media\" alt=\"\" width=\"313\"><figcaption><p>GSPO Sequence Ratio row wise scaled with advantages</p></figcaption></figure>\n\nEnabling GSPO is simple, all you need to do is set the `importance_sampling_level = \"sequence\"` flag in the GRPO config.",
      "language": "unknown"
    }
  ],
  "headings": [
    {
      "level": "h2",
      "text": ":butterfly:Qwen 2.5 VL Vision RL Issues and Quirks",
      "id": ":butterfly:qwen-2.5-vl-vision-rl-issues-and-quirks"
    },
    {
      "level": "h2",
      "text": ":medal:Reward Functions to reduce gibberish",
      "id": ":medal:reward-functions-to-reduce-gibberish"
    },
    {
      "level": "h2",
      "text": ":checkered\\_flag:GSPO Reinforcement Learning",
      "id": ":checkered\\_flag:gspo-reinforcement-learning"
    }
  ],
  "url": "llms-txt#add-lora-adapter-to-the-model-for-parameter-efficient-fine-tuning",
  "links": []
}