{
  "title": "or:",
  "content": "mask_truncated_completions=True,\npython",
  "code_samples": [
    {
      "code": "{% endhint %}\n\nYou should see the reward increase overtime. We would recommend you train for at least 300 steps which may take 30 mins however, for optimal results, you should train for longer.\n\n{% hint style=\"warning\" %}\nIf you're having issues with your GRPO model not learning, we'd highly recommend to use our [Advanced GRPO notebooks](https://docs.unsloth.ai/unsloth-notebooks#grpo-reasoning-notebooks) as it has a much better reward function and you should see results much faster and frequently.\n{% endhint %}\n\nYou will also see sample answers which allows you to see how the model is learning. Some may have steps, XML tags, attempts etc. and the idea is as trains it's going to get better and better because it's going to get scored higher and higher until we get the outputs we desire with long reasoning chains of answers.\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-f33d6f494605ab9ca69a0b697ed5865dd3a30b18%2Fimage.png?alt=media\" alt=\"\" width=\"563\"><figcaption></figcaption></figure>\n{% endstep %}\n\n{% step %}\n\n#### Run & Evaluate your model\n\nRun your model by clicking the play button. In the first example, there is usually no reasoning in the answer and in order to see the reasoning, we need to first save the LoRA weights we just trained with GRPO first using:\n\n<pre><code><strong>model.save_lora(\"grpo_saved_lora\")\n</strong></code></pre>\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-1ab351622655983aeda4d9d6d217cf354cb280be%2Fimage%20(10)%20(1)%20(1).png?alt=media\" alt=\"\"><figcaption><p>The first inference example run has no reasoning. You must load the LoRA and test it to reveal the reasoning.</p></figcaption></figure>\n\nThen we load the LoRA and test it. Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!\n\nYou can then save your model to GGUF, Ollama etc. by following our [guide here](https://docs.unsloth.ai/fine-tuning-llms-guide#id-7.-running--saving-the-model).\n\n<figure><img src=\"https://3215535692-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FxhOjnexMCB3dmuQFQ2Zq%2Fuploads%2Fgit-blob-38fa0c97184487aaa6b259f5b23b7f27345871d8%2Fimage.png?alt=media\" alt=\"\"><figcaption></figcaption></figure>\n\nIf you are still not getting any reasoning, you may have either trained for too less steps or your reward function/verifier was not optimal.\n{% endstep %}\n\n{% step %}\n\n#### Save your model\n\nWe have multiple options for saving your fine-tuned model, but weâ€™ll focus on the easiest and most popular approaches which you can read more about [here](https://docs.unsloth.ai/basics/inference-and-deployment)\n\n**Saving in 16-bit Precision**\n\nYou can save the model with 16-bit precision using the following command:",
      "language": "unknown"
    }
  ],
  "headings": [],
  "url": "llms-txt#or:",
  "links": []
}