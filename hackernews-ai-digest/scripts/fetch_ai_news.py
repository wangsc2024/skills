#!/usr/bin/env python3
"""
Hacker News AI æ–°é—»æŠ“å–å·¥å…· v2.0

ä½¿ç”¨ Hacker News API è·å–æœ€æ–°çš„ AI ç›¸å…³æ–°é—»ï¼Œ
æŠ“å–å®Œæ•´æ–‡ç« å†…å®¹å¹¶æä¾›ä¸­æ–‡ç¿»è¯‘ã€‚

ç”¨æ³•:
    python fetch_ai_news.py                       # è·å– 10 æ¡ AI æ–°é—»ï¼ˆæ‘˜è¦ï¼‰
    python fetch_ai_news.py --full                # è·å–å®Œæ•´å†…å®¹
    python fetch_ai_news.py --full --translate    # å®Œæ•´å†…å®¹ + ä¸­æ–‡ç¿»è¯‘
    python fetch_ai_news.py --count 20            # è·å– 20 æ¡
    python fetch_ai_news.py --source top          # ä½¿ç”¨çƒ­é—¨æ–°é—»æº
    python fetch_ai_news.py --output news.md      # è¾“å‡ºåˆ°æ–‡ä»¶
"""

import argparse
import json
import os
import re
import requests
import time
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from html import unescape
from urllib.parse import urlparse

# å¯é€‰ä¾èµ–
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False


class HackerNewsAI:
    """Hacker News AI æ–°é—»æŠ“å–å™¨ v2.0"""

    BASE_URL = "https://hacker-news.firebaseio.com/v0"

    # AI ç›¸å…³å…³é”®è¯
    AI_KEYWORDS = [
        # é€šç”¨ AI æœ¯è¯­
        'ai', 'artificial intelligence', 'machine learning', 'ml',
        'deep learning', 'neural network', 'neural net',
        # LLM ç›¸å…³
        'llm', 'large language model', 'language model',
        'gpt', 'gpt-4', 'gpt-5', 'chatgpt',
        'claude', 'anthropic',
        'gemini', 'bard',
        'llama', 'meta ai',
        'deepseek', 'mistral', 'mixtral',
        'openai', 'open ai',
        # æŠ€æœ¯æœ¯è¯­
        'transformer', 'attention mechanism',
        'diffusion', 'stable diffusion', 'midjourney', 'dall-e', 'sora',
        'embedding', 'vector database', 'rag',
        'fine-tuning', 'fine tuning', 'lora', 'qlora',
        'prompt engineering', 'prompt',
        # åº”ç”¨é¢†åŸŸ
        'ai agent', 'ai agents', 'agentic',
        'copilot', 'coding assistant', 'code generation',
        'text-to-image', 'text-to-video', 'text-to-speech',
        'nlp', 'natural language processing',
        'computer vision', 'cv',
        'reinforcement learning', 'rl',
        # AGI ç›¸å…³
        'agi', 'artificial general intelligence',
        'superintelligence', 'alignment',
        # å…¬å¸/äº§å“
        'hugging face', 'huggingface',
        'replicate', 'together ai',
        'perplexity', 'cursor', 'windsurf',
    ]

    def __init__(self, rate_limit: float = 0.1):
        """åˆå§‹åŒ–æŠ“å–å™¨"""
        self.rate_limit = rate_limit
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; HackerNewsAI/2.0)'
        })
        self.translator = None
        if HAS_ANTHROPIC and os.environ.get('ANTHROPIC_API_KEY'):
            self.translator = anthropic.Anthropic()

    def fetch_story_ids(self, source: str = 'new') -> List[int]:
        """è·å–æ–°é—» ID åˆ—è¡¨"""
        endpoints = {
            'new': f"{self.BASE_URL}/newstories.json",
            'top': f"{self.BASE_URL}/topstories.json",
            'best': f"{self.BASE_URL}/beststories.json",
        }
        url = endpoints.get(source, endpoints['new'])
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f"è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def fetch_story(self, story_id: int) -> Optional[Dict]:
        """è·å–å•æ¡æ–°é—»è¯¦æƒ…"""
        url = f"{self.BASE_URL}/item/{story_id}.json"
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.json()
        except requests.RequestException as e:
            print(f"è·å–æ–°é—» {story_id} å¤±è´¥: {e}")
            return None

    def fetch_comments(self, story: Dict, max_comments: int = 10) -> List[Dict]:
        """
        è·å–æ–°é—»è¯„è®º

        Args:
            story: æ–°é—»å­—å…¸
            max_comments: æœ€å¤§è¯„è®ºæ•°

        Returns:
            è¯„è®ºåˆ—è¡¨
        """
        comments = []
        kids = story.get('kids', [])[:max_comments]

        for kid_id in kids:
            comment = self.fetch_story(kid_id)
            if comment and comment.get('text'):
                comments.append({
                    'by': comment.get('by', 'anonymous'),
                    'text': self._clean_html(comment.get('text', '')),
                    'time': comment.get('time', 0),
                })
            time.sleep(self.rate_limit)

        return comments

    def fetch_article_content(self, url: str) -> Optional[str]:
        """
        è·å–æ–‡ç« å®Œæ•´å†…å®¹

        Args:
            url: æ–‡ç«  URL

        Returns:
            æ–‡ç« æ­£æ–‡å†…å®¹
        """
        if not url or not HAS_BS4:
            return None

        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                tag.decompose()

            # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
            content = None
            selectors = [
                'article',
                'main',
                '[role="main"]',
                '.post-content',
                '.article-content',
                '.entry-content',
                '.content',
                '#content',
            ]

            for selector in selectors:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text(separator='\n', strip=True)
                    break

            if not content:
                # å›é€€åˆ° body
                body = soup.find('body')
                if body:
                    content = body.get_text(separator='\n', strip=True)

            if content:
                # æ¸…ç†å†…å®¹
                lines = [line.strip() for line in content.split('\n') if line.strip()]
                content = '\n'.join(lines)
                # é™åˆ¶é•¿åº¦
                if len(content) > 10000:
                    content = content[:10000] + '\n\n[... å†…å®¹å·²æˆªæ–­ ...]'
                return content

        except Exception as e:
            print(f"  è·å–æ–‡ç« å†…å®¹å¤±è´¥: {e}")

        return None

    def _clean_html(self, html: str) -> str:
        """æ¸…ç† HTML æ ‡ç­¾"""
        if not html:
            return ''
        # ç®€å•çš„ HTML æ¸…ç†
        text = re.sub(r'<[^>]+>', ' ', html)
        text = unescape(text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def translate_text(self, text: str, text_type: str = 'content') -> str:
        """
        ä½¿ç”¨ Claude API ç¿»è¯‘æ–‡æœ¬

        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            text_type: æ–‡æœ¬ç±»å‹ ('title', 'content', 'comment')

        Returns:
            ç¿»è¯‘åçš„ä¸­æ–‡æ–‡æœ¬
        """
        if not self.translator or not text:
            return self._simple_translate(text) if text_type == 'title' else text

        prompts = {
            'title': f"å°†ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆç®€æ´çš„ä¸­æ–‡ï¼Œä¿ç•™æŠ€æœ¯æœ¯è¯­åŸæ–‡ï¼ˆå¦‚ LLMã€GPTã€Claude ç­‰ï¼‰ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\n\n{text}",
            'content': f"""å°†ä»¥ä¸‹è‹±æ–‡æ–‡ç« ç¿»è¯‘æˆæµç•…çš„ä¸­æ–‡ã€‚è¦æ±‚ï¼š
1. ä¿ç•™æŠ€æœ¯æœ¯è¯­åŸæ–‡ï¼ˆå¦‚ LLMã€GPTã€Claudeã€API ç­‰ï¼‰
2. ä¿æŒæ®µè½ç»“æ„
3. ç¿»è¯‘è¦é€šé¡ºè‡ªç„¶
4. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜

åŸæ–‡ï¼š
{text}""",
            'comment': f"å°†ä»¥ä¸‹ Hacker News è¯„è®ºç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿ç•™æŠ€æœ¯æœ¯è¯­åŸæ–‡ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\n\n{text}"
        }

        try:
            message = self.translator.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=4096,
                messages=[{
                    "role": "user",
                    "content": prompts.get(text_type, prompts['content'])
                }]
            )
            return message.content[0].text
        except Exception as e:
            print(f"  ç¿»è¯‘å¤±è´¥: {e}")
            return text

    def _simple_translate(self, title: str) -> str:
        """ç®€å•çš„è§„åˆ™ç¿»è¯‘"""
        translations = {
            'Show HN:': 'å±•ç¤ºï¼š',
            'Ask HN:': 'æé—®ï¼š',
            'Tell HN:': 'åˆ†äº«ï¼š',
            'Launch HN:': 'å‘å¸ƒï¼š',
            'artificial intelligence': 'äººå·¥æ™ºèƒ½',
            'machine learning': 'æœºå™¨å­¦ä¹ ',
            'deep learning': 'æ·±åº¦å­¦ä¹ ',
            'neural network': 'ç¥ç»ç½‘ç»œ',
            'large language model': 'å¤§è¯­è¨€æ¨¡å‹',
            'open source': 'å¼€æº',
            'self-hosted': 'è‡ªæ‰˜ç®¡',
            'real-time': 'å®æ—¶',
        }
        result = title
        for en, zh in translations.items():
            result = result.replace(en, zh)
            result = result.replace(en.title(), zh)
        return result

    def is_ai_related(self, story: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸º AI ç›¸å…³æ–°é—»"""
        if not story or story.get('type') != 'story':
            return False

        title = story.get('title', '').lower()
        url = story.get('url', '').lower()
        text = (story.get('text', '') or '').lower()
        content = f" {title} {url} {text} "

        for keyword in self.AI_KEYWORDS:
            if f" {keyword} " in content or f" {keyword}." in content or \
               f" {keyword}," in content or f" {keyword}:" in content or \
               content.startswith(f"{keyword} ") or content.endswith(f" {keyword}"):
                return True
            if ' ' in keyword and keyword in content:
                return True
        return False

    def get_ai_news(self, count: int = 10, source: str = 'new',
                    max_scan: int = 200, full_content: bool = False,
                    translate: bool = False, max_comments: int = 5) -> List[Dict]:
        """
        è·å– AI ç›¸å…³æ–°é—»

        Args:
            count: éœ€è¦è·å–çš„æ•°é‡
            source: æ–°é—»æº
            max_scan: æœ€å¤§æ‰«ææ•°é‡
            full_content: æ˜¯å¦è·å–å®Œæ•´å†…å®¹
            translate: æ˜¯å¦ç¿»è¯‘
            max_comments: æœ€å¤§è¯„è®ºæ•°

        Returns:
            AI ç›¸å…³æ–°é—»åˆ—è¡¨
        """
        print(f"æ­£åœ¨ä» {source} è·å–æ–°é—»åˆ—è¡¨...")
        story_ids = self.fetch_story_ids(source)

        if not story_ids:
            print("æ— æ³•è·å–æ–°é—»åˆ—è¡¨")
            return []

        print(f"å…± {len(story_ids)} æ¡æ–°é—»ï¼Œå¼€å§‹ç­›é€‰ AI ç›¸å…³å†…å®¹...")

        ai_stories = []
        scanned = 0

        for story_id in story_ids[:max_scan]:
            if len(ai_stories) >= count:
                break

            story = self.fetch_story(story_id)
            scanned += 1

            if story and self.is_ai_related(story):
                print(f"  [{len(ai_stories)+1}/{count}] {story.get('title', '')[:50]}...")

                # è·å–å®Œæ•´å†…å®¹
                if full_content:
                    url = story.get('url', '')
                    if url:
                        print(f"    è·å–æ–‡ç« å†…å®¹...")
                        article_content = self.fetch_article_content(url)
                        story['article_content'] = article_content

                        if translate and article_content:
                            print(f"    ç¿»è¯‘æ–‡ç« å†…å®¹...")
                            story['article_content_zh'] = self.translate_text(
                                article_content[:5000], 'content'
                            )

                    # è·å–è¯„è®º
                    print(f"    è·å–çƒ­é—¨è¯„è®º...")
                    comments = self.fetch_comments(story, max_comments)
                    story['top_comments'] = comments

                    if translate and comments:
                        print(f"    ç¿»è¯‘è¯„è®º...")
                        for comment in comments:
                            comment['text_zh'] = self.translate_text(
                                comment['text'][:1000], 'comment'
                            )

                # ç¿»è¯‘æ ‡é¢˜
                if translate:
                    story['title_zh'] = self.translate_text(story.get('title', ''), 'title')

                ai_stories.append(story)

            time.sleep(self.rate_limit)

        print(f"æ‰«æ {scanned} æ¡æ–°é—»ï¼Œæ‰¾åˆ° {len(ai_stories)} æ¡ AI ç›¸å…³")
        return ai_stories

    def generate_markdown(self, stories: List[Dict], full_content: bool = False,
                          translate: bool = False) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼è¾“å‡º"""
        now = datetime.now().strftime("%Y-%m-%d %H:%M")

        md = f"""# Hacker News AI æ–°é—»ç²¾é€‰

> æ›´æ–°æ—¶é—´ï¼š{now}
> æ¥æºï¼š[Hacker News](https://news.ycombinator.com/)
> ç­›é€‰æ¡ä»¶ï¼šAI / ML / LLM ç›¸å…³
> æ¨¡å¼ï¼š{'å®Œæ•´å†…å®¹' if full_content else 'æ‘˜è¦'} | {'ä¸­æ–‡ç¿»è¯‘' if translate else 'åŸæ–‡'}

---

"""
        for i, story in enumerate(stories, 1):
            title = story.get('title', 'No Title')
            title_zh = story.get('title_zh', self._simple_translate(title))
            score = story.get('score', 0)
            comments = story.get('descendants', 0) or 0
            story_id = story.get('id')
            url = story.get('url', '')
            author = story.get('by', 'unknown')
            timestamp = story.get('time', 0)
            date_str = datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M") if timestamp else 'N/A'

            md += f"""## {i}. {title_zh}

**åŸæ ‡é¢˜**: {title}

**çƒ­åº¦**: ğŸ”¥ {score} points | ğŸ’¬ {comments} comments | ğŸ‘¤ {author}

**æ—¶é—´**: {date_str}

"""
            if url:
                domain = urlparse(url).netloc
                md += f"""**æ¥æº**: [{domain}]({url})

"""

            md += f"""**HN è®¨è®º**: [Hacker News #{story_id}](https://news.ycombinator.com/item?id={story_id})

"""

            # å®Œæ•´å†…å®¹
            if full_content:
                article_zh = story.get('article_content_zh')
                article_en = story.get('article_content')

                if article_zh:
                    md += f"""### ğŸ“„ æ–‡ç« å†…å®¹ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰

{article_zh}

"""
                elif article_en:
                    md += f"""### ğŸ“„ æ–‡ç« å†…å®¹ï¼ˆåŸæ–‡ï¼‰

{article_en[:3000]}{'...' if len(article_en) > 3000 else ''}

"""

                # çƒ­é—¨è¯„è®º
                top_comments = story.get('top_comments', [])
                if top_comments:
                    md += f"""### ğŸ’¬ çƒ­é—¨è¯„è®º

"""
                    for j, comment in enumerate(top_comments, 1):
                        text = comment.get('text_zh') if translate else comment.get('text')
                        if text:
                            text_preview = text[:500] + '...' if len(text) > 500 else text
                            md += f"""**{j}. @{comment.get('by', 'anonymous')}**:

> {text_preview}

"""

            md += """---

"""

        # ç»Ÿè®¡ä¿¡æ¯
        total_score = sum(s.get('score', 0) for s in stories)
        total_comments = sum(s.get('descendants', 0) or 0 for s in stories)

        md += f"""
## ğŸ“Š ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ–°é—»æ•°é‡ | {len(stories)} |
| æ€»çƒ­åº¦ | {total_score} points |
| æ€»è¯„è®º | {total_comments} comments |
| å¹³å‡çƒ­åº¦ | {total_score // len(stories) if stories else 0} points |

---

> ç”± hackernews-ai-digest skill ç”Ÿæˆ
> {'ä½¿ç”¨ Claude API è¿›è¡Œç¿»è¯‘' if translate and self.translator else 'ä½¿ç”¨è§„åˆ™ç¿»è¯‘'}
"""

        return md


def main():
    parser = argparse.ArgumentParser(
        description='è·å– Hacker News AI ç›¸å…³æ–°é—»ï¼ˆæ”¯æŒå®Œæ•´å†…å®¹å’Œä¸­æ–‡ç¿»è¯‘ï¼‰'
    )
    parser.add_argument(
        '--count', '-c', type=int, default=10,
        help='è·å–æ–°é—»æ•°é‡ (é»˜è®¤: 10)'
    )
    parser.add_argument(
        '--source', '-s', choices=['new', 'top', 'best'], default='top',
        help='æ–°é—»æº (é»˜è®¤: top)'
    )
    parser.add_argument(
        '--output', '-o', type=str, default=None,
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: æ‰“å°åˆ°ç»ˆç«¯)'
    )
    parser.add_argument(
        '--full', '-f', action='store_true',
        help='è·å–å®Œæ•´æ–‡ç« å†…å®¹å’Œè¯„è®º'
    )
    parser.add_argument(
        '--translate', '-t', action='store_true',
        help='ç¿»è¯‘æˆä¸­æ–‡ (éœ€è¦ ANTHROPIC_API_KEY)'
    )
    parser.add_argument(
        '--max-scan', type=int, default=200,
        help='æœ€å¤§æ‰«ææ•°é‡ (é»˜è®¤: 200)'
    )
    parser.add_argument(
        '--max-comments', type=int, default=5,
        help='æ¯æ¡æ–°é—»æœ€å¤§è¯„è®ºæ•° (é»˜è®¤: 5)'
    )

    args = parser.parse_args()

    # æ£€æŸ¥ä¾èµ–
    if args.full and not HAS_BS4:
        print("è­¦å‘Š: æœªå®‰è£… beautifulsoup4ï¼Œæ— æ³•è·å–å®Œæ•´å†…å®¹")
        print("  å®‰è£…: pip install beautifulsoup4")

    if args.translate and not HAS_ANTHROPIC:
        print("è­¦å‘Š: æœªå®‰è£… anthropicï¼Œæ— æ³•ä½¿ç”¨ Claude API ç¿»è¯‘")
        print("  å®‰è£…: pip install anthropic")
        print("  è®¾ç½®: export ANTHROPIC_API_KEY=sk-ant-...")
    elif args.translate and not os.environ.get('ANTHROPIC_API_KEY'):
        print("è­¦å‘Š: æœªè®¾ç½® ANTHROPIC_API_KEYï¼Œå°†ä½¿ç”¨è§„åˆ™ç¿»è¯‘")

    # åˆ›å»ºæŠ“å–å™¨
    hn = HackerNewsAI(rate_limit=0.2 if args.full else 0.1)

    # è·å– AI æ–°é—»
    stories = hn.get_ai_news(
        count=args.count,
        source=args.source,
        max_scan=args.max_scan,
        full_content=args.full,
        translate=args.translate,
        max_comments=args.max_comments
    )

    if not stories:
        print("æœªæ‰¾åˆ° AI ç›¸å…³æ–°é—»")
        return

    # ç”Ÿæˆ Markdown
    markdown = hn.generate_markdown(
        stories,
        full_content=args.full,
        translate=args.translate
    )

    # è¾“å‡º
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(markdown)
        print(f"\nå·²ä¿å­˜åˆ°: {args.output}")
    else:
        print("\n" + "=" * 60)
        print(markdown)


if __name__ == "__main__":
    main()
