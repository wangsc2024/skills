---
name: skill-seekers
description: "è‡ªå‹•åŒ–å·¥å…·ï¼Œå°‡æŠ€è¡“æ–‡ä»¶ç¶²ç«™ã€GitHub å€‰åº«ã€PDF æª”æ¡ˆè½‰æ›ç‚ºçµæ§‹åŒ– AI çŸ¥è­˜åº«ã€‚ç•¶ä½¿ç”¨è€…è¦æ±‚æŠ“å–æ–‡ä»¶ã€å»ºç«‹ skillã€åˆ†æ GitHub å€‰åº«ã€æˆ–æåˆ°ã€ŒçŸ¥è­˜åº«ã€ã€Œæ–‡ä»¶è½‰æ›ã€ç­‰é—œéµå­—æ™‚ä½¿ç”¨æ­¤æŠ€èƒ½ã€‚"
---

# Skill Seekers

## Overview

Skill Seekers æ˜¯ä¸€å€‹è‡ªå‹•åŒ–å·¥å…·ï¼Œèƒ½å°‡æŠ€è¡“æ–‡ä»¶ç¶²ç«™ã€GitHub å€‰åº«ã€PDF æª”æ¡ˆè½‰æ›ç‚ºçµæ§‹åŒ–çš„ AI çŸ¥è­˜åº«ã€‚å®ƒæœƒè‡ªå‹•æŠ“å–å…§å®¹ã€æ™ºæ…§åˆ†é¡ã€æå–ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼Œä¸¦ç”Ÿæˆæ¨™æº–åŒ–çš„ Skill æ ¼å¼è¼¸å‡ºã€‚

## Quick Reference

| ä»»å‹™ | æ–¹æ³• |
|------|------|
| æŠ“å–æ–‡ä»¶ç¶²ç«™ | ä½¿ç”¨ `scripts/scraper.py` |
| æŠ“å– GitHub å€‰åº« | ä½¿ç”¨ `scripts/github_scraper.py` |
| è™•ç† PDF | ä½¿ç”¨ `scripts/pdf_extractor.py` |
| ç”Ÿæˆ Skill | ä½¿ç”¨ `scripts/skill_generator.py` |

### æ ¸å¿ƒå·¥ä½œæµç¨‹

```
1. åˆ†æä¾†æº â†’ æª¢æŸ¥ llms.txtã€sitemapã€å°èˆªçµæ§‹
2. æŠ“å–å…§å®¹ â†’ å¿«é€Ÿ(20é ) / å®Œæ•´(100+é ) / è‡ªè¨‚
3. å…§å®¹åˆ†é¡ â†’ è‡ªå‹•åˆ†é¡ç‚º 8 å¤§é¡åˆ¥
4. ç”Ÿæˆè¼¸å‡º â†’ SKILL.md + references/*.md + metadata.json
```

### åˆ†é¡é—œéµå­—

| é¡åˆ¥ | é—œéµå­— |
|------|--------|
| `getting_started` | intro, quickstart, installation, setup |
| `core_concepts` | concepts, fundamentals, basics, overview |
| `api_reference` | api, reference, methods, functions |
| `guides` | guide, tutorial, how-to, walkthrough |
| `examples` | example, sample, demo, cookbook |
| `configuration` | config, settings, options |
| `troubleshooting` | error, debug, faq, troubleshoot |
| `advanced` | advanced, deep-dive, internals |

---

## æŠ“å–æ–‡ä»¶ç¶²ç«™

### åŸºæœ¬ç”¨æ³•

```python
import asyncio
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

async def scrape_documentation(base_url: str, max_pages: int = 50):
    """æŠ“å–æ–‡ä»¶ç¶²ç«™"""
    
    visited = set()
    pages = []
    to_visit = [base_url]
    
    parsed_base = urlparse(base_url)
    base_domain = parsed_base.netloc
    
    # æ’é™¤è·¯å¾‘
    exclude_patterns = ['/blog', '/changelog', '/about', '/community']
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        # å…ˆæª¢æŸ¥ llms.txt
        llms_content = await check_llms_txt(client, base_url)
        if llms_content:
            print("âœ… ç™¼ç¾ llms.txtï¼Œä½¿ç”¨å„ªåŒ–ç‰ˆæœ¬")
            return {"source": "llms.txt", "content": llms_content}
        
        while to_visit and len(pages) < max_pages:
            url = to_visit.pop(0)
            if url in visited:
                continue
            
            visited.add(url)
            
            try:
                response = await client.get(url, follow_redirects=True)
                if response.status_code != 200:
                    continue
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–é é¢å…§å®¹
                page_data = extract_page_content(soup, url)
                pages.append(page_data)
                
                # æ¢ç´¢é€£çµ
                for a in soup.select('a[href]'):
                    href = a.get('href', '')
                    full_url = urljoin(url, href)
                    parsed = urlparse(full_url)
                    
                    if parsed.netloc == base_domain:
                        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                        if not any(p in clean_url.lower() for p in exclude_patterns):
                            if clean_url not in visited:
                                to_visit.append(clean_url)
                
                await asyncio.sleep(0.5)  # é€Ÿç‡é™åˆ¶
                
            except Exception as e:
                print(f"éŒ¯èª¤: {url} - {e}")
    
    return categorize_and_build_result(pages)

async def check_llms_txt(client, base_url: str):
    """æª¢æŸ¥ llms.txt"""
    parsed = urlparse(base_url)
    base = f"{parsed.scheme}://{parsed.netloc}"
    
    for path in ['/llms-full.txt', '/llms.txt', '/llms-small.txt']:
        try:
            response = await client.get(f"{base}{path}")
            if response.status_code == 200:
                return response.text
        except:
            pass
    return None

def extract_page_content(soup, url: str):
    """æå–é é¢å…§å®¹"""
    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    for tag in soup.select('nav, footer, header, script, style, .sidebar'):
        tag.decompose()
    
    # æ¨™é¡Œ
    title = ""
    if soup.h1:
        title = soup.h1.get_text(strip=True)
    elif soup.title:
        title = soup.title.string or ""
    
    # ä¸»è¦å…§å®¹
    main = soup.select_one('article, main, .content, [role="main"]')
    content = main.get_text(separator='\n', strip=True) if main else ""
    
    # ç¨‹å¼ç¢¼å€å¡Š
    code_blocks = []
    for code in soup.select('pre code, pre'):
        code_text = code.get_text(strip=True)
        if len(code_text) > 20:
            lang = detect_language(code_text, code.get('class', []))
            code_blocks.append({"language": lang, "code": code_text[:2000]})
    
    # åˆ†é¡
    category = categorize_page(url, title)
    
    return {
        "url": url,
        "title": title,
        "content": content[:10000],
        "code_blocks": code_blocks[:10],
        "category": category
    }

def detect_language(code: str, classes: list) -> str:
    """åµæ¸¬ç¨‹å¼ç¢¼èªè¨€"""
    for cls in classes:
        if isinstance(cls, str) and 'language-' in cls:
            return cls.replace('language-', '')
    
    patterns = {
        "python": [r"\bdef\s+\w+\(", r"\bimport\s+\w+"],
        "javascript": [r"\bconst\s+\w+\s*=", r"=>\s*{"],
        "typescript": [r":\s*(string|number|boolean)"],
        "jsx": [r"useState\s*\(", r"useEffect\s*\("],
        "bash": [r"#!/bin/bash", r"\$\w+"],
    }
    
    for lang, pats in patterns.items():
        for p in pats:
            if re.search(p, code):
                return lang
    return "text"

def categorize_page(url: str, title: str) -> str:
    """åˆ†é¡é é¢"""
    keywords = {
        "getting_started": ["intro", "quickstart", "installation", "setup"],
        "core_concepts": ["concepts", "fundamentals", "basics", "overview"],
        "api_reference": ["api", "reference", "methods", "functions"],
        "guides": ["guide", "tutorial", "how-to"],
        "examples": ["example", "sample", "demo"],
        "configuration": ["config", "settings", "options"],
        "troubleshooting": ["error", "debug", "faq"],
        "advanced": ["advanced", "deep-dive", "internals"],
    }
    
    text = (url + title).lower()
    for cat, kws in keywords.items():
        if any(kw in text for kw in kws):
            return cat
    return "general"

def categorize_and_build_result(pages: list) -> dict:
    """æ•´ç†çµæœ"""
    categories = {}
    languages = set()
    code_count = 0
    
    for page in pages:
        cat = page.get("category", "general")
        categories[cat] = categories.get(cat, 0) + 1
        for block in page.get("code_blocks", []):
            code_count += 1
            if block.get("language"):
                languages.add(block["language"])
    
    return {
        "pages": pages,
        "categories": categories,
        "statistics": {
            "total_pages": len(pages),
            "total_code_blocks": code_count,
            "languages_detected": list(languages)
        }
    }
```

### åŸ·è¡Œç¯„ä¾‹

```python
# æŠ“å– React æ–‡ä»¶
result = asyncio.run(scrape_documentation("https://react.dev/learn", max_pages=30))
print(f"æŠ“å–å®Œæˆ: {result['statistics']['total_pages']} é ")
```

---

## æŠ“å– GitHub å€‰åº«

```python
import httpx
import base64

async def scrape_github_repo(repo: str, include_code: bool = False):
    """æŠ“å– GitHub å€‰åº«"""
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        headers = {}
        # å¦‚æœ‰ token: headers["Authorization"] = f"token {os.environ.get('GITHUB_TOKEN')}"
        
        api_base = f"https://api.github.com/repos/{repo}"
        
        # å€‰åº«è³‡è¨Š
        repo_resp = await client.get(api_base, headers=headers)
        repo_data = repo_resp.json()
        
        # README
        readme_content = ""
        readme_resp = await client.get(f"{api_base}/readme", headers=headers)
        if readme_resp.status_code == 200:
            readme_data = readme_resp.json()
            readme_content = base64.b64decode(readme_data.get("content", "")).decode("utf-8")
        
        # ç›®éŒ„çµæ§‹
        structure = []
        contents_resp = await client.get(f"{api_base}/contents", headers=headers)
        if contents_resp.status_code == 200:
            for item in contents_resp.json():
                structure.append({
                    "name": item.get("name"),
                    "type": item.get("type"),
                    "path": item.get("path")
                })
        
        # docs ç›®éŒ„
        docs = []
        docs_resp = await client.get(f"{api_base}/contents/docs", headers=headers)
        if docs_resp.status_code == 200:
            for item in docs_resp.json()[:20]:
                if item.get("name", "").endswith(".md"):
                    docs.append({
                        "name": item.get("name"),
                        "path": item.get("path")
                    })
        
        return {
            "repo": repo,
            "description": repo_data.get("description", ""),
            "stars": repo_data.get("stargazers_count", 0),
            "language": repo_data.get("language", ""),
            "topics": repo_data.get("topics", []),
            "readme": readme_content[:10000],
            "structure": structure,
            "docs": docs
        }

# åŸ·è¡Œ
result = asyncio.run(scrape_github_repo("facebook/react"))
print(f"â­ Stars: {result['stars']}")
```

---

## ç”Ÿæˆ Skill

```python
import json
import os
from datetime import datetime

def generate_skill(content: dict, name: str, output_dir: str):
    """ç”Ÿæˆå®Œæ•´ Skill"""
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/references", exist_ok=True)
    
    pages = content.get("pages", [])
    stats = content.get("statistics", {})
    categories = content.get("categories", {})
    
    # æ”¶é›†ç¨‹å¼ç¢¼ç¯„ä¾‹
    examples = []
    for page in pages:
        for block in page.get("code_blocks", [])[:2]:
            if len(block.get("code", "")) > 50:
                examples.append({
                    "source": page.get("title", ""),
                    "language": block.get("language", ""),
                    "code": block.get("code", "")
                })
    
    # ç”Ÿæˆ SKILL.md
    skill_md = f"""# {name} Skill

## Description

{name} æŠ€è¡“æ–‡ä»¶çŸ¥è­˜åº«ï¼ŒåŒ…å«æ ¸å¿ƒæ¦‚å¿µã€API åƒè€ƒèˆ‡å¯¦ç”¨ç¨‹å¼ç¢¼ç¯„ä¾‹ã€‚

## When to Use

- éœ€è¦æŸ¥è©¢ {name} ç›¸é—œæŠ€è¡“å•é¡Œ
- å°‹æ‰¾ç¨‹å¼ç¢¼ç¯„ä¾‹èˆ‡æœ€ä½³å¯¦è¸
- äº†è§£ API ç”¨æ³•èˆ‡è¨­å®šæ–¹å¼

## Statistics

| é …ç›® | æ•¸å€¼ |
|------|------|
| ç¸½é æ•¸ | {stats.get('total_pages', 0)} |
| ç¨‹å¼ç¢¼å€å¡Š | {stats.get('total_code_blocks', 0)} |
| åµæ¸¬èªè¨€ | {', '.join(stats.get('languages_detected', []))} |

## Categories

"""
    
    for cat, count in sorted(categories.items(), key=lambda x: -x[1]):
        skill_md += f"- **{cat.replace('_', ' ').title()}**: {count} é \n"
    
    skill_md += "\n## Core Concepts\n\n"
    
    concept_pages = [p for p in pages if p.get("category") in ["core_concepts", "getting_started"]][:5]
    for page in concept_pages:
        skill_md += f"### {page.get('title', 'Untitled')}\n\n"
        skill_md += f"{page.get('content', '')[:500]}...\n\n"
    
    skill_md += "## Code Examples\n\n"
    
    for i, ex in enumerate(examples[:5], 1):
        skill_md += f"### Example {i}: {ex.get('source', '')}\n\n"
        skill_md += f"```{ex.get('language', '')}\n{ex.get('code', '')}\n```\n\n"
    
    skill_md += """## Related Resources

- åŸå§‹æ–‡ä»¶ä¾†æº
- GitHub å€‰åº«ï¼ˆå¦‚é©ç”¨ï¼‰
"""
    
    # å„²å­˜ SKILL.md
    with open(f"{output_dir}/SKILL.md", "w", encoding="utf-8") as f:
        f.write(skill_md)
    
    # å„²å­˜åˆ†é¡æ–‡ä»¶
    for cat in categories.keys():
        cat_pages = [p for p in pages if p.get("category") == cat]
        cat_md = f"# {cat.replace('_', ' ').title()}\n\n"
        
        for page in cat_pages[:20]:
            cat_md += f"## {page.get('title', 'Untitled')}\n\n"
            cat_md += f"{page.get('content', '')[:2000]}\n\n"
        
        with open(f"{output_dir}/references/{cat}.md", "w", encoding="utf-8") as f:
            f.write(cat_md)
    
    # å„²å­˜ metadata.json
    metadata = {
        "skill_name": name.lower().replace(" ", "-"),
        "version": "1.0.0",
        "generated_at": datetime.now().isoformat(),
        "statistics": stats,
        "categories": list(categories.keys())
    }
    
    with open(f"{output_dir}/metadata.json", "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Skill å·²å„²å­˜è‡³ {output_dir}/")
    return skill_md
```

---

## å®Œæ•´å·¥ä½œæµç¨‹ç¯„ä¾‹

```python
import asyncio

async def create_skill_from_url(url: str, name: str):
    """å®Œæ•´å·¥ä½œæµç¨‹ï¼šå¾ URL å»ºç«‹ Skill"""
    
    print(f"ğŸ”„ é–‹å§‹æŠ“å– {url}...")
    
    # 1. æŠ“å–å…§å®¹
    content = await scrape_documentation(url, max_pages=50)
    
    print(f"âœ… æŠ“å–å®Œæˆ: {content['statistics']['total_pages']} é ")
    print(f"ğŸ“ åˆ†é¡: {list(content['categories'].keys())}")
    
    # 2. ç”Ÿæˆ Skill
    output_dir = f"output/{name.lower()}"
    generate_skill(content, name, output_dir)
    
    print(f"ğŸ‰ å®Œæˆï¼Skill ä½æ–¼ {output_dir}/")
    return output_dir

# åŸ·è¡Œ
asyncio.run(create_skill_from_url(
    url="https://fastapi.tiangolo.com/",
    name="FastAPI"
))
```

---

## ä¾è³´å®‰è£

```bash
pip install httpx beautifulsoup4
```

## é™åˆ¶èªªæ˜

1. **éœ€è¦ç™»å…¥çš„é é¢** - ç„¡æ³•æŠ“å–ï¼Œå»ºè­°ä½¿ç”¨è€…è²¼ä¸Šå…§å®¹
2. **JavaScript å‹•æ…‹è¼‰å…¥** - å¯èƒ½æŠ“å–ä¸å®Œæ•´ï¼Œå»ºè­°ä½¿ç”¨ llms.txt
3. **å¤§å‹ç¶²ç«™ (500+ é )** - å»ºè­°åˆ†æ‰¹è™•ç†
4. **é€Ÿç‡é™åˆ¶** - å…§å»º 0.5 ç§’å»¶é²

## ç›¸é—œåƒè€ƒ

- `references/scraping.md` - ç¶²é æŠ“å–è©³ç´°æŒ‡å—
- `references/github.md` - GitHub æŠ“å–æŒ‡å—
- `references/categorization.md` - åˆ†é¡é‚è¼¯èªªæ˜
- `references/output-formats.md` - è¼¸å‡ºæ ¼å¼è¦ç¯„
