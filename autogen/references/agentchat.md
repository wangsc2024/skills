# Autogen - Agentchat

**Pages:** 33

---

## Agents — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html

**Contents:**
- Agents#
- Assistant Agent#
- Getting Result#
- Multi-Modal Input#
- Streaming Messages#
- Using Tools and Workbench#
  - Built-in Tools and Workbench#
  - Function Tool#
  - Model Context Protocol (MCP) Workbench#
  - Agent as a Tool#

AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods:

name: The unique name of the agent.

description: The description of the agent in text.

run: The method that runs the agent given a task as a string or a list of messages, and returns a TaskResult. Agents are expected to be stateful and this method is expected to be called with new messages, not complete history.

run_stream: Same as run() but returns an iterator of messages that subclass BaseAgentEvent or BaseChatMessage followed by a TaskResult as the last item.

See autogen_agentchat.messages for more information on AgentChat message types.

AssistantAgent is a built-in agent that uses a language model and has the ability to use tools.

AssistantAgent is a “kitchen sink” agent for prototyping and educational purpose – it is very general. Make sure you read the documentation and implementation to understand the design choices. Once you fully understand the design, you may want to implement your own agent. See Custom Agent.

We can use the run() method to get the agent run on a given task.

The call to the run() method returns a TaskResult with the list of messages in the messages attribute, which stores the agent’s “thought process” as well as the final response.

It is important to note that run() will update the internal state of the agent – it will add the messages to the agent’s message history. You can also call run() without a task to get the agent to generate responses given its current state.

Unlike in v0.2 AgentChat, the tools are executed by the same agent directly within the same call to run(). By default, the agent will return the result of the tool call as the final response.

The AssistantAgent can handle multi-modal input by providing the input as a MultiModalMessage.

We can also stream each message as it is generated by the agent by using the run_stream() method, and use Console to print the messages as they appear to the console.

The run_stream() method returns an asynchronous generator that yields each message generated by the agent, followed by a TaskResult as the last item.

From the messages, you can observe that the assistant agent utilized the web_search tool to gather information and responded based on the search results.

Large Language Models (LLMs) are typically limited to generating text or code responses. However, many complex tasks benefit from the ability to use external tools that perform specific actions, such as fetching data from APIs or databases.

To address this limitation, modern LLMs can now accept a list of available tool schemas (descriptions of tools and their arguments) and generate a tool call message. This capability is known as Tool Calling or Function Calling and is becoming a popular pattern in building intelligent agent-based applications. Refer to the documentation from OpenAI and Anthropic for more information about tool calling in LLMs.

In AgentChat, the AssistantAgent can use tools to perform specific actions. The web_search tool is one such tool that allows the assistant agent to search the web for information. A single custom tool can be a Python function or a subclass of the BaseTool.

On the other hand, a Workbench is a collection of tools that share state and resources.

For how to use model clients directly with tools and workbench, refer to the Tools and Workbench sections in the Core User Guide.

By default, when AssistantAgent executes a tool, it will return the tool’s output as a string in ToolCallSummaryMessage in its response. If your tool does not return a well-formed string in natural language, you can add a reflection step to have the model summarize the tool’s output, by setting the reflect_on_tool_use=True parameter in the AssistantAgent constructor.

AutoGen Extension provides a set of built-in tools that can be used with the Assistant Agent. Head over to the API documentation for all the available tools under the autogen_ext.tools namespace. For example, you can find the following tools:

graphrag: Tools for using GraphRAG index.

http: Tools for making HTTP requests.

langchain: Adaptor for using LangChain tools.

mcp: Tools and workbench for using Model Chat Protocol (MCP) servers.

The AssistantAgent automatically converts a Python function into a FunctionTool which can be used as a tool by the agent and automatically generates the tool schema from the function signature and docstring.

The web_search_func tool is an example of a function tool. The schema is automatically generated.

The AssistantAgent can also use tools that are served from a Model Context Protocol (MCP) server using McpWorkbench().

Any BaseChatAgent can be used as a tool by wrapping it in a AgentTool. This allows for a dynamic, model-driven multi-agent workflow where the agent can call other agents as tools to solve tasks.

Some models support parallel tool calls, which can be useful for tasks that require multiple tools to be called simultaneously. By default, if the model client produces multiple tool calls, AssistantAgent will call the tools in parallel.

You may want to disable parallel tool calls when the tools have side effects that may interfere with each other, or, when agent behavior needs to be consistent across different models. This should be done at the model client level.

When using AgentTool or TeamTool, you must disable parallel tool calls to avoid concurrency issues. These tools cannot run concurrently as agents and teams maintain internal state that would conflict with parallel execution.

For OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient, set parallel_tool_calls=False to disable parallel tool calls.

One model call followed by one tool call or parallel tool calls is a single tool iteration. By default, the AssistantAgent will execute at most one iteration.

The agent can be configured to execute multiple iterations until the model stops generating tool calls or the maximum number of iterations is reached. You can control the maximum number of iterations by setting the max_tool_iterations parameter in the AssistantAgent constructor.

Structured output allows models to return structured JSON text with pre-defined schema provided by the application. Different from JSON-mode, the schema can be provided as a Pydantic BaseModel class, which can also be used to validate the output.

Once you specify the base model class in the output_content_type parameter of the AssistantAgent constructor, the agent will respond with a StructuredMessage whose content’s type is the type of the base model class.

This way, you can integrate agent’s response directly into your application and use the model’s output as a structured object.

When the output_content_type is set, it by default requires the agent to reflect on the tool use and return the a structured output message based on the tool call result. You can disable this behavior by setting reflect_on_tool_use=False explictly.

Structured output is also useful for incorporating Chain-of-Thought reasoning in the agent’s responses. See the example below for how to use structured output with the assistant agent.

You can stream the tokens generated by the model client by setting model_client_stream=True. This will cause the agent to yield ModelClientStreamingChunkEvent messages in run_stream().

The underlying model API must support streaming tokens for this to work. Please check with your model provider to see if this is supported.

You can see the streaming chunks in the output above. The chunks are generated by the model client and are yielded by the agent as they are received. The final response, the concatenation of all the chunks, is yielded right after the last chunk.

AssistantAgent has a model_context parameter that can be used to pass in a ChatCompletionContext object. This allows the agent to use different model contexts, such as BufferedChatCompletionContext to limit the context sent to the model.

By default, AssistantAgent uses the UnboundedChatCompletionContext which sends the full conversation history to the model. To limit the context to the last n messages, you can use the BufferedChatCompletionContext. To limit the context by token count, you can use the TokenLimitedChatCompletionContext.

The following preset agents are available:

UserProxyAgent: An agent that takes user input returns it as responses.

CodeExecutorAgent: An agent that can execute code.

OpenAIAssistantAgent: An agent that is backed by an OpenAI Assistant, with ability to use custom tools.

MultimodalWebSurfer: A multi-modal agent that can search the web and visit web pages for information.

FileSurfer: An agent that can search and browse local files for information.

VideoSurfer: An agent that can watch videos for information.

Having explored the usage of the AssistantAgent, we can now proceed to the next section to learn about the teams feature in AgentChat.

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import StructuredMessage
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 2 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import StructuredMessage
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 3 (python):
```python
# Define a tool that searches the web for information.
# For simplicity, we will use a mock function here that returns a static string.
async def web_search(query: str) -> str:
    """Find information on the web"""
    return "AutoGen is a programming framework for building multi-agent applications."


# Create an agent that uses the OpenAI GPT-4o model.
model_client = OpenAIChatCompletionClient(
    model="gpt-4.1-nano",
    # api_key="YOUR_API_KEY",
)
agent = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[web_search],
    system_message="Use tools to solve tasks.",
)
```

Example 4 (python):
```python
# Define a tool that searches the web for information.
# For simplicity, we will use a mock function here that returns a static string.
async def web_search(query: str) -> str:
    """Find information on the web"""
    return "AutoGen is a programming framework for building multi-agent applications."


# Create an agent that uses the OpenAI GPT-4o model.
model_client = OpenAIChatCompletionClient(
    model="gpt-4.1-nano",
    # api_key="YOUR_API_KEY",
)
agent = AssistantAgent(
    name="assistant",
    model_client=model_client,
    tools=[web_search],
    system_message="Use tools to solve tasks.",
)
```

---

## autogen_agentchat.agents — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html

**Contents:**
- autogen_agentchat.agents#

This module initializes various pre-defined agents provided by the package. BaseChatAgent is the base class for all agents in AgentChat.

Bases: ChatAgent, ABC, ComponentBase[BaseModel]

Base class for a chat agent.

This abstract class provides a base implementation for a ChatAgent. To create a new chat agent, subclass this class and implement the on_messages(), on_reset(), and produced_message_types. If streaming is required, also implement the on_messages_stream() method.

An agent is considered stateful and maintains its state between calls to the on_messages() or on_messages_stream() methods. The agent should store its state in the agent instance. The agent should also implement the on_reset() method to reset the agent to its initialization state.

The caller should only pass the new messages to the agent on each call to the on_messages() or on_messages_stream() method. Do not pass the entire conversation history to the agent on each call. This design principle must be followed when creating a new agent.

The logical type of the component.

The name of the agent. This is used by team to uniquely identify the agent. It should be unique within the team.

The description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it.

The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types.

Handles incoming messages and returns a response.

Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.

Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response.

Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.

Run the agent with the given task and return the result.

Run the agent with the given task and return a stream of messages and the final task result as the last item in the stream.

task – The task to run. Can be a string, a single message, or a sequence of messages.

cancellation_token – The cancellation token to kill the task immediately.

output_task_messages – Whether to include task messages in the output stream. Defaults to True for backward compatibility.

Resets the agent to its initialization state.

Called when the agent is paused while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom pause behavior.

Called when the agent is resumed from a pause while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom resume behavior.

Export state. Default implementation for stateless agents.

Restore agent from saved state. Default implementation for stateless agents.

Release any resources held by the agent. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom close behavior.

Bases: BaseChatAgent, Component[AssistantAgentConfig]

An agent that provides assistance with tool use. The on_messages() returns a Response in which chat_message is the final response message.

The on_messages_stream() creates an async generator that produces the inner messages as they are created, and the Response object as the last item before closing the generator.

The BaseChatAgent.run() method returns a TaskResult containing the messages produced by the agent. In the list of messages, messages, the last message is the final response message.

The BaseChatAgent.run_stream() method creates an async generator that produces the inner messages as they are created, and the TaskResult object as the last item before closing the generator.

The caller must only pass the new messages to the agent on each call to the on_messages(), on_messages_stream(), BaseChatAgent.run(), or BaseChatAgent.run_stream() methods. The agent maintains its state between calls to these methods. Do not pass the entire conversation history to the agent on each call.

The assistant agent is not thread-safe or coroutine-safe. It should not be shared between multiple tasks or coroutines, and it should not call its methods concurrently.

The following diagram shows how the assistant agent works:

If the output_content_type is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response by default.

Currently, setting output_content_type prevents the agent from being able to call load_component and dum_component methods for serializable configuration. This will be fixed soon in the future.

If the model returns no tool call, then the response is immediately returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message. This ends the tool call iteration loop regardless of the max_tool_iterations setting.

When reflect_on_tool_use is False, the tool call results are returned as a ToolCallSummaryMessage in chat_message. You can customise the summary with either a static format string (tool_call_summary_format) or a callable (tool_call_summary_formatter); the callable is evaluated once per tool call.

When reflect_on_tool_use is True, the another model inference is made using the tool calls and results, and final response is returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message.

reflect_on_tool_use is set to True by default when output_content_type is set.

reflect_on_tool_use is set to False by default when output_content_type is not set.

If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient.

The max_tool_iterations parameter controls how many sequential tool call iterations the agent can perform in a single run. When set to 1 (default), the agent executes tool calls once and returns the result. When set higher, the agent can make additional model calls to execute more tool calls if the model continues to request them, enabling multi-step tool-based workflows. The agent stops when either the model returns a text response (instead of tool calls) or the maximum number of iterations is reached.

By default, the tool call results are returned as the response when tool calls are made, so pay close attention to how the tools’ return values are formatted—especially if another agent expects a specific schema.

Use `tool_call_summary_format` for a simple static template.

Use `tool_call_summary_formatter` for full programmatic control (e.g., “hide large success payloads, show full details on error”).

Note: tool_call_summary_formatter is not serializable and will be ignored when an agent is loaded from, or exported to, YAML/JSON configuration files.

If a handoff is triggered, a HandoffMessage will be returned in chat_message.

If there are tool calls, they will also be executed right away before returning the handoff.

The tool calls and results are passed to the target agent through context.

If multiple handoffs are detected, only the first handoff is executed. To avoid this, disable parallel tool calls in the model client configuration.

Limit context size sent to the model:

You can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. Another option is to use a TokenLimitedChatCompletionContext which will limit the number of tokens sent to the model. You can also create your own model context by subclassing ChatCompletionContext.

The assistant agent can be used in streaming mode by setting model_client_stream=True. In this mode, the on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. The chunk messages will not be included in the final response’s inner messages.

name (str) – The name of the agent.

model_client (ChatCompletionClient) – The model client to use for inference.

tools (List[BaseTool[Any, Any] | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – The tools to register with the agent.

workbench (Workbench | Sequence[Workbench] | None, optional) – The workbench or list of workbenches to use for the agent. Tools cannot be used when workbench is set and vice versa.

handoffs (List[HandoffBase | str] | None, optional) – The handoff configurations for the agent, allowing it to transfer to other agents by responding with a HandoffMessage. The transfer is only executed when the team is in Swarm. If a handoff is a string, it should represent the target agent’s name.

model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.

description (str, optional) – The description of the agent.

system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable.

model_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False.

reflect_on_tool_use (bool, optional) – If True, the agent will make another model inference using the tool call and result to generate a response. If False, the tool call result will be returned as the response. By default, if output_content_type is set, this will be True; if output_content_type is not set, this will be False.

output_content_type (type[BaseModel] | None, optional) – The output content type for StructuredMessage response as a Pydantic model. This will be used with the model client to generate structured output. If this is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response, unless reflect_on_tool_use is False and a tool call is made.

output_content_type_format (str | None, optional) – (Experimental) The format string used for the content of a StructuredMessage response.

max_tool_iterations (int, optional) – The maximum number of tool iterations to perform until the model stops making tool calls. Defaults to 1, which means the agent will only execute the tool calls made by the model once, and return the result as a ToolCallSummaryMessage, or a TextMessage or a StructuredMessage (when using structured output) in chat_message as the final response. As soon as the model stops making tool calls, the agent will stop executing tool calls and return the result as the final response. The value must be greater than or equal to 1.

tool_call_summary_format (str, optional) – Static format string applied to each tool call result when composing the ToolCallSummaryMessage. Defaults to "{result}". Ignored if tool_call_summary_formatter is provided. When reflect_on_tool_use is False, the summaries for all tool calls are concatenated with a newline (’n’) and returned as the response. Placeholders available in the template: {tool_name}, {arguments}, {result}, {is_error}.

tool_call_summary_formatter (Callable[[FunctionCall, FunctionExecutionResult], str] | None, optional) – Callable that receives the FunctionCall and its FunctionExecutionResult and returns the summary string. Overrides tool_call_summary_format when supplied and allows conditional logic — for example, emitting static string like "Tool FooBar executed successfully." on success and a full payload (including all passed arguments etc.) only on failure. Limitation: The callable is not serializable; values provided via YAML/JSON configs are ignored.

Callable that receives the FunctionCall and its FunctionExecutionResult and returns the summary string. Overrides tool_call_summary_format when supplied and allows conditional logic — for example, emitting static string like "Tool FooBar executed successfully." on success and a full payload (including all passed arguments etc.) only on failure.

Limitation: The callable is not serializable; values provided via YAML/JSON configs are ignored.

tool_call_summary_formatter is intended for in-code use only. It cannot currently be saved or restored via configuration files.

memory (Sequence[Memory] | None, optional): The memory store to use for the agent. Defaults to None. metadata (Dict[str, str] | None, optional): Optional metadata for tracking.

ValueError – If tool names are not unique.

ValueError – If handoff names are not unique.

ValueError – If handoff names are not unique from tool names.

ValueError – If maximum number of tool iterations is less than 1.

Example 1: basic agent

The following example demonstrates how to create an assistant agent with a model client and generate a response to a simple task.

Example 2: model client token streaming

This example demonstrates how to create an assistant agent with a model client and generate a token stream by setting model_client_stream=True.

Example 3: agent with tools

The following example demonstrates how to create an assistant agent with a model client and a tool, generate a stream of messages for a task, and print the messages to the console using Console.

The tool is a simple function that returns the current time. Under the hood, the function is wrapped in a FunctionTool and used with the agent’s model client. The doc string of the function is used as the tool description, the function name is used as the tool name, and the function signature including the type hints is used as the tool arguments.

Example 4: agent with max_tool_iterations

The following example demonstrates how to use the max_tool_iterations parameter to control how many times the agent can execute tool calls in a single run. This is useful when you want the agent to perform multiple sequential tool operations to reach a goal.

Example 5: agent with Model-Context Protocol (MCP) workbench

The following example demonstrates how to create an assistant agent with a model client and an McpWorkbench for interacting with a Model-Context Protocol (MCP) server.

Example 6: agent with structured output and tool

The following example demonstrates how to create an assistant agent with a model client configured to use structured output and a tool. Note that you need to use FunctionTool to create the tool and the strict=True is required for structured output mode. Because the model is configured to use structured output, the output reflection response will be a JSON formatted string.

Example 7: agent with bounded model context

The following example shows how to use a BufferedChatCompletionContext that only keeps the last 2 messages (1 user + 1 assistant). Bounded model context is useful when the model has a limit on the number of tokens it can process.

Example 8: agent with memory

The following example shows how to use a list-based memory with the assistant agent. The memory is preloaded with some initial content. Under the hood, the memory is used to update the model context before making an inference, using the update_context() method.

Example 9: agent with `o1-mini`

The following example shows how to use o1-mini model with the assistant agent.

The o1-preview and o1-mini models do not support system message and function calling. So the system_message should be set to None and the tools and handoffs should not be set. See o1 beta limitations for more details.

Example 10: agent using reasoning model with custom model context.

The following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent. The model context is used to filter out the thought field from the assistant message.

For detailed examples and usage, see the Examples section below.

The version of the component, if schema incompatibilities are introduced this should be updated.

alias of AssistantAgentConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Get the types of messages this agent can produce.

Sequence of message types this agent can generate

Get the model context used by this agent.

The chat completion context for this agent

Process incoming messages and generate a response.

messages – Sequence of messages to process

cancellation_token – Token for cancelling operation

Response containing the agent’s reply

Process messages and stream the response.

messages – Sequence of messages to process

cancellation_token – Token for cancelling operation

Events, messages and final response during processing

Reset the assistant agent to its initialization state.

Save the current state of the assistant agent.

Load the state of the assistant agent

Bases: BaseChatAgent, Component[CodeExecutorAgentConfig]

(Experimental) An agent that generates and executes code snippets based on user instructions.

This agent is experimental and may change in future releases.

It is typically used within a team with another agent that generates code snippets to be executed or alone with model_client provided so that it can generate code based on user query, execute it and reflect on the code result.

When used with model_client, it will generate code snippets using the model and execute them using the provided code_executor. The model will also reflect on the code execution results. The agent will yield the final reflection result from the model as the final response.

When used without model_client, it will only execute code blocks found in TextMessage messages and returns the output of the code execution.

Using AssistantAgent with PythonCodeExecutionTool is an alternative to this agent. However, the model for that agent will have to generate properly escaped code string as a parameter to the tool.

name (str) – The name of the agent.

code_executor (CodeExecutor) – The code executor responsible for executing code received in messages (DockerCommandLineCodeExecutor recommended. See example below)

model_client (ChatCompletionClient, optional) – The model client to use for inference and generating code. If not provided, the agent will only execute code blocks found in input messages. Currently, the model must support structured output mode, which is required for the automatic retry mechanism to work.

model_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False.

description (str, optional) – The description of the agent. If not provided, DEFAULT_AGENT_DESCRIPTION will be used.

system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable. Defaults to DEFAULT_SYSTEM_MESSAGE. This is only used if model_client is provided.

sources (Sequence[str], optional) – Check only messages from the specified agents for the code to execute. This is useful when the agent is part of a group chat and you want to limit the code execution to messages from specific agents. If not provided, all messages will be checked for code blocks. This is only used if model_client is not provided.

max_retries_on_error (int, optional) – The maximum number of retries on error. If the code execution fails, the agent will retry up to this number of times. If the code execution fails after this number of retries, the agent will yield a reflection result.

supported_languages (List[str], optional) – List of programming languages that will be parsed and executed from agent response; others will be ignored. Defaults to DEFAULT_SUPPORTED_LANGUAGES.

approval_func (Optional[Union[Callable[[ApprovalRequest], ApprovalResponse], Callable[[ApprovalRequest], Awaitable[ApprovalResponse]]]], optional) – A function that is called before each code execution to get approval. The function takes an ApprovalRequest containing the code to be executed and the current context, and returns an ApprovalResponse. The function can be either synchronous or asynchronous. If None (default), all code executions are automatically approved. If set, the agent cannot be serialized using dump_component().

It is recommended that the CodeExecutorAgent agent uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running. Follow the installation instructions for Docker.

The code executor only processes code that is properly formatted in markdown code blocks using triple backticks. For example:

In this example, we show how to set up a CodeExecutorAgent agent that uses the DockerCommandLineCodeExecutor to execute code snippets in a Docker container. The work_dir parameter indicates where all executed files are first saved locally before being executed in the Docker container.

In this example, we show how to set up a CodeExecutorAgent agent that uses the DeviceRequest to expose a GPU to the container for cuda-accelerated code execution.

In the following example, we show how to setup CodeExecutorAgent without model_client parameter for executing code blocks generated by other agents in a group chat using DockerCommandLineCodeExecutor

In the following example, we show how to setup CodeExecutorAgent with model_client that can generate its own code without the help of any other agent and executing it in DockerCommandLineCodeExecutor. It also demonstrates using a model-based approval function that reviews the code for safety before execution.

alias of CodeExecutorAgentConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

The types of messages that the code executor agent produces.

The model context in use by the agent.

Handles incoming messages and returns a response.

Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.

Process the incoming messages with the assistant agent and yield events/responses as they happen.

Its a no-op as the code executor agent has no mutable state.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: BaseChatAgent, Component[SocietyOfMindAgentConfig]

An agent that uses an inner team of agents to generate responses.

Each time the agent’s on_messages() or on_messages_stream() method is called, it runs the inner team of agents and then uses the model client to generate a response based on the inner team’s messages. Once the response is generated, the agent resets the inner team by calling Team.reset().

Limit context size sent to the model:

You can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. You can also create your own model context by subclassing ChatCompletionContext.

name (str) – The name of the agent.

team (Team) – The team of agents to use.

model_client (ChatCompletionClient) – The model client to use for preparing responses.

description (str, optional) – The description of the agent.

instruction (str, optional) – The instruction to use when generating a response using the inner team’s messages. Defaults to DEFAULT_INSTRUCTION. It assumes the role of ‘system’.

response_prompt (str, optional) – The response prompt to use when generating a response using the inner team’s messages. Defaults to DEFAULT_RESPONSE_PROMPT. It assumes the role of ‘system’.

model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset.

alias of SocietyOfMindAgentConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

The default instruction to use when generating a response using the inner team’s messages. The instruction will be prepended to the inner team’s messages when generating a response using the model. It assumes the role of ‘system’.

The default response prompt to use when generating a response using the inner team’s messages. It assumes the role of ‘system’.

The default description for a SocietyOfMindAgent.

The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types.

The model context in use by the agent.

Handles incoming messages and returns a response.

Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.

Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response.

Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.

Resets the agent to its initialization state.

Export state. Default implementation for stateless agents.

Restore agent from saved state. Default implementation for stateless agents.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: BaseChatAgent, Component[UserProxyAgentConfig]

An agent that can represent a human user through an input function.

This agent can be used to represent a human user in a chat system by providing a custom input function.

Using UserProxyAgent puts a running team in a temporary blocked state until the user responds. So it is important to time out the user input function and cancel using the CancellationToken if the user does not respond. The input function should also handle exceptions and return a default response if needed.

For typical use cases that involve slow human responses, it is recommended to use termination conditions such as HandoffTermination or SourceMatchTermination to stop the running team and return the control to the application. You can run the team again with the user input. This way, the state of the team can be saved and restored when the user responds.

See Human-in-the-loop for more information.

name (str) – The name of the agent.

description (str, optional) – A description of the agent.

input_func (Optional[Callable[[str], str]], Callable[[str, Optional[CancellationToken]], Awaitable[str]]) – A function that takes a prompt and returns a user input string.

For examples of integrating with web and UI frameworks, see the following:

Cancellable usage case:

The logical type of the component.

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

alias of UserProxyAgentConfig

Message types this agent can produce.

Handles incoming messages and returns a response.

Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.

Handle incoming messages by requesting user input.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: BaseChatAgent, Component[MessageFilterAgentConfig]

A wrapper agent that filters incoming messages before passing them to the inner agent.

This is an experimental feature, and the API will change in the future releases.

This is useful in scenarios like multi-agent workflows where an agent should only process a subset of the full message history—for example, only the last message from each upstream agent, or only the first message from a specific source.

Filtering is configured using MessageFilterConfig, which supports: - Filtering by message source (e.g., only messages from “user” or another agent) - Selecting the first N or last N messages from each source - If position is None, all messages from that source are included

This agent is compatible with both direct message passing and team-based execution such as GraphFlow.

Suppose you have a looping multi-agent graph: A → B → A → B → C.

You want: - A to only see the user message and the last message from B - B to see the user message, last message from A, and its own prior responses (for reflection) - C to see the user message and the last message from B

Wrap the agents like so:

Then define the graph:

This will ensure each agent sees only what is needed for its decision or action logic.

alias of MessageFilterAgentConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types.

Handles incoming messages and returns a response.

Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.

Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response.

Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state.

Resets the agent to its initialization state.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Show JSON schema{ "title": "MessageFilterConfig", "type": "object", "properties": { "per_source": { "items": { "$ref": "#/$defs/PerSourceFilter" }, "title": "Per Source", "type": "array" } }, "$defs": { "PerSourceFilter": { "properties": { "source": { "title": "Source", "type": "string" }, "position": { "anyOf": [ { "enum": [ "first", "last" ], "type": "string" }, { "type": "null" } ], "default": null, "title": "Position" }, "count": { "anyOf": [ { "type": "integer" }, { "type": "null" } ], "default": null, "title": "Count" } }, "required": [ "source" ], "title": "PerSourceFilter", "type": "object" } }, "required": [ "per_source" ] }

per_source (List[autogen_agentchat.agents._message_filter_agent.PerSourceFilter])

Show JSON schema{ "title": "PerSourceFilter", "type": "object", "properties": { "source": { "title": "Source", "type": "string" }, "position": { "anyOf": [ { "enum": [ "first", "last" ], "type": "string" }, { "type": "null" } ], "default": null, "title": "Position" }, "count": { "anyOf": [ { "type": "integer" }, { "type": "null" } ], "default": null, "title": "Count" } }, "required": [ "source" ] }

position (Literal['first', 'last'] | None)

Request for approval of code execution.

Show JSON schema{ "title": "ApprovalRequest", "description": "Request for approval of code execution.", "type": "object", "properties": { "code": { "title": "Code", "type": "string" }, "context": { "items": { "discriminator": { "mapping": { "AssistantMessage": "#/$defs/AssistantMessage", "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage", "SystemMessage": "#/$defs/SystemMessage", "UserMessage": "#/$defs/UserMessage" }, "propertyName": "type" }, "oneOf": [ { "$ref": "#/$defs/SystemMessage" }, { "$ref": "#/$defs/UserMessage" }, { "$ref": "#/$defs/AssistantMessage" }, { "$ref": "#/$defs/FunctionExecutionResultMessage" } ] }, "title": "Context", "type": "array" } }, "$defs": { "AssistantMessage": { "description": "Assistant message are sampled from the language model.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "$ref": "#/$defs/FunctionCall" }, "type": "array" } ], "title": "Content" }, "thought": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Thought" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "AssistantMessage", "default": "AssistantMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ], "title": "AssistantMessage", "type": "object" }, "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" }, "FunctionExecutionResultMessage": { "description": "Function execution result message contains the output of multiple function calls.", "properties": { "content": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Content", "type": "array" }, "type": { "const": "FunctionExecutionResultMessage", "default": "FunctionExecutionResultMessage", "title": "Type", "type": "string" } }, "required": [ "content" ], "title": "FunctionExecutionResultMessage", "type": "object" }, "SystemMessage": { "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n Open AI is moving away from using 'system' role in favor of 'developer' role.\n See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n on the server side.\n So, you can use `SystemMessage` for developer messages.", "properties": { "content": { "title": "Content", "type": "string" }, "type": { "const": "SystemMessage", "default": "SystemMessage", "title": "Type", "type": "string" } }, "required": [ "content" ], "title": "SystemMessage", "type": "object" }, "UserMessage": { "description": "User message contains input from end users, or a catch-all for data provided to the model.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "anyOf": [ { "type": "string" }, {} ] }, "type": "array" } ], "title": "Content" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "UserMessage", "default": "UserMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ], "title": "UserMessage", "type": "object" } }, "required": [ "code", "context" ] }

context (List[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage])

Response to approval request.

Show JSON schema{ "title": "ApprovalResponse", "description": "Response to approval request.", "type": "object", "properties": { "approved": { "title": "Approved", "type": "boolean" }, "reason": { "title": "Reason", "type": "string" } }, "required": [ "approved", "reason" ] }

autogen_agentchat.base

**Examples:**

Example 1 (python):
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client)

    result = await agent.run(task="Name two cities in North America.")
    print(result)


asyncio.run(main())
```

Example 2 (python):
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(name="assistant", model_client=model_client)

    result = await agent.run(task="Name two cities in North America.")
    print(result)


asyncio.run(main())
```

Example 3 (python):
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_client_stream=True,
    )

    stream = agent.run_stream(task="Name two cities in North America.")
    async for message in stream:
        print(message)


asyncio.run(main())
```

Example 4 (python):
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent


async def main() -> None:
    model_client = OpenAIChatCompletionClient(
        model="gpt-4o",
        # api_key = "your_openai_api_key"
    )
    agent = AssistantAgent(
        name="assistant",
        model_client=model_client,
        model_client_stream=True,
    )

    stream = agent.run_stream(task="Name two cities in North America.")
    async for message in stream:
        print(message)


asyncio.run(main())
```

---

## autogen_agentchat.base — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.base.html

**Contents:**
- autogen_agentchat.base#

Bases: ABC, TaskRunner, ComponentBase[BaseModel]

Protocol for a chat agent.

The logical type of the component.

The name of the agent. This is used by team to uniquely identify the agent. It should be unique within the team.

The description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it.

The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types.

Handles incoming messages and returns a response.

Handles incoming messages and returns a stream of inner messages and and the final item is the response.

Resets the agent to its initialization state.

Called when the agent is paused. The agent may be running in on_messages() or on_messages_stream() when this method is called.

Called when the agent is resumed. The agent may be running in on_messages() or on_messages_stream() when this method is called.

Save agent state for later restoration

Restore agent from saved state

Release any resources held by the agent.

A response from calling ChatAgent.on_messages().

A chat message produced by the agent as the response.

Inner messages produced by the agent, they can be BaseAgentEvent or BaseChatMessage.

Bases: ABC, TaskRunner, ComponentBase[BaseModel]

The logical type of the component.

The name of the team. This is used by team to uniquely identify itself in a larger team of teams.

A description of the team. This is used to provide context about the team and its purpose to its parent orchestrator.

Reset the team and all its participants to its initial state.

Pause the team and all its participants. This is useful for pausing the autogen_agentchat.base.TaskRunner.run() or autogen_agentchat.base.TaskRunner.run_stream() methods from concurrently, while keeping them alive.

Resume the team and all its participants from a pause after pause() was called.

Save the current state of the team.

Load the state of the team.

Bases: ABC, ComponentBase[BaseModel]

A stateful condition that determines when a conversation should be terminated.

A termination condition is a callable that takes a sequence of BaseChatMessage objects since the last time the condition was called, and returns a StopMessage if the conversation should be terminated, or None otherwise. Once a termination condition has been reached, it must be reset before it can be used again.

Termination conditions can be combined using the AND and OR operators.

The logical type of the component.

Check if the termination condition has been reached

Reset the termination condition.

Bases: TerminationCondition, Component[AndTerminationConditionConfig]

alias of AndTerminationConditionConfig

The logical type of the component.

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Bases: TerminationCondition, Component[OrTerminationConditionConfig]

alias of OrTerminationConditionConfig

The logical type of the component.

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Result of running a task.

Show JSON schema{ "title": "TaskResult", "description": "Result of running a task.", "type": "object", "properties": { "messages": { "items": { "anyOf": [ { "$ref": "#/$defs/BaseAgentEvent" }, { "$ref": "#/$defs/BaseChatMessage" } ] }, "title": "Messages", "type": "array" }, "stop_reason": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Stop Reason" } }, "$defs": { "BaseAgentEvent": { "description": "Base class for agent events.\n\n.. note::\n\n If you want to create a new message type for signaling observable events\n to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" } }, "required": [ "source" ], "title": "BaseAgentEvent", "type": "object" }, "BaseChatMessage": { "description": "Abstract base class for chat messages.\n\n.. note::\n\n If you want to create a new message type that is used for agent-to-agent\n communication, inherit from this class, or simply use\n :class:`StructuredMessage` if your content type is a subclass of\n Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" } }, "required": [ "source" ], "title": "BaseChatMessage", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "messages" ] }

messages (Sequence[autogen_agentchat.messages.BaseAgentEvent | autogen_agentchat.messages.BaseChatMessage])

stop_reason (str | None)

Messages produced by the task.

The reason the task stopped.

Run the task and return the result.

The task can be a string, a single message, or a sequence of messages.

The runner is stateful and a subsequent call to this method will continue from where the previous call left off. If the task is not specified, the runner will continue with the current task.

task – The task to run. Can be a string, a single message, or a sequence of messages.

cancellation_token – The cancellation token to kill the task immediately.

output_task_messages – Whether to include task messages in TaskResult.messages. Defaults to True for backward compatibility.

Run the task and produces a stream of messages and the final result TaskResult as the last item in the stream.

The task can be a string, a single message, or a sequence of messages.

The runner is stateful and a subsequent call to this method will continue from where the previous call left off. If the task is not specified, the runner will continue with the current task.

task – The task to run. Can be a string, a single message, or a sequence of messages.

cancellation_token – The cancellation token to kill the task immediately.

output_task_messages – Whether to include task messages in the output stream. Defaults to True for backward compatibility.

Handoff configuration.

Show JSON schema{ "title": "Handoff", "description": "Handoff configuration.", "type": "object", "properties": { "target": { "title": "Target", "type": "string" }, "description": { "default": "", "title": "Description", "type": "string" }, "name": { "default": "", "title": "Name", "type": "string" }, "message": { "default": "", "title": "Message", "type": "string" } }, "required": [ "target" ] }

set_defaults » all fields

The name of the target agent to handoff to.

The description of the handoff such as the condition under which it should happen and the target agent’s ability. If not provided, it is generated from the target agent’s name.

The name of this handoff configuration. If not provided, it is generated from the target agent’s name.

The message to the target agent. By default, it will be the result for the handoff tool. If not provided, it is generated from the target agent’s name.

Create a handoff tool from this handoff configuration.

autogen_agentchat.agents

autogen_agentchat.conditions

**Examples:**

Example 1 (python):
```python
import asyncio
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination


async def main() -> None:
    # Terminate the conversation after 10 turns or if the text "TERMINATE" is mentioned.
    cond1 = MaxMessageTermination(10) | TextMentionTermination("TERMINATE")

    # Terminate the conversation after 10 turns and if the text "TERMINATE" is mentioned.
    cond2 = MaxMessageTermination(10) & TextMentionTermination("TERMINATE")

    # ...

    # Reset the termination condition.
    await cond1.reset()
    await cond2.reset()


asyncio.run(main())
```

Example 2 (python):
```python
import asyncio
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination


async def main() -> None:
    # Terminate the conversation after 10 turns or if the text "TERMINATE" is mentioned.
    cond1 = MaxMessageTermination(10) | TextMentionTermination("TERMINATE")

    # Terminate the conversation after 10 turns and if the text "TERMINATE" is mentioned.
    cond2 = MaxMessageTermination(10) & TextMentionTermination("TERMINATE")

    # ...

    # Reset the termination condition.
    await cond1.reset()
    await cond2.reset()


asyncio.run(main())
```

Example 3 (json):
```json
{
   "title": "TaskResult",
   "description": "Result of running a task.",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "anyOf": [
               {
                  "$ref": "#/$defs/BaseAgentEvent"
               },
               {
                  "$ref": "#/$defs/BaseChatMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      },
      "stop_reason": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Reason"
      }
   },
   "$defs": {
      "BaseAgentEvent": {
         "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            },
            "created_at": {
               "format": "date-time",
               "title": "Created At",
               "type": "string"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseAgentEvent",
         "type": "object"
      },
      "BaseChatMessage": {
         "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            },
            "created_at": {
               "format": "date-time",
               "title": "Created At",
               "type": "string"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseChatMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "messages"
   ]
}
```

Example 4 (json):
```json
{
   "title": "TaskResult",
   "description": "Result of running a task.",
   "type": "object",
   "properties": {
      "messages": {
         "items": {
            "anyOf": [
               {
                  "$ref": "#/$defs/BaseAgentEvent"
               },
               {
                  "$ref": "#/$defs/BaseChatMessage"
               }
            ]
         },
         "title": "Messages",
         "type": "array"
      },
      "stop_reason": {
         "anyOf": [
            {
               "type": "string"
            },
            {
               "type": "null"
            }
         ],
         "default": null,
         "title": "Stop Reason"
      }
   },
   "$defs": {
      "BaseAgentEvent": {
         "description": "Base class for agent events.\n\n.. note::\n\n    If you want to create a new message type for signaling observable events\n    to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.",
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            },
            "created_at": {
               "format": "date-time",
               "title": "Created At",
               "type": "string"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseAgentEvent",
         "type": "object"
      },
      "BaseChatMessage": {
         "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
         "properties": {
            "id": {
               "title": "Id",
               "type": "string"
            },
            "source": {
               "title": "Source",
               "type": "string"
            },
            "models_usage": {
               "anyOf": [
                  {
                     "$ref": "#/$defs/RequestUsage"
                  },
                  {
                     "type": "null"
                  }
               ],
               "default": null
            },
            "metadata": {
               "additionalProperties": {
                  "type": "string"
               },
               "default": {},
               "title": "Metadata",
               "type": "object"
            },
            "created_at": {
               "format": "date-time",
               "title": "Created At",
               "type": "string"
            }
         },
         "required": [
            "source"
         ],
         "title": "BaseChatMessage",
         "type": "object"
      },
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "messages"
   ]
}
```

---

## autogen_agentchat.conditions — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html

**Contents:**
- autogen_agentchat.conditions#

This module provides various termination conditions for controlling the behavior of multi-agent teams.

Bases: TerminationCondition, Component[MaxMessageTerminationConfig]

Terminate the conversation after a maximum number of messages have been exchanged.

max_messages – The maximum number of messages allowed in the conversation.

include_agent_event – If True, include BaseAgentEvent in the message count. Otherwise, only include BaseChatMessage. Defaults to False.

alias of MaxMessageTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[TextMentionTerminationConfig]

Terminate the conversation if a specific text is mentioned.

text – The text to look for in the messages.

sources – Check only messages of the specified agents for the text to look for.

alias of TextMentionTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[StopMessageTerminationConfig]

Terminate the conversation if a StopMessage is received.

alias of StopMessageTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[TokenUsageTerminationConfig]

Terminate the conversation if a token usage limit is reached.

max_total_token – The maximum total number of tokens allowed in the conversation.

max_prompt_token – The maximum number of prompt tokens allowed in the conversation.

max_completion_token – The maximum number of completion tokens allowed in the conversation.

ValueError – If none of max_total_token, max_prompt_token, or max_completion_token is provided.

alias of TokenUsageTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[HandoffTerminationConfig]

Terminate the conversation if a HandoffMessage with the given target is received.

target (str) – The target of the handoff message.

alias of HandoffTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[TimeoutTerminationConfig]

Terminate the conversation after a specified duration has passed.

timeout_seconds – The maximum duration in seconds before terminating the conversation.

alias of TimeoutTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[ExternalTerminationConfig]

A termination condition that is externally controlled by calling the set() method.

alias of ExternalTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Set the termination condition to terminated.

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[SourceMatchTerminationConfig]

Terminate the conversation after a specific source responds.

sources (List[str]) – List of source names to terminate the conversation.

TerminatedException – If the termination condition has already been reached.

alias of SourceMatchTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[TextMessageTerminationConfig]

Terminate the conversation if a TextMessage is received.

This termination condition checks for TextMessage instances in the message sequence. When a TextMessage is found, it terminates the conversation if either: - No source was specified (terminates on any TextMessage) - The message source matches the specified source

source (str | None, optional) – The source name to match against incoming messages. If None, matches any source. Defaults to None.

alias of TextMessageTerminationConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition, Component[FunctionCallTerminationConfig]

Terminate the conversation if a FunctionExecutionResult with a specific name was received.

function_name (str) – The name of the function to look for in the messages.

TerminatedException – If the termination condition has already been reached.

alias of FunctionCallTerminationConfig

The schema for the component configuration.

Check if the termination condition has been reached

Reset the termination condition.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TerminationCondition

Terminate the conversation if an functional expression is met.

func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]]) – A function that takes a sequence of messages and returns True if the termination condition is met, False otherwise. The function can be a callable or an async callable.

Check if the termination condition has been reached

Reset the termination condition.

autogen_agentchat.base

autogen_agentchat.messages

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.conditions import ExternalTermination

termination = ExternalTermination()

# Run the team in an asyncio task.
...

# Set the termination condition externally
termination.set()
```

Example 2 (sql):
```sql
from autogen_agentchat.conditions import ExternalTermination

termination = ExternalTermination()

# Run the team in an asyncio task.
...

# Set the termination condition externally
termination.set()
```

Example 3 (python):
```python
import asyncio
from typing import Sequence

from autogen_agentchat.conditions import FunctionalTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage


def expression(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> bool:
    # Check if the last message is a stop message
    return isinstance(messages[-1], StopMessage)


termination = FunctionalTermination(expression)


async def run() -> None:
    messages = [
        StopMessage(source="agent1", content="Stop"),
    ]
    result = await termination(messages)
    print(result)


asyncio.run(run())
```

Example 4 (python):
```python
import asyncio
from typing import Sequence

from autogen_agentchat.conditions import FunctionalTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage


def expression(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> bool:
    # Check if the last message is a stop message
    return isinstance(messages[-1], StopMessage)


termination = FunctionalTermination(expression)


async def run() -> None:
    messages = [
        StopMessage(source="agent1", content="Stop"),
    ]
    result = await termination(messages)
    print(result)


asyncio.run(run())
```

---

## autogen_agentchat.messages — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.messages.html

**Contents:**
- autogen_agentchat.messages#

This module defines various message types used for agent-to-agent communication. Each message type inherits either from the BaseChatMessage class or BaseAgentEvent class and includes specific fields relevant to the type of message being sent.

The union type of all built-in concrete subclasses of BaseAgentEvent.

alias of Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator=’type’)]

Bases: BaseModel, ABC

Abstract base class for all message types in AgentChat.

If you want to create a new message type, do not inherit from this class. Instead, inherit from BaseChatMessage or BaseAgentEvent to clarify the purpose of the message type.

Show JSON schema{ "title": "BaseMessage", "description": "Abstract base class for all message types in AgentChat.\n\n.. warning::\n\n If you want to create a new message type, do not inherit from this class.\n Instead, inherit from :class:`BaseChatMessage` or :class:`BaseAgentEvent`\n to clarify the purpose of the message type.", "type": "object", "properties": {} }

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Convert the message to a JSON-serializable dictionary.

The default implementation uses the Pydantic model’s model_dump() method to convert the message to a dictionary. Datetime objects are automatically converted to ISO format strings to ensure JSON serialization compatibility. Override this method if you want to customize the serialization process or add additional fields to the output.

Create a message from a dictionary of JSON-serializable data.

The default implementation uses the Pydantic model’s model_validate() method to create the message from the data. Override this method if you want to customize the deserialization process or add additional fields to the input data.

The union type of all built-in concrete subclasses of BaseChatMessage. It does not include StructuredMessage types.

alias of Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator=’type’)]

Bases: BaseMessage, ABC

Abstract base class for chat messages.

If you want to create a new message type that is used for agent-to-agent communication, inherit from this class, or simply use StructuredMessage if your content type is a subclass of Pydantic BaseModel.

This class is used for messages that are sent between agents in a chat conversation. Agents are expected to process the content of the message using models and return a response as another BaseChatMessage.

Show JSON schema{ "title": "BaseChatMessage", "description": "Abstract base class for chat messages.\n\n.. note::\n\n If you want to create a new message type that is used for agent-to-agent\n communication, inherit from this class, or simply use\n :class:`StructuredMessage` if your content type is a subclass of\n Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source" ] }

created_at (datetime.datetime)

metadata (Dict[str, str])

models_usage (autogen_core.models._types.RequestUsage | None)

Unique identifier for this message.

The name of the agent that sent this message.

The model client usage incurred when producing this message.

Additional metadata about the message.

The time when the message was created.

Convert the content of the message to text-only representation. This is used for creating text-only content for models.

This is not used for rendering the message in console. For that, use to_text().

The difference between this and to_model_message() is that this is used to construct parts of the a message for the model client, while to_model_message() is used to create a complete message for the model client.

Convert the message content to a UserMessage for use with model client, e.g., ChatCompletionClient.

Bases: BaseMessage, ABC

Base class for agent events.

If you want to create a new message type for signaling observable events to user and application, inherit from this class.

Agent events are used to signal actions and thoughts produced by agents and teams to user and applications. They are not used for agent-to-agent communication and are not expected to be processed by other agents.

You should override the to_text() method if you want to provide a custom rendering of the content.

Show JSON schema{ "title": "BaseAgentEvent", "description": "Base class for agent events.\n\n.. note::\n\n If you want to create a new message type for signaling observable events\n to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source" ] }

created_at (datetime.datetime)

metadata (Dict[str, str])

models_usage (autogen_core.models._types.RequestUsage | None)

Unique identifier for this event.

The name of the agent that sent this message.

The model client usage incurred when producing this message.

Additional metadata about the message.

The time when the message was created.

Bases: BaseChatMessage, ABC

Base class for all text-only BaseChatMessage types. It has implementations for to_text(), to_model_text(), and to_model_message() methods.

Inherit from this class if your message content type is a string.

Show JSON schema{ "title": "BaseTextChatMessage", "description": "Base class for all text-only :class:`BaseChatMessage` types.\nIt has implementations for :meth:`to_text`, :meth:`to_model_text`,\nand :meth:`to_model_message` methods.\n\nInherit from this class if your message content type is a string.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

The content of the message.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Convert the content of the message to text-only representation. This is used for creating text-only content for models.

This is not used for rendering the message in console. For that, use to_text().

The difference between this and to_model_message() is that this is used to construct parts of the a message for the model client, while to_model_message() is used to create a complete message for the model client.

Convert the message content to a UserMessage for use with model client, e.g., ChatCompletionClient.

Type variable for structured content types.

alias of TypeVar(‘StructuredContentType’, bound=BaseModel, covariant=True)

Bases: BaseChatMessage, Generic[StructuredContentType]

A BaseChatMessage type with an unspecified content type.

To create a new structured message type, specify the content type as a subclass of Pydantic BaseModel.

Show JSON schema{ "title": "StructuredMessage", "description": "A :class:`BaseChatMessage` type with an unspecified content type.\n\nTo create a new structured message type, specify the content type\nas a subclass of `Pydantic BaseModel <https://docs.pydantic.dev/latest/concepts/models/>`_.\n\n.. code-block:: python\n\n from pydantic import BaseModel\n from autogen_agentchat.messages import StructuredMessage\n\n\n class MyMessageContent(BaseModel):\n text: str\n number: int\n\n\n message = StructuredMessage[MyMessageContent](\n content=MyMessageContent(text=\"Hello\", number=42),\n source=\"agent1\",\n )\n\n print(message.to_text()) # {\"text\": \"Hello\", \"number\": 42}\n\n.. code-block:: python\n\n from pydantic import BaseModel\n from autogen_agentchat.messages import StructuredMessage\n\n\n class MyMessageContent(BaseModel):\n text: str\n number: int\n\n\n message = StructuredMessage[MyMessageContent](\n content=MyMessageContent(text=\"Hello\", number=42),\n source=\"agent\",\n format_string=\"Hello, {text} {number}!\",\n )\n\n print(message.to_text()) # Hello, agent 42!", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "$ref": "#/$defs/BaseModel" }, "format_string": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Format String" } }, "$defs": { "BaseModel": { "properties": {}, "title": "BaseModel", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

content (autogen_agentchat.messages.StructuredContentType)

format_string (str | None)

The content of the message. Must be a subclass of Pydantic BaseModel.

(Experimental) An optional format string to render the content into a human-readable format. The format string can use the fields of the content model as placeholders. For example, if the content model has a field name, you can use {name} in the format string to include the value of that field. The format string is used in the to_text() method to create a human-readable representation of the message. This setting is experimental and will change in the future.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Convert the content of the message to text-only representation. This is used for creating text-only content for models.

This is not used for rendering the message in console. For that, use to_text().

The difference between this and to_model_message() is that this is used to construct parts of the a message for the model client, while to_model_message() is used to create a complete message for the model client.

Convert the message content to a UserMessage for use with model client, e.g., ChatCompletionClient.

Bases: BaseTextChatMessage

A message requesting handoff of a conversation to another agent.

Show JSON schema{ "title": "HandoffMessage", "description": "A message requesting handoff of a conversation to another agent.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "target": { "title": "Target", "type": "string" }, "context": { "default": [], "items": { "discriminator": { "mapping": { "AssistantMessage": "#/$defs/AssistantMessage", "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage", "SystemMessage": "#/$defs/SystemMessage", "UserMessage": "#/$defs/UserMessage" }, "propertyName": "type" }, "oneOf": [ { "$ref": "#/$defs/SystemMessage" }, { "$ref": "#/$defs/UserMessage" }, { "$ref": "#/$defs/AssistantMessage" }, { "$ref": "#/$defs/FunctionExecutionResultMessage" } ] }, "title": "Context", "type": "array" }, "type": { "const": "HandoffMessage", "default": "HandoffMessage", "title": "Type", "type": "string" } }, "$defs": { "AssistantMessage": { "description": "Assistant message are sampled from the language model.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "$ref": "#/$defs/FunctionCall" }, "type": "array" } ], "title": "Content" }, "thought": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Thought" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "AssistantMessage", "default": "AssistantMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ], "title": "AssistantMessage", "type": "object" }, "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" }, "FunctionExecutionResultMessage": { "description": "Function execution result message contains the output of multiple function calls.", "properties": { "content": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Content", "type": "array" }, "type": { "const": "FunctionExecutionResultMessage", "default": "FunctionExecutionResultMessage", "title": "Type", "type": "string" } }, "required": [ "content" ], "title": "FunctionExecutionResultMessage", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" }, "SystemMessage": { "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n Open AI is moving away from using 'system' role in favor of 'developer' role.\n See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n on the server side.\n So, you can use `SystemMessage` for developer messages.", "properties": { "content": { "title": "Content", "type": "string" }, "type": { "const": "SystemMessage", "default": "SystemMessage", "title": "Type", "type": "string" } }, "required": [ "content" ], "title": "SystemMessage", "type": "object" }, "UserMessage": { "description": "User message contains input from end users, or a catch-all for data provided to the model.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "anyOf": [ { "type": "string" }, {} ] }, "type": "array" } ], "title": "Content" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "UserMessage", "default": "UserMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ], "title": "UserMessage", "type": "object" } }, "required": [ "source", "content", "target" ] }

context (List[Annotated[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]])

type (Literal['HandoffMessage'])

The name of the target agent to handoff to.

The model context to be passed to the target agent.

Bases: BaseChatMessage

A multimodal message.

Show JSON schema{ "title": "MultiModalMessage", "description": "A multimodal message.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "anyOf": [ { "type": "string" }, {} ] }, "title": "Content", "type": "array" }, "type": { "const": "MultiModalMessage", "default": "MultiModalMessage", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

content (List[str | autogen_core._image.Image])

type (Literal['MultiModalMessage'])

The content of the message.

Convert the content of the message to a string-only representation. If an image is present, it will be replaced with the image placeholder by default, otherwise it will be a base64 string when set to None.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Convert the message content to a UserMessage for use with model client, e.g., ChatCompletionClient.

Bases: BaseTextChatMessage

A message requesting stop of a conversation.

Show JSON schema{ "title": "StopMessage", "description": "A message requesting stop of a conversation.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "type": { "const": "StopMessage", "default": "StopMessage", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

type (Literal['StopMessage'])

Bases: BaseTextChatMessage

A text message with string-only content.

Show JSON schema{ "title": "TextMessage", "description": "A text message with string-only content.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "type": { "const": "TextMessage", "default": "TextMessage", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

type (Literal['TextMessage'])

Bases: BaseAgentEvent

An event signaling the execution of tool calls.

Show JSON schema{ "title": "ToolCallExecutionEvent", "description": "An event signaling the execution of tool calls.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Content", "type": "array" }, "type": { "const": "ToolCallExecutionEvent", "default": "ToolCallExecutionEvent", "title": "Type", "type": "string" } }, "$defs": { "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

content (List[autogen_core.models._types.FunctionExecutionResult])

type (Literal['ToolCallExecutionEvent'])

The tool call results.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Bases: BaseAgentEvent

An event signaling a request to use tools.

Show JSON schema{ "title": "ToolCallRequestEvent", "description": "An event signaling a request to use tools.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "$ref": "#/$defs/FunctionCall" }, "title": "Content", "type": "array" }, "type": { "const": "ToolCallRequestEvent", "default": "ToolCallRequestEvent", "title": "Type", "type": "string" } }, "$defs": { "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

content (List[autogen_core._types.FunctionCall])

type (Literal['ToolCallRequestEvent'])

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Bases: BaseTextChatMessage

A message signaling the summary of tool call results.

Show JSON schema{ "title": "ToolCallSummaryMessage", "description": "A message signaling the summary of tool call results.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "type": { "const": "ToolCallSummaryMessage", "default": "ToolCallSummaryMessage", "title": "Type", "type": "string" }, "tool_calls": { "items": { "$ref": "#/$defs/FunctionCall" }, "title": "Tool Calls", "type": "array" }, "results": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Results", "type": "array" } }, "$defs": { "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content", "tool_calls", "results" ] }

results (List[autogen_core.models._types.FunctionExecutionResult])

tool_calls (List[autogen_core._types.FunctionCall])

type (Literal['ToolCallSummaryMessage'])

The tool calls that were made.

The results of the tool calls.

Bases: BaseAgentEvent

An event signaling the results of memory queries.

Show JSON schema{ "title": "MemoryQueryEvent", "description": "An event signaling the results of memory queries.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "$ref": "#/$defs/MemoryContent" }, "title": "Content", "type": "array" }, "type": { "const": "MemoryQueryEvent", "default": "MemoryQueryEvent", "title": "Type", "type": "string" } }, "$defs": { "MemoryContent": { "description": "A memory content item.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "format": "binary", "type": "string" }, { "type": "object" }, {} ], "title": "Content" }, "mime_type": { "anyOf": [ { "$ref": "#/$defs/MemoryMimeType" }, { "type": "string" } ], "title": "Mime Type" }, "metadata": { "anyOf": [ { "type": "object" }, { "type": "null" } ], "default": null, "title": "Metadata" } }, "required": [ "content", "mime_type" ], "title": "MemoryContent", "type": "object" }, "MemoryMimeType": { "description": "Supported MIME types for memory content.", "enum": [ "text/plain", "application/json", "text/markdown", "image/*", "application/octet-stream" ], "title": "MemoryMimeType", "type": "string" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

content (List[autogen_core.memory._base_memory.MemoryContent])

type (Literal['MemoryQueryEvent'])

The memory query results.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Bases: BaseAgentEvent

An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.

Show JSON schema{ "title": "UserInputRequestedEvent", "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "request_id": { "title": "Request Id", "type": "string" }, "content": { "const": "", "default": "", "title": "Content", "type": "string" }, "type": { "const": "UserInputRequestedEvent", "default": "UserInputRequestedEvent", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "request_id" ] }

content (Literal[''])

type (Literal['UserInputRequestedEvent'])

Identifier for the user input request.

Empty content for compat with consumers expecting a content field.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Bases: BaseAgentEvent

An event signaling a text output chunk from a model client in streaming mode.

Show JSON schema{ "title": "ModelClientStreamingChunkEvent", "description": "An event signaling a text output chunk from a model client in streaming mode.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "full_message_id": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Full Message Id" }, "type": { "const": "ModelClientStreamingChunkEvent", "default": "ModelClientStreamingChunkEvent", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

full_message_id (str | None)

type (Literal['ModelClientStreamingChunkEvent'])

A string chunk from the model client.

Optional reference to the complete message that may come after the chunks. This allows consumers of the stream to correlate chunks with the eventual completed message.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Bases: BaseAgentEvent

An event signaling the thought process of a model. It is used to communicate the reasoning tokens generated by a reasoning model, or the extra text content generated by a function call.

Show JSON schema{ "title": "ThoughtEvent", "description": "An event signaling the thought process of a model.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "type": { "const": "ThoughtEvent", "default": "ThoughtEvent", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

type (Literal['ThoughtEvent'])

The thought process of the model.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Bases: BaseAgentEvent

An event signaling the selection of speakers for a conversation.

Show JSON schema{ "title": "SelectSpeakerEvent", "description": "An event signaling the selection of speakers for a conversation.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "type": "string" }, "title": "Content", "type": "array" }, "type": { "const": "SelectSpeakerEvent", "default": "SelectSpeakerEvent", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] }

type (Literal['SelectSpeakerEvent'])

The names of the selected speakers.

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Bases: BaseAgentEvent

An event signaling code generation event.

Show JSON schema{ "title": "CodeGenerationEvent", "description": "An event signaling code generation event.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "retry_attempt": { "title": "Retry Attempt", "type": "integer" }, "content": { "title": "Content", "type": "string" }, "code_blocks": { "items": { "$ref": "#/$defs/CodeBlock" }, "title": "Code Blocks", "type": "array" }, "type": { "const": "CodeGenerationEvent", "default": "CodeGenerationEvent", "title": "Type", "type": "string" } }, "$defs": { "CodeBlock": { "properties": { "code": { "title": "Code", "type": "string" }, "language": { "title": "Language", "type": "string" } }, "required": [ "code", "language" ], "title": "CodeBlock", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "retry_attempt", "content", "code_blocks" ] }

code_blocks (List[autogen_core.code_executor._base.CodeBlock])

type (Literal['CodeGenerationEvent'])

Retry number, 0 means first generation

The complete content as string.

List of code blocks present in content

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

Bases: BaseAgentEvent

An event signaling code execution event.

Show JSON schema{ "title": "CodeExecutionEvent", "description": "An event signaling code execution event.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "retry_attempt": { "title": "Retry Attempt", "type": "integer" }, "result": { "$ref": "#/$defs/CodeResult" }, "type": { "const": "CodeExecutionEvent", "default": "CodeExecutionEvent", "title": "Type", "type": "string" } }, "$defs": { "CodeResult": { "properties": { "exit_code": { "title": "Exit Code", "type": "integer" }, "output": { "title": "Output", "type": "string" } }, "required": [ "exit_code", "output" ], "title": "CodeResult", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "retry_attempt", "result" ] }

result (autogen_core.code_executor._base.CodeResult)

type (Literal['CodeExecutionEvent'])

Retry number, 0 means first execution

Code Execution Result

Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead.

autogen_agentchat.conditions

autogen_agentchat.state

**Examples:**

Example 1 (json):
```json
{
   "title": "BaseMessage",
   "description": "Abstract base class for all message types in AgentChat.\n\n.. warning::\n\n    If you want to create a new message type, do not inherit from this class.\n    Instead, inherit from :class:`BaseChatMessage` or :class:`BaseAgentEvent`\n    to clarify the purpose of the message type.",
   "type": "object",
   "properties": {}
}
```

Example 2 (json):
```json
{
   "title": "BaseMessage",
   "description": "Abstract base class for all message types in AgentChat.\n\n.. warning::\n\n    If you want to create a new message type, do not inherit from this class.\n    Instead, inherit from :class:`BaseChatMessage` or :class:`BaseAgentEvent`\n    to clarify the purpose of the message type.",
   "type": "object",
   "properties": {}
}
```

Example 3 (json):
```json
{
   "title": "BaseChatMessage",
   "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
   "type": "object",
   "properties": {
      "id": {
         "title": "Id",
         "type": "string"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "created_at": {
         "format": "date-time",
         "title": "Created At",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}
```

Example 4 (json):
```json
{
   "title": "BaseChatMessage",
   "description": "Abstract base class for chat messages.\n\n.. note::\n\n    If you want to create a new message type that is used for agent-to-agent\n    communication, inherit from this class, or simply use\n    :class:`StructuredMessage` if your content type is a subclass of\n    Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.",
   "type": "object",
   "properties": {
      "id": {
         "title": "Id",
         "type": "string"
      },
      "source": {
         "title": "Source",
         "type": "string"
      },
      "models_usage": {
         "anyOf": [
            {
               "$ref": "#/$defs/RequestUsage"
            },
            {
               "type": "null"
            }
         ],
         "default": null
      },
      "metadata": {
         "additionalProperties": {
            "type": "string"
         },
         "default": {},
         "title": "Metadata",
         "type": "object"
      },
      "created_at": {
         "format": "date-time",
         "title": "Created At",
         "type": "string"
      }
   },
   "$defs": {
      "RequestUsage": {
         "properties": {
            "prompt_tokens": {
               "title": "Prompt Tokens",
               "type": "integer"
            },
            "completion_tokens": {
               "title": "Completion Tokens",
               "type": "integer"
            }
         },
         "required": [
            "prompt_tokens",
            "completion_tokens"
         ],
         "title": "RequestUsage",
         "type": "object"
      }
   },
   "required": [
      "source"
   ]
}
```

---

## autogen_agentchat.state — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.state.html

**Contents:**
- autogen_agentchat.state#

State management for agents, teams and termination conditions.

Base class for all saveable state

Show JSON schema{ "title": "BaseState", "description": "Base class for all saveable state", "type": "object", "properties": { "type": { "default": "BaseState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" } } }

State for an assistant agent.

Show JSON schema{ "title": "AssistantAgentState", "description": "State for an assistant agent.", "type": "object", "properties": { "type": { "default": "AssistantAgentState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "llm_context": { "title": "Llm Context", "type": "object" } } }

llm_context (Mapping[str, Any])

Base state for all group chat managers.

Show JSON schema{ "title": "BaseGroupChatManagerState", "description": "Base state for all group chat managers.", "type": "object", "properties": { "type": { "default": "BaseGroupChatManagerState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "message_thread": { "items": { "type": "object" }, "title": "Message Thread", "type": "array" }, "current_turn": { "default": 0, "title": "Current Turn", "type": "integer" } } }

message_thread (List[Mapping[str, Any]])

State for a container of chat agents.

Show JSON schema{ "title": "ChatAgentContainerState", "description": "State for a container of chat agents.", "type": "object", "properties": { "type": { "default": "ChatAgentContainerState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "agent_state": { "title": "Agent State", "type": "object" }, "message_buffer": { "items": { "type": "object" }, "title": "Message Buffer", "type": "array" } } }

agent_state (Mapping[str, Any])

message_buffer (List[Mapping[str, Any]])

Bases: BaseGroupChatManagerState

State for RoundRobinGroupChat manager.

Show JSON schema{ "title": "RoundRobinManagerState", "description": "State for :class:`~autogen_agentchat.teams.RoundRobinGroupChat` manager.", "type": "object", "properties": { "type": { "default": "RoundRobinManagerState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "message_thread": { "items": { "type": "object" }, "title": "Message Thread", "type": "array" }, "current_turn": { "default": 0, "title": "Current Turn", "type": "integer" }, "next_speaker_index": { "default": 0, "title": "Next Speaker Index", "type": "integer" } } }

next_speaker_index (int)

Bases: BaseGroupChatManagerState

State for SelectorGroupChat manager.

Show JSON schema{ "title": "SelectorManagerState", "description": "State for :class:`~autogen_agentchat.teams.SelectorGroupChat` manager.", "type": "object", "properties": { "type": { "default": "SelectorManagerState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "message_thread": { "items": { "type": "object" }, "title": "Message Thread", "type": "array" }, "current_turn": { "default": 0, "title": "Current Turn", "type": "integer" }, "previous_speaker": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Previous Speaker" } } }

previous_speaker (str | None)

Bases: BaseGroupChatManagerState

State for Swarm manager.

Show JSON schema{ "title": "SwarmManagerState", "description": "State for :class:`~autogen_agentchat.teams.Swarm` manager.", "type": "object", "properties": { "type": { "default": "SwarmManagerState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "message_thread": { "items": { "type": "object" }, "title": "Message Thread", "type": "array" }, "current_turn": { "default": 0, "title": "Current Turn", "type": "integer" }, "current_speaker": { "default": "", "title": "Current Speaker", "type": "string" } } }

current_speaker (str)

Bases: BaseGroupChatManagerState

State for MagneticOneGroupChat orchestrator.

Show JSON schema{ "title": "MagenticOneOrchestratorState", "description": "State for :class:`~autogen_agentchat.teams.MagneticOneGroupChat` orchestrator.", "type": "object", "properties": { "type": { "default": "MagenticOneOrchestratorState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "message_thread": { "items": { "type": "object" }, "title": "Message Thread", "type": "array" }, "current_turn": { "default": 0, "title": "Current Turn", "type": "integer" }, "task": { "default": "", "title": "Task", "type": "string" }, "facts": { "default": "", "title": "Facts", "type": "string" }, "plan": { "default": "", "title": "Plan", "type": "string" }, "n_rounds": { "default": 0, "title": "N Rounds", "type": "integer" }, "n_stalls": { "default": 0, "title": "N Stalls", "type": "integer" } } }

State for a team of agents.

Show JSON schema{ "title": "TeamState", "description": "State for a team of agents.", "type": "object", "properties": { "type": { "default": "TeamState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "agent_states": { "title": "Agent States", "type": "object" } } }

agent_states (Mapping[str, Any])

State for a Society of Mind agent.

Show JSON schema{ "title": "SocietyOfMindAgentState", "description": "State for a Society of Mind agent.", "type": "object", "properties": { "type": { "default": "SocietyOfMindAgentState", "title": "Type", "type": "string" }, "version": { "default": "1.0.0", "title": "Version", "type": "string" }, "inner_team_state": { "title": "Inner Team State", "type": "object" } } }

inner_team_state (Mapping[str, Any])

autogen_agentchat.messages

autogen_agentchat.teams

**Examples:**

Example 1 (json):
```json
{
   "title": "BaseState",
   "description": "Base class for all saveable state",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      }
   }
}
```

Example 2 (json):
```json
{
   "title": "BaseState",
   "description": "Base class for all saveable state",
   "type": "object",
   "properties": {
      "type": {
         "default": "BaseState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      }
   }
}
```

Example 3 (json):
```json
{
   "title": "AssistantAgentState",
   "description": "State for an assistant agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "AssistantAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "llm_context": {
         "title": "Llm Context",
         "type": "object"
      }
   }
}
```

Example 4 (json):
```json
{
   "title": "AssistantAgentState",
   "description": "State for an assistant agent.",
   "type": "object",
   "properties": {
      "type": {
         "default": "AssistantAgentState",
         "title": "Type",
         "type": "string"
      },
      "version": {
         "default": "1.0.0",
         "title": "Version",
         "type": "string"
      },
      "llm_context": {
         "title": "Llm Context",
         "type": "object"
      }
   }
}
```

---

## autogen_agentchat.teams — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html

**Contents:**
- autogen_agentchat.teams#

This module provides implementation of various pre-defined multi-agent teams. Each team inherits from the BaseGroupChat class.

Bases: Team, ABC, ComponentBase[BaseModel]

The base class for group chat teams.

In a group chat team, participants share context by publishing their messages to all other participants.

If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat.

If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat.

To implement a group chat team, first create a subclass of BaseGroupChatManager and then create a subclass of BaseGroupChat that uses the group chat manager.

This base class provides the mapping between the agents of the AgentChat API and the agent runtime of the Core API, and handles high-level features like running, pausing, resuming, and resetting the team.

The logical type of the component.

The name of the group chat team.

A description of the group chat team.

Run the team and return the result. The base implementation uses run_stream() to run the team and then returns the final result. Once the team is stopped, the termination condition is reset.

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.

cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead.

result – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason.

Example using the RoundRobinGroupChat team:

Example using the CancellationToken to cancel the task:

Run the team and produces a stream of messages and the final result of the type TaskResult as the last item in the stream. Once the team is stopped, the termination condition is reset.

If an agent produces ModelClientStreamingChunkEvent, the message will be yielded in the stream but it will not be included in the messages.

task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage.

cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead.

output_task_messages (bool) – Whether to include task messages in the output stream. Defaults to True for backward compatibility.

stream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream.

Example using the RoundRobinGroupChat team:

Example using the CancellationToken to cancel the task:

Reset the team and its participants to their initial state.

The team must be stopped before it can be reset.

RuntimeError – If the team has not been initialized or is currently running.

Example using the RoundRobinGroupChat team:

Pause its participants when the team is running by calling their on_pause() method via direct RPC calls.

This is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future.

The team must be initialized before it can be paused.

Different from termination, pausing the team does not cause the run() or run_stream() method to return. It calls the on_pause() method on each participant, and if the participant does not implement the method, it will be a no-op.

It is the responsibility of the agent class to handle the pause and ensure that the agent can be resumed later. Make sure to implement the on_pause() method in your agent class for custom pause behavior. By default, the agent will not do anything when called.

RuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_pause are propagated to this method and raised.

Resume its participants when the team is running and paused by calling their on_resume() method via direct RPC calls.

This is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future.

The team must be initialized before it can be resumed.

Different from termination and restart with a new task, resuming the team does not cause the run() or run_stream() method to return. It calls the on_resume() method on each participant, and if the participant does not implement the method, it will be a no-op.

It is the responsibility of the agent class to handle the resume and ensure that the agent continues from where it was paused. Make sure to implement the on_resume() method in your agent class for custom resume behavior.

RuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_resume method are propagated to this method and raised.

Save the state of the group chat team.

The state is saved by calling the agent_save_state() method on each participant and the group chat manager with their internal agent ID. The state is returned as a nested dictionary: a dictionary with key agent_states, which is a dictionary the agent names as keys and the state as values.

Starting v0.4.9, the state is using the agent name as the key instead of the agent ID, and the team_id field is removed from the state. This is to allow the state to be portable across different teams and runtimes. States saved with the old format may not be compatible with the new format in the future.

When calling save_state() on a team while it is running, the state may not be consistent and may result in an unexpected state. It is recommended to call this method when the team is not running or after it is stopped.

Load an external state and overwrite the current state of the group chat team.

The state is loaded by calling the agent_load_state() method on each participant and the group chat manager with their internal agent ID. See save_state() for the expected format of the state.

Bases: BaseGroupChat, Component[RoundRobinGroupChatConfig]

A team that runs a group chat with participants taking turns in a round-robin fashion to publish a message to all.

If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat.

If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat.

If a single participant is in the team, the participant will be the only speaker.

participants (List[ChatAgent | Team]) – The participants in the group chat.

name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team.

description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided.

termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely.

max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.

custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.

emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.

ValueError – If no participants are provided or if participant names are not unique.

A team with one participant with tools:

A team with multiple participants:

A team of user proxy and a nested team of writer and reviewer agents:

alias of RoundRobinGroupChatConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: BaseGroupChat, Component[SelectorGroupChatConfig]

A group chat team that have participants takes turn to publish a message to all, using a ChatCompletion model to select the next speaker after each message.

If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat.

If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat.

participants (List[ChatAgent | Team]) – The participants in the group chat, must have unique names and at least two participants.

model_client (ChatCompletionClient) – The ChatCompletion model client used to select the next speaker.

name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team.

description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided.

termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely.

max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.

selector_prompt (str, optional) – The prompt template to use for selecting the next speaker. Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’. {participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …]. {roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”. {history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”.

allow_repeated_speaker (bool, optional) – Whether to include the previous speaker in the list of candidates to be selected for the next turn. Defaults to False. The model may still select the previous speaker – a warning will be logged if this happens.

max_selector_attempts (int, optional) – The maximum number of attempts to select a speaker using the model. Defaults to 3. If the model fails to select a speaker after the maximum number of attempts, the previous speaker will be used if available, otherwise the first participant will be used.

selector_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]], optional) – A custom selector function that takes the conversation history and returns the name of the next speaker. If provided, this function will be used to override the model to select the next speaker. If the function returns None, the model will be used to select the next speaker. NOTE: selector_func is not serializable and will be ignored during serialization and deserialization process.

candidate_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]], optional) – A custom function that takes the conversation history and returns a filtered list of candidates for the next speaker selection using model. If the function returns an empty list or None, SelectorGroupChat will raise a ValueError. This function is only used if selector_func is not set. The allow_repeated_speaker will be ignored if set.

custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.

emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.

model_client_streaming (bool, optional) – Whether to use streaming for the model client. (This is useful for reasoning models like QwQ). Defaults to False.

model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. Messages stored in model context will be used for speaker selection. The initial messages will be cleared when the team is reset.

ValueError – If the number of participants is less than two or if the selector prompt is invalid.

A team with multiple participants:

A team with a custom selector function:

A team with custom model context:

alias of SelectorGroupChatConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: BaseGroupChat, Component[SwarmConfig]

A group chat team that selects the next speaker based on handoff message only.

The first participant in the list of participants is the initial speaker. The next speaker is selected based on the HandoffMessage message sent by the current speaker. If no handoff message is sent, the current speaker continues to be the speaker.

Unlike RoundRobinGroupChat and SelectorGroupChat, this group chat team does not support inner teams as participants.

participants (List[ChatAgent]) – The agents participating in the group chat. The first agent in the list is the initial speaker.

name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team.

description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided.

termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely.

max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit.

custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.

emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.

Using the HandoffTermination for human-in-the-loop handoff:

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: BaseGroupChat, Component[MagenticOneGroupChatConfig]

A team that runs a group chat with participants managed by the MagenticOneOrchestrator.

The orchestrator handles the conversation flow, ensuring that the task is completed efficiently by managing the participants’ interactions.

The orchestrator is based on the Magentic-One architecture, which is a generalist multi-agent system for solving complex tasks (see references below).

Unlike RoundRobinGroupChat and SelectorGroupChat, the MagenticOneGroupChat does not support using team as participant.

participants (List[ChatAgent]) – The participants in the group chat.

model_client (ChatCompletionClient) – The model client used for generating responses.

termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached.

max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to 20.

max_stalls (int, optional) – The maximum number of stalls allowed before re-planning. Defaults to 3.

final_answer_prompt (str, optional) – The LLM prompt used to generate the final answer or response from the team’s transcript. A default (sensible for GPT-4o class models) is provided.

custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage.

emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False.

ValueError – In orchestration logic if progress ledger does not have required keys or if next speaker is not valid.

MagenticOneGroupChat with one assistant agent:

If you use the MagenticOneGroupChat in your work, please cite the following paper:

alias of MagenticOneGroupChatConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

A fluent builder for constructing DiGraph execution graphs used in GraphFlow.

This is an experimental feature, and the API will change in the future releases.

This utility provides a convenient way to programmatically build a graph of agent interactions, including complex execution flows such as:

Conditional branching

Cyclic loops with safe exits

Each node in the graph represents an agent. Edges define execution paths between agents, and can optionally be conditioned on message content using callable functions.

The builder is compatible with the Graph runner and supports both standard and filtered agents.

Add an agent node to the graph.

Connect two nodes optionally with a condition.

Add multiple conditional edges from a source.

Define the default start node (optional).

Generate a validated DiGraph.

Return the list of added agents.

Add a node to the graph and register its agent.

Add a directed edge from source to target, optionally with a condition.

source – Source node (agent name or agent object)

target – Target node (agent name or agent object)

condition – Optional condition for edge activation. If string, activates when substring is found in message. If callable, activates when function returns True for the message.

Self for method chaining

ValueError – If source or target node doesn’t exist in the builder

Add multiple conditional edges from a source node based on keyword checks.

This method interface will be changed in the future to support callable conditions. Please use add_edge if you need to specify custom conditions.

source – Source node (agent name or agent object)

condition_to_target – Mapping from condition strings to target nodes Each key is a keyword that will be checked in the message content Each value is the target node to activate when condition is met For each key (keyword), a lambda will be created that checks if the keyword is in the message text.

Mapping from condition strings to target nodes Each key is a keyword that will be checked in the message content Each value is the target node to activate when condition is met

For each key (keyword), a lambda will be created that checks if the keyword is in the message text.

Self for method chaining

Set the default start node of the graph.

Build and validate the DiGraph.

Return the list of agents in the builder, in insertion order.

Defines a directed graph structure with nodes and edges. GraphFlow uses this to determine execution order and conditions.

This is an experimental feature, and the API will change in the future releases.

Show JSON schema{ "title": "DiGraph", "type": "object", "properties": { "nodes": { "default": null, "title": "Nodes" }, "default_start_node": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Default Start Node" } } }

default_start_node (str | None)

nodes (Dict[str, autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphNode])

Compute a mapping of each node to its parent nodes.

Return the nodes that have no incoming edges (entry points).

Return nodes that have no outgoing edges (final output nodes).

Check if the graph has any cycles and validate that each cycle has at least one conditional edge.

bool – True if there is at least one cycle and all cycles have an exit condition. False if there are no cycles.

ValueError – If there is a cycle without any conditional edge.

Indicates if the graph has at least one cycle (with valid exit conditions).

Validate graph structure and execution rules.

Get the remaining map that tracks how many edges point to each target node with each activation group.

Dictionary mapping target nodes to their activation groups and remaining counts

This function is meant to behave like a BaseModel method to initialise private attributes.

It takes context as an argument since that’s what pydantic-core passes when calling it.

self – The BaseModel instance.

context – The context.

Represents a node (agent) in a DiGraph, with its outgoing edges and activation type.

This is an experimental feature, and the API will change in the future releases.

Show JSON schema{ "title": "DiGraphNode", "type": "object", "properties": { "name": { "title": "Name", "type": "string" }, "edges": { "default": null, "title": "Edges" }, "activation": { "default": "all", "enum": [ "all", "any" ], "title": "Activation", "type": "string" } }, "required": [ "name" ] }

activation (Literal['all', 'any'])

edges (List[autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphEdge])

Represents a directed edge in a DiGraph, with an optional execution condition.

This is an experimental feature, and the API will change in the future releases.

If the condition is a callable, it will not be serialized in the model.

Show JSON schema{ "title": "DiGraphEdge", "type": "object", "properties": { "target": { "title": "Target", "type": "string" }, "condition": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Condition" }, "condition_function": { "default": null, "title": "Condition Function" }, "activation_group": { "default": "", "title": "Activation Group", "type": "string" }, "activation_condition": { "default": "all", "enum": [ "all", "any" ], "title": "Activation Condition", "type": "string" } }, "required": [ "target" ] }

activation_condition (Literal['all', 'any'])

activation_group (str)

condition (str | Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None)

condition_function (Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None)

_validate_condition » all fields

(Experimental) Condition to execute this edge. If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message. If a callable, the edge is conditional on the callable returning True when given the last message.

Group identifier for forward dependencies.

When multiple edges point to the same target node, they are grouped by this field. This allows distinguishing between different cycles or dependency patterns.

Example: In a graph containing a cycle like A->B->C->B, the two edges pointing to B (A->B and C->B) can be in different activation groups to control how B is activated. Defaults to the target node name if not specified.

Determines how forward dependencies within the same activation_group are evaluated.

“all”: All edges in this activation group must be satisfied before the target node can execute

“any”: Any single edge in this activation group being satisfied allows the target node to execute

This is used to handle complex dependency patterns in cyclic graphs where multiple paths can lead to the same target node.

Check if the edge condition is satisfied for the given message.

message – The message to check the condition against.

True if condition is satisfied (None condition always returns True)

Bases: BaseGroupChat, Component[GraphFlowConfig]

A team that runs a group chat following a Directed Graph execution pattern.

This is an experimental feature, and the API will change in the future releases.

This group chat executes agents based on a directed graph (DiGraph) structure, allowing complex workflows such as sequential execution, parallel fan-out, conditional branching, join patterns, and loops with explicit exit conditions.

The execution order is determined by the edges defined in the DiGraph. Each node in the graph corresponds to an agent, and edges define the flow of messages between agents. Nodes can be configured to activate when:

All parent nodes have completed (activation=”all”) → default

Any parent node completes (activation=”any”)

Conditional branching is supported using edge conditions, where the next agent(s) are selected based on content in the chat history. Loops are permitted as long as there is a condition that eventually exits the loop.

Use the DiGraphBuilder class to create a DiGraph easily. It provides a fluent API for adding nodes and edges, setting entry points, and validating the graph structure. See the DiGraphBuilder documentation for more details. The GraphFlow class is designed to be used with the DiGraphBuilder for creating complex workflows.

When using callable conditions in edges, they will not be serialized when calling dump_component(). This will be addressed in future releases.

participants (List[ChatAgent]) – The participants in the group chat.

termination_condition (TerminationCondition, optional) – Termination condition for the chat.

max_turns (int, optional) – Maximum number of turns before forcing termination.

graph (DiGraph) – Directed execution graph defining node flow and conditions.

ValueError – If participant names are not unique, or if graph validation fails (e.g., cycles without exit).

Sequential Flow: A → B → C

Parallel Fan-out: A → (B, C)

Conditional Branching: A → B (if ‘yes’) or C (otherwise)

Loop with exit condition: A → B → C (if ‘APPROVE’) or A (otherwise)

alias of GraphFlowConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

autogen_agentchat.state

autogen_agentchat.tools

**Examples:**

Example 1 (python):
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())
```

Example 2 (python):
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    result = await team.run(task="Count from 1 to 10, respond one at a time.")
    print(result)

    # Run the team again without a task to continue the previous task.
    result = await team.run()
    print(result)


asyncio.run(main())
```

Example 3 (python):
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())
```

Example 4 (python):
```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    agent1 = AssistantAgent("Assistant1", model_client=model_client)
    agent2 = AssistantAgent("Assistant2", model_client=model_client)
    termination = MaxMessageTermination(3)
    team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination)

    cancellation_token = CancellationToken()

    # Create a task to run the team in the background.
    run_task = asyncio.create_task(
        team.run(
            task="Count from 1 to 10, respond one at a time.",
            cancellation_token=cancellation_token,
        )
    )

    # Wait for 1 second and then cancel the task.
    await asyncio.sleep(1)
    cancellation_token.cancel()

    # This will raise a cancellation error.
    await run_task


asyncio.run(main())
```

---

## autogen_agentchat.tools — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.tools.html

**Contents:**
- autogen_agentchat.tools#

Bases: TaskRunnerTool, Component[AgentToolConfig]

Tool that can be used to run a task using an agent.

The tool returns the result of the task execution as a TaskResult object.

When using AgentTool, you must disable parallel tool calls in the model client configuration to avoid concurrency issues. Agents cannot run concurrently as they maintain internal state that would conflict with parallel execution. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient.

agent (BaseChatAgent) – The agent to be used for running the task.

return_value_as_last_message (bool) – Whether to use the last message content of the task result as the return value of the tool in return_value_as_string(). If set to True, the last message content will be returned as a string. If set to False, the tool will return all messages in the task result as a string concatenated together, with each message prefixed by its source (e.g., “writer: …”, “assistant: …”).

alias of AgentToolConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

Bases: TaskRunnerTool, Component[TeamToolConfig]

Tool that can be used to run a task.

The tool returns the result of the task execution as a TaskResult object.

When using TeamTool, you must disable parallel tool calls in the model client configuration to avoid concurrency issues. Teams cannot run concurrently as they maintain internal state that would conflict with parallel execution. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient.

team (BaseGroupChat) – The team to be used for running the task.

name (str) – The name of the tool.

description (str) – The description of the tool.

return_value_as_last_message (bool) – Whether to use the last message content of the task result as the return value of the tool in return_value_as_string(). If set to True, the last message content will be returned as a string. If set to False, the tool will return all messages in the task result as a string concatenated together, with each message prefixed by its source (e.g., “writer: …”, “assistant: …”).

alias of TeamToolConfig

Override the provider string for the component. This should be used to prevent internal module names being a part of the module name.

Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance.

T – The configuration of the component.

Create a new instance of the component from a configuration object.

config (T) – The configuration object.

Self – The new instance of the component.

autogen_agentchat.teams

**Examples:**

Example 1 (python):
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    writer = AssistantAgent(
        name="writer",
        description="A writer agent for generating text.",
        model_client=model_client,
        system_message="Write well.",
    )
    writer_tool = AgentTool(agent=writer)

    # Create model client with parallel tool calls disabled for the main agent
    main_model_client = OpenAIChatCompletionClient(model="gpt-4.1", parallel_tool_calls=False)
    assistant = AssistantAgent(
        name="assistant",
        model_client=main_model_client,
        tools=[writer_tool],
        system_message="You are a helpful assistant.",
    )
    await Console(assistant.run_stream(task="Write a poem about the sea."))


asyncio.run(main())
```

Example 2 (python):
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.tools import AgentTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")
    writer = AssistantAgent(
        name="writer",
        description="A writer agent for generating text.",
        model_client=model_client,
        system_message="Write well.",
    )
    writer_tool = AgentTool(agent=writer)

    # Create model client with parallel tool calls disabled for the main agent
    main_model_client = OpenAIChatCompletionClient(model="gpt-4.1", parallel_tool_calls=False)
    assistant = AssistantAgent(
        name="assistant",
        model_client=main_model_client,
        tools=[writer_tool],
        system_message="You are a helpful assistant.",
    )
    await Console(assistant.run_stream(task="Write a poem about the sea."))


asyncio.run(main())
```

Example 3 (python):
```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import SourceMatchTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.tools import TeamTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Disable parallel tool calls when using TeamTool
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")

    writer = AssistantAgent(name="writer", model_client=model_client, system_message="You are a helpful assistant.")
    reviewer = AssistantAgent(
        name="reviewer", model_client=model_client, system_message="You are a critical reviewer."
    )
    summarizer = AssistantAgent(
        name="summarizer",
        model_client=model_client,
        system_message="You combine the review and produce a revised response.",
    )
    team = RoundRobinGroupChat(
        [writer, reviewer, summarizer], termination_condition=SourceMatchTermination(sources=["summarizer"])
    )

    # Create a TeamTool that uses the team to run tasks, returning the last message as the result.
    tool = TeamTool(
        team=team,
        name="writing_team",
        description="A tool for writing tasks.",
        return_value_as_last_message=True,
    )

    # Create model client with parallel tool calls disabled for the main agent
    main_model_client = OpenAIChatCompletionClient(model="gpt-4.1", parallel_tool_calls=False)
    main_agent = AssistantAgent(
        name="main_agent",
        model_client=main_model_client,
        system_message="You are a helpful assistant that can use the writing tool.",
        tools=[tool],
    )
    # For handling each events manually.
    # async for message in main_agent.run_stream(
    #     task="Write a short story about a robot learning to love.",
    # ):
    #     print(message)
    # Use Console to display the messages in a more readable format.
    await Console(
        main_agent.run_stream(
            task="Write a short story about a robot learning to love.",
        )
    )


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
```

Example 4 (python):
```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import SourceMatchTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.tools import TeamTool
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient


async def main() -> None:
    # Disable parallel tool calls when using TeamTool
    model_client = OpenAIChatCompletionClient(model="gpt-4.1")

    writer = AssistantAgent(name="writer", model_client=model_client, system_message="You are a helpful assistant.")
    reviewer = AssistantAgent(
        name="reviewer", model_client=model_client, system_message="You are a critical reviewer."
    )
    summarizer = AssistantAgent(
        name="summarizer",
        model_client=model_client,
        system_message="You combine the review and produce a revised response.",
    )
    team = RoundRobinGroupChat(
        [writer, reviewer, summarizer], termination_condition=SourceMatchTermination(sources=["summarizer"])
    )

    # Create a TeamTool that uses the team to run tasks, returning the last message as the result.
    tool = TeamTool(
        team=team,
        name="writing_team",
        description="A tool for writing tasks.",
        return_value_as_last_message=True,
    )

    # Create model client with parallel tool calls disabled for the main agent
    main_model_client = OpenAIChatCompletionClient(model="gpt-4.1", parallel_tool_calls=False)
    main_agent = AssistantAgent(
        name="main_agent",
        model_client=main_model_client,
        system_message="You are a helpful assistant that can use the writing tool.",
        tools=[tool],
    )
    # For handling each events manually.
    # async for message in main_agent.run_stream(
    #     task="Write a short story about a robot learning to love.",
    # ):
    #     print(message)
    # Use Console to display the messages in a more readable format.
    await Console(
        main_agent.run_stream(
            task="Write a short story about a robot learning to love.",
        )
    )


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
```

---

## autogen_agentchat.ui — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html

**Contents:**
- autogen_agentchat.ui#

This module implements utility classes for formatting/printing agent messages.

Consumes the message stream from run_stream() or on_messages_stream() and renders the messages to the console. Returns the last processed TaskResult or Response.

output_stats is experimental and the stats may not be accurate. It will be improved in future releases.

stream (AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None] | AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]) – Message stream to render. This can be from run_stream() or on_messages_stream().

no_inline_images (bool, optional) – If terminal is iTerm2 will render images inline. Use this to disable this behavior. Defaults to False.

output_stats (bool, optional) – (Experimental) If True, will output a summary of the messages and inline token usage info. Defaults to False.

last_processed – A TaskResult if the stream is from run_stream() or a Response if the stream is from on_messages_stream().

autogen_agentchat.tools

autogen_agentchat.utils

---

## autogen_agentchat.utils — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.utils.html

**Contents:**
- autogen_agentchat.utils#

This module implements various utilities common to AgentChat agents and teams.

Convert the content of an LLMMessage to a string.

Remove images from a list of LLMMessages

---

## autogen_agentchat — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.html

**Contents:**
- autogen_agentchat#

This module provides the main entry point for the autogen_agentchat package. It includes logger names for trace and event logs, and retrieves the package version.

Logger name for trace logs.

Logger name for event logs.

autogen_agentchat.agents

---

## Company Research — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/company-research.html

**Contents:**
- Company Research#
- Defining Tools#
- Defining Agents#
- Creating the Team#

Conducting company research, or competitive analysis, is a critical part of any business strategy. In this notebook, we will demonstrate how to create a team of agents to address this task. While there are many ways to translate a task into an agentic implementation, we will explore a sequential approach. We will create agents corresponding to steps in the research process and give them tools to perform their tasks.

Search Agent: Searches the web for information about a company. Will have access to a search engine API tool to retrieve search results.

Stock Analysis Agent: Retrieves the company’s stock information from a financial data API, computes basic statistics (current price, 52-week high, 52-week low, etc.), and generates a plot of the stock price year-to-date, saving it to a file. Will have access to a financial data API tool to retrieve stock information.

Report Agent: Generates a report based on the information collected by the search and stock analysis agents.

First, let’s import the necessary modules.

Next, we will define the tools that the agents will use to perform their tasks. We will create a google_search that uses the Google Search API to search the web for information about a company. We will also create a analyze_stock function that uses the yfinance library to retrieve stock information for a company.

Finally, we will wrap these functions into a FunctionTool class that will allow us to use them as tools in our agents.

Note: The google_search function requires an API key to work. You can create a .env file in the same directory as this notebook and add your API key as

Also install required libraries

Next, we will define the agents that will perform the tasks. We will create a search_agent that searches the web for information about a company, a stock_analysis_agent that retrieves stock information for a company, and a report_agent that generates a report based on the information collected by the other agents.

Finally, let’s create a team of the three agents and set them to work on researching a company.

We use max_turns=3 to limit the number of turns to exactly the same number of agents in the team. This effectively makes the agents work in a sequential manner.

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 2 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 3 (unknown):
```unknown
GOOGLE_SEARCH_ENGINE_ID =xxx
GOOGLE_API_KEY=xxx
```

Example 4 (unknown):
```unknown
GOOGLE_SEARCH_ENGINE_ID =xxx
GOOGLE_API_KEY=xxx
```

---

## Custom Agents — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/custom-agents.html

**Contents:**
- Custom Agents#
- CountDownAgent#
- ArithmeticAgent#
- Using Custom Model Clients in Custom Agents#
- Making the Custom Agent Declarative#
- Next Steps#

You may have agents with behaviors that do not fall into a preset. In such cases, you can build custom agents.

All agents in AgentChat inherit from BaseChatAgent class and implement the following abstract methods and attributes:

on_messages(): The abstract method that defines the behavior of the agent in response to messages. This method is called when the agent is asked to provide a response in run(). It returns a Response object.

on_reset(): The abstract method that resets the agent to its initial state. This method is called when the agent is asked to reset itself.

produced_message_types: The list of possible BaseChatMessage message types the agent can produce in its response.

Optionally, you can implement the the on_messages_stream() method to stream messages as they are generated by the agent. This method is called by run_stream() to stream messages. If this method is not implemented, the agent uses the default implementation of on_messages_stream() that calls the on_messages() method and yields all messages in the response.

In this example, we create a simple agent that counts down from a given number to zero, and produces a stream of messages with the current count.

In this example, we create an agent class that can perform simple arithmetic operations on a given integer. Then, we will use different instances of this agent class in a SelectorGroupChat to transform a given integer into another integer by applying a sequence of arithmetic operations.

The ArithmeticAgent class takes an operator_func that takes an integer and returns an integer, after applying an arithmetic operation to the integer. In its on_messages method, it applies the operator_func to the integer in the input message, and returns a response with the result.

The on_messages method may be called with an empty list of messages, in which case it means the agent was called previously and is now being called again, without any new messages from the caller. So it is important to keep a history of the previous messages received by the agent, and use that history to generate the response.

Now we can create a SelectorGroupChat with 5 instances of ArithmeticAgent:

one that adds 1 to the input integer,

one that subtracts 1 from the input integer,

one that multiplies the input integer by 2,

one that divides the input integer by 2 and rounds down to the nearest integer, and

one that returns the input integer unchanged.

We then create a SelectorGroupChat with these agents, and set the appropriate selector settings:

allow the same agent to be selected consecutively to allow for repeated operations, and

customize the selector prompt to tailor the model’s response to the specific task.

From the output, we can see that the agents have successfully transformed the input integer from 10 to 25 by choosing appropriate agents that apply the arithmetic operations in sequence.

One of the key features of the AssistantAgent preset in AgentChat is that it takes a model_client argument and can use it in responding to messages. However, in some cases, you may want your agent to use a custom model client that is not currently supported (see supported model clients) or custom model behaviours.

You can accomplish this with a custom agent that implements your custom model client.

In the example below, we will walk through an example of a custom agent that uses the Google Gemini SDK directly to respond to messages.

Note: You will need to install the Google Gemini SDK to run this example. You can install it using the following command:

In the example above, we have chosen to provide model, api_key and system_message as arguments - you can choose to provide any other arguments that are required by the model client you are using or fits with your application design.

Now, let us explore how to use this custom agent as part of a team in AgentChat.

In section above, we show several very important concepts:

We have developed a custom agent that uses the Google Gemini SDK to respond to messages.

We show that this custom agent can be used as part of the broader AgentChat ecosystem - in this case as a participant in a RoundRobinGroupChat as long as it inherits from BaseChatAgent.

Autogen provides a Component interface for making the configuration of components serializable to a declarative format. This is useful for saving and loading configurations, and for sharing configurations with others.

We accomplish this by inheriting from the Component class and implementing the _from_config and _to_config methods. The declarative class can be serialized to a JSON format using the dump_component method, and deserialized from a JSON format using the load_component method.

Now that we have the required methods implemented, we can now load and dump the custom agent to and from a JSON format, and then load the agent from the JSON format.

Note: You should set the component_provider_override class variable to the full path of the module containing the custom agent class e.g., (mypackage.agents.GeminiAssistantAgent). This is used by load_component method to determine how to instantiate the class.

So far, we have seen how to create custom agents, add custom model clients to agents, and make custom agents declarative. There are a few ways in which this basic sample can be extended:

Extend the Gemini model client to handle function calling similar to the AssistantAgent class. https://ai.google.dev/gemini-api/docs/function-calling

Implement a package with a custom agent and experiment with using its declarative format in a tool like AutoGen Studio.

**Examples:**

Example 1 (python):
```python
from typing import AsyncGenerator, List, Sequence

from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage
from autogen_core import CancellationToken


class CountDownAgent(BaseChatAgent):
    def __init__(self, name: str, count: int = 3):
        super().__init__(name, "A simple agent that counts down.")
        self._count = count

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (TextMessage,)

    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:
        # Calls the on_messages_stream.
        response: Response | None = None
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                response = message
        assert response is not None
        return response

    async def on_messages_stream(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []
        for i in range(self._count, 0, -1):
            msg = TextMessage(content=f"{i}...", source=self.name)
            inner_messages.append(msg)
            yield msg
        # The response is returned at the end of the stream.
        # It contains the final message and all the inner messages.
        yield Response(chat_message=TextMessage(content="Done!", source=self.name), inner_messages=inner_messages)

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass


async def run_countdown_agent() -> None:
    # Create a countdown agent.
    countdown_agent = CountDownAgent("countdown")

    # Run the agent with a given task and stream the response.
    async for message in countdown_agent.on_messages_stream([], CancellationToken()):
        if isinstance(message, Response):
            print(message.chat_message)
        else:
            print(message)


# Use asyncio.run(run_countdown_agent()) when running in a script.
await run_countdown_agent()
```

Example 2 (python):
```python
from typing import AsyncGenerator, List, Sequence

from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, TextMessage
from autogen_core import CancellationToken


class CountDownAgent(BaseChatAgent):
    def __init__(self, name: str, count: int = 3):
        super().__init__(name, "A simple agent that counts down.")
        self._count = count

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        return (TextMessage,)

    async def on_messages(self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) -> Response:
        # Calls the on_messages_stream.
        response: Response | None = None
        async for message in self.on_messages_stream(messages, cancellation_token):
            if isinstance(message, Response):
                response = message
        assert response is not None
        return response

    async def on_messages_stream(
        self, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        inner_messages: List[BaseAgentEvent | BaseChatMessage] = []
        for i in range(self._count, 0, -1):
            msg = TextMessage(content=f"{i}...", source=self.name)
            inner_messages.append(msg)
            yield msg
        # The response is returned at the end of the stream.
        # It contains the final message and all the inner messages.
        yield Response(chat_message=TextMessage(content="Done!", source=self.name), inner_messages=inner_messages)

    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        pass


async def run_countdown_agent() -> None:
    # Create a countdown agent.
    countdown_agent = CountDownAgent("countdown")

    # Run the agent with a given task and stream the response.
    async for message in countdown_agent.on_messages_stream([], CancellationToken()):
        if isinstance(message, Response):
            print(message.chat_message)
        else:
            print(message)


# Use asyncio.run(run_countdown_agent()) when running in a script.
await run_countdown_agent()
```

Example 3 (unknown):
```unknown
3...
2...
1...
Done!
```

Example 4 (unknown):
```unknown
3...
2...
1...
Done!
```

---

## GraphFlow (Workflows) — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/graph-flow.html

**Contents:**
- GraphFlow (Workflows)#
- Creating and Running a Flow#
  - Sequential Flow#
  - Parallel Flow with Join#
- Message Filtering#
  - Execution Graph vs. Message Graph#
- 🔁 Advanced Example: Conditional Loop + Filtered Summary#
- 🔁 Advanced Example: Cycles With Activation Group Examples#
  - Example 1: Loop with Multiple Paths - “All” Activation (A→B→C→B)#
  - Example 2: Loop with Multiple Paths - “Any” Activation (A→B→(C1,C2)→B)#

In this section you’ll learn how to create an multi-agent workflow using GraphFlow, or simply “flow” for short. It uses structured execution and precisely controls how agents interact to accomplish a task.

We’ll first show you how to create and run a flow. We’ll then explain how to observe and debug flow behavior, and discuss important operations for managing execution.

AutoGen AgentChat provides a team for directed graph execution:

GraphFlow: A team that follows a DiGraph to control the execution flow between agents. Supports sequential, parallel, conditional, and looping behaviors.

When should you use GraphFlow?

Use Graph when you need strict control over the order in which agents act, or when different outcomes must lead to different next steps. Start with a simple team such as RoundRobinGroupChat or SelectorGroupChat if ad-hoc conversation flow is sufficient. Transition to a structured workflow when your task requires deterministic control, conditional branching, or handling complex multi-step processes with cycles.

Warning: GraphFlow is an experimental feature. Its API, behavior, and capabilities are subject to change in future releases.

DiGraphBuilder is a fluent utility that lets you easily construct execution graphs for workflows. It supports building:

Conditional branching

Loops with safe exit conditions

Each node in the graph represents an agent, and edges define the allowed execution paths. Edges can optionally have conditions based on agent messages.

We will begin by creating a simple workflow where a writer drafts a paragraph and a reviewer provides feedback. This graph terminates after the reviewer comments on the writer.

Note, the flow automatically computes all the source and leaf nodes of the graph and the execution starts at all the source nodes in the graph and completes execution when no nodes are left to execute.

We now create a slightly more complex flow:

A writer drafts a paragraph.

Two editors independently edit for grammar and style (parallel fan-out).

A final reviewer consolidates their edits (join).

Execution starts at the writer, fans out to editor1 and editor2 simultaneously, and then both feed into the final reviewer.

In GraphFlow, the execution graph is defined using DiGraph, which controls the order in which agents execute. However, the execution graph does not control what messages an agent receives from other agents. By default, all messages are sent to all agents in the graph.

Message filtering is a separate feature that allows you to filter the messages received by each agent and limiting their model context to only the relevant information. The set of message filters defines the message graph in the flow.

Specifying the message graph can help with:

Reduce hallucinations

Focus agents only on relevant information

You can use MessageFilterAgent together with MessageFilterConfig and PerSourceFilter to define these rules.

This example demonstrates:

A loop between generator and reviewer (which exits when reviewer says “APPROVE”)

A summarizer agent that only sees the first user input and the last reviewer message

The following examples demonstrate how to use activation_group and activation_condition to handle complex dependency patterns in cyclic graphs, especially when multiple paths lead to the same target node.

In this scenario, we have A → B → C → B, where B has two incoming edges (from A and from C). By default, B requires all its dependencies to be satisfied before executing.

This example shows a review loop where both the initial input (A) and the feedback (C) must be processed before B can execute again.

In this more complex scenario, we have A → B → (C1, C2) → B, where:

B fans out to both C1 and C2 in parallel

Both C1 and C2 feed back to B

B uses “any” activation, meaning it executes as soon as either C1 or C2 completes

This is useful for scenarios where you want the fastest response to trigger the next step.

This example shows how different activation groups can coexist in the same graph. We have a scenario where:

Node D receives inputs from multiple sources with different activation requirements

Some dependencies use “all” activation (must wait for all inputs)

Other dependencies use “any” activation (proceed on first input)

This pattern is useful for complex workflows where different types of dependencies have different urgency levels.

activation_group: Groups edges that point to the same target node, allowing you to define different dependency patterns.

activation_condition:

"all" (default): Target node waits for ALL edges in the group to be satisfied

"any": Target node executes as soon as ANY edge in the group is satisfied

Cycles with multiple entry points: Different activation groups prevent conflicts

Priority-based execution: Mix “all” and “any” conditions for different urgency levels

Parallel processing with early termination: Use “any” to proceed with the fastest result

Use descriptive group names ("critical", "optional", "feedback", etc.)

Keep activation conditions consistent within the same group

Test your graph logic with different execution paths

These patterns enable sophisticated workflow control while maintaining clear, understandable execution semantics.

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Create an OpenAI model client
client = OpenAIChatCompletionClient(model="gpt-4.1-nano")

# Create the writer agent
writer = AssistantAgent("writer", model_client=client, system_message="Draft a short paragraph on climate change.")

# Create the reviewer agent
reviewer = AssistantAgent("reviewer", model_client=client, system_message="Review the draft and suggest improvements.")

# Build the graph
builder = DiGraphBuilder()
builder.add_node(writer).add_node(reviewer)
builder.add_edge(writer, reviewer)

# Build and validate the graph
graph = builder.build()

# Create the flow
flow = GraphFlow([writer, reviewer], graph=graph)
```

Example 2 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import DiGraphBuilder, GraphFlow
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Create an OpenAI model client
client = OpenAIChatCompletionClient(model="gpt-4.1-nano")

# Create the writer agent
writer = AssistantAgent("writer", model_client=client, system_message="Draft a short paragraph on climate change.")

# Create the reviewer agent
reviewer = AssistantAgent("reviewer", model_client=client, system_message="Review the draft and suggest improvements.")

# Build the graph
builder = DiGraphBuilder()
builder.add_node(writer).add_node(reviewer)
builder.add_edge(writer, reviewer)

# Build and validate the graph
graph = builder.build()

# Create the flow
flow = GraphFlow([writer, reviewer], graph=graph)
```

Example 3 (javascript):
```javascript
# Use `asyncio.run(...)` and wrap the below in a async function when running in a script.
stream = flow.run_stream(task="Write a short paragraph about climate change.")
async for event in stream:  # type: ignore
    print(event)
# Use Console(flow.run_stream(...)) for better formatting in console.
```

Example 4 (javascript):
```javascript
# Use `asyncio.run(...)` and wrap the below in a async function when running in a script.
stream = flow.run_stream(task="Write a short paragraph about climate change.")
async for event in stream:  # type: ignore
    print(event)
# Use Console(flow.run_stream(...)) for better formatting in console.
```

---

## Human-in-the-Loop — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html

**Contents:**
- Human-in-the-Loop#
- Providing Feedback During a Run#
- Providing Feedback to the Next Run#
  - Using Max Turns#
  - Using Termination Conditions#

In the previous section Teams, we have seen how to create, observe, and control a team of agents. This section will focus on how to interact with the team from your application, and provide human feedback to the team.

There are two main ways to interact with the team from your application:

During a team’s run – execution of run() or run_stream(), provide feedback through a UserProxyAgent.

Once the run terminates, provide feedback through input to the next call to run() or run_stream().

We will cover both methods in this section.

To jump straight to code samples on integration with web and UI frameworks, see the following links:

AgentChat + Streamlit

The UserProxyAgent is a special built-in agent that acts as a proxy for a user to provide feedback to the team.

To use the UserProxyAgent, you can create an instance of it and include it in the team before running the team. The team will decide when to call the UserProxyAgent to ask for feedback from the user.

For example in a RoundRobinGroupChat team, the UserProxyAgent is called in the order in which it is passed to the team, while in a SelectorGroupChat team, the selector prompt or selector function determines when the UserProxyAgent is called.

The following diagram illustrates how you can use UserProxyAgent to get feedback from the user during a team’s run:

The bold arrows indicates the flow of control during a team’s run: when the team calls the UserProxyAgent, it transfers the control to the application/user, and waits for the feedback; once the feedback is provided, the control is transferred back to the team and the team continues its execution.

When UserProxyAgent is called during a run, it blocks the execution of the team until the user provides feedback or errors out. This will hold up the team’s progress and put the team in an unstable state that cannot be saved or resumed.

Due to the blocking nature of this approach, it is recommended to use it only for short interactions that require immediate feedback from the user, such as asking for approval or disapproval with a button click, or an alert requiring immediate attention otherwise failing the task.

Here is an example of how to use the UserProxyAgent in a RoundRobinGroupChat for a poetry generation task:

From the console output, you can see the team solicited feedback from the user through user_proxy to approve the generated poem.

You can provide your own input function to the UserProxyAgent to customize the feedback process. For example, when the team is running as a web service, you can use a custom input function to wait for message from a web socket connection. The following code snippet shows an example of custom input function when using the FastAPI web framework:

See the AgentChat FastAPI sample for a complete example.

For ChainLit integration with UserProxyAgent, see the AgentChat ChainLit sample.

Often times, an application or a user interacts with the team of agents in an interactive loop: the team runs until termination, the application or user provides feedback, and the team runs again with the feedback.

This approach is useful in a persisted session with asynchronous communication between the team and the application/user: Once a team finishes a run, the application saves the state of the team, puts it in a persistent storage, and resumes the team when the feedback arrives.

For how to save and load the state of a team, please refer to Managing State. This section will focus on the feedback mechanisms.

The following diagram illustrates the flow of control in this approach:

There are two ways to implement this approach:

Set the maximum number of turns so that the team always stops after the specified number of turns.

Use termination conditions such as TextMentionTermination and HandoffTermination to allow the team to decide when to stop and give control back, given the team’s internal state.

You can use both methods together to achieve your desired behavior.

This method allows you to pause the team for user input by setting a maximum number of turns. For instance, you can configure the team to stop after the first agent responds by setting max_turns to 1. This is particularly useful in scenarios where continuous user engagement is required, such as in a chatbot.

To implement this, set the max_turns parameter in the RoundRobinGroupChat() constructor.

Once the team stops, the turn count will be reset. When you resume the team, it will start from 0 again. However, the team’s internal state will be preserved, for example, the RoundRobinGroupChat will resume from the next agent in the list with the same conversation history.

max_turn is specific to the team class and is currently only supported by RoundRobinGroupChat, SelectorGroupChat, and Swarm. When used with termination conditions, the team will stop when either condition is met.

Here is an example of how to use max_turns in a RoundRobinGroupChat for a poetry generation task with a maximum of 1 turn:

You can see that the team stopped immediately after one agent responded.

We have already seen several examples of termination conditions in the previous sections. In this section, we focus on HandoffTermination which stops the team when an agent sends a HandoffMessage message.

Let’s create a team with a single AssistantAgent agent with a handoff setting, and run the team with a task that requires additional input from the user because the agent doesn’t have relevant tools to continue processing the task.

The model used with AssistantAgent must support tool call to use the handoff feature.

You can see the team stopped due to the handoff message was detected. Let’s continue the team by providing the information the agent needs.

You can see the team continued after the user provided the information.

If you are using Swarm team with HandoffTermination targeting user, to resume the team, you need to set the task to a HandoffMessage with the target set to the next agent you want to run. See Swarm for more details.

**Examples:**

Example 1 (python):
```python
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Create the agents.
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
assistant = AssistantAgent("assistant", model_client=model_client)
user_proxy = UserProxyAgent("user_proxy", input_func=input)  # Use input() to get user input from console.

# Create the termination condition which will end the conversation when the user says "APPROVE".
termination = TextMentionTermination("APPROVE")

# Create the team.
team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)

# Run the conversation and stream to the console.
stream = team.run_stream(task="Write a 4-line poem about the ocean.")
# Use asyncio.run(...) when running in a script.
await Console(stream)
await model_client.close()
```

Example 2 (python):
```python
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Create the agents.
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
assistant = AssistantAgent("assistant", model_client=model_client)
user_proxy = UserProxyAgent("user_proxy", input_func=input)  # Use input() to get user input from console.

# Create the termination condition which will end the conversation when the user says "APPROVE".
termination = TextMentionTermination("APPROVE")

# Create the team.
team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)

# Run the conversation and stream to the console.
stream = team.run_stream(task="Write a 4-line poem about the ocean.")
# Use asyncio.run(...) when running in a script.
await Console(stream)
await model_client.close()
```

Example 3 (sql):
```sql
---------- user ----------
Write a 4-line poem about the ocean.
---------- assistant ----------
In endless blue where whispers play,  
The ocean's waves dance night and day.  
A world of depths, both calm and wild,  
Nature's heart, forever beguiled.  
TERMINATE
---------- user_proxy ----------
APPROVE
```

Example 4 (sql):
```sql
---------- user ----------
Write a 4-line poem about the ocean.
---------- assistant ----------
In endless blue where whispers play,  
The ocean's waves dance night and day.  
A world of depths, both calm and wild,  
Nature's heart, forever beguiled.  
TERMINATE
---------- user_proxy ----------
APPROVE
```

---

## Literature Review — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/literature-review.html

**Contents:**
- Literature Review#
- Defining Tools#
- Defining Agents#
- Creating the Team#

A common task while exploring a new topic is to conduct a literature review. In this example we will explore how a multi-agent team can be configured to conduct a simple literature review.

Arxiv Search Agent: Use the Arxiv API to search for papers related to a given topic and return results.

Google Search Agent: Use the Google Search api to find papers related to a given topic and return results.

Report Agent: Generate a report based on the information collected by the arxviv search and Google search agents.

First, let us import the necessary modules.

Next, we will define the tools that the agents will use to perform their tasks. In this case we will define a simple function search_arxiv that will use the arxiv library to search for papers related to a given topic.

Finally, we will wrap the functions into a FunctionTool class that will allow us to use it as a tool in the agents.

Note: You will need to set the appropriate environment variables for tools as needed.

Also install required libraries:

Next, we will define the agents that will perform the tasks.

Finally, we will create a team of agents and configure them to perform the tasks.

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 2 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core.tools import FunctionTool
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 3 (unknown):
```unknown
!pip install arxiv
```

Example 4 (unknown):
```unknown
!pip install arxiv
```

---

## Logging — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/logging.html

**Contents:**
- Logging#

AutoGen uses Python’s built-in logging module.

To enable logging for AgentChat, you can use the following code:

To enable additional logs such as model client calls and agent runtime events, please refer to the Core Logging Guide.

Serializing Components

**Examples:**

Example 1 (python):
```python
import logging

from autogen_agentchat import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME

logging.basicConfig(level=logging.WARNING)

# For trace logging.
trace_logger = logging.getLogger(TRACE_LOGGER_NAME)
trace_logger.addHandler(logging.StreamHandler())
trace_logger.setLevel(logging.DEBUG)

# For structured message logging, such as low-level messages between agents.
event_logger = logging.getLogger(EVENT_LOGGER_NAME)
event_logger.addHandler(logging.StreamHandler())
event_logger.setLevel(logging.DEBUG)
```

Example 2 (python):
```python
import logging

from autogen_agentchat import EVENT_LOGGER_NAME, TRACE_LOGGER_NAME

logging.basicConfig(level=logging.WARNING)

# For trace logging.
trace_logger = logging.getLogger(TRACE_LOGGER_NAME)
trace_logger.addHandler(logging.StreamHandler())
trace_logger.setLevel(logging.DEBUG)

# For structured message logging, such as low-level messages between agents.
event_logger = logging.getLogger(EVENT_LOGGER_NAME)
event_logger.addHandler(logging.StreamHandler())
event_logger.setLevel(logging.DEBUG)
```

---

## Magentic-One — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/magentic-one.html

**Contents:**
- Magentic-One#
- Getting started#
- Architecture#
- Citation#

Magentic-One is a generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. It represents a significant step forward for multi-agent systems, achieving competitive performance on a number of agentic benchmarks (see the technical report for full details).

When originally released in November 2024 Magentic-One was implemented directly on the autogen-core library. We have now ported Magentic-One to use autogen-agentchat, providing a more modular and easier to use interface.

To this end, the Magentic-One orchestrator MagenticOneGroupChat is now simply an AgentChat team, supporting all standard AgentChat agents and features. Likewise, Magentic-One’s MultimodalWebSurfer, FileSurfer, and MagenticOneCoderAgent agents are now broadly available as AgentChat agents, to be used in any AgentChat workflows.

Lastly, there is a helper class, MagenticOne, which bundles all of this together as it was in the paper with minimal configuration.

Find additional information about Magentic-one in our blog post and technical report.

Example: The figure above illustrates Magentic-One multi-agent team completing a complex task from the GAIA benchmark. Magentic-One’s Orchestrator agent creates a plan, delegates tasks to other agents, and tracks progress towards the goal, dynamically revising the plan as needed. The Orchestrator can delegate tasks to a FileSurfer agent to read and handle files, a WebSurfer agent to operate a web browser, or a Coder or Computer Terminal agent to write or execute code, respectively.

Using Magentic-One involves interacting with a digital world designed for humans, which carries inherent risks. To minimize these risks, consider the following precautions:

Use Containers: Run all tasks in docker containers to isolate the agents and prevent direct system attacks.

Virtual Environment: Use a virtual environment to run the agents and prevent them from accessing sensitive data.

Monitor Logs: Closely monitor logs during and after execution to detect and mitigate risky behavior.

Human Oversight: Run the examples with a human in the loop to supervise the agents and prevent unintended consequences.

Limit Access: Restrict the agents’ access to the internet and other resources to prevent unauthorized actions.

Safeguard Data: Ensure that the agents do not have access to sensitive data or resources that could be compromised. Do not share sensitive information with the agents. Be aware that agents may occasionally attempt risky actions, such as recruiting humans for help or accepting cookie agreements without human involvement. Always ensure agents are monitored and operate within a controlled environment to prevent unintended consequences. Moreover, be cautious that Magentic-One may be susceptible to prompt injection attacks from webpages.

Install the required packages:

If you haven’t done so already, go through the AgentChat tutorial to learn about the concepts of AgentChat.

Then, you can try swapping out a autogen_agentchat.teams.SelectorGroupChat with MagenticOneGroupChat.

To use a different model, see Models for more information.

Or, use the Magentic-One agents in a team:

The example code may download files from the internet, execute code, and interact with web pages. Ensure you are in a safe environment before running the example code.

Or, use the MagenticOne helper class with all the agents bundled together:

Magentic-One work is based on a multi-agent architecture where a lead Orchestrator agent is responsible for high-level planning, directing other agents and tracking task progress. The Orchestrator begins by creating a plan to tackle the task, gathering needed facts and educated guesses in a Task Ledger that is maintained. At each step of its plan, the Orchestrator creates a Progress Ledger where it self-reflects on task progress and checks whether the task is completed. If the task is not yet completed, it assigns one of Magentic-One other agents a subtask to complete. After the assigned agent completes its subtask, the Orchestrator updates the Progress Ledger and continues in this way until the task is complete. If the Orchestrator finds that progress is not being made for enough steps, it can update the Task Ledger and create a new plan. This is illustrated in the figure above; the Orchestrator work is thus divided into an outer loop where it updates the Task Ledger and an inner loop to update the Progress Ledger.

Overall, Magentic-One consists of the following agents:

Orchestrator: the lead agent responsible for task decomposition and planning, directing other agents in executing subtasks, tracking overall progress, and taking corrective actions as needed

WebSurfer: This is an LLM-based agent that is proficient in commanding and managing the state of a Chromium-based web browser. With each incoming request, the WebSurfer performs an action on the browser then reports on the new state of the web page The action space of the WebSurfer includes navigation (e.g. visiting a URL, performing a web search); web page actions (e.g., clicking and typing); and reading actions (e.g., summarizing or answering questions). The WebSurfer relies on the accessibility tree of the browser and on set-of-marks prompting to perform its actions.

FileSurfer: This is an LLM-based agent that commands a markdown-based file preview application to read local files of most types. The FileSurfer can also perform common navigation tasks such as listing the contents of directories and navigating a folder structure.

Coder: This is an LLM-based agent specialized through its system prompt for writing code, analyzing information collected from the other agents, or creating new artifacts.

ComputerTerminal: Finally, ComputerTerminal provides the team with access to a console shell where the Coder’s programs can be executed, and where new programming libraries can be installed.

Together, Magentic-One’s agents provide the Orchestrator with the tools and capabilities that it needs to solve a broad variety of open-ended problems, as well as the ability to autonomously adapt to, and act in, dynamic and ever-changing web and file-system environments.

While the default multimodal LLM we use for all agents is GPT-4o, Magentic-One is model agnostic and can incorporate heterogonous models to support different capabilities or meet different cost requirements when getting tasks done. For example, it can use different LLMs and SLMs and their specialized versions to power different agents. We recommend a strong reasoning model for the Orchestrator agent such as GPT-4o. In a different configuration of Magentic-One, we also experiment with using OpenAI o1-preview for the outer loop of the Orchestrator and for the Coder, while other agents continue to use GPT-4o.

GraphFlow (Workflows)

**Examples:**

Example 1 (julia):
```julia
pip install "autogen-agentchat" "autogen-ext[magentic-one,openai]"

# If using the MultimodalWebSurfer, you also need to install playwright dependencies:
playwright install --with-deps chromium
```

Example 2 (julia):
```julia
pip install "autogen-agentchat" "autogen-ext[magentic-one,openai]"

# If using the MultimodalWebSurfer, you also need to install playwright dependencies:
playwright install --with-deps chromium
```

Example 3 (python):
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
    )
    team = MagenticOneGroupChat([assistant], model_client=model_client)
    await Console(team.run_stream(task="Provide a different proof for Fermat's Last Theorem"))
    await model_client.close()


asyncio.run(main())
```

Example 4 (python):
```python
import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console


async def main() -> None:
    model_client = OpenAIChatCompletionClient(model="gpt-4o")

    assistant = AssistantAgent(
        "Assistant",
        model_client=model_client,
    )
    team = MagenticOneGroupChat([assistant], model_client=model_client)
    await Console(team.run_stream(task="Provide a different proof for Fermat's Last Theorem"))
    await model_client.close()


asyncio.run(main())
```

---

## Managing State — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/state.html

**Contents:**
- Managing State#
- Saving and Loading Agents#
- Saving and Loading Teams#
- Persisting State (File or Database)#

So far, we have discussed how to build components in a multi-agent application - agents, teams, termination conditions. In many cases, it is useful to save the state of these components to disk and load them back later. This is particularly useful in a web application where stateless endpoints respond to requests and need to load the state of the application from persistent storage.

In this notebook, we will discuss how to save and load the state of agents, teams, and termination conditions.

We can get the state of an agent by calling save_state() method on an AssistantAgent.

For AssistantAgent, its state consists of the model_context. If you write your own custom agent, consider overriding the save_state() and load_state() methods to customize the behavior. The default implementations save and load an empty state.

We can get the state of a team by calling save_state method on the team and load it back by calling load_state method on the team.

When we call save_state on a team, it saves the state of all the agents in the team.

We will begin by creating a simple RoundRobinGroupChat team with a single agent and ask it to write a poem.

If we reset the team (simulating instantiation of the team), and ask the question What was the last line of the poem you wrote?, we see that the team is unable to accomplish this as there is no reference to the previous run.

Next, we load the state of the team and ask the same question. We see that the team is able to accurately return the last line of the poem it wrote.

In many cases, we may want to persist the state of the team to disk (or a database) and load it back later. State is a dictionary that can be serialized to a file or written to a database.

**Examples:**

Example 1 (python):
```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")

assistant_agent = AssistantAgent(
    name="assistant_agent",
    system_message="You are a helpful assistant",
    model_client=model_client,
)

# Use asyncio.run(...) when running in a script.
response = await assistant_agent.on_messages(
    [TextMessage(content="Write a 3 line poem on lake tangayika", source="user")], CancellationToken()
)
print(response.chat_message)
await model_client.close()
```

Example 2 (python):
```python
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(model="gpt-4o-2024-08-06")

assistant_agent = AssistantAgent(
    name="assistant_agent",
    system_message="You are a helpful assistant",
    model_client=model_client,
)

# Use asyncio.run(...) when running in a script.
response = await assistant_agent.on_messages(
    [TextMessage(content="Write a 3 line poem on lake tangayika", source="user")], CancellationToken()
)
print(response.chat_message)
await model_client.close()
```

Example 3 (sql):
```sql
In Tanganyika's embrace so wide and deep,  
Ancient waters cradle secrets they keep,  
Echoes of time where horizons sleep.
```

Example 4 (sql):
```sql
In Tanganyika's embrace so wide and deep,  
Ancient waters cradle secrets they keep,  
Echoes of time where horizons sleep.
```

---

## Memory and RAG — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/memory.html

**Contents:**
- Memory and RAG#
- ListMemory Example#
- Custom Memory Stores (Vector DBs, etc.)#
- Redis Memory#
- RAG Agent: Putting It All Together#
- Building a Simple RAG Agent#
- Mem0Memory Example#

There are several use cases where it is valuable to maintain a store of useful facts that can be intelligently added to the context of the agent just before a specific step. The typically use case here is a RAG pattern where a query is used to retrieve relevant information from a database that is then added to the agent’s context.

AgentChat provides a Memory protocol that can be extended to provide this functionality. The key methods are query, update_context, add, clear, and close.

add: add new entries to the memory store

query: retrieve relevant information from the memory store

update_context: mutate an agent’s internal model_context by adding the retrieved information (used in the AssistantAgent class)

clear: clear all entries from the memory store

close: clean up any resources used by the memory store

ListMemory is provided as an example implementation of the Memory protocol. It is a simple list-based memory implementation that maintains memories in chronological order, appending the most recent memories to the model’s context. The implementation is designed to be straightforward and predictable, making it easy to understand and debug. In the following example, we will use ListMemory to maintain a memory bank of user preferences and demonstrate how it can be used to provide consistent context for agent responses over time.

We can inspect that the assistant_agent model_context is actually updated with the retrieved memory entries. The transform method is used to format the retrieved memory entries into a string that can be used by the agent. In this case, we simply concatenate the content of each memory entry into a single string.

We see above that the weather is returned in Centigrade as stated in the user preferences.

Similarly, assuming we ask a separate question about generating a meal plan, the agent is able to retrieve relevant information from the memory store and provide a personalized (vegan) response.

You can build on the Memory protocol to implement more complex memory stores. For example, you could implement a custom memory store that uses a vector database to store and retrieve information, or a memory store that uses a machine learning model to generate personalized responses based on the user’s preferences etc.

Specifically, you will need to overload the add, query and update_context methods to implement the desired functionality and pass the memory store to your agent.

Currently the following example memory stores are available as part of the autogen_ext extensions package.

autogen_ext.memory.chromadb.ChromaDBVectorMemory: A memory store that uses a vector database to store and retrieve information.

autogen_ext.memory.chromadb.SentenceTransformerEmbeddingFunctionConfig: A configuration class for the SentenceTransformer embedding function used by the ChromaDBVectorMemory store. Note that other embedding functions such as autogen_ext.memory.openai.OpenAIEmbeddingFunctionConfig can also be used with the ChromaDBVectorMemory store.

autogen_ext.memory.redis.RedisMemory: A memory store that uses a Redis vector database to store and retrieve information.

Note that you can also serialize the ChromaDBVectorMemory and save it to disk.

You can perform the same persistent memory storage using Redis. Note, you will need to have a running Redis instance to connect to.

See RedisMemory for instructions to run Redis locally or via Docker.

The RAG (Retrieval Augmented Generation) pattern which is common in building AI systems encompasses two distinct phases:

Indexing: Loading documents, chunking them, and storing them in a vector database

Retrieval: Finding and using relevant chunks during conversation runtime

In our previous examples, we manually added items to memory and passed them to our agents. In practice, the indexing process is usually automated and based on much larger document sources like product documentation, internal files, or knowledge bases.

Note: The quality of a RAG system is dependent on the quality of the chunking and retrieval process (models, embeddings, etc.). You may need to experiement with more advanced chunking and retrieval models to get the best results.

To begin, let’s create a simple document indexer that we will used to load documents, chunk them, and store them in a ChromaDBVectorMemory memory store.

Now let’s use our indexer with ChromaDBVectorMemory to build a complete RAG agent:

This implementation provides a RAG agent that can answer questions based on AutoGen documentation. When a question is asked, the Memory system retrieves relevant chunks and adds them to the context, enabling the assistant to generate informed responses.

For production systems, you might want to:

Implement more sophisticated chunking strategies

Add metadata filtering capabilities

Customize the retrieval scoring

Optimize embedding models for your specific domain

autogen_ext.memory.mem0.Mem0Memory provides integration with Mem0.ai’s memory system. It supports both cloud-based and local backends, offering advanced memory capabilities for agents. The implementation handles proper retrieval and context updating, making it suitable for production environments.

In the following example, we’ll demonstrate how to use Mem0Memory to maintain persistent memories across conversations:

The example above demonstrates how Mem0Memory can be used with an assistant agent. The memory integration ensures that:

All agent interactions are stored in Mem0 for future reference

Relevant memories (like user preferences) are automatically retrieved and added to the context

The agent can maintain consistent behavior based on stored memories

Mem0Memory is particularly useful for:

Long-running agent deployments that need persistent memory

Applications requiring enhanced privacy controls

Teams wanting unified memory management across agents

Use cases needing advanced memory filtering and analytics

Just like ChromaDBVectorMemory, you can serialize Mem0Memory configurations:

GraphFlow (Workflows)

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 2 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.ui import Console
from autogen_core.memory import ListMemory, MemoryContent, MemoryMimeType
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 3 (python):
```python
# Initialize user memory
user_memory = ListMemory()

# Add user preferences to memory
await user_memory.add(MemoryContent(content="The weather should be in metric units", mime_type=MemoryMimeType.TEXT))

await user_memory.add(MemoryContent(content="Meal recipe must be vegan", mime_type=MemoryMimeType.TEXT))


async def get_weather(city: str, units: str = "imperial") -> str:
    if units == "imperial":
        return f"The weather in {city} is 73 °F and Sunny."
    elif units == "metric":
        return f"The weather in {city} is 23 °C and Sunny."
    else:
        return f"Sorry, I don't know the weather in {city}."


assistant_agent = AssistantAgent(
    name="assistant_agent",
    model_client=OpenAIChatCompletionClient(
        model="gpt-4o-2024-08-06",
    ),
    tools=[get_weather],
    memory=[user_memory],
)
```

Example 4 (python):
```python
# Initialize user memory
user_memory = ListMemory()

# Add user preferences to memory
await user_memory.add(MemoryContent(content="The weather should be in metric units", mime_type=MemoryMimeType.TEXT))

await user_memory.add(MemoryContent(content="Meal recipe must be vegan", mime_type=MemoryMimeType.TEXT))


async def get_weather(city: str, units: str = "imperial") -> str:
    if units == "imperial":
        return f"The weather in {city} is 73 °F and Sunny."
    elif units == "metric":
        return f"The weather in {city} is 23 °C and Sunny."
    else:
        return f"Sorry, I don't know the weather in {city}."


assistant_agent = AssistantAgent(
    name="assistant_agent",
    model_client=OpenAIChatCompletionClient(
        model="gpt-4o-2024-08-06",
    ),
    tools=[get_weather],
    memory=[user_memory],
)
```

---

## Messages — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/messages.html

**Contents:**
- Messages#
- Types of Messages#
  - Agent-Agent Messages#
  - Internal Events#
- Custom Message Types#

In AutoGen AgentChat, messages facilitate communication and information exchange with other agents, orchestrators, and applications. AgentChat supports various message types, each designed for specific purposes.

At a high level, messages in AgentChat can be categorized into two types: agent-agent messages and an agent’s internal events and messages.

AgentChat supports many message types for agent-to-agent communication. They belong to subclasses of the base class BaseChatMessage. Concrete subclasses covers basic text and multimodal communication, such as TextMessage and MultiModalMessage.

For example, the following code snippet demonstrates how to create a text message, which accepts a string content and a string source:

Similarly, the following code snippet demonstrates how to create a multimodal message, which accepts a list of strings or Image objects:

The TextMessage and MultiModalMessage we have created can be passed to agents directly via the on_messages method, or as tasks given to a team run() method. Messages are also used in the responses of an agent. We will explain these in more detail in Agents and Teams.

AgentChat also supports the concept of events - messages that are internal to an agent. These messages are used to communicate events and information on actions within the agent itself, and belong to subclasses of the base class BaseAgentEvent.

Examples of these include ToolCallRequestEvent, which indicates that a request was made to call a tool, and ToolCallExecutionEvent, which contains the results of tool calls.

Typically, events are created by the agent itself and are contained in the inner_messages field of the Response returned from on_messages. If you are building a custom agent and have events that you want to communicate to other entities (e.g., a UI), you can include these in the inner_messages field of the Response. We will show examples of this in Custom Agents.

You can read about the full set of messages supported in AgentChat in the messages module.

You can create custom message types by subclassing the base class BaseChatMessage or BaseAgentEvent. This allows you to define your own message formats and behaviors, tailored to your application. Custom message types are useful when you write custom agents.

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.messages import TextMessage

text_message = TextMessage(content="Hello, world!", source="User")
```

Example 2 (sql):
```sql
from autogen_agentchat.messages import TextMessage

text_message = TextMessage(content="Hello, world!", source="User")
```

Example 3 (python):
```python
from io import BytesIO

import requests
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image as AGImage
from PIL import Image

pil_image = Image.open(BytesIO(requests.get("https://picsum.photos/300/200").content))
img = AGImage(pil_image)
multi_modal_message = MultiModalMessage(content=["Can you describe the content of this image?", img], source="User")
img
```

Example 4 (python):
```python
from io import BytesIO

import requests
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image as AGImage
from PIL import Image

pil_image = Image.open(BytesIO(requests.get("https://picsum.photos/300/200").content))
img = AGImage(pil_image)
multi_modal_message = MultiModalMessage(content=["Can you describe the content of this image?", img], source="User")
img
```

---

## Migration Guide for v0.2 to v0.4 — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html

**Contents:**
- Migration Guide for v0.2 to v0.4#
- What is v0.4?#
- New to AutoGen?#
- What’s in this guide?#
- Model Client#
  - Use component config#
  - Use model client class directly#
- Model Client for OpenAI-Compatible APIs#
- Model Client Cache#
- Assistant Agent#

This is a migration guide for users of the v0.2.* versions of autogen-agentchat to the v0.4 version, which introduces a new set of APIs and features. The v0.4 version contains breaking changes. Please read this guide carefully. We still maintain the v0.2 version in the 0.2 branch; however, we highly recommend you upgrade to the v0.4 version.

We no longer have admin access to the pyautogen PyPI package, and the releases from that package are no longer from Microsoft since version 0.2.34. To continue use the v0.2 version of AutoGen, install it using autogen-agentchat~=0.2. Please read our clarification statement regarding forks.

Since the release of AutoGen in 2023, we have intensively listened to our community and users from small startups and large enterprises, gathering much feedback. Based on that feedback, we built AutoGen v0.4, a from-the-ground-up rewrite adopting an asynchronous, event-driven architecture to address issues such as observability, flexibility, interactive control, and scale.

The v0.4 API is layered: the Core API is the foundation layer offering a scalable, event-driven actor framework for creating agentic workflows; the AgentChat API is built on Core, offering a task-driven, high-level framework for building interactive agentic applications. It is a replacement for AutoGen v0.2.

Most of this guide focuses on v0.4’s AgentChat API; however, you can also build your own high-level framework using just the Core API.

Jump straight to the AgentChat Tutorial to get started with v0.4.

We provide a detailed guide on how to migrate your existing codebase from v0.2 to v0.4.

See each feature below for detailed information on how to migrate.

Migration Guide for v0.2 to v0.4

What’s in this guide?

Use model client class directly

Model Client for OpenAI-Compatible APIs

Conversable Agent and Register Reply

Save and Load Agent State

Conversion between v0.2 and v0.4 Messages

Group Chat with Resume

Save and Load Group Chat State

Group Chat with Tool Use

Group Chat with Custom Selector (Stateflow)

Long Context Handling

Observability and Control

The following features currently in v0.2 will be provided in the future releases of v0.4.* versions:

Model Client Cost #4835

We will update this guide when the missing features become available.

In v0.2 you configure the model client as follows, and create the OpenAIWrapper object.

Note: In AutoGen 0.2, the OpenAI client would try configs in the list until one worked. 0.4 instead expects a specfic model configuration to be chosen.

In v0.4, we offer two ways to create a model client.

AutoGen 0.4 has a generic component configuration system. Model clients are a great use case for this. See below for how to create an OpenAI chat completion client.

Read more on OpenAIChatCompletionClient.

You can use a the OpenAIChatCompletionClient to connect to an OpenAI-Compatible API, but you need to specify the base_url and model_info.

Note: We don’t test all the OpenAI-Compatible APIs, and many of them works differently from the OpenAI API even though they may claim to suppor it. Please test them before using them.

Read about Model Clients in AgentChat Tutorial and more detailed information on Core API Docs

Support for other hosted models will be added in the future.

In v0.2, you can set the cache seed through the cache_seed parameter in the LLM config. The cache is enabled by default.

In v0.4, the cache is not enabled by default, to use it you need to use a ChatCompletionCache wrapper around the model client.

You can use a DiskCacheStore or RedisStore to store the cache.

Here’s an example of using diskcache for local caching:

In v0.2, you create an assistant agent as follows:

In v0.4, it is similar, but you need to specify model_client instead of llm_config.

However, the usage is somewhat different. In v0.4, instead of calling assistant.send, you call assistant.on_messages or assistant.on_messages_stream to handle incoming messages. Furthermore, the on_messages and on_messages_stream methods are asynchronous, and the latter returns an async generator to stream the inner thoughts of the agent.

Here is how you can call the assistant agent in v0.4 directly, continuing from the above example:

The CancellationToken can be used to cancel the request asynchronously when you call cancellation_token.cancel(), which will cause the await on the on_messages call to raise a CancelledError.

Read more on Agent Tutorial and AssistantAgent.

The AssistantAgent in v0.4 supports multi-modal inputs if the model client supports it. The vision capability of the model client is used to determine if the agent supports multi-modal inputs.

In v0.2, you create a user proxy as follows:

This user proxy would take input from the user through console, and would terminate if the incoming message ends with “TERMINATE”.

In v0.4, a user proxy is simply an agent that takes user input only, there is no other special configuration needed. You can create a user proxy as follows:

See UserProxyAgent for more details and how to customize the input function with timeout.

In v0.2, there was the concept of teachable agents as well as a RAG agents that could take a database config.

In v0.4, you can implement a RAG agent using the Memory class. Specifically, you can define a memory store class, and pass that as a parameter to the assistant agent. See the Memory tutorial for more details.

This clear separation of concerns allows you to implement a memory store that uses any database or storage system you want (you have to inherit from the Memory class) and use it with an assistant agent. The example below shows how to use a ChromaDB vector memory store with the assistant agent. In addition, your application logic should determine how and when to add content to the memory store. For example, you may choose to call memory.add for every response from the assistant agent or use a separate LLM call to determine if the content should be added to the memory store.

In v0.2, you can create a conversable agent and register a reply function as follows:

Rather than guessing what the reply_func does, all its parameters, and what the position should be, in v0.4, we can simply create a custom agent and implement the on_messages, on_reset, and produced_message_types methods.

You can then use the custom agent in the same way as the AssistantAgent. See Custom Agent Tutorial for more details.

In v0.2 there is no built-in way to save and load an agent’s state: you need to implement it yourself by exporting the chat_messages attribute of ConversableAgent and importing it back through the chat_messages parameter.

In v0.4, you can call save_state and load_state methods on agents to save and load their state.

You can also call save_state and load_state on any teams, such as RoundRobinGroupChat to save and load the state of the entire team.

In v0.2, you can create a two-agent chat for code execution as follows:

To get the same behavior in v0.4, you can use the AssistantAgent and CodeExecutorAgent together in a RoundRobinGroupChat.

In v0.2, to create a tool use chatbot, you must have two agents, one for calling the tool and one for executing the tool. You need to initiate a two-agent chat for every user request.

In v0.4, you really just need one agent – the AssistantAgent – to handle both the tool calling and tool execution.

When using tool-equipped agents inside a group chat such as RoundRobinGroupChat, you simply do the same as above to add tools to the agents, and create a group chat with the agents.

In v0.2, you get a ChatResult object from the initiate_chat method. For example:

See ChatResult Docs for more details.

In v0.4, you get a TaskResult object from a run or run_stream method. The TaskResult object contains the messages which is the message history of the chat, including both agents’ private (tool calls, etc.) and public messages.

There are some notable differences between TaskResult and ChatResult:

The messages list in TaskResult uses different message format than the ChatResult.chat_history list.

There is no summary field. It is up to the application to decide how to summarize the chat using the messages list.

human_input is not provided in the TaskResult object, as the user input can be extracted from the messages list by filtering with the source field.

cost is not provided in the TaskResult object, however, you can calculate the cost based on token usage. It would be a great community extension to add cost calculation. See community extensions.

You can use the following conversion functions to convert between a v0.4 message in autogen_agentchat.base.TaskResult.messages and a v0.2 message in ChatResult.chat_history.

In v0.2, you need to create a GroupChat class and pass it into a GroupChatManager, and have a participant that is a user proxy to initiate the chat. For a simple scenario of a writer and a critic, you can do the following:

In v0.4, you can use the RoundRobinGroupChat to achieve the same behavior.

For LLM-based speaker selection, you can use the SelectorGroupChat instead. See Selector Group Chat Tutorial and SelectorGroupChat for more details.

Note: In v0.4, you do not need to register functions on a user proxy to use tools in a group chat. You can simply pass the tool functions to the AssistantAgent as shown in the Tool Use section. The agent will automatically call the tools when needed. If your tool doesn’t output well formed response, you can use the reflect_on_tool_use parameter to have the model reflect on the tool use.

In v0.2, group chat with resume is a bit complicated. You need to explicitly save the group chat messages and load them back when you want to resume the chat. See Resuming Group Chat in v0.2 for more details.

In v0.4, you can simply call run or run_stream again with the same group chat object to resume the chat. To export and load the state, you can use save_state and load_state methods.

In v0.2, you need to explicitly save the group chat messages and load them back when you want to resume the chat.

In v0.4, you can simply call save_state and load_state methods on the group chat object. See Group Chat with Resume for an example.

In v0.2 group chat, when tools are involved, you need to register the tool functions on a user proxy, and include the user proxy in the group chat. The tool calls made by other agents will be routed to the user proxy to execute.

We have observed numerous issues with this approach, such as the the tool call routing not working as expected, and the tool call request and result cannot be accepted by models without support for function calling.

In v0.4, there is no need to register the tool functions on a user proxy, as the tools are directly executed within the AssistantAgent, which publishes the response from the tool to the group chat. So the group chat manager does not need to be involved in routing tool calls.

See Selector Group Chat Tutorial for an example of using tools in a group chat.

In v0.2 group chat, when the speaker_selection_method is set to a custom function, it can override the default selection method. This is useful for implementing a state-based selection method. For more details, see Custom Sepaker Selection in v0.2.

In v0.4, you can use the SelectorGroupChat with selector_func to achieve the same behavior. The selector_func is a function that takes the current message thread of the group chat and returns the next speaker’s name. If None is returned, the LLM-based selection method will be used.

Here is an example of using the state-based selection method to implement a web search/analysis scenario.

Nested chat allows you to nest a whole team or another agent inside an agent. This is useful for creating a hierarchical structure of agents or “information silos”, as the nested agents cannot communicate directly with other agents outside of the same group.

In v0.2, nested chat is supported by using the register_nested_chats method on the ConversableAgent class. You need to specify the nested sequence of agents using dictionaries, See Nested Chat in v0.2 for more details.

In v0.4, nested chat is an implementation detail of a custom agent. You can create a custom agent that takes a team or another agent as a parameter and implements the on_messages method to trigger the nested team or agent. It is up to the application to decide how to pass or transform the messages from and to the nested team or agent.

The following example shows a simple nested chat that counts numbers.

You should see the following output:

You can take a look at SocietyOfMindAgent for a more complex implementation.

In v0.2, sequential chat is supported by using the initiate_chats function. It takes input a list of dictionary configurations for each step of the sequence. See Sequential Chat in v0.2 for more details.

Base on the feedback from the community, the initiate_chats function is too opinionated and not flexible enough to support the diverse set of scenarios that users want to implement. We often find users struggling to get the initiate_chats function to work when they can easily glue the steps together usign basic Python code. Therefore, in v0.4, we do not provide a built-in function for sequential chat in the AgentChat API.

Instead, you can create an event-driven sequential workflow using the Core API, and use the other components provided the AgentChat API to implement each step of the workflow. See an example of sequential workflow in the Core API Tutorial.

We recognize that the concept of workflow is at the heart of many applications, and we will provide more built-in support for workflows in the future.

In v0.2, GPTAssistantAgent is a special agent class that is backed by the OpenAI Assistant API.

In v0.4, the equivalent is the OpenAIAssistantAgent class. It supports the same set of features as the GPTAssistantAgent in v0.2 with more such as customizable threads and file uploads. See OpenAIAssistantAgent for more details.

In v0.2, long context that overflows the model’s context window can be handled by using the transforms capability that is added to an ConversableAgent after which is contructed.

The feedbacks from our community has led us to believe this feature is essential and should be a built-in component of AssistantAgent, and can be used for every custom agent.

In v0.4, we introduce the ChatCompletionContext base class that manages message history and provides a virtual view of the history. Applications can use built-in implementations such as BufferedChatCompletionContext to limit the message history sent to the model, or provide their own implementations that creates different virtual views.

To use BufferedChatCompletionContext in an AssistantAgent in a chatbot scenario.

In this example, the chatbot can only read the last 10 messages in the history.

In v0.4 AgentChat, you can observe the agents by using the on_messages_stream method which returns an async generator to stream the inner thoughts and actions of the agent. For teams, you can use the run_stream method to stream the inner conversation among the agents in the team. Your application can use these streams to observe the agents and teams in real-time.

Both the on_messages_stream and run_stream methods takes a CancellationToken as a parameter which can be used to cancel the output stream asynchronously and stop the agent or team. For teams, you can also use termination conditions to stop the team when a certain condition is met. See Termination Condition Tutorial for more details.

Unlike the v0.2 which comes with a special logging module, the v0.4 API simply uses Python’s logging module to log events such as model client calls. See Logging in the Core API documentation for more details.

The code executors in v0.2 and v0.4 are nearly identical except the v0.4 executors support async API. You can also use CancellationToken to cancel a code execution if it takes too long. See Command Line Code Executors Tutorial in the Core API documentation.

We also added ACADynamicSessionsCodeExecutor that can use Azure Container Apps (ACA) dynamic sessions for code execution. See ACA Dynamic Sessions Code Executor Docs.

**Examples:**

Example 1 (json):
```json
from autogen.oai import OpenAIWrapper

config_list = [
    {"model": "gpt-4o", "api_key": "sk-xxx"},
    {"model": "gpt-4o-mini", "api_key": "sk-xxx"},
]

model_client = OpenAIWrapper(config_list=config_list)
```

Example 2 (json):
```json
from autogen.oai import OpenAIWrapper

config_list = [
    {"model": "gpt-4o", "api_key": "sk-xxx"},
    {"model": "gpt-4o-mini", "api_key": "sk-xxx"},
]

model_client = OpenAIWrapper(config_list=config_list)
```

Example 3 (json):
```json
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o",
        "api_key": "sk-xxx" # os.environ["...']
    }
}

model_client = ChatCompletionClient.load_component(config)
```

Example 4 (json):
```json
from autogen_core.models import ChatCompletionClient

config = {
    "provider": "OpenAIChatCompletionClient",
    "config": {
        "model": "gpt-4o",
        "api_key": "sk-xxx" # os.environ["...']
    }
}

model_client = ChatCompletionClient.load_component(config)
```

---

## Models — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html

**Contents:**
- Models#
- Log Model Calls#
- OpenAI#
- Azure OpenAI#
- Azure AI Foundry#
- Anthropic (experimental)#
- Ollama (experimental)#
- Gemini (experimental)#
- Llama API (experimental)#
- Semantic Kernel Adapter#

In many cases, agents need access to LLM model services such as OpenAI, Azure OpenAI, or local models. Since there are many different providers with different APIs, autogen-core implements a protocol for model clients and autogen-ext implements a set of model clients for popular model services. AgentChat can use these model clients to interact with model services.

This section provides a quick overview of available model clients. For more details on how to use them directly, please refer to Model Clients in the Core API documentation.

See ChatCompletionCache for a caching wrapper to use with the following clients.

AutoGen uses standard Python logging module to log events like model calls and responses. The logger name is autogen_core.EVENT_LOGGER_NAME, and the event type is LLMCall.

To access OpenAI models, install the openai extension, which allows you to use the OpenAIChatCompletionClient.

You will also need to obtain an API key from OpenAI.

To test the model client, you can use the following code:

You can use this client with models hosted on OpenAI-compatible endpoints, however, we have not tested this functionality. See OpenAIChatCompletionClient for more information.

Similarly, install the azure and openai extensions to use the AzureOpenAIChatCompletionClient.

To use the client, you need to provide your deployment id, Azure Cognitive Services endpoint, api version, and model capabilities. For authentication, you can either provide an API key or an Azure Active Directory (AAD) token credential.

The following code snippet shows how to use AAD authentication. The identity used must be assigned the Cognitive Services OpenAI User role.

See here for how to use the Azure client directly or for more information.

Azure AI Foundry (previously known as Azure AI Studio) offers models hosted on Azure. To use those models, you use the AzureAIChatCompletionClient.

You need to install the azure extra to use this client.

Below is an example of using this client with the Phi-4 model from GitHub Marketplace.

To use the AnthropicChatCompletionClient, you need to install the anthropic extra. Underneath, it uses the anthropic python sdk to access the models. You will also need to obtain an API key from Anthropic.

Ollama is a local model server that can run models locally on your machine.

Small local models are typically not as capable as larger models on the cloud. For some tasks they may not perform as well and the output may be suprising.

To use Ollama, install the ollama extension and use the OllamaChatCompletionClient.

Gemini currently offers an OpenAI-compatible API (beta). So you can use the OpenAIChatCompletionClient with the Gemini API.

While some model providers may offer OpenAI-compatible APIs, they may still have minor differences. For example, the finish_reason field may be different in the response.

Also, as Gemini adds new models, you may need to define the models capabilities via the model_info field. For example, to use gemini-2.0-flash-lite or a similar new model, you can use the following code:

Llama API is the Meta’s first party API offering. It currently offers an OpenAI compatible endpoint. So you can use the OpenAIChatCompletionClient with the Llama API.

This endpoint fully supports the following OpenAI client library features:

Structured output (JSON mode)

Function calling (tools)

The SKChatCompletionAdapter allows you to use Semantic kernel model clients as a ChatCompletionClient by adapting them to the required interface.

You need to install the relevant provider extras to use this adapter.

The list of extras that can be installed:

semantic-kernel-anthropic: Install this extra to use Anthropic models.

semantic-kernel-google: Install this extra to use Google Gemini models.

semantic-kernel-ollama: Install this extra to use Ollama models.

semantic-kernel-mistralai: Install this extra to use MistralAI models.

semantic-kernel-aws: Install this extra to use AWS models.

semantic-kernel-hugging-face: Install this extra to use Hugging Face models.

For example, to use Anthropic models, you need to install semantic-kernel-anthropic.

To use this adapter, you need create a Semantic Kernel model client and pass it to the adapter.

For example, to use the Anthropic model:

Read more about the Semantic Kernel Adapter.

**Examples:**

Example 1 (python):
```python
import logging

from autogen_core import EVENT_LOGGER_NAME

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.addHandler(logging.StreamHandler())
logger.setLevel(logging.INFO)
```

Example 2 (python):
```python
import logging

from autogen_core import EVENT_LOGGER_NAME

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(EVENT_LOGGER_NAME)
logger.addHandler(logging.StreamHandler())
logger.setLevel(logging.INFO)
```

Example 3 (unknown):
```unknown
pip install "autogen-ext[openai]"
```

Example 4 (unknown):
```unknown
pip install "autogen-ext[openai]"
```

---

## Selector Group Chat — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html

**Contents:**
- Selector Group Chat#
- How Does it Work?#
- Example: Web Search/Analysis#
  - Agents#
  - Workflow#
  - Termination Conditions#
  - Selector Prompt#
  - Running the Team#
- Custom Selector Function#
- Custom Candidate Function#

SelectorGroupChat implements a team where participants take turns broadcasting messages to all other members. A generative model (e.g., an LLM) selects the next speaker based on the shared context, enabling dynamic, context-aware collaboration.

Key features include:

Model-based speaker selection

Configurable participant roles and descriptions

Prevention of consecutive turns by the same speaker (optional)

Customizable selection prompting

Customizable selection function to override the default model-based selection

Customizable candidate function to narrow-down the set of agents for selection using model

SelectorGroupChat is a high-level API. For more control and customization, refer to the Group Chat Pattern in the Core API documentation to implement your own group chat logic.

SelectorGroupChat is a group chat similar to RoundRobinGroupChat, but with a model-based next speaker selection mechanism. When the team receives a task through run() or run_stream(), the following steps are executed:

The team analyzes the current conversation context, including the conversation history and participants’ name and description attributes, to determine the next speaker using a model. By default, the team will not select the same speak consecutively unless it is the only agent available. This can be changed by setting allow_repeated_speaker=True. You can also override the model by providing a custom selection function.

The team prompts the selected speaker agent to provide a response, which is then broadcasted to all other participants.

The termination condition is checked to determine if the conversation should end, if not, the process repeats from step 1.

When the conversation ends, the team returns the TaskResult containing the conversation history from this task.

Once the team finishes the task, the conversation context is kept within the team and all participants, so the next task can continue from the previous conversation context. You can reset the conversation context by calling reset().

In this section, we will demonstrate how to use SelectorGroupChat with a simple example for a web search and data analysis task.

This system uses three specialized agents:

Planning Agent: The strategic coordinator that breaks down complex tasks into manageable subtasks.

Web Search Agent: An information retrieval specialist that interfaces with the search_web_tool.

Data Analyst Agent: An agent specialist in performing calculations equipped with percentage_change_tool.

The tools search_web_tool and percentage_change_tool are external tools that the agents can use to perform their tasks.

Let’s create the specialized agents using the AssistantAgent class. It is important to note that the agents’ name and description attributes are used by the model to determine the next speaker, so it is recommended to provide meaningful names and descriptions.

By default, AssistantAgent returns the tool output as the response. If your tool does not return a well-formed string in natural language format, you may want to add a reflection step within the agent by setting reflect_on_tool_use=True when creating the agent. This will allow the agent to reflect on the tool output and provide a natural language response.

The task is received by the SelectorGroupChat which, based on agent descriptions, selects the most appropriate agent to handle the initial task (typically the Planning Agent).

The Planning Agent analyzes the task and breaks it down into subtasks, assigning each to the most appropriate agent using the format: <agent> : <task>

Based on the conversation context and agent descriptions, the SelectorGroupChat manager dynamically selects the next agent to handle their assigned subtask.

The Web Search Agent performs searches one at a time, storing results in the shared conversation history.

The Data Analyst processes the gathered information using available calculation tools when selected.

The workflow continues with agents being dynamically selected until either:

The Planning Agent determines all subtasks are complete and sends “TERMINATE”

An alternative termination condition is met (e.g., a maximum number of messages)

When defining your agents, make sure to include a helpful description since this is used to decide which agent to select next.

Let’s use two termination conditions: TextMentionTermination to end the conversation when the Planning Agent sends “TERMINATE”, and MaxMessageTermination to limit the conversation to 25 messages to avoid infinite loop.

SelectorGroupChat uses a model to select the next speaker based on the conversation context. We will use a custom selector prompt to properly align with the workflow.

The string variables available in the selector prompt are:

{participants}: The names of candidates for selection. The format is ["<name1>", "<name2>", ...].

{roles}: A newline-separated list of names and descriptions of the candidate agents. The format for each line is: "<name> : <description>".

{history}: The conversation history formatted as a double newline separated of names and message content. The format for each message is: "<name> : <message content>".

Try not to overload the model with too much instruction in the selector prompt.

What is too much? It depends on the capabilities of the model you are using. For GPT-4o and equivalents, you can use a selector prompt with a condition for when each speaker should be selected. For smaller models such as Phi-4, you should keep the selector prompt as simple as possible such as the one used in this example.

Generally, if you find yourself writing multiple conditions for each agent, it is a sign that you should consider using a custom selection function, or breaking down the task into smaller, sequential tasks to be handled by separate agents or teams.

Let’s create the team with the agents, termination conditions, and custom selector prompt.

Now we run the team with a task to find information about an NBA player.

As we can see, after the Web Search Agent conducts the necessary searches and the Data Analyst Agent completes the necessary calculations, we find that Dwayne Wade was the Miami Heat player with the highest points in the 2006-2007 season, and the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons is 85.98%!

Often times we want better control over the selection process. To this end, we can set the selector_func argument with a custom selector function to override the default model-based selection. This allows us to implement more complex selection logic and state-based transitions.

For instance, we want the Planning Agent to speak immediately after any specialized agent to check the progress.

Returning None from the custom selector function will use the default model-based selection.

Custom selector functions are not serialized when .dump_component() is called on the SelectorGroupChat team . If you need to serialize team configurations with custom selector functions, consider implementing custom workflows and serialization logic.

You can see from the conversation log that the Planning Agent always speaks immediately after the specialized agents.

Each participant agent only makes one step (executing tools, generating a response, etc.) on each turn. If you want an AssistantAgent to repeat until it stop returning a ToolCallSummaryMessage when it has finished running all the tools it needs to run, you can do so by checking the last message and returning the agent if it is a ToolCallSummaryMessage.

One more possible requirement might be to automatically select the next speaker from a filtered list of agents. For this, we can set candidate_func parameter with a custom candidate function to filter down the list of potential agents for speaker selection for each turn of groupchat.

This allow us to restrict speaker selection to a specific set of agents after a given agent.

The candidate_func is only valid if selector_func is not set. Returning None or an empty list [] from the custom candidate function will raise a ValueError.

You can see from the conversation log that the Planning Agent returns to conversation once the Web Search Agent and Data Analyst Agent took their turns and it finds that the task was not finished as expected so it called the WebSearchAgent again to get rebound values and then called DataAnalysetAgent to get the percentage change.

We can add UserProxyAgent to the team to provide user feedback during a run. See Human-in-the-Loop for more details about UserProxyAgent.

To use the UserProxyAgent in the web search example, we simply add it to the team and update the selector function to always check for user feedback after the planning agent speaks. If the user responds with "APPROVE", the conversation continues, otherwise, the planning agent tries again, until the user approves.

Now, the user’s feedback is incorporated into the conversation flow, and the user can approve or reject the planning agent’s decisions.

So far in the examples, we have used a gpt-4o model. Models like gpt-4o and gemini-1.5-flash are great at following instructions, so you can have relatively detailed instructions in the selector prompt for the team and the system messages for each agent to guide their behavior.

However, if you are using a reasoning model like o3-mini, you will need to keep the selector prompt and system messages as simple and to the point as possible. This is because the reasoning models are already good at coming up with their own instructions given the context provided to them.

This also means that we don’t need a planning agent to break down the task anymore, since the SelectorGroupChat that uses a reasoning model can do that on its own.

In the following example, we will use o3-mini as the model for the agents and the team, and we will not use a planning agent. Also, we are keeping the selector prompt and system messages as simple as possible.

For more guidance on how to prompt reasoning models, see the Azure AI Services Blog on Prompt Engineering for OpenAI’s O1 and O3-mini Reasoning Models

**Examples:**

Example 1 (python):
```python
from typing import List, Sequence

from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 2 (python):
```python
from typing import List, Sequence

from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 3 (python):
```python
# Note: This example uses mock tools instead of real APIs for demonstration purposes
def search_web_tool(query: str) -> str:
    if "2006-2007" in query:
        return """Here are the total points scored by Miami Heat players in the 2006-2007 season:
        Udonis Haslem: 844 points
        Dwayne Wade: 1397 points
        James Posey: 550 points
        ...
        """
    elif "2007-2008" in query:
        return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214."
    elif "2008-2009" in query:
        return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398."
    return "No data found."


def percentage_change_tool(start: float, end: float) -> float:
    return ((end - start) / start) * 100
```

Example 4 (python):
```python
# Note: This example uses mock tools instead of real APIs for demonstration purposes
def search_web_tool(query: str) -> str:
    if "2006-2007" in query:
        return """Here are the total points scored by Miami Heat players in the 2006-2007 season:
        Udonis Haslem: 844 points
        Dwayne Wade: 1397 points
        James Posey: 550 points
        ...
        """
    elif "2007-2008" in query:
        return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214."
    elif "2008-2009" in query:
        return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398."
    return "No data found."


def percentage_change_tool(start: float, end: float) -> float:
    return ((end - start) / start) * 100
```

---

## Serializing Components — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/serialize-components.html

**Contents:**
- Serializing Components#
- Termination Condition Example#
- Agent Example#
- Team Example#

AutoGen provides a Component configuration class that defines behaviours to serialize/deserialize component into declarative specifications. We can accomplish this by calling .dump_component() and .load_component() respectively. This is useful for debugging, visualizing, and even for sharing your work with others. In this notebook, we will demonstrate how to serialize multiple components to a declarative specification like a JSON file.

ONLY LOAD COMPONENTS FROM TRUSTED SOURCES.

With serilized components, each component implements the logic for how it is serialized and deserialized - i.e., how the declarative specification is generated and how it is converted back to an object.

In some cases, creating an object may include executing code (e.g., a serialized function). ONLY LOAD COMPONENTS FROM TRUSTED SOURCES.

selector_func is not serializable and will be ignored during serialization and deserialization process.

In the example below, we will define termination conditions (a part of an agent team) in python, export this to a dictionary/json and also demonstrate how the termination condition object can be loaded from the dictionary/json.

In the example below, we will define an agent in python, export this to a dictionary/json and also demonstrate how the agent object can be loaded from the dictionary/json.

A similar approach can be used to serialize the MultiModalWebSurfer agent.

In the example below, we will define a team in python, export this to a dictionary/json and also demonstrate how the team object can be loaded from the dictionary/json.

Tracing and Observability

**Examples:**

Example 1 (python):
```python
from autogen_agentchat.conditions import MaxMessageTermination, StopMessageTermination

max_termination = MaxMessageTermination(5)
stop_termination = StopMessageTermination()

or_termination = max_termination | stop_termination

or_term_config = or_termination.dump_component()
print("Config: ", or_term_config.model_dump_json())

new_or_termination = or_termination.load_component(or_term_config)
```

Example 2 (python):
```python
from autogen_agentchat.conditions import MaxMessageTermination, StopMessageTermination

max_termination = MaxMessageTermination(5)
stop_termination = StopMessageTermination()

or_termination = max_termination | stop_termination

or_term_config = or_termination.dump_component()
print("Config: ", or_term_config.model_dump_json())

new_or_termination = or_termination.load_component(or_term_config)
```

Example 3 (json):
```json
Config:  {"provider":"autogen_agentchat.base.OrTerminationCondition","component_type":"termination","version":1,"component_version":1,"description":null,"config":{"conditions":[{"provider":"autogen_agentchat.conditions.MaxMessageTermination","component_type":"termination","version":1,"component_version":1,"config":{"max_messages":5}},{"provider":"autogen_agentchat.conditions.StopMessageTermination","component_type":"termination","version":1,"component_version":1,"config":{}}]}}
```

Example 4 (json):
```json
Config:  {"provider":"autogen_agentchat.base.OrTerminationCondition","component_type":"termination","version":1,"component_version":1,"description":null,"config":{"conditions":[{"provider":"autogen_agentchat.conditions.MaxMessageTermination","component_type":"termination","version":1,"component_version":1,"config":{"max_messages":5}},{"provider":"autogen_agentchat.conditions.StopMessageTermination","component_type":"termination","version":1,"component_version":1,"config":{}}]}}
```

---

## Swarm — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/swarm.html

**Contents:**
- Swarm#
- How Does It Work?#
- Customer Support Example#
  - Workflow#
  - Tools#
  - Agents#
- Stock Research Example#
  - Workflow#
  - Tools#

Swarm implements a team in which agents can hand off task to other agents based on their capabilities. It is a multi-agent design pattern first introduced by OpenAI in Swarm. The key idea is to let agent delegate tasks to other agents using a special tool call, while all agents share the same message context. This enables agents to make local decisions about task planning, rather than relying on a central orchestrator such as in SelectorGroupChat.

Swarm is a high-level API. If you need more control and customization that is not supported by this API, you can take a look at the Handoff Pattern in the Core API documentation and implement your own version of the Swarm pattern.

At its core, the Swarm team is a group chat where agents take turn to generate a response. Similar to SelectorGroupChat and RoundRobinGroupChat, participant agents broadcast their responses so all agents share the same message context.

Different from the other two group chat teams, at each turn, the speaker agent is selected based on the most recent HandoffMessage message in the context. This naturally requires each agent in the team to be able to generate HandoffMessage to signal which other agents that it hands off to.

For AssistantAgent, you can set the handoffs argument to specify which agents it can hand off to. You can use Handoff to customize the message content and handoff behavior.

The overall process can be summarized as follows:

Each agent has the ability to generate HandoffMessage to signal which other agents it can hand off to. For AssistantAgent, this means setting the handoffs argument.

When the team starts on a task, the first speaker agents operate on the task and make localized decision about whether to hand off and to whom.

When an agent generates a HandoffMessage, the receiving agent takes over the task with the same message context.

The process continues until a termination condition is met.

The AssistantAgent uses the tool calling capability of the model to generate handoffs. This means that the model must support tool calling. If the model does parallel tool calling, multiple handoffs may be generated at the same time. This can lead to unexpected behavior. To avoid this, you can disable parallel tool calling by configuring the model client. For OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient, you can set parallel_tool_calls=False in the configuration.

In this section, we will show you two examples of how to use the Swarm team:

A customer support team with human-in-the-loop handoff.

An automonous team for content generation.

This system implements a flights refund scenario with two agents:

Travel Agent: Handles general travel and refund coordination.

Flights Refunder: Specializes in processing flight refunds with the refund_flight tool.

Additionally, we let the user interact with the agents, when agents handoff to "user".

The Travel Agent initiates the conversation and evaluates the user’s request.

Based on the request:

For refund-related tasks, the Travel Agent hands off to the Flights Refunder.

For information needed from the customer, either agent can hand off to the "user".

The Flights Refunder processes refunds using the refund_flight tool when appropriate.

If an agent hands off to the "user", the team execution will stop and wait for the user to input a response.

When the user provides input, it’s sent back to the team as a HandoffMessage. This message is directed to the agent that originally requested user input.

The process continues until the Travel Agent determines the task is complete and terminates the workflow.

This system is designed to perform stock research tasks by leveraging four agents:

Planner: The central coordinator that delegates specific tasks to specialized agents based on their expertise. The planner ensures that each agent is utilized efficiently and oversees the overall workflow.

Financial Analyst: A specialized agent responsible for analyzing financial metrics and stock data using tools such as get_stock_data.

News Analyst: An agent focused on gathering and summarizing recent news articles relevant to the stock, using tools such as get_news.

Writer: An agent tasked with compiling the findings from the stock and news analysis into a cohesive final report.

The Planner initiates the research process by delegating tasks to the appropriate agents in a step-by-step manner.

Each agent performs its task independently and appends their work to the shared message thread/history. Rather than directly returning results to the planner, all agents contribute to and read from this shared message history. When agents generate their work using the LLM, they have access to this shared message history, which provides context and helps track the overall progress of the task.

Once an agent completes its task, it hands off control back to the planner.

The process continues until the planner determines that all necessary tasks have been completed and decides to terminate the workflow.

**Examples:**

Example 1 (python):
```python
from typing import Any, Dict, List

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination
from autogen_agentchat.messages import HandoffMessage
from autogen_agentchat.teams import Swarm
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 2 (python):
```python
from typing import Any, Dict, List

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination
from autogen_agentchat.messages import HandoffMessage
from autogen_agentchat.teams import Swarm
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 3 (python):
```python
def refund_flight(flight_id: str) -> str:
    """Refund a flight"""
    return f"Flight {flight_id} refunded"
```

Example 4 (python):
```python
def refund_flight(flight_id: str) -> str:
    """Refund a flight"""
    return f"Flight {flight_id} refunded"
```

---

## Teams — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html

**Contents:**
- Teams#
- Creating a Team#
- Running a Team#
- Observing a Team#
- Resetting a Team#
- Stopping a Team#
- Resuming a Team#
- Aborting a Team#
- Single-Agent Team#

In this section you’ll learn how to create a multi-agent team (or simply team) using AutoGen. A team is a group of agents that work together to achieve a common goal.

We’ll first show you how to create and run a team. We’ll then explain how to observe the team’s behavior, which is crucial for debugging and understanding the team’s performance, and common operations to control the team’s behavior.

AgentChat supports several team presets:

RoundRobinGroupChat: A team that runs a group chat with participants taking turns in a round-robin fashion (covered on this page). Tutorial

SelectorGroupChat: A team that selects the next speaker using a ChatCompletion model after each message. Tutorial

MagenticOneGroupChat: A generalist multi-agent system for solving open-ended web and file-based tasks across a variety of domains. Tutorial

Swarm: A team that uses HandoffMessage to signal transitions between agents. Tutorial

When should you use a team?

Teams are for complex tasks that require collaboration and diverse expertise. However, they also demand more scaffolding to steer compared to single agents. While AutoGen simplifies the process of working with teams, start with a single agent for simpler tasks, and transition to a multi-agent team when a single agent proves inadequate. Ensure that you have optimized your single agent with the appropriate tools and instructions before moving to a team-based approach.

RoundRobinGroupChat is a simple yet effective team configuration where all agents share the same context and take turns responding in a round-robin fashion. Each agent, during its turn, broadcasts its response to all other agents, ensuring that the entire team maintains a consistent context.

We will begin by creating a team with two AssistantAgent and a TextMentionTermination condition that stops the team when a specific word is detected in the agent’s response.

The two-agent team implements the reflection pattern, a multi-agent design pattern where a critic agent evaluates the responses of a primary agent. Learn more about the reflection pattern using the Core API.

Let’s call the run() method to start the team with a task.

The team runs the agents until the termination condition was met. In this case, the team ran agents following a round-robin order until the the termination condition was met when the word “APPROVE” was detected in the agent’s response. When the team stops, it returns a TaskResult object with all the messages produced by the agents in the team.

Similar to the agent’s on_messages_stream() method, you can stream the team’s messages while it is running by calling the run_stream() method. This method returns a generator that yields messages produced by the agents in the team as they are generated, with the final item being the TaskResult object.

As demonstrated in the example above, you can determine the reason why the team stopped by checking the stop_reason attribute.

The Console() method provides a convenient way to print messages to the console with proper formatting.

You can reset the team by calling the reset() method. This method will clear the team’s state, including all agents. It will call the each agent’s on_reset() method to clear the agent’s state.

It is usually a good idea to reset the team if the next task is not related to the previous task. However, if the next task is related to the previous task, you don’t need to reset and you can instead resume the team.

Apart from automatic termination conditions such as TextMentionTermination that stops the team based on the internal state of the team, you can also stop the team from outside by using the ExternalTermination.

Calling set() on ExternalTermination will stop the team when the current agent’s turn is over. Thus, the team may not stop immediately. This allows the current agent to finish its turn and broadcast the final message to the team before the team stops, keeping the team’s state consistent.

From the ouput above, you can see the team stopped because the external termination condition was met, but the speaking agent was able to finish its turn before the team stopped.

Teams are stateful and maintains the conversation history and context after each run, unless you reset the team.

You can resume a team to continue from where it left off by calling the run() or run_stream() method again without a new task. RoundRobinGroupChat will continue from the next agent in the round-robin order.

You can see the team resumed from where it left off in the output above, and the first message is from the next agent after the last agent that spoke before the team stopped.

Let’s resume the team again with a new task while keeping the context about the previous task.

You can abort a call to run() or run_stream() during execution by setting a CancellationToken passed to the cancellation_token parameter.

Different from stopping a team, aborting a team will immediately stop the team and raise a CancelledError exception.

The caller will get a CancelledError exception when the team is aborted.

Starting with version 0.6.2, you can use AssistantAgent with max_tool_iterations to run the agent with multiple iterations of tool calls. So you may not need to use a single-agent team if you just want to run the agent in a tool-calling loop.

Often, you may want to run a single agent in a team configuration. This is useful for running the AssistantAgent in a loop until a termination condition is met.

This is different from running the AssistantAgent using its run() or run_stream() method, which only runs the agent for one step and returns the result. See AssistantAgent for more details about a single step.

Here is an example of running a single agent in a RoundRobinGroupChat team configuration with a TextMessageTermination condition. The task is to increment a number until it reaches 10 using a tool. The agent will keep calling the tool until the number reaches 10, and then it will return a final TextMessage which will stop the run.

The key is to focus on the termination condition. In this example, we use a TextMessageTermination condition that stops the team when the agent stop producing ToolCallSummaryMessage. The team will keep running until the agent produces a TextMessage with the final result.

You can also use other termination conditions to control the agent. See Termination Conditions for more details.

**Examples:**

Example 1 (python):
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.base import TaskResult
from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Create an OpenAI model client.
model_client = OpenAIChatCompletionClient(
    model="gpt-4o-2024-08-06",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
)

# Create the primary agent.
primary_agent = AssistantAgent(
    "primary",
    model_client=model_client,
    system_message="You are a helpful AI assistant.",
)

# Create the critic agent.
critic_agent = AssistantAgent(
    "critic",
    model_client=model_client,
    system_message="Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.",
)

# Define a termination condition that stops the task if the critic approves.
text_termination = TextMentionTermination("APPROVE")

# Create a team with the primary and critic agents.
team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=text_termination)
```

Example 2 (python):
```python
import asyncio

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.base import TaskResult
from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient

# Create an OpenAI model client.
model_client = OpenAIChatCompletionClient(
    model="gpt-4o-2024-08-06",
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
)

# Create the primary agent.
primary_agent = AssistantAgent(
    "primary",
    model_client=model_client,
    system_message="You are a helpful AI assistant.",
)

# Create the critic agent.
critic_agent = AssistantAgent(
    "critic",
    model_client=model_client,
    system_message="Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.",
)

# Define a termination condition that stops the task if the critic approves.
text_termination = TextMentionTermination("APPROVE")

# Create a team with the primary and critic agents.
team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=text_termination)
```

Example 3 (python):
```python
# Use `asyncio.run(...)` when running in a script.
result = await team.run(task="Write a short poem about the fall season.")
print(result)
```

Example 4 (python):
```python
# Use `asyncio.run(...)` when running in a script.
result = await team.run(task="Write a short poem about the fall season.")
print(result)
```

---

## Termination — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html

**Contents:**
- Termination#
- Basic Usage#
- Combining Termination Conditions#
- Custom Termination Condition#

In the previous section, we explored how to define agents, and organize them into teams that can solve tasks. However, a run can go on forever, and in many cases, we need to know when to stop them. This is the role of the termination condition.

AgentChat supports several termination condition by providing a base TerminationCondition class and several implementations that inherit from it.

A termination condition is a callable that takes a sequence of BaseAgentEvent or BaseChatMessage objects since the last time the condition was called, and returns a StopMessage if the conversation should be terminated, or None otherwise. Once a termination condition has been reached, it must be reset by calling reset() before it can be used again.

Some important things to note about termination conditions:

They are stateful but reset automatically after each run (run() or run_stream()) is finished.

They can be combined using the AND and OR operators.

For group chat teams (i.e., RoundRobinGroupChat, SelectorGroupChat, and Swarm), the termination condition is called after each agent responds. While a response may contain multiple inner messages, the team calls its termination condition just once for all the messages from a single response. So the condition is called with the “delta sequence” of messages since the last time it was called.

Built-In Termination Conditions:

MaxMessageTermination: Stops after a specified number of messages have been produced, including both agent and task messages.

TextMentionTermination: Stops when specific text or string is mentioned in a message (e.g., “TERMINATE”).

TokenUsageTermination: Stops when a certain number of prompt or completion tokens are used. This requires the agents to report token usage in their messages.

TimeoutTermination: Stops after a specified duration in seconds.

HandoffTermination: Stops when a handoff to a specific target is requested. Handoff messages can be used to build patterns such as Swarm. This is useful when you want to pause the run and allow application or user to provide input when an agent hands off to them.

SourceMatchTermination: Stops after a specific agent responds.

ExternalTermination: Enables programmatic control of termination from outside the run. This is useful for UI integration (e.g., “Stop” buttons in chat interfaces).

StopMessageTermination: Stops when a StopMessage is produced by an agent.

TextMessageTermination: Stops when a TextMessage is produced by an agent.

FunctionCallTermination: Stops when a ToolCallExecutionEvent containing a FunctionExecutionResult with a matching name is produced by an agent.

FunctionalTermination: Stop when a function expression is evaluated to True on the last delta sequence of messages. This is useful for quickly create custom termination conditions that are not covered by the built-in ones.

To demonstrate the characteristics of termination conditions, we’ll create a team consisting of two agents: a primary agent responsible for text generation and a critic agent that reviews and provides feedback on the generated text.

Let’s explore how termination conditions automatically reset after each run or run_stream call, allowing the team to resume its conversation from where it left off.

The conversation stopped after reaching the maximum message limit. Since the primary agent didn’t get to respond to the feedback, let’s continue the conversation.

The team continued from where it left off, allowing the primary agent to respond to the feedback.

Let’s show how termination conditions can be combined using the AND (&) and OR (|) operators to create more complex termination logic. For example, we’ll create a team that stops either after 10 messages are generated or when the critic agent approves a message.

The conversation stopped after the critic agent approved the message, although it could have also stopped if 10 messages were generated.

Alternatively, if we want to stop the run only when both conditions are met, we can use the AND (&) operator.

The built-in termination conditions are sufficient for most use cases. However, there may be cases where you need to implement a custom termination condition that doesn’t fit into the existing ones. You can do this by subclassing the TerminationCondition class.

In this example, we create a custom termination condition that stops the conversation when a specific function call is made.

Let’s use this new termination condition to stop the conversation when the critic agent approves a message using the approve function call.

First we create a simple function that will be called when the critic agent approves a message.

Then we create the agents. The critic agent is equipped with the approve tool.

Now, we create the termination condition and the team. We run the team with the poem-writing task.

You can see that the conversation stopped when the critic agent approved the message using the approve function call.

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    temperature=1,
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
)

# Create the primary agent.
primary_agent = AssistantAgent(
    "primary",
    model_client=model_client,
    system_message="You are a helpful AI assistant.",
)

# Create the critic agent.
critic_agent = AssistantAgent(
    "critic",
    model_client=model_client,
    system_message="Provide constructive feedback for every message. Respond with 'APPROVE' to when your feedbacks are addressed.",
)
```

Example 2 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient

model_client = OpenAIChatCompletionClient(
    model="gpt-4o",
    temperature=1,
    # api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
)

# Create the primary agent.
primary_agent = AssistantAgent(
    "primary",
    model_client=model_client,
    system_message="You are a helpful AI assistant.",
)

# Create the critic agent.
critic_agent = AssistantAgent(
    "critic",
    model_client=model_client,
    system_message="Provide constructive feedback for every message. Respond with 'APPROVE' to when your feedbacks are addressed.",
)
```

Example 3 (markdown):
```markdown
max_msg_termination = MaxMessageTermination(max_messages=3)
round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=max_msg_termination)

# Use asyncio.run(...) if you are running this script as a standalone script.
await Console(round_robin_team.run_stream(task="Write a unique, Haiku about the weather in Paris"))
```

Example 4 (markdown):
```markdown
max_msg_termination = MaxMessageTermination(max_messages=3)
round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=max_msg_termination)

# Use asyncio.run(...) if you are running this script as a standalone script.
await Console(round_robin_team.run_stream(task="Write a unique, Haiku about the weather in Paris"))
```

---

## Tracing and Observability — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tracing.html

**Contents:**
- Tracing and Observability#
- Setup#
- Telemetry Backend#
- Tracing an AgentChat Team#

AutoGen has built-in support for tracing and observability for collecting comprehensive records on the execution of your application. This feature is useful for debugging, performance analysis, and understanding the flow of your application.

This capability is powered by the OpenTelemetry library, which means you can use any OpenTelemetry-compatible backend to collect and analyze traces.

AutoGen follows the OpenTelemetry Semantic Conventions for tracing, for agents and tools. It also follows the Semantic Conventions for GenAI Systems currently under development.

To begin, you need to install the OpenTelemetry Python package. You can do this using pip:

Once you have the SDK installed, the simplest way to set up tracing in AutoGen is to:

Configure an OpenTelemetry tracer provider

Set up an exporter to send traces to your backend

Connect the tracer provider to the AutoGen runtime

To collect and view traces, you need to set up a telemetry backend. Several open-source options are available, including Jaeger, Zipkin. For this example, we will use Jaeger as our telemetry backend.

For a quick start, you can run Jaeger locally using Docker:

This command starts a Jaeger instance that listens on port 16686 for the Jaeger UI and port 4317 for the OpenTelemetry collector. You can access the Jaeger UI at http://localhost:16686.

In the following section, we will review how to enable tracing with an AutoGen GroupChat team. The AutoGen runtime already supports open telemetry (automatically logging message metadata). To begin, we will create a tracing service that will be used to instrument the AutoGen runtime.

All of the code to create a team should already be familiar to you.

AgentChat teams are run using the AutoGen Core’s agent runtime. In turn, the runtime is already instrumented to log, see Core Telemetry Guide. To disable the agent runtime telemetry, you can set the trace_provider to opentelemetry.trace.NoOpTracerProvider in the runtime constructor.

Additionally, you can set the environment variable AUTOGEN_DISABLE_RUNTIME_TRACING to true to disable the agent runtime telemetry if you don’t have access to the runtime constructor. For example, if you are using ComponentConfig.

You can then use the Jaeger UI to view the traces collected from the application run above.

Serializing Components

**Examples:**

Example 1 (unknown):
```unknown
pip install opentelemetry-sdk opentelemetry-exporter-otlp-proto-grpc opentelemetry-instrumentation-openai
```

Example 2 (unknown):
```unknown
pip install opentelemetry-sdk opentelemetry-exporter-otlp-proto-grpc opentelemetry-instrumentation-openai
```

Example 3 (json):
```json
docker run -d --name jaeger \
  -e COLLECTOR_OTLP_ENABLED=true \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
```

Example 4 (json):
```json
docker run -d --name jaeger \
  -e COLLECTOR_OTLP_ENABLED=true \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
```

---

## Travel Planning — AutoGen

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/examples/travel-planning.html

**Contents:**
- Travel Planning#
- Defining Agents#

In this example, we’ll walk through the process of creating a sophisticated travel planning system using AgentChat. Our travel planner will utilize multiple AI agents, each with a specific role, to collaboratively create a comprehensive travel itinerary.

First, let us import the necessary modules.

In the next section we will define the agents that will be used in the travel planning team.

**Examples:**

Example 1 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 2 (sql):
```sql
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

Example 3 (sql):
```sql
model_client = OpenAIChatCompletionClient(model="gpt-4o")

planner_agent = AssistantAgent(
    "planner_agent",
    model_client=model_client,
    description="A helpful assistant that can plan trips.",
    system_message="You are a helpful assistant that can suggest a travel plan for a user based on their request.",
)

local_agent = AssistantAgent(
    "local_agent",
    model_client=model_client,
    description="A local assistant that can suggest local activities or places to visit.",
    system_message="You are a helpful assistant that can suggest authentic and interesting local activities or places to visit for a user and can utilize any context information provided.",
)

language_agent = AssistantAgent(
    "language_agent",
    model_client=model_client,
    description="A helpful assistant that can provide language tips for a given destination.",
    system_message="You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale.",
)

travel_summary_agent = AssistantAgent(
    "travel_summary_agent",
    model_client=model_client,
    description="A helpful assistant that can summarize the travel plan.",
    system_message="You are a helpful assistant that can take in all of the suggestions and advice from the other agents and provide a detailed final travel plan. You must ensure that the final plan is integrated and complete. YOUR FINAL RESPONSE MUST BE THE COMPLETE PLAN. When the plan is complete and all perspectives are integrated, you can respond with TERMINATE.",
)
```

Example 4 (sql):
```sql
model_client = OpenAIChatCompletionClient(model="gpt-4o")

planner_agent = AssistantAgent(
    "planner_agent",
    model_client=model_client,
    description="A helpful assistant that can plan trips.",
    system_message="You are a helpful assistant that can suggest a travel plan for a user based on their request.",
)

local_agent = AssistantAgent(
    "local_agent",
    model_client=model_client,
    description="A local assistant that can suggest local activities or places to visit.",
    system_message="You are a helpful assistant that can suggest authentic and interesting local activities or places to visit for a user and can utilize any context information provided.",
)

language_agent = AssistantAgent(
    "language_agent",
    model_client=model_client,
    description="A helpful assistant that can provide language tips for a given destination.",
    system_message="You are a helpful assistant that can review travel plans, providing feedback on important/critical tips about how best to address language or communication challenges for the given destination. If the plan already includes language tips, you can mention that the plan is satisfactory, with rationale.",
)

travel_summary_agent = AssistantAgent(
    "travel_summary_agent",
    model_client=model_client,
    description="A helpful assistant that can summarize the travel plan.",
    system_message="You are a helpful assistant that can take in all of the suggestions and advice from the other agents and provide a detailed final travel plan. You must ensure that the final plan is integrated and complete. YOUR FINAL RESPONSE MUST BE THE COMPLETE PLAN. When the plan is complete and all perspectives are integrated, you can respond with TERMINATE.",
)
```

---

## 

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/selector-group-chat.html

---

## 

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/swarm.html

---

## 

**URL:** https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/custom-agents.html

---
