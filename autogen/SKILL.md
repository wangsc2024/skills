---
name: autogen
description: AutoGen is Microsoft's open-source framework for building AI agents and multi-agent applications. Use this skill when working with conversational agents, multi-agent orchestration, AgentChat, group chats, human-in-the-loop workflows, and AI agent collaboration in Python.
---

# Autogen Skill

Comprehensive assistance with autogen development, generated from official documentation.

## When to Use This Skill

This skill should be triggered when:
- Working with autogen
- Asking about autogen features or APIs
- Implementing autogen solutions
- Debugging autogen code
- Learning autogen best practices

## Quick Reference

### Common Patterns

**Pattern 1:** API Reference autogen_agentchat.agents autogen_agentchat.agents# This module initializes various pre-defined agents provided by the package. BaseChatAgent is the base class for all agents in AgentChat. class BaseChatAgent(name: str, description: str)[source]# Bases: ChatAgent, ABC, ComponentBase[BaseModel] Base class for a chat agent. This abstract class provides a base implementation for a ChatAgent. To create a new chat agent, subclass this class and implement the on_messages(), on_reset(), and produced_message_types. If streaming is required, also implement the on_messages_stream() method. An agent is considered stateful and maintains its state between calls to the on_messages() or on_messages_stream() methods. The agent should store its state in the agent instance. The agent should also implement the on_reset() method to reset the agent to its initialization state. Note The caller should only pass the new messages to the agent on each call to the on_messages() or on_messages_stream() method. Do not pass the entire conversation history to the agent on each call. This design principle must be followed when creating a new agent. component_type: ClassVar[ComponentType] = 'agent'# The logical type of the component. property name: str# The name of the agent. This is used by team to uniquely identify the agent. It should be unique within the team. property description: str# The description of the agent. This is used by team to make decisions about which agents to use. The description should describe the agent’s capabilities and how to interact with it. abstract property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. abstract async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → TaskResult[source]# Run the agent with the given task and return the result. async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]# Run the agent with the given task and return a stream of messages and the final task result as the last item in the stream. Parameters: task – The task to run. Can be a string, a single message, or a sequence of messages. cancellation_token – The cancellation token to kill the task immediately. output_task_messages – Whether to include task messages in the output stream. Defaults to True for backward compatibility. abstract async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. async on_pause(cancellation_token: CancellationToken) → None[source]# Called when the agent is paused while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom pause behavior. async on_resume(cancellation_token: CancellationToken) → None[source]# Called when the agent is resumed from a pause while running in its on_messages() or on_messages_stream() method. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom resume behavior. async save_state() → Mapping[str, Any][source]# Export state. Default implementation for stateless agents. async load_state(state: Mapping[str, Any]) → None[source]# Restore agent from saved state. Default implementation for stateless agents. async close() → None[source]# Release any resources held by the agent. This is a no-op by default in the BaseChatAgent class. Subclasses can override this method to implement custom close behavior. class AssistantAgent(name: str, model_client: ChatCompletionClient, *, tools: List[BaseTool[Any, Any] | Callable[[...], Any] | Callable[[...], Awaitable[Any]]] | None = None, workbench: Workbench | Sequence[Workbench] | None = None, handoffs: List[Handoff | str] | None = None, model_context: ChatCompletionContext | None = None, description: str = 'An agent that provides assistance with ability to use tools.', system_message: str | None = 'You are a helpful AI assistant. Solve tasks using your tools. Reply with TERMINATE when the task has been completed.', model_client_stream: bool = False, reflect_on_tool_use: bool | None = None, max_tool_iterations: int = 1, tool_call_summary_format: str = '{result}', tool_call_summary_formatter: Callable[[FunctionCall, FunctionExecutionResult], str] | None = None, output_content_type: type[BaseModel] | None = None, output_content_type_format: str | None = None, memory: Sequence[Memory] | None = None, metadata: Dict[str, str] | None = None)[source]# Bases: BaseChatAgent, Component[AssistantAgentConfig] An agent that provides assistance with tool use. The on_messages() returns a Response in which chat_message is the final response message. The on_messages_stream() creates an async generator that produces the inner messages as they are created, and the Response object as the last item before closing the generator. The BaseChatAgent.run() method returns a TaskResult containing the messages produced by the agent. In the list of messages, messages, the last message is the final response message. The BaseChatAgent.run_stream() method creates an async generator that produces the inner messages as they are created, and the TaskResult object as the last item before closing the generator. Attention The caller must only pass the new messages to the agent on each call to the on_messages(), on_messages_stream(), BaseChatAgent.run(), or BaseChatAgent.run_stream() methods. The agent maintains its state between calls to these methods. Do not pass the entire conversation history to the agent on each call. Warning The assistant agent is not thread-safe or coroutine-safe. It should not be shared between multiple tasks or coroutines, and it should not call its methods concurrently. The following diagram shows how the assistant agent works: Structured output: If the output_content_type is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response by default. Note Currently, setting output_content_type prevents the agent from being able to call load_component and dum_component methods for serializable configuration. This will be fixed soon in the future. Tool call behavior: If the model returns no tool call, then the response is immediately returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message. This ends the tool call iteration loop regardless of the max_tool_iterations setting. When the model returns tool calls, they will be executed right away: When reflect_on_tool_use is False, the tool call results are returned as a ToolCallSummaryMessage in chat_message. You can customise the summary with either a static format string (tool_call_summary_format) or a callable (tool_call_summary_formatter); the callable is evaluated once per tool call. When reflect_on_tool_use is True, the another model inference is made using the tool calls and results, and final response is returned as a TextMessage or a StructuredMessage (when using structured output) in chat_message. reflect_on_tool_use is set to True by default when output_content_type is set. reflect_on_tool_use is set to False by default when output_content_type is not set. If the model returns multiple tool calls, they will be executed concurrently. To disable parallel tool calls you need to configure the model client. For example, set parallel_tool_calls=False for OpenAIChatCompletionClient and AzureOpenAIChatCompletionClient. The max_tool_iterations parameter controls how many sequential tool call iterations the agent can perform in a single run. When set to 1 (default), the agent executes tool calls once and returns the result. When set higher, the agent can make additional model calls to execute more tool calls if the model continues to request them, enabling multi-step tool-based workflows. The agent stops when either the model returns a text response (instead of tool calls) or the maximum number of iterations is reached. Tip By default, the tool call results are returned as the response when tool calls are made, so pay close attention to how the tools’ return values are formatted—especially if another agent expects a specific schema. Use `tool_call_summary_format` for a simple static template. Use `tool_call_summary_formatter` for full programmatic control (e.g., “hide large success payloads, show full details on error”). Note: tool_call_summary_formatter is not serializable and will be ignored when an agent is loaded from, or exported to, YAML/JSON configuration files. Hand off behavior: If a handoff is triggered, a HandoffMessage will be returned in chat_message. If there are tool calls, they will also be executed right away before returning the handoff. The tool calls and results are passed to the target agent through context. Note If multiple handoffs are detected, only the first handoff is executed. To avoid this, disable parallel tool calls in the model client configuration. Limit context size sent to the model: You can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. Another option is to use a TokenLimitedChatCompletionContext which will limit the number of tokens sent to the model. You can also create your own model context by subclassing ChatCompletionContext. Streaming mode: The assistant agent can be used in streaming mode by setting model_client_stream=True. In this mode, the on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. The chunk messages will not be included in the final response’s inner messages. Parameters: name (str) – The name of the agent. model_client (ChatCompletionClient) – The model client to use for inference. tools (List[BaseTool[Any, Any] | Callable[..., Any] | Callable[..., Awaitable[Any]]] | None, optional) – The tools to register with the agent. workbench (Workbench | Sequence[Workbench] | None, optional) – The workbench or list of workbenches to use for the agent. Tools cannot be used when workbench is set and vice versa. handoffs (List[HandoffBase | str] | None, optional) – The handoff configurations for the agent, allowing it to transfer to other agents by responding with a HandoffMessage. The transfer is only executed when the team is in Swarm. If a handoff is a string, it should represent the target agent’s name. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset. description (str, optional) – The description of the agent. system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable. model_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False. reflect_on_tool_use (bool, optional) – If True, the agent will make another model inference using the tool call and result to generate a response. If False, the tool call result will be returned as the response. By default, if output_content_type is set, this will be True; if output_content_type is not set, this will be False. output_content_type (type[BaseModel] | None, optional) – The output content type for StructuredMessage response as a Pydantic model. This will be used with the model client to generate structured output. If this is set, the agent will respond with a StructuredMessage instead of a TextMessage in the final response, unless reflect_on_tool_use is False and a tool call is made. output_content_type_format (str | None, optional) – (Experimental) The format string used for the content of a StructuredMessage response. max_tool_iterations (int, optional) – The maximum number of tool iterations to perform until the model stops making tool calls. Defaults to 1, which means the agent will only execute the tool calls made by the model once, and return the result as a ToolCallSummaryMessage, or a TextMessage or a StructuredMessage (when using structured output) in chat_message as the final response. As soon as the model stops making tool calls, the agent will stop executing tool calls and return the result as the final response. The value must be greater than or equal to 1. tool_call_summary_format (str, optional) – Static format string applied to each tool call result when composing the ToolCallSummaryMessage. Defaults to "{result}". Ignored if tool_call_summary_formatter is provided. When reflect_on_tool_use is False, the summaries for all tool calls are concatenated with a newline (’n’) and returned as the response. Placeholders available in the template: {tool_name}, {arguments}, {result}, {is_error}. tool_call_summary_formatter (Callable[[FunctionCall, FunctionExecutionResult], str] | None, optional) – Callable that receives the FunctionCall and its FunctionExecutionResult and returns the summary string. Overrides tool_call_summary_format when supplied and allows conditional logic — for example, emitting static string like "Tool FooBar executed successfully." on success and a full payload (including all passed arguments etc.) only on failure. Limitation: The callable is not serializable; values provided via YAML/JSON configs are ignored. Note tool_call_summary_formatter is intended for in-code use only. It cannot currently be saved or restored via configuration files. memory (Sequence[Memory] | None, optional): The memory store to use for the agent. Defaults to None. metadata (Dict[str, str] | None, optional): Optional metadata for tracking. Raises: ValueError – If tool names are not unique. ValueError – If handoff names are not unique. ValueError – If handoff names are not unique from tool names. ValueError – If maximum number of tool iterations is less than 1. Examples Example 1: basic agent The following example demonstrates how to create an assistant agent with a model client and generate a response to a simple task. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model="gpt-4o", # api_key = "your_openai_api_key" ) agent = AssistantAgent(name="assistant", model_client=model_client) result = await agent.run(task="Name two cities in North America.") print(result) asyncio.run(main()) Example 2: model client token streaming This example demonstrates how to create an assistant agent with a model client and generate a token stream by setting model_client_stream=True. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model="gpt-4o", # api_key = "your_openai_api_key" ) agent = AssistantAgent( name="assistant", model_client=model_client, model_client_stream=True, ) stream = agent.run_stream(task="Name two cities in North America.") async for message in stream: print(message) asyncio.run(main()) source='user' models_usage=None metadata={} content='Name two cities in North America.' type='TextMessage' source='assistant' models_usage=None metadata={} content='Two' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' cities' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' in' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' North' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' America' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' are' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' New' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' York' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' City' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' and' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' Toronto' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content='.' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content=' TERMIN' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=None metadata={} content='ATE' type='ModelClientStreamingChunkEvent' source='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) metadata={} content='Two cities in North America are New York City and Toronto. TERMINATE' type='TextMessage' messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), metadata={}, content='Two cities in North America are New York City and Toronto. TERMINATE', type='TextMessage')] stop_reason=None Example 3: agent with tools The following example demonstrates how to create an assistant agent with a model client and a tool, generate a stream of messages for a task, and print the messages to the console using Console. The tool is a simple function that returns the current time. Under the hood, the function is wrapped in a FunctionTool and used with the agent’s model client. The doc string of the function is used as the tool description, the function name is used as the tool name, and the function signature including the type hints is used as the tool arguments. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console async def get_current_time() -> str: return "The current time is 12:00 PM." async def main() -> None: model_client = OpenAIChatCompletionClient( model="gpt-4o", # api_key = "your_openai_api_key" ) agent = AssistantAgent(name="assistant", model_client=model_client, tools=[get_current_time]) await Console(agent.run_stream(task="What is the current time?")) asyncio.run(main()) Example 4: agent with max_tool_iterations The following example demonstrates how to use the max_tool_iterations parameter to control how many times the agent can execute tool calls in a single run. This is useful when you want the agent to perform multiple sequential tool operations to reach a goal. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console # Global counter state counter = 0 def increment_counter() -> str: """Increment the counter by 1 and return the current value.""" global counter counter += 1 return f"Counter incremented to: {counter}" def get_counter() -> str: """Get the current counter value.""" global counter return f"Current counter value: {counter}" async def main() -> None: model_client = OpenAIChatCompletionClient( model="gpt-4o", # api_key = "your_openai_api_key" ) # Create agent with max_tool_iterations=5 to allow multiple tool calls agent = AssistantAgent( name="assistant", model_client=model_client, tools=[increment_counter, get_counter], max_tool_iterations=5, # Allow up to 5 tool call iterations reflect_on_tool_use=True, # Get a final summary after tool calls ) await Console(agent.run_stream(task="Increment the counter 3 times and then tell me the final value.")) asyncio.run(main()) Example 5: agent with Model-Context Protocol (MCP) workbench The following example demonstrates how to create an assistant agent with a model client and an McpWorkbench for interacting with a Model-Context Protocol (MCP) server. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_ext.tools.mcp import StdioServerParams, McpWorkbench async def main() -> None: params = StdioServerParams( command="uvx", args=["mcp-server-fetch"], read_timeout_seconds=60, ) # You can also use `start()` and `stop()` to manage the session. async with McpWorkbench(server_params=params) as workbench: model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano") assistant = AssistantAgent( name="Assistant", model_client=model_client, workbench=workbench, reflect_on_tool_use=True, ) await Console( assistant.run_stream(task="Go to https://github.com/microsoft/autogen and tell me what you see.") ) asyncio.run(main()) Example 6: agent with structured output and tool The following example demonstrates how to create an assistant agent with a model client configured to use structured output and a tool. Note that you need to use FunctionTool to create the tool and the strict=True is required for structured output mode. Because the model is configured to use structured output, the output reflection response will be a JSON formatted string. import asyncio from typing import Literal from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.ui import Console from autogen_core.tools import FunctionTool from autogen_ext.models.openai import OpenAIChatCompletionClient from pydantic import BaseModel # Define the structured output format. class AgentResponse(BaseModel): thoughts: str response: Literal["happy", "sad", "neutral"] # Define the function to be called as a tool. def sentiment_analysis(text: str) -> str: """Given a text, return the sentiment.""" return "happy" if "happy" in text else "sad" if "sad" in text else "neutral" # Create a FunctionTool instance with `strict=True`, # which is required for structured output mode. tool = FunctionTool(sentiment_analysis, description="Sentiment Analysis", strict=True) # Create an OpenAIChatCompletionClient instance that supports structured output. model_client = OpenAIChatCompletionClient( model="gpt-4o-mini", ) # Create an AssistantAgent instance that uses the tool and model client. agent = AssistantAgent( name="assistant", model_client=model_client, tools=[tool], system_message="Use the tool to analyze sentiment.", output_content_type=AgentResponse, ) async def main() -> None: stream = agent.run_stream(task="I am happy today!") await Console(stream) asyncio.run(main()) ---------- assistant ---------- [FunctionCall(id='call_tIZjAVyKEDuijbBwLY6RHV2p', arguments='{"text":"I am happy today!"}', name='sentiment_analysis')] ---------- assistant ---------- [FunctionExecutionResult(content='happy', call_id='call_tIZjAVyKEDuijbBwLY6RHV2p', is_error=False)] ---------- assistant ---------- {"thoughts":"The user expresses a clear positive emotion by stating they are happy today, suggesting an upbeat mood.","response":"happy"} Example 7: agent with bounded model context The following example shows how to use a BufferedChatCompletionContext that only keeps the last 2 messages (1 user + 1 assistant). Bounded model context is useful when the model has a limit on the number of tokens it can process. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_core.model_context import BufferedChatCompletionContext from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: # Create a model client. model_client = OpenAIChatCompletionClient( model="gpt-4o-mini", # api_key = "your_openai_api_key" ) # Create a model context that only keeps the last 2 messages (1 user + 1 assistant). model_context = BufferedChatCompletionContext(buffer_size=2) # Create an AssistantAgent instance with the model client and context. agent = AssistantAgent( name="assistant", model_client=model_client, model_context=model_context, system_message="You are a helpful assistant.", ) result = await agent.run(task="Name two cities in North America.") print(result.messages[-1].content) # type: ignore result = await agent.run(task="My favorite color is blue.") print(result.messages[-1].content) # type: ignore result = await agent.run(task="Did I ask you any question?") print(result.messages[-1].content) # type: ignore asyncio.run(main()) Two cities in North America are New York City and Toronto. That's great! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you like, or any particular reason why it's your favorite? No, you didn't ask a question. I apologize for any misunderstanding. If you have something specific you'd like to discuss or ask, feel free to let me know! Example 8: agent with memory The following example shows how to use a list-based memory with the assistant agent. The memory is preloaded with some initial content. Under the hood, the memory is used to update the model context before making an inference, using the update_context() method. import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_core.memory import ListMemory, MemoryContent from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: # Create a model client. model_client = OpenAIChatCompletionClient( model="gpt-4o-mini", # api_key = "your_openai_api_key" ) # Create a list-based memory with some initial content. memory = ListMemory() await memory.add(MemoryContent(content="User likes pizza.", mime_type="text/plain")) await memory.add(MemoryContent(content="User dislikes cheese.", mime_type="text/plain")) # Create an AssistantAgent instance with the model client and memory. agent = AssistantAgent( name="assistant", model_client=model_client, memory=[memory], system_message="You are a helpful assistant.", ) result = await agent.run(task="What is a good dinner idea?") print(result.messages[-1].content) # type: ignore asyncio.run(main()) How about making a delicious pizza without cheese? You can create a flavorful veggie pizza with a variety of toppings. Here's a quick idea: **Veggie Tomato Sauce Pizza** - Start with a pizza crust (store-bought or homemade). - Spread a layer of marinara or tomato sauce evenly over the crust. - Top with your favorite vegetables like bell peppers, mushrooms, onions, olives, and spinach. - Add some protein if you'd like, such as grilled chicken or pepperoni (ensure it's cheese-free). - Sprinkle with herbs like oregano and basil, and maybe a drizzle of olive oil. - Bake according to the crust instructions until the edges are golden and the veggies are cooked. Serve it with a side salad or some garlic bread to complete the meal! Enjoy your dinner! Example 9: agent with `o1-mini` The following example shows how to use o1-mini model with the assistant agent. import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent async def main() -> None: model_client = OpenAIChatCompletionClient( model="o1-mini", # api_key = "your_openai_api_key" ) # The system message is not supported by the o1 series model. agent = AssistantAgent(name="assistant", model_client=model_client, system_message=None) result = await agent.run(task="What is the capital of France?") print(result.messages[-1].content) # type: ignore asyncio.run(main()) Note The o1-preview and o1-mini models do not support system message and function calling. So the system_message should be set to None and the tools and handoffs should not be set. See o1 beta limitations for more details. Example 10: agent using reasoning model with custom model context. The following example shows how to use a reasoning model (DeepSeek R1) with the assistant agent. The model context is used to filter out the thought field from the assistant message. import asyncio from typing import List from autogen_agentchat.agents import AssistantAgent from autogen_core.model_context import UnboundedChatCompletionContext from autogen_core.models import AssistantMessage, LLMMessage, ModelFamily from autogen_ext.models.ollama import OllamaChatCompletionClient class ReasoningModelContext(UnboundedChatCompletionContext): """A model context for reasoning models.""" async def get_messages(self) -> List[LLMMessage]: messages = await super().get_messages() # Filter out thought field from AssistantMessage. messages_out: List[LLMMessage] = [] for message in messages: if isinstance(message, AssistantMessage): message.thought = None messages_out.append(message) return messages_out # Create an instance of the model client for DeepSeek R1 hosted locally on Ollama. model_client = OllamaChatCompletionClient( model="deepseek-r1:8b", model_info={ "vision": False, "function_calling": False, "json_output": False, "family": ModelFamily.R1, "structured_output": True, }, ) agent = AssistantAgent( "reasoning_agent", model_client=model_client, model_context=ReasoningModelContext(), # Use the custom model context. ) async def run_reasoning_agent() -> None: result = await agent.run(task="What is the capital of France?") print(result) asyncio.run(run_reasoning_agent()) For detailed examples and usage, see the Examples section below. component_version: ClassVar[int] = 2# The version of the component, if schema incompatibilities are introduced this should be updated. component_config_schema# alias of AssistantAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.AssistantAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# Get the types of messages this agent can produce. Returns: Sequence of message types this agent can generate property model_context: ChatCompletionContext# Get the model context used by this agent. Returns: The chat completion context for this agent async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Process incoming messages and generate a response. Parameters: messages – Sequence of messages to process cancellation_token – Token for cancelling operation Returns: Response containing the agent’s reply async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Process messages and stream the response. Parameters: messages – Sequence of messages to process cancellation_token – Token for cancelling operation Yields: Events, messages and final response during processing async on_reset(cancellation_token: CancellationToken) → None[source]# Reset the assistant agent to its initialization state. async save_state() → Mapping[str, Any][source]# Save the current state of the assistant agent. async load_state(state: Mapping[str, Any]) → None[source]# Load the state of the assistant agent class CodeExecutorAgent(name: str, code_executor: CodeExecutor, *, model_client: ChatCompletionClient | None = None, model_context: ChatCompletionContext | None = None, model_client_stream: bool = False, max_retries_on_error: int = 0, description: str | None = None, system_message: str | None = DEFAULT_SYSTEM_MESSAGE, sources: Sequence[str] | None = None, supported_languages: List[str] | None = None, approval_func: Callable[[ApprovalRequest], ApprovalResponse] | Callable[[ApprovalRequest], Awaitable[ApprovalResponse]] | None = None)[source]# Bases: BaseChatAgent, Component[CodeExecutorAgentConfig] (Experimental) An agent that generates and executes code snippets based on user instructions. Note This agent is experimental and may change in future releases. It is typically used within a team with another agent that generates code snippets to be executed or alone with model_client provided so that it can generate code based on user query, execute it and reflect on the code result. When used with model_client, it will generate code snippets using the model and execute them using the provided code_executor. The model will also reflect on the code execution results. The agent will yield the final reflection result from the model as the final response. When used without model_client, it will only execute code blocks found in TextMessage messages and returns the output of the code execution. Note Using AssistantAgent with PythonCodeExecutionTool is an alternative to this agent. However, the model for that agent will have to generate properly escaped code string as a parameter to the tool. Parameters: name (str) – The name of the agent. code_executor (CodeExecutor) – The code executor responsible for executing code received in messages (DockerCommandLineCodeExecutor recommended. See example below) model_client (ChatCompletionClient, optional) – The model client to use for inference and generating code. If not provided, the agent will only execute code blocks found in input messages. Currently, the model must support structured output mode, which is required for the automatic retry mechanism to work. model_client_stream (bool, optional) – If True, the model client will be used in streaming mode. on_messages_stream() and BaseChatAgent.run_stream() methods will also yield ModelClientStreamingChunkEvent messages as the model client produces chunks of response. Defaults to False. description (str, optional) – The description of the agent. If not provided, DEFAULT_AGENT_DESCRIPTION will be used. system_message (str, optional) – The system message for the model. If provided, it will be prepended to the messages in the model context when making an inference. Set to None to disable. Defaults to DEFAULT_SYSTEM_MESSAGE. This is only used if model_client is provided. sources (Sequence[str], optional) – Check only messages from the specified agents for the code to execute. This is useful when the agent is part of a group chat and you want to limit the code execution to messages from specific agents. If not provided, all messages will be checked for code blocks. This is only used if model_client is not provided. max_retries_on_error (int, optional) – The maximum number of retries on error. If the code execution fails, the agent will retry up to this number of times. If the code execution fails after this number of retries, the agent will yield a reflection result. supported_languages (List[str], optional) – List of programming languages that will be parsed and executed from agent response; others will be ignored. Defaults to DEFAULT_SUPPORTED_LANGUAGES. approval_func (Optional[Union[Callable[[ApprovalRequest], ApprovalResponse], Callable[[ApprovalRequest], Awaitable[ApprovalResponse]]]], optional) – A function that is called before each code execution to get approval. The function takes an ApprovalRequest containing the code to be executed and the current context, and returns an ApprovalResponse. The function can be either synchronous or asynchronous. If None (default), all code executions are automatically approved. If set, the agent cannot be serialized using dump_component(). Note It is recommended that the CodeExecutorAgent agent uses a Docker container to execute code. This ensures that model-generated code is executed in an isolated environment. To use Docker, your environment must have Docker installed and running. Follow the installation instructions for Docker. Note The code executor only processes code that is properly formatted in markdown code blocks using triple backticks. For example: ```python print("Hello World") ``` # or ```sh echo "Hello World" ``` In this example, we show how to set up a CodeExecutorAgent agent that uses the DockerCommandLineCodeExecutor to execute code snippets in a Docker container. The work_dir parameter indicates where all executed files are first saved locally before being executed in the Docker container. import asyncio from autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.messages import TextMessage from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_core import CancellationToken def simple_approval_func(request: ApprovalRequest) -> ApprovalResponse: """Simple approval function that requests user input for code execution approval.""" print("Code execution approval requested:") print("=" * 50) print(request.code) print("=" * 50) while True: user_input = input("Do you want to execute this code? (y/n): ").strip().lower() if user_input in ['y', 'yes']: return ApprovalResponse(approved=True, reason='Approved by user') elif user_input in ['n', 'no']: return ApprovalResponse(approved=False, reason='Denied by user') else: print("Please enter 'y' for yes or 'n' for no.") async def run_code_executor_agent() -> None: # Create a code executor agent that uses a Docker container to execute code. code_executor = DockerCommandLineCodeExecutor(work_dir="coding") await code_executor.start() code_executor_agent = CodeExecutorAgent( "code_executor", code_executor=code_executor, approval_func=simple_approval_func ) # Run the agent with a given code snippet. task = TextMessage( content='''Here is some code ```python print('Hello world') ``` ''', source="user", ) response = await code_executor_agent.on_messages([task], CancellationToken()) print(response.chat_message) # Stop the code executor. await code_executor.stop() asyncio.run(run_code_executor_agent()) In this example, we show how to set up a CodeExecutorAgent agent that uses the DeviceRequest to expose a GPU to the container for cuda-accelerated code execution. import asyncio from autogen_agentchat.agents import CodeExecutorAgent from autogen_agentchat.messages import TextMessage from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_core import CancellationToken from docker.types import DeviceRequest async def run_code_executor_agent() -> None: # Create a code executor agent that uses a Docker container to execute code. code_executor = DockerCommandLineCodeExecutor( work_dir="coding", device_requests=[DeviceRequest(count=-1, capabilities=[["gpu"]])] ) await code_executor.start() code_executor_agent = CodeExecutorAgent("code_executor", code_executor=code_executor) # Display the GPU information task = TextMessage( content='''Here is some code ```sh nvidia-smi ``` ''', source="user", ) response = await code_executor_agent.on_messages([task], CancellationToken()) print(response.chat_message) # Stop the code executor. await code_executor.stop() asyncio.run(run_code_executor_agent()) In the following example, we show how to setup CodeExecutorAgent without model_client parameter for executing code blocks generated by other agents in a group chat using DockerCommandLineCodeExecutor import asyncio from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console termination_condition = MaxMessageTermination(3) def group_chat_approval_func(request: ApprovalRequest) -> ApprovalResponse: """Approval function for group chat that allows basic Python operations.""" # Allow common safe operations safe_operations = ["print(", "import ", "def ", "class ", "if ", "for ", "while "] if any(op in request.code for op in safe_operations): return ApprovalResponse(approved=True, reason='Safe Python operation') # Deny file system operations in group chat dangerous_operations = ["open(", "file(", "os.", "subprocess", "eval(", "exec("] if any(op in request.code for op in dangerous_operations): return ApprovalResponse(approved=False, reason='File system or dangerous operation not allowed') return ApprovalResponse(approved=True, reason='Operation approved') async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") # define the Docker CLI Code Executor code_executor = DockerCommandLineCodeExecutor(work_dir="coding") # start the execution container await code_executor.start() code_executor_agent = CodeExecutorAgent( "code_executor_agent", code_executor=code_executor, approval_func=group_chat_approval_func ) coder_agent = AssistantAgent("coder_agent", model_client=model_client) groupchat = RoundRobinGroupChat( participants=[coder_agent, code_executor_agent], termination_condition=termination_condition ) task = "Write python code to print Hello World!" await Console(groupchat.run_stream(task=task)) # stop the execution container await code_executor.stop() asyncio.run(main()) ---------- user ---------- Write python code to print Hello World! ---------- coder_agent ---------- Certainly! Here's a simple Python code to print "Hello World!": ```python print("Hello World!") ``` You can run this code in any Python environment to display the message. ---------- code_executor_agent ---------- Hello World! In the following example, we show how to setup CodeExecutorAgent with model_client that can generate its own code without the help of any other agent and executing it in DockerCommandLineCodeExecutor. It also demonstrates using a model-based approval function that reviews the code for safety before execution. import asyncio from autogen_ext.code_executors.docker import DockerCommandLineCodeExecutor from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_core.models import SystemMessage, UserMessage from autogen_agentchat.agents import CodeExecutorAgent, ApprovalRequest, ApprovalResponse from autogen_agentchat.conditions import TextMessageTermination from autogen_agentchat.ui import Console termination_condition = TextMessageTermination("code_executor_agent") async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") async def model_client_approval_func(request: ApprovalRequest) -> ApprovalResponse: instruction = "Approve or reject the code in the last message based on whether it is dangerous or not. Use the following JSON format for your response: {approved: true/false, reason: 'your reason here'}" response = await model_client.create( messages=[SystemMessage(content=instruction)] + request.context + [UserMessage(content=request.code, source="user")], json_output=ApprovalResponse, ) assert isinstance(response.content, str) return ApprovalResponse.model_validate_json(response.content) # define the Docker CLI Code Executor code_executor = DockerCommandLineCodeExecutor(work_dir="coding") # start the execution container await code_executor.start() code_executor_agent = CodeExecutorAgent( "code_executor_agent", code_executor=code_executor, model_client=model_client, approval_func=model_client_approval_func, ) task = "Write python code to print Hello World!" await Console(code_executor_agent.run_stream(task=task)) # stop the execution container await code_executor.stop() asyncio.run(main()) ---------- user ---------- Write python code to print Hello World! ---------- code_executor_agent ---------- Certainly! Here is a simple Python code to print "Hello World!" to the console: ```python print("Hello World!") ``` Let's execute it to confirm the output. ---------- code_executor_agent ---------- Hello World! ---------- code_executor_agent ---------- The code has been executed successfully, and it printed "Hello World!" as expected. If you have any more requests or questions, feel free to ask! DEFAULT_TERMINAL_DESCRIPTION = 'A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).'# DEFAULT_AGENT_DESCRIPTION = 'A Code Execution Agent that generates and executes Python and shell scripts based on user instructions. It ensures correctness, efficiency, and minimal errors while gracefully handling edge cases.'# DEFAULT_SYSTEM_MESSAGE = 'You are a Code Execution Agent. Your role is to generate and execute Python code and shell scripts based on user instructions, ensuring correctness, efficiency, and minimal errors. Handle edge cases gracefully. Python code should be provided in ```python code blocks, and sh shell scripts should be provided in ```sh code blocks for execution.'# NO_CODE_BLOCKS_FOUND_MESSAGE = 'No code blocks found in the thread. Please provide at least one markdown-encoded code block to execute (i.e., quoting code in ```python or ```sh code blocks).'# DEFAULT_SUPPORTED_LANGUAGES = ['python', 'sh']# component_config_schema# alias of CodeExecutorAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.CodeExecutorAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the code executor agent produces. property model_context: ChatCompletionContext# The model context in use by the agent. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Process the incoming messages with the assistant agent and yield events/responses as they happen. async extract_code_blocks_from_messages(messages: Sequence[BaseChatMessage]) → List[CodeBlock][source]# async execute_code_block(code_blocks: List[CodeBlock], cancellation_token: CancellationToken) → CodeResult[source]# async on_reset(cancellation_token: CancellationToken) → None[source]# Its a no-op as the code executor agent has no mutable state. _to_config() → CodeExecutorAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: CodeExecutorAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class SocietyOfMindAgent(name: str, team: Team, model_client: ChatCompletionClient, *, description: str = DEFAULT_DESCRIPTION, instruction: str = DEFAULT_INSTRUCTION, response_prompt: str = DEFAULT_RESPONSE_PROMPT, model_context: ChatCompletionContext | None = None)[source]# Bases: BaseChatAgent, Component[SocietyOfMindAgentConfig] An agent that uses an inner team of agents to generate responses. Each time the agent’s on_messages() or on_messages_stream() method is called, it runs the inner team of agents and then uses the model client to generate a response based on the inner team’s messages. Once the response is generated, the agent resets the inner team by calling Team.reset(). Limit context size sent to the model: You can limit the number of messages sent to the model by setting the model_context parameter to a BufferedChatCompletionContext. This will limit the number of recent messages sent to the model and can be useful when the model has a limit on the number of tokens it can process. You can also create your own model context by subclassing ChatCompletionContext. Parameters: name (str) – The name of the agent. team (Team) – The team of agents to use. model_client (ChatCompletionClient) – The model client to use for preparing responses. description (str, optional) – The description of the agent. instruction (str, optional) – The instruction to use when generating a response using the inner team’s messages. Defaults to DEFAULT_INSTRUCTION. It assumes the role of ‘system’. response_prompt (str, optional) – The response prompt to use when generating a response using the inner team’s messages. Defaults to DEFAULT_RESPONSE_PROMPT. It assumes the role of ‘system’. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. The initial messages will be cleared when the agent is reset. Example: import asyncio from autogen_agentchat.ui import Console from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent1 = AssistantAgent("assistant1", model_client=model_client, system_message="You are a writer, write well.") agent2 = AssistantAgent( "assistant2", model_client=model_client, system_message="You are an editor, provide critical feedback. Respond with 'APPROVE' if the text addresses all feedbacks.", ) inner_termination = TextMentionTermination("APPROVE") inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination) society_of_mind_agent = SocietyOfMindAgent("society_of_mind", team=inner_team, model_client=model_client) agent3 = AssistantAgent( "assistant3", model_client=model_client, system_message="Translate the text to Spanish." ) team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2) stream = team.run_stream(task="Write a short story with a surprising ending.") await Console(stream) asyncio.run(main()) component_config_schema# alias of SocietyOfMindAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.SocietyOfMindAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_INSTRUCTION = 'Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation:'# The default instruction to use when generating a response using the inner team’s messages. The instruction will be prepended to the inner team’s messages when generating a response using the model. It assumes the role of ‘system’. Type: str DEFAULT_RESPONSE_PROMPT = 'Output a standalone response to the original request, without mentioning any of the intermediate discussion.'# The default response prompt to use when generating a response using the inner team’s messages. It assumes the role of ‘system’. Type: str DEFAULT_DESCRIPTION = 'An agent that uses an inner team of agents to generate responses.'# The default description for a SocietyOfMindAgent. Type: str property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. property model_context: ChatCompletionContext# The model context in use by the agent. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. async save_state() → Mapping[str, Any][source]# Export state. Default implementation for stateless agents. async load_state(state: Mapping[str, Any]) → None[source]# Restore agent from saved state. Default implementation for stateless agents. _to_config() → SocietyOfMindAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SocietyOfMindAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class UserProxyAgent(name: str, *, description: str = 'A human user', input_func: Callable[[str], str] | Callable[[str, CancellationToken | None], Awaitable[str]] | None = None)[source]# Bases: BaseChatAgent, Component[UserProxyAgentConfig] An agent that can represent a human user through an input function. This agent can be used to represent a human user in a chat system by providing a custom input function. Note Using UserProxyAgent puts a running team in a temporary blocked state until the user responds. So it is important to time out the user input function and cancel using the CancellationToken if the user does not respond. The input function should also handle exceptions and return a default response if needed. For typical use cases that involve slow human responses, it is recommended to use termination conditions such as HandoffTermination or SourceMatchTermination to stop the running team and return the control to the application. You can run the team again with the user input. This way, the state of the team can be saved and restored when the user responds. See Human-in-the-loop for more information. Parameters: name (str) – The name of the agent. description (str, optional) – A description of the agent. input_func (Optional[Callable[[str], str]], Callable[[str, Optional[CancellationToken]], Awaitable[str]]) – A function that takes a prompt and returns a user input string. For examples of integrating with web and UI frameworks, see the following: FastAPI ChainLit Example Simple usage case: import asyncio from autogen_core import CancellationToken from autogen_agentchat.agents import UserProxyAgent from autogen_agentchat.messages import TextMessage async def simple_user_agent(): agent = UserProxyAgent("user_proxy") response = await asyncio.create_task( agent.on_messages( [TextMessage(content="What is your name? ", source="user")], cancellation_token=CancellationToken(), ) ) assert isinstance(response.chat_message, TextMessage) print(f"Your name is {response.chat_message.content}") Example Cancellable usage case: import asyncio from typing import Any from autogen_core import CancellationToken from autogen_agentchat.agents import UserProxyAgent from autogen_agentchat.messages import TextMessage token = CancellationToken() agent = UserProxyAgent("user_proxy") async def timeout(delay: float): await asyncio.sleep(delay) def cancellation_callback(task: asyncio.Task[Any]): token.cancel() async def cancellable_user_agent(): try: timeout_task = asyncio.create_task(timeout(3)) timeout_task.add_done_callback(cancellation_callback) agent_task = asyncio.create_task( agent.on_messages( [TextMessage(content="What is your name? ", source="user")], cancellation_token=token, ) ) response = await agent_task assert isinstance(response.chat_message, TextMessage) print(f"Your name is {response.chat_message.content}") except Exception as e: print(f"Exception: {e}") except BaseException as e: print(f"BaseException: {e}") component_type: ClassVar[ComponentType] = 'agent'# The logical type of the component. component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.UserProxyAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. component_config_schema# alias of UserProxyAgentConfig class InputRequestContext[source]# Bases: object classmethod request_id() → str[source]# property produced_message_types: Sequence[type[BaseChatMessage]]# Message types this agent can produce. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handle incoming messages by requesting user input. async on_reset(cancellation_token: CancellationToken | None = None) → None[source]# Reset agent state. _to_config() → UserProxyAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: UserProxyAgentConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class MessageFilterAgent(name: str, wrapped_agent: BaseChatAgent, filter: MessageFilterConfig)[source]# Bases: BaseChatAgent, Component[MessageFilterAgentConfig] A wrapper agent that filters incoming messages before passing them to the inner agent. Warning This is an experimental feature, and the API will change in the future releases. This is useful in scenarios like multi-agent workflows where an agent should only process a subset of the full message history—for example, only the last message from each upstream agent, or only the first message from a specific source. Filtering is configured using MessageFilterConfig, which supports: - Filtering by message source (e.g., only messages from “user” or another agent) - Selecting the first N or last N messages from each source - If position is None, all messages from that source are included This agent is compatible with both direct message passing and team-based execution such as GraphFlow. Example >>> agent_a = MessageFilterAgent( ... name="A", ... wrapped_agent=some_other_agent, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source="user", position="first", count=1), ... PerSourceFilter(source="B", position="last", count=2), ... ] ... ), ... ) Example use case with Graph:Suppose you have a looping multi-agent graph: A → B → A → B → C. You want: - A to only see the user message and the last message from B - B to see the user message, last message from A, and its own prior responses (for reflection) - C to see the user message and the last message from B Wrap the agents like so: >>> agent_a = MessageFilterAgent( ... name="A", ... wrapped_agent=agent_a_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source="user", position="first", count=1), ... PerSourceFilter(source="B", position="last", count=1), ... ] ... ), ... ) >>> agent_b = MessageFilterAgent( ... name="B", ... wrapped_agent=agent_b_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source="user", position="first", count=1), ... PerSourceFilter(source="A", position="last", count=1), ... PerSourceFilter(source="B", position="last", count=10), ... ] ... ), ... ) >>> agent_c = MessageFilterAgent( ... name="C", ... wrapped_agent=agent_c_inner, ... filter=MessageFilterConfig( ... per_source=[ ... PerSourceFilter(source="user", position="first", count=1), ... PerSourceFilter(source="B", position="last", count=1), ... ] ... ), ... ) Then define the graph: >>> graph = DiGraph( ... nodes={ ... "A": DiGraphNode(name="A", edges=[DiGraphEdge(target="B")]), ... "B": DiGraphNode( ... name="B", ... edges=[ ... DiGraphEdge(target="C", condition="exit"), ... DiGraphEdge(target="A", condition="loop"), ... ], ... ), ... "C": DiGraphNode(name="C", edges=[]), ... }, ... default_start_node="A", ... ) This will ensure each agent sees only what is needed for its decision or action logic. component_config_schema# alias of MessageFilterAgentConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.agents.MessageFilterAgent'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property produced_message_types: Sequence[type[BaseChatMessage]]# The types of messages that the agent produces in the Response.chat_message field. They must be BaseChatMessage types. async on_messages(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → Response[source]# Handles incoming messages and returns a response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_messages_stream(messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None][source]# Handles incoming messages and returns a stream of messages and and the final item is the response. The base implementation in BaseChatAgent simply calls on_messages() and yields the messages in the response. Note Agents are stateful and the messages passed to this method should be the new messages since the last call to this method. The agent should maintain its state between calls to this method. For example, if the agent needs to remember the previous messages to respond to the current message, it should store the previous messages in the agent state. async on_reset(cancellation_token: CancellationToken) → None[source]# Resets the agent to its initialization state. _to_config() → MessageFilterAgentConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: MessageFilterAgentConfig) → MessageFilterAgent[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. pydantic model MessageFilterConfig[source]# Bases: BaseModel Show JSON schema{ "title": "MessageFilterConfig", "type": "object", "properties": { "per_source": { "items": { "$ref": "#/$defs/PerSourceFilter" }, "title": "Per Source", "type": "array" } }, "$defs": { "PerSourceFilter": { "properties": { "source": { "title": "Source", "type": "string" }, "position": { "anyOf": [ { "enum": [ "first", "last" ], "type": "string" }, { "type": "null" } ], "default": null, "title": "Position" }, "count": { "anyOf": [ { "type": "integer" }, { "type": "null" } ], "default": null, "title": "Count" } }, "required": [ "source" ], "title": "PerSourceFilter", "type": "object" } }, "required": [ "per_source" ] } Fields: per_source (List[autogen_agentchat.agents._message_filter_agent.PerSourceFilter]) field per_source: List[PerSourceFilter] [Required]# pydantic model PerSourceFilter[source]# Bases: BaseModel Show JSON schema{ "title": "PerSourceFilter", "type": "object", "properties": { "source": { "title": "Source", "type": "string" }, "position": { "anyOf": [ { "enum": [ "first", "last" ], "type": "string" }, { "type": "null" } ], "default": null, "title": "Position" }, "count": { "anyOf": [ { "type": "integer" }, { "type": "null" } ], "default": null, "title": "Count" } }, "required": [ "source" ] } Fields: count (int | None) position (Literal['first', 'last'] | None) source (str) field source: str [Required]# field position: Literal['first', 'last'] | None = None# field count: int | None = None# pydantic model ApprovalRequest[source]# Bases: BaseModel Request for approval of code execution. Show JSON schema{ "title": "ApprovalRequest", "description": "Request for approval of code execution.", "type": "object", "properties": { "code": { "title": "Code", "type": "string" }, "context": { "items": { "discriminator": { "mapping": { "AssistantMessage": "#/$defs/AssistantMessage", "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage", "SystemMessage": "#/$defs/SystemMessage", "UserMessage": "#/$defs/UserMessage" }, "propertyName": "type" }, "oneOf": [ { "$ref": "#/$defs/SystemMessage" }, { "$ref": "#/$defs/UserMessage" }, { "$ref": "#/$defs/AssistantMessage" }, { "$ref": "#/$defs/FunctionExecutionResultMessage" } ] }, "title": "Context", "type": "array" } }, "$defs": { "AssistantMessage": { "description": "Assistant message are sampled from the language model.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "$ref": "#/$defs/FunctionCall" }, "type": "array" } ], "title": "Content" }, "thought": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Thought" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "AssistantMessage", "default": "AssistantMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ], "title": "AssistantMessage", "type": "object" }, "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" }, "FunctionExecutionResultMessage": { "description": "Function execution result message contains the output of multiple function calls.", "properties": { "content": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Content", "type": "array" }, "type": { "const": "FunctionExecutionResultMessage", "default": "FunctionExecutionResultMessage", "title": "Type", "type": "string" } }, "required": [ "content" ], "title": "FunctionExecutionResultMessage", "type": "object" }, "SystemMessage": { "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n Open AI is moving away from using 'system' role in favor of 'developer' role.\n See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n on the server side.\n So, you can use `SystemMessage` for developer messages.", "properties": { "content": { "title": "Content", "type": "string" }, "type": { "const": "SystemMessage", "default": "SystemMessage", "title": "Type", "type": "string" } }, "required": [ "content" ], "title": "SystemMessage", "type": "object" }, "UserMessage": { "description": "User message contains input from end users, or a catch-all for data provided to the model.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "anyOf": [ { "type": "string" }, {} ] }, "type": "array" } ], "title": "Content" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "UserMessage", "default": "UserMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ], "title": "UserMessage", "type": "object" } }, "required": [ "code", "context" ] } Fields: code (str) context (List[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage]) field code: str [Required]# field context: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] [Required]# pydantic model ApprovalResponse[source]# Bases: BaseModel Response to approval request. Show JSON schema{ "title": "ApprovalResponse", "description": "Response to approval request.", "type": "object", "properties": { "approved": { "title": "Approved", "type": "boolean" }, "reason": { "title": "Reason", "type": "string" } }, "required": [ "approved", "reason" ] } Fields: approved (bool) reason (str) field approved: bool [Required]# field reason: str [Required]# previous autogen_agentchat next autogen_agentchat.base On this page BaseChatAgent BaseChatAgent.component_type BaseChatAgent.name BaseChatAgent.description BaseChatAgent.produced_message_types BaseChatAgent.on_messages() BaseChatAgent.on_messages_stream() BaseChatAgent.run() BaseChatAgent.run_stream() BaseChatAgent.on_reset() BaseChatAgent.on_pause() BaseChatAgent.on_resume() BaseChatAgent.save_state() BaseChatAgent.load_state() BaseChatAgent.close() AssistantAgent AssistantAgent.component_version AssistantAgent.component_config_schema AssistantAgent.component_provider_override AssistantAgent.produced_message_types AssistantAgent.model_context AssistantAgent.on_messages() AssistantAgent.on_messages_stream() AssistantAgent.on_reset() AssistantAgent.save_state() AssistantAgent.load_state() CodeExecutorAgent CodeExecutorAgent.DEFAULT_TERMINAL_DESCRIPTION CodeExecutorAgent.DEFAULT_AGENT_DESCRIPTION CodeExecutorAgent.DEFAULT_SYSTEM_MESSAGE CodeExecutorAgent.NO_CODE_BLOCKS_FOUND_MESSAGE CodeExecutorAgent.DEFAULT_SUPPORTED_LANGUAGES CodeExecutorAgent.component_config_schema CodeExecutorAgent.component_provider_override CodeExecutorAgent.produced_message_types CodeExecutorAgent.model_context CodeExecutorAgent.on_messages() CodeExecutorAgent.on_messages_stream() CodeExecutorAgent.extract_code_blocks_from_messages() CodeExecutorAgent.execute_code_block() CodeExecutorAgent.on_reset() CodeExecutorAgent._to_config() CodeExecutorAgent._from_config() SocietyOfMindAgent SocietyOfMindAgent.component_config_schema SocietyOfMindAgent.component_provider_override SocietyOfMindAgent.DEFAULT_INSTRUCTION SocietyOfMindAgent.DEFAULT_RESPONSE_PROMPT SocietyOfMindAgent.DEFAULT_DESCRIPTION SocietyOfMindAgent.produced_message_types SocietyOfMindAgent.model_context SocietyOfMindAgent.on_messages() SocietyOfMindAgent.on_messages_stream() SocietyOfMindAgent.on_reset() SocietyOfMindAgent.save_state() SocietyOfMindAgent.load_state() SocietyOfMindAgent._to_config() SocietyOfMindAgent._from_config() UserProxyAgent UserProxyAgent.component_type UserProxyAgent.component_provider_override UserProxyAgent.component_config_schema UserProxyAgent.InputRequestContext UserProxyAgent.InputRequestContext.request_id() UserProxyAgent.produced_message_types UserProxyAgent.on_messages() UserProxyAgent.on_messages_stream() UserProxyAgent.on_reset() UserProxyAgent._to_config() UserProxyAgent._from_config() MessageFilterAgent MessageFilterAgent.component_config_schema MessageFilterAgent.component_provider_override MessageFilterAgent.produced_message_types MessageFilterAgent.on_messages() MessageFilterAgent.on_messages_stream() MessageFilterAgent.on_reset() MessageFilterAgent._to_config() MessageFilterAgent._from_config() MessageFilterConfig MessageFilterConfig.per_source PerSourceFilter PerSourceFilter.source PerSourceFilter.position PerSourceFilter.count ApprovalRequest ApprovalRequest.code ApprovalRequest.context ApprovalResponse ApprovalResponse.approved ApprovalResponse.reason Edit on GitHub Show Source

```
ChatAgent
```

**Pattern 2:** Note The code executor only processes code that is properly formatted in markdown code blocks using triple backticks. For example: ```python print("Hello World") ``` # or ```sh echo "Hello World" ```

```
```python
print("Hello World")
```

# or

```sh
echo "Hello World"
```
```

**Pattern 3:** API Reference autogen_agentchat.conditions autogen_agentchat.conditions# This module provides various termination conditions for controlling the behavior of multi-agent teams. class MaxMessageTermination(max_messages: int, include_agent_event: bool = False)[source]# Bases: TerminationCondition, Component[MaxMessageTerminationConfig] Terminate the conversation after a maximum number of messages have been exchanged. Parameters: max_messages – The maximum number of messages allowed in the conversation. include_agent_event – If True, include BaseAgentEvent in the message count. Otherwise, only include BaseChatMessage. Defaults to False. component_config_schema# alias of MaxMessageTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.MaxMessageTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → MaxMessageTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: MaxMessageTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class TextMentionTermination(text: str, sources: Sequence[str] | None = None)[source]# Bases: TerminationCondition, Component[TextMentionTerminationConfig] Terminate the conversation if a specific text is mentioned. Parameters: text – The text to look for in the messages. sources – Check only messages of the specified agents for the text to look for. component_config_schema# alias of TextMentionTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMentionTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → TextMentionTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: TextMentionTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class StopMessageTermination[source]# Bases: TerminationCondition, Component[StopMessageTerminationConfig] Terminate the conversation if a StopMessage is received. component_config_schema# alias of StopMessageTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.StopMessageTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → StopMessageTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: StopMessageTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class TokenUsageTermination(max_total_token: int | None = None, max_prompt_token: int | None = None, max_completion_token: int | None = None)[source]# Bases: TerminationCondition, Component[TokenUsageTerminationConfig] Terminate the conversation if a token usage limit is reached. Parameters: max_total_token – The maximum total number of tokens allowed in the conversation. max_prompt_token – The maximum number of prompt tokens allowed in the conversation. max_completion_token – The maximum number of completion tokens allowed in the conversation. Raises: ValueError – If none of max_total_token, max_prompt_token, or max_completion_token is provided. component_config_schema# alias of TokenUsageTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TokenUsageTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → TokenUsageTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: TokenUsageTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class HandoffTermination(target: str)[source]# Bases: TerminationCondition, Component[HandoffTerminationConfig] Terminate the conversation if a HandoffMessage with the given target is received. Parameters: target (str) – The target of the handoff message. component_config_schema# alias of HandoffTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.HandoffTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → HandoffTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: HandoffTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class TimeoutTermination(timeout_seconds: float)[source]# Bases: TerminationCondition, Component[TimeoutTerminationConfig] Terminate the conversation after a specified duration has passed. Parameters: timeout_seconds – The maximum duration in seconds before terminating the conversation. component_config_schema# alias of TimeoutTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TimeoutTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → TimeoutTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: TimeoutTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class ExternalTermination[source]# Bases: TerminationCondition, Component[ExternalTerminationConfig] A termination condition that is externally controlled by calling the set() method. Example: from autogen_agentchat.conditions import ExternalTermination termination = ExternalTermination() # Run the team in an asyncio task. ... # Set the termination condition externally termination.set() component_config_schema# alias of ExternalTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.ExternalTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached set() → None[source]# Set the termination condition to terminated. async reset() → None[source]# Reset the termination condition. _to_config() → ExternalTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: ExternalTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class SourceMatchTermination(sources: List[str])[source]# Bases: TerminationCondition, Component[SourceMatchTerminationConfig] Terminate the conversation after a specific source responds. Parameters: sources (List[str]) – List of source names to terminate the conversation. Raises: TerminatedException – If the termination condition has already been reached. component_config_schema# alias of SourceMatchTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.SourceMatchTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → SourceMatchTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SourceMatchTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class TextMessageTermination(source: str | None = None)[source]# Bases: TerminationCondition, Component[TextMessageTerminationConfig] Terminate the conversation if a TextMessage is received. This termination condition checks for TextMessage instances in the message sequence. When a TextMessage is found, it terminates the conversation if either: - No source was specified (terminates on any TextMessage) - The message source matches the specified source Parameters: source (str | None, optional) – The source name to match against incoming messages. If None, matches any source. Defaults to None. component_config_schema# alias of TextMessageTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.TextMessageTermination'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → TextMessageTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: TextMessageTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class FunctionCallTermination(function_name: str)[source]# Bases: TerminationCondition, Component[FunctionCallTerminationConfig] Terminate the conversation if a FunctionExecutionResult with a specific name was received. Parameters: function_name (str) – The name of the function to look for in the messages. Raises: TerminatedException – If the termination condition has already been reached. component_config_schema# alias of FunctionCallTerminationConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.conditions.FunctionCallTermination'# The schema for the component configuration. property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. _to_config() → FunctionCallTerminationConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: FunctionCallTerminationConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class FunctionalTermination(func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]])[source]# Bases: TerminationCondition Terminate the conversation if an functional expression is met. Parameters: func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], bool] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[bool]]) – A function that takes a sequence of messages and returns True if the termination condition is met, False otherwise. The function can be a callable or an async callable. Example import asyncio from typing import Sequence from autogen_agentchat.conditions import FunctionalTermination from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage, StopMessage def expression(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> bool: # Check if the last message is a stop message return isinstance(messages[-1], StopMessage) termination = FunctionalTermination(expression) async def run() -> None: messages = [ StopMessage(source="agent1", content="Stop"), ] result = await termination(messages) print(result) asyncio.run(run()) StopMessage(source="FunctionalTermination", content="Functional termination condition met") property terminated: bool# Check if the termination condition has been reached async reset() → None[source]# Reset the termination condition. previous autogen_agentchat.base next autogen_agentchat.messages On this page MaxMessageTermination MaxMessageTermination.component_config_schema MaxMessageTermination.component_provider_override MaxMessageTermination.terminated MaxMessageTermination.reset() MaxMessageTermination._to_config() MaxMessageTermination._from_config() TextMentionTermination TextMentionTermination.component_config_schema TextMentionTermination.component_provider_override TextMentionTermination.terminated TextMentionTermination.reset() TextMentionTermination._to_config() TextMentionTermination._from_config() StopMessageTermination StopMessageTermination.component_config_schema StopMessageTermination.component_provider_override StopMessageTermination.terminated StopMessageTermination.reset() StopMessageTermination._to_config() StopMessageTermination._from_config() TokenUsageTermination TokenUsageTermination.component_config_schema TokenUsageTermination.component_provider_override TokenUsageTermination.terminated TokenUsageTermination.reset() TokenUsageTermination._to_config() TokenUsageTermination._from_config() HandoffTermination HandoffTermination.component_config_schema HandoffTermination.component_provider_override HandoffTermination.terminated HandoffTermination.reset() HandoffTermination._to_config() HandoffTermination._from_config() TimeoutTermination TimeoutTermination.component_config_schema TimeoutTermination.component_provider_override TimeoutTermination.terminated TimeoutTermination.reset() TimeoutTermination._to_config() TimeoutTermination._from_config() ExternalTermination ExternalTermination.component_config_schema ExternalTermination.component_provider_override ExternalTermination.terminated ExternalTermination.set() ExternalTermination.reset() ExternalTermination._to_config() ExternalTermination._from_config() SourceMatchTermination SourceMatchTermination.component_config_schema SourceMatchTermination.component_provider_override SourceMatchTermination.terminated SourceMatchTermination.reset() SourceMatchTermination._to_config() SourceMatchTermination._from_config() TextMessageTermination TextMessageTermination.component_config_schema TextMessageTermination.component_provider_override TextMessageTermination.terminated TextMessageTermination.reset() TextMessageTermination._to_config() TextMessageTermination._from_config() FunctionCallTermination FunctionCallTermination.component_config_schema FunctionCallTermination.component_provider_override FunctionCallTermination.terminated FunctionCallTermination.reset() FunctionCallTermination._to_config() FunctionCallTermination._from_config() FunctionalTermination FunctionalTermination.terminated FunctionalTermination.reset() Edit on GitHub Show Source

```
TerminationCondition
```

**Pattern 4:** Example:

```
from autogen_agentchat.conditions import ExternalTermination

termination = ExternalTermination()

# Run the team in an asyncio task.
...

# Set the termination condition externally
termination.set()
```

**Pattern 5:** API Reference autogen_agentchat.messages autogen_agentchat.messages# This module defines various message types used for agent-to-agent communication. Each message type inherits either from the BaseChatMessage class or BaseAgentEvent class and includes specific fields relevant to the type of message being sent. AgentEvent# The union type of all built-in concrete subclasses of BaseAgentEvent. alias of Annotated[ToolCallRequestEvent | ToolCallExecutionEvent | MemoryQueryEvent | UserInputRequestedEvent | ModelClientStreamingChunkEvent | ThoughtEvent | SelectSpeakerEvent | CodeGenerationEvent | CodeExecutionEvent, FieldInfo(annotation=NoneType, required=True, discriminator=’type’)] pydantic model BaseMessage[source]# Bases: BaseModel, ABC Abstract base class for all message types in AgentChat. Warning If you want to create a new message type, do not inherit from this class. Instead, inherit from BaseChatMessage or BaseAgentEvent to clarify the purpose of the message type. Show JSON schema{ "title": "BaseMessage", "description": "Abstract base class for all message types in AgentChat.\n\n.. warning::\n\n If you want to create a new message type, do not inherit from this class.\n Instead, inherit from :class:`BaseChatMessage` or :class:`BaseAgentEvent`\n to clarify the purpose of the message type.", "type": "object", "properties": {} } abstract to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. dump() → Mapping[str, Any][source]# Convert the message to a JSON-serializable dictionary. The default implementation uses the Pydantic model’s model_dump() method to convert the message to a dictionary. Datetime objects are automatically converted to ISO format strings to ensure JSON serialization compatibility. Override this method if you want to customize the serialization process or add additional fields to the output. classmethod load(data: Mapping[str, Any]) → Self[source]# Create a message from a dictionary of JSON-serializable data. The default implementation uses the Pydantic model’s model_validate() method to create the message from the data. Override this method if you want to customize the deserialization process or add additional fields to the input data. ChatMessage# The union type of all built-in concrete subclasses of BaseChatMessage. It does not include StructuredMessage types. alias of Annotated[TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, FieldInfo(annotation=NoneType, required=True, discriminator=’type’)] pydantic model BaseChatMessage[source]# Bases: BaseMessage, ABC Abstract base class for chat messages. Note If you want to create a new message type that is used for agent-to-agent communication, inherit from this class, or simply use StructuredMessage if your content type is a subclass of Pydantic BaseModel. This class is used for messages that are sent between agents in a chat conversation. Agents are expected to process the content of the message using models and return a response as another BaseChatMessage. Show JSON schema{ "title": "BaseChatMessage", "description": "Abstract base class for chat messages.\n\n.. note::\n\n If you want to create a new message type that is used for agent-to-agent\n communication, inherit from this class, or simply use\n :class:`StructuredMessage` if your content type is a subclass of\n Pydantic BaseModel.\n\nThis class is used for messages that are sent between agents in a chat\nconversation. Agents are expected to process the content of the\nmessage using models and return a response as another :class:`BaseChatMessage`.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source" ] } Fields: created_at (datetime.datetime) id (str) metadata (Dict[str, str]) models_usage (autogen_core.models._types.RequestUsage | None) source (str) field id: str [Optional]# Unique identifier for this message. field source: str [Required]# The name of the agent that sent this message. field models_usage: RequestUsage | None = None# The model client usage incurred when producing this message. field metadata: Dict[str, str] = {}# Additional metadata about the message. field created_at: datetime [Optional]# The time when the message was created. abstract to_model_text() → str[source]# Convert the content of the message to text-only representation. This is used for creating text-only content for models. This is not used for rendering the message in console. For that, use to_text(). The difference between this and to_model_message() is that this is used to construct parts of the a message for the model client, while to_model_message() is used to create a complete message for the model client. abstract to_model_message() → UserMessage[source]# Convert the message content to a UserMessage for use with model client, e.g., ChatCompletionClient. pydantic model BaseAgentEvent[source]# Bases: BaseMessage, ABC Base class for agent events. Note If you want to create a new message type for signaling observable events to user and application, inherit from this class. Agent events are used to signal actions and thoughts produced by agents and teams to user and applications. They are not used for agent-to-agent communication and are not expected to be processed by other agents. You should override the to_text() method if you want to provide a custom rendering of the content. Show JSON schema{ "title": "BaseAgentEvent", "description": "Base class for agent events.\n\n.. note::\n\n If you want to create a new message type for signaling observable events\n to user and application, inherit from this class.\n\nAgent events are used to signal actions and thoughts produced by agents\nand teams to user and applications. They are not used for agent-to-agent\ncommunication and are not expected to be processed by other agents.\n\nYou should override the :meth:`to_text` method if you want to provide\na custom rendering of the content.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source" ] } Fields: created_at (datetime.datetime) id (str) metadata (Dict[str, str]) models_usage (autogen_core.models._types.RequestUsage | None) source (str) field id: str [Optional]# Unique identifier for this event. field source: str [Required]# The name of the agent that sent this message. field models_usage: RequestUsage | None = None# The model client usage incurred when producing this message. field metadata: Dict[str, str] = {}# Additional metadata about the message. field created_at: datetime [Optional]# The time when the message was created. pydantic model BaseTextChatMessage[source]# Bases: BaseChatMessage, ABC Base class for all text-only BaseChatMessage types. It has implementations for to_text(), to_model_text(), and to_model_message() methods. Inherit from this class if your message content type is a string. Show JSON schema{ "title": "BaseTextChatMessage", "description": "Base class for all text-only :class:`BaseChatMessage` types.\nIt has implementations for :meth:`to_text`, :meth:`to_model_text`,\nand :meth:`to_model_message` methods.\n\nInherit from this class if your message content type is a string.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (str) field content: str [Required]# The content of the message. to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. to_model_text() → str[source]# Convert the content of the message to text-only representation. This is used for creating text-only content for models. This is not used for rendering the message in console. For that, use to_text(). The difference between this and to_model_message() is that this is used to construct parts of the a message for the model client, while to_model_message() is used to create a complete message for the model client. to_model_message() → UserMessage[source]# Convert the message content to a UserMessage for use with model client, e.g., ChatCompletionClient. class StructuredContentType# Type variable for structured content types. alias of TypeVar(‘StructuredContentType’, bound=BaseModel, covariant=True) pydantic model StructuredMessage[source]# Bases: BaseChatMessage, Generic[StructuredContentType] A BaseChatMessage type with an unspecified content type. To create a new structured message type, specify the content type as a subclass of Pydantic BaseModel. from pydantic import BaseModel from autogen_agentchat.messages import StructuredMessage class MyMessageContent(BaseModel): text: str number: int message = StructuredMessage[MyMessageContent]( content=MyMessageContent(text="Hello", number=42), source="agent1", ) print(message.to_text()) # {"text": "Hello", "number": 42} from pydantic import BaseModel from autogen_agentchat.messages import StructuredMessage class MyMessageContent(BaseModel): text: str number: int message = StructuredMessage[MyMessageContent]( content=MyMessageContent(text="Hello", number=42), source="agent", format_string="Hello, {text} {number}!", ) print(message.to_text()) # Hello, agent 42! Show JSON schema{ "title": "StructuredMessage", "description": "A :class:`BaseChatMessage` type with an unspecified content type.\n\nTo create a new structured message type, specify the content type\nas a subclass of `Pydantic BaseModel <https://docs.pydantic.dev/latest/concepts/models/>`_.\n\n.. code-block:: python\n\n from pydantic import BaseModel\n from autogen_agentchat.messages import StructuredMessage\n\n\n class MyMessageContent(BaseModel):\n text: str\n number: int\n\n\n message = StructuredMessage[MyMessageContent](\n content=MyMessageContent(text=\"Hello\", number=42),\n source=\"agent1\",\n )\n\n print(message.to_text()) # {\"text\": \"Hello\", \"number\": 42}\n\n.. code-block:: python\n\n from pydantic import BaseModel\n from autogen_agentchat.messages import StructuredMessage\n\n\n class MyMessageContent(BaseModel):\n text: str\n number: int\n\n\n message = StructuredMessage[MyMessageContent](\n content=MyMessageContent(text=\"Hello\", number=42),\n source=\"agent\",\n format_string=\"Hello, {text} {number}!\",\n )\n\n print(message.to_text()) # Hello, agent 42!", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "$ref": "#/$defs/BaseModel" }, "format_string": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Format String" } }, "$defs": { "BaseModel": { "properties": {}, "title": "BaseModel", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (autogen_agentchat.messages.StructuredContentType) format_string (str | None) field content: StructuredContentType [Required]# The content of the message. Must be a subclass of Pydantic BaseModel. field format_string: str | None = None# (Experimental) An optional format string to render the content into a human-readable format. The format string can use the fields of the content model as placeholders. For example, if the content model has a field name, you can use {name} in the format string to include the value of that field. The format string is used in the to_text() method to create a human-readable representation of the message. This setting is experimental and will change in the future. property type: str# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. to_model_text() → str[source]# Convert the content of the message to text-only representation. This is used for creating text-only content for models. This is not used for rendering the message in console. For that, use to_text(). The difference between this and to_model_message() is that this is used to construct parts of the a message for the model client, while to_model_message() is used to create a complete message for the model client. to_model_message() → UserMessage[source]# Convert the message content to a UserMessage for use with model client, e.g., ChatCompletionClient. pydantic model HandoffMessage[source]# Bases: BaseTextChatMessage A message requesting handoff of a conversation to another agent. Show JSON schema{ "title": "HandoffMessage", "description": "A message requesting handoff of a conversation to another agent.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "target": { "title": "Target", "type": "string" }, "context": { "default": [], "items": { "discriminator": { "mapping": { "AssistantMessage": "#/$defs/AssistantMessage", "FunctionExecutionResultMessage": "#/$defs/FunctionExecutionResultMessage", "SystemMessage": "#/$defs/SystemMessage", "UserMessage": "#/$defs/UserMessage" }, "propertyName": "type" }, "oneOf": [ { "$ref": "#/$defs/SystemMessage" }, { "$ref": "#/$defs/UserMessage" }, { "$ref": "#/$defs/AssistantMessage" }, { "$ref": "#/$defs/FunctionExecutionResultMessage" } ] }, "title": "Context", "type": "array" }, "type": { "const": "HandoffMessage", "default": "HandoffMessage", "title": "Type", "type": "string" } }, "$defs": { "AssistantMessage": { "description": "Assistant message are sampled from the language model.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "$ref": "#/$defs/FunctionCall" }, "type": "array" } ], "title": "Content" }, "thought": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Thought" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "AssistantMessage", "default": "AssistantMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ], "title": "AssistantMessage", "type": "object" }, "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" }, "FunctionExecutionResultMessage": { "description": "Function execution result message contains the output of multiple function calls.", "properties": { "content": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Content", "type": "array" }, "type": { "const": "FunctionExecutionResultMessage", "default": "FunctionExecutionResultMessage", "title": "Type", "type": "string" } }, "required": [ "content" ], "title": "FunctionExecutionResultMessage", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" }, "SystemMessage": { "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n Open AI is moving away from using 'system' role in favor of 'developer' role.\n See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n on the server side.\n So, you can use `SystemMessage` for developer messages.", "properties": { "content": { "title": "Content", "type": "string" }, "type": { "const": "SystemMessage", "default": "SystemMessage", "title": "Type", "type": "string" } }, "required": [ "content" ], "title": "SystemMessage", "type": "object" }, "UserMessage": { "description": "User message contains input from end users, or a catch-all for data provided to the model.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "anyOf": [ { "type": "string" }, {} ] }, "type": "array" } ], "title": "Content" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "UserMessage", "default": "UserMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ], "title": "UserMessage", "type": "object" } }, "required": [ "source", "content", "target" ] } Fields: context (List[Annotated[autogen_core.models._types.SystemMessage | autogen_core.models._types.UserMessage | autogen_core.models._types.AssistantMessage | autogen_core.models._types.FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]]) target (str) type (Literal['HandoffMessage']) field target: str [Required]# The name of the target agent to handoff to. field context: List[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]] = []# The model context to be passed to the target agent. field type: Literal['HandoffMessage'] = 'HandoffMessage'# pydantic model MultiModalMessage[source]# Bases: BaseChatMessage A multimodal message. Show JSON schema{ "title": "MultiModalMessage", "description": "A multimodal message.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "anyOf": [ { "type": "string" }, {} ] }, "title": "Content", "type": "array" }, "type": { "const": "MultiModalMessage", "default": "MultiModalMessage", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (List[str | autogen_core._image.Image]) type (Literal['MultiModalMessage']) field content: List[str | Image] [Required]# The content of the message. field type: Literal['MultiModalMessage'] = 'MultiModalMessage'# to_model_text(image_placeholder: str | None = '[image]') → str[source]# Convert the content of the message to a string-only representation. If an image is present, it will be replaced with the image placeholder by default, otherwise it will be a base64 string when set to None. to_text(iterm: bool = False) → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. to_model_message() → UserMessage[source]# Convert the message content to a UserMessage for use with model client, e.g., ChatCompletionClient. pydantic model StopMessage[source]# Bases: BaseTextChatMessage A message requesting stop of a conversation. Show JSON schema{ "title": "StopMessage", "description": "A message requesting stop of a conversation.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "type": { "const": "StopMessage", "default": "StopMessage", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: type (Literal['StopMessage']) field type: Literal['StopMessage'] = 'StopMessage'# pydantic model TextMessage[source]# Bases: BaseTextChatMessage A text message with string-only content. Show JSON schema{ "title": "TextMessage", "description": "A text message with string-only content.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "type": { "const": "TextMessage", "default": "TextMessage", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: type (Literal['TextMessage']) field type: Literal['TextMessage'] = 'TextMessage'# pydantic model ToolCallExecutionEvent[source]# Bases: BaseAgentEvent An event signaling the execution of tool calls. Show JSON schema{ "title": "ToolCallExecutionEvent", "description": "An event signaling the execution of tool calls.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Content", "type": "array" }, "type": { "const": "ToolCallExecutionEvent", "default": "ToolCallExecutionEvent", "title": "Type", "type": "string" } }, "$defs": { "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (List[autogen_core.models._types.FunctionExecutionResult]) type (Literal['ToolCallExecutionEvent']) field content: List[FunctionExecutionResult] [Required]# The tool call results. field type: Literal['ToolCallExecutionEvent'] = 'ToolCallExecutionEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. pydantic model ToolCallRequestEvent[source]# Bases: BaseAgentEvent An event signaling a request to use tools. Show JSON schema{ "title": "ToolCallRequestEvent", "description": "An event signaling a request to use tools.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "$ref": "#/$defs/FunctionCall" }, "title": "Content", "type": "array" }, "type": { "const": "ToolCallRequestEvent", "default": "ToolCallRequestEvent", "title": "Type", "type": "string" } }, "$defs": { "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (List[autogen_core._types.FunctionCall]) type (Literal['ToolCallRequestEvent']) field content: List[FunctionCall] [Required]# The tool calls. field type: Literal['ToolCallRequestEvent'] = 'ToolCallRequestEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. pydantic model ToolCallSummaryMessage[source]# Bases: BaseTextChatMessage A message signaling the summary of tool call results. Show JSON schema{ "title": "ToolCallSummaryMessage", "description": "A message signaling the summary of tool call results.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "type": { "const": "ToolCallSummaryMessage", "default": "ToolCallSummaryMessage", "title": "Type", "type": "string" }, "tool_calls": { "items": { "$ref": "#/$defs/FunctionCall" }, "title": "Tool Calls", "type": "array" }, "results": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Results", "type": "array" } }, "$defs": { "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content", "tool_calls", "results" ] } Fields: results (List[autogen_core.models._types.FunctionExecutionResult]) tool_calls (List[autogen_core._types.FunctionCall]) type (Literal['ToolCallSummaryMessage']) field type: Literal['ToolCallSummaryMessage'] = 'ToolCallSummaryMessage'# field tool_calls: List[FunctionCall] [Required]# The tool calls that were made. field results: List[FunctionExecutionResult] [Required]# The results of the tool calls. pydantic model MemoryQueryEvent[source]# Bases: BaseAgentEvent An event signaling the results of memory queries. Show JSON schema{ "title": "MemoryQueryEvent", "description": "An event signaling the results of memory queries.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "$ref": "#/$defs/MemoryContent" }, "title": "Content", "type": "array" }, "type": { "const": "MemoryQueryEvent", "default": "MemoryQueryEvent", "title": "Type", "type": "string" } }, "$defs": { "MemoryContent": { "description": "A memory content item.", "properties": { "content": { "anyOf": [ { "type": "string" }, { "format": "binary", "type": "string" }, { "type": "object" }, {} ], "title": "Content" }, "mime_type": { "anyOf": [ { "$ref": "#/$defs/MemoryMimeType" }, { "type": "string" } ], "title": "Mime Type" }, "metadata": { "anyOf": [ { "type": "object" }, { "type": "null" } ], "default": null, "title": "Metadata" } }, "required": [ "content", "mime_type" ], "title": "MemoryContent", "type": "object" }, "MemoryMimeType": { "description": "Supported MIME types for memory content.", "enum": [ "text/plain", "application/json", "text/markdown", "image/*", "application/octet-stream" ], "title": "MemoryMimeType", "type": "string" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (List[autogen_core.memory._base_memory.MemoryContent]) type (Literal['MemoryQueryEvent']) field content: List[MemoryContent] [Required]# The memory query results. field type: Literal['MemoryQueryEvent'] = 'MemoryQueryEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. pydantic model UserInputRequestedEvent[source]# Bases: BaseAgentEvent An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback. Show JSON schema{ "title": "UserInputRequestedEvent", "description": "An event signaling a that the user proxy has requested user input. Published prior to invoking the input callback.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "request_id": { "title": "Request Id", "type": "string" }, "content": { "const": "", "default": "", "title": "Content", "type": "string" }, "type": { "const": "UserInputRequestedEvent", "default": "UserInputRequestedEvent", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "request_id" ] } Fields: content (Literal['']) request_id (str) type (Literal['UserInputRequestedEvent']) field request_id: str [Required]# Identifier for the user input request. field content: Literal[''] = ''# Empty content for compat with consumers expecting a content field. field type: Literal['UserInputRequestedEvent'] = 'UserInputRequestedEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. pydantic model ModelClientStreamingChunkEvent[source]# Bases: BaseAgentEvent An event signaling a text output chunk from a model client in streaming mode. Show JSON schema{ "title": "ModelClientStreamingChunkEvent", "description": "An event signaling a text output chunk from a model client in streaming mode.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "full_message_id": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Full Message Id" }, "type": { "const": "ModelClientStreamingChunkEvent", "default": "ModelClientStreamingChunkEvent", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (str) full_message_id (str | None) type (Literal['ModelClientStreamingChunkEvent']) field content: str [Required]# A string chunk from the model client. field full_message_id: str | None = None# Optional reference to the complete message that may come after the chunks. This allows consumers of the stream to correlate chunks with the eventual completed message. field type: Literal['ModelClientStreamingChunkEvent'] = 'ModelClientStreamingChunkEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. pydantic model ThoughtEvent[source]# Bases: BaseAgentEvent An event signaling the thought process of a model. It is used to communicate the reasoning tokens generated by a reasoning model, or the extra text content generated by a function call. Show JSON schema{ "title": "ThoughtEvent", "description": "An event signaling the thought process of a model.\nIt is used to communicate the reasoning tokens generated by a reasoning model,\nor the extra text content generated by a function call.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "title": "Content", "type": "string" }, "type": { "const": "ThoughtEvent", "default": "ThoughtEvent", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (str) type (Literal['ThoughtEvent']) field content: str [Required]# The thought process of the model. field type: Literal['ThoughtEvent'] = 'ThoughtEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. pydantic model SelectSpeakerEvent[source]# Bases: BaseAgentEvent An event signaling the selection of speakers for a conversation. Show JSON schema{ "title": "SelectSpeakerEvent", "description": "An event signaling the selection of speakers for a conversation.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "content": { "items": { "type": "string" }, "title": "Content", "type": "array" }, "type": { "const": "SelectSpeakerEvent", "default": "SelectSpeakerEvent", "title": "Type", "type": "string" } }, "$defs": { "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "content" ] } Fields: content (List[str]) type (Literal['SelectSpeakerEvent']) field content: List[str] [Required]# The names of the selected speakers. field type: Literal['SelectSpeakerEvent'] = 'SelectSpeakerEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. pydantic model CodeGenerationEvent[source]# Bases: BaseAgentEvent An event signaling code generation event. Show JSON schema{ "title": "CodeGenerationEvent", "description": "An event signaling code generation event.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "retry_attempt": { "title": "Retry Attempt", "type": "integer" }, "content": { "title": "Content", "type": "string" }, "code_blocks": { "items": { "$ref": "#/$defs/CodeBlock" }, "title": "Code Blocks", "type": "array" }, "type": { "const": "CodeGenerationEvent", "default": "CodeGenerationEvent", "title": "Type", "type": "string" } }, "$defs": { "CodeBlock": { "properties": { "code": { "title": "Code", "type": "string" }, "language": { "title": "Language", "type": "string" } }, "required": [ "code", "language" ], "title": "CodeBlock", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "retry_attempt", "content", "code_blocks" ] } Fields: code_blocks (List[autogen_core.code_executor._base.CodeBlock]) content (str) retry_attempt (int) type (Literal['CodeGenerationEvent']) field retry_attempt: int [Required]# Retry number, 0 means first generation field content: str [Required]# The complete content as string. field code_blocks: List[CodeBlock] [Required]# List of code blocks present in content field type: Literal['CodeGenerationEvent'] = 'CodeGenerationEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. pydantic model CodeExecutionEvent[source]# Bases: BaseAgentEvent An event signaling code execution event. Show JSON schema{ "title": "CodeExecutionEvent", "description": "An event signaling code execution event.", "type": "object", "properties": { "id": { "title": "Id", "type": "string" }, "source": { "title": "Source", "type": "string" }, "models_usage": { "anyOf": [ { "$ref": "#/$defs/RequestUsage" }, { "type": "null" } ], "default": null }, "metadata": { "additionalProperties": { "type": "string" }, "default": {}, "title": "Metadata", "type": "object" }, "created_at": { "format": "date-time", "title": "Created At", "type": "string" }, "retry_attempt": { "title": "Retry Attempt", "type": "integer" }, "result": { "$ref": "#/$defs/CodeResult" }, "type": { "const": "CodeExecutionEvent", "default": "CodeExecutionEvent", "title": "Type", "type": "string" } }, "$defs": { "CodeResult": { "properties": { "exit_code": { "title": "Exit Code", "type": "integer" }, "output": { "title": "Output", "type": "string" } }, "required": [ "exit_code", "output" ], "title": "CodeResult", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" } }, "required": [ "source", "retry_attempt", "result" ] } Fields: result (autogen_core.code_executor._base.CodeResult) retry_attempt (int) type (Literal['CodeExecutionEvent']) field retry_attempt: int [Required]# Retry number, 0 means first execution field result: CodeResult [Required]# Code Execution Result field type: Literal['CodeExecutionEvent'] = 'CodeExecutionEvent'# to_text() → str[source]# Convert the message content to a string-only representation that can be rendered in the console and inspected by the user or conditions. This is not used for creating text-only content for models. For BaseChatMessage types, use to_model_text() instead. previous autogen_agentchat.conditions next autogen_agentchat.state On this page AgentEvent BaseMessage BaseMessage.to_text() BaseMessage.dump() BaseMessage.load() ChatMessage BaseChatMessage BaseChatMessage.id BaseChatMessage.source BaseChatMessage.models_usage BaseChatMessage.metadata BaseChatMessage.created_at BaseChatMessage.to_model_text() BaseChatMessage.to_model_message() BaseAgentEvent BaseAgentEvent.id BaseAgentEvent.source BaseAgentEvent.models_usage BaseAgentEvent.metadata BaseAgentEvent.created_at BaseTextChatMessage BaseTextChatMessage.content BaseTextChatMessage.to_text() BaseTextChatMessage.to_model_text() BaseTextChatMessage.to_model_message() StructuredContentType StructuredMessage StructuredMessage.content StructuredMessage.format_string StructuredMessage.type StructuredMessage.to_text() StructuredMessage.to_model_text() StructuredMessage.to_model_message() HandoffMessage HandoffMessage.target HandoffMessage.context HandoffMessage.type MultiModalMessage MultiModalMessage.content MultiModalMessage.type MultiModalMessage.to_model_text() MultiModalMessage.to_text() MultiModalMessage.to_model_message() StopMessage StopMessage.type TextMessage TextMessage.type ToolCallExecutionEvent ToolCallExecutionEvent.content ToolCallExecutionEvent.type ToolCallExecutionEvent.to_text() ToolCallRequestEvent ToolCallRequestEvent.content ToolCallRequestEvent.type ToolCallRequestEvent.to_text() ToolCallSummaryMessage ToolCallSummaryMessage.type ToolCallSummaryMessage.tool_calls ToolCallSummaryMessage.results MemoryQueryEvent MemoryQueryEvent.content MemoryQueryEvent.type MemoryQueryEvent.to_text() UserInputRequestedEvent UserInputRequestedEvent.request_id UserInputRequestedEvent.content UserInputRequestedEvent.type UserInputRequestedEvent.to_text() ModelClientStreamingChunkEvent ModelClientStreamingChunkEvent.content ModelClientStreamingChunkEvent.full_message_id ModelClientStreamingChunkEvent.type ModelClientStreamingChunkEvent.to_text() ThoughtEvent ThoughtEvent.content ThoughtEvent.type ThoughtEvent.to_text() SelectSpeakerEvent SelectSpeakerEvent.content SelectSpeakerEvent.type SelectSpeakerEvent.to_text() CodeGenerationEvent CodeGenerationEvent.retry_attempt CodeGenerationEvent.content CodeGenerationEvent.code_blocks CodeGenerationEvent.type CodeGenerationEvent.to_text() CodeExecutionEvent CodeExecutionEvent.retry_attempt CodeExecutionEvent.result CodeExecutionEvent.type CodeExecutionEvent.to_text() Edit on GitHub Show Source

```
BaseAgentEvent
```

**Pattern 6:** API Reference autogen_agentchat.teams autogen_agentchat.teams# This module provides implementation of various pre-defined multi-agent teams. Each team inherits from the BaseGroupChat class. class BaseGroupChat(name: str, description: str, participants: List[ChatAgent | Team], group_chat_manager_name: str, group_chat_manager_class: type[SequentialRoutedAgent], termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: Team, ABC, ComponentBase[BaseModel] The base class for group chat teams. In a group chat team, participants share context by publishing their messages to all other participants. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. To implement a group chat team, first create a subclass of BaseGroupChatManager and then create a subclass of BaseGroupChat that uses the group chat manager. This base class provides the mapping between the agents of the AgentChat API and the agent runtime of the Core API, and handles high-level features like running, pausing, resuming, and resetting the team. component_type: ClassVar[ComponentType] = 'team'# The logical type of the component. property name: str# The name of the group chat team. property description: str# A description of the group chat team. async run(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → TaskResult[source]# Run the team and return the result. The base implementation uses run_stream() to run the team and then returns the final result. Once the team is stopped, the termination condition is reset. Parameters: task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage. cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead. Returns: result – The result of the task as TaskResult. The result contains the messages produced by the team and the stop reason. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent1 = AssistantAgent("Assistant1", model_client=model_client) agent2 = AssistantAgent("Assistant2", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) result = await team.run(task="Count from 1 to 10, respond one at a time.") print(result) # Run the team again without a task to continue the previous task. result = await team.run() print(result) asyncio.run(main()) Example using the CancellationToken to cancel the task: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent1 = AssistantAgent("Assistant1", model_client=model_client) agent2 = AssistantAgent("Assistant2", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) cancellation_token = CancellationToken() # Create a task to run the team in the background. run_task = asyncio.create_task( team.run( task="Count from 1 to 10, respond one at a time.", cancellation_token=cancellation_token, ) ) # Wait for 1 second and then cancel the task. await asyncio.sleep(1) cancellation_token.cancel() # This will raise a cancellation error. await run_task asyncio.run(main()) async run_stream(*, task: str | BaseChatMessage | Sequence[BaseChatMessage] | None = None, cancellation_token: CancellationToken | None = None, output_task_messages: bool = True) → AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None][source]# Run the team and produces a stream of messages and the final result of the type TaskResult as the last item in the stream. Once the team is stopped, the termination condition is reset. Note If an agent produces ModelClientStreamingChunkEvent, the message will be yielded in the stream but it will not be included in the messages. Parameters: task (str | BaseChatMessage | Sequence[BaseChatMessage] | None) – The task to run the team with. Can be a string, a single BaseChatMessage , or a list of BaseChatMessage. cancellation_token (CancellationToken | None) – The cancellation token to kill the task immediately. Setting the cancellation token potentially put the team in an inconsistent state, and it may not reset the termination condition. To gracefully stop the team, use ExternalTermination instead. output_task_messages (bool) – Whether to include task messages in the output stream. Defaults to True for backward compatibility. Returns: stream – an AsyncGenerator that yields BaseAgentEvent, BaseChatMessage, and the final result TaskResult as the last item in the stream. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent1 = AssistantAgent("Assistant1", model_client=model_client) agent2 = AssistantAgent("Assistant2", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) stream = team.run_stream(task="Count from 1 to 10, respond one at a time.") async for message in stream: print(message) # Run the team again without a task to continue the previous task. stream = team.run_stream() async for message in stream: print(message) asyncio.run(main()) Example using the CancellationToken to cancel the task: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.ui import Console from autogen_agentchat.teams import RoundRobinGroupChat from autogen_core import CancellationToken from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent1 = AssistantAgent("Assistant1", model_client=model_client) agent2 = AssistantAgent("Assistant2", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) cancellation_token = CancellationToken() # Create a task to run the team in the background. run_task = asyncio.create_task( Console( team.run_stream( task="Count from 1 to 10, respond one at a time.", cancellation_token=cancellation_token, ) ) ) # Wait for 1 second and then cancel the task. await asyncio.sleep(1) cancellation_token.cancel() # This will raise a cancellation error. await run_task asyncio.run(main()) async reset() → None[source]# Reset the team and its participants to their initial state. The team must be stopped before it can be reset. Raises: RuntimeError – If the team has not been initialized or is currently running. Example using the RoundRobinGroupChat team: import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent1 = AssistantAgent("Assistant1", model_client=model_client) agent2 = AssistantAgent("Assistant2", model_client=model_client) termination = MaxMessageTermination(3) team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) stream = team.run_stream(task="Count from 1 to 10, respond one at a time.") async for message in stream: print(message) # Reset the team. await team.reset() stream = team.run_stream(task="Count from 1 to 10, respond one at a time.") async for message in stream: print(message) asyncio.run(main()) async pause() → None[source]# Pause its participants when the team is running by calling their on_pause() method via direct RPC calls. Attention This is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future. The team must be initialized before it can be paused. Different from termination, pausing the team does not cause the run() or run_stream() method to return. It calls the on_pause() method on each participant, and if the participant does not implement the method, it will be a no-op. Note It is the responsibility of the agent class to handle the pause and ensure that the agent can be resumed later. Make sure to implement the on_pause() method in your agent class for custom pause behavior. By default, the agent will not do anything when called. Raises: RuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_pause are propagated to this method and raised. async resume() → None[source]# Resume its participants when the team is running and paused by calling their on_resume() method via direct RPC calls. Attention This is an experimental feature introduced in v0.4.9 and may subject to change or removal in the future. The team must be initialized before it can be resumed. Different from termination and restart with a new task, resuming the team does not cause the run() or run_stream() method to return. It calls the on_resume() method on each participant, and if the participant does not implement the method, it will be a no-op. Note It is the responsibility of the agent class to handle the resume and ensure that the agent continues from where it was paused. Make sure to implement the on_resume() method in your agent class for custom resume behavior. Raises: RuntimeError – If the team has not been initialized. Exceptions from the participants when calling their implementations of on_resume method are propagated to this method and raised. async save_state() → Mapping[str, Any][source]# Save the state of the group chat team. The state is saved by calling the agent_save_state() method on each participant and the group chat manager with their internal agent ID. The state is returned as a nested dictionary: a dictionary with key agent_states, which is a dictionary the agent names as keys and the state as values. { "agent_states": { "agent1": ..., "agent2": ..., "RoundRobinGroupChatManager": ... } } Note Starting v0.4.9, the state is using the agent name as the key instead of the agent ID, and the team_id field is removed from the state. This is to allow the state to be portable across different teams and runtimes. States saved with the old format may not be compatible with the new format in the future. Caution When calling save_state() on a team while it is running, the state may not be consistent and may result in an unexpected state. It is recommended to call this method when the team is not running or after it is stopped. async load_state(state: Mapping[str, Any]) → None[source]# Load an external state and overwrite the current state of the group chat team. The state is loaded by calling the agent_load_state() method on each participant and the group chat manager with their internal agent ID. See save_state() for the expected format of the state. class RoundRobinGroupChat(participants: List[ChatAgent | Team], *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[RoundRobinGroupChatConfig] A team that runs a group chat with participants taking turns in a round-robin fashion to publish a message to all. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. If a single participant is in the team, the participant will be the only speaker. Parameters: participants (List[ChatAgent | Team]) – The participants in the group chat. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Raises: ValueError – If no participants are provided or if participant names are not unique. Examples A team with one participant with tools: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") async def get_weather(location: str) -> str: return f"The weather in {location} is sunny." assistant = AssistantAgent( "Assistant", model_client=model_client, tools=[get_weather], ) termination = TextMentionTermination("TERMINATE") team = RoundRobinGroupChat([assistant], termination_condition=termination) await Console(team.run_stream(task="What's the weather in New York?")) asyncio.run(main()) A team with multiple participants: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent1 = AssistantAgent("Assistant1", model_client=model_client) agent2 = AssistantAgent("Assistant2", model_client=model_client) termination = TextMentionTermination("TERMINATE") team = RoundRobinGroupChat([agent1, agent2], termination_condition=termination) await Console(team.run_stream(task="Tell me some jokes.")) asyncio.run(main()) A team of user proxy and a nested team of writer and reviewer agents: import asyncio from autogen_agentchat.agents import UserProxyAgent, AssistantAgent from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.ui import Console from autogen_ext.models.openai import OpenAIChatCompletionClient async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano") writer = AssistantAgent( "writer", model_client=model_client, system_message="You are a writer.", model_client_stream=True ) reviewer = AssistantAgent( "reviewer", model_client=model_client, system_message="Provide feedback to the input and suggest improvements.", model_client_stream=True, ) # NOTE: you can skip input by pressing Enter. user_proxy = UserProxyAgent("user_proxy") # Maximum 1 round of review and revision. inner_termination = MaxMessageTermination(max_messages=4) # The outter-loop termination condition that will terminate the team when the user types "exit". outter_termination = TextMentionTermination("exit", sources=["user_proxy"]) team = RoundRobinGroupChat( [ # For each turn, the writer writes a summary and the reviewer reviews it. RoundRobinGroupChat([writer, reviewer], termination_condition=inner_termination), # The user proxy gets user input once the writer and reviewer have finished their actions. user_proxy, ], termination_condition=outter_termination, ) # Start the team and wait for it to terminate. await Console(team.run_stream(task="Write a short essay about the impact of AI on society.")) asyncio.run(main()) component_config_schema# alias of RoundRobinGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.RoundRobinGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'RoundRobinGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → RoundRobinGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: RoundRobinGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class SelectorGroupChat(participants: List[ChatAgent | Team], model_client: ChatCompletionClient, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, selector_prompt: str = 'You are in a role play game. The following roles are available:\n{roles}.\nRead the following conversation. Then select the next role from {participants} to play. Only return the role.\n\n{history}\n\nRead the above conversation. Then select the next role from {participants} to play. Only return the role.\n', allow_repeated_speaker: bool = False, max_selector_attempts: int = 3, selector_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]] | None = None, candidate_func: Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]] | Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]] | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False, model_client_streaming: bool = False, model_context: ChatCompletionContext | None = None)[source]# Bases: BaseGroupChat, Component[SelectorGroupChatConfig] A group chat team that have participants takes turn to publish a message to all, using a ChatCompletion model to select the next speaker after each message. If an ChatAgent is a participant, the BaseChatMessage from the agent response’s chat_message will be published to other participants in the group chat. If a Team is a participant, the BaseChatMessage from the team result’ messages will be published to other participants in the group chat. Parameters: participants (List[ChatAgent | Team]) – The participants in the group chat, must have unique names and at least two participants. model_client (ChatCompletionClient) – The ChatCompletion model client used to select the next speaker. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. selector_prompt (str, optional) – The prompt template to use for selecting the next speaker. Available fields: ‘{roles}’, ‘{participants}’, and ‘{history}’. {participants} is the names of candidates for selection. The format is [“<name1>”, “<name2>”, …]. {roles} is a newline-separated list of names and descriptions of the candidate agents. The format for each line is: “<name> : <description>”. {history} is the conversation history formatted as a double newline separated of names and message content. The format for each message is: “<name> : <message content>”. allow_repeated_speaker (bool, optional) – Whether to include the previous speaker in the list of candidates to be selected for the next turn. Defaults to False. The model may still select the previous speaker – a warning will be logged if this happens. max_selector_attempts (int, optional) – The maximum number of attempts to select a speaker using the model. Defaults to 3. If the model fails to select a speaker after the maximum number of attempts, the previous speaker will be used if available, otherwise the first participant will be used. selector_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], str | None], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[str | None]], optional) – A custom selector function that takes the conversation history and returns the name of the next speaker. If provided, this function will be used to override the model to select the next speaker. If the function returns None, the model will be used to select the next speaker. NOTE: selector_func is not serializable and will be ignored during serialization and deserialization process. candidate_func (Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], List[str]], Callable[[Sequence[BaseAgentEvent | BaseChatMessage]], Awaitable[List[str]]], optional) – A custom function that takes the conversation history and returns a filtered list of candidates for the next speaker selection using model. If the function returns an empty list or None, SelectorGroupChat will raise a ValueError. This function is only used if selector_func is not set. The allow_repeated_speaker will be ignored if set. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. model_client_streaming (bool, optional) – Whether to use streaming for the model client. (This is useful for reasoning models like QwQ). Defaults to False. model_context (ChatCompletionContext | None, optional) – The model context for storing and retrieving LLMMessage. It can be preloaded with initial messages. Messages stored in model context will be used for speaker selection. The initial messages will be cleared when the team is reset. Raises: ValueError – If the number of participants is less than two or if the selector prompt is invalid. Examples: A team with multiple participants: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") async def lookup_hotel(location: str) -> str: return f"Here are some hotels in {location}: hotel1, hotel2, hotel3." async def lookup_flight(origin: str, destination: str) -> str: return f"Here are some flights from {origin} to {destination}: flight1, flight2, flight3." async def book_trip() -> str: return "Your trip is booked!" travel_advisor = AssistantAgent( "Travel_Advisor", model_client, tools=[book_trip], description="Helps with travel planning.", ) hotel_agent = AssistantAgent( "Hotel_Agent", model_client, tools=[lookup_hotel], description="Helps with hotel booking.", ) flight_agent = AssistantAgent( "Flight_Agent", model_client, tools=[lookup_flight], description="Helps with flight booking.", ) termination = TextMentionTermination("TERMINATE") team = SelectorGroupChat( [travel_advisor, hotel_agent, flight_agent], model_client=model_client, termination_condition=termination, ) await Console(team.run_stream(task="Book a 3-day trip to new york.")) asyncio.run(main()) A team with a custom selector function: import asyncio from typing import Sequence from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.ui import Console from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") def check_calculation(x: int, y: int, answer: int) -> str: if x + y == answer: return "Correct!" else: return "Incorrect!" agent1 = AssistantAgent( "Agent1", model_client, description="For calculation", system_message="Calculate the sum of two numbers", ) agent2 = AssistantAgent( "Agent2", model_client, tools=[check_calculation], description="For checking calculation", system_message="Check the answer and respond with 'Correct!' or 'Incorrect!'", ) def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None: if len(messages) == 1 or messages[-1].to_text() == "Incorrect!": return "Agent1" if messages[-1].source == "Agent1": return "Agent2" return None termination = TextMentionTermination("Correct!") team = SelectorGroupChat( [agent1, agent2], model_client=model_client, selector_func=selector_func, termination_condition=termination, ) await Console(team.run_stream(task="What is 1 + 1?")) asyncio.run(main()) A team with custom model context: import asyncio from autogen_core.model_context import BufferedChatCompletionContext from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import TextMentionTermination from autogen_agentchat.teams import SelectorGroupChat from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") model_context = BufferedChatCompletionContext(buffer_size=5) async def lookup_hotel(location: str) -> str: return f"Here are some hotels in {location}: hotel1, hotel2, hotel3." async def lookup_flight(origin: str, destination: str) -> str: return f"Here are some flights from {origin} to {destination}: flight1, flight2, flight3." async def book_trip() -> str: return "Your trip is booked!" travel_advisor = AssistantAgent( "Travel_Advisor", model_client, tools=[book_trip], description="Helps with travel planning.", ) hotel_agent = AssistantAgent( "Hotel_Agent", model_client, tools=[lookup_hotel], description="Helps with hotel booking.", ) flight_agent = AssistantAgent( "Flight_Agent", model_client, tools=[lookup_flight], description="Helps with flight booking.", ) termination = TextMentionTermination("TERMINATE") team = SelectorGroupChat( [travel_advisor, hotel_agent, flight_agent], model_client=model_client, termination_condition=termination, model_context=model_context, ) await Console(team.run_stream(task="Book a 3-day trip to new york.")) asyncio.run(main()) component_config_schema# alias of SelectorGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.SelectorGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'SelectorGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → SelectorGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SelectorGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class Swarm(participants: List[ChatAgent], *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[SwarmConfig] A group chat team that selects the next speaker based on handoff message only. The first participant in the list of participants is the initial speaker. The next speaker is selected based on the HandoffMessage message sent by the current speaker. If no handoff message is sent, the current speaker continues to be the speaker. Note Unlike RoundRobinGroupChat and SelectorGroupChat, this group chat team does not support inner teams as participants. Parameters: participants (List[ChatAgent]) – The agents participating in the group chat. The first agent in the list is the initial speaker. name (str | None, optional) – The name of the group chat, using DEFAULT_NAME if not provided. The name is used by a parent team to identify this group chat so it must be unique within the parent team. description (str | None, optional) – The description of the group chat, using DEFAULT_DESCRIPTION if not provided. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run indefinitely. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to None, meaning no limit. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Basic example: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import Swarm from autogen_agentchat.conditions import MaxMessageTermination async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent1 = AssistantAgent( "Alice", model_client=model_client, handoffs=["Bob"], system_message="You are Alice and you only answer questions about yourself.", ) agent2 = AssistantAgent( "Bob", model_client=model_client, system_message="You are Bob and your birthday is on 1st January." ) termination = MaxMessageTermination(3) team = Swarm([agent1, agent2], termination_condition=termination) stream = team.run_stream(task="What is bob's birthday?") async for message in stream: print(message) asyncio.run(main()) Using the HandoffTermination for human-in-the-loop handoff: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import Swarm from autogen_agentchat.conditions import HandoffTermination, MaxMessageTermination from autogen_agentchat.ui import Console from autogen_agentchat.messages import HandoffMessage async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") agent = AssistantAgent( "Alice", model_client=model_client, handoffs=["user"], system_message="You are Alice and you only answer questions about yourself, ask the user for help if needed.", ) termination = HandoffTermination(target="user") | MaxMessageTermination(3) team = Swarm([agent], termination_condition=termination) # Start the conversation. await Console(team.run_stream(task="What is bob's birthday?")) # Resume with user feedback. await Console( team.run_stream( task=HandoffMessage(source="user", target="Alice", content="Bob's birthday is on 1st January.") ) ) asyncio.run(main()) component_config_schema# alias of SwarmConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.Swarm'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'Swarm'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → SwarmConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: SwarmConfig) → Swarm[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class MagenticOneGroupChat(participants: List[ChatAgent], model_client: ChatCompletionClient, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = 20, runtime: AgentRuntime | None = None, max_stalls: int = 3, final_answer_prompt: str = ORCHESTRATOR_FINAL_ANSWER_PROMPT, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None, emit_team_events: bool = False)[source]# Bases: BaseGroupChat, Component[MagenticOneGroupChatConfig] A team that runs a group chat with participants managed by the MagenticOneOrchestrator. The orchestrator handles the conversation flow, ensuring that the task is completed efficiently by managing the participants’ interactions. The orchestrator is based on the Magentic-One architecture, which is a generalist multi-agent system for solving complex tasks (see references below). Unlike RoundRobinGroupChat and SelectorGroupChat, the MagenticOneGroupChat does not support using team as participant. Parameters: participants (List[ChatAgent]) – The participants in the group chat. model_client (ChatCompletionClient) – The model client used for generating responses. termination_condition (TerminationCondition, optional) – The termination condition for the group chat. Defaults to None. Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached. max_turns (int, optional) – The maximum number of turns in the group chat before stopping. Defaults to 20. max_stalls (int, optional) – The maximum number of stalls allowed before re-planning. Defaults to 3. final_answer_prompt (str, optional) – The LLM prompt used to generate the final answer or response from the team’s transcript. A default (sensible for GPT-4o class models) is provided. custom_message_types (List[type[BaseAgentEvent | BaseChatMessage]], optional) – A list of custom message types that will be used in the group chat. If you are using custom message types or your agents produces custom message types, you need to specify them here. Make sure your custom message types are subclasses of BaseAgentEvent or BaseChatMessage. emit_team_events (bool, optional) – Whether to emit team events through BaseGroupChat.run_stream(). Defaults to False. Raises: ValueError – In orchestration logic if progress ledger does not have required keys or if next speaker is not valid. Examples: MagenticOneGroupChat with one assistant agent: import asyncio from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.teams import MagenticOneGroupChat from autogen_agentchat.ui import Console async def main() -> None: model_client = OpenAIChatCompletionClient(model="gpt-4o") assistant = AssistantAgent( "Assistant", model_client=model_client, ) team = MagenticOneGroupChat([assistant], model_client=model_client) await Console(team.run_stream(task="Provide a different proof to Fermat last theorem")) asyncio.run(main()) References If you use the MagenticOneGroupChat in your work, please cite the following paper: @article{fourney2024magentic, title={Magentic-one: A generalist multi-agent system for solving complex tasks}, author={Fourney, Adam and Bansal, Gagan and Mozannar, Hussein and Tan, Cheng and Salinas, Eduardo and Niedtner, Friederike and Proebsting, Grace and Bassman, Griffin and Gerrits, Jack and Alber, Jacob and others}, journal={arXiv preprint arXiv:2411.04468}, year={2024} } component_config_schema# alias of MagenticOneGroupChatConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.MagenticOneGroupChat'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'MagenticOneGroupChat'# DEFAULT_DESCRIPTION = 'A team of agents.'# _to_config() → MagenticOneGroupChatConfig[source]# Dump the configuration that would be requite to create a new instance of a component matching the configuration of this instance. Returns: T – The configuration of the component. classmethod _from_config(config: MagenticOneGroupChatConfig) → Self[source]# Create a new instance of the component from a configuration object. Parameters: config (T) – The configuration object. Returns: Self – The new instance of the component. class DiGraphBuilder[source]# Bases: object A fluent builder for constructing DiGraph execution graphs used in GraphFlow. Warning This is an experimental feature, and the API will change in the future releases. This utility provides a convenient way to programmatically build a graph of agent interactions, including complex execution flows such as: Sequential chains Parallel fan-outs Conditional branching Cyclic loops with safe exits Each node in the graph represents an agent. Edges define execution paths between agents, and can optionally be conditioned on message content using callable functions. The builder is compatible with the Graph runner and supports both standard and filtered agents. - add_node(agent, activation) Add an agent node to the graph. - add_edge(source, target, condition) Connect two nodes optionally with a condition. - add_conditional_edges(source, condition_to_target) Add multiple conditional edges from a source. - set_entry_point(agent) Define the default start node (optional). - build() Generate a validated DiGraph. - get_participants() Return the list of added agents. Example — Sequential Flow A → B → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c) >>> team = Graph( ... participants=builder.get_participants(), ... graph=builder.build(), ... termination_condition=MaxMessageTermination(5), ... ) Example — Parallel Fan-out A → (B, C):>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c) Example — Conditional Branching A → B or A → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> # Add conditional edges using keyword check >>> builder.add_edge(agent_a, agent_b, condition="keyword1") >>> builder.add_edge(agent_a, agent_c, condition="keyword2") Example — Using Custom String Conditions:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> # Add condition strings to check in messages >>> builder.add_edge(agent_a, agent_b, condition="big") >>> builder.add_edge(agent_a, agent_c, condition="small") Example — Loop: A → B → A or B → C:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b) >> # Add a loop back to agent A >>> builder.add_edge(agent_b, agent_a, condition=lambda msg: "loop" in msg.to_model_text()) >>> # Add exit condition to break the loop >>> builder.add_edge(agent_b, agent_c, condition=lambda msg: "loop" not in msg.to_model_text()) Example — Loop with multiple paths to the same node: A → B → C → B:>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) >>> builder.add_edge(agent_a, agent_b) >>> builder.add_edge(agent_b, agent_c) >>> builder.add_edge(agent_c, agent_b, activation_group="loop_back") Example — Loop with multiple paths to the same node with any activation condition: A → B → (C1, C2) → B → E(exit):>>> builder = GraphBuilder() >>> builder.add_node(agent_a).add_node(agent_b).add_node(agent_c1).add_node(agent_c2).add_node(agent_e) >>> builder.add_edge(agent_a, agent_b) >>> builder.add_edge(agent_b, agent_c1) >>> builder.add_edge(agent_b, agent_c2) >>> builder.add_edge(agent_b, agent_e, condition="exit") >>> builder.add_edge(agent_c1, agent_b, activation_group="loop_back_group", activation_condition="any") >>> builder.add_edge(agent_c2, agent_b, activation_group="loop_back_group", activation_condition="any") add_node(agent: ChatAgent, activation: Literal['all', 'any'] = 'all') → DiGraphBuilder[source]# Add a node to the graph and register its agent. add_edge(source: str | ChatAgent, target: str | ChatAgent, condition: str | Callable[[BaseChatMessage], bool] | None = None, activation_group: str | None = None, activation_condition: Literal['all', 'any'] | None = None) → DiGraphBuilder[source]# Add a directed edge from source to target, optionally with a condition. Parameters: source – Source node (agent name or agent object) target – Target node (agent name or agent object) condition – Optional condition for edge activation. If string, activates when substring is found in message. If callable, activates when function returns True for the message. Returns: Self for method chaining Raises: ValueError – If source or target node doesn’t exist in the builder add_conditional_edges(source: str | ChatAgent, condition_to_target: Dict[str, str | ChatAgent]) → DiGraphBuilder[source]# Add multiple conditional edges from a source node based on keyword checks. Warning This method interface will be changed in the future to support callable conditions. Please use add_edge if you need to specify custom conditions. Parameters: source – Source node (agent name or agent object) condition_to_target – Mapping from condition strings to target nodes Each key is a keyword that will be checked in the message content Each value is the target node to activate when condition is met For each key (keyword), a lambda will be created that checks if the keyword is in the message text. Returns: Self for method chaining set_entry_point(name: str | ChatAgent) → DiGraphBuilder[source]# Set the default start node of the graph. build() → DiGraph[source]# Build and validate the DiGraph. get_participants() → list[ChatAgent][source]# Return the list of agents in the builder, in insertion order. pydantic model DiGraph[source]# Bases: BaseModel Defines a directed graph structure with nodes and edges. GraphFlow uses this to determine execution order and conditions. Warning This is an experimental feature, and the API will change in the future releases. Show JSON schema{ "title": "DiGraph", "type": "object", "properties": { "nodes": { "default": null, "title": "Nodes" }, "default_start_node": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Default Start Node" } } } Fields: default_start_node (str | None) nodes (Dict[str, autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphNode]) field nodes: Dict[str, DiGraphNode] [Required]# field default_start_node: str | None = None# get_parents() → Dict[str, List[str]][source]# Compute a mapping of each node to its parent nodes. get_start_nodes() → Set[str][source]# Return the nodes that have no incoming edges (entry points). get_leaf_nodes() → Set[str][source]# Return nodes that have no outgoing edges (final output nodes). has_cycles_with_exit() → bool[source]# Check if the graph has any cycles and validate that each cycle has at least one conditional edge. Returns: bool – True if there is at least one cycle and all cycles have an exit condition. False if there are no cycles. Raises: ValueError – If there is a cycle without any conditional edge. get_has_cycles() → bool[source]# Indicates if the graph has at least one cycle (with valid exit conditions). graph_validate() → None[source]# Validate graph structure and execution rules. get_remaining_map() → Dict[str, Dict[str, int]][source]# Get the remaining map that tracks how many edges point to each target node with each activation group. Returns: Dictionary mapping target nodes to their activation groups and remaining counts model_post_init(context: Any, /) → None# This function is meant to behave like a BaseModel method to initialise private attributes. It takes context as an argument since that’s what pydantic-core passes when calling it. Parameters: self – The BaseModel instance. context – The context. pydantic model DiGraphNode[source]# Bases: BaseModel Represents a node (agent) in a DiGraph, with its outgoing edges and activation type. Warning This is an experimental feature, and the API will change in the future releases. Show JSON schema{ "title": "DiGraphNode", "type": "object", "properties": { "name": { "title": "Name", "type": "string" }, "edges": { "default": null, "title": "Edges" }, "activation": { "default": "all", "enum": [ "all", "any" ], "title": "Activation", "type": "string" } }, "required": [ "name" ] } Fields: activation (Literal['all', 'any']) edges (List[autogen_agentchat.teams._group_chat._graph._digraph_group_chat.DiGraphEdge]) name (str) field name: str [Required]# field edges: List[DiGraphEdge] = []# field activation: Literal['all', 'any'] = 'all'# pydantic model DiGraphEdge[source]# Bases: BaseModel Represents a directed edge in a DiGraph, with an optional execution condition. Warning This is an experimental feature, and the API will change in the future releases. Warning If the condition is a callable, it will not be serialized in the model. Show JSON schema{ "title": "DiGraphEdge", "type": "object", "properties": { "target": { "title": "Target", "type": "string" }, "condition": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Condition" }, "condition_function": { "default": null, "title": "Condition Function" }, "activation_group": { "default": "", "title": "Activation Group", "type": "string" }, "activation_condition": { "default": "all", "enum": [ "all", "any" ], "title": "Activation Condition", "type": "string" } }, "required": [ "target" ] } Fields: activation_condition (Literal['all', 'any']) activation_group (str) condition (str | Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None) condition_function (Callable[[autogen_agentchat.messages.BaseChatMessage], bool] | None) target (str) Validators: _validate_condition » all fields field target: str [Required]# Validated by: _validate_condition field condition: str | Callable[[BaseChatMessage], bool] | None = None# (Experimental) Condition to execute this edge. If None, the edge is unconditional. If a string, the edge is conditional on the presence of that string in the last agent chat message. If a callable, the edge is conditional on the callable returning True when given the last message. Validated by: _validate_condition field condition_function: Callable[[BaseChatMessage], bool] | None = None# Validated by: _validate_condition field activation_group: str = ''# Group identifier for forward dependencies. When multiple edges point to the same target node, they are grouped by this field. This allows distinguishing between different cycles or dependency patterns. Example: In a graph containing a cycle like A->B->C->B, the two edges pointing to B (A->B and C->B) can be in different activation groups to control how B is activated. Defaults to the target node name if not specified. Validated by: _validate_condition field activation_condition: Literal['all', 'any'] = 'all'# Determines how forward dependencies within the same activation_group are evaluated. “all”: All edges in this activation group must be satisfied before the target node can execute “any”: Any single edge in this activation group being satisfied allows the target node to execute This is used to handle complex dependency patterns in cyclic graphs where multiple paths can lead to the same target node. Validated by: _validate_condition check_condition(message: BaseChatMessage) → bool[source]# Check if the edge condition is satisfied for the given message. Parameters: message – The message to check the condition against. Returns: True if condition is satisfied (None condition always returns True) False otherwise. class GraphFlow(participants: List[ChatAgent], graph: DiGraph, *, name: str | None = None, description: str | None = None, termination_condition: TerminationCondition | None = None, max_turns: int | None = None, runtime: AgentRuntime | None = None, custom_message_types: List[type[BaseAgentEvent | BaseChatMessage]] | None = None)[source]# Bases: BaseGroupChat, Component[GraphFlowConfig] A team that runs a group chat following a Directed Graph execution pattern. Warning This is an experimental feature, and the API will change in the future releases. This group chat executes agents based on a directed graph (DiGraph) structure, allowing complex workflows such as sequential execution, parallel fan-out, conditional branching, join patterns, and loops with explicit exit conditions. The execution order is determined by the edges defined in the DiGraph. Each node in the graph corresponds to an agent, and edges define the flow of messages between agents. Nodes can be configured to activate when: All parent nodes have completed (activation=”all”) → default Any parent node completes (activation=”any”) Conditional branching is supported using edge conditions, where the next agent(s) are selected based on content in the chat history. Loops are permitted as long as there is a condition that eventually exits the loop. Note Use the DiGraphBuilder class to create a DiGraph easily. It provides a fluent API for adding nodes and edges, setting entry points, and validating the graph structure. See the DiGraphBuilder documentation for more details. The GraphFlow class is designed to be used with the DiGraphBuilder for creating complex workflows. Warning When using callable conditions in edges, they will not be serialized when calling dump_component(). This will be addressed in future releases. Parameters: participants (List[ChatAgent]) – The participants in the group chat. termination_condition (TerminationCondition, optional) – Termination condition for the chat. max_turns (int, optional) – Maximum number of turns before forcing termination. graph (DiGraph) – Directed execution graph defining node flow and conditions. Raises: ValueError – If participant names are not unique, or if graph validation fails (e.g., cycles without exit). Examples Sequential Flow: A → B → C import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano") agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.") agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.") agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to English.") # Create a directed graph with sequential flow A -> B -> C. builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b).add_edge(agent_b, agent_c) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task="Write a short story about a cat."): print(event) asyncio.run(main()) Parallel Fan-out: A → (B, C) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano") agent_a = AssistantAgent("A", model_client=model_client, system_message="You are a helpful assistant.") agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to Chinese.") agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Japanese.") # Create a directed graph with fan-out flow A -> (B, C). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task="Write a short story about a cat."): print(event) asyncio.run(main()) Conditional Branching: A → B (if ‘yes’) or C (otherwise) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model="gpt-4.1-nano") agent_a = AssistantAgent( "A", model_client=model_client, system_message="Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.", ) agent_b = AssistantAgent("B", model_client=model_client, system_message="Translate input to English.") agent_c = AssistantAgent("C", model_client=model_client, system_message="Translate input to Chinese.") # Create a directed graph with conditional branching flow A -> B ("yes"), A -> C (otherwise). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) # Create conditions as callables that check the message content. builder.add_edge(agent_a, agent_b, condition=lambda msg: "yes" in msg.to_model_text()) builder.add_edge(agent_a, agent_c, condition=lambda msg: "yes" not in msg.to_model_text()) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(5), ) # Run the team and print the events. async for event in team.run_stream(task="AutoGen is a framework for building AI agents."): print(event) asyncio.run(main()) Loop with exit condition: A → B → C (if ‘APPROVE’) or A (otherwise) import asyncio from autogen_agentchat.agents import AssistantAgent from autogen_agentchat.conditions import MaxMessageTermination from autogen_agentchat.teams import DiGraphBuilder, GraphFlow from autogen_ext.models.openai import OpenAIChatCompletionClient async def main(): # Initialize agents with OpenAI model clients. model_client = OpenAIChatCompletionClient(model="gpt-4.1") agent_a = AssistantAgent( "A", model_client=model_client, system_message="You are a helpful assistant.", ) agent_b = AssistantAgent( "B", model_client=model_client, system_message="Provide feedback on the input, if your feedback has been addressed, " "say 'APPROVE', otherwise provide a reason for rejection.", ) agent_c = AssistantAgent( "C", model_client=model_client, system_message="Translate the final product to Korean." ) # Create a loop graph with conditional exit: A -> B -> C ("APPROVE"), B -> A (otherwise). builder = DiGraphBuilder() builder.add_node(agent_a).add_node(agent_b).add_node(agent_c) builder.add_edge(agent_a, agent_b) # Create conditional edges using strings builder.add_edge(agent_b, agent_c, condition=lambda msg: "APPROVE" in msg.to_model_text()) builder.add_edge(agent_b, agent_a, condition=lambda msg: "APPROVE" not in msg.to_model_text()) builder.set_entry_point(agent_a) graph = builder.build() # Create a GraphFlow team with the directed graph. team = GraphFlow( participants=[agent_a, agent_b, agent_c], graph=graph, termination_condition=MaxMessageTermination(20), # Max 20 messages to avoid infinite loop. ) # Run the team and print the events. async for event in team.run_stream(task="Write a short poem about AI Agents."): print(event) asyncio.run(main()) component_config_schema# alias of GraphFlowConfig component_provider_override: ClassVar[str | None] = 'autogen_agentchat.teams.GraphFlow'# Override the provider string for the component. This should be used to prevent internal module names being a part of the module name. DEFAULT_NAME = 'GraphFlow'# DEFAULT_DESCRIPTION = 'A team of agents'# previous autogen_agentchat.state next autogen_agentchat.tools On this page BaseGroupChat BaseGroupChat.component_type BaseGroupChat.name BaseGroupChat.description BaseGroupChat.run() BaseGroupChat.run_stream() BaseGroupChat.reset() BaseGroupChat.pause() BaseGroupChat.resume() BaseGroupChat.save_state() BaseGroupChat.load_state() RoundRobinGroupChat RoundRobinGroupChat.component_config_schema RoundRobinGroupChat.component_provider_override RoundRobinGroupChat.DEFAULT_NAME RoundRobinGroupChat.DEFAULT_DESCRIPTION RoundRobinGroupChat._to_config() RoundRobinGroupChat._from_config() SelectorGroupChat SelectorGroupChat.component_config_schema SelectorGroupChat.component_provider_override SelectorGroupChat.DEFAULT_NAME SelectorGroupChat.DEFAULT_DESCRIPTION SelectorGroupChat._to_config() SelectorGroupChat._from_config() Swarm Swarm.component_config_schema Swarm.component_provider_override Swarm.DEFAULT_NAME Swarm.DEFAULT_DESCRIPTION Swarm._to_config() Swarm._from_config() MagenticOneGroupChat MagenticOneGroupChat.component_config_schema MagenticOneGroupChat.component_provider_override MagenticOneGroupChat.DEFAULT_NAME MagenticOneGroupChat.DEFAULT_DESCRIPTION MagenticOneGroupChat._to_config() MagenticOneGroupChat._from_config() DiGraphBuilder DiGraphBuilder.add_node() DiGraphBuilder.add_edge() DiGraphBuilder.add_conditional_edges() DiGraphBuilder.set_entry_point() DiGraphBuilder.build() DiGraphBuilder.get_participants() DiGraph DiGraph.nodes DiGraph.default_start_node DiGraph.get_parents() DiGraph.get_start_nodes() DiGraph.get_leaf_nodes() DiGraph.has_cycles_with_exit() DiGraph.get_has_cycles() DiGraph.graph_validate() DiGraph.get_remaining_map() DiGraph.model_post_init() DiGraphNode DiGraphNode.name DiGraphNode.edges DiGraphNode.activation DiGraphEdge DiGraphEdge.target DiGraphEdge.condition DiGraphEdge.condition_function DiGraphEdge.activation_group DiGraphEdge.activation_condition DiGraphEdge.check_condition() GraphFlow GraphFlow.component_config_schema GraphFlow.component_provider_override GraphFlow.DEFAULT_NAME GraphFlow.DEFAULT_DESCRIPTION Edit on GitHub Show Source

```
Team
```

**Pattern 7:** Example: In a graph containing a cycle like A->B->C->B, the two edges pointing to B (A->B and C->B) can be in different activation groups to control how B is activated. Defaults to the target node name if not specified.

```
_validate_condition
```

**Pattern 8:** API Reference autogen_core.models autogen_core.models# class ModelCapabilities(**kwargs)[source]# Bases: TypedDict vision: Required[bool]# function_calling: Required[bool]# json_output: Required[bool]# class ChatCompletionClient[source]# Bases: ComponentBase[BaseModel], ABC abstract async create(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → CreateResult[source]# Creates a single response from the model. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal["auto", "required", "none"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: CreateResult – The result of the model call. abstract create_stream(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = [], tool_choice: Tool | Literal['auto', 'required', 'none'] = 'auto', json_output: bool | type[BaseModel] | None = None, extra_create_args: Mapping[str, Any] = {}, cancellation_token: CancellationToken | None = None) → AsyncGenerator[str | CreateResult, None][source]# Creates a stream of string chunks from the model ending with a CreateResult. Parameters: messages (Sequence[LLMMessage]) – The messages to send to the model. tools (Sequence[Tool | ToolSchema], optional) – The tools to use with the model. Defaults to []. tool_choice (Tool | Literal["auto", "required", "none"], optional) – A single Tool object to force the model to use, “auto” to let the model choose any available tool, “required” to force tool usage, or “none” to disable tool usage. Defaults to “auto”. json_output (Optional[bool | type[BaseModel]], optional) – Whether to use JSON mode, structured output, or neither. Defaults to None. If set to a Pydantic BaseModel type, it will be used as the output type for structured output. If set to a boolean, it will be used to determine whether to use JSON mode or not. If set to True, make sure to instruct the model to produce JSON output in the instruction or prompt. extra_create_args (Mapping[str, Any], optional) – Extra arguments to pass to the underlying client. Defaults to {}. cancellation_token (Optional[CancellationToken], optional) – A token for cancellation. Defaults to None. Returns: AsyncGenerator[Union[str, CreateResult], None] – A generator that yields string chunks and ends with a CreateResult. abstract async close() → None[source]# abstract actual_usage() → RequestUsage[source]# abstract total_usage() → RequestUsage[source]# abstract count_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# abstract remaining_tokens(messages: Sequence[Annotated[SystemMessage | UserMessage | AssistantMessage | FunctionExecutionResultMessage, FieldInfo(annotation=NoneType, required=True, discriminator='type')]], *, tools: Sequence[Tool | ToolSchema] = []) → int[source]# abstract property capabilities: ModelCapabilities# abstract property model_info: ModelInfo# pydantic model SystemMessage[source]# Bases: BaseModel System message contains instructions for the model coming from the developer. Note Open AI is moving away from using ‘system’ role in favor of ‘developer’ role. See Model Spec for more details. However, the ‘system’ role is still allowed in their API and will be automatically converted to ‘developer’ role on the server side. So, you can use SystemMessage for developer messages. Show JSON schema{ "title": "SystemMessage", "description": "System message contains instructions for the model coming from the developer.\n\n.. note::\n\n Open AI is moving away from using 'system' role in favor of 'developer' role.\n See `Model Spec <https://cdn.openai.com/spec/model-spec-2024-05-08.html#definitions>`_ for more details.\n However, the 'system' role is still allowed in their API and will be automatically converted to 'developer' role\n on the server side.\n So, you can use `SystemMessage` for developer messages.", "type": "object", "properties": { "content": { "title": "Content", "type": "string" }, "type": { "const": "SystemMessage", "default": "SystemMessage", "title": "Type", "type": "string" } }, "required": [ "content" ] } Fields: content (str) type (Literal['SystemMessage']) field content: str [Required]# The content of the message. field type: Literal['SystemMessage'] = 'SystemMessage'# pydantic model UserMessage[source]# Bases: BaseModel User message contains input from end users, or a catch-all for data provided to the model. Show JSON schema{ "title": "UserMessage", "description": "User message contains input from end users, or a catch-all for data provided to the model.", "type": "object", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "anyOf": [ { "type": "string" }, {} ] }, "type": "array" } ], "title": "Content" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "UserMessage", "default": "UserMessage", "title": "Type", "type": "string" } }, "required": [ "content", "source" ] } Fields: content (str | List[str | autogen_core._image.Image]) source (str) type (Literal['UserMessage']) field content: str | List[str | Image] [Required]# The content of the message. field source: str [Required]# The name of the agent that sent this message. field type: Literal['UserMessage'] = 'UserMessage'# pydantic model AssistantMessage[source]# Bases: BaseModel Assistant message are sampled from the language model. Show JSON schema{ "title": "AssistantMessage", "description": "Assistant message are sampled from the language model.", "type": "object", "properties": { "content": { "anyOf": [ { "type": "string" }, { "items": { "$ref": "#/$defs/FunctionCall" }, "type": "array" } ], "title": "Content" }, "thought": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Thought" }, "source": { "title": "Source", "type": "string" }, "type": { "const": "AssistantMessage", "default": "AssistantMessage", "title": "Type", "type": "string" } }, "$defs": { "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" } }, "required": [ "content", "source" ] } Fields: content (str | List[autogen_core._types.FunctionCall]) source (str) thought (str | None) type (Literal['AssistantMessage']) field content: str | List[FunctionCall] [Required]# The content of the message. field thought: str | None = None# The reasoning text for the completion if available. Used for reasoning model and additional text content besides function calls. field source: str [Required]# The name of the agent that sent this message. field type: Literal['AssistantMessage'] = 'AssistantMessage'# pydantic model FunctionExecutionResult[source]# Bases: BaseModel Function execution result contains the output of a function call. Show JSON schema{ "title": "FunctionExecutionResult", "description": "Function execution result contains the output of a function call.", "type": "object", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ] } Fields: call_id (str) content (str) is_error (bool | None) name (str) field content: str [Required]# The output of the function call. field name: str [Required]# (New in v0.4.8) The name of the function that was called. field call_id: str [Required]# The ID of the function call. Note this ID may be empty for some models. field is_error: bool | None = None# Whether the function call resulted in an error. pydantic model FunctionExecutionResultMessage[source]# Bases: BaseModel Function execution result message contains the output of multiple function calls. Show JSON schema{ "title": "FunctionExecutionResultMessage", "description": "Function execution result message contains the output of multiple function calls.", "type": "object", "properties": { "content": { "items": { "$ref": "#/$defs/FunctionExecutionResult" }, "title": "Content", "type": "array" }, "type": { "const": "FunctionExecutionResultMessage", "default": "FunctionExecutionResultMessage", "title": "Type", "type": "string" } }, "$defs": { "FunctionExecutionResult": { "description": "Function execution result contains the output of a function call.", "properties": { "content": { "title": "Content", "type": "string" }, "name": { "title": "Name", "type": "string" }, "call_id": { "title": "Call Id", "type": "string" }, "is_error": { "anyOf": [ { "type": "boolean" }, { "type": "null" } ], "default": null, "title": "Is Error" } }, "required": [ "content", "name", "call_id" ], "title": "FunctionExecutionResult", "type": "object" } }, "required": [ "content" ] } Fields: content (List[autogen_core.models._types.FunctionExecutionResult]) type (Literal['FunctionExecutionResultMessage']) field content: List[FunctionExecutionResult] [Required]# field type: Literal['FunctionExecutionResultMessage'] = 'FunctionExecutionResultMessage'# class RequestUsage(prompt_tokens: int, completion_tokens: int)[source]# Bases: object prompt_tokens: int# completion_tokens: int# pydantic model CreateResult[source]# Bases: BaseModel Create result contains the output of a model completion. Show JSON schema{ "title": "CreateResult", "description": "Create result contains the output of a model completion.", "type": "object", "properties": { "finish_reason": { "enum": [ "stop", "length", "function_calls", "content_filter", "unknown" ], "title": "Finish Reason", "type": "string" }, "content": { "anyOf": [ { "type": "string" }, { "items": { "$ref": "#/$defs/FunctionCall" }, "type": "array" } ], "title": "Content" }, "usage": { "$ref": "#/$defs/RequestUsage" }, "cached": { "title": "Cached", "type": "boolean" }, "logprobs": { "anyOf": [ { "items": { "$ref": "#/$defs/ChatCompletionTokenLogprob" }, "type": "array" }, { "type": "null" } ], "default": null, "title": "Logprobs" }, "thought": { "anyOf": [ { "type": "string" }, { "type": "null" } ], "default": null, "title": "Thought" } }, "$defs": { "ChatCompletionTokenLogprob": { "properties": { "token": { "title": "Token", "type": "string" }, "logprob": { "title": "Logprob", "type": "number" }, "top_logprobs": { "anyOf": [ { "items": { "$ref": "#/$defs/TopLogprob" }, "type": "array" }, { "type": "null" } ], "default": null, "title": "Top Logprobs" }, "bytes": { "anyOf": [ { "items": { "type": "integer" }, "type": "array" }, { "type": "null" } ], "default": null, "title": "Bytes" } }, "required": [ "token", "logprob" ], "title": "ChatCompletionTokenLogprob", "type": "object" }, "FunctionCall": { "properties": { "id": { "title": "Id", "type": "string" }, "arguments": { "title": "Arguments", "type": "string" }, "name": { "title": "Name", "type": "string" } }, "required": [ "id", "arguments", "name" ], "title": "FunctionCall", "type": "object" }, "RequestUsage": { "properties": { "prompt_tokens": { "title": "Prompt Tokens", "type": "integer" }, "completion_tokens": { "title": "Completion Tokens", "type": "integer" } }, "required": [ "prompt_tokens", "completion_tokens" ], "title": "RequestUsage", "type": "object" }, "TopLogprob": { "properties": { "logprob": { "title": "Logprob", "type": "number" }, "bytes": { "anyOf": [ { "items": { "type": "integer" }, "type": "array" }, { "type": "null" } ], "default": null, "title": "Bytes" } }, "required": [ "logprob" ], "title": "TopLogprob", "type": "object" } }, "required": [ "finish_reason", "content", "usage", "cached" ] } Fields: cached (bool) content (str | List[autogen_core._types.FunctionCall]) finish_reason (Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown']) logprobs (List[autogen_core.models._types.ChatCompletionTokenLogprob] | None) thought (str | None) usage (autogen_core.models._types.RequestUsage) field finish_reason: Literal['stop', 'length', 'function_calls', 'content_filter', 'unknown'] [Required]# The reason the model finished generating the completion. field content: str | List[FunctionCall] [Required]# The output of the model completion. field usage: RequestUsage [Required]# The usage of tokens in the prompt and completion. field cached: bool [Required]# Whether the completion was generated from a cached response. field logprobs: List[ChatCompletionTokenLogprob] | None = None# The logprobs of the tokens in the completion. field thought: str | None = None# The reasoning text for the completion if available. Used for reasoning models and additional text content besides function calls. class TopLogprob(logprob: float, bytes: List[int] | None = None)[source]# Bases: object logprob: float# bytes: List[int] | None = None# pydantic model ChatCompletionTokenLogprob[source]# Bases: BaseModel Show JSON schema{ "title": "ChatCompletionTokenLogprob", "type": "object", "properties": { "token": { "title": "Token", "type": "string" }, "logprob": { "title": "Logprob", "type": "number" }, "top_logprobs": { "anyOf": [ { "items": { "$ref": "#/$defs/TopLogprob" }, "type": "array" }, { "type": "null" } ], "default": null, "title": "Top Logprobs" }, "bytes": { "anyOf": [ { "items": { "type": "integer" }, "type": "array" }, { "type": "null" } ], "default": null, "title": "Bytes" } }, "$defs": { "TopLogprob": { "properties": { "logprob": { "title": "Logprob", "type": "number" }, "bytes": { "anyOf": [ { "items": { "type": "integer" }, "type": "array" }, { "type": "null" } ], "default": null, "title": "Bytes" } }, "required": [ "logprob" ], "title": "TopLogprob", "type": "object" } }, "required": [ "token", "logprob" ] } Fields: bytes (List[int] | None) logprob (float) token (str) top_logprobs (List[autogen_core.models._types.TopLogprob] | None) field token: str [Required]# field logprob: float [Required]# field top_logprobs: List[TopLogprob] | None = None# field bytes: List[int] | None = None# class ModelFamily(*args: Any, **kwargs: Any)[source]# Bases: object A model family is a group of models that share similar characteristics from a capabilities perspective. This is different to discrete supported features such as vision, function calling, and JSON output. This namespace class holds constants for the model families that AutoGen understands. Other families definitely exist and can be represented by a string, however, AutoGen will treat them as unknown. GPT_5 = 'gpt-5'# GPT_41 = 'gpt-41'# GPT_45 = 'gpt-45'# GPT_4O = 'gpt-4o'# O1 = 'o1'# O3 = 'o3'# O4 = 'o4'# GPT_4 = 'gpt-4'# GPT_35 = 'gpt-35'# R1 = 'r1'# GEMINI_1_5_FLASH = 'gemini-1.5-flash'# GEMINI_1_5_PRO = 'gemini-1.5-pro'# GEMINI_2_0_FLASH = 'gemini-2.0-flash'# GEMINI_2_5_PRO = 'gemini-2.5-pro'# GEMINI_2_5_FLASH = 'gemini-2.5-flash'# CLAUDE_3_HAIKU = 'claude-3-haiku'# CLAUDE_3_SONNET = 'claude-3-sonnet'# CLAUDE_3_OPUS = 'claude-3-opus'# CLAUDE_3_5_HAIKU = 'claude-3-5-haiku'# CLAUDE_3_5_SONNET = 'claude-3-5-sonnet'# CLAUDE_3_7_SONNET = 'claude-3-7-sonnet'# CLAUDE_4_OPUS = 'claude-4-opus'# CLAUDE_4_SONNET = 'claude-4-sonnet'# LLAMA_3_3_8B = 'llama-3.3-8b'# LLAMA_3_3_70B = 'llama-3.3-70b'# LLAMA_4_SCOUT = 'llama-4-scout'# LLAMA_4_MAVERICK = 'llama-4-maverick'# CODESRAL = 'codestral'# OPEN_CODESRAL_MAMBA = 'open-codestral-mamba'# MISTRAL = 'mistral'# MINISTRAL = 'ministral'# PIXTRAL = 'pixtral'# UNKNOWN = 'unknown'# ANY# alias of Literal[‘gpt-5’, ‘gpt-41’, ‘gpt-45’, ‘gpt-4o’, ‘o1’, ‘o3’, ‘o4’, ‘gpt-4’, ‘gpt-35’, ‘r1’, ‘gemini-1.5-flash’, ‘gemini-1.5-pro’, ‘gemini-2.0-flash’, ‘gemini-2.5-pro’, ‘gemini-2.5-flash’, ‘claude-3-haiku’, ‘claude-3-sonnet’, ‘claude-3-opus’, ‘claude-3-5-haiku’, ‘claude-3-5-sonnet’, ‘claude-3-7-sonnet’, ‘claude-4-opus’, ‘claude-4-sonnet’, ‘llama-3.3-8b’, ‘llama-3.3-70b’, ‘llama-4-scout’, ‘llama-4-maverick’, ‘codestral’, ‘open-codestral-mamba’, ‘mistral’, ‘ministral’, ‘pixtral’, ‘unknown’] static is_claude(family: str) → bool[source]# static is_gemini(family: str) → bool[source]# static is_openai(family: str) → bool[source]# static is_llama(family: str) → bool[source]# static is_mistral(family: str) → bool[source]# class ModelInfo[source]# Bases: TypedDict ModelInfo is a dictionary that contains information about a model’s properties. It is expected to be used in the model_info property of a model client. We are expecting this to grow over time as we add more features. vision: Required[bool]# True if the model supports vision, aka image input, otherwise False. function_calling: Required[bool]# True if the model supports function calling, otherwise False. json_output: Required[bool]# this is different to structured json. Type: True if the model supports json output, otherwise False. Note family: Required[Literal['gpt-5', 'gpt-41', 'gpt-45', 'gpt-4o', 'o1', 'o3', 'o4', 'gpt-4', 'gpt-35', 'r1', 'gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-2.5-pro', 'gemini-2.5-flash', 'claude-3-haiku', 'claude-3-sonnet', 'claude-3-opus', 'claude-3-5-haiku', 'claude-3-5-sonnet', 'claude-3-7-sonnet', 'claude-4-opus', 'claude-4-sonnet', 'llama-3.3-8b', 'llama-3.3-70b', 'llama-4-scout', 'llama-4-maverick', 'codestral', 'open-codestral-mamba', 'mistral', 'ministral', 'pixtral', 'unknown'] | str]# Model family should be one of the constants from ModelFamily or a string representing an unknown model family. structured_output: Required[bool]# True if the model supports structured output, otherwise False. This is different to json_output. multiple_system_messages: bool | None# True if the model supports multiple, non-consecutive system messages, otherwise False. validate_model_info(model_info: ModelInfo) → None[source]# Validates the model info dictionary. Raises: ValueError – If the model info dictionary is missing required fields. previous autogen_core.model_context next autogen_core.tool_agent On this page ModelCapabilities ModelCapabilities.vision ModelCapabilities.function_calling ModelCapabilities.json_output ChatCompletionClient ChatCompletionClient.create() ChatCompletionClient.create_stream() ChatCompletionClient.close() ChatCompletionClient.actual_usage() ChatCompletionClient.total_usage() ChatCompletionClient.count_tokens() ChatCompletionClient.remaining_tokens() ChatCompletionClient.capabilities ChatCompletionClient.model_info SystemMessage SystemMessage.content SystemMessage.type UserMessage UserMessage.content UserMessage.source UserMessage.type AssistantMessage AssistantMessage.content AssistantMessage.thought AssistantMessage.source AssistantMessage.type FunctionExecutionResult FunctionExecutionResult.content FunctionExecutionResult.name FunctionExecutionResult.call_id FunctionExecutionResult.is_error FunctionExecutionResultMessage FunctionExecutionResultMessage.content FunctionExecutionResultMessage.type RequestUsage RequestUsage.prompt_tokens RequestUsage.completion_tokens CreateResult CreateResult.finish_reason CreateResult.content CreateResult.usage CreateResult.cached CreateResult.logprobs CreateResult.thought TopLogprob TopLogprob.logprob TopLogprob.bytes ChatCompletionTokenLogprob ChatCompletionTokenLogprob.token ChatCompletionTokenLogprob.logprob ChatCompletionTokenLogprob.top_logprobs ChatCompletionTokenLogprob.bytes ModelFamily ModelFamily.GPT_5 ModelFamily.GPT_41 ModelFamily.GPT_45 ModelFamily.GPT_4O ModelFamily.O1 ModelFamily.O3 ModelFamily.O4 ModelFamily.GPT_4 ModelFamily.GPT_35 ModelFamily.R1 ModelFamily.GEMINI_1_5_FLASH ModelFamily.GEMINI_1_5_PRO ModelFamily.GEMINI_2_0_FLASH ModelFamily.GEMINI_2_5_PRO ModelFamily.GEMINI_2_5_FLASH ModelFamily.CLAUDE_3_HAIKU ModelFamily.CLAUDE_3_SONNET ModelFamily.CLAUDE_3_OPUS ModelFamily.CLAUDE_3_5_HAIKU ModelFamily.CLAUDE_3_5_SONNET ModelFamily.CLAUDE_3_7_SONNET ModelFamily.CLAUDE_4_OPUS ModelFamily.CLAUDE_4_SONNET ModelFamily.LLAMA_3_3_8B ModelFamily.LLAMA_3_3_70B ModelFamily.LLAMA_4_SCOUT ModelFamily.LLAMA_4_MAVERICK ModelFamily.CODESRAL ModelFamily.OPEN_CODESRAL_MAMBA ModelFamily.MISTRAL ModelFamily.MINISTRAL ModelFamily.PIXTRAL ModelFamily.UNKNOWN ModelFamily.ANY ModelFamily.is_claude() ModelFamily.is_gemini() ModelFamily.is_openai() ModelFamily.is_llama() ModelFamily.is_mistral() ModelInfo ModelInfo.vision ModelInfo.function_calling ModelInfo.json_output ModelInfo.family ModelInfo.structured_output ModelInfo.multiple_system_messages validate_model_info() Edit on GitHub Show Source

```
TypedDict
```

### Example Code Patterns

**Example 1** (python):
```python
def get_token(
    self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None, **kwargs: Any
) -> azure.core.credentials.AccessToken
```

**Example 2** (python):
```python
def get_token(
    self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None, **kwargs: Any
) -> azure.core.credentials.AccessToken
```

**Example 3** (json):
```json
[project]
# ...
dependencies = [
    "autogen-core>=0.4,<0.5"
]
```

**Example 4** (json):
```json
[project]
# ...
dependencies = [
    "autogen-core>=0.4,<0.5"
]
```

## Reference Files

This skill includes comprehensive documentation in `references/`:

- **advanced.md** - Advanced documentation
- **agentchat.md** - Agentchat documentation
- **agents.md** - Agents documentation
- **core.md** - Core documentation
- **extensions.md** - Extensions documentation
- **getting_started.md** - Getting Started documentation
- **memory.md** - Memory documentation
- **messages.md** - Messages documentation
- **models.md** - Models documentation
- **other.md** - Other documentation
- **studio.md** - Studio documentation
- **teams.md** - Teams documentation
- **tools.md** - Tools documentation

Use `view` to read specific reference files when detailed information is needed.

## Working with This Skill

### For Beginners
Start with the getting_started or tutorials reference files for foundational concepts.

### For Specific Features
Use the appropriate category reference file (api, guides, etc.) for detailed information.

### For Code Examples
The quick reference section above contains common patterns extracted from the official docs.

## Resources

### references/
Organized documentation extracted from official sources. These files contain:
- Detailed explanations
- Code examples with language annotations
- Links to original documentation
- Table of contents for quick navigation

### scripts/
Add helper scripts here for common automation tasks.

### assets/
Add templates, boilerplate, or example projects here.

## Notes

- This skill was automatically generated from official documentation
- Reference files preserve the structure and examples from source docs
- Code examples include language detection for better syntax highlighting
- Quick reference patterns are extracted from common usage examples in the docs

## Updating

To refresh this skill with updated documentation:
1. Re-run the scraper with the same configuration
2. The skill will be rebuilt with the latest information
