# Langchain - Models

**Pages:** 959

---

## 1. Create and/or select your dataset

**URL:** llms-txt#1.-create-and/or-select-your-dataset

ls_client = Client()
dataset = ls_client.clone_public_dataset(
    "https://smith.langchain.com/public/a63525f9-bdf2-4512-83e3-077dc9417f96/d"
)

---

## 1. Specify config schema

**URL:** llms-txt#1.-specify-config-schema

class ContextSchema(TypedDict):
    my_runtime_value: str

---

## 2. Define an evaluator

**URL:** llms-txt#2.-define-an-evaluator

def is_concise(outputs: dict, reference_outputs: dict) -> bool:
    return len(outputs["answer"]) < (3 * len(reference_outputs["answer"]))

---

## 2. Define a graph that accesses the config in a node

**URL:** llms-txt#2.-define-a-graph-that-accesses-the-config-in-a-node

class State(TypedDict):
    my_state_value: str

def node(state: State, runtime: Runtime[ContextSchema]):  # [!code highlight]
    if runtime.context["my_runtime_value"] == "a":  # [!code highlight]
        return {"my_state_value": 1}
    elif runtime.context["my_runtime_value"] == "b":  # [!code highlight]
        return {"my_state_value": 2}
    else:
        raise ValueError("Unknown values.")

builder = StateGraph(State, context_schema=ContextSchema)  # [!code highlight]
builder.add_node(node)
builder.add_edge(START, "node")
builder.add_edge("node", END)

graph = builder.compile()

---

## 3. Define the interface to your app

**URL:** llms-txt#3.-define-the-interface-to-your-app

def chatbot(inputs: dict) -> dict:
    return {"answer": inputs["question"] + " is a good question. I don't know the answer."}

---

## 3. Pass in configuration at runtime:

**URL:** llms-txt#3.-pass-in-configuration-at-runtime:

**Contents:**
- Add retry policies
- Add node caching
- Create a sequence of steps

print(graph.invoke({}, context={"my_runtime_value": "a"}))  # [!code highlight]
print(graph.invoke({}, context={"my_runtime_value": "b"}))  # [!code highlight]

{'my_state_value': 1}
{'my_state_value': 2}
python  theme={null}
  from dataclasses import dataclass

from langchain.chat_models import init_chat_model
  from langgraph.graph import MessagesState, END, StateGraph, START
  from langgraph.runtime import Runtime
  from typing_extensions import TypedDict

@dataclass
  class ContextSchema:
      model_provider: str = "anthropic"

MODELS = {
      "anthropic": init_chat_model("claude-haiku-4-5-20251001"),
      "openai": init_chat_model("gpt-4.1-mini"),
  }

def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):
      model = MODELS[runtime.context.model_provider]
      response = model.invoke(state["messages"])
      return {"messages": [response]}

builder = StateGraph(MessagesState, context_schema=ContextSchema)
  builder.add_node("model", call_model)
  builder.add_edge(START, "model")
  builder.add_edge("model", END)

graph = builder.compile()

# Usage
  input_message = {"role": "user", "content": "hi"}
  # With no configuration, uses default (Anthropic)
  response_1 = graph.invoke({"messages": [input_message]}, context=ContextSchema())["messages"][-1]
  # Or, can set OpenAI
  response_2 = graph.invoke({"messages": [input_message]}, context={"model_provider": "openai"})["messages"][-1]

print(response_1.response_metadata["model_name"])
  print(response_2.response_metadata["model_name"])
  
  claude-haiku-4-5-20251001
  gpt-4.1-mini-2025-04-14
  python  theme={null}
  from dataclasses import dataclass
  from langchain.chat_models import init_chat_model
  from langchain.messages import SystemMessage
  from langgraph.graph import END, MessagesState, StateGraph, START
  from langgraph.runtime import Runtime
  from typing_extensions import TypedDict

@dataclass
  class ContextSchema:
      model_provider: str = "anthropic"
      system_message: str | None = None

MODELS = {
      "anthropic": init_chat_model("claude-haiku-4-5-20251001"),
      "openai": init_chat_model("gpt-4.1-mini"),
  }

def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):
      model = MODELS[runtime.context.model_provider]
      messages = state["messages"]
      if (system_message := runtime.context.system_message):
          messages = [SystemMessage(system_message)] + messages
      response = model.invoke(messages)
      return {"messages": [response]}

builder = StateGraph(MessagesState, context_schema=ContextSchema)
  builder.add_node("model", call_model)
  builder.add_edge(START, "model")
  builder.add_edge("model", END)

graph = builder.compile()

# Usage
  input_message = {"role": "user", "content": "hi"}
  response = graph.invoke({"messages": [input_message]}, context={"model_provider": "openai", "system_message": "Respond in Italian."})
  for message in response["messages"]:
      message.pretty_print()
  
  ================================ Human Message ================================

hi
  ================================== Ai Message ==================================

Ciao! Come posso aiutarti oggi?
  python  theme={null}
from langgraph.types import RetryPolicy

builder.add_node(
    "node_name",
    node_function,
    retry_policy=RetryPolicy(),
)
python  theme={null}
  import sqlite3
  from typing_extensions import TypedDict
  from langchain.chat_models import init_chat_model
  from langgraph.graph import END, MessagesState, StateGraph, START
  from langgraph.types import RetryPolicy
  from langchain_community.utilities import SQLDatabase
  from langchain.messages import AIMessage

db = SQLDatabase.from_uri("sqlite:///:memory:")
  model = init_chat_model("claude-haiku-4-5-20251001")

def query_database(state: MessagesState):
      query_result = db.run("SELECT * FROM Artist LIMIT 10;")
      return {"messages": [AIMessage(content=query_result)]}

def call_model(state: MessagesState):
      response = model.invoke(state["messages"])
      return {"messages": [response]}

# Define a new graph
  builder = StateGraph(MessagesState)
  builder.add_node(
      "query_database",
      query_database,
      retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),
  )
  builder.add_node("model", call_model, retry_policy=RetryPolicy(max_attempts=5))
  builder.add_edge(START, "model")
  builder.add_edge("model", "query_database")
  builder.add_edge("query_database", END)
  graph = builder.compile()
  python  theme={null}
from langgraph.types import CachePolicy

builder.add_node(
    "node_name",
    node_function,
    cache_policy=CachePolicy(ttl=120),
)
python  theme={null}
from langgraph.cache.memory import InMemoryCache

graph = builder.compile(cache=InMemoryCache())
python  theme={null}
from langgraph.graph import START, StateGraph

builder = StateGraph(State)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<Accordion title="Extended example: specifying LLM at runtime">
  Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</Accordion>

<Accordion title="Extended example: specifying model and system message at runtime">
  Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.
```

---

## 4. Run an evaluation

**URL:** llms-txt#4.-run-an-evaluation

experiment = ls_client.evaluate(
    chatbot,
    data=dataset,
    evaluators=[is_concise],
    experiment_prefix="my-first-experiment",
    # 'upload_results' is the relevant arg.
    upload_results=False
)

---

## 5. Analyze results locally

**URL:** llms-txt#5.-analyze-results-locally

results = list(experiment)

---

## A2A Post

**URL:** llms-txt#a2a-post

Source: https://docs.langchain.com/langsmith/agent-server-api/a2a/a2a-post

langsmith/agent-server-openapi.json post /a2a/{assistant_id}
Communicate with an assistant using the Agent-to-Agent Protocol.
Sends a JSON-RPC 2.0 message to the assistant.

- **Request**: Provide an object with `jsonrpc`, `id`, `method`, and optional `params`.
- **Response**: Returns a JSON-RPC response with task information or error.

**Supported Methods:**
- `message/send`: Send a message to the assistant
- `tasks/get`: Get the status and result of a task

**Notes:**
- Supports threaded conversations via thread context
- Messages can contain text and data parts
- Tasks run asynchronously and return completion status

---

## Accept with data

**URL:** llms-txt#accept-with-data

ElicitResult(action="accept", content={"email": "user@example.com", "age": 25})

---

## Access custom state fields

**URL:** llms-txt#access-custom-state-fields

@tool
def get_user_preference(
    pref_name: str,
    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model
) -> str:
    """Get a user preference value."""
    preferences = runtime.state.get("user_preferences", {})
    return preferences.get(pref_name, "Not set")
python  theme={null}
from langgraph.types import Command
from langchain.messages import RemoveMessage
from langgraph.graph.message import REMOVE_ALL_MESSAGES
from langchain.tools import tool, ToolRuntime

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref_name` in the tool schema - `runtime` is *not* included in the request.
</Warning>

**Updating state:**

Use [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow:
```

---

## Access memory

**URL:** llms-txt#access-memory

@tool
def get_user_info(user_id: str, runtime: ToolRuntime) -> str:
    """Look up user info."""
    store = runtime.store
    user_info = store.get(("users",), user_id)
    return str(user_info.value) if user_info else "Unknown user"

---

## Access multimodal content from tool messages

**URL:** llms-txt#access-multimodal-content-from-tool-messages

**Contents:**
  - Resources

for message in result["messages"]:
    if message.type == "tool":
        # Raw content in provider-native format
        print(f"Raw content: {message.content}")

# Standardized content blocks  # [!code highlight]
        for block in message.content_blocks:  # [!code highlight]
            if block["type"] == "text":  # [!code highlight]
                print(f"Text: {block['text']}")  # [!code highlight]
            elif block["type"] == "image":  # [!code highlight]
                print(f"Image URL: {block.get('url')}")  # [!code highlight]
                print(f"Image base64: {block.get('base64', '')[:50]}...")  # [!code highlight]
python  theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient

client = MultiServerMCPClient({...})

**Examples:**

Example 1 (unknown):
```unknown
This allows you to handle multimodal tool responses in a provider-agnostic way, regardless of how the underlying MCP server formats its content.

### Resources

[Resources](https://modelcontextprotocol.io/docs/concepts/resources) allow MCP servers to expose data—such as files, database records, or API responses—that can be read by clients. LangChain converts MCP resources into [Blob](/docs/reference/langchain-core/documents#Blob) objects, which provide a unified interface for handling both text and binary content.

#### Loading resources

Use `client.get_resources()` to load resources from an MCP server:
```

---

## Access the current conversation state

**URL:** llms-txt#access-the-current-conversation-state

@tool
def summarize_conversation(
    runtime: ToolRuntime
) -> str:
    """Summarize the conversation so far."""
    messages = runtime.state["messages"]

human_msgs = sum(1 for m in messages if m.__class__.__name__ == "HumanMessage")
    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == "AIMessage")
    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == "ToolMessage")

return f"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results"

---

## Add attachment processor FIRST (runs before LangSmith processor)

**URL:** llms-txt#add-attachment-processor-first-(runs-before-langsmith-processor)

attachment_processor = AttachmentSpanProcessor()
provider.add_span_processor(attachment_processor)

---

## Add custom authentication

**URL:** llms-txt#add-custom-authentication

**Contents:**
- Add custom authentication to your deployment
- Enable agent authentication
  - Authorizing a user for Studio

Source: https://docs.langchain.com/langsmith/custom-auth

This guide shows you how to add custom authentication to your LangSmith application. The steps on this page apply to both [cloud](/langsmith/cloud) and [self-hosted](/langsmith/self-hosted) deployments. It does not apply to isolated usage of the [LangGraph open source library](/oss/python/langgraph/overview) in your own custom server.

## Add custom authentication to your deployment

To leverage custom authentication and access user-level metadata in your deployments, set up custom authentication to automatically populate the `config["configurable"]["langgraph_auth_user"]` object through a custom authentication handler. You can then access this object in your graph with the `langgraph_auth_user` key to [allow an agent to perform authenticated actions on behalf of the user](#enable-agent-authentication).

1. Implement authentication:

<Note>
     Without a custom `@auth.authenticate` handler, LangGraph sees only the API-key owner (usually the developer), so requests aren’t scoped to individual end-users. To propagate custom tokens, you must implement your own handler.
   </Note>

* This handler receives the request (headers, etc.), validates the user, and returns a dictionary with at least an identity field.
* You can add any custom fields you want (e.g., OAuth tokens, roles, org IDs, etc.).

2. In your [`langgraph.json`](/langsmith/application-structure#configuration-file), add the path to your auth file:

3. Once you've set up authentication in your server, requests must include the required authorization information based on your chosen scheme. Assuming you are using JWT token authentication, you could access your deployments using any of the following methods:

<Tabs>
     <Tab title="Python Client">
       
     </Tab>

<Tab title="Python RemoteGraph">
       
     </Tab>

<Tab title="JavaScript Client">
       
     </Tab>

<Tab title="JavaScript RemoteGraph">
       
     </Tab>

<Tab title="CURL">
       
     </Tab>
   </Tabs>

For more details on RemoteGraph, refer to the [Use RemoteGraph](/langsmith/use-remote-graph) guide.

## Enable agent authentication

After [authentication](#add-custom-authentication-to-your-deployment), the platform creates a special configuration object (`config`) that is passed to LangSmith deployment. This object contains information about the current user, including any custom fields you return from your `@auth.authenticate` handler.

To allow an agent to perform authenticated actions on behalf of the user, access this object in your graph with the `langgraph_auth_user` key:

<Note>
  Fetch user credentials from a secure secret store. Storing secrets in graph state is not recommended.
</Note>

### Authorizing a user for Studio

By default, if you add custom authorization on your resources, this will also apply to interactions made from [Studio](/langsmith/studio). If you want, you can handle logged-in Studio users differently by checking [is\_studio\_user()](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph_sdk.auth.types.StudioUser).

<Note>
  `is_studio_user` was added in version 0.1.73 of the langgraph-sdk. If you're on an older version, you can still check whether `isinstance(ctx.user, StudioUser)`.
</Note>

```python  theme={null}
from langgraph_sdk.auth import is_studio_user, Auth
auth = Auth()

**Examples:**

Example 1 (unknown):
```unknown
* This handler receives the request (headers, etc.), validates the user, and returns a dictionary with at least an identity field.
* You can add any custom fields you want (e.g., OAuth tokens, roles, org IDs, etc.).

2. In your [`langgraph.json`](/langsmith/application-structure#configuration-file), add the path to your auth file:
```

Example 2 (unknown):
```unknown
3. Once you've set up authentication in your server, requests must include the required authorization information based on your chosen scheme. Assuming you are using JWT token authentication, you could access your deployments using any of the following methods:

   <Tabs>
     <Tab title="Python Client">
```

Example 3 (unknown):
```unknown
</Tab>

     <Tab title="Python RemoteGraph">
```

Example 4 (unknown):
```unknown
</Tab>

     <Tab title="JavaScript Client">
```

---

## Add edges

**URL:** llms-txt#add-edges

**Contents:**
- Create branches
  - Run graph nodes in parallel
  - Defer node execution
  - Conditional branching
- Map-Reduce and the Send API

builder.add_edge(START, "step_1")
builder.add_edge("step_1", "step_2")
builder.add_edge("step_2", "step_3")
python  theme={null}
builder = StateGraph(State).add_sequence([step_1, step_2, step_3])
builder.add_edge(START, "step_1")
python  theme={null}
  from typing_extensions import TypedDict

class State(TypedDict):
      value_1: str
      value_2: int
  python  theme={null}
  def step_1(state: State):
      return {"value_1": "a"}

def step_2(state: State):
      current_value_1 = state["value_1"]
      return {"value_1": f"{current_value_1} b"}

def step_3(state: State):
      return {"value_2": 10}
  python  theme={null}
  from langgraph.graph import START, StateGraph

builder = StateGraph(State)

# Add nodes
  builder.add_node(step_1)
  builder.add_node(step_2)
  builder.add_node(step_3)

# Add edges
  builder.add_edge(START, "step_1")
  builder.add_edge("step_1", "step_2")
  builder.add_edge("step_2", "step_3")
  python  theme={null}
    builder.add_node("my_node", step_1)
    python  theme={null}
  graph = builder.compile()
  python  theme={null}
  from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
  python  theme={null}
  graph.invoke({"value_1": "c"})
  
  {'value_1': 'a b', 'value_2': 10}
  python  theme={null}
    builder = StateGraph(State).add_sequence([step_1, step_2, step_3])  # [!code highlight]
    builder.add_edge(START, "step_1")

graph = builder.compile()

graph.invoke({"value_1": "c"})
    python  theme={null}
import operator
from typing import Annotated, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

def d(state: State):
    print(f'Adding "D" to {state["aggregate"]}')
    return {"aggregate": ["D"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_node(d)
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []}, {"configurable": {"thread_id": "foo"}})

Adding "A" to []
Adding "B" to ['A']
Adding "C" to ['A']
Adding "D" to ['A', 'B', 'C']
python  theme={null}
  graph.invoke({"value_1": "c"}, {"configurable": {"max_concurrency": 10}})
  python  theme={null}
import operator
from typing import Annotated, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def b_2(state: State):
    print(f'Adding "B_2" to {state["aggregate"]}')
    return {"aggregate": ["B_2"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

def d(state: State):
    print(f'Adding "D" to {state["aggregate"]}')
    return {"aggregate": ["D"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(b_2)
builder.add_node(c)
builder.add_node(d, defer=True)  # [!code highlight]
builder.add_edge(START, "a")
builder.add_edge("a", "b")
builder.add_edge("a", "c")
builder.add_edge("b", "b_2")
builder.add_edge("b_2", "d")
builder.add_edge("c", "d")
builder.add_edge("d", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []})

Adding "A" to []
Adding "B" to ['A']
Adding "C" to ['A']
Adding "B_2" to ['A', 'B', 'C']
Adding "D" to ['A', 'B', 'C', 'B_2']
python  theme={null}
import operator
from typing import Annotated, Literal, Sequence
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    aggregate: Annotated[list, operator.add]
    # Add a key to the state. We will set this key to determine
    # how we branch.
    which: str

def a(state: State):
    print(f'Adding "A" to {state["aggregate"]}')
    return {"aggregate": ["A"], "which": "c"}  # [!code highlight]

def b(state: State):
    print(f'Adding "B" to {state["aggregate"]}')
    return {"aggregate": ["B"]}

def c(state: State):
    print(f'Adding "C" to {state["aggregate"]}')
    return {"aggregate": ["C"]}

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)
builder.add_node(c)
builder.add_edge(START, "a")
builder.add_edge("b", END)
builder.add_edge("c", END)

def conditional_edge(state: State) -> Literal["b", "c"]:
    # Fill in arbitrary logic here that uses the state
    # to determine the next node
    return state["which"]

builder.add_conditional_edges("a", conditional_edge)  # [!code highlight]

graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
result = graph.invoke({"aggregate": []})
print(result)

Adding "A" to []
Adding "C" to ['A']
{'aggregate': ['A', 'C'], 'which': 'c'}
python  theme={null}
  def route_bc_or_cd(state: State) -> Sequence[str]:
      if state["which"] == "cd":
          return ["c", "d"]
      return ["b", "c"]
  python  theme={null}
from langgraph.graph import StateGraph, START, END
from langgraph.types import Send
from typing_extensions import TypedDict, Annotated
import operator

class OverallState(TypedDict):
    topic: str
    subjects: list[str]
    jokes: Annotated[list[str], operator.add]
    best_selected_joke: str

def generate_topics(state: OverallState):
    return {"subjects": ["lions", "elephants", "penguins"]}

def generate_joke(state: OverallState):
    joke_map = {
        "lions": "Why don't lions like fast food? Because they can't catch it!",
        "elephants": "Why don't elephants use computers? They're afraid of the mouse!",
        "penguins": "Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice."
    }
    return {"jokes": [joke_map[state["subject"]]]}

def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state["subjects"]]

def best_joke(state: OverallState):
    return {"best_selected_joke": "penguins"}

builder = StateGraph(OverallState)
builder.add_node("generate_topics", generate_topics)
builder.add_node("generate_joke", generate_joke)
builder.add_node("best_joke", best_joke)
builder.add_edge(START, "generate_topics")
builder.add_conditional_edges("generate_topics", continue_to_jokes, ["generate_joke"])
builder.add_edge("generate_joke", "best_joke")
builder.add_edge("best_joke", END)
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We can also use the built-in shorthand `.add_sequence`:
```

Example 2 (unknown):
```unknown
<Accordion title="Why split application steps into a sequence with LangGraph?">
  LangGraph makes it easy to add an underlying persistence layer to your application.
  This allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:

  * How state updates are [checkpointed](/oss/python/langgraph/persistence)
  * How interruptions are resumed in [human-in-the-loop](/oss/python/langgraph/interrupts) workflows
  * How we can "rewind" and branch-off executions using LangGraph's [time travel](/oss/python/langgraph/use-time-travel) features

  They also determine how execution steps are [streamed](/oss/python/langgraph/streaming), and how your application is visualized and debugged using [Studio](/langsmith/studio).

  Let's demonstrate an end-to-end example. We will create a sequence of three steps:

  1. Populate a value in a key of the state
  2. Update the same value
  3. Populate a different value

  Let's first define our [state](/oss/python/langgraph/graph-api#state). This governs the [schema of the graph](/oss/python/langgraph/graph-api#schema), and can also specify how to apply updates. See [this section](#process-state-updates-with-reducers) for more detail.

  In our case, we will just keep track of two values:
```

Example 3 (unknown):
```unknown
Our [nodes](/oss/python/langgraph/graph-api#nodes) are just Python functions that read our graph's state and make updates to it. The first argument to this function will always be the state:
```

Example 4 (unknown):
```unknown
<Note>
    Note that when issuing updates to the state, each node can just specify the value of the key it wishes to update.

    By default, this will **overwrite** the value of the corresponding key. You can also use [reducers](/oss/python/langgraph/graph-api#reducers) to control how updates are processed— for example, you can append successive updates to a key instead. See [this section](#process-state-updates-with-reducers) for more detail.
  </Note>

  Finally, we define the graph. We use [StateGraph](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state.

  We will then use [`add_node`](/oss/python/langgraph/graph-api#messagesstate) and [`add_edge`](/oss/python/langgraph/graph-api#edges) to populate our graph and define its control flow.
```

---

## Add edges to connect nodes

**URL:** llms-txt#add-edges-to-connect-nodes

orchestrator_worker_builder.add_edge(START, "orchestrator")
orchestrator_worker_builder.add_conditional_edges(
    "orchestrator", assign_workers, ["llm_call"]
)
orchestrator_worker_builder.add_edge("llm_call", "synthesizer")
orchestrator_worker_builder.add_edge("synthesizer", END)

---

## Add LangSmith processor SECOND (receives already-modified spans)

**URL:** llms-txt#add-langsmith-processor-second-(receives-already-modified-spans)

**Contents:**
- Advanced configuration
  - Use OpenTelemetry Collector for fan-out
  - Distributed tracing with LangChain and OpenTelemetry

langsmith_processor = OtelSpanProcessor(project="travel-assistant")
provider.add_span_processor(langsmith_processor)

def get_flight_info(destination: str, departure_date: str) -> dict:
    """Get flight information for a destination."""
    return {
        "destination": destination,
        "departure_date": departure_date,
        "price": "$450",
        "duration": "5h 30m",
        "airline": "Example Airways"
    }

def get_hotel_recommendations(city: str, check_in: str) -> dict:
    """Get hotel recommendations for a city."""
    return {
        "city": city,
        "check_in": check_in,
        "hotels": [
            {"name": "Grand Plaza Hotel", "rating": 4.5, "price": "$120/night"},
            {"name": "City Center Inn", "rating": 4.2, "price": "$95/night"}
        ]
    }

async def main():
    # Prepare the attachment
    receipt_path = Path("receipt-template-example.png")
    with open(receipt_path, "rb") as img_file:
        image_bytes = img_file.read()
        image_base64 = base64.b64encode(image_bytes).decode("ascii")

attachment_data = {
        "name": "receipt-template-example",
        "content": image_base64,
        "mime_type": "image/jpeg",
    }

attachment_processor.set_attachment(attachment_data)

# Create ADK agent
    agent = LlmAgent(
        name="travel_assistant",
        tools=[get_flight_info, get_hotel_recommendations],
        model="gemini-2.0-flash-exp",
        instruction="You are a helpful travel assistant that can help with flights and hotels.",
    )

# Set up session and runner
    session_service = InMemorySessionService()
    runner = Runner(
        app_name="travel_app",
        agent=agent,
        session_service=session_service
    )

await session_service.create_session(
        app_name="travel_app",
        user_id="traveler_456",
        session_id="session_789"
    )

# Send a message to the agent
    new_message = types.Content(
        parts=[types.Part(text="I need to book a flight to Paris for March 15th and find a good hotel.")],
        role="user",
    )

# Run the agent and process events
    events = runner.run(
        user_id="traveler_456",
        session_id="session_789",
        new_message=new_message,
    )

for event in events:
        print(event)

if __name__ == "__main__":
    asyncio.run(main())
yaml  theme={null}
   receivers:
     otlp:
       protocols:
         grpc:
           endpoint: 0.0.0.0:4317
         http:
           endpoint: 0.0.0.0:4318

processors:
     batch:

exporters:
     otlphttp/langsmith:
       endpoint: https://api.smith.langchain.com/otel/v1/traces
       headers:
         x-api-key: ${env:LANGSMITH_API_KEY}
         Langsmith-Project: my_project
     otlphttp/other_provider:
       endpoint: https://otel.your-provider.com/v1/traces
       headers:
         api-key: ${env:OTHER_PROVIDER_API_KEY}

service:
     pipelines:
       traces:
         receivers: [otlp]
         processors: [batch]
         exporters: [otlphttp/langsmith, otlphttp/other_provider]
   python  theme={null}
   import os
   from opentelemetry import trace
   from opentelemetry.sdk.trace import TracerProvider
   from opentelemetry.sdk.trace.export import BatchSpanProcessor
   from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
   from langchain_openai import ChatOpenAI
   from langchain_core.prompts import ChatPromptTemplate

# Point to your local OpenTelemetry Collector
   otlp_exporter = OTLPSpanExporter(
       endpoint="http://localhost:4318/v1/traces"
   )
   provider = TracerProvider()
   processor = BatchSpanProcessor(otlp_exporter)
   provider.add_span_processor(processor)
   trace.set_tracer_provider(provider)

# Set environment variables for LangChain
   os.environ["LANGSMITH_OTEL_ENABLED"] = "true"
   os.environ["LANGSMITH_TRACING"] = "true"

# Create and run a LangChain application
   prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
   model = ChatOpenAI()
   chain = prompt | model
   result = chain.invoke({"topic": "programming"})
   print(result.content)
   python  theme={null}
import os
from opentelemetry import trace
from opentelemetry.propagate import inject, extract
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
import requests
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

**Examples:**

Example 1 (unknown):
```unknown
Here is an [example](https://smith.langchain.com/public/9574f70a-b893-49fe-8c62-691bd114bf14/r) of what the resulting trace looks like in LangSmith.

## Advanced configuration

### Use OpenTelemetry Collector for fan-out

For more advanced scenarios, you can use the OpenTelemetry Collector to fan out your telemetry data to multiple destinations. This is a more scalable approach than configuring multiple exporters in your application code.

1. [Install the OpenTelemetry Collector](https://opentelemetry.io/docs/collector/getting-started/) for your environment.

2. Create a configuration file (e.g., `otel-collector-config.yaml`) that exports to multiple destinations:
```

Example 2 (unknown):
```unknown
3. Configure your application to send to the collector:
```

Example 3 (unknown):
```unknown
This approach offers several advantages:

* Centralized configuration for all your telemetry destinations
* Reduced overhead in your application code
* Better scalability and resilience
* Ability to add or remove destinations without changing application code

### Distributed tracing with LangChain and OpenTelemetry

Distributed tracing is essential when your LLM application spans multiple services or processes. OpenTelemetry's context propagation capabilities ensure that traces remain connected across service boundaries.

#### Context propagation in distributed tracing

In distributed systems, context propagation passes trace metadata between services so that related spans are linked to the same trace:

* **Trace ID**: A unique identifier for the entire trace
* **Span ID**: A unique identifier for the current span
* **Sampling Decision**: Indicates whether this trace should be sampled

#### Set up distributed tracing with LangChain

To enable distributed tracing across multiple services:
```

---

## Add metadata and tags to traces

**URL:** llms-txt#add-metadata-and-tags-to-traces

Source: https://docs.langchain.com/langsmith/add-metadata-tags

LangSmith supports sending arbitrary metadata and tags along with traces.

Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.

Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID. For more information on tags and metadata, see the [Concepts](/langsmith/observability-concepts#tags) page. For information on how to query traces and runs by metadata and tags, see the [Filter traces in the application](/langsmith/filter-traces-in-application) page.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/add-metadata-tags.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Add nodes

**URL:** llms-txt#add-nodes

workflow.add_node("generate_topic", generate_topic)
workflow.add_node("write_joke", write_joke)

---

## Add OtelSpanProcessor to the tracer provider

**URL:** llms-txt#add-otelspanprocessor-to-the-tracer-provider

tracer_provider.add_span_processor(OtelSpanProcessor())

---

## Add the function to the kernel

**URL:** llms-txt#add-the-function-to-the-kernel

code_analyzer = kernel.add_function(
    function_name="analyzeCode",
    plugin_name="codeAnalysisPlugin",
    prompt_template_config=prompt_template_config,
)

---

## Add the middleware to the app

**URL:** llms-txt#add-the-middleware-to-the-app

**Contents:**
- Configure `langgraph.json`
  - Customize middleware ordering
- Start server
- Deploying
- Next steps

app.add_middleware(CustomHeaderMiddleware)
json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app"
  }
  // Other configuration options like auth, store, etc.
}
json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app",
    "middleware_order": "auth_first"
  },
  "auth": {
    "path": "./auth.py:my_auth"
  }
}
bash  theme={null}
langgraph dev --no-browser
```

Now any request to your server will include the custom header `X-Custom-Header` in its response.

You can deploy this app as-is to cloud or to your self-hosted platform.

Now that you've added custom middleware to your deployment, you can use similar techniques to add [custom routes](/langsmith/custom-routes) or define [custom lifespan events](/langsmith/custom-lifespan) to further customize your server's behavior.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-middleware.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the `webapp.py` file you created above.
```

Example 2 (unknown):
```unknown
### Customize middleware ordering

By default, custom middleware runs before authentication logic. To run custom middleware *after* authentication, set `middleware_order` to `auth_first` in your `http` configuration. (This customization is supported starting with API server v0.4.35 and later.)
```

Example 3 (unknown):
```unknown
## Start server

Test the server out locally:
```

---

## Add the nodes

**URL:** llms-txt#add-the-nodes

orchestrator_worker_builder.add_node("orchestrator", orchestrator)
orchestrator_worker_builder.add_node("llm_call", llm_call)
orchestrator_worker_builder.add_node("synthesizer", synthesizer)

---

## Add to conversation history

**URL:** llms-txt#add-to-conversation-history

**Contents:**
  - Tool Message

messages = [
    SystemMessage("You are a helpful assistant"),
    HumanMessage("Can you help me?"),
    ai_msg,  # Insert as if it came from the model
    HumanMessage("Great! What's 2+2?")
]

response = model.invoke(messages)
python  theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano")

def get_weather(location: str) -> str:
    """Get the weather at a location."""
    ...

model_with_tools = model.bind_tools([get_weather])
response = model_with_tools.invoke("What's the weather in Paris?")

for tool_call in response.tool_calls:
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
    print(f"ID: {tool_call['id']}")
python  theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano")

response = model.invoke("Hello!")
response.usage_metadata

{'input_tokens': 8,
 'output_tokens': 304,
 'total_tokens': 312,
 'input_token_details': {'audio': 0, 'cache_read': 0},
 'output_token_details': {'audio': 0, 'reasoning': 256}}
python  theme={null}
chunks = []
full_message = None
for chunk in model.stream("Hi"):
    chunks.append(chunk)
    print(chunk.text)
    full_message = chunk if full_message is None else full_message + chunk
python  theme={null}
from langchain.messages import AIMessage
from langchain.messages import ToolMessage

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Attributes">
  <ParamField path="text" type="string">
    The text content of the message.
  </ParamField>

  <ParamField path="content" type="string | dict[]">
    The raw content of the message.
  </ParamField>

  <ParamField path="content_blocks" type="ContentBlock[]">
    The standardized [content blocks](#message-content) of the message.
  </ParamField>

  <ParamField path="tool_calls" type="dict[] | None">
    The tool calls made by the model.

    Empty if no tools are called.
  </ParamField>

  <ParamField path="id" type="string">
    A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)
  </ParamField>

  <ParamField path="usage_metadata" type="dict | None">
    The usage metadata of the message, which can contain token counts when available.
  </ParamField>

  <ParamField path="response_metadata" type="ResponseMetadata | None">
    The response metadata of the message.
  </ParamField>
</Accordion>

#### Tool calls

When models make [tool calls](/oss/python/langchain/models#tool-calling), they're included in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage):
```

Example 2 (unknown):
```unknown
Other structured data, such as reasoning or citations, can also appear in message [content](/oss/python/langchain/messages#message-content).

#### Token usage

An [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) can hold token counts and other usage metadata in its [`usage_metadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) field:
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
See [`UsageMetadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) for details.

#### Streaming and chunks

During streaming, you'll receive [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects that can be combined into a full message object:
```

---

## Add to your pipeline

**URL:** llms-txt#add-to-your-pipeline

pipeline = Pipeline([
    transport.input(),
    stt,
    context_aggregator.user(),
    llm,
    tts,
    audio_recorder,              # Full conversation recording
    turn_audio_recorder,         # Per-turn audio snippets
    transport.output(),
    context_aggregator.assistant(),
])

---

## After a model makes a tool call

**URL:** llms-txt#after-a-model-makes-a-tool-call

---

## After: Graph API

**URL:** llms-txt#after:-graph-api

class WorkflowState(TypedDict):
    input_data: dict
    step1_result: dict
    analysis: dict
    final_result: dict

def should_analyze(state):
    return "analyze" if state["step1_result"]["needs_analysis"] else "simple_path"

def confidence_check(state):
    return "high_confidence" if state["analysis"]["confidence"] > 0.8 else "low_confidence"

workflow = StateGraph(WorkflowState)
workflow.add_node("step1", process_step1_node)
workflow.add_conditional_edges("step1", should_analyze)
workflow.add_node("analyze", analyze_data_node)
workflow.add_conditional_edges("analyze", confidence_check)

---

## After: Simplified Functional API

**URL:** llms-txt#after:-simplified-functional-api

**Contents:**
- Summary

@entrypoint(checkpointer=checkpointer)
def simple_workflow(input_data: str) -> str:
    step1 = process_step1(input_data).result()
    step2 = process_step2(step1).result()
    return finalize_result(step2).result()
```

Choose the **Graph API** when you need explicit control over workflow structure, complex branching, parallel processing, or team collaboration benefits.

Choose the **Functional API** when you want to add LangGraph features to existing code with minimal changes, have simple linear workflows, or need rapid prototyping capabilities.

Both APIs provide the same core LangGraph features (persistence, streaming, human-in-the-loop, memory) but package them in different paradigms to suit different development styles and use cases.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/choosing-apis.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## ahead of time and use those to disambiguate the user input. E.g. if a user searches for

**URL:** llms-txt#ahead-of-time-and-use-those-to-disambiguate-the-user-input.-e.g.-if-a-user-searches-for

---

## AIMessage(content='bar', ...)

**URL:** llms-txt#aimessage(content='bar',-...)

**Contents:**
  - InMemorySaver Checkpointer

python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model,
    tools=[],
    checkpointer=InMemorySaver()
)

**Examples:**

Example 1 (unknown):
```unknown
### InMemorySaver Checkpointer

To enable persistence during testing, you can use the [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) checkpointer. This allows you to simulate multiple turns to test state-dependent behavior:
```

---

## AIMessage(content='', ..., tool_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call_1', 'type': 'tool_call'}])

**URL:** llms-txt#aimessage(content='',-...,-tool_calls=[{'name':-'foo',-'args':-{'bar':-'baz'},-'id':-'call_1',-'type':-'tool_call'}])

python  theme={null}
model.invoke("hello, again!")

**Examples:**

Example 1 (unknown):
```unknown
If we invoke the model again, it will return the next item in the iterator:
```

---

## Alerts in LangSmith

**URL:** llms-txt#alerts-in-langsmith

**Contents:**
- Overview
- Configuring an alert
  - Step 1: Navigate To Create Alert
  - Step 2: Select Metric Type
  - Step 2: Define Alert Conditions
  - Step 3: Configure Notification Channel
- Best Practices

Source: https://docs.langchain.com/langsmith/alerts

<Note>
  **Self-hosted Version Requirement**

Access to alerts requires Helm chart version **0.10.3** or later.
</Note>

Effective observability in LLM applications requires proactive detection of failures, performance degradations, and regressions. LangSmith's alerts feature helps identify critical issues such as:

* API rate limit violations from model providers
* Latency increases for your application
* Application changes that affect feedback scores reflecting end-user experience

Alerts in LangSmith are project-scoped, requiring separate configuration for each monitored project.

## Configuring an alert

### Step 1: Navigate To Create Alert

First navigate to the Tracing project that you would like to configure alerts for. Click the Alerts icon on the top right hand corner of the page to view existing alerts for that project and set up a new alert.

### Step 2: Select Metric Type

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=932f55b512d866906160e3ebe9a78ad7" alt="Alert Metrics" data-og-width="597" width="597" data-og-height="134" height="134" data-path="langsmith/images/alert-metric.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9a0140bfcf9df907ccaeffc0abc6d324 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=774b40c4cf122330c3b7e7e39bffecde 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=599617a29917cffe79547c1a85d110c3 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e963933afa346141fc2623286f55b48 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fcb38466705fd5d8b94443ec9916a6ee 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-metric.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d738df80eee5db727e6627c4a0e85ce9 2500w" />
</div>

LangSmith offers threshold-based alerting on three core metrics:

| Metric Type        | Description                         | Use Case                                                                                                                                                |
| ------------------ | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Errored Runs**   | Track runs with an error status     | Monitors for failures in an application.                                                                                                                |
| **Feedback Score** | Measures the average feedback score | Track [feedback from end users](/langsmith/attach-user-feedback) or [online evaluation results](/langsmith/online-evaluations) to alert on regressions. |
| **Latency**        | Measures average run execution time | Tracks the latency of your application to alert on spikes and performance bottlenecks.                                                                  |

Additionally, for **Errored Runs** and **Run Latency**, you can define filters to narrow down the runs that trigger alerts. For example, you might create an error alert filter for all `llm` runs tagged with `support_agent` that encounter a `RateLimitExceeded` error.

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b2dd48ba21e857c8a99a26a0d896f950" alt="Alert Metrics" data-og-width="407" width="407" data-og-height="273" height="273" data-path="langsmith/images/alerts-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d776aa4bb261605c45f4691b95822ad1 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1cace263d141b044c73a8615c4c9cd15 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a77dfdb2a2e5a119d11675fc01a857ce 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d582ea675732440f5b4bae57ae35b766 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8780c7b52bc0a61c938a7c75357cd068 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alerts-filter.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6d2d7349e8856d8575bed75ccde61871 2500w" />
</div>

### Step 2: Define Alert Conditions

Alert conditions consist of several components:

* **Aggregation Method**: Average, Percentage, or Count
* **Comparison Operator**: `>=`, `<=`, or exceeds threshold
* **Threshold Value**: Numerical value triggering the alert
* **Aggregation Window**: Time period for metric calculation (currently choose between 5 or 15 minutes)
* **Feedback Key** (Feedback Score alerts only): Specific feedback metric to monitor

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d92406d84dec4f1b827b82a989df30b9" alt="Alert Condition Configuration" data-og-width="597" width="597" data-og-height="112" height="112" data-path="langsmith/images/define-conditions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3311a45f1a32527a54c71d4966fdac3b 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6ed12bea3c447c20bfff16e4e58d27e6 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=78955506ecd68ba0bac2ea7053837d6e 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4a0bf3da7b34bdd56777a350315b3f6a 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=56a4a9e40b9c2a870b999c52dd13dd68 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/define-conditions.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5207fb3afe3b40873280d9f23e3e0e24 2500w" />
</div>

**Example:** The configuration shown above would generate an alert when more than 5% of runs within the past 5 minutes result in errors.

You can preview alert behavior over a historical time window to understand how many datapoints—and which ones—would have triggered an alert at a chosen threshold (indicated in red). For example, setting an average latency threshold of 60 seconds for a project lets you visualize potential alerts, as shown in the image below.

<div style={{ textAlign: 'center' }}>
    <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d7f26bce1113c50bec8f5853c6448415" alt="Alert Metrics" data-og-width="863" width="863" data-og-height="545" height="545" data-path="langsmith/images/alert-preview.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a508e02a73579624ae120276664e0e6a 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4f7c5616752dfea80a346be50532f442 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=dd7d2d27fdb2335640d5ac43b6747baf 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=abbaea739f003fcbe97ee00e55e68927 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=55672ba9518816caf74921bc26694ffa 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f67b99c4b5d709b1756d5b674a20dba1 2500w" />
</div>

### Step 3: Configure Notification Channel

LangSmith supports the following notification channels:

1. [PagerDuty Integration](/langsmith/alerts-pagerduty)
2. [Webhook Notifications](/langsmith/alerts-webhook)

Select the appropriate channel to ensure notifications reach the responsible team members.

* Adjust sensitivity based on application criticality
* Start with broader thresholds and refine based on observed patterns
* Ensure alert routing reaches appropriate on-call personnel

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/alerts.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Alice can still create threads

**URL:** llms-txt#alice-can-still-create-threads

**Contents:**
- Next steps

alice_thread = await alice.threads.create()
print(f"✅ Alice created thread: {alice_thread['thread_id']}")
bash  theme={null}
✅ Alice created thread: dcea5cd8-eb70-4a01-a4b6-643b14e8f754
✅ Bob correctly denied access: Client error '404 Not Found' for url 'http://localhost:2024/threads/dcea5cd8-eb70-4a01-a4b6-643b14e8f754'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
✅ Bob created his own thread: 400f8d41-e946-429f-8f93-4fe395bc3eed
✅ Alice sees 1 thread
✅ Bob sees 1 thread
✅ Alice correctly denied access:
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500
✅ Alice correctly denied access to searching assistants:
```

Congratulations! You've built a chatbot where each user has their own private conversations. While this system uses simple token-based authentication, these authorization patterns will work with implementing any real authentication system. In the next tutorial, you'll replace your test users with real user accounts using OAuth2.

Now that you can control access to resources, you might want to:

1. Move on to [Connect an authentication provider](/langsmith/add-auth-server) to add real user accounts.
2. Read more about [authorization patterns](/langsmith/auth#authorization).
3. Check out the [API reference](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) for details about the interfaces and methods used in this tutorial.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/resource-auth.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Output:
```

---

## Alice creates an assistant

**URL:** llms-txt#alice-creates-an-assistant

alice_assistant = await alice.assistants.create()
print(f"✅ Alice created assistant: {alice_assistant['assistant_id']}")

---

## Alice creates a thread and chats

**URL:** llms-txt#alice-creates-a-thread-and-chats

alice_thread = await alice.threads.create()
print(f"✅ Alice created thread: {alice_thread['thread_id']}")

await alice.runs.create(
    thread_id=alice_thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hi, this is Alice's private chat"}]}
)

---

## All fetch operations run in parallel

**URL:** llms-txt#all-fetch-operations-run-in-parallel

workflow.add_edge(START, "fetch_news")
workflow.add_edge(START, "fetch_weather")
workflow.add_edge(START, "fetch_stocks")

---

## All integrations

**URL:** llms-txt#all-integrations

**Contents:**
- Top providers
- LangGraph integrations
- Chat Models
- LLMs
- Text Embedding Models
- Vector Stores
- Document loaders
  - File Loaders
  - Web Loaders
- Document Transformers

Source: https://docs.langchain.com/oss/javascript/integrations/providers/all_providers

Browse the complete collection of integrations available for JavaScript/TypeScript. LangChain.js offers hundreds of integrations across providers, tools, vector stores, document loaders, and more.

<Columns cols={3}>
  <Card title="Anthropic" href="/oss/javascript/integrations/providers/anthropic" icon="anthropic">
    Integrate with Anthropic's Claude models for advanced reasoning and conversation.
  </Card>

<Card title="AWS" href="/oss/javascript/integrations/providers/aws" icon="aws">
    Access AWS services and foundation models through comprehensive integrations.
  </Card>

<Card title="Google" href="/oss/javascript/integrations/providers/google" icon="google">
    Integrate with Google's AI services including Gemini and Vertex AI.
  </Card>

<Card title="Microsoft" href="/oss/javascript/integrations/providers/microsoft" icon="microsoft">
    Connect to Microsoft Azure services and AI platforms.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/providers/openai" icon="openai">
    Build with GPT models and OpenAI's comprehensive AI platform.
  </Card>
</Columns>

## LangGraph integrations

Connect LangGraph agents to front ends. See the [LangGraph integrations](/oss/javascript/langgraph/integrations) page for more details.

<Columns cols={3}>
  <Card title="AG-UI Protocol" href="https://docs.ag-ui.com/getting-started/quickstart-langgraph-js" icon="link">
    Open event-based protocol for connecting LangGraph agents to any frontend.
  </Card>

<Card title="CopilotKit" href="https://docs.copilotkit.ai/coagents/quickstart/langgraph-js" icon="react">
    React framework with pre-built UI components for AI copilots.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Alibaba Tongyi" href="/oss/javascript/integrations/chat/alibaba_tongyi">
    Alibaba's Tongyi language model for Chinese and multilingual applications.
  </Card>

<Card title="Anthropic" href="/oss/javascript/integrations/chat/anthropic" icon="anthropic">
    Claude models for advanced conversational AI and reasoning.
  </Card>

<Card title="Arcjet" href="/oss/javascript/integrations/chat/arcjet">
    Security-focused AI chat integration with built-in protections.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/chat/azure" icon="microsoft">
    OpenAI models through Microsoft Azure's enterprise platform.
  </Card>

<Card title="Baidu Qianfan" href="/oss/javascript/integrations/chat/baidu_qianfan">
    Baidu's Qianfan platform for Chinese language AI models.
  </Card>

<Card title="Baidu Wenxin" href="/oss/javascript/integrations/chat/baidu_wenxin">
    Baidu's Wenxin (ERNIE) models for natural language processing.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/chat/bedrock" icon="aws">
    Access foundation models through Amazon Bedrock's managed service.
  </Card>

<Card title="Bedrock Converse" href="/oss/javascript/integrations/chat/bedrock_converse" icon="aws">
    Unified Bedrock Converse API for multiple foundation models.
  </Card>

<Card title="Cerebras" href="/oss/javascript/integrations/chat/cerebras">
    Ultra-fast inference with Cerebras Systems' AI processors.
  </Card>

<Card title="Cloudflare Workers AI" href="/oss/javascript/integrations/chat/cloudflare_workersai">
    Run AI models on Cloudflare's edge computing platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/chat/cohere" icon="cohere">
    Cohere's language models for text generation and understanding.
  </Card>

<Card title="Deep Infra" href="/oss/javascript/integrations/chat/deep_infra">
    Access open-source models through Deep Infra's cloud platform.
  </Card>

<Card title="DeepSeek" href="/oss/javascript/integrations/chat/deepseek">
    DeepSeek's advanced reasoning and coding models.
  </Card>

<Card title="Fake LLM" href="/oss/javascript/integrations/chat/fake">
    Mock chat model for testing and development purposes.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/chat/fireworks" icon="fireworks">
    High-performance inference for open-source models.
  </Card>

<Card title="Friendli" href="/oss/javascript/integrations/chat/friendli">
    Optimized inference engine for efficient model serving.
  </Card>

<Card title="Google Generative AI" href="/oss/javascript/integrations/chat/google_generative_ai" icon="google">
    Google's Gemini models and generative AI capabilities.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/chat/google_vertex_ai" icon="google">
    Enterprise AI platform with Google Cloud's Vertex AI.
  </Card>

<Card title="Groq" href="/oss/javascript/integrations/chat/groq" icon="groq">
    Ultra-fast inference with Groq's specialized hardware.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/chat/ibm">
    IBM Watson AI models and enterprise solutions.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/chat/llama_cpp">
    Run local Llama models with llama.cpp backend.
  </Card>

<Card title="Minimax" href="/oss/javascript/integrations/chat/minimax">
    Minimax's conversational AI models and services.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/chat/mistral" icon="mistral">
    Mistral's efficient and powerful language models.
  </Card>

<Card title="Moonshot" href="/oss/javascript/integrations/chat/moonshot">
    Moonshot's AI models for various language tasks.
  </Card>

<Card title="Neural Internet Bittensor" href="/oss/javascript/integrations/chat/ni_bittensor">
    Decentralized AI network through Bittensor protocol.
  </Card>

<Card title="Novita" href="/oss/javascript/integrations/chat/novita">
    Novita's AI models and cloud computing platform.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/chat/ollama" icon="ollama">
    Run local models with Ollama's lightweight inference engine.
  </Card>

<Card title="Ollama Functions" href="/oss/javascript/integrations/chat/ollama_functions" icon="ollama">
    Function calling capabilities with Ollama models.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/chat/openai" icon="openai">
    GPT models and OpenAI's comprehensive chat capabilities.
  </Card>

<Card title="Perplexity" href="/oss/javascript/integrations/chat/perplexity">
    Perplexity's search-augmented language models.
  </Card>

<Card title="PremAI" href="/oss/javascript/integrations/chat/premai">
    PremAI's platform for AI model deployment and management.
  </Card>

<Card title="PromptLayer OpenAI" href="/oss/javascript/integrations/chat/prompt_layer_openai">
    OpenAI integration with PromptLayer's observability features.
  </Card>

<Card title="Tencent Hunyuan" href="/oss/javascript/integrations/chat/tencent_hunyuan">
    Tencent's Hunyuan models for Chinese language processing.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/chat/togetherai" icon="together">
    Open-source models through Together AI's cloud platform.
  </Card>

<Card title="WebLLM" href="/oss/javascript/integrations/chat/web_llm">
    Run language models directly in web browsers.
  </Card>

<Card title="xAI" href="/oss/javascript/integrations/chat/xai">
    xAI's Grok models for conversational AI.
  </Card>

<Card title="Yandex" href="/oss/javascript/integrations/chat/yandex">
    Yandex's AI models and language processing services.
  </Card>

<Card title="ZhipuAI" href="/oss/javascript/integrations/chat/zhipuai">
    ZhipuAI's ChatGLM and other Chinese language models.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AI21" href="/oss/javascript/integrations/llms/ai21">
    AI21 Labs' Jurassic models for text generation.
  </Card>

<Card title="Aleph Alpha" href="/oss/javascript/integrations/llms/aleph_alpha">
    European AI company's multilingual language models.
  </Card>

<Card title="Arcjet" href="/oss/javascript/integrations/llms/arcjet">
    Security-focused LLM integration with built-in protections.
  </Card>

<Card title="AWS SageMaker" href="/oss/javascript/integrations/llms/aws_sagemaker" icon="aws">
    Deploy models on Amazon SageMaker's ML platform.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/llms/azure" icon="microsoft">
    OpenAI models through Microsoft Azure's enterprise platform.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/llms/bedrock" icon="aws">
    Foundation models through Amazon Bedrock service.
  </Card>

<Card title="Chrome AI" href="/oss/javascript/integrations/llms/chrome_ai">
    Browser-based AI using Chrome's built-in capabilities.
  </Card>

<Card title="Cloudflare Workers AI" href="/oss/javascript/integrations/llms/cloudflare_workersai">
    AI models on Cloudflare's edge computing platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/llms/cohere" icon="cohere">
    Cohere's language models for various NLP tasks.
  </Card>

<Card title="Deep Infra" href="/oss/javascript/integrations/llms/deep_infra">
    Open-source models through Deep Infra's infrastructure.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/llms/fireworks" icon="fireworks">
    Fast inference for open-source language models.
  </Card>

<Card title="Friendli" href="/oss/javascript/integrations/llms/friendli">
    Optimized serving for efficient model inference.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/llms/google_vertex_ai" icon="google">
    Google Cloud's enterprise AI and ML platform.
  </Card>

<Card title="Gradient AI" href="/oss/javascript/integrations/llms/gradient_ai">
    Private AI model training and deployment platform.
  </Card>

<Card title="Hugging Face" href="/oss/javascript/integrations/llms/huggingface_inference">
    Access thousands of models via Hugging Face Inference API.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/llms/ibm">
    IBM Watson AI and language model services.
  </Card>

<Card title="JigsawStack" href="/oss/javascript/integrations/llms/jigsawstack">
    JigsawStack's AI infrastructure and model services.
  </Card>

<Card title="LayerUp Security" href="/oss/javascript/integrations/llms/layerup_security">
    Security-enhanced LLM integration with monitoring.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/llms/llama_cpp">
    Run Llama models locally with C++ implementation.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/llms/mistral" icon="mistral">
    Mistral's open-source and commercial language models.
  </Card>

<Card title="Neural Internet Bittensor" href="/oss/javascript/integrations/llms/ni_bittensor">
    Decentralized AI through Bittensor's peer-to-peer network.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/llms/ollama" icon="ollama">
    Local model serving with Ollama's simple interface.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/llms/openai" icon="openai">
    GPT models and OpenAI's language model APIs.
  </Card>

<Card title="PromptLayer OpenAI" href="/oss/javascript/integrations/llms/prompt_layer_openai">
    OpenAI with PromptLayer's logging and observability.
  </Card>

<Card title="Raycast" href="/oss/javascript/integrations/llms/raycast">
    AI integration for Raycast productivity tool.
  </Card>

<Card title="Replicate" href="/oss/javascript/integrations/llms/replicate">
    Run open-source models through Replicate's cloud platform.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/llms/together" icon="together">
    Fast inference for open-source models on Together's platform.
  </Card>

<Card title="WRITER" href="/oss/javascript/integrations/llms/writer">
    Enterprise models and tools for building, activating, and supervising AI agents.
  </Card>

<Card title="Yandex" href="/oss/javascript/integrations/llms/yandex">
    Yandex's language models and AI services.
  </Card>
</Columns>

## Text Embedding Models

<Columns cols={3}>
  <Card title="Alibaba Tongyi" href="/oss/javascript/integrations/text_embedding/alibaba_tongyi">
    Alibaba's embedding models for multilingual text representation.
  </Card>

<Card title="Azure OpenAI" href="/oss/javascript/integrations/text_embedding/azure_openai" icon="microsoft">
    OpenAI embeddings through Microsoft Azure platform.
  </Card>

<Card title="Baidu Qianfan" href="/oss/javascript/integrations/text_embedding/baidu_qianfan">
    Baidu's text embedding models for Chinese content.
  </Card>

<Card title="Amazon Bedrock" href="/oss/javascript/integrations/text_embedding/bedrock" icon="aws">
    Foundation model embeddings through Amazon Bedrock.
  </Card>

<Card title="ByteDance Doubao" href="/oss/javascript/integrations/text_embedding/bytedance_doubao">
    ByteDance's embedding models for content understanding.
  </Card>

<Card title="Cloudflare AI" href="/oss/javascript/integrations/text_embedding/cloudflare_ai">
    Text embeddings on Cloudflare's edge AI platform.
  </Card>

<Card title="Cohere" href="/oss/javascript/integrations/text_embedding/cohere" icon="cohere">
    Cohere's multilingual embedding models.
  </Card>

<Card title="DeepInfra" href="/oss/javascript/integrations/text_embedding/deepinfra">
    Open-source embedding models via DeepInfra.
  </Card>

<Card title="Fireworks" href="/oss/javascript/integrations/text_embedding/fireworks" icon="fireworks">
    Fast embedding inference through Fireworks platform.
  </Card>

<Card title="Google Generative AI" href="/oss/javascript/integrations/text_embedding/google_generative_ai" icon="google">
    Google's embedding models for text representation.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/text_embedding/google_vertex_ai" icon="google">
    Enterprise embedding models through Vertex AI.
  </Card>

<Card title="Gradient AI" href="/oss/javascript/integrations/text_embedding/gradient_ai">
    Private embedding models with Gradient AI platform.
  </Card>

<Card title="Hugging Face" href="/oss/javascript/integrations/text_embedding/hugging_face_inference">
    Thousands of embedding models via Hugging Face.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/text_embedding/ibm">
    IBM Watson embedding models and AI services.
  </Card>

<Card title="Jina" href="/oss/javascript/integrations/text_embedding/jina">
    Jina's neural search and embedding models.
  </Card>

<Card title="Llama.cpp" href="/oss/javascript/integrations/text_embedding/llama_cpp">
    Local embedding generation with llama.cpp.
  </Card>

<Card title="Minimax" href="/oss/javascript/integrations/text_embedding/minimax">
    Minimax's text embedding and representation models.
  </Card>

<Card title="Mistral" href="/oss/javascript/integrations/text_embedding/mistralai" icon="mistral">
    Mistral's efficient embedding models.
  </Card>

<Card title="MixedBread AI" href="/oss/javascript/integrations/text_embedding/mixedbread_ai">
    High-quality multilingual embedding models.
  </Card>

<Card title="Nomic" href="/oss/javascript/integrations/text_embedding/nomic">
    Nomic's open-source embedding models.
  </Card>

<Card title="Ollama" href="/oss/javascript/integrations/text_embedding/ollama" icon="ollama">
    Local embedding models through Ollama.
  </Card>

<Card title="OpenAI" href="/oss/javascript/integrations/text_embedding/openai" icon="openai">
    OpenAI's text-embedding models for semantic search.
  </Card>

<Card title="Pinecone" href="/oss/javascript/integrations/text_embedding/pinecone">
    Pinecone's embedding models and vector database.
  </Card>

<Card title="PremAI" href="/oss/javascript/integrations/text_embedding/premai">
    PremAI's embedding models and AI platform.
  </Card>

<Card title="Tencent Hunyuan" href="/oss/javascript/integrations/text_embedding/tencent_hunyuan">
    Tencent's embedding models for Chinese text.
  </Card>

<Card title="TensorFlow" href="/oss/javascript/integrations/text_embedding/tensorflow">
    TensorFlow-based embedding models and inference.
  </Card>

<Card title="Together AI" href="/oss/javascript/integrations/text_embedding/togetherai" icon="together">
    Open-source embedding models on Together platform.
  </Card>

<Card title="Transformers" href="/oss/javascript/integrations/text_embedding/transformers">
    Local transformer-based embedding models.
  </Card>

<Card title="Voyage AI" href="/oss/javascript/integrations/text_embedding/voyageai">
    Voyage AI's domain-specific embedding models.
  </Card>

<Card title="ZhipuAI" href="/oss/javascript/integrations/text_embedding/zhipuai">
    ZhipuAI's Chinese language embedding models.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AnalyticDB" href="/oss/javascript/integrations/vectorstores/analyticdb">
    Alibaba Cloud's AnalyticDB for vector storage and search.
  </Card>

<Card title="AstraDB" href="/oss/javascript/integrations/vectorstores/astradb">
    DataStax Astra DB vector database for scalable storage.
  </Card>

<Card title="Azion EdgeSQL" href="/oss/javascript/integrations/vectorstores/azion-edgesql">
    Edge-based vector storage with Azion's EdgeSQL.
  </Card>

<Card title="Azure AI Search" href="/oss/javascript/integrations/vectorstores/azure_aisearch" icon="microsoft">
    Microsoft Azure's AI-powered search and vector storage.
  </Card>

<Card title="Azure Cosmos DB MongoDB" href="/oss/javascript/integrations/vectorstores/azure_cosmosdb_mongodb" icon="microsoft">
    Vector search in Azure Cosmos DB with MongoDB API.
  </Card>

<Card title="Azure Cosmos DB NoSQL" href="/oss/javascript/integrations/vectorstores/azure_cosmosdb_nosql" icon="microsoft">
    Vector storage in Azure Cosmos DB NoSQL API.
  </Card>

<Card title="Cassandra" href="/oss/javascript/integrations/vectorstores/cassandra">
    Apache Cassandra vector search capabilities.
  </Card>

<Card title="Chroma" href="/oss/javascript/integrations/vectorstores/chroma">
    Open-source embedding database for AI applications.
  </Card>

<Card title="ClickHouse" href="/oss/javascript/integrations/vectorstores/clickhouse">
    Fast columnar database with vector search support.
  </Card>

<Card title="CloseVector" href="/oss/javascript/integrations/vectorstores/closevector">
    High-performance vector database for similarity search.
  </Card>

<Card title="Cloudflare Vectorize" href="/oss/javascript/integrations/vectorstores/cloudflare_vectorize">
    Serverless vector database on Cloudflare's edge.
  </Card>

<Card title="Convex" href="/oss/javascript/integrations/vectorstores/convex">
    Full-stack platform with integrated vector storage.
  </Card>

<Card title="Couchbase Query" href="/oss/javascript/integrations/vectorstores/couchbase_query">
    Recommended vector search method in Couchbase NoSQL database via query service.
  </Card>

<Card title="Couchbase Search" href="/oss/javascript/integrations/vectorstores/couchbase_search">
    Alternative vector search method in Couchbase NoSQL database via search service.
  </Card>

<Card title="Elasticsearch" href="/oss/javascript/integrations/vectorstores/elasticsearch">
    Distributed search engine with vector search support.
  </Card>

<Card title="Faiss" href="/oss/javascript/integrations/vectorstores/faiss">
    Facebook's library for efficient similarity search.
  </Card>

<Card title="Google Cloud SQL PostgreSQL" href="/oss/javascript/integrations/vectorstores/google_cloudsql_pg" icon="google">
    PostgreSQL with vector extensions on Google Cloud.
  </Card>

<Card title="Google Vertex AI" href="/oss/javascript/integrations/vectorstores/googlevertexai" icon="google">
    Vector search through Google Vertex AI platform.
  </Card>

<Card title="SAP HANA Vector" href="/oss/javascript/integrations/vectorstores/hanavector">
    Enterprise vector database with SAP HANA.
  </Card>

<Card title="Hnswlib" href="/oss/javascript/integrations/vectorstores/hnswlib">
    Fast approximate nearest neighbor search library.
  </Card>

<Card title="LanceDB" href="/oss/javascript/integrations/vectorstores/lancedb">
    Developer-friendly embedded vector database.
  </Card>

<Card title="LibSQL" href="/oss/javascript/integrations/vectorstores/libsql">
    SQLite-compatible database with vector extensions.
  </Card>

<Card title="MariaDB" href="/oss/javascript/integrations/vectorstores/mariadb">
    Open-source database with vector search capabilities.
  </Card>

<Card title="Memory Vector Store" href="/oss/javascript/integrations/vectorstores/memory">
    In-memory vector storage for development and testing.
  </Card>

<Card title="Milvus" href="/oss/javascript/integrations/vectorstores/milvus">
    Open-source vector database for AI applications.
  </Card>

<Card title="Momento Vector Index" href="/oss/javascript/integrations/vectorstores/momento_vector_index">
    Serverless vector indexing with Momento's platform.
  </Card>

<Card title="MongoDB Atlas" href="/oss/javascript/integrations/vectorstores/mongodb_atlas">
    Vector search in MongoDB Atlas cloud database.
  </Card>

<Card title="MyScale" href="/oss/javascript/integrations/vectorstores/myscale">
    SQL-compatible vector database for analytics.
  </Card>

<Card title="Neo4j Vector" href="/oss/javascript/integrations/vectorstores/neo4jvector">
    Graph database with integrated vector search.
  </Card>

<Card title="Neon" href="/oss/javascript/integrations/vectorstores/neon">
    Serverless PostgreSQL with vector extensions.
  </Card>

<Card title="OpenSearch" href="/oss/javascript/integrations/vectorstores/opensearch">
    Open-source search engine with vector capabilities.
  </Card>

<Card title="PGVector" href="/oss/javascript/integrations/vectorstores/pgvector">
    PostgreSQL extension for vector similarity search.
  </Card>

<Card title="Pinecone" href="/oss/javascript/integrations/vectorstores/pinecone">
    Managed vector database for machine learning applications.
  </Card>

<Card title="Prisma" href="/oss/javascript/integrations/vectorstores/prisma">
    Type-safe database client with vector support.
  </Card>

<Card title="Qdrant" href="/oss/javascript/integrations/vectorstores/qdrant">
    Open-source vector similarity search engine.
  </Card>

<Card title="Redis" href="/oss/javascript/integrations/vectorstores/redis">
    In-memory database with vector search capabilities.
  </Card>

<Card title="Rockset" href="/oss/javascript/integrations/vectorstores/rockset">
    Real-time analytics database with vector search.
  </Card>

<Card title="SingleStore" href="/oss/javascript/integrations/vectorstores/singlestore">
    Distributed database with built-in vector functions.
  </Card>

<Card title="Supabase" href="/oss/javascript/integrations/vectorstores/supabase">
    Open-source Firebase alternative with vector support.
  </Card>

<Card title="Tigris" href="/oss/javascript/integrations/vectorstores/tigris">
    Developer-focused database with vector search.
  </Card>

<Card title="Turbopuffer" href="/oss/javascript/integrations/vectorstores/turbopuffer">
    High-performance vector database for embeddings.
  </Card>

<Card title="TypeORM" href="/oss/javascript/integrations/vectorstores/typeorm">
    TypeScript ORM with vector database support.
  </Card>

<Card title="Typesense" href="/oss/javascript/integrations/vectorstores/typesense">
    Open-source search engine with vector capabilities.
  </Card>

<Card title="Upstash Vector" href="/oss/javascript/integrations/vectorstores/upstash">
    Serverless vector database with Redis compatibility.
  </Card>

<Card title="USearch" href="/oss/javascript/integrations/vectorstores/usearch">
    Smaller and faster single-file vector search engine.
  </Card>

<Card title="Vectara" href="/oss/javascript/integrations/vectorstores/vectara">
    Neural search platform with built-in understanding.
  </Card>

<Card title="Vercel Postgres" href="/oss/javascript/integrations/vectorstores/vercel_postgres">
    PostgreSQL database with vector extensions on Vercel.
  </Card>

<Card title="Voy" href="/oss/javascript/integrations/vectorstores/voy">
    WebAssembly-based vector database for browsers.
  </Card>

<Card title="Weaviate" href="/oss/javascript/integrations/vectorstores/weaviate">
    Open-source vector database with GraphQL API.
  </Card>

<Card title="Xata" href="/oss/javascript/integrations/vectorstores/xata">
    Serverless database with built-in vector search.
  </Card>

<Card title="Zep Cloud" href="/oss/javascript/integrations/vectorstores/zep_cloud">
    Long-term memory for AI assistants in the cloud.
  </Card>

<Card title="Zep" href="/oss/javascript/integrations/vectorstores/zep">
    Long-term memory for AI assistants and agents.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="ChatGPT" href="/oss/javascript/integrations/document_loaders/file_loaders/chatgpt">
    Load and parse ChatGPT conversation exports.
  </Card>

<Card title="CSV" href="/oss/javascript/integrations/document_loaders/file_loaders/csv">
    Load data from CSV files with customizable parsing.
  </Card>

<Card title="Directory" href="/oss/javascript/integrations/document_loaders/file_loaders/directory">
    Recursively load documents from filesystem directories.
  </Card>

<Card title="DOCX" href="/oss/javascript/integrations/document_loaders/file_loaders/docx">
    Extract text and metadata from Microsoft Word documents.
  </Card>

<Card title="EPUB" href="/oss/javascript/integrations/document_loaders/file_loaders/epub">
    Load and parse EPUB e-book files.
  </Card>

<Card title="JSON" href="/oss/javascript/integrations/document_loaders/file_loaders/json">
    Load and parse JSON files with flexible structure handling.
  </Card>

<Card title="JSON Lines" href="/oss/javascript/integrations/document_loaders/file_loaders/jsonlines">
    Load newline-delimited JSON files.
  </Card>

<Card title="Multi-File" href="/oss/javascript/integrations/document_loaders/file_loaders/multi_file">
    Load multiple files of different types simultaneously.
  </Card>

<Card title="Notion Markdown" href="/oss/javascript/integrations/document_loaders/file_loaders/notion_markdown">
    Load Notion pages exported as Markdown.
  </Card>

<Card title="OpenAI Whisper Audio" href="/oss/javascript/integrations/document_loaders/file_loaders/openai_whisper_audio">
    Transcribe audio files using OpenAI's Whisper model.
  </Card>

<Card title="PDF" href="/oss/javascript/integrations/document_loaders/file_loaders/pdf">
    Extract text from PDF documents.
  </Card>

<Card title="PPTX" href="/oss/javascript/integrations/document_loaders/file_loaders/pptx">
    Load Microsoft PowerPoint presentations.
  </Card>

<Card title="Subtitles" href="/oss/javascript/integrations/document_loaders/file_loaders/subtitles">
    Load subtitle files (SRT, VTT formats).
  </Card>

<Card title="Text" href="/oss/javascript/integrations/document_loaders/file_loaders/text">
    Load plain text files with encoding detection.
  </Card>

<Card title="Unstructured" href="/oss/javascript/integrations/document_loaders/file_loaders/unstructured">
    Load various file formats using Unstructured.io.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Airtable" href="/oss/javascript/integrations/document_loaders/web_loaders/airtable">
    Load records from Airtable bases.
  </Card>

<Card title="Apify Dataset" href="/oss/javascript/integrations/document_loaders/web_loaders/apify_dataset">
    Load data from Apify web scraping datasets.
  </Card>

<Card title="AssemblyAI Audio" href="/oss/javascript/integrations/document_loaders/web_loaders/assemblyai_audio_transcription">
    Transcribe audio using AssemblyAI's API.
  </Card>

<Card title="Azure Blob Storage Container" href="/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_container" icon="microsoft">
    Load files from Azure Blob Storage containers.
  </Card>

<Card title="Azure Blob Storage File" href="/oss/javascript/integrations/document_loaders/web_loaders/azure_blob_storage_file" icon="microsoft">
    Load individual files from Azure Blob Storage.
  </Card>

<Card title="Browserbase" href="/oss/javascript/integrations/document_loaders/web_loaders/browserbase">
    Load web content using Browserbase's cloud browsers.
  </Card>

<Card title="College Confidential" href="/oss/javascript/integrations/document_loaders/web_loaders/college_confidential">
    Scrape College Confidential forum content.
  </Card>

<Card title="Confluence" href="/oss/javascript/integrations/document_loaders/web_loaders/confluence">
    Load pages from Atlassian Confluence.
  </Card>

<Card title="Couchbase" href="/oss/javascript/integrations/document_loaders/web_loaders/couchbase">
    Load documents from Couchbase databases.
  </Card>

<Card title="Figma" href="/oss/javascript/integrations/document_loaders/web_loaders/figma">
    Load Figma design files and comments.
  </Card>

<Card title="Firecrawl" href="/oss/javascript/integrations/document_loaders/web_loaders/firecrawl">
    Crawl websites using Firecrawl's web scraping API.
  </Card>

<Card title="GitBook" href="/oss/javascript/integrations/document_loaders/web_loaders/gitbook">
    Load content from GitBook documentation sites.
  </Card>

<Card title="GitHub" href="/oss/javascript/integrations/document_loaders/web_loaders/github">
    Load files and repositories from GitHub.
  </Card>

<Card title="Google Cloud Storage" href="/oss/javascript/integrations/document_loaders/web_loaders/google_cloud_storage" icon="google">
    Load files from Google Cloud Storage buckets.
  </Card>

<Card title="Google Cloud SQL PostgreSQL" href="/oss/javascript/integrations/document_loaders/web_loaders/google_cloudsql_pg" icon="google">
    Load data from Google Cloud SQL PostgreSQL databases.
  </Card>

<Card title="Hacker News" href="/oss/javascript/integrations/document_loaders/web_loaders/hn">
    Load posts and comments from Hacker News.
  </Card>

<Card title="IMSDb" href="/oss/javascript/integrations/document_loaders/web_loaders/imsdb">
    Load movie scripts from the Internet Movie Script Database.
  </Card>

<Card title="Jira" href="/oss/javascript/integrations/document_loaders/web_loaders/jira">
    Load issues and projects from Atlassian Jira.
  </Card>

<Card title="LangSmith" href="/oss/javascript/integrations/document_loaders/web_loaders/langsmith">
    Load runs and datasets from LangSmith.
  </Card>

<Card title="Notion API" href="/oss/javascript/integrations/document_loaders/web_loaders/notionapi">
    Load pages and databases from Notion.
  </Card>

<Card title="PDF (Web)" href="/oss/javascript/integrations/document_loaders/web_loaders/pdf">
    Load PDF files from web URLs.
  </Card>

<Card title="Recursive URL" href="/oss/javascript/integrations/document_loaders/web_loaders/recursive_url_loader">
    Recursively crawl and load web pages.
  </Card>

<Card title="S3" href="/oss/javascript/integrations/document_loaders/web_loaders/s3" icon="aws">
    Load files from Amazon S3 buckets.
  </Card>

<Card title="SearchAPI" href="/oss/javascript/integrations/document_loaders/web_loaders/searchapi">
    Load search results using SearchAPI.
  </Card>

<Card title="SerpAPI" href="/oss/javascript/integrations/document_loaders/web_loaders/serpapi">
    Load search results using SerpAPI.
  </Card>

<Card title="Sitemap" href="/oss/javascript/integrations/document_loaders/web_loaders/sitemap">
    Load URLs from website sitemaps.
  </Card>

<Card title="Sonix Audio" href="/oss/javascript/integrations/document_loaders/web_loaders/sonix_audio_transcription">
    Transcribe audio using Sonix's transcription API.
  </Card>

<Card title="Sort.xyz Blockchain" href="/oss/javascript/integrations/document_loaders/web_loaders/sort_xyz_blockchain">
    Load blockchain data from Sort.xyz.
  </Card>

<Card title="Spider" href="/oss/javascript/integrations/document_loaders/web_loaders/spider">
    Fast web crawling using Spider API.
  </Card>

<Card title="Taskade" href="/oss/javascript/integrations/document_loaders/web_loaders/taskade">
    Load projects and tasks from Taskade.
  </Card>

<Card title="Web Cheerio" href="/oss/javascript/integrations/document_loaders/web_loaders/web_cheerio">
    Scrape web pages using Cheerio for server-side parsing.
  </Card>

<Card title="Web Playwright" href="/oss/javascript/integrations/document_loaders/web_loaders/web_playwright">
    Load dynamic web content using Playwright browser automation.
  </Card>

<Card title="Web Puppeteer" href="/oss/javascript/integrations/document_loaders/web_loaders/web_puppeteer">
    Scrape JavaScript-heavy websites using Puppeteer.
  </Card>

<Card title="YouTube" href="/oss/javascript/integrations/document_loaders/web_loaders/youtube">
    Load YouTube video transcripts and metadata.
  </Card>
</Columns>

## Document Transformers

<Columns cols={3}>
  <Card title="HTML to Text" href="/oss/javascript/integrations/document_transformers/html-to-text">
    Convert HTML content to clean, readable text.
  </Card>

<Card title="Mozilla Readability" href="/oss/javascript/integrations/document_transformers/mozilla_readability">
    Extract main content from web pages using Mozilla's Readability.
  </Card>

<Card title="OpenAI Metadata Tagger" href="/oss/javascript/integrations/document_transformers/openai_metadata_tagger" icon="openai">
    Generate metadata tags for documents using OpenAI.
  </Card>
</Columns>

## Document Compressors

<Columns cols={3}>
  <Card title="Cohere Rerank" href="/oss/javascript/integrations/document_compressors/cohere_rerank" icon="cohere">
    Rerank documents using Cohere's reranking models.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/document_compressors/ibm">
    Document compression using IBM Watson AI services.
  </Card>

<Card title="MixedBread AI" href="/oss/javascript/integrations/document_compressors/mixedbread_ai">
    Rerank and compress documents using MixedBread AI.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="AI Plugin Tool" href="/oss/javascript/integrations/tools/aiplugin-tool">
    Execute OpenAI ChatGPT plugins as tools.
  </Card>

<Card title="Azure Dynamic Sessions" href="/oss/javascript/integrations/tools/azure_dynamic_sessions" icon="microsoft">
    Secure code execution in Azure Dynamic Sessions.
  </Card>

<Card title="Connery" href="/oss/javascript/integrations/tools/connery">
    Modular AI actions and integrations with Connery.
  </Card>

<Card title="Connery Toolkit" href="/oss/javascript/integrations/tools/connery_toolkit">
    Access Connery's toolkit of pre-built actions.
  </Card>

<Card title="DALL-E" href="/oss/javascript/integrations/tools/dalle" icon="openai">
    Generate images using OpenAI's DALL-E models.
  </Card>

<Card title="Decodo" href="/oss/javascript/integrations/tools/decodo">
    Code execution and analysis with Decodo.
  </Card>

<Card title="Discord" href="/oss/javascript/integrations/tools/discord_tool">
    Interact with Discord servers and channels.
  </Card>

<Card title="DuckDuckGo Search" href="/oss/javascript/integrations/tools/duckduckgo_search">
    Privacy-focused web search with DuckDuckGo.
  </Card>

<Card title="Exa Search" href="/oss/javascript/integrations/tools/exa_search">
    AI-powered search engine for better results.
  </Card>

<Card title="Gmail" href="/oss/javascript/integrations/tools/google_gmail" icon="google">
    Read and send emails through Gmail API.
  </Card>

<Card title="Goat" href="/oss/javascript/integrations/tools/goat">
    Simple tool execution framework.
  </Card>

<Card title="Google Calendar" href="/oss/javascript/integrations/tools/google_calendar" icon="google">
    Manage events and schedules in Google Calendar.
  </Card>

<Card title="Google Places" href="/oss/javascript/integrations/tools/google_places" icon="google">
    Search for places using Google Places API.
  </Card>

<Card title="Google Routes" href="/oss/javascript/integrations/tools/google_routes" icon="google">
    Get directions and routing information.
  </Card>

<Card title="Google Scholar" href="/oss/javascript/integrations/tools/google_scholar" icon="google">
    Search academic papers and citations.
  </Card>

<Card title="Google Trends" href="/oss/javascript/integrations/tools/google_trends" icon="google">
    Analyze search trends and popularity data.
  </Card>

<Card title="IBM" href="/oss/javascript/integrations/tools/ibm">
    Access IBM Watson AI tools and services.
  </Card>

<Card title="JigsawStack" href="/oss/javascript/integrations/tools/jigsawstack">
    AI infrastructure tools from JigsawStack.
  </Card>

<Card title="JSON Tool" href="/oss/javascript/integrations/tools/json">
    Parse and manipulate JSON data structures.
  </Card>

<Card title="Lambda Agent" href="/oss/javascript/integrations/tools/lambda_agent" icon="aws">
    Execute code in AWS Lambda functions.
  </Card>

<Card title="MCP Toolbox" href="/oss/javascript/integrations/tools/mcp_toolbox">
    Model Context Protocol tools and utilities.
  </Card>

<Card title="OpenAPI" href="/oss/javascript/integrations/tools/openapi">
    Generate tools from OpenAPI specifications.
  </Card>

<Card title="Python Interpreter" href="/oss/javascript/integrations/tools/pyinterpreter">
    Execute Python code in a sandboxed environment.
  </Card>

<Card title="SearchAPI" href="/oss/javascript/integrations/tools/searchapi">
    Web search capabilities through SearchAPI.
  </Card>

<Card title="SearXNG" href="/oss/javascript/integrations/tools/searxng">
    Privacy-respecting metasearch engine.
  </Card>

<Card title="SerpAPI" href="/oss/javascript/integrations/tools/serpapi">
    Google Search results through SerpAPI.
  </Card>

<Card title="Step Functions Agent" href="/oss/javascript/integrations/tools/sfn_agent" icon="aws">
    Execute AWS Step Functions workflows.
  </Card>

<Card title="SQL" href="/oss/javascript/integrations/tools/sql">
    Query databases using natural language.
  </Card>

<Card title="StackExchange" href="/oss/javascript/integrations/tools/stackexchange">
    Search Stack Overflow and other SE sites.
  </Card>

<Card title="Stagehand" href="/oss/javascript/integrations/tools/stagehand">
    Browser automation for web interactions.
  </Card>

<Card title="Tavily Crawl" href="/oss/javascript/integrations/tools/tavily_crawl">
    Web crawling capabilities with Tavily.
  </Card>

<Card title="Tavily Extract" href="/oss/javascript/integrations/tools/tavily_extract">
    Extract structured data from web pages.
  </Card>

<Card title="Tavily Map" href="/oss/javascript/integrations/tools/tavily_map">
    Map and visualize web crawling results.
  </Card>

<Card title="Tavily Search" href="/oss/javascript/integrations/tools/tavily_search">
    AI-optimized search for retrieval applications.
  </Card>

<Card title="Tavily Search Community" href="/oss/javascript/integrations/tools/tavily_search_community">
    Community-powered search through Tavily.
  </Card>

<Card title="Vector Store" href="/oss/javascript/integrations/tools/vectorstore">
    Query vector databases as tools.
  </Card>

<Card title="Web Browser" href="/oss/javascript/integrations/tools/webbrowser">
    Automated web browsing and interaction.
  </Card>

<Card title="Wikipedia" href="/oss/javascript/integrations/tools/wikipedia">
    Search and retrieve Wikipedia articles.
  </Card>

<Card title="Wolfram Alpha" href="/oss/javascript/integrations/tools/wolframalpha">
    Computational knowledge through Wolfram Alpha.
  </Card>

<Card title="Zapier Agent" href="/oss/javascript/integrations/tools/zapier_agent">
    Automate workflows using Zapier integrations.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="ArXiv" href="/oss/javascript/integrations/retrievers/arxiv-retriever">
    Search and retrieve academic papers from ArXiv.
  </Card>

<Card title="Azion EdgeSQL" href="/oss/javascript/integrations/retrievers/azion-edgesql">
    Edge-based document retrieval with Azion.
  </Card>

<Card title="Bedrock Knowledge Bases" href="/oss/javascript/integrations/retrievers/bedrock-knowledge-bases" icon="aws">
    Retrieve from Amazon Bedrock Knowledge Bases.
  </Card>

<Card title="BM25" href="/oss/javascript/integrations/retrievers/bm25">
    BM25 algorithm for keyword-based retrieval.
  </Card>

<Card title="Chaindesk" href="/oss/javascript/integrations/retrievers/chaindesk-retriever">
    Document retrieval using Chaindesk platform.
  </Card>

<Card title="ChatGPT Retriever Plugin" href="/oss/javascript/integrations/retrievers/chatgpt-retriever-plugin" icon="openai">
    Official ChatGPT retriever plugin integration.
  </Card>

<Card title="Dria" href="/oss/javascript/integrations/retrievers/dria">
    Decentralized knowledge retrieval with Dria.
  </Card>

<Card title="Exa" href="/oss/javascript/integrations/retrievers/exa">
    AI-powered web search and retrieval.
  </Card>

<Card title="HyDE" href="/oss/javascript/integrations/retrievers/hyde">
    Hypothetical Document Embeddings for better retrieval.
  </Card>

<Card title="Kendra" href="/oss/javascript/integrations/retrievers/kendra-retriever" icon="aws">
    Enterprise search with Amazon Kendra.
  </Card>

<Card title="Metal" href="/oss/javascript/integrations/retrievers/metal-retriever">
    Managed vector search with Metal.
  </Card>

<Card title="Supabase Hybrid" href="/oss/javascript/integrations/retrievers/supabase-hybrid">
    Hybrid search combining vector and keyword search.
  </Card>

<Card title="Tavily" href="/oss/javascript/integrations/retrievers/tavily">
    AI-optimized search for RAG applications.
  </Card>

<Card title="Time-Weighted" href="/oss/javascript/integrations/retrievers/time-weighted-retriever">
    Time-aware document retrieval and ranking.
  </Card>

<Card title="Vespa" href="/oss/javascript/integrations/retrievers/vespa-retriever">
    Big data serving engine for vector search.
  </Card>

<Card title="Zep Cloud" href="/oss/javascript/integrations/retrievers/zep-cloud-retriever">
    Cloud-based long-term memory retrieval.
  </Card>

<Card title="Zep" href="/oss/javascript/integrations/retrievers/zep-retriever">
    Long-term memory and context retrieval.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Cassandra Storage" href="/oss/javascript/integrations/stores/cassandra_storage">
    Distributed key-value storage using Cassandra.
  </Card>

<Card title="File System" href="/oss/javascript/integrations/stores/file_system">
    Local file system storage for development.
  </Card>

<Card title="In-Memory" href="/oss/javascript/integrations/stores/in_memory">
    Fast in-memory storage for temporary data.
  </Card>

<Card title="IoRedis Storage" href="/oss/javascript/integrations/stores/ioredis_storage">
    Redis-based storage using IoRedis client.
  </Card>

<Card title="Upstash Redis Storage" href="/oss/javascript/integrations/stores/upstash_redis_storage">
    Serverless Redis storage with Upstash.
  </Card>

<Card title="Vercel KV Storage" href="/oss/javascript/integrations/stores/vercel_kv_storage">
    Key-value storage on Vercel's edge network.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Azure Cosmos DB NoSQL" href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql" icon="microsoft">
    Cache LLM responses in Azure Cosmos DB.
  </Card>
</Columns>

<Columns cols={3}>
  <Card title="Datadog Tracer" href="/oss/javascript/integrations/callbacks/datadog_tracer">
    Monitor and trace LangChain applications with Datadog.
  </Card>

<Card title="Upstash Rate Limit" href="/oss/javascript/integrations/callbacks/upstash_ratelimit_callback">
    Rate limiting for AI applications using Upstash.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/providers/all_providers.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## All integration providers

**URL:** llms-txt#all-integration-providers

**Contents:**
- Providers

Source: https://docs.langchain.com/oss/python/integrations/providers/all_providers

Browse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more.

<Columns cols={3}>
  <Card title="Abso" href="/oss/python/integrations/providers/abso" icon="link">
    Custom AI integration platform for enterprise workflows.
  </Card>

<Card title="Acreom" href="/oss/python/integrations/providers/acreom" icon="link">
    Knowledge management platform with AI-powered organization.
  </Card>

<Card title="ActiveLoop DeepLake" href="/oss/python/integrations/providers/activeloop_deeplake" icon="link">
    Vector database for AI applications with deep learning focus.
  </Card>

<Card title="Ads4GPTs" href="/oss/python/integrations/providers/ads4gpts" icon="link">
    Advertising platform for GPT applications and AI services.
  </Card>

<Card title="AG-UI Protocol" href="https://docs.ag-ui.com/getting-started/quickstart-langgraph-python" icon="link">
    Open event-based protocol for connecting LangGraph agents to any frontend.
  </Card>

<Card title="AgentQL" href="/oss/python/integrations/providers/agentql" icon="link">
    Web scraping with natural language queries.
  </Card>

<Card title="AI21" href="/oss/python/integrations/providers/ai21" icon="link">
    AI21 Labs' Jurassic models for text generation.
  </Card>

<Card title="AIM Tracking" href="/oss/python/integrations/providers/aim_tracking" icon="link">
    Experiment tracking and management platform.
  </Card>

<Card title="AI/ML API" href="/oss/python/integrations/providers/aimlapi" icon="link">
    Unified API for multiple AI and ML services.
  </Card>

<Card title="AI Network" href="/oss/python/integrations/providers/ainetwork" icon="link">
    Decentralized AI computing network platform.
  </Card>

<Card title="Airbyte" href="/oss/python/integrations/providers/airbyte" icon="link">
    Data integration platform for ETL and ELT pipelines.
  </Card>

<Card title="Airtable" href="/oss/python/integrations/providers/airtable" icon="link">
    Cloud-based spreadsheet and database platform.
  </Card>

<Card title="Alchemy" href="/oss/python/integrations/providers/alchemy" icon="link">
    Blockchain development platform and APIs.
  </Card>

<Card title="Aleph Alpha" href="/oss/python/integrations/providers/aleph_alpha" icon="link">
    European AI company's multilingual language models.
  </Card>

<Card title="Alibaba Cloud" href="/oss/python/integrations/providers/alibaba_cloud" icon="link">
    Alibaba's cloud computing and AI services.
  </Card>

<Card title="AnalyticDB" href="/oss/python/integrations/providers/analyticdb" icon="link">
    Alibaba Cloud's real-time analytics database.
  </Card>

<Card title="Anchor Browser" href="/oss/python/integrations/providers/anchor_browser" icon="link">
    Browser automation and web scraping tools.
  </Card>

<Card title="Annoy" href="/oss/python/integrations/providers/annoy" icon="link">
    Approximate nearest neighbors search library.
  </Card>

<Card title="Anthropic" href="/oss/python/integrations/providers/anthropic" icon="anthropic">
    Claude models for advanced reasoning and conversation.
  </Card>

<Card title="Anyscale" href="/oss/python/integrations/providers/anyscale" icon="link">
    Distributed computing platform for ML workloads.
  </Card>

<Card title="Apache Doris" href="/oss/python/integrations/providers/apache_doris" icon="link">
    Real-time analytical database management system.
  </Card>

<Card title="Apache" href="/oss/python/integrations/providers/apache" icon="link">
    Apache Software Foundation tools and libraries.
  </Card>

<Card title="Apify" href="/oss/python/integrations/providers/apify" icon="link">
    Web scraping and automation platform.
  </Card>

<Card title="Apple" href="/oss/python/integrations/providers/apple" icon="link">
    Apple's machine learning and AI frameworks.
  </Card>

<Card title="ArangoDB" href="/oss/python/integrations/providers/arangodb" icon="link">
    Multi-model database with graph capabilities.
  </Card>

<Card title="Arcee" href="/oss/python/integrations/providers/arcee" icon="link">
    Domain-specific language model training platform.
  </Card>

<Card title="ArcGIS" href="/oss/python/integrations/providers/arcgis" icon="link">
    Geographic information system platform.
  </Card>

<Card title="Argilla" href="/oss/python/integrations/providers/argilla" icon="link">
    Data labeling and annotation platform for NLP.
  </Card>

<Card title="Arize" href="/oss/python/integrations/providers/arize" icon="link">
    ML observability and performance monitoring.
  </Card>

<Card title="Arthur Tracking" href="/oss/python/integrations/providers/arthur_tracking" icon="link">
    AI model monitoring and governance platform.
  </Card>

<Card title="arXiv" href="/oss/python/integrations/providers/arxiv" icon="link">
    Academic paper repository and search platform.
  </Card>

<Card title="Ascend" href="/oss/python/integrations/providers/ascend" icon="link">
    Data engineering and pipeline automation platform.
  </Card>

<Card title="Ask News" href="/oss/python/integrations/providers/asknews" icon="link">
    Real-time news search and analysis API.
  </Card>

<Card title="AssemblyAI" href="/oss/python/integrations/providers/assemblyai" icon="link">
    Speech-to-text and audio intelligence API.
  </Card>

<Card title="assistant-ui" icon="file-code" href="https://www.assistant-ui.com/docs/runtimes/langgraph">
    React framework for building AI chat interfaces with streaming support and LangGraph integration.
  </Card>

<Card title="AstraDB" href="/oss/python/integrations/providers/astradb" icon="link">
    DataStax Astra DB vector database platform.
  </Card>

<Card title="Atlas" href="/oss/python/integrations/providers/atlas" icon="link">
    Data visualization and exploration platform.
  </Card>

<Card title="AwaDB" href="/oss/python/integrations/providers/awadb" icon="link">
    Vector database for AI and ML applications.
  </Card>

<Card title="AWS" href="/oss/python/integrations/providers/aws" icon="aws">
    Amazon Web Services cloud platform and AI services.
  </Card>

<Card title="AZLyrics" href="/oss/python/integrations/providers/azlyrics" icon="link">
    Song lyrics database and search platform.
  </Card>

<Card title="Azure AI" href="/oss/python/integrations/providers/azure_ai" icon="microsoft">
    Microsoft Azure AI and cognitive services.
  </Card>

<Card title="BAAI" href="/oss/python/integrations/providers/baai" icon="link">
    Beijing Academy of AI research and models.
  </Card>

<Card title="Bagel" href="/oss/python/integrations/providers/bagel" icon="link">
    Vector database and semantic search platform.
  </Card>

<Card title="BagelDB" href="/oss/python/integrations/providers/bageldb" icon="link">
    Multi-modal AI database and storage system.
  </Card>

<Card title="Baichuan" href="/oss/python/integrations/providers/baichuan" icon="link">
    Chinese language model from Baichuan AI.
  </Card>

<Card title="Baidu" href="/oss/python/integrations/providers/baidu" icon="link">
    Baidu's AI services and language models.
  </Card>

<Card title="BananaDev" href="/oss/python/integrations/providers/bananadev" icon="link">
    Serverless GPU infrastructure for ML models.
  </Card>

<Card title="Baseten" href="/oss/python/integrations/providers/baseten" icon="link">
    ML model deployment and serving platform.
  </Card>

<Card title="Beam" href="/oss/python/integrations/providers/beam" icon="link">
    Serverless GPU computing platform.
  </Card>

<Card title="Beautiful Soup" href="/oss/python/integrations/providers/beautiful_soup" icon="link">
    HTML and XML parsing library for web scraping.
  </Card>

<Card title="BibTeX" href="/oss/python/integrations/providers/bibtex" icon="link">
    Bibliography management and citation format.
  </Card>

<Card title="Bilibili" href="/oss/python/integrations/providers/bilibili" icon="link">
    Chinese video sharing platform integration.
  </Card>

<Card title="Bittensor" href="/oss/python/integrations/providers/bittensor" icon="link">
    Decentralized AI network and incentive protocol.
  </Card>

<Card title="Blackboard" href="/oss/python/integrations/providers/blackboard" icon="link">
    Educational technology and learning management.
  </Card>

<Card title="Bodo DataFrames" href="/oss/python/integrations/providers/bodo" icon="link">
    High-performance analytics and data processing.
  </Card>

<Card title="BookendAI" href="/oss/python/integrations/providers/bookendai" icon="link">
    AI-powered reading and research assistant.
  </Card>

<Card title="Box" href="/oss/python/integrations/providers/box" icon="link">
    Cloud content management and collaboration.
  </Card>

<Card title="Brave Search" href="/oss/python/integrations/providers/brave_search" icon="link">
    Privacy-focused search engine API.
  </Card>

<Card title="Breebs" href="/oss/python/integrations/providers/breebs" icon="link">
    AI knowledge management and retrieval platform.
  </Card>

<Card title="Brightdata" href="/oss/python/integrations/providers/brightdata" icon="link">
    Web data platform and proxy services.
  </Card>

<Card title="Browserbase" href="/oss/python/integrations/providers/browserbase" icon="link">
    Headless browser automation platform.
  </Card>

<Card title="Browserless" href="/oss/python/integrations/providers/browserless" icon="link">
    Serverless browser automation service.
  </Card>

<Card title="ByteDance" href="/oss/python/integrations/providers/byte_dance" icon="link">
    ByteDance's AI models and services.
  </Card>

<Card title="Cassandra" href="/oss/python/integrations/providers/cassandra" icon="link">
    Distributed NoSQL database management system.
  </Card>

<Card title="Cerebras" href="/oss/python/integrations/providers/cerebras" icon="link">
    AI compute platform with specialized processors.
  </Card>

<Card title="CerebriumAI" href="/oss/python/integrations/providers/cerebriumai" icon="link">
    Serverless GPU platform for AI applications.
  </Card>

<Card title="Chaindesk" href="/oss/python/integrations/providers/chaindesk" icon="link">
    No-code AI chatbot and automation platform.
  </Card>

<Card title="Chroma" href="/oss/python/integrations/providers/chroma" icon="link">
    Open-source embedding database for AI apps.
  </Card>

<Card title="Clarifai" href="/oss/python/integrations/providers/clarifai" icon="link">
    Computer vision and AI model platform.
  </Card>

<Card title="ClearML Tracking" href="/oss/python/integrations/providers/clearml_tracking" icon="link">
    ML experiment tracking and automation.
  </Card>

<Card title="CopilotKit" href="https://docs.copilotkit.ai/langgraph/" icon="link">
    React framework with pre-built UI components for AI copilots.
  </Card>

<Card title="ClickHouse" href="/oss/python/integrations/providers/clickhouse" icon="link">
    Fast columnar database for analytics.
  </Card>

<Card title="ClickUp" href="/oss/python/integrations/providers/clickup" icon="link">
    Project management and productivity platform.
  </Card>

<Card title="Cloudflare" href="/oss/python/integrations/providers/cloudflare" icon="link">
    Web infrastructure and security services.
  </Card>

<Card title="Clova" href="/oss/python/integrations/providers/clova" icon="link">
    Naver's AI assistant and NLP platform.
  </Card>

<Card title="CnosDB" href="/oss/python/integrations/providers/cnosdb" icon="link">
    Time series database for IoT and analytics.
  </Card>

<Card title="Cognee" href="/oss/python/integrations/providers/cognee" icon="link">
    Memory layer for AI applications and agents.
  </Card>

<Card title="CogniSwitch" href="/oss/python/integrations/providers/cogniswitch" icon="link">
    AI knowledge management and retrieval system.
  </Card>

<Card title="Cohere" href="/oss/python/integrations/providers/cohere" icon="link">
    Language AI platform for enterprise applications.
  </Card>

<Card title="College Confidential" href="/oss/python/integrations/providers/college_confidential" icon="link">
    College admissions and education platform.
  </Card>

<Card title="Comet Tracking" href="/oss/python/integrations/providers/comet_tracking" icon="link">
    ML experiment tracking and model management.
  </Card>

<Card title="Confident" href="/oss/python/integrations/providers/confident" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="Confluence" href="/oss/python/integrations/providers/confluence" icon="link">
    Team collaboration and documentation platform.
  </Card>

<Card title="Connery" href="/oss/python/integrations/providers/connery" icon="link">
    Plugin system for AI agents and applications.
  </Card>

<Card title="Context" href="/oss/python/integrations/providers/context" icon="link">
    Context management for AI applications.
  </Card>

<Card title="Contextual" href="/oss/python/integrations/providers/contextual" icon="link">
    Contextual AI and language understanding.
  </Card>

<Card title="Couchbase" href="/oss/python/integrations/providers/couchbase" icon="link">
    NoSQL cloud database platform.
  </Card>

<Card title="Coze" href="/oss/python/integrations/providers/coze" icon="link">
    Conversational AI platform and chatbot builder.
  </Card>

<Card title="CrateDB" href="/oss/python/integrations/providers/cratedb" icon="link">
    Distributed SQL database for machine data.
  </Card>

<Card title="CTransformers" href="/oss/python/integrations/providers/ctransformers" icon="link">
    Python bindings for transformer models in C/C++.
  </Card>

<Card title="CTranslate2" href="/oss/python/integrations/providers/ctranslate2" icon="link">
    Fast inference engine for Transformer models.
  </Card>

<Card title="Cube" href="/oss/python/integrations/providers/cube" icon="link">
    Semantic layer for building data applications.
  </Card>

<Card title="Dappier" href="/oss/python/integrations/providers/dappier" icon="link">
    Real-time AI data platform and API.
  </Card>

<Card title="DashVector" href="/oss/python/integrations/providers/dashvector" icon="link">
    Alibaba Cloud's vector database service.
  </Card>

<Card title="Databricks" href="/oss/python/integrations/providers/databricks" icon="link">
    Unified analytics platform for big data and ML.
  </Card>

<Card title="Datadog" href="/oss/python/integrations/providers/datadog" icon="link">
    Monitoring and analytics platform for applications.
  </Card>

<Card title="Datadog Logs" href="/oss/python/integrations/providers/datadog_logs" icon="link">
    Log management and analysis platform.
  </Card>

<Card title="DataForSEO" href="/oss/python/integrations/providers/dataforseo" icon="link">
    SEO and SERP data API platform.
  </Card>

<Card title="DataHerald" href="/oss/python/integrations/providers/dataherald" icon="link">
    Natural language to SQL query platform.
  </Card>

<Card title="Daytona" href="/oss/python/integrations/providers/daytona" icon="link">
    Secure and elastic infrastructure for running your AI-generated code.
  </Card>

<Card title="Dedoc" href="/oss/python/integrations/providers/dedoc" icon="link">
    Document analysis and structure detection.
  </Card>

<Card title="DeepInfra" href="/oss/python/integrations/providers/deepinfra" icon="link">
    Serverless inference for deep learning models.
  </Card>

<Card title="DeepLake" href="/oss/python/integrations/providers/deeplake" icon="link">
    Vector database for deep learning applications.
  </Card>

<Card title="DeepSeek" href="/oss/python/integrations/providers/deepseek" icon="link">
    Advanced reasoning and coding AI models.
  </Card>

<Card title="DeepSparse" href="/oss/python/integrations/providers/deepsparse" icon="link">
    Inference runtime for sparse neural networks.
  </Card>

<Card title="Dell" href="/oss/python/integrations/providers/dell" icon="link">
    Dell Technologies AI and computing solutions.
  </Card>

<Card title="Diffbot" href="/oss/python/integrations/providers/diffbot" icon="link">
    Web data extraction and knowledge graph.
  </Card>

<Card title="Dingo" href="/oss/python/integrations/providers/dingo" icon="link">
    Distributed vector database system.
  </Card>

<Card title="Discord" href="/oss/python/integrations/providers/discord" icon="link">
    Communication platform integration and bots.
  </Card>

<Card title="Discord Shikenso" href="/oss/python/integrations/providers/discord-shikenso" icon="link">
    Discord analytics and moderation tools.
  </Card>

<Card title="DocArray" href="/oss/python/integrations/providers/docarray" icon="link">
    Data structure for multimodal AI applications.
  </Card>

<Card title="Docling" href="/oss/python/integrations/providers/docling" icon="link">
    Document processing and AI integration.
  </Card>

<Card title="Doctran" href="/oss/python/integrations/providers/doctran" icon="link">
    Document transformation and processing.
  </Card>

<Card title="Docugami" href="/oss/python/integrations/providers/docugami" icon="link">
    Document AI and semantic processing.
  </Card>

<Card title="Docusaurus" href="/oss/python/integrations/providers/docusaurus" icon="link">
    Documentation website generator and platform.
  </Card>

<Card title="Dria" href="/oss/python/integrations/providers/dria" icon="link">
    Decentralized knowledge retrieval network.
  </Card>

<Card title="Dropbox" href="/oss/python/integrations/providers/dropbox" icon="link">
    Cloud storage and file sharing platform.
  </Card>

<Card title="DuckDB" href="/oss/python/integrations/providers/duckdb" icon="link">
    In-process SQL OLAP database management system.
  </Card>

<Card title="DuckDuckGo Search" href="/oss/python/integrations/providers/duckduckgo_search" icon="link">
    Privacy-focused search engine integration.
  </Card>

<Card title="E2B" href="/oss/python/integrations/providers/e2b" icon="link">
    Cloud development environment platform.
  </Card>

<Card title="EdenAI" href="/oss/python/integrations/providers/edenai" icon="link">
    Unified API for multiple AI services.
  </Card>

<Card title="Elasticsearch" href="/oss/python/integrations/providers/elasticsearch" icon="link">
    Distributed search and analytics engine.
  </Card>

<Card title="ElevenLabs" href="/oss/python/integrations/providers/elevenlabs" icon="link">
    AI voice synthesis and speech platform.
  </Card>

<Card title="EmbedChain" href="/oss/python/integrations/providers/embedchain" icon="link">
    Framework for creating RAG applications.
  </Card>

<Card title="Epsilla" href="/oss/python/integrations/providers/epsilla" icon="link">
    Vector database for AI and ML applications.
  </Card>

<Card title="Etherscan" href="/oss/python/integrations/providers/etherscan" icon="link">
    Ethereum blockchain explorer and analytics.
  </Card>

<Card title="EverlyAI" href="/oss/python/integrations/providers/everlyai" icon="link">
    Serverless AI inference platform.
  </Card>

<Card title="Evernote" href="/oss/python/integrations/providers/evernote" icon="link">
    Note-taking and organization platform.
  </Card>

<Card title="Exa Search" href="/oss/python/integrations/providers/exa_search" icon="link">
    AI-powered search engine for developers.
  </Card>

<Card title="Facebook" href="/oss/python/integrations/providers/facebook" icon="link">
    Meta's social platform integration and APIs.
  </Card>

<Card title="FalkorDB" href="/oss/python/integrations/providers/falkordb" icon="link">
    Graph database with ultra-low latency.
  </Card>

<Card title="Fauna" href="/oss/python/integrations/providers/fauna" icon="link">
    Serverless, globally distributed database.
  </Card>

<Card title="Featherless AI" href="/oss/python/integrations/providers/featherless-ai" icon="link">
    Fast and efficient AI model serving.
  </Card>

<Card title="Fiddler" href="/oss/python/integrations/providers/fiddler" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="Figma" href="/oss/python/integrations/providers/figma" icon="link">
    Design collaboration and prototyping platform.
  </Card>

<Card title="FireCrawl" href="/oss/python/integrations/providers/firecrawl" icon="link">
    Web scraping and crawling API service.
  </Card>

<Card title="Fireworks" href="/oss/python/integrations/providers/fireworks" icon="link">
    Fast inference platform for open-source models.
  </Card>

<Card title="Flyte" href="/oss/python/integrations/providers/flyte" icon="link">
    Workflow orchestration for ML and data processing.
  </Card>

<Card title="FMP Data" href="/oss/python/integrations/providers/fmp-data" icon="link">
    Financial market data and analytics API.
  </Card>

<Card title="ForefrontAI" href="/oss/python/integrations/providers/forefrontai" icon="link">
    Fine-tuning platform for language models.
  </Card>

<Card title="Friendli" href="/oss/python/integrations/providers/friendli" icon="link">
    Optimized serving engine for AI models.
  </Card>

<Card title="Galaxia" href="/oss/python/integrations/providers/galaxia" icon="link">
    Prompt-driven engineering assistant.
  </Card>

<Card title="Gel" href="/oss/python/integrations/providers/gel" icon="link">
    Knowledge extraction and NLP platform.
  </Card>

<Card title="GeoPandas" href="/oss/python/integrations/providers/geopandas" icon="link">
    Geographic data analysis with Python.
  </Card>

<Card title="Git" href="/oss/python/integrations/providers/git" icon="link">
    Version control system integration.
  </Card>

<Card title="GitBook" href="/oss/python/integrations/providers/gitbook" icon="link">
    Documentation platform and knowledge base.
  </Card>

<Card title="GitHub" href="/oss/python/integrations/providers/github" icon="link">
    Code hosting and collaboration platform.
  </Card>

<Card title="GitLab" href="/oss/python/integrations/providers/gitlab" icon="link">
    DevOps platform and code repository.
  </Card>

<Card title="GOAT" href="/oss/python/integrations/providers/goat" icon="link">
    Tool use framework for AI agents.
  </Card>

<Card title="Golden" href="/oss/python/integrations/providers/golden" icon="link">
    Knowledge graph and data platform.
  </Card>

<Card title="Google" href="/oss/python/integrations/providers/google" icon="google">
    Google's AI services and cloud platform.
  </Card>

<Card title="Google Serper" href="/oss/python/integrations/providers/google_serper" icon="google">
    Google Search API service.
  </Card>

<Card title="GooseAI" href="/oss/python/integrations/providers/gooseai" icon="link">
    Fully managed NLP-as-a-Service platform.
  </Card>

<Card title="GPT4All" href="/oss/python/integrations/providers/gpt4all" icon="link">
    Open-source LLM ecosystem for local deployment.
  </Card>

<Card title="Gradient" href="/oss/python/integrations/providers/gradient" icon="link">
    AI model training and deployment platform.
  </Card>

<Card title="DigitalOcean Gradient AI Platform" href="/oss/python/integrations/providers/gradientai" icon="link">
    Single endpoint to multiple LLMs via serverless inference.
  </Card>

<Card title="Graph RAG" href="/oss/python/integrations/providers/graph_rag" icon="link">
    Graph-based retrieval augmented generation.
  </Card>

<Card title="GraphSignal" href="/oss/python/integrations/providers/graphsignal" icon="link">
    AI observability and monitoring platform.
  </Card>

<Card title="GreenNode" href="/oss/python/integrations/providers/greennode" icon="link">
    Sustainable AI computing platform.
  </Card>

<Card title="GROBID" href="/oss/python/integrations/providers/grobid" icon="link">
    Machine learning library for bibliographic data.
  </Card>

<Card title="Groq" href="/oss/python/integrations/providers/groq" icon="link">
    Ultra-fast inference with specialized hardware.
  </Card>

<Card title="Gutenberg" href="/oss/python/integrations/providers/gutenberg" icon="link">
    Project Gutenberg digital library access.
  </Card>

<Card title="Hacker News" href="/oss/python/integrations/providers/hacker_news" icon="link">
    Tech news and discussion platform.
  </Card>

<Card title="Hazy Research" href="/oss/python/integrations/providers/hazy_research" icon="link">
    Machine learning research and tools.
  </Card>

<Card title="Helicone" href="/oss/python/integrations/providers/helicone" icon="link">
    LLM observability and monitoring platform.
  </Card>

<Card title="Hologres" href="/oss/python/integrations/providers/hologres" icon="link">
    Real-time interactive analytics service.
  </Card>

<Card title="HTML2Text" href="/oss/python/integrations/providers/html2text" icon="link">
    HTML to plain text conversion utility.
  </Card>

<Card title="Huawei" href="/oss/python/integrations/providers/huawei" icon="link">
    Huawei Cloud AI services and models.
  </Card>

<Card title="Hugging Face" href="/oss/python/integrations/providers/huggingface" icon="link">
    Open platform for ML models and datasets.
  </Card>

<Card title="HyperBrowser" href="/oss/python/integrations/providers/hyperbrowser" icon="link">
    Web automation and scraping platform.
  </Card>

<Card title="IBM" href="/oss/python/integrations/providers/ibm" icon="link">
    IBM Watson AI and enterprise solutions.
  </Card>

<Card title="IEIT Systems" href="/oss/python/integrations/providers/ieit_systems" icon="link">
    Enterprise AI and system integration.
  </Card>

<Card title="iFixit" href="/oss/python/integrations/providers/ifixit" icon="link">
    Repair guides and technical documentation.
  </Card>

<Card title="iFlytek" href="/oss/python/integrations/providers/iflytek" icon="link">
    Chinese speech and language AI platform.
  </Card>

<Card title="IMSDb" href="/oss/python/integrations/providers/imsdb" icon="link">
    Internet Movie Script Database access.
  </Card>

<Card title="InfinispanVS" href="/oss/python/integrations/providers/infinispanvs" icon="link">
    Distributed cache and data grid platform.
  </Card>

<Card title="Infinity" href="/oss/python/integrations/providers/infinity" icon="link">
    High-performance embedding inference server.
  </Card>

<Card title="Infino" href="/oss/python/integrations/providers/infino" icon="link">
    Observability and monitoring platform.
  </Card>

<Card title="Intel" href="/oss/python/integrations/providers/intel" icon="link">
    Intel's AI optimization tools and libraries.
  </Card>

<Card title="Isaacus" href="/oss/python/integrations/providers/isaacus" icon="link">
    Legal AI models, apps, and data.
  </Card>

<Card title="IUGU" href="/oss/python/integrations/providers/iugu" icon="link">
    Brazilian payment processing platform.
  </Card>

<Card title="Jaguar" href="/oss/python/integrations/providers/jaguar" icon="link">
    Vector database and search platform.
  </Card>

<Card title="Javelin AI Gateway" href="/oss/python/integrations/providers/javelin_ai_gateway" icon="link">
    AI model gateway and management platform.
  </Card>

<Card title="Jenkins" href="/oss/python/integrations/providers/jenkins" icon="link">
    Automation server and CI/CD platform.
  </Card>

<Card title="Jina" href="/oss/python/integrations/providers/jina" icon="link">
    Neural search framework and cloud platform.
  </Card>

<Card title="John Snow Labs" href="/oss/python/integrations/providers/johnsnowlabs" icon="link">
    Enterprise NLP and healthcare AI platform.
  </Card>

<Card title="Joplin" href="/oss/python/integrations/providers/joplin" icon="link">
    Open-source note taking and organization.
  </Card>

<Card title="KDB.AI" href="/oss/python/integrations/providers/kdbai" icon="link">
    Time-series vector database platform.
  </Card>

<Card title="Kinetica" href="/oss/python/integrations/providers/kinetica" icon="link">
    Real-time analytics and database platform.
  </Card>

<Card title="KoboldAI" href="/oss/python/integrations/providers/koboldai" icon="link">
    Browser-based AI writing assistant.
  </Card>

<Card title="Konko" href="/oss/python/integrations/providers/konko" icon="link">
    Generative AI platform and model hosting.
  </Card>

<Card title="KoNLPy" href="/oss/python/integrations/providers/konlpy" icon="link">
    Korean natural language processing toolkit.
  </Card>

<Card title="Kuzu" href="/oss/python/integrations/providers/kuzu" icon="link">
    Embedded graph database management system.
  </Card>

<Card title="Label Studio" href="/oss/python/integrations/providers/labelstudio" icon="link">
    Data labeling and annotation platform.
  </Card>

<Card title="LakeFS" href="/oss/python/integrations/providers/lakefs" icon="link">
    Git-like version control for data lakes.
  </Card>

<Card title="LanceDB" href="/oss/python/integrations/providers/lancedb" icon="link">
    Developer-friendly embedded vector database.
  </Card>

<Card title="LangChain Decorators" href="/oss/python/integrations/providers/langchain_decorators" icon="link">
    Syntactic sugar and utilities for LangChain.
  </Card>

<Card title="LangFair" href="/oss/python/integrations/providers/langfair" icon="link">
    Bias testing framework for language models.
  </Card>

<Card title="LangFuse" href="/oss/python/integrations/providers/langfuse" icon="link">
    LLM engineering platform and observability.
  </Card>

<Card title="Lantern" href="/oss/python/integrations/providers/lantern" icon="link">
    PostgreSQL vector database extension.
  </Card>

<Card title="Lindorm" href="/oss/python/integrations/providers/lindorm" icon="link">
    Alibaba Cloud's multi-model database service.
  </Card>

<Card title="LinkUp" href="/oss/python/integrations/providers/linkup" icon="link">
    Real-time job market data and search.
  </Card>

<Card title="LiteLLM" href="/oss/python/integrations/providers/litellm" icon="link">
    Unified interface for 100+ LLM APIs.
  </Card>

<Card title="LlamaIndex" href="/oss/python/integrations/providers/llama_index" icon="link">
    Data framework for LLM applications.
  </Card>

<Card title="LlamaCPP" href="/oss/python/integrations/providers/llamacpp" icon="link">
    Port of Meta's LLaMA model in C/C++.
  </Card>

<Card title="LlamaEdge" href="/oss/python/integrations/providers/llamaedge" icon="link">
    Edge computing platform for LLaMA models.
  </Card>

<Card title="LlamaFile" href="/oss/python/integrations/providers/llamafile" icon="link">
    Single-file executable for running LLMs.
  </Card>

<Card title="LLMonitor" href="/oss/python/integrations/providers/llmonitor" icon="link">
    Observability platform for LLM applications.
  </Card>

<Card title="LocalAI" href="/oss/python/integrations/providers/localai" icon="link">
    Self-hosted OpenAI-compatible API server.
  </Card>

<Card title="Log10" href="/oss/python/integrations/providers/log10" icon="link">
    LLM data management and observability.
  </Card>

<Card title="MariaDB" href="/oss/python/integrations/providers/mariadb" icon="link">
    Open-source relational database management.
  </Card>

<Card title="MaritALK" href="/oss/python/integrations/providers/maritalk" icon="link">
    Brazilian Portuguese language model.
  </Card>

<Card title="Marqo" href="/oss/python/integrations/providers/marqo" icon="link">
    End-to-end vector search engine.
  </Card>

<Card title="MediaWiki Dump" href="/oss/python/integrations/providers/mediawikidump" icon="link">
    Wikipedia and MediaWiki data processing.
  </Card>

<Card title="Meilisearch" href="/oss/python/integrations/providers/meilisearch" icon="link">
    Lightning-fast search engine platform.
  </Card>

<Card title="Memcached" href="/oss/python/integrations/providers/memcached" icon="link">
    Distributed memory caching system.
  </Card>

<Card title="Memgraph" href="/oss/python/integrations/providers/memgraph" icon="link">
    Real-time graph database platform.
  </Card>

<Card title="Metal" href="/oss/python/integrations/providers/metal" icon="link">
    Managed vector search and retrieval.
  </Card>

<Card title="Microsoft" href="/oss/python/integrations/providers/microsoft" icon="microsoft">
    Microsoft Azure AI and enterprise services.
  </Card>

<Card title="Milvus" href="/oss/python/integrations/providers/milvus" icon="link">
    Open-source vector database for AI applications.
  </Card>

<Card title="MindsDB" href="/oss/python/integrations/providers/mindsdb" icon="link">
    AI layer for databases and data platforms.
  </Card>

<Card title="Minimax" href="/oss/python/integrations/providers/minimax" icon="link">
    Chinese AI company's language models.
  </Card>

<Card title="MistralAI" href="/oss/python/integrations/providers/mistralai" icon="link">
    Efficient open-source language models.
  </Card>

<Card title="MLflow" href="/oss/python/integrations/providers/mlflow" icon="link">
    ML lifecycle management platform.
  </Card>

<Card title="MLflow Tracking" href="/oss/python/integrations/providers/mlflow_tracking" icon="link">
    Experiment tracking and model registry.
  </Card>

<Card title="MLX" href="/oss/python/integrations/providers/mlx" icon="link">
    Apple's machine learning framework.
  </Card>

<Card title="Modal" href="/oss/python/integrations/providers/modal" icon="link">
    Serverless cloud computing for data science.
  </Card>

<Card title="ModelScope" href="/oss/python/integrations/providers/modelscope" icon="link">
    Alibaba's open-source model hub.
  </Card>

<Card title="Modern Treasury" href="/oss/python/integrations/providers/modern_treasury" icon="link">
    Payment operations and treasury management.
  </Card>

<Card title="Momento" href="/oss/python/integrations/providers/momento" icon="link">
    Serverless cache and vector index.
  </Card>

<Card title="MongoDB" href="/oss/python/integrations/providers/mongodb" icon="link">
    Document-based NoSQL database platform.
  </Card>

<Card title="MongoDB Atlas" href="/oss/python/integrations/providers/mongodb_atlas" icon="link">
    Cloud-hosted MongoDB with vector search.
  </Card>

<Card title="MotherDuck" href="/oss/python/integrations/providers/motherduck" icon="link">
    Serverless analytics with DuckDB in the cloud.
  </Card>

<Card title="Motorhead" href="/oss/python/integrations/providers/motorhead" icon="link">
    Long-term memory for AI conversations.
  </Card>

<Card title="MyScale" href="/oss/python/integrations/providers/myscale" icon="link">
    SQL-compatible vector database platform.
  </Card>

<Card title="Naver" href="/oss/python/integrations/providers/naver" icon="link">
    Naver's AI services and language models.
  </Card>

<Card title="Nebius" href="/oss/python/integrations/providers/nebius" icon="link">
    AI cloud platform and infrastructure.
  </Card>

<Card title="Neo4j" href="/oss/python/integrations/providers/neo4j" icon="link">
    Native graph database and analytics platform.
  </Card>

<Card title="NetMind" href="/oss/python/integrations/providers/netmind" icon="link">
    Decentralized AI computing network.
  </Card>

<Card title="Nimble" href="/oss/python/integrations/providers/nimble" icon="link">
    Web intelligence and data extraction.
  </Card>

<Card title="NLP Cloud" href="/oss/python/integrations/providers/nlpcloud" icon="link">
    Production-ready NLP API platform.
  </Card>

<Card title="Nomic" href="/oss/python/integrations/providers/nomic" icon="link">
    Open-source embedding models and tools.
  </Card>

<Card title="Notion" href="/oss/python/integrations/providers/notion" icon="link">
    All-in-one workspace and collaboration platform.
  </Card>

<Card title="Nuclia" href="/oss/python/integrations/providers/nuclia" icon="link">
    AI-powered search and understanding platform.
  </Card>

<Card title="NVIDIA" href="/oss/python/integrations/providers/nvidia" icon="link">
    NVIDIA's AI computing platform and models.
  </Card>

<Card title="Obsidian" href="/oss/python/integrations/providers/obsidian" icon="link">
    Connected note-taking and knowledge management.
  </Card>

<Card title="OceanBase" href="/oss/python/integrations/providers/oceanbase" icon="link">
    Distributed relational database system.
  </Card>

<Card title="OCI" href="/oss/python/integrations/providers/oci" icon="link">
    Oracle Cloud Infrastructure AI services.
  </Card>

<Card title="OctoAI" href="/oss/python/integrations/providers/octoai" icon="link">
    Efficient AI compute and model serving.
  </Card>

<Card title="Ollama" href="/oss/python/integrations/providers/ollama" icon="link">
    Run Large Language Models (LLMs) locally.
  </Card>

<Card title="Ontotext GraphDB" href="/oss/python/integrations/providers/ontotext_graphdb" icon="link">
    RDF database and semantic graph platform.
  </Card>

<Card title="OpenAI" href="/oss/python/integrations/providers/openai" icon="openai">
    GPT models and comprehensive AI platform.
  </Card>

<Card title="OpenDataLoader PDF" href="/oss/python/integrations/providers/opendataloader_pdf" icon="link">
    Safe, Open, High-Performance — PDF for AI
  </Card>

<Card title="OpenGradient" href="/oss/python/integrations/providers/opengradient" icon="link">
    AI model training and fine-tuning platform.
  </Card>

<Card title="OpenLLM" href="/oss/python/integrations/providers/openllm" icon="link">
    Operating LLMs in production environment.
  </Card>

<Card title="Open Agent Spec (PyAgentSpec)" href="https://oracle.github.io/agent-spec/adapters/langgraph.html" icon="link">
    Framework-agnostic declarative language by Oracle for defining agentic systems. Define agents and workflows in a portable JSON/YAML format that can be executed across different runtimes.
  </Card>

<Card title="OpenSearch" href="/oss/python/integrations/providers/opensearch" icon="link">
    Distributed search and analytics suite.
  </Card>

<Card title="OpenWeatherMap" href="/oss/python/integrations/providers/openweathermap" icon="link">
    Weather data and forecasting API.
  </Card>

<Card title="Oracle AI" href="/oss/python/integrations/providers/oracleai" icon="link">
    Oracle's AI and machine learning services.
  </Card>

<Card title="Outline" href="/oss/python/integrations/providers/outline" icon="link">
    Team knowledge base and wiki platform.
  </Card>

<Card title="Outlines" href="/oss/python/integrations/providers/outlines" icon="link">
    Structured generation for language models.
  </Card>

<Card title="Oxylabs" href="/oss/python/integrations/providers/oxylabs" icon="link">
    Web scraping and proxy services.
  </Card>

<Card title="Pandas" href="/oss/python/integrations/providers/pandas" icon="link">
    Data analysis and manipulation library.
  </Card>

<Card title="Parallel" href="/oss/python/integrations/providers/parallel" icon="link">
    AI-powered web search and content extraction for LLMs.
  </Card>

<Card title="Perigon" href="/oss/python/integrations/providers/perigon" icon="link">
    Real-time news and media monitoring.
  </Card>

<Card title="Permit" href="/oss/python/integrations/providers/permit" icon="link">
    Authorization and access control platform.
  </Card>

<Card title="Perplexity" href="/oss/python/integrations/providers/perplexity" icon="link">
    AI-powered search and reasoning engine.
  </Card>

<Card title="Petals" href="/oss/python/integrations/providers/petals" icon="link">
    Distributed inference for Large Language Models.
  </Card>

<Card title="PG Embedding" href="/oss/python/integrations/providers/pg_embedding" icon="link">
    PostgreSQL vector embedding extensions.
  </Card>

<Card title="pgvector" href="/oss/python/integrations/providers/pgvector" icon="link">
    Vector similarity search for PostgreSQL.
  </Card>

<Card title="Pinecone" href="/oss/python/integrations/providers/pinecone" icon="link">
    Managed vector database for ML applications.
  </Card>

<Card title="PipelineAI" href="/oss/python/integrations/providers/pipelineai" icon="link">
    ML pipeline and model deployment platform.
  </Card>

<Card title="Pipeshift" href="/oss/python/integrations/providers/pipeshift" icon="link">
    AI-powered content moderation platform.
  </Card>

<Card title="PolarisAIDataInsight" href="/oss/python/integrations/providers/polaris_ai_datainsight" icon="link">
    Document-loaders for various file formats.
  </Card>

<Card title="Portkey" href="/oss/python/integrations/providers/portkey/logging_tracing_portkey" icon="link">
    AI gateway and observability platform.
  </Card>

<Card title="Predibase" href="/oss/python/integrations/providers/predibase" icon="link">
    Fine-tuning platform for Large Language Models.
  </Card>

<Card title="PredictionGuard" href="/oss/python/integrations/providers/predictionguard" icon="link">
    AI model security and compliance platform.
  </Card>

<Card title="PreMAI" href="/oss/python/integrations/providers/premai" icon="link">
    AI platform for model deployment and management.
  </Card>

<Card title="Privy" href="/oss/python/integrations/providers/privy" icon="link">
    Wallets and payments for AI agents.
  </Card>

<Card title="Prolog" href="/oss/python/integrations/providers/prolog" icon="link">
    Logic programming language integration.
  </Card>

<Card title="PromptLayer" href="/oss/python/integrations/providers/promptlayer" icon="link">
    Prompt engineering and observability platform.
  </Card>

<Card title="Psychic" href="/oss/python/integrations/providers/psychic" icon="link">
    Universal API for SaaS integrations.
  </Card>

<Card title="PubMed" href="/oss/python/integrations/providers/pubmed" icon="link">
    Biomedical literature database access.
  </Card>

<Card title="Pull MD" href="/oss/python/integrations/providers/pull-md" icon="link">
    Markdown content extraction and processing.
  </Card>

<Card title="PygmalionAI" href="/oss/python/integrations/providers/pygmalionai" icon="link">
    Conversational AI model platform.
  </Card>

<Card title="PyMuPDF4LLM" href="/oss/python/integrations/providers/pymupdf4llm" icon="link">
    PDF processing optimized for LLM ingestion.
  </Card>

<Card title="Qdrant" href="/oss/python/integrations/providers/qdrant" icon="link">
    Vector similarity search engine.
  </Card>

<Card title="Ragatouille" href="/oss/python/integrations/providers/ragatouille" icon="link">
    RAG toolkit with ColBERT indexing.
  </Card>

<Card title="Rank BM25" href="/oss/python/integrations/providers/rank_bm25" icon="link">
    BM25 ranking algorithm implementation.
  </Card>

<Card title="Ray Serve" href="/oss/python/integrations/providers/ray_serve" icon="link">
    Scalable model serving framework.
  </Card>

<Card title="Rebuff" href="/oss/python/integrations/providers/rebuff" icon="link">
    Prompt injection detection and prevention.
  </Card>

<Card title="Reddit" href="/oss/python/integrations/providers/reddit" icon="link">
    Social media platform integration and APIs.
  </Card>

<Card title="Redis" href="/oss/python/integrations/providers/redis" icon="link">
    In-memory data structure store and cache.
  </Card>

<Card title="Remembrall" href="/oss/python/integrations/providers/remembrall" icon="link">
    AI memory and context management.
  </Card>

<Card title="Replicate" href="/oss/python/integrations/providers/replicate" icon="link">
    Cloud platform for running ML models.
  </Card>

<Card title="Roam" href="/oss/python/integrations/providers/roam" icon="link">
    Research and note-taking platform.
  </Card>

<Card title="Robocorp" href="/oss/python/integrations/providers/robocorp" icon="link">
    Python automation and RPA platform.
  </Card>

<Card title="Rockset" href="/oss/python/integrations/providers/rockset" icon="link">
    Real-time analytics database platform.
  </Card>

<Card title="RunPod" href="/oss/python/integrations/providers/runpod" icon="link">
    GPU cloud platform for AI workloads.
  </Card>

<Card title="Salesforce" href="/oss/python/integrations/providers/salesforce" icon="link">
    CRM platform and business automation.
  </Card>

<Card title="SambaNova" href="/oss/python/integrations/providers/sambanova" icon="link">
    AI platform with specialized hardware.
  </Card>

<Card title="SAP" href="/oss/python/integrations/providers/sap" icon="link">
    Enterprise software and AI solutions.
  </Card>

<Card title="ScrapeGraph" href="/oss/python/integrations/providers/scrapegraph" icon="link">
    AI-powered web scraping framework.
  </Card>

<Card title="Scrapeless" href="/oss/python/integrations/providers/scrapeless" icon="link">
    Web scraping API and proxy service.
  </Card>

<Card title="SearchAPI" href="/oss/python/integrations/providers/searchapi" icon="link">
    Real-time search engine results API.
  </Card>

<Card title="SearX" href="/oss/python/integrations/providers/searx" icon="link">
    Privacy-respecting metasearch engine.
  </Card>

<Card title="SemaDB" href="/oss/python/integrations/providers/semadb" icon="link">
    Vector database for semantic search.
  </Card>

<Card title="SerpAPI" href="/oss/python/integrations/providers/serpapi" icon="link">
    Google Search results scraping API.
  </Card>

<Card title="Shale Protocol" href="/oss/python/integrations/providers/shaleprotocol" icon="link">
    Decentralized AI inference protocol.
  </Card>

<Card title="SingleStore" href="/oss/python/integrations/providers/singlestore" icon="link">
    Distributed database with vector capabilities.
  </Card>

<Card title="scikit-learn" href="/oss/python/integrations/providers/sklearn" icon="link">
    Machine learning library for Python.
  </Card>

<Card title="Slack" href="/oss/python/integrations/providers/slack" icon="link">
    Business communication and collaboration.
  </Card>

<Card title="Snowflake" href="/oss/python/integrations/providers/snowflake" icon="link">
    Cloud data platform and analytics.
  </Card>

<Card title="spaCy" href="/oss/python/integrations/providers/spacy" icon="link">
    Industrial-strength NLP library.
  </Card>

<Card title="Spark" href="/oss/python/integrations/providers/spark" icon="link">
    Unified analytics engine for big data.
  </Card>

<Card title="SparkLLM" href="/oss/python/integrations/providers/sparkllm" icon="link">
    iFlytek's multilingual language model.
  </Card>

<Card title="Spreedly" href="/oss/python/integrations/providers/spreedly" icon="link">
    Payment orchestration platform.
  </Card>

<Card title="SQLite" href="/oss/python/integrations/providers/sqlite" icon="link">
    Embedded relational database engine.
  </Card>

<Card title="StackExchange" href="/oss/python/integrations/providers/stackexchange" icon="link">
    Q\&A platform network integration.
  </Card>

<Card title="StarRocks" href="/oss/python/integrations/providers/starrocks" icon="link">
    High-performance analytical database.
  </Card>

<Card title="StochasticAI" href="/oss/python/integrations/providers/stochasticai" icon="link">
    GPU cloud platform for ML acceleration.
  </Card>

<Card title="Streamlit" href="/oss/python/integrations/providers/streamlit" icon="link">
    Web app framework for data science.
  </Card>

<Card title="Stripe" href="/oss/python/integrations/providers/stripe" icon="link">
    Online payment processing platform.
  </Card>

<Card title="Supabase" href="/oss/python/integrations/providers/supabase" icon="link">
    Open-source Firebase alternative.
  </Card>

<Card title="SurrealDB" href="/oss/python/integrations/providers/surrealdb" icon="link">
    Multi-model database for modern applications.
  </Card>

<Card title="Symbl.ai Nebula" href="/oss/python/integrations/providers/symblai_nebula" icon="link">
    Conversation intelligence platform.
  </Card>

<Card title="Tableau" href="/oss/python/integrations/providers/tableau" icon="link">
    Data visualization and business intelligence.
  </Card>

<Card title="Taiga" href="/oss/python/integrations/providers/taiga" icon="link">
    Project management platform for agile teams.
  </Card>

<Card title="Tair" href="/oss/python/integrations/providers/tair" icon="link">
    Alibaba Cloud's in-memory database.
  </Card>

<Card title="Tavily" href="/oss/python/integrations/providers/tavily" icon="link">
    AI-optimized search API for applications.
  </Card>

<Card title="Telegram" href="/oss/python/integrations/providers/telegram" icon="link">
    Messaging platform and bot integration.
  </Card>

<Card title="Tencent" href="/oss/python/integrations/providers/tencent" icon="link">
    Tencent Cloud AI services and models.
  </Card>

<Card title="TensorFlow Datasets" href="/oss/python/integrations/providers/tensorflow_datasets" icon="link">
    Collection of ready-to-use datasets.
  </Card>

<Card title="TensorLake" href="/oss/python/integrations/providers/tensorlake" icon="link">
    Data infrastructure for ML applications.
  </Card>

<Card title="TiDB" href="/oss/python/integrations/providers/tidb" icon="link">
    Distributed SQL database platform.
  </Card>

<Card title="TigerGraph" href="/oss/python/integrations/providers/tigergraph" icon="link">
    Scalable graph database and analytics.
  </Card>

<Card title="Tigris" href="/oss/python/integrations/providers/tigris" icon="link">
    Globally distributed database platform.
  </Card>

<Card title="Tilores" href="/oss/python/integrations/providers/tilores" icon="link">
    Entity resolution and data matching.
  </Card>

<Card title="Timbr" href="/oss/python/integrations/providers/timbr" icon="link">
    Semantic layer for data integration and querying.
  </Card>

<Card title="Together" href="/oss/python/integrations/providers/together" icon="link">
    Fast inference for open-source models.
  </Card>

<Card title="ToMarkdown" href="/oss/python/integrations/providers/tomarkdown" icon="link">
    HTML to Markdown conversion utility.
  </Card>

<Card title="Toolbox LangChain" href="/oss/python/integrations/providers/toolbox" icon="link">
    Extended toolkit for LangChain applications.
  </Card>

<Card title="Transwarp" href="/oss/python/integrations/providers/transwarp" icon="link">
    Big data platform and analytics suite.
  </Card>

<Card title="Trello" href="/oss/python/integrations/providers/trello" icon="link">
    Visual project management and collaboration.
  </Card>

<Card title="Trubrics" href="/oss/python/integrations/providers/trubrics" icon="link">
    LLM evaluation and analytics platform.
  </Card>

<Card title="TrueFoundry" href="/oss/python/integrations/providers/truefoundry" icon="link">
    ML platform for model deployment.
  </Card>

<Card title="TrueLens" href="/oss/python/integrations/providers/trulens" icon="link">
    Evaluation framework for LLM applications.
  </Card>

<Card title="Twitter" href="/oss/python/integrations/providers/twitter" icon="link">
    Social media platform integration.
  </Card>

<Card title="Typesense" href="/oss/python/integrations/providers/typesense" icon="link">
    Fast and typo-tolerant search engine.
  </Card>

<Card title="UnDatasIO" href="/oss/python/integrations/providers/undatasio" icon="link">
    Data extraction and processing platform.
  </Card>

<Card title="Unstructured" href="/oss/python/integrations/providers/unstructured" icon="link">
    Document processing and data extraction.
  </Card>

<Card title="Upstage" href="/oss/python/integrations/providers/upstage" icon="link">
    Document AI and OCR platform.
  </Card>

<Card title="Upstash" href="/oss/python/integrations/providers/upstash" icon="link">
    Serverless data platform for Redis and Kafka.
  </Card>

<Card title="UpTrain" href="/oss/python/integrations/providers/uptrain" icon="link">
    ML observability and evaluation platform.
  </Card>

<Card title="USearch" href="/oss/python/integrations/providers/usearch" icon="link">
    Single-file vector search engine.
  </Card>

<Card title="Valthera" href="/oss/python/integrations/providers/valthera" icon="link">
    AI platform for healthcare applications.
  </Card>

<Card title="Valyu" href="/oss/python/integrations/providers/valyu" icon="link">
    AI-powered data analysis platform.
  </Card>

<Card title="VDMS" href="/oss/python/integrations/providers/vdms" icon="link">
    Visual data management system.
  </Card>

<Card title="Vearch" href="/oss/python/integrations/providers/vearch" icon="link">
    Distributed vector search engine.
  </Card>

<Card title="Vectara" href="/oss/python/integrations/providers/vectara" icon="link">
    Neural search platform with built-in understanding.
  </Card>

<Card title="Vectorize" href="/oss/python/integrations/providers/vectorize" icon="link">
    Vector database and semantic search.
  </Card>

<Card title="Vespa" href="/oss/python/integrations/providers/vespa" icon="link">
    Big data serving engine for vector search.
  </Card>

<Card title="VLite" href="/oss/python/integrations/providers/vlite" icon="link">
    Simple vector database for embeddings.
  </Card>

<Card title="VoyageAI" href="/oss/python/integrations/providers/voyageai" icon="link">
    Embedding models and semantic search.
  </Card>

<Card title="Weights & Biases" href="/oss/python/integrations/providers/wandb" icon="link">
    ML experiment tracking and collaboration.
  </Card>

<Card title="Weights & Biases Tracking" href="/oss/python/integrations/providers/wandb_tracking" icon="link">
    Experiment tracking and model management.
  </Card>

<Card title="Weights & Biases Tracing" href="/oss/python/integrations/providers/wandb_tracing" icon="link">
    LLM tracing and observability.
  </Card>

<Card title="Weather" href="/oss/python/integrations/providers/weather" icon="link">
    Weather data and forecasting services.
  </Card>

<Card title="Weaviate" href="/oss/python/integrations/providers/weaviate" icon="link">
    Open-source vector database with GraphQL.
  </Card>

<Card title="WhatsApp" href="/oss/python/integrations/providers/whatsapp" icon="link">
    Messaging platform integration and automation.
  </Card>

<Card title="WhyLabs Profiling" href="/oss/python/integrations/providers/whylabs_profiling" icon="link">
    AI observability and data monitoring.
  </Card>

<Card title="Wikipedia" href="/oss/python/integrations/providers/wikipedia" icon="link">
    Wikipedia content access and search.
  </Card>

<Card title="Wolfram Alpha" href="/oss/python/integrations/providers/wolfram_alpha" icon="link">
    Computational knowledge engine.
  </Card>

<Card title="WRITER" href="/oss/python/integrations/providers/writer" icon="link">
    Enterprise models and tools for building, activating, and supervising AI agents.
  </Card>

<Card title="XAI" href="/oss/python/integrations/providers/xai" icon="link">
    xAI's Grok models for conversational AI.
  </Card>

<Card title="Xata" href="/oss/python/integrations/providers/xata" icon="link">
    Serverless database with vector search.
  </Card>

<Card title="Xinference" href="/oss/python/integrations/providers/xinference" icon="link">
    Distributed inference framework for LLMs.
  </Card>

<Card title="Yahoo" href="/oss/python/integrations/providers/yahoo" icon="link">
    Yahoo services and data integration.
  </Card>

<Card title="Yandex" href="/oss/python/integrations/providers/yandex" icon="link">
    Yandex AI services and language models.
  </Card>

<Card title="YDB" href="/oss/python/integrations/providers/ydb" icon="link">
    Yandex Database distributed storage system.
  </Card>

<Card title="YeagerAI" href="/oss/python/integrations/providers/yeagerai" icon="link">
    AI agent framework and development platform.
  </Card>

<Card title="Yellowbrick" href="/oss/python/integrations/providers/yellowbrick" icon="link">
    Data warehouse and analytics platform.
  </Card>

<Card title="Yi" href="/oss/python/integrations/providers/yi" icon="link">
    01.AI's bilingual language models.
  </Card>

<Card title="You" href="/oss/python/integrations/providers/you" icon="link">
    You.com search engine and AI platform.
  </Card>

<Card title="YouTube" href="/oss/python/integrations/providers/youtube" icon="link">
    Video platform integration and content access.
  </Card>

<Card title="Zep" href="/oss/python/integrations/providers/zep" icon="link">
    Long-term memory for AI assistants.
  </Card>

<Card title="ZeusDB" href="/oss/python/integrations/providers/zeusdb" icon="link">
    High-performance vector database.
  </Card>

<Card title="ZhipuAI" href="/oss/python/integrations/providers/zhipuai" icon="link">
    ChatGLM and other Chinese language models.
  </Card>

<Card title="Zilliz" href="/oss/python/integrations/providers/zilliz" icon="link">
    Managed Milvus vector database service.
  </Card>

<Card title="Zotero" href="/oss/python/integrations/providers/zotero" icon="link">
    Reference management and research tool.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/all_providers.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Analyze an experiment

**URL:** llms-txt#analyze-an-experiment

**Contents:**
- Analyze a single experiment
  - Open the experiment view
  - View experiment results
  - Group results by metadata
  - Repetitions
  - Compare to another experiment
- Download experiment results as a CSV
- Rename an experiment

Source: https://docs.langchain.com/langsmith/analyze-an-experiment

This page describes some of the essential tasks for working with [*experiments*](/langsmith/evaluation-concepts#experiment) in LangSmith:

* **[Analyze a single experiment](#analyze-a-single-experiment)**: View and interpret experiment results, customize columns, filter data, and compare runs.
* **[Download experiment results as a CSV](#how-to-download-experiment-results-as-a-csv)**: Export your experiment data for external analysis and sharing.
* **[Rename an experiment](#how-to-rename-an-experiment)**: Update experiment names in both the Playground and Experiments view.

## Analyze a single experiment

After running an experiment, you can use LangSmith's experiment view to analyze the results and draw insights about your experiment's performance.

### Open the experiment view

To open the experiment view, select the relevant [*dataset*](/langsmith/evaluation-concepts#datasets) from the **Dataset & Experiments** page and then select the experiment you want to view.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=74207f0a2422f89fdc75b23f0a88c58f" alt="Open experiment view" data-og-width="1640" width="1640" data-og-height="899" height="899" data-path="langsmith/images/select-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fa173f885a87adac0c9ced9b3d553876 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=011a588eeca2032ab40c3612345a0b4f 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7baa325ce358d73c61dbab0cce54222b 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2810aba5a4ce3f7f0098f167bff7a78f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=78d38c66b183cee29fa66959f339954c 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-experiment.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0bf7c03ef6f6ca6e269053a984f29c3a 2500w" />

### View experiment results

#### Customize columns

By default, the experiment view shows the input, output, and reference output for each [example](/langsmith/evaluation-concepts#examples) in the dataset, feedback scores from evaluations and experiment metrics like cost, token counts, latency and status.

You can customize the columns using the **Display** button to make it easier to interpret experiment results:

* **Break out fields from inputs, outputs, and reference outputs** into their own columns. This is especially helpful if you have long inputs/outputs/reference outputs and want to surface important fields.
* **Hide and reorder columns** to create focused views for analysis.
* **Control decimal precision on feedback scores**. By default, LangSmith surfaces numerical feedback scores with a decimal precision of 2, but you can customize this setting to be up to 6 decimals.
* **Set the Heat Map threshold** to high, middle, and low for numeric feedback scores in your experiment, which affects the threshold at which score chips render as red or green:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0203a449f0f7df70900735ba540d712" alt="Column heatmap configuration" data-og-width="1780" width="1780" data-og-height="1688" height="1688" data-path="langsmith/images/column-heat-map.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1ac06a00a4d11c8455d3996e3b3cc7ea 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c2207a21fea1078e5002d0d96c8c989 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ea673195e58c5bae676a782cb03bbbaa 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac4708a7612a3f7e7f82db28ac3a7b91 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=da649bbe343e2fd5e87d1845d1b19944 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/column-heat-map.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f06ba0a04ef257e2d69193021938e761 2500w" />

<Tip>
  You can set default configurations for an entire dataset or temporarily save settings just for yourself.
</Tip>

To sort or filter feedback scores, you can use the actions in the column headers.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=067490743d1229ae233f15e46236ed67" alt="Sort and filter" data-og-width="1633" width="1633" data-og-height="788" height="788" data-path="langsmith/images/sort-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a116c731329b3fc088b57eae1d2f41a4 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d4ab881725f50ca23dc2650aa5376efd 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=62027a48b9fd97a335caf3bd7e99d0dc 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=137a7384faf8c1958dd309d5cfeba998 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3ea99639ad38e02f997f66f504663b8a 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/sort-filter.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2372af19c9bd6d45b420e870f7b77402 2500w" />

Depending on the view most useful for your analysis, you can change the formatting of the table by toggling between a compact view, a full, view, and a diff view.

* The **Compact** view shows each run as a one-line row, for ease of comparing scores at a glance.
* The **Full** view shows the full output for each run for digging into the details of individual runs.
* The **Diff** view shows the text difference between the reference output and the output for each run.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fb916d33cea2f344f3483b42d3670696" alt="Diff view" data-og-width="1638" width="1638" data-og-height="969" height="969" data-path="langsmith/images/diff-mode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d30d2281df2f18a7c45fcae5eb839ded 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=edcf8cc4a3aca2bc3498a7c8f97b31b1 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3d0b55249029b6d72a98fea716d4dae7 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ef89e5bf58911f5362635cfefe8ead27 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=337796d8a22712dea4c848ba8bc0e94a 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/diff-mode.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b1019a7ec0249a9b58da199421689bc 2500w" />

Hover over any of the output cells, and click on the trace icon to view the trace for that run. This will open up a trace in the side panel.

To view the entire tracing project, click on the **View Project** button in the top right of the header.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c94c0d2ecedf248c639c971bf29196e6" alt="View trace" data-og-width="1634" width="1634" data-og-height="835" height="835" data-path="langsmith/images/view-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=45822be0900e0aaf9a06b9ddf7a8d91c 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c0bc6ecc7cb67144c7899b8345d6ccef 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=85387bf5a2dd30a8411759069d4b3bbc 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f5edba01b477e0c8a4f7a5e81123ffb7 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0241b8036e5e409a7d365e39d8d72bf1 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-trace.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9a7f0042a8319a0398427665d50347d3 2500w" />

#### View evaluator runs

For evaluator scores, you can view the source run by hovering over the evaluator score cell and clicking on the arrow icon. This will open up a trace in the side panel. If you're running a [LLM-as-a-judge evaluator](/langsmith/llm-as-judge), you can view the prompt used for the evaluator in this run. If your experiment has [repetitions](/langsmith/evaluation-concepts#repetitions), you can click on the aggregate average score to find links to all of the individual runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fc8df7233285b0f5a4ca9b44c06fcb47" alt="View evaluator runs" data-og-width="1634" width="1634" data-og-height="831" height="831" data-path="langsmith/images/evaluator-run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=368b84b566407689f0e1b69f7e2d1ec8 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9f7888c2163589263d5ed0827bf9b55a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a9cf41bd478fa7bb86b5594f4a0f163a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=644229f6557ca6b1b68df6be37b79cf9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=da59385aa685d9e2c163f18385ee201c 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-run.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=039cad7ccdd3c0ed5d5e0e30151ed916 2500w" />

### Group results by metadata

You can add metadata to examples to categorize and organize them. For example, if you're evaluating factual accuracy on a question answering dataset, the metadata might include which subject area each question belongs to. Metadata can be added either [via the UI](/langsmith/manage-datasets-in-application#edit-example-metadata) or [via the SDK](/langsmith/manage-datasets-programmatically#update-single-example).

To analyze results by metadata, use the **Group by** dropdown in the top right corner of the experiment view and select your desired metadata key. This displays average feedback scores, latency, total tokens, and cost for each metadata group.

<Info>
  You will only be able to group by example metadata on experiments created after February 20th, 2025. Any experiments before that date can still be grouped by metadata, but only if the metadata is on the experiment traces themselves.
</Info>

If you've run your experiment with [*repetitions*](/langsmith/evaluation-concepts#repetitions), there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view.

When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=60962de04e5533d7718ca60fa9c7dcce" alt="Repetitions" data-og-width="1636" width="1636" data-og-height="959" height="959" data-path="langsmith/images/repetitions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8be83801a53f2544883faf173bc16ef1 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7a924559be193efcc2c77dba3fea1231 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=25cbd580d06bda48419b83401c268c2d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9da3908c81d1c8fd44dde6d3ec7dfe1d 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=775af0be371e662bea7ba7e29c2f21fd 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/repetitions.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4d593460688be852a64638f092cba9f3 2500w" />

### Compare to another experiment

In the top right of the experiment view, you can select another experiment to compare to. This will open up a comparison view, where you can see how the two experiments compare. To learn more about the comparison view, see [how to compare experiment results](/langsmith/compare-experiment-results).

## Download experiment results as a CSV

LangSmith lets you download experiment results as a CSV file, which allows you to analyze and share your results.

To download as a CSV, click the download icon at the top of the experiment view. The icon is directly to the left of the [Compact toggle](/langsmith/compare-experiment-results#adjust-the-table-display).

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f237eb4b252a1018097be113434c22fa" alt="Download CSV" data-og-width="1705" width="1705" data-og-height="1345" height="1345" data-path="langsmith/images/download-experiment-results-as-csv.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=679144e623fdd6a5ff643d66378f5f21 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ca7f980d6a11cf6ac6f54a2e8799ac08 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ae945f1172fad11a20e16c15ae859409 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e64dbaffad47fb30e1d44f032533bdcb 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=72f1815228051375821c98a26edd0452 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/download-experiment-results-as-csv.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=51944ba9dddf5dddaa77b12a8a1c8d8a 2500w" />

## Rename an experiment

<Note>
  Experiment names must be unique per workspace.
</Note>

You can rename an experiment in the LangSmith UI in:

* The [Playground](#renaming-an-experiment-in-the-playground). When running experiments in the Playground, a default name with the format `pg::prompt-name::model::uuid` (eg. `pg::gpt-4o-mini::897ee630`) is automatically assigned.

You can rename an experiment immediately after running it by editing its name in the Playground table header.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5b647ff1894376bbb727dabc4d73f039" alt="Edit name in playground" data-og-width="1372" width="1372" data-og-height="200" height="200" data-path="langsmith/images/rename-in-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d505597b1d2e180ebfa05d4361d3225 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d09703c543257203a19434a8a30458e1 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b3a8f6f2bd9fbfb9ba0c3f6c7477e04c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=591176ffbeb1f0a16f084f31f6717c6f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=381a59f8752307204b3e9f82a2fdbd16 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-playground.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cba501d5c0d826f6f61b57d01e1d94c9 2500w" />

* The [Experiments view](#renaming-an-experiment-in-the-experiments-view). When viewing results in the experiments view, you can rename an experiment by using the pencil icon beside the experiment name.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=16afa853361ec265a0c7917d815f3132" alt="Edit name in experiments view" data-og-width="1628" width="1628" data-og-height="224" height="224" data-path="langsmith/images/rename-in-experiments-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d87e21875d807e1553289f096137af3f 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=761aab57b06c1e3644ac633f10565e26 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0dd7d731b4aae11a4b08644e06ac0eb9 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=333a93f0e1d2ff38f9bdcfcfed33c825 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=48d1b7cacab8e71bb4d0fe79573d6f11 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rename-in-experiments-view.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8185ba3772c13f40ea57501dd4982bc3 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/analyze-an-experiment.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## and this is also supported

**URL:** llms-txt#and-this-is-also-supported

**Contents:**
- Nodes

{"messages": [{"type": "human", "content": "message"}]}
python  theme={null}
from langchain.messages import AnyMessage
from langgraph.graph.message import add_messages
from typing import Annotated
from typing_extensions import TypedDict

class GraphState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
python  theme={null}
from langgraph.graph import MessagesState

class State(MessagesState):
    documents: list[str]
python  theme={null}
from dataclasses import dataclass
from typing_extensions import TypedDict

from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph
from langgraph.runtime import Runtime

class State(TypedDict):
    input: str
    results: str

@dataclass
class Context:
    user_id: str

builder = StateGraph(State)

def plain_node(state: State):
    return state

def node_with_runtime(state: State, runtime: Runtime[Context]):
    print("In node: ", runtime.context.user_id)
    return {"results": f"Hello, {state['input']}!"}

def node_with_config(state: State, config: RunnableConfig):
    print("In node with thread_id: ", config["configurable"]["thread_id"])
    return {"results": f"Hello, {state['input']}!"}

builder.add_node("plain_node", plain_node)
builder.add_node("node_with_runtime", node_with_runtime)
builder.add_node("node_with_config", node_with_config)
...
python  theme={null}
builder.add_node(my_node)

**Examples:**

Example 1 (unknown):
```unknown
Since the state updates are always deserialized into LangChain `Messages` when using [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages), you should use dot notation to access message attributes, like `state["messages"][-1].content`.

Below is an example of a graph that uses [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) as its reducer function.
```

Example 2 (unknown):
```unknown
#### MessagesState

Since having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:
```

Example 3 (unknown):
```unknown
## Nodes

In LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:

1. `state` – The [state](#state) of the graph
2. `config` – A [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) object that contains configuration information like `thread_id` and tracing information like `tags`
3. `runtime` – A `Runtime` object that contains [runtime `context`](#runtime-context) and other information like `store` and `stream_writer`

Similar to `NetworkX`, you add these nodes to a graph using the [`add_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_node) method:
```

Example 4 (unknown):
```unknown
Behind the scenes, functions are converted to [`RunnableLambda`](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html), which add batch and async support to your function, along with native tracing and debugging.

If you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.
```

---

## Annotate traces and runs inline

**URL:** llms-txt#annotate-traces-and-runs-inline

Source: https://docs.langchain.com/langsmith/annotate-traces-inline

LangSmith allows you to manually annotate traces with feedback within the application. This can be useful for adding context to a trace, such as a user's comment or a note about a specific issue.
You can annotate a trace either inline or by sending the trace to an annotation queue, which allows you to closely inspect and log feedbacks to runs one at a time.
Feedback tags are associated with your [workspace](/langsmith/administration-overview#workspaces).

<Note>
  **You can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.**

This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline.
</Note>

To annotate a trace inline, click on the `Annotate` in the upper right corner of trace view for any particular run that is part of the trace.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=193363baef8b46e592fa63b299b407af" alt="Annotate trace inline" data-og-width="1722" width="1722" data-og-height="1035" height="1035" data-path="langsmith/images/annotate-trace-inline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3751be3488a8a5a488eb4277b4bc574e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e4c94e427fd4a4c14e5fca6b1a4fce14 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3cc298e64a52af87bb4d46760352c958 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6f13e3f345430b443c6f45642d6031eb 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4259e830834b95f09e4b457b1e9a2807 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-trace-inline.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f65fa998f3962f7b971c8433c8be36aa 2500w" />

This will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow [this guide](./set-up-feedback-criteria) to set up feedback tags for your workspace.
You can also set up new feedback criteria from within the pane itself.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a16e79d91b435f6c5de94d0d58daa59" alt="Annotation sidebar" data-og-width="1376" width="1376" data-og-height="758" height="758" data-path="langsmith/images/annotation-sidebar.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6b0ee76da4d19ca7d7b5640865f738a7 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2e1acb5f129b2f865eef0399b0b06217 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ac7a1f2abf6faeb918eee11b702bb450 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1454e680d2c557541ea68c794018ceae 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5f45921db9ec69f138be197f71ab6c22 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotation-sidebar.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c0c75ec7d51de574a3b7b1283ed06907 2500w" />

You can use the labeled keyboard shortcuts to streamline the annotation process.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotate-traces-inline.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Anthropic (Claude)

**URL:** llms-txt#anthropic-(claude)

**Contents:**
- Model interfaces
- Other

Source: https://docs.langchain.com/oss/python/integrations/providers/anthropic

This page covers all LangChain integrations with [Anthropic](https://www.anthropic.com/), the makers of Claude.

<Columns cols={2}>
  <Card title="ChatAnthropic" href="/oss/python/integrations/chat/anthropic" cta="Get started" icon="message" arrow>
    Anthropic chat models.
  </Card>

<Card title="Anthropic middleware" href="/oss/python/integrations/middleware/anthropic" cta="Get started" icon="layer-group" arrow>
    Anthropic-specific middleware for Claude models.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="AnthropicLLM" href="/oss/python/integrations/llms/anthropic" cta="Get started" icon="i-cursor" arrow>
    (Legacy) Anthropic text completion models.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/anthropic.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## An experiment is a collection of runs with a reference to the dataset used

**URL:** llms-txt#an-experiment-is-a-collection-of-runs-with-a-reference-to-the-dataset-used

---

## API Reference: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post

**URL:** llms-txt#api-reference:-https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post

resp = requests.post(
    "https://api.smith.langchain.com/api/v1/datasets/comparative",
    json={
        "experiment_ids": experiment_ids,
        "name": "Toxicity detection - API Example - Comparative - " + str(uuid4())[0:8],
        "description": "An optional description for the comparative experiment",
        "extra": {
            "metadata": {"foo": "bar"},  # Optional metadata
        },
        "reference_dataset_id": str(dataset_id),
    },
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

comparative_experiment = resp.json()
comparative_experiment_id = comparative_experiment["id"]

---

## API Reference: https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get

**URL:** llms-txt#api-reference:-https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get

dataset_id = dataset.id
params = { "dataset": dataset_id }

resp = requests.get(
    "https://api.smith.langchain.com/api/v1/examples",
    params=params,
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

examples = resp.json()
python  theme={null}
os.environ["OPENAI_API_KEY"] = "sk-..."

def run_completion_on_example(example, model_name, experiment_id):
    """Run completions on a list of examples."""
    # We are using the OpenAI API here, but you can use any model you like

def _post_run(run_id, name, run_type, inputs, parent_id=None):
        """Function to post a new run to the API.
        API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/create_run_api_v1_runs_post
        """
        data = {
            "id": run_id.hex,
            "name": name,
            "run_type": run_type,
            "inputs": inputs,
            "start_time": datetime.utcnow().isoformat(),
            "reference_example_id": example["id"],
            "session_id": experiment_id,
        }
        if parent_id:
            data["parent_run_id"] = parent_id.hex
        resp = requests.post(
            "https://api.smith.langchain.com/api/v1/runs", # Update appropriately for self-hosted installations or the EU region
            json=data,
            headers=headers
        )
        resp.raise_for_status()

def _patch_run(run_id, outputs):
        """Function to patch a run with outputs.
        API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/update_run_api_v1_runs__run_id__patch
        """
        resp = requests.patch(
            f"https://api.smith.langchain.com/api/v1/runs/{run_id}",
            json={
                "outputs": outputs,
                "end_time": datetime.utcnow().isoformat(),
            },
            headers=headers,
        )
        resp.raise_for_status()

# Send your API Key in the request headers
    headers = {"x-api-key": os.environ["LANGSMITH_API_KEY"]}

text = example["inputs"]["text"]

messages = [
        {
            "role": "system",
            "content": "Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.",
        },
        {"role": "user", "content": text},
    ]

# Create parent run
    parent_run_id = uuid7()
    _post_run(parent_run_id, "LLM Pipeline", "chain", {"text": text})

# Create child run
    child_run_id = uuid7()
    _post_run(child_run_id, "OpenAI Call", "llm", {"messages": messages}, parent_run_id)

# Generate completion
    chat_completion = oa_client.chat.completions.create(model=model_name, messages=messages)
    output_text = chat_completion.choices[0].message.content

# End run
    _patch_run(child_run_id, {
    "messages": messages,
        "output": output_text,
        "model": model_name
    })

_patch_run(parent_run_id, {"label": output_text})
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
from langsmith import uuid7

Next, define a function that will run your model on a single example and log the results to LangSmith. When using the API directly, you're responsible for:

* Creating run objects via POST to `/runs` with `reference_example_id` and `session_id` set.
* Tracking parent-child relationships between runs (e.g., a parent "chain" run containing a child "llm" run).
* Updating runs with outputs via PATCH to `/runs/{run_id}`.
```

Example 2 (unknown):
```unknown
Now create the experiments and run completions on all examples. In the API, an "experiment" is represented as a session (or "tracer session") that references a dataset via `reference_dataset_id`. The key difference from regular tracing is that runs in an experiment must have a `reference_example_id` that links each run to a specific example in the dataset.
```

---

## API Reference: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post

**URL:** llms-txt#api-reference:-https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post

runs = requests.post(
    f"https://api.smith.langchain.com/api/v1/runs/query",
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]},
    json={
        "session": experiment_ids,
        "is_root": True, # Only fetch root runs (spans) which contain the end outputs
        "select": ["id", "reference_example_id", "outputs"],
    }
).json()
runs = runs["runs"]
for run in runs:
    example_id = run["reference_example_id"]
    example_id_to_runs_map[example_id].append(run)

for example_id, runs in example_id_to_runs_map.items():
    print(f"Example ID: {example_id}")
    # Preferentially rank the outputs, in this case we will always prefer the first output
    # In reality, you can use an LLM to rank the outputs
    feedback_group_id = uuid4()

# Post a feedback score for each run, with the first run being the preferred one
    # API Reference: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post
    # We'll use the feedback group ID to associate the feedback scores with the same group
    for i, run in enumerate(runs):
        print(f"Run ID: {run['id']}")
        feedback = {
            "score": 1 if i == 0 else 0,
            "run_id": str(run["id"]),
            "key": "ranked_preference",
            "feedback_group_id": str(feedback_group_id),
            "comparative_experiment_id": comparative_experiment_id,
        }
        resp = requests.post(
            "https://api.smith.langchain.com/api/v1/feedback",
            json=feedback,
            headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
        )
        resp.raise_for_status()
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-evals-api-only.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## API Reference: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post

**URL:** llms-txt#api-reference:-https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post

**Contents:**
  - Add evaluation feedback

model_names = ("gpt-3.5-turbo", "gpt-4o-mini")
experiment_ids = []
for model_name in model_names:
    resp = requests.post(
        "https://api.smith.langchain.com/api/v1/sessions",
        json={
            "start_time": datetime.utcnow().isoformat(),
            "reference_dataset_id": str(dataset_id),
            "description": "An optional description for the experiment",
            "name": f"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}",  # A name for the experiment
            "extra": {
                "metadata": {"foo": "bar"},  # Optional metadata
            },
        },
        headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
    )

experiment = resp.json()
    experiment_ids.append(experiment["id"])

# Run completions on all examples
    for example in examples:
        run_completion_on_example(example, model_name, experiment["id"])

# Issue a patch request to "end" the experiment by updating the end_time
    requests.patch(
        f"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}",
        json={"end_time": datetime.utcnow().isoformat()},
        headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
    )
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Add evaluation feedback

After running your [experiments](/langsmith/evaluation-concepts#experiment), you'll typically want to evaluate the results by adding feedback scores. This allows you to track metrics like correctness, accuracy, or any custom evaluation criteria.

In this example, the evaluation checks if each model's output matches the expected label in the dataset. The code posts a "correctness" score (1.0 for correct, 0.0 for incorrect) to track how accurately each model classifies toxic vs. non-toxic text.

The following code adds feedback to the runs from the [single experiment example](#run-a-single-experiment):
```

---

## Application structure

**URL:** llms-txt#application-structure

**Contents:**
- Key Concepts
- File structure
- Configuration file
  - Examples
- Dependencies
- Graphs
- Environment variables

Source: https://docs.langchain.com/oss/python/langgraph/application-structure

A LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables.

This guide shows a typical structure of an application and shows you how to provide the required configuration to deploy an application with [LangSmith Deployment](/langsmith/deployments).

<Info>
  LangSmith Deployment is a managed hosting platform for deploying and scaling LangGraph agents. It handles the infrastructure, scaling, and operational concerns so you can deploy your stateful, long-running agents directly from your repository. Learn more in the [Deployment documentation](/langsmith/deployments).
</Info>

To deploy using the LangSmith, the following information should be provided:

1. A [LangGraph configuration file](#configuration-file-concepts) (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application.
2. The [graphs](#graphs) that implement the logic of the application.
3. A file that specifies [dependencies](#dependencies) required to run the application.
4. [Environment variables](#environment-variables) that are required for the application to run.

Below are examples of directory structures for applications:

<Tabs>
  <Tab title="Python (requirements.txt)">
    
  </Tab>

<Tab title="Python (pyproject.toml)">
    
  </Tab>
</Tabs>

<Note>
  The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.
</Note>

<a id="configuration-file-concepts" />

## Configuration file

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.

See the [LangGraph configuration file reference](/langsmith/cli#configuration-file) for details on all supported keys in the JSON file.

<Tip>
  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.
</Tip>

* The dependencies involve a custom local package and the `langchain_openai` package.
* A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.
* The environment variables are loaded from the `.env` file.

A LangGraph application may depend on other Python packages.

You will generally need to specify the following information for dependencies to be set up correctly:

1. A file in the directory that specifies the dependencies (e.g. `requirements.txt`, `pyproject.toml`, or `package.json`).

2. A `dependencies` key in the [LangGraph configuration file](#configuration-file-concepts) that specifies the dependencies required to run the LangGraph application.

3. Any additional binaries or system libraries can be specified using `dockerfile_lines` key in the [LangGraph configuration file](#configuration-file-concepts).

Use the `graphs` key in the [LangGraph configuration file](#configuration-file-concepts) to specify which graphs will be available in the deployed LangGraph application.

You can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.

## Environment variables

If you're working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the [LangGraph configuration file](#configuration-file-concepts).

For a production deployment, you will typically want to configure the environment variables in the deployment environment.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/application-structure.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Python (pyproject.toml)">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

<Note>
  The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.
</Note>

<a id="configuration-file-concepts" />

## Configuration file

The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.

See the [LangGraph configuration file reference](/langsmith/cli#configuration-file) for details on all supported keys in the JSON file.

<Tip>
  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.
</Tip>

### Examples

* The dependencies involve a custom local package and the `langchain_openai` package.
* A single graph will be loaded from the file `./your_package/your_file.py` with the variable `variable`.
* The environment variables are loaded from the `.env` file.
```

---

## App development in LangSmith Deployment

**URL:** llms-txt#app-development-in-langsmith-deployment

Source: https://docs.langchain.com/langsmith/app-development

**LangSmith Deployment** builds on the open-source [LangGraph](/oss/python/langgraph/overview) framework for developing stateful, multi-agent applications.
LangGraph provides the core abstractions and execution model, while LangSmith adds managed infrastructure, observability, deployment options, assistants, and concurrency controls—supporting the full lifecycle from development to production.

<Callout icon="cubes" color="#4F46E5" iconType="regular">
  LangSmith Deployment is framework-agnostic: you can deploy agents built with LangGraph or [other frameworks](/langsmith/deploy-other-frameworks). To get started with LangGraph itself, refer to the [LangGraph quickstart](/oss/python/langgraph/quickstart).
</Callout>

<CardGroup>
  <Card title="Assistants" cta="Explore assistants" href="/langsmith/assistants" icon="user-gear">
    Manage agent configurations, connect to threads, and build interactive assistants.
  </Card>

<Card title="Runs" cta="Learn about runs" href="/langsmith/background-run" icon="play">
    Execute background jobs, stateless runs, cron jobs, and manage configurable headers.
  </Card>

<Card title="Core capabilities" cta="See core features" href="/langsmith/streaming" icon="gear">
    Streaming, human-in-the-loop, webhooks, and concurrency controls like double-texting.
  </Card>

<Card title="Tutorials" cta="View tutorials" href="/langsmith/deploy-other-frameworks" icon="graduation-cap">
    Step-by-step examples: AutoGen integration, streaming UI, and generative UI in React.
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/app-development.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## artist vectorstore for "prince" we'll get back the value "Prince", which we can then

**URL:** llms-txt#artist-vectorstore-for-"prince"-we'll-get-back-the-value-"prince",-which-we-can-then

---

## Ask for a SQL query

**URL:** llms-txt#ask-for-a-sql-query

result = agent.invoke(  # [!code highlight]
    {
        "messages": [
            {
                "role": "user",
                "content": (
                    "Write a SQL query to find all customers "
                    "who made orders over $1000 in the last month"
                ),
            }
        ]
    },
    config
)

---

## Assistants

**URL:** llms-txt#assistants

**Contents:**
- When to use assistants
- How assistants work with deployments
  - Configuration
  - Versioning
  - Execution
- Video guide

Source: https://docs.langchain.com/langsmith/assistants

*Assistants* allow you to manage configurations (e.g., prompts, LLM selection, tools) separately from your graph's core logic. This enables you to create multiple, specialized versions of the same graph architecture with different behavior at runtime. Through configuration variations (rather than structural graph changes), each assistant is optimized for a different [use case](#when-to-use-assistants).

For example, imagine a general-purpose writing agent built on a common graph architecture. While the structure remains the same, different writing styles—such as blog posts and tweets—require tailored configurations to optimize performance. To support these variations, you can create multiple assistants (e.g., one for blogs and another for tweets) that share the underlying graph but differ in model selection and system prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=05402316c8fe86fead077ec774e873f0" alt="assistant versions" data-og-width="1824" width="1824" data-og-height="692" height="692" data-path="langsmith/images/assistants.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=3ac250197ee8463950b74dc5f6bcd37f 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=d6b01c6ae96bd96b580bf43228610224 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=6125bf9aed49385ec8422e27cb377dad 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=c54cde5d8a052ceac26d67131407aa73 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=780c08f1695bc2e5ba0b6261febb1954 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/assistants.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=ed8fba40ce7c1b3455027df735f9bdba 2500w" />

The Agent Server API provides several endpoints for creating and managing assistants and their versions. See the [API reference](/langsmith/server-api-ref) for more details.

<Info>
  Assistants are a [LangSmith Deployment](/langsmith/deployments) concept. They are not available in the open source LangGraph library.
</Info>

## When to use assistants

Assistants are ideal when you need to deploy the same graph architecture with different configurations. Common use cases include:

* **User-level personalization**
  * Customize model selection, system prompts, or tool availability per user.
  * Store user preferences and apply them automatically to each interaction.
  * Enable users to choose between different AI personalities or expertise levels.

* **Customer or organization-specific configurations**
  * Maintain separate configurations for different customers or organizations.
  * Customize behavior for each client without deploying separate infrastructure.
  * Isolate configuration changes to specific customers.

* **Environment-specific configurations**
  * Use different models or settings for development, staging, and production.
  * Test configuration changes in staging before promoting to production.
  * Reduce costs in non-production environments with smaller models.

* **A/B testing and experimentation**
  * Compare different prompts, models, or parameter settings.
  * Roll out configuration changes gradually to a subset of users.
  * Measure performance differences between configuration variants.

* **Specialized task variants**
  * Create domain-specific versions of a general-purpose agent.
  * Optimize configurations for different languages, regions, or industries.
  * Maintain consistent graph logic while varying the execution details.

## How assistants work with deployments

When you deploy a graph with LangSmith Deployment, [Agent Server](/langsmith/agent-server) automatically creates a **default assistant** tied to that graph's default configuration. You can then create additional assistants for the same graph, each with its own configuration.

If your deployment defines multiple graphs in [`langgraph.json`](/langmsith/application-structure#configuration-file), each graph gets its own default assistant:

That is, there can be multiple default assistants—one for each graph defined in your deployment.

Assistants have several key features:

* **[Managed via API and UI](/langsmith/configuration-cloud)**: Create, list, update, version, and get assistants using the Agent Server/LangGraph SDKs or the [LangSmith UI](https://smith.langchain.com).
* **One graph, multiple assistants**: A single deployed graph can support multiple assistants, each with different configurations (e.g., prompts, models, tools).
* **[Versioned](#versioning) configurations**: Each assistant maintains its own configuration history through versioning. Editing an assistant creates a new version, and you can promote or roll back to any version.
* **[Configuration](#configuration) updates without graph changes**: Update prompts, model selection, and other settings through assistant configurations, enabling rapid iteration without modifying or redeploying your graph code.

<Note>
  When invoking an assistant, you can specify either in [`langgraph.json`](/langsmith/application-structure#configuration-file):

* A **graph ID** (e.g., `"agent"`): Uses the default assistant for that graph
  * An **assistant ID** (UUID): Uses a specific assistant configuration

This flexibility allows you to quickly test with default settings or precisely control which configuration is used.
</Note>

Assistants build on the LangGraph open source concept of [configuration](/oss/python/langgraph/graph-api#runtime-context).

While configuration is available in the open source LangGraph library, assistants are only present in [LangSmith Deployment](/langsmith/deployments) because they are tightly coupled to your deployed graph. Upon deployment, [Agent Server](/langsmith/agent-server) will automatically create a default assistant for each graph using the graph's default configuration settings.

In practice, an assistant is just an *instance* of a graph with a specific configuration. Therefore, multiple assistants can reference the same graph but can contain different configurations (e.g. prompts, models, tools). The LangSmith Deployment API provides several endpoints for creating and managing assistants. See the [API reference](/langsmith/server-api-ref) and [this how-to](/langsmith/configuration-cloud) for more details on how to create assistants.

Assistants support versioning to track changes over time. Once you've created an assistant, subsequent edits will automatically create new versions.

* Each update creates a new version of the assistant.
* You can promote any version to be the active version.
* Rolling back to a previous version is as simple as setting it as active.
* All versions remain available for reference and rollback.

<Warning>
  When updating an assistant, you must provide the entire configuration payload. The update endpoint creates new versions from scratch and does not merge with previous versions. Make sure to include all configuration fields you want to retain.
</Warning>

For more details on how to manage assistant versions, refer to the [Manage assistants guide](/langsmith/configuration-cloud#create-a-new-version-for-your-assistant).

A *run* is an invocation of an assistant. When you execute a run, you specify which assistant to use (either by graph ID for the default assistant or by assistant ID for a specific configuration).

This diagram shows how a **run** combines an assistant with a thread to execute the graph:

* **Graph** (blue): The deployed code containing your agent's logic
* **Assistants** (light blue): Configuration options (model, prompts, tools)
* **Threads** (orange): State containers for conversation history
* **Runs** (green): Executions that pair an assistant + thread

**Example combinations:**

* **Run: A1 + T1**: Assistant 1 configuration applied to User A's conversation
* **Run: A1 + T2**: Same assistant serving User B (different conversation)
* **Run: A2 + T1**: Different assistant applied to User A's conversation (configuration switch)

When executing a run:

* Each run may have its own input, configuration overrides, and metadata.
* Runs can be stateless (no thread) or stateful (executed on a [thread](/oss/python/langgraph/persistence#threads) for conversation persistence).
* Multiple runs can use the same assistant configuration.
* The assistant's configuration affects how the underlying graph executes.

The Agent Server API provides several endpoints for creating and managing runs. For more details, refer to the [API reference](/langsmith/server-api-ref)).

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/fMsQX6pwXkE?si=6Q28l0taGOynO7sU" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/assistants.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
* **Environment-specific configurations**
  * Use different models or settings for development, staging, and production.
  * Test configuration changes in staging before promoting to production.
  * Reduce costs in non-production environments with smaller models.

* **A/B testing and experimentation**
  * Compare different prompts, models, or parameter settings.
  * Roll out configuration changes gradually to a subset of users.
  * Measure performance differences between configuration variants.

* **Specialized task variants**
  * Create domain-specific versions of a general-purpose agent.
  * Optimize configurations for different languages, regions, or industries.
  * Maintain consistent graph logic while varying the execution details.
```

Example 2 (unknown):
```unknown
## How assistants work with deployments

When you deploy a graph with LangSmith Deployment, [Agent Server](/langsmith/agent-server) automatically creates a **default assistant** tied to that graph's default configuration. You can then create additional assistants for the same graph, each with its own configuration.

If your deployment defines multiple graphs in [`langgraph.json`](/langmsith/application-structure#configuration-file), each graph gets its own default assistant:
```

Example 3 (unknown):
```unknown
That is, there can be multiple default assistants—one for each graph defined in your deployment.

Assistants have several key features:

* **[Managed via API and UI](/langsmith/configuration-cloud)**: Create, list, update, version, and get assistants using the Agent Server/LangGraph SDKs or the [LangSmith UI](https://smith.langchain.com).
* **One graph, multiple assistants**: A single deployed graph can support multiple assistants, each with different configurations (e.g., prompts, models, tools).
* **[Versioned](#versioning) configurations**: Each assistant maintains its own configuration history through versioning. Editing an assistant creates a new version, and you can promote or roll back to any version.
* **[Configuration](#configuration) updates without graph changes**: Update prompts, model selection, and other settings through assistant configurations, enabling rapid iteration without modifying or redeploying your graph code.

<Note>
  When invoking an assistant, you can specify either in [`langgraph.json`](/langsmith/application-structure#configuration-file):

  * A **graph ID** (e.g., `"agent"`): Uses the default assistant for that graph
  * An **assistant ID** (UUID): Uses a specific assistant configuration

  This flexibility allows you to quickly test with default settings or precisely control which configuration is used.
</Note>

### Configuration

Assistants build on the LangGraph open source concept of [configuration](/oss/python/langgraph/graph-api#runtime-context).

While configuration is available in the open source LangGraph library, assistants are only present in [LangSmith Deployment](/langsmith/deployments) because they are tightly coupled to your deployed graph. Upon deployment, [Agent Server](/langsmith/agent-server) will automatically create a default assistant for each graph using the graph's default configuration settings.

In practice, an assistant is just an *instance* of a graph with a specific configuration. Therefore, multiple assistants can reference the same graph but can contain different configurations (e.g. prompts, models, tools). The LangSmith Deployment API provides several endpoints for creating and managing assistants. See the [API reference](/langsmith/server-api-ref) and [this how-to](/langsmith/configuration-cloud) for more details on how to create assistants.

### Versioning

Assistants support versioning to track changes over time. Once you've created an assistant, subsequent edits will automatically create new versions.

* Each update creates a new version of the assistant.
* You can promote any version to be the active version.
* Rolling back to a previous version is as simple as setting it as active.
* All versions remain available for reference and rollback.

<Warning>
  When updating an assistant, you must provide the entire configuration payload. The update endpoint creates new versions from scratch and does not merge with previous versions. Make sure to include all configuration fields you want to retain.
</Warning>

For more details on how to manage assistant versions, refer to the [Manage assistants guide](/langsmith/configuration-cloud#create-a-new-version-for-your-assistant).

### Execution

A *run* is an invocation of an assistant. When you execute a run, you specify which assistant to use (either by graph ID for the default assistant or by assistant ID for a specific configuration).
```

---

## Assistant creation

**URL:** llms-txt#assistant-creation

**Contents:**
  - Filter operations
- Common access patterns
  - Single-owner resources
  - Permission-based access

@auth.on.assistants.create
async def on_assistant_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.assistants.create.value
):
    if "assistants:create" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="User lacks the required permissions."
        )
python  theme={null}
@auth.on
async def owner_only(ctx: Auth.types.AuthContext, value: dict):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Notice that we are mixing global and resource-specific handlers in the above example. Since each request is handled by the most specific handler, a request to create a `thread` would match the `on_thread_create` handler but NOT the `reject_unhandled_requests` handler. A request to `update` a thread, however would be handled by the global handler, since we don't have a more specific handler for that resource and action.

<a id="filter-operations" />

### Filter operations

Authorization handlers can return `None`, a boolean, or a filter dictionary.

* `None` and `True` mean "authorize access to all underling resources"
* `False` means "deny access to all underling resources (raises a 403 exception)"
* A metadata filter dictionary will restrict access to resources

A filter dictionary is a dictionary with keys that match the resource metadata. It supports three operators:

* The default value is a shorthand for exact match, or "\$eq", below. For example, `{"owner": user_id}` will include only resources with metadata containing `{"owner": user_id}`
* `$eq`: Exact match (e.g., `{"owner": {"$eq": user_id}}`) - this is equivalent to the shorthand above, `{"owner": user_id}`
* `$contains`: List membership (e.g., `{"allowed_users": {"$contains": user_id}}`) or list containment (e.g., `{"allowed_users": {"$contains": [user_id_1, user_id_2]}}`). The value here must be an element of the list or a subset of the elements of the list, respectively. The metadata in the stored resource must be a list/container type.

A dictionary with multiple keys is treated using a logical `AND` filter. For example, `{"owner": org_id, "allowed_users": {"$contains": user_id}}` will only match resources with metadata whose "owner" is `org_id` and whose "allowed\_users" list contains `user_id`.
See the reference [`Auth`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth)(Auth) for more information.

## Common access patterns

Here are some typical authorization patterns:

### Single-owner resources

This common pattern lets you scope all threads, assistants, crons, and runs to a single user. It's useful for common single-user use cases like regular chatbot-style apps.
```

Example 2 (unknown):
```unknown
### Permission-based access

This pattern lets you control access based on **permissions**. It's useful if you want certain roles to have broader or more restricted access to resources.
```

---

## Assumes you organize information in store like (user_id, resource_type, resource_id)

**URL:** llms-txt#assumes-you-organize-information-in-store-like-(user_id,-resource_type,-resource_id)

@auth.on.store()
async def authorize_store(ctx: Auth.types.AuthContext, value: dict):
    # The "namespace" field for each store item is a tuple you can think of as the directory of an item.
    namespace: tuple = value["namespace"]
    assert namespace[0] == ctx.user.identity, "Not authorized"
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Notice that instead of one global handler, you now have specific handlers for:

1. Creating threads
2. Reading threads
3. Accessing assistants

The first three of these match specific **actions** on each resource (see [resource actions](/langsmith/auth#resource-specific-handlers)), while the last one (`@auth.on.assistants`) matches *any* action on the `assistants` resource. For each request, LangGraph will run the most specific handler that matches the resource and action being accessed. This means that the four handlers above will run rather than the broadly scoped "`@auth.on`" handler.

Try adding the following test code to your test file:
```

---

## async_client = wrap_anthropic(anthropic.AsyncAnthropic())

**URL:** llms-txt#async_client-=-wrap_anthropic(anthropic.asyncanthropic())

@traceable(run_type="tool", name="Retrieve Context")
def my_tool(question: str) -> str:
    return "During this morning's meeting, we solved all world conflict."

@traceable(name="Chat Pipeline")
def chat_pipeline(question: str):
    context = my_tool(question)
    messages = [
        { "role": "user", "content": f"Question: {question}\nContext: {context}"}
    ]
    messages = client.messages.create(
      model="claude-sonnet-4-5-20250929",
      messages=messages,
      max_tokens=1024,
      system="You are a helpful assistant. Please respond to the user's request only based on the given context."
    )
    return messages

chat_pipeline("Can you summarize this morning's meetings?")
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-anthropic.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Augment the LLM with schema for structured output

**URL:** llms-txt#augment-the-llm-with-schema-for-structured-output

structured_llm = llm.with_structured_output(SearchQuery)

---

## Augment the LLM with tools

**URL:** llms-txt#augment-the-llm-with-tools

tools = [add, multiply, divide]
tools_by_name = {tool.name: tool for tool in tools}
llm_with_tools = llm.bind_tools(tools)
python Graph API theme={null}
  from langgraph.graph import MessagesState
  from langchain.messages import SystemMessage, HumanMessage, ToolMessage

# Nodes
  def llm_call(state: MessagesState):
      """LLM decides whether to call a tool or not"""

return {
          "messages": [
              llm_with_tools.invoke(
                  [
                      SystemMessage(
                          content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."
                      )
                  ]
                  + state["messages"]
              )
          ]
      }

def tool_node(state: dict):
      """Performs the tool call"""

result = []
      for tool_call in state["messages"][-1].tool_calls:
          tool = tools_by_name[tool_call["name"]]
          observation = tool.invoke(tool_call["args"])
          result.append(ToolMessage(content=observation, tool_call_id=tool_call["id"]))
      return {"messages": result}

# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call
  def should_continue(state: MessagesState) -> Literal["tool_node", END]:
      """Decide if we should continue the loop or stop based upon whether the LLM made a tool call"""

messages = state["messages"]
      last_message = messages[-1]

# If the LLM makes a tool call, then perform an action
      if last_message.tool_calls:
          return "tool_node"

# Otherwise, we stop (reply to the user)
      return END

# Build workflow
  agent_builder = StateGraph(MessagesState)

# Add nodes
  agent_builder.add_node("llm_call", llm_call)
  agent_builder.add_node("tool_node", tool_node)

# Add edges to connect nodes
  agent_builder.add_edge(START, "llm_call")
  agent_builder.add_conditional_edges(
      "llm_call",
      should_continue,
      ["tool_node", END]
  )
  agent_builder.add_edge("tool_node", "llm_call")

# Compile the agent
  agent = agent_builder.compile()

# Show the agent
  display(Image(agent.get_graph(xray=True).draw_mermaid_png()))

# Invoke
  messages = [HumanMessage(content="Add 3 and 4.")]
  messages = agent.invoke({"messages": messages})
  for m in messages["messages"]:
      m.pretty_print()
  python Functional API theme={null}
  from langgraph.graph import add_messages
  from langchain.messages import (
      SystemMessage,
      HumanMessage,
      ToolCall,
  )
  from langchain_core.messages import BaseMessage

@task
  def call_llm(messages: list[BaseMessage]):
      """LLM decides whether to call a tool or not"""
      return llm_with_tools.invoke(
          [
              SystemMessage(
                  content="You are a helpful assistant tasked with performing arithmetic on a set of inputs."
              )
          ]
          + messages
      )

@task
  def call_tool(tool_call: ToolCall):
      """Performs the tool call"""
      tool = tools_by_name[tool_call["name"]]
      return tool.invoke(tool_call)

@entrypoint()
  def agent(messages: list[BaseMessage]):
      llm_response = call_llm(messages).result()

while True:
          if not llm_response.tool_calls:
              break

# Execute tools
          tool_result_futures = [
              call_tool(tool_call) for tool_call in llm_response.tool_calls
          ]
          tool_results = [fut.result() for fut in tool_result_futures]
          messages = add_messages(messages, [llm_response, *tool_results])
          llm_response = call_llm(messages).result()

messages = add_messages(messages, llm_response)
      return messages

# Invoke
  messages = [HumanMessage(content="Add 3 and 4.")]
  for chunk in agent.stream(messages, stream_mode="updates"):
      print(chunk)
      print("\n")
  ```
</CodeGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/workflows-agents.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Authenticate

**URL:** llms-txt#authenticate

Source: https://docs.langchain.com/api-reference/auth-service-v2/authenticate

https://api.host.langchain.com/openapi.json post /v2/auth/authenticate
Get OAuth token or start authentication flow if needed.

---

## Authentication & access control

**URL:** llms-txt#authentication-&-access-control

**Contents:**
- Core Concepts
  - Authentication vs authorization
- Default security models
  - LangSmith
  - Self-hosted
- System architecture
- Authentication
  - Agent authentication
  - Agent authentication with MCP
- Authorization

Source: https://docs.langchain.com/langsmith/auth

LangSmith provides a flexible authentication and authorization system that can integrate with most authentication schemes.

### Authentication vs authorization

While often used interchangeably, these terms represent distinct security concepts:

* [**Authentication**](#authentication) ("AuthN") verifies *who* you are. This runs as middleware for every request.
* [**Authorization**](#authorization) ("AuthZ") determines *what you can do*. This validates the user's privileges and roles on a per-resource basis.

In LangSmith, authentication is handled by your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler, and authorization is handled by your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.on) handlers.

## Default security models

LangSmith provides different security defaults:

* Uses LangSmith API keys by default
* Requires valid API key in `x-api-key` header
* Can be customized with your auth handler

<Note>
  **Custom auth**
  Custom auth **is supported** for all plans in LangSmith.
</Note>

* No default authentication
* Complete flexibility to implement your security model
* You control all aspects of authentication and authorization

## System architecture

A typical authentication setup involves three main components:

1. **Authentication Provider** (Identity Provider/IdP)

* A dedicated service that manages user identities and credentials
* Handles user registration, login, password resets, etc.
* Issues tokens (JWT, session tokens, etc.) after successful authentication
* Examples: Auth0, Supabase Auth, Okta, or your own auth server

2. **Agent Server** (Resource Server)

* Your agent or LangGraph application, which contains business logic and protected resources
* Validates tokens with the auth provider
* Enforces access control based on user identity and permissions
* Doesn't store user credentials directly

3. **Client Application** (Frontend)

* Web app, mobile app, or API client
* Collects time-sensitive user credentials and sends to auth provider
* Receives tokens from auth provider
* Includes these tokens in requests to the Agent Server

Here's how these components typically interact:

Your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler in LangGraph handles steps 4-6, while your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.on) handlers implement step 7.

Authentication in LangGraph runs as middleware on every request. Your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler receives request information and should:

1. Validate the credentials
2. Return [user info](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.MinimalUserDict) containing the user's identity and user information if valid
3. Raise an [HTTP exception](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.exceptions.HTTPException) or AssertionError if invalid

The returned user information is available:

* To your authorization handlers via [`ctx.user`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AuthContext)
* In your application via `config["configuration"]["langgraph_auth_user"]`

<Accordion title="Supported Parameters">
  The [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler can accept any of the following parameters by name:

* request (Request): The raw ASGI request object
  * path (str): The request path, e.g., `"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream"`
  * method (str): The HTTP method, e.g., `"GET"`
  * path\_params (dict\[str, str]): URL path parameters, e.g., `{"thread_id": "abcd-1234-abcd-1234", "run_id": "abcd-1234-abcd-1234"}`
  * query\_params (dict\[str, str]): URL query parameters, e.g., `{"stream": "true"}`
  * headers (dict\[bytes, bytes]): Request headers
  * authorization (str | None): The Authorization header value (e.g., `"Bearer <token>"`)

In many of our tutorials, we will just show the "authorization" parameter to be concise, but you can opt to accept more information as needed
  to implement your custom authentication scheme.
</Accordion>

### Agent authentication

Custom authentication permits delegated access. The values you return in  `@auth.authenticate` are added to the run context, giving agents user-scoped credentials lets them access resources on the user’s behalf.

After authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context.
This object contains information about the current user, including any custom fields you return from your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler.

To enable an agent to act on behalf of the user, use [custom authentication middleware](/langsmith/custom-auth). This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user.

For more information, see the [Use custom auth](/langsmith/custom-auth#enable-agent-authentication) guide.

### Agent authentication with MCP

For information on how to authenticate an agent to an MCP server, see the [MCP conceptual guide](/oss/python/langchain/mcp).

After authentication, LangGraph calls your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:

1. Add metadata to be saved during resource creation by mutating the `value["metadata"]` dictionary directly. See the [supported actions table](#supported-actions) for the list of types the value can take for each action.
2. Filter resources by metadata during search/list or read operations by returning a [filter dictionary](#filter-operations).
3. Raise an HTTP exception if access is denied.

If you want to just implement simple user-scoped access control, you can use a single [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) handler for all resources and actions. If you want to have different control depending on the resource and action, you can use [resource-specific handlers](#resource-specific-handlers). See the [Supported Resources](#supported-resources) section for a full list of the resources that support access control.

<a id="resource-specific-handlers" />

### Resource-specific handlers

You can register handlers for specific resources and actions by chaining the resource and action names together with the [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) decorator.
When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:

1. Authenticated users are able to create threads, read threads, and create runs on threads
2. Only users with the "assistants:create" permission are allowed to create new assistants
3. All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.

<Tip>
  **Supported Handlers**
  For a full list of supported resources and actions, see the [Supported Resources](#supported-resources) section below.
</Tip>

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler in LangGraph handles steps 4-6, while your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.on) handlers implement step 7.

## Authentication

Authentication in LangGraph runs as middleware on every request. Your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler receives request information and should:

1. Validate the credentials
2. Return [user info](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.MinimalUserDict) containing the user's identity and user information if valid
3. Raise an [HTTP exception](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.exceptions.HTTPException) or AssertionError if invalid
```

Example 2 (unknown):
```unknown
The returned user information is available:

* To your authorization handlers via [`ctx.user`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AuthContext)
* In your application via `config["configuration"]["langgraph_auth_user"]`

<Accordion title="Supported Parameters">
  The [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler can accept any of the following parameters by name:

  * request (Request): The raw ASGI request object
  * path (str): The request path, e.g., `"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream"`
  * method (str): The HTTP method, e.g., `"GET"`
  * path\_params (dict\[str, str]): URL path parameters, e.g., `{"thread_id": "abcd-1234-abcd-1234", "run_id": "abcd-1234-abcd-1234"}`
  * query\_params (dict\[str, str]): URL query parameters, e.g., `{"stream": "true"}`
  * headers (dict\[bytes, bytes]): Request headers
  * authorization (str | None): The Authorization header value (e.g., `"Bearer <token>"`)

  In many of our tutorials, we will just show the "authorization" parameter to be concise, but you can opt to accept more information as needed
  to implement your custom authentication scheme.
</Accordion>

### Agent authentication

Custom authentication permits delegated access. The values you return in  `@auth.authenticate` are added to the run context, giving agents user-scoped credentials lets them access resources on the user’s behalf.
```

Example 3 (unknown):
```unknown
After authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context.
This object contains information about the current user, including any custom fields you return from your [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler.

To enable an agent to act on behalf of the user, use [custom authentication middleware](/langsmith/custom-auth). This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user.

For more information, see the [Use custom auth](/langsmith/custom-auth#enable-agent-authentication) guide.

### Agent authentication with MCP

For information on how to authenticate an agent to an MCP server, see the [MCP conceptual guide](/oss/python/langchain/mcp).

## Authorization

After authentication, LangGraph calls your [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can:

1. Add metadata to be saved during resource creation by mutating the `value["metadata"]` dictionary directly. See the [supported actions table](#supported-actions) for the list of types the value can take for each action.
2. Filter resources by metadata during search/list or read operations by returning a [filter dictionary](#filter-operations).
3. Raise an HTTP exception if access is denied.

If you want to just implement simple user-scoped access control, you can use a single [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) handler for all resources and actions. If you want to have different control depending on the resource and action, you can use [resource-specific handlers](#resource-specific-handlers). See the [Supported Resources](#supported-resources) section for a full list of the resources that support access control.
```

Example 4 (unknown):
```unknown
<a id="resource-specific-handlers" />

### Resource-specific handlers

You can register handlers for specific resources and actions by chaining the resource and action names together with the [`@auth.on`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) decorator.
When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup:

1. Authenticated users are able to create threads, read threads, and create runs on threads
2. Only users with the "assistants:create" permission are allowed to create new assistants
3. All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users.

<Tip>
  **Supported Handlers**
  For a full list of supported resources and actions, see the [Supported Resources](#supported-resources) section below.
</Tip>
```

---

## Authentication methods

**URL:** llms-txt#authentication-methods

**Contents:**
- Cloud
  - Email/Password
  - Social Providers
  - SAML SSO
- Self-Hosted
  - SSO with OAuth 2.0 and OIDC
  - Email/Password a.k.a. basic auth
  - None

Source: https://docs.langchain.com/langsmith/authentication-methods

LangSmith supports multiple authentication methods for easy sign-up and login.

Users can use an email address and password to sign up and login to LangSmith.

Users can alternatively use their credentials from GitHub or Google.

Enterprise customers can configure [SAML SSO](/langsmith/user-management) and [SCIM](/langsmith/user-management)

Self-hosted customers have more control over how their users can login to LangSmith. For more in-depth coverage of configuration options, see [the self-hosting docs](/langsmith/self-hosted) and [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith).

### SSO with OAuth 2.0 and OIDC

Production installations should configure SSO in order to use an external identity provider. This enables users to login through an identity platform like Auth0/Okta. LangSmith supports almost any OIDC-compliant provider. Learn more about configuring SSO in the [SSO configuration guide](/langsmith/self-host-sso)

### Email/Password a.k.a. basic auth

This auth method requires very little configuration as it does not require an external identity provider. It is most appropriate to use for self-hosted trials. Learn more in the [basic auth configuration guide](/langsmith/self-host-basic-auth)

<Warning>
  This authentication mode will be removed after the launch of Basic Auth.
</Warning>

If zero authentication methods are enabled, a self-hosted installation does not require any login/sign-up. This configuration should only be used for verifying installation at the infrastructure level, as the feature set supported in this mode is restricted with only a single organization and workspace.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/authentication-methods.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Auth-aware tool responses

**URL:** llms-txt#auth-aware-tool-responses

**Contents:**
- Return shape to request auth

Source: https://docs.langchain.com/langsmith/agent-builder-auth-format

Format tool responses to trigger OAuth flows and resume execution automatically.

Some [tools](/langsmith/agent-builder-tools) require user authorization (for example, Google, Slack, GitHub). Agent Builder includes middleware to detect when a tool needs authorization and to pause the run with a clear prompt to the user. After the user completes auth, the same tool call is retried automatically.

## Return shape to request auth

If a tool detects missing authorization, return a JSON string containing the following fields:

* `auth_required`: set to `true` to signal an interrupt is needed.
* `auth_url`: where the user should be redirected to authorize.
* `auth_id`: optional correlation ID to track the auth session.

When Agent Builder detects this response, it interrupts the run, displays the authentication UI to the user, and automatically retries the tool call once authorization completes.

If you want your custom tools to reuse the same authentication required interrupt + UI, ensure your tools return the same shape of JSON.

<Note>
  Return only this JSON as the tool's output. Avoid including additional text or content. Agent Builder parses the response to trigger the authentication flow.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-builder-auth-format.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Automatically run evaluators on experiments

**URL:** llms-txt#automatically-run-evaluators-on-experiments

**Contents:**
- Configuring an evaluator on a dataset
- LLM-as-a-judge evaluators
- Custom code evaluators
- Next steps

Source: https://docs.langchain.com/langsmith/bind-evaluator-to-dataset

LangSmith supports two ways to grade experiments created via the SDK:

* **Programmatically**, by specifying evaluators in your code (see [this guide](/langsmith/evaluate-llm-application) for details)
* By **binding evaluators to a dataset** in the UI. This will automatically run the evaluators on any new experiments created, in addition to any evaluators you've set up via the SDK. This is useful when you're iterating on your application (target function), and have a standard set of evaluators you want to run for all experiments.

## Configuring an evaluator on a dataset

1. Click on the **Datasets and Experiments** tab in the sidebar.
2. Select the dataset you want to configure the evaluator for.
3. Click on the **+ Evaluator** button to add an evaluator to the dataset. This will open a pane you can use to configure the evaluator.

<Note>
  When you configure an evaluator for a dataset, it will only affect the experiment runs that are created after the evaluator is configured. It will not affect the evaluation of experiment runs that were created before the evaluator was configured.
</Note>

## LLM-as-a-judge evaluators

The process for binding evaluators to a dataset is very similar to the process for configuring a LLM-as-a-judge evaluator in the Playground. View instructions for [configuring an LLM-as-a-judge evaluator in the Playground.](/langsmith/llm-as-judge?mode=ui)

## Custom code evaluators

The process for binding a code evaluators to a dataset is very similar to the process for configuring a code evaluator in online evaluation. View instruction for [configuring code evaluators](/langsmith/online-evaluations#configure-a-custom-code-evaluator).

The only difference between configuring a code evaluator in online evaluation and binding a code evaluator to a dataset is that the custom code evaluator can reference outputs that are part of the dataset's `Example`.

For custom code evaluators bound to a dataset, the evaluator function takes in two arguments:

* A `Run` ([reference](/langsmith/run-data-format)). This represents the new run in your experiment. For example, if you ran an experiment via SDK, this would contain the input/output from your chain or model you are testing.
* An `Example` ([reference](/langsmith/example-data-format)). This represents the reference example in your dataset that the chain or model you are testing uses. The `inputs` to the Run and Example should be the same. If your Example has a reference `outputs`, then you can use this to compare to the run's output for scoring.

The code below shows an example of a simple evaluator function that checks that the outputs exactly equal the reference outputs.

* Analyze your experiment results in the [experiments tab](/langsmith/analyze-an-experiment)
* Compare your experiment results in the [comparison view](/langsmith/compare-experiment-results)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/bind-evaluator-to-dataset.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## autoscaling:

**URL:** llms-txt#autoscaling:

---

## await aevaluate(...)

**URL:** llms-txt#await-aevaluate(...)

**Contents:**
- Related

results = await ls_client.aevaluate(
    researcher_app,
    data=dataset,
    evaluators=[concise],
    # Optional, add concurrency.
    max_concurrency=2,  # Optional, add concurrency.
    experiment_prefix="gpt-4o-mini-baseline"  # Optional, random by default.
)
```

* [Run an evaluation (synchronously)](/langsmith/evaluate-llm-application)
* [Handle model rate limits](/langsmith/rate-limiting)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-async.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## AWS (Amazon)

**URL:** llms-txt#aws-(amazon)

**Contents:**
- Chat models
  - Bedrock Chat
  - Bedrock Converse
- LLMs
  - Bedrock
  - Amazon API Gateway
  - SageMaker Endpoint
- Embedding Models
  - Bedrock
  - SageMaker Endpoint

Source: https://docs.langchain.com/oss/python/integrations/providers/aws

This page covers all LangChain integrations with the [Amazon Web Services (AWS)](https://aws.amazon.com/) platform.

> [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of
> high-performing foundation models (FMs) from leading AI companies like `AI21 Labs`, `Anthropic`, `Cohere`,
> `Meta`, `Stability AI`, and `Amazon` via a single API, along with a broad set of capabilities you need to
> build generative AI applications with security, privacy, and responsible AI. Using `Amazon Bedrock`,
> you can easily experiment with and evaluate top FMs for your use case, privately customize them with
> your data using techniques such as fine-tuning and `Retrieval Augmented Generation` (`RAG`), and build
> agents that execute tasks using your enterprise systems and data sources. Since `Amazon Bedrock` is
> serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy
> generative AI capabilities into your applications using the AWS services you are already familiar with.

See a [usage example](/oss/python/integrations/chat/bedrock).

AWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)
that provides a unified conversational interface for Bedrock models. This API does not
yet support custom models. You can see a list of all
[models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).

<Info>
  **We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html).**
</Info>

See a [usage example](/oss/python/integrations/chat/bedrock).

See a [usage example](/oss/python/integrations/llms/bedrock).

### Amazon API Gateway

> [Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for
> developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door"
> for applications to access data, business logic, or functionality from your backend services. Using
> `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication
> applications. `API Gateway` supports containerized and serverless workloads, as well as web applications.
>
> `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of
> concurrent API calls, including traffic management, CORS support, authorization and access control,
> throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs.
> You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway`
> tiered pricing model, you can reduce your cost as your API usage scales.

See a [usage example](/oss/python/integrations/llms/amazon_api_gateway).

### SageMaker Endpoint

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy
> machine learning (ML) models with fully managed infrastructure, tools, and workflows.

We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.

See a [usage example](/oss/python/integrations/llms/sagemaker).

See a [usage example](/oss/python/integrations/text_embedding/bedrock).

### SageMaker Endpoint

See a [usage example](/oss/python/integrations/text_embedding/sagemaker-endpoint).

### AWS S3 Directory and File

> [Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)
> is an object storage service.
> [AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html)
> [AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html)

See a [usage example for S3DirectoryLoader](/oss/python/integrations/document_loaders/aws_s3_directory).

See a [usage example for S3FileLoader](/oss/python/integrations/document_loaders/aws_s3_file).

> [Amazon Textract](https://docs.aws.amazon.com/managedservices/latest/userguide/textract.html) is a machine
> learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.

See a [usage example](/oss/python/integrations/document_loaders/amazon_textract).

> [Amazon Athena](https://aws.amazon.com/athena/) is a serverless, interactive analytics service built
> on open-source frameworks, supporting open-table and file formats.

See a [usage example](/oss/python/integrations/document_loaders/athena).

> The [AWS Glue Data Catalog](https://docs.aws.amazon.com/en_en/glue/latest/dg/catalog-and-crawler.html) is a centralized metadata
> repository that allows you to manage, access, and share metadata about
> your data stored in AWS. It acts as a metadata store for your data assets,
> enabling various AWS services and your applications to query and connect
> to the data they need efficiently.

See a [usage example](/oss/python/integrations/document_loaders/glue_catalog).

### Amazon OpenSearch Service

> [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) performs
> interactive log analytics, real-time application monitoring, website search, and more. `OpenSearch` is
> an open source,
> distributed search and analytics suite derived from `Elasticsearch`. `Amazon OpenSearch Service` offers the
> latest versions of `OpenSearch`, support for many versions of `Elasticsearch`, as well as
> visualization capabilities powered by `OpenSearch Dashboards` and `Kibana`.

We need to install several python libraries.

See a [usage example](/oss/python/integrations/vectorstores/opensearch#using-aos-amazon-opensearch-service).

### Amazon DocumentDB Vector Search

> [Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.
> With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB.
> Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search.

#### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/documentdb).

We need to install the `pymongo` python package.

#### Deploy DocumentDB on AWS

[Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud.

AWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see [Cloud Computing with Amazon Web Services](https://aws.amazon.com/what-is-aws/).

See a [usage example](/oss/python/integrations/vectorstores/documentdb).

[Amazon MemoryDB](https://aws.amazon.com/memorydb/) is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store,
enabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today.

InMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.

See a [usage example](/oss/python/integrations/vectorstores/memorydb).

> [Amazon Kendra](https://docs.aws.amazon.com/kendra/latest/dg/what-is-kendra.html) is an intelligent search service
> provided by `Amazon Web Services` (`AWS`). It utilizes advanced natural language processing (NLP) and machine
> learning algorithms to enable powerful search capabilities across various data sources within an organization.
> `Kendra` is designed to help users find the information they need quickly and accurately,
> improving productivity and decision-making.

> With `Kendra`, we can search across a wide range of content types, including documents, FAQs, knowledge bases,
> manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and
> contextual meanings to provide highly relevant search results.

We need to install the `langchain-aws` library.

See a [usage example](/oss/python/integrations/retrievers/amazon_kendra_retriever).

### Amazon Bedrock (Knowledge Bases)

> [Knowledge bases for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) is an
> `Amazon Web Services` (`AWS`) offering which lets you quickly build RAG applications by using your
> private data to customize foundation model response.

We need to install the `langchain-aws` library.

See a [usage example](/oss/python/integrations/retrievers/bedrock).

> [`Amazon AWS Lambda`](https://aws.amazon.com/pm/lambda/) is a serverless computing service provided by
> `Amazon Web Services` (`AWS`). It helps developers to build and run applications and services without
> provisioning or managing servers. This serverless architecture enables you to focus on writing and
> deploying code, while AWS automatically takes care of scaling, patching, and managing the
> infrastructure required to run your applications.

We need to install `boto3` python library.

See a [usage example](/oss/python/integrations/tools/awslambda).

> [Amazon Neptune](https://aws.amazon.com/neptune/)
> is a high-performance graph analytics and serverless database for superior scalability and availability.

For the Cypher and SPARQL integrations below, we need to install the `langchain-aws` library.

### Amazon Neptune with Cypher

See a [usage example](/oss/python/integrations/graphs/amazon_neptune_open_cypher).

### Amazon Neptune with SPARQL

### Bedrock token usage

### SageMaker Tracking

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service that is used to quickly
> and easily build, train and deploy machine learning (ML) models.

> [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) is a capability
> of `Amazon SageMaker` that lets you organize, track,
> compare and evaluate ML experiments and model versions.

We need to install several python libraries.

See a [usage example](/oss/python/integrations/callbacks/sagemaker_tracking).

### Amazon Comprehend Moderation Chain

> [Amazon Comprehend](https://aws.amazon.com/comprehend/) is a natural-language processing (NLP) service that
> uses machine learning to uncover valuable insights and connections in text.

We need to install the `boto3` and `nltk` libraries.

See a [usage example](https://python.langchain.com/v0.1/docs/guides/productionization/safety/amazon_comprehend_chain/).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/aws.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Bedrock Converse

AWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)
that provides a unified conversational interface for Bedrock models. This API does not
yet support custom models. You can see a list of all
[models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).

<Info>
  **We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api_reference/aws/chat_models/langchain_aws.chat_models.bedrock_converse.ChatBedrockConverse.html).**
</Info>

See a [usage example](/oss/python/integrations/chat/bedrock).
```

Example 2 (unknown):
```unknown
## LLMs

### Bedrock

See a [usage example](/oss/python/integrations/llms/bedrock).
```

Example 3 (unknown):
```unknown
### Amazon API Gateway

> [Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for
> developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the "front door"
> for applications to access data, business logic, or functionality from your backend services. Using
> `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication
> applications. `API Gateway` supports containerized and serverless workloads, as well as web applications.
>
> `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of
> concurrent API calls, including traffic management, CORS support, authorization and access control,
> throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs.
> You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway`
> tiered pricing model, you can reduce your cost as your API usage scales.

See a [usage example](/oss/python/integrations/llms/amazon_api_gateway).
```

Example 4 (unknown):
```unknown
### SageMaker Endpoint

> [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy
> machine learning (ML) models with fully managed infrastructure, tools, and workflows.

We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`.

See a [usage example](/oss/python/integrations/llms/sagemaker).
```

---

## AzureChatOpenAI

**URL:** llms-txt#azurechatopenai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/azure

Azure OpenAI is a Microsoft Azure service that provides powerful language models from OpenAI.

This will help you getting started with AzureChatOpenAI [chat models](/oss/javascript/langchain/models). For detailed documentation of all AzureChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html).

### Integration details

| Class                                                                                         | Package                                                                | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/azure_chat_openai) |                                             Downloads                                             |                                             Version                                            |
| :-------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------- | :---: | :----------: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [AzureChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.AzureChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) |   ❌   |       ✅      |                                          ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ✅     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ✅                               |

[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview).

If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

You'll also need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance. Then, if using Node.js, you can set your credentials as environment variables:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:
```

---

## AzureOpenAIEmbeddings

**URL:** llms-txt#azureopenaiembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/azure_openai

[Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.

LangChain.js supports integration with [Azure OpenAI](https://azure.microsoft.com/products/ai-services/openai-service/) using the new Azure integration in the [OpenAI SDK](https://github.com/openai/openai-node).

You can learn more about Azure OpenAI and its difference with the OpenAI API on [this page](https://learn.microsoft.com/azure/ai-services/openai/overview). If you don't have an Azure account, you can [create a free account](https://azure.microsoft.com/free/) to get started.

This will help you get started with AzureOpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `AzureOpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html).

<Info>
  **Previously, LangChain.js supported integration with Azure OpenAI using the dedicated [Azure OpenAI SDK](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai). This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seamless transition between the OpenAI API and Azure OpenAI.**

If you are using Azure OpenAI with the deprecated SDK, see the [migration guide](#migration-from-azure-openai-sdk) to update to the new API.
</Info>

### Integration details

| Class                                                                                                     | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/azure_openai/) |                                             Downloads                                             |                                             Version                                            |
| :-------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [AzureOpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.AzureOpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   ❌   |                                             ✅                                             | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

To access Azure OpenAI embedding models you'll need to create an Azure account, get an API key, and install the `@langchain/openai` integration package.

You'll need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following [this guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource?pivots=web-portal).

Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the "Keys and Endpoint" section of your instance.

If you're using Node.js, you can define the following environment variables to use the service:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:
```

---

## A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments

**URL:** llms-txt#a-comparative-experiment-allows-you-to-provide-a-preferential-ranking-on-the-outputs-of-two-or-more-experiments

---

## baseline_results.to_pandas()

**URL:** llms-txt#baseline_results.to_pandas()

**Contents:**
  - Define and evaluate new system

python  theme={null}
candidate_results = await client.aevaluate(
    agent.with_config(model="gpt-4o"),
    data=dataset_name,
    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],
    experiment_prefix="candidate-gpt-4o",
)

**Examples:**

Example 1 (unknown):
```unknown
### Define and evaluate new system

Now, let's define and evaluate our new system. In this example our new system will be the same as the old system, but will use GPT-4o instead of GPT-3.5. Since we've made our model configurable we can just update the default config passed to our agent:
```

---

## Basic authentication with email and password

**URL:** llms-txt#basic-authentication-with-email-and-password

**Contents:**
- Requirements and features
  - Migrating from None auth
  - Configuration

Source: https://docs.langchain.com/langsmith/self-host-basic-auth

LangSmith supports login via username/password with a few limitations:

* You cannot change an existing installation from basic auth mode to OAuth with PKCE (deprecated) or vice versa - installations must be either one or the other. **A basic auth installation requires a completely fresh installation including a separate PostgreSQL database/schema, unless migrating from an existing `None` type installation (see below).**
* Users must be given their initial auto-generated password once they are invited. This password may be changed later by any Organization Admin.
* You cannot use both basic auth and OAuth with client secret at the same time.

## Requirements and features

* There is a single `Default` organization that is provisioned during initial installation, and creating additional organizations is not supported
* Your initial password (configured below) must be least 12 characters long and have at least one lowercase, uppercase, and symbol
* There are no strict requirements for the secret used for signing JWTs, but we recommend securely generating a string of at least 32 characters. For example: `openssl rand -base64 32`

### Migrating from None auth

**Only supported in versions 0.7 and above.**

Migrating an installation from [None](/langsmith/authentication-methods#none) auth mode replaces the single "default" user with a user with the configured credentials and keeps all existing resources. The single pre-existing workspace ID post-migration remains `00000000-0000-0000-0000-000000000000`, but everything else about the migrated installation is standard for a basic auth installation.

To migrate, simply update your configuration as shown below and run `helm upgrade` (or `docker-compose up`) as usual.

<Note>
  Changing the JWT secret will log out your users
</Note>

Additionally, in docker-compose you will need to run the bootstrap command to create the initial organization and user:

Once configured, you will see a login screen like the one below. You should be able to login with the `initialOrgAdminEmail` and `initialOrgAdminPassword` values, and your user will be auto-provisioned with role `Organization Admin`. See the [admin guide](/langsmith/administration-overview#organization-roles) for more details on organization roles.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2f88a4bffacc308d7fbd22cba1f1c9f1" alt="LangSmith UI with basic auth" data-og-width="1424" width="1424" data-og-height="1156" height="1156" data-path="langsmith/images/langsmith-ui-basic-auth.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d0297842dd132b61fa295b52e93859e 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=975b75e1f4262b5278e2d56896bec326 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=54a1827e8611f72359a121dd209b5860 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=42618816bdfbd171e32d064fd00652da 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a73742556b59333c002f9d0f5d314e6e 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-basic-auth.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=93b93edf04b03f1ab652df38270364c2 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-basic-auth.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Additionally, in docker-compose you will need to run the bootstrap command to create the initial organization and user:
```

---

## Becomes:

**URL:** llms-txt#becomes:

[Send("github", {"query": "..."}), Send("notion", {"query": "..."})]

---

## BedrockChat

**URL:** llms-txt#bedrockchat

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/bedrock

[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

This will help you getting started with Amazon Bedrock [chat models](/oss/javascript/langchain/models). For detailed documentation of all `BedrockChat` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html).

<Tip>
  The newer [`ChatBedrockConverse` chat model is now available via the dedicated `@langchain/aws`](/oss/javascript/integrations/chat/bedrock_converse) integration package. Use [tool calling](/oss/javascript/langchain/tools) with more models with this package.
</Tip>

### Integration details

| Class                                                                                                          | Package                                                          | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/bedrock/) |                                               Downloads                                              |                                              Version                                              |
| :------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------- | :---: | :----------: | :------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |
| [`BedrockChat`](https://api.js.langchain.com/classes/langchain_community_chat_models_bedrock.BedrockChat.html) | [`@langchain/community`](https://npmjs.com/@langchain/community) |   ❌   |       ✅      |                                      ✅                                     | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/community?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/community?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ❌     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ❌                               |

To access Bedrock models you'll need to create an AWS account, set up the Bedrock API service, get an access key ID and secret key, and install the `@langchain/community` integration package.

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash  theme={null}

---

## BedrockEmbeddings

**URL:** llms-txt#bedrockembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/bedrock

[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.

This will help you get started with Amazon Bedrock [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `Bedrock` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html).

### Integration details

| Class                                                                                | Package                                                                   | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/bedrock/) |                                            Downloads                                           |                                           Version                                           |
| :----------------------------------------------------------------------------------- | :------------------------------------------------------------------------ | :---: | :----------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------: |
| [Bedrock](https://api.js.langchain.com/classes/langchain_aws.BedrockEmbeddings.html) | [@langchain/aws](https://api.js.langchain.com/modules/langchain_aws.html) |   ❌   |                                           ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/aws?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/aws?style=flat-square\&label=%20&) |

To access Bedrock embedding models you'll need to create an AWS account, get an API key, and install the `@langchain/aws` integration package.

Head to the [AWS docs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to sign up for AWS and setup your credentials. You'll also need to turn on model access for your account, which you can do by [following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html).

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash  theme={null}

---

## Before: Functional API

**URL:** llms-txt#before:-functional-api

@entrypoint(checkpointer=checkpointer)
def complex_workflow(input_data: dict) -> dict:
    step1 = process_step1(input_data).result()

if step1["needs_analysis"]:
        analysis = analyze_data(step1).result()
        if analysis["confidence"] > 0.8:
            result = high_confidence_path(analysis).result()
        else:
            result = low_confidence_path(analysis).result()
    else:
        result = simple_path(step1).result()

---

## Before: Over-engineered Graph API

**URL:** llms-txt#before:-over-engineered-graph-api

class SimpleState(TypedDict):
    input: str
    step1: str
    step2: str
    result: str

---

## Beta LangSmith Collector-Proxy

**URL:** llms-txt#beta-langsmith-collector-proxy

**Contents:**
- When to Use the Collector-Proxy
- Key Features
- Configuration
  - Project Configuration
  - Authentication
- Deployment (Docker)
- Usage
- Health & Scaling
- Horizontal Scaling
- Fork & Extend

Source: https://docs.langchain.com/langsmith/collector-proxy

<Note>
  This is a beta feature. The API may change in future releases.
</Note>

The LangSmith Collector-Proxy is a lightweight, high-performance proxy server that sits between your application and the LangSmith backend. It batches and compresses trace data before sending it to LangSmith, reducing network overhead and improving performance.

## When to Use the Collector-Proxy

The Collector-Proxy is particularly valuable when:

* You're running multiple instances of your application in parallel and need to efficiently aggregate traces
* You want more efficient tracing than direct OTEL API calls to LangSmith (the collector optimizes batching and compression)
* You're using a language that doesn't have a native LangSmith SDK

* **Efficient Data Transfer** Batches multiple spans into fewer, larger uploads.
* **Compression** Uses zstd to minimize payload size.
* **OTLP Support** Accepts OTLP JSON and Protobuf over HTTP POST.
* **Semantic Translation** Maps GenAI/OpenInference conventions to the LangSmith Run model.
* **Flexible Batching** Flush by span count or time interval.

Configure via environment variables:

| Variable             | Description                       | Default                           |
| -------------------- | --------------------------------- | --------------------------------- |
| `HTTP_PORT`          | Port to run the proxy server      | `4318`                            |
| `LANGSMITH_ENDPOINT` | LangSmith backend URL             | `https://api.smith.langchain.com` |
| `LANGSMITH_API_KEY`  | API key for LangSmith             | **Required** (env var or header)  |
| `LANGSMITH_PROJECT`  | Default tracing project           | Default project if not specified  |
| `BATCH_SIZE`         | Spans per upload batch            | `100`                             |
| `FLUSH_INTERVAL_MS`  | Flush interval in milliseconds    | `1000`                            |
| `MAX_BUFFER_BYTES`   | Max uncompressed buffer size      | `10485760` (10 MB)                |
| `MAX_BODY_BYTES`     | Max incoming request body size    | `209715200` (200 MB)              |
| `MAX_RETRIES`        | Retry attempts for failed uploads | `3`                               |
| `RETRY_BACKOFF_MS`   | Initial backoff in milliseconds   | `100`                             |

### Project Configuration

The Collector-Proxy supports LangSmith project configuration with the following priority:

1. If a project is specified in the request headers (`Langsmith-Project`), that project will be used
2. If no project is specified in headers, it will use the project set in the `LANGSMITH_PROJECT` environment variable
3. If neither is set, it will trace to the `default` project.

The API key can be provided either:

* As an environment variable (`LANGSMITH_API_KEY`)
* In the request headers (`X-API-Key`)

## Deployment (Docker)

You can deploy the Collector-Proxy with Docker:

1. **Build the image**

2. **Run the container**

Point any OTLP-compatible client or the OpenTelemetry Collector exporter at:

* **Liveness**: `GET /live` → 200
* **Readiness**: `GET /ready` → 200

## Horizontal Scaling

To ensure full traces are batched correctly, route spans with the same trace ID to the same instance (e.g., via consistent hashing).

Fork the [Collector-Proxy repo on GitHub](https://github.com/langchain-ai/langsmith-collector-proxy) and implement your own converter:

* Create a custom `GenAiConverter` or modify the existing one in `internal/translator/otel_converter.go`
* Register the custom converter in `internal/translator/translator.go`

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/collector-proxy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. **Run the container**
```

Example 2 (unknown):
```unknown
## Usage

Point any OTLP-compatible client or the OpenTelemetry Collector exporter at:
```

Example 3 (unknown):
```unknown
Send a test trace:
```

---

## Bob creates his own thread

**URL:** llms-txt#bob-creates-his-own-thread

bob_thread = await bob.threads.create()
await bob.runs.create(
    thread_id=bob_thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hi, this is Bob's private chat"}]}
)
print(f"✅ Bob created his own thread: {bob_thread['thread_id']}")

---

## Bob tries to access Alice's thread

**URL:** llms-txt#bob-tries-to-access-alice's-thread

try:
    await bob.threads.get(alice_thread["thread_id"])
    print("❌ Bob shouldn't see Alice's thread!")
except Exception as e:
    print("✅ Bob correctly denied access:", e)

---

## Building our graph

**URL:** llms-txt#building-our-graph

graph_builder = StateGraph(State)

graph_builder.add_node(gather_info)
graph_builder.add_node(refund)
graph_builder.add_node(lookup)

graph_builder.set_entry_point("gather_info")
graph_builder.add_edge("lookup", END)
graph_builder.add_edge("refund", END)

refund_graph = graph_builder.compile()

**Examples:**

Example 1 (unknown):
```unknown
We can visualize our refund graph:
```

---

## Build a semantic search engine with LangChain

**URL:** llms-txt#build-a-semantic-search-engine-with-langchain

**Contents:**
- Overview
  - Concepts
- Setup
  - Installation
  - LangSmith
- 1. Documents and Document Loaders
  - Loading documents
  - Splitting
- 2. Embeddings
- 3. Vector stores

Source: https://docs.langchain.com/oss/python/langchain/knowledge-base

This tutorial will familiarize you with LangChain's [document loader](/oss/python/langchain/retrieval#document-loaders), [embedding](/oss/python/langchain/retrieval#embedding-models), and [vector store](/oss/python/langchain/retrieval#vector-store) abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources -- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/oss/python/langchain/retrieval).

Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.

This guide focuses on retrieval of text data. We will cover the following concepts:

* [Documents and document loaders](/oss/python/integrations/document_loaders);
* [Text splitters](/oss/python/integrations/splitters);
* [Embeddings](/oss/python/integrations/text_embedding);
* [Vector stores](/oss/python/integrations/vectorstores) and [retrievers](/oss/python/integrations/retrievers).

This tutorial requires the `langchain-community` and `pypdf` packages:

For more details, see our [Installation guide](/oss/python/langchain/install).

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:

Or, if in a notebook, you can set them with:

## 1. Documents and Document Loaders

LangChain implements a [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:

* `page_content`: a string representing the content;
* `metadata`: a dict containing arbitrary metadata;
* `id`: (optional) a string identifier for the document.

The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) object often represents a chunk of a larger document.

We can generate sample documents when desired:

However, the LangChain ecosystem implements [document loaders](/oss/python/langchain/retrieval#document-loaders) that [integrate with hundreds of common sources](/oss/python/integrations/document_loaders/). This makes it easy to incorporate data from these sources into your AI application.

### Loading documents

Let's load a PDF into a sequence of [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects. [Here is a sample PDF](https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/example_data/nke-10k-2023.pdf) -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for [available PDF document loaders](/oss/python/integrations/document_loaders/#pdfs).

`PyPDFLoader` loads one [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) object per PDF page. For each, we can easily access:

* The string content of the page;
* Metadata containing the file name and page number.

For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not "washed out" by surrounding text.

We can use [text splitters](/oss/python/langchain/retrieval#text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters
with 200 characters of overlap between chunks. The overlap helps
mitigate the possibility of separating a statement from important
context related to it. We use the
`RecursiveCharacterTextSplitter`,
which will recursively split the document using common separators like
new lines until each chunk is the appropriate size. This is the
recommended text splitter for generic text use cases.

We set `add_start_index=True` so that the character index where each
split Document starts within the initial Document is preserved as
metadata attribute “start\_index”.

Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/oss/python/langchain/retrieval#embedding_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.

LangChain supports embeddings from [dozens of providers](/oss/python/integrations/text_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:

<Tabs>
  <Tab title="OpenAI">

<Tab title="Google Gemini">

<Tab title="Google Vertex">

<Tab title="HuggingFace">

<Tab title="MistralAI">

<Tab title="Voyage AI">

<Tab title="IBM watsonx">

<Tab title="Isaacus">

Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.

LangChain [VectorStore](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) objects contain methods for adding text and [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/oss/python/langchain/retrieval#embedding_models) models, which determine how text data is translated to numeric vectors.

LangChain includes a suite of [integrations](/oss/python/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/oss/python/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:

<Tabs>
  <Tab title="In-memory">

<Tab title="Amazon OpenSearch">

<Tab title="AstraDB">

<Tab title="MongoDB">

<Tab title="PGVector">

<Tab title="PGVectorStore">

<Tab title="Pinecone">

Having instantiated our vector store, we can now index the documents.

Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/oss/python/integrations/vectorstores) for more detail.

Once we've instantiated a [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) that contains documents, we can query it. [VectorStore](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) includes methods for querying:

* Synchronously and asynchronously;
* By string query and by vector;
* With and without returning similarity scores;
* By similarity and [maximum marginal relevance](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore.max_marginal_relevance_search) (to balance similarity with query to diversity in retrieved results).

The methods will generally include a list of [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects in their outputs.

Embeddings typically represent text as a "dense" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.

Return documents based on similarity to a string query:

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

For more details, see our [Installation guide](/oss/python/langchain/install).

### LangSmith

Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
The best way to do this is with [LangSmith](https://smith.langchain.com).

After you sign up at the link above, make sure to set your environment variables to start logging traces:
```

Example 3 (unknown):
```unknown
Or, if in a notebook, you can set them with:
```

Example 4 (unknown):
```unknown
## 1. Documents and Document Loaders

LangChain implements a [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:

* `page_content`: a string representing the content;
* `metadata`: a dict containing arbitrary metadata;
* `id`: (optional) a string identifier for the document.

The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) object often represents a chunk of a larger document.

We can generate sample documents when desired:
```

---

## Build a SQL assistant with on-demand skills

**URL:** llms-txt#build-a-sql-assistant-with-on-demand-skills

**Contents:**
- How it works
- Setup
  - Installation
  - LangSmith
  - Select an LLM
- 1. Define skills
- 2. Create skill loading tool
- 3. Build skill middleware
- 4. Create the agent with skill support

Source: https://docs.langchain.com/oss/python/langchain/multi-agent/skills-sql-assistant

This tutorial shows how to use **progressive disclosure** - a context management technique where the agent loads information on-demand rather than upfront - to implement **skills** (specialized prompt-based instructions). The agent loads skills via tool calls, rather than dynamically changing the system prompt, discovering and loading only the skills it needs for each task.

**Use case:** Imagine building an agent to help write SQL queries across different business verticals in a large enterprise. Your organization might have separate datastores for each vertical, or a single monolithic database with thousands of tables. Either way, loading all schemas upfront would overwhelm the context window. Progressive disclosure solves this by loading only the relevant schema when needed. This architecture also enables different product owners and stakeholders to independently contribute and maintain skills for their specific business verticals.

**What you'll build:** A SQL query assistant with two skills (sales analytics and inventory management). The agent sees lightweight skill descriptions in its system prompt, then loads full database schemas and business logic through tool calls only when relevant to the user's query.

<Note>
  For a more complete example of a SQL agent with query execution, error correction, and validation, see our [SQL Agent tutorial](/oss/python/langchain/sql-agent). This tutorial focuses on the progressive disclosure pattern which can be applied to any domain.
</Note>

<Tip>
  Progressive disclosure was popularized by Anthropic as a technique for building scalable agent skills systems. This approach uses a three-level architecture (metadata → core content → detailed resources) where agents load information only as needed. For more on this technique, see [Equipping agents for the real world with Agent Skills](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills).
</Tip>

Here's the flow when a user asks for a SQL query:

**Why progressive disclosure:**

* **Reduces context usage** - load only the 2-3 skills needed for a task, not all available skills
* **Enables team autonomy** - different teams can develop specialized skills independently (similar to other multi-agent architectures)
* **Scales efficiently** - add dozens or hundreds of skills without overwhelming context
* **Simplifies conversation history** - single agent with one conversation thread

**What are skills:** Skills, as popularized by Claude Code, are primarily prompt-based: self-contained units of specialized instructions for specific business tasks. In Claude Code, skills are exposed as directories with files on the file system, discovered through file operations. Skills guide behavior through prompts and can provide information about tool usage or include sample code for a coding agent to execute.

<Tip>
  Skills with progressive disclosure can be viewed as a form of [RAG (Retrieval-Augmented Generation)](/oss/python/langchain/rag), where each skill is a retrieval unit—though not necessarily backed by embeddings or keyword search, but by tools for browsing content (like file operations or, in this tutorial, direct lookup).
</Tip>

* **Latency**: Loading skills on-demand requires additional tool calls, which adds latency to the first request that needs each skill
* **Workflow control**: Basic implementations rely on prompting to guide skill usage - you cannot enforce hard constraints like "always try skill A before skill B" without custom logic

<Tip>
  **Implementing your own skills system**

When building your own skills implementation (as we do in this tutorial), the core concept is progressive disclosure - loading information on-demand. Beyond that, you have full flexibility in implementation:

* **Storage**: databases, S3, in-memory data structures, or any backend
  * **Discovery**: direct lookup (this tutorial), RAG for large skill collections, file system scanning, or API calls
  * **Loading logic**: customize latency characteristics and add logic to search through skill content or rank relevance
  * **Side effects**: define what happens when a skill loads, such as exposing tools associated with that skill (covered in section 8)

This flexibility lets you optimize for your specific requirements around performance, storage, and workflow control.
</Tip>

This tutorial requires the `langchain` package:

For more details, see our [Installation guide](/oss/python/langchain/install).

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:

Select a chat model from LangChain's suite of integrations:

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="AWS Bedrock">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)

</CodeGroup>
  </Tab>

<Tab title="HuggingFace">
    👉 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)

</CodeGroup>
  </Tab>
</Tabs>

First, define the structure for skills. Each skill has a name, a brief description (shown in the system prompt), and full content (loaded on-demand):

Now define example skills for a SQL query assistant. The skills are designed to be **lightweight in description** (shown to the agent upfront) but **detailed in content** (loaded only when needed):

<Accordion title="View complete skill definitions">
  
</Accordion>

## 2. Create skill loading tool

Create a tool to load full skill content on-demand:

The `load_skill` tool returns the full skill content as a string, which becomes part of the conversation as a ToolMessage. For more details on creating and using tools, see the [Tools guide](/oss/python/langchain/tools).

## 3. Build skill middleware

Create custom middleware that injects skill descriptions into the system prompt. This middleware makes skills discoverable without loading their full content upfront.

<Note>
  This guide demonstrates creating custom middleware. For a comprehensive guide on middleware concepts and patterns, see the [custom middleware documentation](/oss/python/langchain/middleware/custom).
</Note>

The middleware appends skill descriptions to the system prompt, making the agent aware of available skills without loading their full content. The `load_skill` tool is registered as a class variable, making it available to the agent.

<Note>
  **Production consideration**: This tutorial loads the skill list in `__init__` for simplicity. In a production system, you may want to load skills in the `before_agent` hook instead, allowing them to be refreshed periodically to reflect up-to-date changes (e.g., when new skills are added or existing ones are modified). See the [before\_agent hook documentation](/oss/python/langchain/middleware/custom#before_agent) for details.
</Note>

## 4. Create the agent with skill support

Now create the agent with the skill middleware and a checkpointer for state persistence:

```python  theme={null}
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

**Examples:**

Example 1 (unknown):
```unknown
**Why progressive disclosure:**

* **Reduces context usage** - load only the 2-3 skills needed for a task, not all available skills
* **Enables team autonomy** - different teams can develop specialized skills independently (similar to other multi-agent architectures)
* **Scales efficiently** - add dozens or hundreds of skills without overwhelming context
* **Simplifies conversation history** - single agent with one conversation thread

**What are skills:** Skills, as popularized by Claude Code, are primarily prompt-based: self-contained units of specialized instructions for specific business tasks. In Claude Code, skills are exposed as directories with files on the file system, discovered through file operations. Skills guide behavior through prompts and can provide information about tool usage or include sample code for a coding agent to execute.

<Tip>
  Skills with progressive disclosure can be viewed as a form of [RAG (Retrieval-Augmented Generation)](/oss/python/langchain/rag), where each skill is a retrieval unit—though not necessarily backed by embeddings or keyword search, but by tools for browsing content (like file operations or, in this tutorial, direct lookup).
</Tip>

**Trade-offs:**

* **Latency**: Loading skills on-demand requires additional tool calls, which adds latency to the first request that needs each skill
* **Workflow control**: Basic implementations rely on prompting to guide skill usage - you cannot enforce hard constraints like "always try skill A before skill B" without custom logic

<Tip>
  **Implementing your own skills system**

  When building your own skills implementation (as we do in this tutorial), the core concept is progressive disclosure - loading information on-demand. Beyond that, you have full flexibility in implementation:

  * **Storage**: databases, S3, in-memory data structures, or any backend
  * **Discovery**: direct lookup (this tutorial), RAG for large skill collections, file system scanning, or API calls
  * **Loading logic**: customize latency characteristics and add logic to search through skill content or rank relevance
  * **Side effects**: define what happens when a skill loads, such as exposing tools associated with that skill (covered in section 8)

  This flexibility lets you optimize for your specific requirements around performance, storage, and workflow control.
</Tip>

## Setup

### Installation

This tutorial requires the `langchain` package:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

For more details, see our [Installation guide](/oss/python/langchain/install).

### LangSmith

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:

<CodeGroup>
```

---

## build based on the provided config

**URL:** llms-txt#build-based-on-the-provided-config

def make_graph(config: RunnableConfig):
    user_id = config.get("configurable", {}).get("user_id")
    # route to different graph state / structure based on the user ID
    if user_id == "1":
        return make_default_graph()
    else:
        return make_alternative_graph()

{
    "dependencies": ["."],
    "graphs": {
        "openai_agent": "./openai_agent.py:make_graph",
    },
    "env": "./.env"
}
```

See more info on LangGraph API configuration file [here](/langsmith/cli#configuration-file)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/graph-rebuild.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Finally, you need to specify the path to your graph-making function (`make_graph`) in `langgraph.json`:
```

---

## Build customer support with handoffs

**URL:** llms-txt#build-customer-support-with-handoffs

**Contents:**
- Setup
  - Installation
  - LangSmith
  - Select an LLM
- 1. Define custom state

Source: https://docs.langchain.com/oss/python/langchain/multi-agent/handoffs-customer-support

The [state machine pattern](/oss/python/langchain/multi-agent/handoffs) describes workflows where an agent's behavior changes as it moves through different states of a task. This tutorial shows how to implement a state machine by using tool calls to dynamically change a single agent's configuration—updating its available tools and instructions based on the current state. The state can be determined from multiple sources: the agent's past actions (tool calls), external state (such as API call results), or even initial user input (for example, by running a classifier to determine user intent).

In this tutorial, you'll build a customer support agent that does the following:

* Collects warranty information before proceeding.
* Classifies issues as hardware or software.
* Provides solutions or escalates to human support.
* Maintains conversation state across multiple turns.

Unlike the [subagents pattern](/oss/python/langchain/multi-agent/subagents-personal-assistant) where sub-agents are called as tools, the **state machine pattern** uses a single agent whose configuration changes based on workflow progress. Each "step" is just a different configuration (system prompt + tools) of the same underlying agent, selected dynamically based on state.

Here's the workflow we'll build:

This tutorial requires the `langchain` package:

For more details, see our [Installation guide](/oss/python/langchain/install).

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:

Select a chat model from LangChain's suite of integrations:

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="AWS Bedrock">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)

</CodeGroup>
  </Tab>

<Tab title="HuggingFace">
    👉 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)

</CodeGroup>
  </Tab>
</Tabs>

## 1. Define custom state

First, define a custom state schema that tracks which step is currently active:

```python  theme={null}
from langchain.agents import AgentState
from typing_extensions import NotRequired
from typing import Literal

**Examples:**

Example 1 (unknown):
```unknown
## Setup

### Installation

This tutorial requires the `langchain` package:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

For more details, see our [Installation guide](/oss/python/langchain/install).

### LangSmith

Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:

<CodeGroup>
```

---

## Build Docker image

**URL:** llms-txt#build-docker-image

langgraph build -t my-agent:latest

---

## Build graph

**URL:** llms-txt#build-graph

builder = StateGraph(State)
builder.add_node("agent", agent_with_monitoring)
builder.add_edge(START, "agent")
builder.add_conditional_edges("agent", route_decision)
graph = builder.compile()

---

## Build the graph with explicit schemas

**URL:** llms-txt#build-the-graph-with-explicit-schemas

builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
builder.add_node(answer_node)
builder.add_edge(START, "answer_node")
builder.add_edge("answer_node", END)
graph = builder.compile()

---

## Build the graph with input and output schemas specified

**URL:** llms-txt#build-the-graph-with-input-and-output-schemas-specified

builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)
builder.add_node(answer_node)  # Add the answer node
builder.add_edge(START, "answer_node")  # Define the starting edge
builder.add_edge("answer_node", END)  # Define the ending edge
graph = builder.compile()  # Compile the graph

---

## Build the state graph

**URL:** llms-txt#build-the-state-graph

builder = StateGraph(OverallState)
builder.add_node(node)  # node_1 is the first node
builder.add_edge(START, "node")  # Start the graph with node_1
builder.add_edge("node", END)  # End the graph after node_1
graph = builder.compile()

---

## Build workflow

**URL:** llms-txt#build-workflow

orchestrator_worker_builder = StateGraph(State)

---

## Bulk Exporting Trace Data

**URL:** llms-txt#bulk-exporting-trace-data

**Contents:**
- Destinations
- Exporting Data
  - Destinations - Providing a S3 bucket
  - Preparing the Destination
  - Create an export job
  - Scheduled exports
- Monitoring the Export Job
  - Monitor Export Status
  - List Runs for an Export
  - List All Exports

Source: https://docs.langchain.com/langsmith/data-export

<Info>
  **Plan restrictions apply**

Please note that the Data Export functionality is only supported for [LangSmith Plus or Enterprise tiers](https://www.langchain.com/pricing-langsmith).
</Info>

LangSmith's bulk data export functionality allows you to export your traces into an external destination. This can be useful if you want to analyze the
data offline in a tool such as BigQuery, Snowflake, RedShift, Jupyter Notebooks, etc.

An export can be launched to target a specific LangSmith project and date range. Once a batch export is launched, our system will handle the orchestration and resilience of the export process.
Please note that exporting your data may take some time depending on the size of your data. We also have a limit on how many of your exports can run at the same time.
Bulk exports also have a runtime timeout of 24 hours.

Currently we support exporting to an S3 bucket or S3 API compatible bucket that you provide. The data will be exported in
[Parquet](https://parquet.apache.org/docs/overview/) columnar format. This format will allow you to easily import the data into
other systems. The data export will contain equivalent data fields as the [Run data format](/langsmith/run-data-format).

### Destinations - Providing a S3 bucket

To export LangSmith data, you will need to provide an S3 bucket where the data will be exported to.

The following information is needed for the export:

* **Bucket Name**: The name of the S3 bucket where the data will be exported to.
* **Prefix**: The root prefix within the bucket where the data will be exported to.
* **S3 Region**: The region of the bucket - this is needed for AWS S3 buckets.
* **Endpoint URL**: The endpoint URL for the S3 bucket - this is needed for S3 API compatible buckets.
* **Access Key**: The access key for the S3 bucket.
* **Secret Key**: The secret key for the S3 bucket.
* **Include Bucket in Prefix** (optional): Whether to include the bucket name as part of the path prefix. Defaults to `false` for new destinations or when the bucket name is already present in the path. Set to `true` for legacy compatibility or when using storage systems that require the bucket name in the path.

We support any S3 compatible bucket, for non AWS buckets such as GCS or MinIO, you will need to provide the endpoint URL.

### Preparing the Destination

<Note>
  **For self-hosted and EU region deployments**

Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below.
  For the EU region, use `eu.api.smith.langchain.com`.
</Note>

<Note>
  **Permissions required**

Both the `backend` and `queue` services require write access to the destination bucket:

* The `backend` service attempts to write a test file to the destination bucket when the export destination is created.
    It will delete the test file if it has permission to do so (delete access is optional).
  * The `queue` service is responsible for bulk export execution and uploading the files to the bucket.
</Note>

The following example demonstrates how to create a destination using cURL. Replace the placeholder values with your actual configuration details.
Note that credentials will be stored securely in an encrypted form in our system.

Use the returned `id` to reference this destination in subsequent bulk export operations.

**If you receive an error while creating a destination, see [debug destination errors](#debugging-destination-errors) for details on how to debug this.**

#### Credentials configuration

<Note>**Requires LangSmith Helm version >= `0.10.34` (application version >= `0.10.91`)**</Note>

We support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:

* To use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html) that include an AWS session token,
  additionally provide the `credentials.session_token` key when creating the bulk export destination.
* (Self-hosted only): To use environment-based credentials such as with [AWS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA),
  omit the `credentials` key from the request when creating the bulk export destination.
  In this case, the [standard Boto3 credentials locations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#credentials) will be checked in the order defined by the library.

For AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.

#### Google GCS XML S3 compatible bucket

When using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`
which is typically `https://storage.googleapis.com`.
Here is an example of the API request when using the GCS XML API which is compatible with S3:

See [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info

### Create an export job

To export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our [filter query language](/langsmith/trace-query-syntax#filter-query-language) and [examples](/langsmith/export-traces#use-filter-query-language) to determine the correct filter expression for your export.

You can use the following cURL command to create the job:

<Note>
  The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.
</Note>

Use the returned `id` to reference this export in subsequent bulk export operations.

#### Limiting exported fields

<Note>
  Requires LangSmith Helm version >= `0.12.11` (application version >= `0.12.42`). This feature **is supported** in [scheduled bulk exports](#scheduled-exports) and [standard bulk exports](#create-an-export-job).
</Note>

You can improve bulk export speed and reduce row size by limiting which fields are included in the exported Parquet files using the `export_fields` parameter. When `export_fields` is provided, only the specified fields are exported as columns in the Parquet files. When `export_fields` is not provided, all exportable fields are included.

This is particularly useful when you want to exclude larger fields like `inputs` and `outputs`.

The following example creates an export job that only includes specific fields:

The `export_fields` parameter accepts an array of field names. Available fields include the [Run data format](/langsmith/run-data-format) fields as well as additional export-only fields:

* `tenant_id`
* `is_root`

<Tip>
  **Performance tip**: Excluding `inputs` and `outputs` from your export can significantly improve export performance and reduce file sizes, especially for large runs. Only include these fields if you need them for your analysis.
</Tip>

### Scheduled exports

<Note>
  Requires LangSmith Helm version >= `0.10.42` (application version >= `0.10.109`)
</Note>

Scheduled exports collect runs periodically and export to the configured destination.
To create a scheduled export, include `interval_hours` and remove `end_time`:

You can also use `export_fields` with scheduled exports to limit which fields are exported:

* `interval_hours` must be between 1 hour and 168 hours (1 week) inclusive.
* For spawned exports, the first time range exported is `start_time=(scheduled_export_start_time), end_time=(start_time + interval_hours)`.
  Then `start_time=(previous_export_end_time), end_time=(this_export_start_time + interval_hours)`, and so on.
* `end_time` must be omitted for scheduled exports. `end_time` is still required for non-scheduled exports.
* Scheduled exports can be stopped by [cancelling the export](#stop-an-export).
  * Exports that have been spawned by a scheduled export have the `source_bulk_export_id` attribute filled.
  * If desired, these spawned bulk exports must be canceled separately from the source scheduled bulk export -
    canceling the source bulk export **does not** cancel the spawned bulk exports.
* Spawned exports run at `end_time + 10 minutes` to account for any runs that are submitted with `end_time` in the recent past.
* `format_version` (optional): The format version to use for the parquet files. `"v2_beta"` has (1) enhanced datatypes for the columns and (2) a Hive-compliant folder structure.

If a scheduled bulk export is created with `start_time=2025-07-16T00:00:00Z` and `interval_hours=6`:

| Export | Start Time           | End Time             | Runs At              |
| ------ | -------------------- | -------------------- | -------------------- |
| 1      | 2025-07-16T00:00:00Z | 2025-07-16T06:00:00Z | 2025-07-16T06:10:00Z |
| 2      | 2025-07-16T06:00:00Z | 2025-07-16T12:00:00Z | 2025-07-16T12:10:00Z |
| 3      | 2025-07-16T12:00:00Z | 2025-07-16T18:00:00Z | 2025-07-16T18:10:00Z |

## Monitoring the Export Job

### Monitor Export Status

To monitor the status of an export job, use the following cURL command:

Replace `{export_id}` with the ID of the export you want to monitor. This command retrieves the current status of the specified export job.

### List Runs for an Export

An export is typically broken up into multiple runs which correspond to a specific date partition to export.
To list all runs associated with a specific export, use the following cURL command:

This command fetches all runs related to the specified export, providing details such as run ID, status, creation time, rows exported, etc.

To retrieve a list of all export jobs, use the following cURL command:

This command returns a list of all export jobs along with their current statuses and creation timestamps.

To stop an existing export, use the following cURL command:

Replace `{export_id}` with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled,
you will need to create a new export job instead.

## Partitioning Scheme

Data will be exported into your bucket into the follow Hive partitioned format:

## Importing Data into other systems

Importing data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:

To import your data into BigQuery, see [Loading Data from Parquet](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet) and also
[Hive Partitioned loads](https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs).

You can load data into Snowflake from S3 by following the [Load from Cloud Document](https://docs.snowflake.com/en/user-guide/tutorials/load-from-cloud-tutorial).

You can COPY data from S3 or Parquet into Amazon Redshift by following the [AWS COPY command documentation](https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html).

You can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:

See [Clickhouse S3 Integration Documentation](https://clickhouse.com/docs/en/engines/table-engines/integrations/s3) for more information.

You can query the data from S3 in-memory with SQL using DuckDB. See [S3 import Documentation](https://duckdb.org/docs/guides/network_cloud_storage/s3_import.html).

### Debugging Destination Errors

The destinations API endpoint will validate that the destination and credentials are valid and that write access is
is present for the bucket.

If you receive an error, and would like to debug this error, you can use the [AWS CLI](https://aws.amazon.com/cli/)
to test the connectivity to the bucket. You should be able to write a file with the CLI using the same
data that you supplied to the destinations API above.

```bash  theme={null}
aws configure

**Examples:**

Example 1 (unknown):
```unknown
Use the returned `id` to reference this destination in subsequent bulk export operations.

**If you receive an error while creating a destination, see [debug destination errors](#debugging-destination-errors) for details on how to debug this.**

#### Credentials configuration

<Note>**Requires LangSmith Helm version >= `0.10.34` (application version >= `0.10.91`)**</Note>

We support the following additional credentials formats besides static `access_key_id` and `secret_access_key`:

* To use [temporary credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html) that include an AWS session token,
  additionally provide the `credentials.session_token` key when creating the bulk export destination.
* (Self-hosted only): To use environment-based credentials such as with [AWS IAM Roles for Service Accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) (IRSA),
  omit the `credentials` key from the request when creating the bulk export destination.
  In this case, the [standard Boto3 credentials locations](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#credentials) will be checked in the order defined by the library.

#### AWS S3 bucket

For AWS S3, you can leave off the `endpoint_url` and supply the region that matches the region of your bucket.
```

Example 2 (unknown):
```unknown
#### Google GCS XML S3 compatible bucket

When using Google's GCS bucket, you need to use the XML S3 compatible API, and supply the `endpoint_url`
which is typically `https://storage.googleapis.com`.
Here is an example of the API request when using the GCS XML API which is compatible with S3:
```

Example 3 (unknown):
```unknown
See [Google documentation](https://cloud.google.com/storage/docs/interoperability#xml_api) for more info

### Create an export job

To export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our [filter query language](/langsmith/trace-query-syntax#filter-query-language) and [examples](/langsmith/export-traces#use-filter-query-language) to determine the correct filter expression for your export.

You can use the following cURL command to create the job:
```

Example 4 (unknown):
```unknown
<Note>
  The `session_id` is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.
</Note>

Use the returned `id` to reference this export in subsequent bulk export operations.

#### Limiting exported fields

<Note>
  Requires LangSmith Helm version >= `0.12.11` (application version >= `0.12.42`). This feature **is supported** in [scheduled bulk exports](#scheduled-exports) and [standard bulk exports](#create-an-export-job).
</Note>

You can improve bulk export speed and reduce row size by limiting which fields are included in the exported Parquet files using the `export_fields` parameter. When `export_fields` is provided, only the specified fields are exported as columns in the Parquet files. When `export_fields` is not provided, all exportable fields are included.

This is particularly useful when you want to exclude larger fields like `inputs` and `outputs`.

The following example creates an export job that only includes specific fields:
```

---

## By default we provide a StateBackend

**URL:** llms-txt#by-default-we-provide-a-statebackend

agent = create_deep_agent()

---

## Calculate aggregate metrics

**URL:** llms-txt#calculate-aggregate-metrics

total_score = 0
count = 0

for result in results:
    eval_result = result["evaluation_results"]["results"][0]
    total_score += eval_result.score
    count += 1

average_accuracy = total_score / count

print(f"Average accuracy: {average_accuracy:.2%}")

---

## Callbacks

**URL:** llms-txt#callbacks

Source: https://docs.langchain.com/oss/javascript/integrations/callbacks/index

<Columns cols={3}>
  <Card title="Datadog Tracer" icon="link" href="/oss/javascript/integrations/callbacks/datadog_tracer" arrow="true" cta="View guide" />

<Card title="Upstash Rate Limit" icon="link" href="/oss/javascript/integrations/callbacks/upstash_ratelimit_callback" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/callbacks/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Call the function with traced attachments

**URL:** llms-txt#call-the-function-with-traced-attachments

**Contents:**
  - TypeScript

result = trace_with_attachments(
    val=val,
    text=text,
    image=image_attachment,
    audio=audio_attachment,
    video=video_attachment,
    pdf=pdf_attachment,
    csv=csv_attachment,
)
typescript TypeScript theme={null}
type AttachmentData = Uint8Array | ArrayBuffer;
type Attachments = Record<string, [string, AttachmentData]>;

extractAttachments?: (
    ...args: Parameters<Func>
) => [Attachments | undefined, KVMap];
typescript TypeScript theme={null}
import { traceable } from "langsmith/traceable";

const traceableWithAttachments = traceable(
    (
        val: number,
        text: string,
        attachment: Uint8Array,
        attachment2: ArrayBuffer,
        attachment3: Uint8Array,
        attachment4: ArrayBuffer,
        attachment5: Uint8Array,
    ) =>
        `Processed: ${val}, ${text}, ${attachment.length}, ${attachment2.byteLength}, ${attachment3.length}, ${attachment4.byteLength}, ${attachment5.byteLength}`,
    {
        name: "traceWithAttachments",
        extractAttachments: (
            val: number,
            text: string,
            attachment: Uint8Array,
            attachment2: ArrayBuffer,
            attachment3: Uint8Array,
            attachment4: ArrayBuffer,
            attachment5: Uint8Array,
        ) => [
            {
                "image inputs": ["image/png", attachment],
                "mp3 inputs": ["audio/mpeg", new Uint8Array(attachment2)],
                "video inputs": ["video/mp4", attachment3],
                "pdf inputs": ["application/pdf", new Uint8Array(attachment4)],
                "csv inputs": ["text/csv", new Uint8Array(attachment5)],
            },
            { val, text },
        ],
    }
);

const fs = Deno // or Node.js fs module
const image = await fs.readFile("my_image.png"); // Uint8Array
const mp3Buffer = await fs.readFile("my_mp3.mp3");
const mp3ArrayBuffer = mp3Buffer.buffer; // Convert to ArrayBuffer
const video = await fs.readFile("my_video.mp4"); // Uint8Array
const pdfBuffer = await fs.readFile("my_document.pdf");
const pdfArrayBuffer = pdfBuffer.buffer; // Convert to ArrayBuffer
const csv = await fs.readFile("test-vals.csv"); // Uint8Array

// Define example parameters
const val = 42;
const text = "Hello, world!";

// Call traceableWithAttachments with the files
const result = await traceableWithAttachments(
    val, text, image, mp3ArrayBuffer, video, pdfArrayBuffer, csv
);
```

Here is how the above would look in the LangSmith UI. You can expand each attachment to view its contents.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cb21a1c6d8904d1d7b2215652a6127a5" alt="Trace with attachments" data-og-width="3012" width="3012" data-og-height="1696" height="1696" data-path="langsmith/images/trace-with-attachments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4caa7aaa44cd296b2f30ff8d6f6d7199 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=01a4392a7dbf3d10184778d2ab3737a1 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e8047eabe350453619c255e88d6fe1d5 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9b0bbb43e10ea30cf466fb65502af907 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0060d9877954b82d34593d8789f7d0a5 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-with-attachments.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ec15ad89f02769defc2d5f5637d88e11 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/upload-files-with-traces.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### TypeScript

In the TypeScript SDK, you can add attachments to traces by using `Uint8Array` or `ArrayBuffer` as data types. Each attachment's MIME type is specified within `extractAttachments`:

* `Uint8Array`: Useful for handling binary data directly.
* `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.

Wrap your function with `traceable` and include your attachments within the `extractAttachments` option.

In the TypeScript SDK, the `extractAttachments` function is an optional parameter in the `traceable` configuration. When the traceable-wrapped function is invoked, it extracts binary data (e.g., images, audio files) from your inputs and logs them alongside other trace data, specifying their MIME types.

Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.
```

Example 2 (unknown):
```unknown

```

---

## Call the graph: here we call it to generate a list of jokes

**URL:** llms-txt#call-the-graph:-here-we-call-it-to-generate-a-list-of-jokes

**Contents:**
- Create and control loops

for step in graph.stream({"topic": "animals"}):
    print(step)

{'generate_topics': {'subjects': ['lions', 'elephants', 'penguins']}}
{'generate_joke': {'jokes': ["Why don't lions like fast food? Because they can't catch it!"]}}
{'generate_joke': {'jokes': ["Why don't elephants use computers? They're afraid of the mouse!"]}}
{'generate_joke': {'jokes': ['Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.']}}
{'best_joke': {'best_selected_joke': 'penguins'}}
python  theme={null}
builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)

def route(state: State) -> Literal["b", END]:
    if termination_condition(state):
        return END
    else:
        return "b"

builder.add_edge(START, "a")
builder.add_conditional_edges("a", route)
builder.add_edge("b", "a")
graph = builder.compile()
python  theme={null}
from langgraph.errors import GraphRecursionError

try:
    graph.invoke(inputs, {"recursion_limit": 3})
except GraphRecursionError:
    print("Recursion Error")
python  theme={null}
import operator
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END

class State(TypedDict):
    # The operator.add reducer fn makes this append-only
    aggregate: Annotated[list, operator.add]

def a(state: State):
    print(f'Node A sees {state["aggregate"]}')
    return {"aggregate": ["A"]}

def b(state: State):
    print(f'Node B sees {state["aggregate"]}')
    return {"aggregate": ["B"]}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Create and control loops

When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) that routes to the [END](/oss/python/langgraph/graph-api#end-node) node once we reach some termination condition.

You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](/oss/python/langgraph/graph-api#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](/oss/python/langgraph/graph-api#recursion-limit).

Let's consider a simple graph with a loop to better understand how these mechanisms work.

<Tip>
  To return the last value of your state instead of receiving a recursion limit error, see the [next section](#impose-a-recursion-limit).
</Tip>

When creating a loop, you can include a conditional edge that specifies a termination condition:
```

Example 3 (unknown):
```unknown
To control the recursion limit, specify `"recursionLimit"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle:
```

Example 4 (unknown):
```unknown
Let's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.
```

---

## Cancel (abort the operation)

**URL:** llms-txt#cancel-(abort-the-operation)

**Contents:**
- Additional resources

ElicitResult(action="cancel")
```

## Additional resources

* [MCP documentation](https://modelcontextprotocol.io/introduction)
* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports)
* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/mcp.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Cancel Runs

**URL:** llms-txt#cancel-runs

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/cancel-runs

langsmith/agent-server-openapi.json post /runs/cancel
Cancel one or more runs. Can cancel runs by thread ID and run IDs, or by status filter.

---

## Cancel Run

**URL:** llms-txt#cancel-run

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/cancel-run

langsmith/agent-server-openapi.json post /threads/{thread_id}/runs/{run_id}/cancel

---

## candidate_results.to_pandas()

**URL:** llms-txt#candidate_results.to_pandas()

**Contents:**
- Comparing the results

## Comparing the results

After running both experiments, you can view them in your dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1c5d4f1cf212e2c38917319c7bbf7f99" alt="Dataset page" data-og-width="3022" width="3022" data-og-height="1536" height="1536" data-path="langsmith/images/dataset-page.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=01ebc5373bb6428b614eeade16aeb606 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b2308ef5ed76bb80f111a89000457424 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f2c76f9b1194a1a56efaa97d88b885f0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=beb69429d2a2e208b75924593a9a10c1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0fa47df443b9695d1e6863a57ca3a016 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-page.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d257f3444d8357fb0b0340d09c16e127 2500w" />

The results reveal an interesting tradeoff between the two models:

1. GPT-4o shows improved performance in following formatting rules, consistently including the requested number of emojis
2. However, GPT-4o is less reliable at staying grounded in the provided search results

To illustrate the grounding issue: in [this example run](https://smith.langchain.com/public/be060e19-0bc0-4798-94f5-c3d35719a5f6/r/07d43e7a-8632-479d-ae28-c7eac6e54da4), GPT-4o included facts about Abū Bakr Muhammad ibn Zakariyyā al-Rāzī's medical contributions that weren't present in the search results. This demonstrates how it's pulling from its internal knowledge rather than strictly using the provided information.

This backtesting exercise revealed that while GPT-4o is generally considered a more capable model, simply upgrading to it wouldn't improve our tweet-writer. To effectively use GPT-4o, we would need to:

* Refine our prompts to more strongly emphasize using only provided information
* Or modify our system architecture to better constrain the model's outputs

This insight demonstrates the value of backtesting - it helped us identify potential issues before deployment.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a8ab311399f3d0e69554a62f939fd475" alt="Tutorial comparison view" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/tutorial-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=92dca1af013a79d9ce2ee944a17e23a9 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=16ac3bc225307b5408a49c225646a99e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=517037b9b2a372dd5d4b2dc4e41eac6a 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d1b1d90b0838a53d1c693617fad61eb4 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ecbe835c06c9451cac57d7cf0a16d0b9 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tutorial-comparison-view.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=77d1da832c66587699653956fc15ccb6 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-backtests-new-agent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Can equivalently use the 'aevaluate' function directly:

**URL:** llms-txt#can-equivalently-use-the-'aevaluate'-function-directly:

---

## Case studies

**URL:** llms-txt#case-studies

Source: https://docs.langchain.com/oss/python/langgraph/case-studies

This list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, we'd love for you to share your story and add it to the list. You’re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases.

| Company                                                                                                                                 | Industry                             | Use case                                                      | Reference                                                                                                                                                                                                                                                                                                                                     |
| --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [AirTop](https://www.airtop.ai/)                                                                                                        | Software & Technology (GenAI Native) | Browser automation for AI agents                              | [Case study, 2024](https://blog.langchain.dev/customers-airtop/)                                                                                                                                                                                                                                                                              |
| [AppFolio](https://www.appfolio.com/)                                                                                                   | Real Estate                          | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-appfolio/)                                                                                                                                                                                                                                                                            |
| [Athena Intelligence](https://www.athenaintel.com/)                                                                                     | Software & Technology (GenAI Native) | Research & summarization                                      | [Case study, 2024](https://blog.langchain.dev/customers-athena-intelligence/)                                                                                                                                                                                                                                                                 |
| [BlackRock](https://www.blackrock.com/)                                                                                                 | Financial Services                   | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/oyqeCHFM5U4?feature=shared)                                                                                                                                                                                                                                                                           |
| [Captide](https://www.captide.co/)                                                                                                      | Software & Technology (GenAI Native) | Data extraction                                               | [Case study, 2025](https://blog.langchain.dev/how-captide-is-redefining-equity-research-with-agentic-workflows-built-on-langgraph-and-langsmith/)                                                                                                                                                                                             |
| [Cisco CX](https://www.cisco.com/site/us/en/services/modern-data-center/index.html?CCID=cc005911\&DTID=eivtotr001480\&OID=srwsas032775) | Software & Technology                | Customer support                                              | [Interrupt Talk, 2025](https://youtu.be/gPhyPRtIMn0?feature=shared)                                                                                                                                                                                                                                                                           |
| [Cisco Outshift](https://outshift.cisco.com/)                                                                                           | Software & Technology                | DevOps                                                        | [Video story, 2025](https://www.youtube.com/watch?v=htcb-vGR_x0); [Case study, 2025](https://blog.langchain.com/cisco-outshift/); [Blog post, 2025](https://outshift.cisco.com/blog/build-react-agent-application-for-devops-tasks-using-rest-apis)                                                                                           |
| [Cisco TAC](https://www.cisco.com/c/en/us/support/index.html)                                                                           | Software & Technology                | Customer support                                              | [Video story, 2025](https://youtu.be/EAj0HBDGqaE?feature=shared)                                                                                                                                                                                                                                                                              |
| [City of Hope](https://www.cityofhope.org/)                                                                                             | Non-profit                           | Copilot for domain-specific task                              | [Video story, 2025](https://youtu.be/9ABwtK2gIZU?feature=shared)                                                                                                                                                                                                                                                                              |
| [C.H. Robinson](https://www.chrobinson.com/en-us/)                                                                                      | Logistics                            | Automation                                                    | [Case study, 2025](https://blog.langchain.dev/customers-chrobinson/)                                                                                                                                                                                                                                                                          |
| [Definely](https://www.definely.com/)                                                                                                   | Legal                                | Copilot for domain-specific task                              | [Case study, 2025](https://blog.langchain.com/customers-definely/)                                                                                                                                                                                                                                                                            |
| [Docent Pro](https://docentpro.com/)                                                                                                    | Travel                               | GenAI embedded product experiences                            | [Case study, 2025](https://blog.langchain.com/customers-docentpro/)                                                                                                                                                                                                                                                                           |
| [Elastic](https://www.elastic.co/)                                                                                                      | Software & Technology                | Copilot for domain-specific task                              | [Blog post, 2025](https://www.elastic.co/blog/elastic-security-generative-ai-features)                                                                                                                                                                                                                                                        |
| [Exa](https://exa.ai/)                                                                                                                  | Software & Technology (GenAI Native) | Search                                                        | [Case study, 2025](https://blog.langchain.com/exa/)                                                                                                                                                                                                                                                                                           |
| [GitLab](https://about.gitlab.com/)                                                                                                     | Software & Technology                | Code generation                                               | [Duo workflow docs](https://handbook.gitlab.com/handbook/engineering/architecture/design-documents/duo_workflow/)                                                                                                                                                                                                                             |
| [Harmonic](https://harmonic.ai/)                                                                                                        | Software & Technology                | Search                                                        | [Case study, 2025](https://blog.langchain.com/customers-harmonic/)                                                                                                                                                                                                                                                                            |
| [Inconvo](https://inconvo.ai/?ref=blog.langchain.dev)                                                                                   | Software & Technology                | Code generation                                               | [Case study, 2025](https://blog.langchain.dev/customers-inconvo/)                                                                                                                                                                                                                                                                             |
| [Infor](https://infor.com/)                                                                                                             | Software & Technology                | GenAI embedded product experiences; customer support; copilot | [Case study, 2025](https://blog.langchain.dev/customers-infor/)                                                                                                                                                                                                                                                                               |
| [J.P. Morgan](https://www.jpmorganchase.com/)                                                                                           | Financial Services                   | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/yMalr0jiOAc?feature=shared)                                                                                                                                                                                                                                                                           |
| [Klarna](https://www.klarna.com/)                                                                                                       | Fintech                              | Copilot for domain-specific task                              | [Case study, 2025](https://blog.langchain.dev/customers-klarna/)                                                                                                                                                                                                                                                                              |
| [Komodo Health](https://www.komodohealth.com/)                                                                                          | Healthcare                           | Copilot for domain-specific task                              | [Blog post](https://www.komodohealth.com/perspectives/new-gen-ai-assistant-empowers-the-enterprise/)                                                                                                                                                                                                                                          |
| [LinkedIn](https://www.linkedin.com/)                                                                                                   | Social Media                         | Code generation; Search & discovery                           | [Interrupt talk, 2025](https://youtu.be/NmblVxyBhi8?feature=shared); [Blog post, 2025](https://www.linkedin.com/blog/engineering/ai/practical-text-to-sql-for-data-analytics); [Blog post, 2024](https://www.linkedin.com/blog/engineering/generative-ai/behind-the-platform-the-journey-to-create-the-linkedin-genai-application-tech-stack) |
| [Minimal](https://gominimal.ai/)                                                                                                        | E-commerce                           | Customer support                                              | [Case study, 2025](https://blog.langchain.dev/how-minimal-built-a-multi-agent-customer-support-system-with-langgraph-langsmith/)                                                                                                                                                                                                              |
| [Modern Treasury](https://www.moderntreasury.com/)                                                                                      | Fintech                              | GenAI embedded product experiences                            | [Video story, 2025](https://youtu.be/AwAiffXqaCU?feature=shared)                                                                                                                                                                                                                                                                              |
| [Monday](https://monday.com/)                                                                                                           | Software & Technology                | GenAI embedded product experiences                            | [Interrupt talk, 2025](https://blog.langchain.dev/how-minimal-built-a-multi-agent-customer-support-system-with-langgraph-langsmith/)                                                                                                                                                                                                          |
| [Morningstar](https://www.morningstar.com/)                                                                                             | Financial Services                   | Research & summarization                                      | [Video story, 2025](https://youtu.be/6LidoFXCJPs?feature=shared)                                                                                                                                                                                                                                                                              |
| [OpenRecovery](https://www.openrecovery.com/)                                                                                           | Healthcare                           | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-openrecovery/)                                                                                                                                                                                                                                                                        |
| [Pigment](https://www.pigment.com/)                                                                                                     | Fintech                              | GenAI embedded product experiences                            | [Video story, 2025](https://youtu.be/5JVSO2KYOmE?feature=shared)                                                                                                                                                                                                                                                                              |
| [Prosper](https://www.prosper.com/)                                                                                                     | Fintech                              | Customer support                                              | [Video story, 2025](https://youtu.be/9RFNOYtkwsc?feature=shared)                                                                                                                                                                                                                                                                              |
| [Qodo](https://www.qodo.ai/)                                                                                                            | Software & Technology (GenAI Native) | Code generation                                               | [Blog post, 2025](https://www.qodo.ai/blog/why-we-chose-langgraph-to-build-our-coding-agent/)                                                                                                                                                                                                                                                 |
| [Rakuten](https://www.rakuten.com/)                                                                                                     | E-commerce / Fintech                 | Copilot for domain-specific task                              | [Video story, 2025](https://youtu.be/gD1LIjCkuA8?feature=shared); [Blog post, 2025](https://rakuten.today/blog/from-ai-hype-to-real-world-tools-rakuten-teams-up-with-langchain.html)                                                                                                                                                         |
| [Replit](https://replit.com/)                                                                                                           | Software & Technology                | Code generation                                               | [Blog post, 2024](https://blog.langchain.dev/customers-replit/); [Breakout agent story, 2024](https://www.langchain.com/breakoutagents/replit); [Fireside chat video, 2024](https://www.youtube.com/watch?v=ViykMqljjxU)                                                                                                                      |
| [Rexera](https://www.rexera.com/)                                                                                                       | Real Estate (GenAI Native)           | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-rexera/)                                                                                                                                                                                                                                                                              |
| [Abu Dhabi Government](https://www.tamm.abudhabi/)                                                                                      | Government                           | Search                                                        | [Case study, 2025](https://blog.langchain.com/customers-abu-dhabi-government/)                                                                                                                                                                                                                                                                |
| [Tradestack](https://www.tradestack.uk/)                                                                                                | Software & Technology (GenAI Native) | Copilot for domain-specific task                              | [Case study, 2024](https://blog.langchain.dev/customers-tradestack/)                                                                                                                                                                                                                                                                          |
| [Uber](https://www.uber.com/)                                                                                                           | Transportation                       | Developer productivity; Code generation                       | [Interrupt talk, 2025](https://youtu.be/Bugs0dVcNI8?feature=shared); [Presentation, 2024](https://dpe.org/sessions/ty-smith-adam-huda/this-year-in-ubers-ai-driven-developer-productivity-revolution/); [Video, 2024](https://www.youtube.com/watch?v=8rkA5vWUE4Y)                                                                            |
| [Unify](https://www.unifygtm.com/)                                                                                                      | Software & Technology (GenAI Native) | Copilot for domain-specific task                              | [Interrupt talk, 2025](https://youtu.be/pKk-LfhujwI?feature=shared); [Blog post, 2024](https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/)                                                                                                                                             |
| [Vizient](https://www.vizientinc.com/)                                                                                                  | Healthcare                           | Copilot for domain-specific task                              | [Video story, 2025](https://www.youtube.com/watch?v=vrjJ6NuyTWA); [Case study, 2025](https://blog.langchain.dev/p/3d2cd58c-13a5-4df9-bd84-7d54ed0ed82c/)                                                                                                                                                                                      |
| [Vodafone](https://www.vodafone.com/)                                                                                                   | Telecommunications                   | Code generation; internal search                              | [Case study, 2025](https://blog.langchain.dev/customers-vodafone/)                                                                                                                                                                                                                                                                            |
| [WebToon](https://www.webtoons.com/en/)                                                                                                 | Media & Entertainment                | Data extraction                                               | [Case study, 2025](https://blog.langchain.com/customers-webtoon/)                                                                                                                                                                                                                                                                             |
| [11x](https://www.11x.ai/)                                                                                                              | Software & Technology (GenAI Native) | Research & outreach                                           | [Interrupt talk, 2025](https://youtu.be/fegwPmaAPQk?feature=shared)                                                                                                                                                                                                                                                                           |

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/case-studies.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Changelog

**URL:** llms-txt#changelog

Source: https://docs.langchain.com/oss/python/releases/changelog

Log of updates and improvements to our Python packages

<Callout icon="rss" color="#DFC5FE" iconType="regular">
  **Subscribe**: Our changelog includes an [RSS feed](https://docs.langchain.com/oss/python/releases/changelog/rss.xml) that can integrate with [Slack](https://slack.com/help/articles/218688467-Add-RSS-feeds-to-Slack), [email](https://zapier.com/apps/email/integrations/rss/1441/send-new-rss-feed-entries-via-email), Discord bots like [Readybot](https://readybot.io/) or [RSS Feeds to Discord Bot](https://rss.app/en/bots/rssfeeds-discord-bot), and other subscription tools.
</Callout>

<Update label="Dec 15, 2025" tags={["langchain", "integrations"]}>
  ## `langchain` v1.2.0

* [`create_agent`](/oss/python/langchain/agents): Simplified support for provider-specific tool parameters and definitions via a new [`extras`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool.extras) attribute on [tools](/oss/python/langchain/tools). Examples:
    * Provider-specific configuration such as Anthropic's [programmatic tool calling](/oss/python/integrations/chat/anthropic#programmatic-tool-calling) and [tool search](/oss/python/integrations/chat/anthropic#tool-search).
    * Built-in tools that are executed client-side, as supported by [Anthropic](/oss/python/integrations/chat/anthropic#built-in-tools), [OpenAI](/oss/python/integrations/chat/openai#responses-api), and other providers.
  * Support for strict schema-adherence in agent `response_format` (see [`ProviderStrategy`](/oss/python/langchain/structured-output#provider-strategy) docs).
</Update>

<Update label="Dec 8, 2025" tags={["langchain", "integrations"]}>
  ## `langchain-google-genai` v4.0.0

We've re-written the Google GenAI integration to use Google's consolidated Generative AI SDK, which provides access to the Gemini API and Vertex AI Platform under the same interface. This includes minimal breaking changes as well as deprecated packages in `langchain-google-vertexai`.

See the full [release notes and migration guide](https://github.com/langchain-ai/langchain-google/discussions/1422) for details.
</Update>

<Update label="Nov 25, 2025" tags={["langchain"]}>
  ## `langchain` v1.1.0

* [Model profiles](/oss/python/langchain/models#model-profiles): Chat models now expose supported features and capabilities through a `.profile` attribute. These data are derived from [models.dev](https://models.dev), an open source project providing model capability data.
  * [Summarization middleware](/oss/python/langchain/middleware/built-in#summarization): Updated to support flexible trigger points using model profiles for context-aware summarization.
  * [Structured output](/oss/python/langchain/structured-output): `ProviderStrategy` support (native structured output) can now be inferred from model profiles.
  * [`SystemMessage` for `create_agent`](/oss/python/langchain/middleware/custom#working-with-system-messages): Support for passing `SystemMessage` instances directly to `create_agent`'s `system_prompt` parameter, enabling advanced features like cache control and structured content blocks.
  * [Model retry middleware](/oss/python/langchain/middleware/built-in#model-retry): New middleware for automatically retrying failed model calls with configurable exponential backoff.
  * [Content moderation middleware](/oss/python/langchain/middleware/built-in#content-moderation): OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.
</Update>

<Update label="Oct 20, 2025" tags={["langchain", "langgraph"]}>
  ## v1.0.0

* [Release notes](/oss/python/releases/langchain-v1)
  * [Migration guide](/oss/python/migrate/langchain-v1)

* [Release notes](/oss/python/releases/langgraph-v1)
  * [Migration guide](/oss/python/migrate/langgraph-v1)

<Callout icon="bullhorn" color="#DFC5FE" iconType="regular">
    If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=01-langchain.yml) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langchain/tree/v0.3/docs/docs) and [API reference](https://reference.langchain.com/v0.3/python/).
  </Callout>
</Update>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/changelog.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## ChatAnthropic

**URL:** llms-txt#chatanthropic

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/anthropic

[Anthropic](https://www.anthropic.com/) is an AI safety and research company. They are the creator of Claude.

This will help you getting started with Anthropic [chat models](/oss/javascript/langchain/models). For detailed documentation of all `ChatAnthropic` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html).

### Integration details

| Class                                                                                        | Package                                                                      | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/anthropic/) |                                               Downloads                                              |                                              Version                                              |
| :------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------- | :---: | :----------: | :--------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: |
| [ChatAnthropic](https://api.js.langchain.com/classes/langchain_anthropic.ChatAnthropic.html) | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic) |   ❌   |       ✅      |                                       ✅                                      | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/anthropic?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/anthropic?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ✅     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ❌                               |

You'll need to sign up and obtain an [Anthropic API key](https://www.anthropic.com/), and install the `@langchain/anthropic` integration package.

Head to [Anthropic's website](https://www.anthropic.com/) to sign up to Anthropic and generate an API key. Once you've done this set the `ANTHROPIC_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:
```

---

## ChatGoogleGenerativeAI

**URL:** llms-txt#chatgooglegenerativeai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/google_generative_ai

[Google AI](https://ai.google.dev/) offers a number of different chat models, including the powerful Gemini series. For information on the latest models, their features, context windows, etc. head to the [Google AI docs](https://ai.google.dev/gemini-api/docs/models/gemini).

This will help you getting started with `ChatGoogleGenerativeAI` [chat models](/oss/javascript/langchain/models). For detailed documentation of all `ChatGoogleGenerativeAI` features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html).

### Integration details

| Class                                                                                                             | Package                                                                                     | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/google_generative_ai) |                                                Downloads                                                |                                                Version                                               |
| :---------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------ | :---: | :----------: | :------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------: |
| [ChatGoogleGenerativeAI](https://api.js.langchain.com/classes/langchain_google_genai.ChatGoogleGenerativeAI.html) | [@langchain/google-genai](https://api.js.langchain.com/modules/langchain_google_genai.html) |   ❌   |       ✅      |                                            ✅                                           | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/google-genai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/google-genai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ✅     |                               ✅                              |      ✅      |      ✅      |                               ✅                               |                              ✅                              |                                ❌                               |

You can access Google's `gemini` and `gemini-vision` models, as well as other
generative models in LangChain through `ChatGoogleGenerativeAI` class in the
`@langchain/google-genai` integration package.

<Tip>
  You can also access Google's `gemini` family of models via the LangChain VertexAI and VertexAI-web integrations. Click [here](/oss/javascript/integrations/chat/google_vertex_ai) to read the docs.
</Tip>

Get an API key here: [https://ai.google.dev/tutorials/setup](https://ai.google.dev/tutorials/setup)

Then set the `GOOGLE_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:
```

---

## ChatGroq

**URL:** llms-txt#chatgroq

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials
  - Installation
- Instantiation
- Invocation
- API reference

Source: https://docs.langchain.com/oss/python/integrations/chat/groq

Get started using Groq [chat models](/oss/python/langchain/models) in LangChain.

<Warning>
  This page makes reference to [Groq](https://console.groq.com/docs/overview), an AI hardware and software company. For information on how to use Grok models (provided by [xAI](https://docs.x.ai/docs/overview)), see the [xAI provider page](/oss/python/integrations/providers/xai).
</Warning>

<Tip>
  **API Reference**

For detailed documentation of all features and configuration options, head to the [`ChatGroq`](https://reference.langchain.com/python/integrations/langchain_groq/#langchain_groq.ChatGroq) API reference.
</Tip>

For a list of all Groq models, visit their [docs](https://console.groq.com/docs/models?utm_source=langchain).

### Integration details

| Class                                                                                                     | Package                                                                                | Local | Serializable | [JS support](https://js.langchain.com/docs/integrations/chat/groq) |                                            Downloads                                            |                                            Version                                           |
| :-------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------- | :---: | :----------: | :----------------------------------------------------------------: | :---------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------: |
| [`ChatGroq`](https://reference.langchain.com/python/integrations/langchain_groq/#langchain_groq.ChatGroq) | [`langchain-groq`](https://reference.langchain.com/python/integrations/langchain_groq) |   ❌   |     beta     |                                  ✅                                 | ![PyPI - Downloads](https://img.shields.io/pypi/dm/langchain-groq?style=flat-square\&label=%20) | ![PyPI - Version](https://img.shields.io/pypi/v/langchain-groq?style=flat-square\&label=%20) |

| [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output) | JSON mode | [Image input](/oss/python/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/python/langchain/streaming#llm-tokens) | Native async | [Token usage](/oss/python/langchain/models#token-usage) | [Logprobs](/oss/python/langchain/models#log-probabilities) |
| :-----------------------------------------: | :----------------------------------------------------------: | :-------: | :------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------------: | :----------: | :-----------------------------------------------------: | :--------------------------------------------------------: |
|                      ✅                      |                               ✅                              |     ✅     |                             ❌                            |      ❌      |      ❌      |                                  ✅                                  |       ✅      |                            ✅                            |                              ✅                             |

To access Groq models you'll need to create a Groq account, get an API key, and install the `langchain-groq` integration package.

Head to the [Groq console](https://console.groq.com/login?utm_source=langchain\&utm_content=chat_page) to sign up to Groq and generate an API key. Once you've done this set the GROQ\_API\_KEY environment variable:

To enable automated tracing of your model calls, set your [LangSmith](https://docs.langchain.com/langsmith/home) API key:

The LangChain Groq integration lives in the `langchain-groq` package:

Now we can instantiate our model object and generate chat completions.

<Note>
  **Reasoning Format**

If you choose to set a `reasoning_format`, you must ensure that the model you are using supports it. You can find a list of supported models in the [Groq documentation](https://console.groq.com/docs/reasoning).
</Note>

For detailed documentation of all ChatGroq features and configurations head to the [API reference](https://python.langchain.com/api_reference/groq/chat_models/langchain_groq.chat_models.ChatGroq.html).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/groq.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To enable automated tracing of your model calls, set your [LangSmith](https://docs.langchain.com/langsmith/home) API key:
```

Example 2 (unknown):
```unknown
### Installation

The LangChain Groq integration lives in the `langchain-groq` package:
```

Example 3 (unknown):
```unknown
## Instantiation

Now we can instantiate our model object and generate chat completions.

<Note>
  **Reasoning Format**

  If you choose to set a `reasoning_format`, you must ensure that the model you are using supports it. You can find a list of supported models in the [Groq documentation](https://console.groq.com/docs/reasoning).
</Note>
```

Example 4 (unknown):
```unknown
## Invocation
```

---

## ChatOpenAI

**URL:** llms-txt#chatopenai

**Contents:**
- Overview
  - Integration details
  - Model features
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/chat/openai

[OpenAI](https://en.wikipedia.org/wiki/OpenAI) is an artificial intelligence (AI) research laboratory.

This guide will help you getting started with ChatOpenAI [chat models](/oss/javascript/langchain/models). For detailed documentation of all ChatOpenAI features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html).

<Note>
  **Chat Completions API compatibility**

`ChatOpenAI` is fully compatible with OpenAI's (legacy) [Chat Completions API](https://platform.openai.com/docs/guides/completions). If you are looking to connect to other model providers that support the Chat Completions API, you can do so – see [instructions](/oss/javascript/integrations/chat#chat-completions-api).
</Note>

<Info>
  **OpenAI models hosted on Azure**

Note that certain OpenAI models can also be accessed via the [Microsoft Azure platform](https://azure.microsoft.com/en-us/products/ai-foundry/models/openai/).
</Info>

### Integration details

| Class                                                                               | Package                                                                | Local | Serializable | [PY support](https://python.langchain.com/docs/integrations/chat/openai) |                                             Downloads                                             |                                             Version                                            |
| :---------------------------------------------------------------------------------- | :--------------------------------------------------------------------- | :---: | :----------: | :----------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [ChatOpenAI](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html) | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai) |   ❌   |       ✅      |                                     ✅                                    | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

See the links in the table headers below for guides on how to use specific features.

| [Tool calling](/oss/javascript/langchain/tools) | [Structured output](/oss/javascript/langchain/structured-output) | JSON mode | [Image input](/oss/javascript/langchain/messages#multimodal) | Audio input | Video input | [Token-level streaming](/oss/javascript/langchain/streaming/) | [Token usage](/oss/javascript/langchain/models#token-usage) | [Logprobs](/oss/javascript/langchain/models#log-probabilities) |
| :---------------------------------------------: | :--------------------------------------------------------------: | :-------: | :----------------------------------------------------------: | :---------: | :---------: | :-----------------------------------------------------------: | :---------------------------------------------------------: | :------------------------------------------------------------: |
|                        ✅                        |                                 ✅                                |     ✅     |                               ✅                              |      ❌      |      ❌      |                               ✅                               |                              ✅                              |                                ✅                               |

To access OpenAI chat models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

Head to [OpenAI's website](https://platform.openai.com/) to sign up for OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:
```

---

## Chat models

**URL:** llms-txt#chat-models

**Contents:**
- Featured models
- Chat Completions API
- All chat models

Source: https://docs.langchain.com/oss/python/integrations/chat/index

[Chat models](/oss/python/langchain/models) are language models that use a sequence of [messages](/oss/python/langchain/messages) as inputs and return messages as outputs <Tooltip tip="Older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models typically do not include the prefix 'Chat' in their name or include 'LLM' as a suffix.">(as opposed to traditional, plaintext LLMs)</Tooltip>.

<Info>
  **While these LangChain classes support the indicated advanced feature**, you may need to refer to provider-specific documentation to learn which hosted models or backends support the feature.
</Info>

| Model                                                                          | [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output/) | JSON mode | Local | [Multimodal](/oss/python/langchain/messages#multimodal) |
| ------------------------------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------- | --------- | ----- | ------------------------------------------------------- |
| [`ChatAnthropic`](/oss/python/integrations/chat/anthropic)                     | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |
| [`ChatOpenAI`](/oss/python/integrations/chat/openai)                           | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |
| [`AzureChatOpenAI`](/oss/python/integrations/chat/azure_chat_openai)           | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |
| [`ChatVertexAI`](/oss/python/integrations/chat/google_vertex_ai)               | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |
| [`ChatGoogleGenerativeAI`](/oss/python/integrations/chat/google_generative_ai) | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |
| [`ChatGroq`](/oss/python/integrations/chat/groq)                               | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       |
| [`ChatBedrock`](/oss/python/integrations/chat/bedrock)                         | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |
| [`ChatAmazonNova`](/oss/python/integrations/chat/amazon_nova)                  | ✅                                           | ❌                                                             | ❌         | ❌     | ✅                                                       |
| [`ChatHuggingFace`](/oss/python/integrations/chat/huggingface)                 | ✅                                           | ✅                                                             | ❌         | ✅     | ❌                                                       |
| [`ChatOllama`](/oss/python/integrations/chat/ollama)                           | ✅                                           | ✅                                                             | ✅         | ✅     | ❌                                                       |
| [`ChatWatsonx`](/oss/python/integrations/chat/ibm_watsonx)                     | ✅                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |
| [`ChatXAI`](/oss/python/integrations/chat/xai)                                 | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |
| [`ChatNVIDIA`](/oss/python/integrations/chat/nvidia_ai_endpoints)              | ✅                                           | ✅                                                             | ✅         | ✅     | ✅                                                       |
| [`ChatCohere`](/oss/python/integrations/chat/cohere)                           | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |
| [`ChatMistralAI`](/oss/python/integrations/chat/mistralai)                     | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |
| [`ChatTogether`](/oss/python/integrations/chat/together)                       | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       |
| [`ChatFireworks`](/oss/python/integrations/chat/fireworks)                     | ✅                                           | ✅                                                             | ✅         | ❌     | ❌                                                       |
| [`ChatLlamaCpp`](/oss/python/integrations/chat/llamacpp)                       | ✅                                           | ✅                                                             | ❌         | ✅     | ❌                                                       |
| [`ChatDatabricks`](/oss/python/integrations/chat/databricks)                   | ✅                                           | ✅                                                             | ❌         | ❌     | ❌                                                       |
| [`ChatPerplexity`](/oss/python/integrations/chat/perplexity)                   | ❌                                           | ✅                                                             | ✅         | ❌     | ✅                                                       |

## Chat Completions API

Certain model providers offer endpoints that are compatible with OpenAI's (legacy) [Chat Completions API](https://platform.openai.com/docs/guides/completions). In such case, you can use [`ChatOpenAI`](/oss/python/integrations/chat/openai) with a custom `base_url` to connect to these endpoints. Note that features built on top of the Chat Completions API may not be fully supported by `ChatOpenAI`; in such cases, consider using a provider-specific class if available (e.g. [`ChatLiteLLM`](https://github.com/Akshay-Dongare/langchain-litellm/) (community-maintained) for [LiteLLM](https://litellm.ai/)).

<Accordion title="Example: OpenRouter">
  To use OpenRouter, you will need to sign up for an account and obtain an [API key](https://openrouter.ai/docs/api-reference/authentication).

Refer to the [OpenRouter documentation](https://openrouter.ai/docs/quickstart) for more details.

<Note>
    To capture [reasoning tokens](https://openrouter.ai/docs/use-cases/reasoning-tokens),

1. Switch imports from `langchain_openai` to `langchain_deepseek`
    2. Use `ChatDeepSeek` instead of `ChatOpenAI`. You will need to change param `base_url` to `api_base`.
    3. Adjust reasoning parameters as needed under `extra_body`, e.g.:

This is a known limitation with `ChatOpenAI` and will be addressed in a future release.
  </Note>
</Accordion>

<Columns cols={3}>
  <Card title="Abso" icon="link" href="/oss/python/integrations/chat/abso" arrow="true" cta="View guide" />

<Card title="AI21 Labs" icon="link" href="/oss/python/integrations/chat/ai21" arrow="true" cta="View guide" />

<Card title="AI/ML API" icon="link" href="/oss/python/integrations/chat/aimlapi" arrow="true" cta="View guide" />

<Card title="Alibaba Cloud PAI EAS" icon="link" href="/oss/python/integrations/chat/alibaba_cloud_pai_eas" arrow="true" cta="View guide" />

<Card title="Amazon Nova" icon="link" href="/oss/python/integrations/chat/amazon_nova" arrow="true" cta="View guide" />

<Card title="Anthropic" icon="link" href="/oss/python/integrations/chat/anthropic" arrow="true" cta="View guide" />

<Card title="AzureAIChatCompletionsModel" icon="link" href="/oss/python/integrations/chat/azure_ai" arrow="true" cta="View guide" />

<Card title="Azure OpenAI" icon="link" href="/oss/python/integrations/chat/azure_chat_openai" arrow="true" cta="View guide" />

<Card title="Azure ML Endpoint" icon="link" href="/oss/python/integrations/chat/azureml_chat_endpoint" arrow="true" cta="View guide" />

<Card title="Baichuan Chat" icon="link" href="/oss/python/integrations/chat/baichuan" arrow="true" cta="View guide" />

<Card title="Baidu Qianfan" icon="link" href="/oss/python/integrations/chat/baidu_qianfan_endpoint" arrow="true" cta="View guide" />

<Card title="Baseten" icon="link" href="/oss/python/integrations/chat/baseten" arrow="true" cta="View guide" />

<Card title="AWS Bedrock" icon="link" href="/oss/python/integrations/chat/bedrock" arrow="true" cta="View guide" />

<Card title="Cerebras" icon="link" href="/oss/python/integrations/chat/cerebras" arrow="true" cta="View guide" />

<Card title="CloudflareWorkersAI" icon="link" href="/oss/python/integrations/chat/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/python/integrations/chat/cohere" arrow="true" cta="View guide" />

<Card title="ContextualAI" icon="link" href="/oss/python/integrations/chat/contextual" arrow="true" cta="View guide" />

<Card title="Coze Chat" icon="link" href="/oss/python/integrations/chat/coze" arrow="true" cta="View guide" />

<Card title="Dappier AI" icon="link" href="/oss/python/integrations/chat/dappier" arrow="true" cta="View guide" />

<Card title="Databricks" icon="link" href="/oss/python/integrations/chat/databricks" arrow="true" cta="View guide" />

<Card title="DeepInfra" icon="link" href="/oss/python/integrations/chat/deepinfra" arrow="true" cta="View guide" />

<Card title="DeepSeek" icon="link" href="/oss/python/integrations/chat/deepseek" arrow="true" cta="View guide" />

<Card title="Eden AI" icon="link" href="/oss/python/integrations/chat/edenai" arrow="true" cta="View guide" />

<Card title="EverlyAI" icon="link" href="/oss/python/integrations/chat/everlyai" arrow="true" cta="View guide" />

<Card title="Featherless AI" icon="link" href="/oss/python/integrations/chat/featherless_ai" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/python/integrations/chat/fireworks" arrow="true" cta="View guide" />

<Card title="ChatFriendli" icon="link" href="/oss/python/integrations/chat/friendli" arrow="true" cta="View guide" />

<Card title="Google Gemini" icon="link" href="/oss/python/integrations/chat/google_generative_ai" arrow="true" cta="View guide" />

<Card title="Google Cloud Vertex AI" icon="link" href="/oss/python/integrations/chat/google_vertex_ai" arrow="true" cta="View guide" />

<Card title="GPTRouter" icon="link" href="/oss/python/integrations/chat/gpt_router" arrow="true" cta="View guide" />

<Card title="DigitalOcean Gradient" icon="link" href="/oss/python/integrations/chat/gradientai" arrow="true" cta="View guide" />

<Card title="GreenNode" icon="link" href="/oss/python/integrations/chat/greennode" arrow="true" cta="View guide" />

<Card title="Groq" icon="link" href="/oss/python/integrations/chat/groq" arrow="true" cta="View guide" />

<Card title="ChatHuggingFace" icon="link" href="/oss/python/integrations/chat/huggingface" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/python/integrations/chat/ibm_watsonx" arrow="true" cta="View guide" />

<Card title="JinaChat" icon="link" href="/oss/python/integrations/chat/jinachat" arrow="true" cta="View guide" />

<Card title="Kinetica" icon="link" href="/oss/python/integrations/chat/kinetica" arrow="true" cta="View guide" />

<Card title="Konko" icon="link" href="/oss/python/integrations/chat/konko" arrow="true" cta="View guide" />

<Card title="LiteLLM" icon="link" href="/oss/python/integrations/chat/litellm" arrow="true" cta="View guide" />

<Card title="Llama 2 Chat" icon="link" href="/oss/python/integrations/chat/llama2_chat" arrow="true" cta="View guide" />

<Card title="Llama API" icon="link" href="/oss/python/integrations/chat/llama_api" arrow="true" cta="View guide" />

<Card title="LlamaEdge" icon="link" href="/oss/python/integrations/chat/llama_edge" arrow="true" cta="View guide" />

<Card title="Llama.cpp" icon="link" href="/oss/python/integrations/chat/llamacpp" arrow="true" cta="View guide" />

<Card title="maritalk" icon="link" href="/oss/python/integrations/chat/maritalk" arrow="true" cta="View guide" />

<Card title="MiniMax" icon="link" href="/oss/python/integrations/chat/minimax" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/python/integrations/chat/mistralai" arrow="true" cta="View guide" />

<Card title="MLX" icon="link" href="/oss/python/integrations/chat/mlx" arrow="true" cta="View guide" />

<Card title="ModelScope" icon="link" href="/oss/python/integrations/chat/modelscope_chat_endpoint" arrow="true" cta="View guide" />

<Card title="Moonshot" icon="link" href="/oss/python/integrations/chat/moonshot" arrow="true" cta="View guide" />

<Card title="Naver" icon="link" href="/oss/python/integrations/chat/naver" arrow="true" cta="View guide" />

<Card title="Nebius" icon="link" href="/oss/python/integrations/chat/nebius" arrow="true" cta="View guide" />

<Card title="Netmind" icon="link" href="/oss/python/integrations/chat/netmind" arrow="true" cta="View guide" />

<Card title="NVIDIA AI Endpoints" icon="link" href="/oss/python/integrations/chat/nvidia_ai_endpoints" arrow="true" cta="View guide" />

<Card title="ChatOCIModelDeployment" icon="link" href="/oss/python/integrations/chat/oci_data_science" arrow="true" cta="View guide" />

<Card title="OCIGenAI" icon="link" href="/oss/python/integrations/chat/oci_generative_ai" arrow="true" cta="View guide" />

<Card title="ChatOctoAI" icon="link" href="/oss/python/integrations/chat/octoai" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/python/integrations/chat/ollama" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/python/integrations/chat/openai" arrow="true" cta="View guide" />

<Card title="Outlines" icon="link" href="/oss/python/integrations/chat/outlines" arrow="true" cta="View guide" />

<Card title="Perplexity" icon="link" href="/oss/python/integrations/chat/perplexity" arrow="true" cta="View guide" />

<Card title="Pipeshift" icon="link" href="/oss/python/integrations/chat/pipeshift" arrow="true" cta="View guide" />

<Card title="ChatPredictionGuard" icon="link" href="/oss/python/integrations/chat/predictionguard" arrow="true" cta="View guide" />

<Card title="PremAI" icon="link" href="/oss/python/integrations/chat/premai" arrow="true" cta="View guide" />

<Card title="PromptLayer ChatOpenAI" icon="link" href="/oss/python/integrations/chat/promptlayer_chatopenai" arrow="true" cta="View guide" />

<Card title="Qwen QwQ" icon="link" href="/oss/python/integrations/chat/qwq" arrow="true" cta="View guide" />

<Card title="Qwen" icon="link" href="/oss/python/integrations/chat/qwen" arrow="true" cta="View guide" />

<Card title="Reka" icon="link" href="/oss/python/integrations/chat/reka" arrow="true" cta="View guide" />

<Card title="RunPod Chat Model" icon="link" href="/oss/python/integrations/chat/runpod" arrow="true" cta="View guide" />

<Card title="SambaNova" icon="link" href="/oss/python/integrations/chat/sambanova" arrow="true" cta="View guide" />

<Card title="ChatSeekrFlow" icon="link" href="/oss/python/integrations/chat/seekrflow" arrow="true" cta="View guide" />

<Card title="Snowflake Cortex" icon="link" href="/oss/python/integrations/chat/snowflake" arrow="true" cta="View guide" />

<Card title="SparkLLM Chat" icon="link" href="/oss/python/integrations/chat/sparkllm" arrow="true" cta="View guide" />

<Card title="Nebula (Symbl.ai)" icon="link" href="/oss/python/integrations/chat/symblai_nebula" arrow="true" cta="View guide" />

<Card title="Tencent Hunyuan" icon="link" href="/oss/python/integrations/chat/tencent_hunyuan" arrow="true" cta="View guide" />

<Card title="Together" icon="link" href="/oss/python/integrations/chat/together" arrow="true" cta="View guide" />

<Card title="Tongyi Qwen" icon="link" href="/oss/python/integrations/chat/tongyi" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/chat/upstage" arrow="true" cta="View guide" />

<Card title="vLLM Chat" icon="link" href="/oss/python/integrations/chat/vllm" arrow="true" cta="View guide" />

<Card title="Volc Engine Maas" icon="link" href="/oss/python/integrations/chat/volcengine_maas" arrow="true" cta="View guide" />

<Card title="ChatWriter" icon="link" href="/oss/python/integrations/chat/writer" arrow="true" cta="View guide" />

<Card title="xAI" icon="link" href="/oss/python/integrations/chat/xai" arrow="true" cta="View guide" />

<Card title="Xinference" icon="link" href="/oss/python/integrations/chat/xinference" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/python/integrations/chat/yandex" arrow="true" cta="View guide" />

<Card title="ChatYI" icon="link" href="/oss/python/integrations/chat/yi" arrow="true" cta="View guide" />

<Card title="Yuan2.0" icon="link" href="/oss/python/integrations/chat/yuan2" arrow="true" cta="View guide" />

<Card title="ZHIPU AI" icon="link" href="/oss/python/integrations/chat/zhipuai" arrow="true" cta="View guide" />
</Columns>

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/python/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Refer to the [OpenRouter documentation](https://openrouter.ai/docs/quickstart) for more details.

  <Note>
    To capture [reasoning tokens](https://openrouter.ai/docs/use-cases/reasoning-tokens),

    1. Switch imports from `langchain_openai` to `langchain_deepseek`
    2. Use `ChatDeepSeek` instead of `ChatOpenAI`. You will need to change param `base_url` to `api_base`.
    3. Adjust reasoning parameters as needed under `extra_body`, e.g.:
```

---

## Checkpointer is REQUIRED for human-in-the-loop

**URL:** llms-txt#checkpointer-is-required-for-human-in-the-loop

**Contents:**
- Decision types
- Handle interrupts

checkpointer = MemorySaver()

agent = create_deep_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[delete_file, read_file, send_email],
    interrupt_on={
        "delete_file": True,  # Default: approve, edit, reject
        "read_file": False,   # No interrupts needed
        "send_email": {"allowed_decisions": ["approve", "reject"]},  # No editing
    },
    checkpointer=checkpointer  # Required!
)
python  theme={null}
interrupt_on = {
    # Sensitive operations: allow all options
    "delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},

# Moderate risk: approval or rejection only
    "write_file": {"allowed_decisions": ["approve", "reject"]},

# Must approve (no rejection allowed)
    "critical_operation": {"allowed_decisions": ["approve"]},
}
python  theme={null}
import uuid
from langgraph.types import Command

**Examples:**

Example 1 (unknown):
```unknown
## Decision types

The `allowed_decisions` list controls what actions a human can take when reviewing a tool call:

* **`"approve"`**: Execute the tool with the original arguments as proposed by the agent
* **`"edit"`**: Modify the tool arguments before execution
* **`"reject"`**: Skip executing this tool call entirely

You can customize which decisions are available for each tool:
```

Example 2 (unknown):
```unknown
## Handle interrupts

When an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly.
```

---

## Check broken links

**URL:** llms-txt#check-broken-links

make mint-broken-links

---

## Check for linting issues

**URL:** llms-txt#check-for-linting-issues

---

## Check if execution was interrupted

**URL:** llms-txt#check-if-execution-was-interrupted

if result.get("__interrupt__"):
    # Extract interrupt information
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]
    review_configs = interrupts["review_configs"]

# Create a lookup map from tool name to review config
    config_map = {cfg["action_name"]: cfg for cfg in review_configs}

# Display the pending actions to the user
    for action in action_requests:
        review_config = config_map[action["name"]]
        print(f"Tool: {action['name']}")
        print(f"Arguments: {action['args']}")
        print(f"Allowed decisions: {review_config['allowed_decisions']}")

# Get user decisions (one per action_request, in order)
    decisions = [
        {"type": "approve"}  # User approved the deletion
    ]

# Resume execution with decisions
    result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config  # Must use the same config!
    )

---

## Check if 'is_concise' returned False.

**URL:** llms-txt#check-if-'is_concise'-returned-false.

failed = [r for r in results if not r["evaluation_results"]["results"][0].score]

---

## Check what was interrupted

**URL:** llms-txt#check-what-was-interrupted

---

## Choosing between Graph and Functional APIs

**URL:** llms-txt#choosing-between-graph-and-functional-apis

**Contents:**
- Quick decision guide
- Detailed comparison
  - When to use the Graph API

Source: https://docs.langchain.com/oss/python/langgraph/choosing-apis

LangGraph provides two different APIs to build agent workflows: the **Graph API** and the **Functional API**. Both APIs share the same underlying runtime and can be used together in the same application, but they are designed for different use cases and development preferences.

This guide will help you understand when to use each API based on your specific requirements.

## Quick decision guide

Use the **Graph API** when you need:

* **Complex workflow visualization** for debugging and documentation
* **Explicit state management** with shared data across multiple nodes
* **Conditional branching** with multiple decision points
* **Parallel execution paths** that need to merge later
* **Team collaboration** where visual representation aids understanding

Use the **Functional API** when you want:

* **Minimal code changes** to existing procedural code
* **Standard control flow** (if/else, loops, function calls)
* **Function-scoped state** without explicit state management
* **Rapid prototyping** with less boilerplate
* **Linear workflows** with simple branching logic

## Detailed comparison

### When to use the Graph API

The [Graph API](/oss/python/langgraph/graph-api) uses a declarative approach where you define nodes, edges, and shared state to create a visual graph structure.

**1. Complex decision trees and branching logic**

When your workflow has multiple decision points that depend on various conditions, the Graph API makes these branches explicit and easy to visualize.

```python  theme={null}

---

## Classifications: [{"source": "github", "query": "..."}, {"source": "notion", "query": "..."}]

**URL:** llms-txt#classifications:-[{"source":-"github",-"query":-"..."},-{"source":-"notion",-"query":-"..."}]

---

## Clear logs if needed

**URL:** llms-txt#clear-logs-if-needed

> ~/.claude/state/hook.log
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-claude-code.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Clear separation of concerns - each team member can work on different nodes

**URL:** llms-txt#clear-separation-of-concerns---each-team-member-can-work-on-different-nodes

**Contents:**
  - When to use the Functional API

workflow.add_node("data_ingestion", data_team_function)
workflow.add_node("ml_processing", ml_team_function)
workflow.add_node("business_logic", product_team_function)
workflow.add_node("output_formatting", frontend_team_function)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### When to use the Functional API

The [Functional API](/oss/python/langgraph/functional-api) uses an imperative approach that integrates LangGraph features into standard procedural code.

**1. Existing procedural code**

When you have existing code that uses standard control flow and want to add LangGraph features with minimal refactoring.
```

---

## client.py

**URL:** llms-txt#client.py

from langsmith.run_helpers import get_current_run_tree, traceable
import httpx

@traceable
async def my_client_function():
    headers = {}
    async with httpx.AsyncClient(base_url="...") as client:
        if run_tree := get_current_run_tree():
            # add langsmith-id to headers
            headers.update(run_tree.to_headers())
        return await client.post("/my-route", headers=headers)
python  theme={null}
from langsmith import traceable
from langsmith.middleware import TracingMiddleware
from fastapi import FastAPI, Request

app = FastAPI()  # Or Flask, Django, or any other framework
app.add_middleware(TracingMiddleware)

@traceable
async def some_function():
    ...

@app.post("/my-route")
async def fake_route(request: Request):
    return await some_function()
python  theme={null}
from starlette.applications import Starlette
from starlette.middleware import Middleware
from langsmith.middleware import TracingMiddleware

routes = ...
middleware = [
    Middleware(TracingMiddleware),
]
app = Starlette(..., middleware=middleware)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Then the server (or other service) can continue the trace by handling the headers appropriately. If you are using an asgi app Starlette or FastAPI, you can connect the distributed trace using LangSmith's `TracingMiddleware`.

<Info>
  The `TracingMiddleware` class was added in `langsmith==0.1.133`.
</Info>

Example using FastAPI:
```

Example 2 (unknown):
```unknown
Or in Starlette:
```

Example 3 (unknown):
```unknown
If you are using other server frameworks, you can always "receive" the distributed trace by passing the headers in through `langsmith_extra`:
```

---

## Cloud

**URL:** llms-txt#cloud

**Contents:**
- Get started
- Cloud architecture and scalability
  - Architecture
  - Allowlisting IP addresses
  - API rate limits

Source: https://docs.langchain.com/langsmith/cloud

<Callout icon="rocket" color="#4F46E5" iconType="regular">
  If you're ready to deploy your app to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or the [full setup guide](/langsmith/deploy-to-cloud). This page explains the Cloud managed architecture for reference.
</Callout>

The **Cloud** option is a fully managed model where LangChain hosts and operates all LangSmith infrastructure and services:

* **Fully managed infrastructure**: LangChain handles all infrastructure, updates, scaling, and maintenance.
* **Deploy from GitHub**: Connect your repositories and deploy with a few clicks.
* **Automated CI/CD**: Build process is handled automatically by the platform.
* **LangSmith UI**: Full access to [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), [deployment management](/langsmith/deployments), and [Studio](/langsmith/studio).

|                                               | **Who manages it** | **Where it runs** |
| --------------------------------------------- | ------------------ | ----------------- |
| **LangSmith platform (UI, APIs, datastores)** | LangChain          | LangChain's cloud |
| **Your Agent Servers**                        | LangChain          | LangChain's cloud |
| **CI/CD for your apps**                       | LangChain          | LangChain's cloud |

<img src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3f0316122425895270d0ecd47b12e139" alt="Cloud deployment: LangChain hosts and manages all components including the UI, APIs, and your Agent Servers." data-og-width="1425" width="1425" data-og-height="1063" height="1063" data-path="langsmith/images/langgraph-cloud-architecture.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b34a10fb40ca5188dddfb6be69696c75 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d310ce86e8421878575cf4d4fa72bf78 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=7cc3fcee9c36a8d1e8da4cc4abbde3b9 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a184c950c8585e2fdb5e75ac7f0f5642 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=cb778e4a6707b00eaf703886d32569bd 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/langgraph-cloud-architecture.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=00f9d9df44512dded307613502d03299 2500w" />

To deploy your first application to Cloud, follow the [Cloud deployment quickstart](/langsmith/deployment-quickstart) or refer to the [comprehensive setup guide](/langsmith/deploy-to-cloud).

## Cloud architecture and scalability

<Note>
  This section is only relevant for the cloud-managed LangSmith services available at [https://smith.langchain.com](https://smith.langchain.com) and [https://eu.smith.langchain.com](https://eu.smith.langchain.com).

For information on the self-hosted LangSmith solution, please refer to the [self-hosted documentation](/langsmith/self-hosted).
</Note>

LangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. Many customers run production workloads on LangSmith for LLM application observability, evaluation, and agent deployment

The US-based LangSmith service is deployed in the `us-central1` (Iowa) region of GCP.

<Note>
  The [EU-based LangSmith service](https://eu.smith.langchain.com) is now available (as of mid-July 2024) and is deployed in the `europe-west4` (Netherlands) region of GCP. If you are interested in an enterprise plan in this region, [contact our sales team](https://www.langchain.com/contact-sales).
</Note>

#### Regional storage

The resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses [Supabase](https://supabase.com) for authentication/authorization and [ClickHouse Cloud](https://clickhouse.com/cloud) for data warehouse.

|                                                | US                                                                 | EU                                                                       |
| ---------------------------------------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------------ |
| URL                                            | [https://smith.langchain.com](https://smith.langchain.com)         | [https://eu.smith.langchain.com](https://eu.smith.langchain.com)         |
| API URL                                        | [https://api.smith.langchain.com](https://api.smith.langchain.com) | [https://eu.api.smith.langchain.com](https://eu.api.smith.langchain.com) |
| GCP                                            | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |
| Supabase                                       | AWS us-east-1 (N. Virginia)                                        | AWS eu-central-1 (Germany)                                               |
| ClickHouse Cloud                               | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |
| [LangSmith deployment](/langsmith/deployments) | us-central1 (Iowa)                                                 | europe-west4 (Netherlands)                                               |

See the [Regions FAQ](/langsmith/regions-faq) for more information.

#### Region-independent storage

Data listed here is stored exclusively in the US:

* Payment and billing information with Stripe and Metronome

LangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):

* LangSmith Frontend: serves the LangSmith UI.
* LangSmith Backend: serves the LangSmith API.
* LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)
* LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.
* LangSmith Queue: handles processing of asynchronous tasks. (Internal service)

LangSmith uses the following GCP storage services:

* Google Cloud Storage (GCS) for runs inputs and outputs.
* Google Cloud SQL PostgreSQL for transactional workloads.
* Google Cloud Memorystore for Redis for queuing and caching.
* Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.

Some additional GCP services we use include:

* Google Cloud Load Balancer for routing traffic to the LangSmith services.
* Google Cloud CDN for caching static assets.
* Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to [this guide](/langsmith/administration-overview#rate-limits).

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3" alt="Light mode overview" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b" alt="Dark mode overview" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w" />
</div>

### Allowlisting IP addresses

#### Egress from LangChain SaaS

All traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:

| US             | EU             |
| -------------- | -------------- |
| 34.59.65.97    | 34.13.192.67   |
| 34.67.51.221   | 34.147.105.64  |
| 34.46.212.37   | 34.90.22.166   |
| 34.132.150.88  | 34.147.36.213  |
| 35.188.222.201 | 34.32.137.113  |
| 34.58.194.127  | 34.91.238.184  |
| 34.59.97.173   | 35.204.101.241 |
| 104.198.162.55 | 35.204.48.32   |

It may be helpful to allowlist these IP addresses if connecting to your own AzureOpenAI service or other endpoints that may be required by the Playground or Online Evaluation.

#### Ingress into LangChain SaaS

The langchain endpoints map to the following static IP addresses:

| US             | EU           |
| -------------- | ------------ |
| 34.8.121.39    | 34.95.92.214 |
| 34.107.251.234 | 34.13.73.122 |

You may need to allowlist these to enable traffic from your private network to LangSmith SaaS endpoints (`api.smith.langchain.com`, `smith.langchain.com`, `beacon.langchain.com`, `eu.api.smith.langchain.com`, `eu.smith.langchain.com`, `eu.beacon.langchain.com`).

LangSmith enforces rate limits on API endpoints to ensure service stability and fair usage. The following table shows the rate limits for different endpoints in both US and EU regions. Note that:

* Rate limits are expressed as `count / interval` where count is the number of requests allowed within the interval (in seconds). For example, `2000 / 10` means 2000 requests per 10 seconds.
* When no HTTP method is specified in the endpoint column, the rate limit applies to all HTTP methods for that endpoint.
* When a specific method is listed (e.g., `POST`, `GET`), the rate limit applies only to that method.

| Match / Endpoint (method)                   | Identity key     | US prod limit | EU prod limit | Category                                     |
| ------------------------------------------- | ---------------- | ------------- | ------------- | -------------------------------------------- |
| OPTIONS, `/info`, `*/v1/metadata/submit`    | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |
| `/auth`                                     | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |
| `/auth`                                     | `x-user-id` + IP | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |
| `/v1/beacon`                                | IP               | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |
| `/repos`                                    | `x-api-key`      | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |
| `/repos`                                    | `x-user-id` + IP | 100 / 60      | 100 / 60      | [Repository](#rate-limit-categories)         |
| `POST /runs/batch`                          | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |
| `POST /otel/v1/traces`                      | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |
| `POST` containing `/charts`                 | `x-api-key`      | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |
| `POST` containing `/charts`                 | `x-user-id` + IP | 750 / 600     | 750 / 600     | [Charts](#rate-limit-categories)             |
| `POST /runs/multipart`                      | `x-api-key`      | 6000 / 10     | 6000 / 10     | [Multipart ingest](#rate-limit-categories)   |
| `POST /runs/query`                          | `x-api-key`      | 15 / 10       | 15 / 10       | [Run query (API)](#rate-limit-categories)    |
| `POST /runs/query`                          | `x-user-id` + IP | 300 / 10      | 300 / 10      | [Run query (User)](#rate-limit-categories)   |
| `/generate`                                 | `x-api-key`      | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |
| `/generate`                                 | `x-user-id` + IP | 30 / 3600     | 30 / 3600     | [Generation](#rate-limit-categories)         |
| `/commits`                                  | `x-api-key`      | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |
| `/commits`                                  | `x-user-id` + IP | 10000 / 60    | 2000 / 60     | [Commits](#rate-limit-categories)            |
| `DELETE /sessions` or `*/trigger`           | `x-api-key`      | 10 / 60       | 10 / 60       | [Deletion](#rate-limit-categories)           |
| `DELETE /sessions` or `*/trigger`           | `x-user-id` + IP | 30 / 60       | 30 / 60       | [Deletion](#rate-limit-categories)           |
| `POST /runs` (single run ingest)            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |
| `PATCH` containing `/runs`                  | `x-api-key`      | 2000 / 10     | 2000 / 10     | [Run ingest](#rate-limit-categories)         |
| `POST /feedback`                            | `x-api-key`      | 2000 / 10     | 2000 / 10     | [High throughput](#rate-limit-categories)    |
| `GET /runs/{uuid}` or `/api/v1/runs/{uuid}` | `x-api-key`      | 30 / 60       | 30 / 60       | [Run lookup](#rate-limit-categories)         |
| `GET` containing `/examples`                | `x-api-key`      | 5000 / 60     | 5000 / 60     | [Examples](#rate-limit-categories)           |
| Any request with `x-api-key`                | `x-api-key`      | 1000 / 10     | 1000 / 10     | [Default (API key)](#rate-limit-categories)  |
| Any request with `x-user-id`                | `x-user-id` + IP | 1000 / 10     | 1000 / 10     | [Default (User)](#rate-limit-categories)     |
| `/public/download`                          | IP               | 5000 / 60     | 5000 / 60     | [Public download](#rate-limit-categories)    |
| `/runs/stats`                               | `x-api-key`      | 1 / 10        | 20 / 10       | [Stats](#rate-limit-categories)              |
| All other IPs (catch-all)                   | IP               | 100 / 60      | 100 / 60      | [Public (catch-all)](#rate-limit-categories) |

#### Rate limit categories

* **High throughput**: General high-volume endpoints for core operations like authentication, metadata, and feedback.
* **Repository**: Repository and prompt management operations.
* **Run ingest**: Individual trace/run ingestion endpoints for observability.
* **Charts**: Chart generation and visualization endpoints.
* **Multipart ingest**: Bulk run ingestion via multipart upload for high-volume tracing.
* **Run query (API)**: API key-based run query operations with stricter limits for complex queries.
* **Run query (User)**: User-based run query operations with higher limits for interactive use.
* **Generation**: AI-powered code and content generation endpoints (limited to prevent abuse).
* **Commits**: Prompt versioning and commit operations.
* **Deletion**: Session deletion and workflow trigger operations.
* **Run lookup**: Retrieving specific runs by UUID.
* **Examples**: Fetching dataset examples for few-shot prompting.
* **Default (API key)**: Fallback rate limit for authenticated API requests not matching specific patterns.
* **Default (User)**: Fallback rate limit for authenticated user requests not matching specific patterns.
* **Public download**: High-volume public download endpoints for shared resources.
* **Stats**: Run statistics and analytics endpoints (region-specific limits apply).
* **Public (catch-all)**: Default rate limit for unauthenticated public access.

For more information on rate limits and other service limits, refer to the [Administration overview](/langsmith/administration-overview#rate-limits).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cloud.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Collect all tools from all step configurations

**URL:** llms-txt#collect-all-tools-from-all-step-configurations

all_tools = [
    record_warranty_status,
    record_issue_type,
    provide_solution,
    escalate_to_human,
]

---

## Collect results as they stream in

**URL:** llms-txt#collect-results-as-they-stream-in

aggregated_results = []
for result in streamed_results:
    aggregated_results.append(result)

---

## Combine input and output

**URL:** llms-txt#combine-input-and-output

class OverallState(InputState, OutputState):
    pass

---

## Combine waits for all parallel operations to complete

**URL:** llms-txt#combine-waits-for-all-parallel-operations-to-complete

workflow.add_edge("fetch_news", "combine_data")
workflow.add_edge("fetch_weather", "combine_data")
workflow.add_edge("fetch_stocks", "combine_data")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**4. Team development and documentation**

The visual nature of the Graph API makes it easier for teams to understand, document, and maintain complex workflows.
```

---

## Common OTEL settings

**URL:** llms-txt#common-otel-settings

OTEL_ATTRIBUTE_VALUE_LENGTH_LIMIT=4095
OTEL_EXPORTER_OTLP_COMPRESSION=gzip
OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE=delta

---

## Compare traces

**URL:** llms-txt#compare-traces

Source: https://docs.langchain.com/langsmith/compare-traces

To compare traces, click on the **Compare** button in the upper right hand side of any trace view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=794720bc9ef293984be25f19bffc89e7" alt="Compare button" data-og-width="2936" width="2936" data-og-height="1860" height="1860" data-path="langsmith/images/compare-button.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f64c79f89cf94802ed0676cb4d175be7 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=111db8eecaadd0458001072616ef95fa 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1df1b17af22d13e24a613bea71cb2e98 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=476560e917219525466629c49284de13 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=12cf8bf414cd5abd48355693f93d3bcf 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-button.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7242812503868254fde6688b27e3586f 2500w" />

This will show the trace run table. Select the trace you want to compare against the original trace.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=89c9c16780ac8d129736ee800124625a" alt="Select trace" data-og-width="2388" width="2388" data-og-height="1856" height="1856" data-path="langsmith/images/select-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1a7d4eb56fb4c2f8814b0584bce967b1 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=72c89c532151d35facb58cb62b825213 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=acd522e4aa3f54caaa3ef4036eb654b2 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aac705cc1574309b670d54e96bff666f 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=db82f5ea3c9ea13b3919f892db65bd9c 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-trace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b926b0a266e8945063a71a7496034e97 2500w" />

The pane will open with both traces selected in a side by side comparison view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0f3b4479d399d18c3d6d48fb91681f73" alt="Compare trace" data-og-width="2930" width="2930" data-og-height="1868" height="1868" data-path="langsmith/images/compare-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=05c151e7c9d0733ca6bfee6b6ce5ccc6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=579b34d6aa74caea5e055cba22f6fea8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9a06b98a43f3a195eed05292cacae2d2 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=968d1316e524123352412791515af047 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=29065b3b65b98841a47ebe6487a26974 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-trace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=dfe7e02a6db6693d4ff8490e170014bf 2500w" />

To stop comparing, close the pane or click on **Stop comparing** in the upper right hand side of the pane.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/compare-traces.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Compile

**URL:** llms-txt#compile

**Contents:**
  - 1. Run the graph

checkpointer = InMemorySaver()
graph = workflow.compile(checkpointer=checkpointer)
graph
python  theme={null}
config = {
    "configurable": {
        "thread_id": uuid.uuid4(),
    }
}
state = graph.invoke({}, config)

print(state["topic"])
print()
print(state["joke"])

How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!

**Examples:**

Example 1 (unknown):
```unknown
### 1. Run the graph
```

Example 2 (unknown):
```unknown
**Output:**
```

---

## Compile the graph with the checkpointer and store

**URL:** llms-txt#compile-the-graph-with-the-checkpointer-and-store

graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We invoke the graph with a `thread_id`, as before, and also with a `user_id`, which we'll use to namespace our memories to this particular user as we showed above.
```

---

## Compile the workflow

**URL:** llms-txt#compile-the-workflow

orchestrator_worker = orchestrator_worker_builder.compile()

---

## Component architecture

**URL:** llms-txt#component-architecture

**Contents:**
- Core component ecosystem
  - How components connect
- Component categories
- Common patterns
  - RAG (Retrieval-Augmented Generation)
  - Agent with tools
  - Multi-agent system
- Learn more

Source: https://docs.langchain.com/oss/python/langchain/component-architecture

LangChain's power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components.

## Core component ecosystem

The diagram below shows how LangChain's major components connect to form complete AI applications:

### How components connect

Each component layer builds on the previous ones:

1. **Input processing** – Transform raw data into structured documents
2. **Embedding & storage** – Convert text into searchable vector representations
3. **Retrieval** – Find relevant information based on user queries
4. **Generation** – Use AI models to create responses, optionally with tools
5. **Orchestration** – Coordinate everything through agents and memory systems

## Component categories

LangChain organizes components into these main categories:

| Category                                                             | Purpose                     | Key Components                      | Use Cases                                          |
| -------------------------------------------------------------------- | --------------------------- | ----------------------------------- | -------------------------------------------------- |
| **[Models](/oss/python/langchain/models)**                           | AI reasoning and generation | Chat models, LLMs, Embedding models | Text generation, reasoning, semantic understanding |
| **[Tools](/oss/python/langchain/tools)**                             | External capabilities       | APIs, databases, etc.               | Web search, data access, computations              |
| **[Agents](/oss/python/langchain/agents)**                           | Orchestration and reasoning | ReAct agents, tool calling agents   | Nondeterministic workflows, decision making        |
| **[Memory](/oss/python/langchain/short-term-memory)**                | Context preservation        | Message history, custom state       | Conversations, stateful interactions               |
| **[Retrievers](/oss/python/integrations/retrievers)**                | Information access          | Vector retrievers, web retrievers   | RAG, knowledge base search                         |
| **[Document processing](/oss/python/integrations/document_loaders)** | Data ingestion              | Loaders, splitters, transformers    | PDF processing, web scraping                       |
| **[Vector Stores](/oss/python/integrations/vectorstores)**           | Semantic search             | Chroma, Pinecone, FAISS             | Similarity search, embeddings storage              |

### RAG (Retrieval-Augmented Generation)

### Multi-agent system

Now that you understand how components relate to each other, explore specific areas:

* [Building your first RAG system](/oss/python/langchain/knowledge-base)
* [Creating agents](/oss/python/langchain/agents)
* [Working with tools](/oss/python/langchain/tools)
* [Setting up memory](/oss/python/langchain/short-term-memory)
* [Browse integrations](/oss/python/integrations/providers/overview)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/component-architecture.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### How components connect

Each component layer builds on the previous ones:

1. **Input processing** – Transform raw data into structured documents
2. **Embedding & storage** – Convert text into searchable vector representations
3. **Retrieval** – Find relevant information based on user queries
4. **Generation** – Use AI models to create responses, optionally with tools
5. **Orchestration** – Coordinate everything through agents and memory systems

## Component categories

LangChain organizes components into these main categories:

| Category                                                             | Purpose                     | Key Components                      | Use Cases                                          |
| -------------------------------------------------------------------- | --------------------------- | ----------------------------------- | -------------------------------------------------- |
| **[Models](/oss/python/langchain/models)**                           | AI reasoning and generation | Chat models, LLMs, Embedding models | Text generation, reasoning, semantic understanding |
| **[Tools](/oss/python/langchain/tools)**                             | External capabilities       | APIs, databases, etc.               | Web search, data access, computations              |
| **[Agents](/oss/python/langchain/agents)**                           | Orchestration and reasoning | ReAct agents, tool calling agents   | Nondeterministic workflows, decision making        |
| **[Memory](/oss/python/langchain/short-term-memory)**                | Context preservation        | Message history, custom state       | Conversations, stateful interactions               |
| **[Retrievers](/oss/python/integrations/retrievers)**                | Information access          | Vector retrievers, web retrievers   | RAG, knowledge base search                         |
| **[Document processing](/oss/python/integrations/document_loaders)** | Data ingestion              | Loaders, splitters, transformers    | PDF processing, web scraping                       |
| **[Vector Stores](/oss/python/integrations/vectorstores)**           | Semantic search             | Chroma, Pinecone, FAISS             | Similarity search, embeddings storage              |

## Common patterns

### RAG (Retrieval-Augmented Generation)
```

Example 2 (unknown):
```unknown
### Agent with tools
```

Example 3 (unknown):
```unknown
### Multi-agent system
```

---

## Composite evaluators

**URL:** llms-txt#composite-evaluators

**Contents:**
- Create a composite evaluator using the UI
  - 1. Navigate to the tracing project or dataset
  - 2. Configure the composite evaluator
  - 3. View composite evaluator results
- Create composite feedback with the SDK
  - 1. Configure evaluators on a dataset
  - 2. Create composite feedback

Source: https://docs.langchain.com/langsmith/composite-evaluators

*Composite evaluators* are a way to combine multiple evaluator scores into a single [score](/langsmith/evaluation-concepts#evaluator-outputs). This is useful when you want to evaluate multiple aspects of your application and combine the results into a single result.

## Create a composite evaluator using the UI

You can create composite evaluators on a [tracing project](/langsmith/observability-concepts#projects) (for [online evaluations](/langsmith/evaluation-concepts#online-evaluation)) or a [dataset](/langsmith/evaluation-concepts#datasets) (for [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation)). With composite evaluators in the UI, you can compute a weighted average or weighted sum of multiple evaluator scores, with configurable weights.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=b3859ada8b576ebeaf5399ff15359b10" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="756" width="756" data-og-height="594" height="594" data-path="langsmith/images/create_composite_evaluator-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=280&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=9bab5ad812328acdd6ffe858f487262b 280w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=560&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=4637a2dc732f945d98b0214023266180 560w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=840&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=c3e7b24dde21ed45f481b7a513ecc256 840w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=1100&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=1310a99e2a8b37d68d78f794b8ce6606 1100w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=1650&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=6beb89dcc6ec734b2ad012bc46c58821 1650w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-light.png?w=2500&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ba7fd7ba48a3e46d8701b6f64bb68f66 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ac13f4d2d4a5e3b67285284150b7d592" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="761" width="761" data-og-height="585" height="585" data-path="langsmith/images/create_composite_evaluator-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=280&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=bfc19d802f0327a579d90e519441cf9a 280w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=560&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=23ab26db75e25795c17abf07e487ba5d 560w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=840&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=7ce9597b62f3e68b2dc1afa5f17f0e8c 840w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=1100&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=ee7058d60185a820fe23decf003bd2c1 1100w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=1650&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=cff38ad541c55d6834edfa67f5650818 1650w, https://mintcdn.com/langchain-5e9cc07a/cRRwi1N4-QohYC73/langsmith/images/create_composite_evaluator-dark.png?w=2500&fit=max&auto=format&n=cRRwi1N4-QohYC73&q=85&s=0f85093799a489eff72dae01ed5b6d94 2500w" />
</div>

### 1. Navigate to the tracing project or dataset

To start configuring a composite evaluator, navigate to the **Tracing Projects** or **Dataset & Experiments** tab and select a project or dataset.

* From within a tracing project: **+ New** > **Evaluator** > **Composite score**
* From within a dataset: **+ Evaluator** > **Composite score**

### 2. Configure the composite evaluator

1. Name your evaluator.
2. Select an aggregation method, either **Average** or **Sum**.
   * **Average**: ∑(weight\*score) / ∑(weight).
   * **Sum**: ∑(weight\*score).
3. Add the feedback keys you want to include in the composite score.
4. Add the weights for the feedback keys. By default, the weights are equal for each feedback key. Adjust the weights to increase or decrease the importance of specific feedback keys in the final score.
5. Click **Create** to save the evaluator.

<Tip> If you need to adjust the weights for the composite scores, they can be updated after the evaluator is created. The resulting scores will be updated for all runs that have the evaluator configured. </Tip>

### 3. View composite evaluator results

Composite scores are attached to a run as **feedback**, similarly to feedback from a single evaluator. How you can view them depends on where the evaluation was run:

**On a tracing project**:

* Composite scores appear as feedback on runs.
* [Filter for runs](/langsmith/filter-traces-in-application) with a composite score, or where the composite score meets a certain threshold.
* [Create a chart](/langsmith/dashboards#custom-dashboards) to visualize trends in the composite score over time.

* View the composite scores in the experiments tab. You can also filter and sort experiments based on the average composite score of their runs.
* Click into an experiment to view the composite score for each run.

<Note> If any of the constituent evaluators are not configured on the run, the composite score will not be calculated for that run. </Note>

## Create composite feedback with the SDK

This guide describes setting up an evaluation that uses multiple evaluators and combines their scores with a custom aggregation function.

<Note> Requires langsmith>=0.4.29 </Note>

### 1. Configure evaluators on a dataset

Start by configuring your evaluators. In this example, the application generates a tweet from a blog introduction and uses three evaluators — summary, tone, and formatting — to assess the output.

If you already have your own dataset with evaluators configured, you can skip this step.

<Accordion title="Configure evaluators on a dataset.">
  
</Accordion>

### 2. Create composite feedback

Create composite feedback that aggregates the individual evaluator scores using your custom function. This example uses a weighted average of the individual evaluator scores.

<Accordion title="Create a composite feedback.">
  
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/composite-evaluators.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

### 2. Create composite feedback

Create composite feedback that aggregates the individual evaluator scores using your custom function. This example uses a weighted average of the individual evaluator scores.

<Accordion title="Create a composite feedback.">
```

---

## Conditional edge function to create llm_call workers that each write a section of the report

**URL:** llms-txt#conditional-edge-function-to-create-llm_call-workers-that-each-write-a-section-of-the-report

def assign_workers(state: State):
    """Assign a worker to each section in the plan"""

# Kick off section writing in parallel via Send() API
    return [Send("llm_call", {"section": s}) for s in state["sections"]]

---

## Configuration for this conversation thread

**URL:** llms-txt#configuration-for-this-conversation-thread

thread_id = str(uuid.uuid4())
config = {"configurable": {"thread_id": thread_id}}

---

## Configure API URL and key

**URL:** llms-txt#configure-api-url-and-key

---

## Configure custom TLS certificates

**URL:** llms-txt#configure-custom-tls-certificates

**Contents:**
- Mount internal CAs for TLS
- Use custom TLS certificates for model providers

Source: https://docs.langchain.com/langsmith/self-host-custom-tls-certificates

Use this guide to configure TLS in LangSmith. Start by mounting internal certificate authorities (CAs) so your deployment trusts the right roots system‑wide, for database or external service calls. You can then configure [Playground](/langsmith/prompt-engineering-concepts#prompt-playground)-specific mTLS for communicating securely with supported model providers.

* [Mounting internal certificate authorities](#mount-internal-cas-for-tls) (CAs) system-wide to enable TLS for database connections and Playground model calls
* Using Playground-specific TLS settings to provide client certs/keys for mTLS with supported model providers

## Mount internal CAs for TLS

<Note>
  You must use Helm chart version 0.11.9 or later to mount internal CAs using the configuration below.
</Note>

Use this approach to make internal/public CAs trusted system‑wide by LangSmith (Playground model calls and [database/external service connections](/langsmith/self-hosted#storage-services)).

1. Create a file containing all CAs required for TLS with databases and external services. If your deployment is communicating directly to `beacon.langchain.com` without a proxy, make sure to include a public trusted CA. All certs should be concatenated in this file with an empty line in between.
   
2. Create a Kubernetes secret with a key containing the contents of this file.
   
3. If using custom CA for TLS with your databases and other external services, provide the following values to your LangSmith helm chart:
   
4. Make sure to use TLS supported connection strings:
   * <b>Postgres</b>: Add `?sslmode=verify-full&sslrootcert=system` to the end.
   * <b>Redis</b>: Use `rediss://` instead of `redis://` as the prefix.

## Use custom TLS certificates for model providers

<Note>
  This feature is currently only available for the following model providers:

* Azure OpenAI
  * OpenAI
  * Custom (our custom model server). Refer to the [custom model server documentation](/langsmith/custom-endpoint) for more information.

These TLS settings apply to all invocations of the selected model providers (including Online Evaluation). Use them when the provider requires mutual TLS (client cert/key) or when you must override trust with a specific CA for provider calls. They complement the internal CA bundle configured above.
</Note>

You can use custom TLS certificates to connect to model providers in the LangSmith Playground. This is useful if you are using a self-signed certificate, a certificate from a custom certificate authority, or mutual TLS authentication.

To use custom TLS certificates, set the following environment variables. See the [self hosted deployment section](/langsmith/architectural-overview) for more information on how to configure application settings.

* \[Optional] `LANGSMITH_PLAYGROUND_TLS_KEY`: The private key in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.
* \[Optional] `LANGSMITH_PLAYGROUND_TLS_CERT`: The certificate in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication.
* \[Optional] `LANGSMITH_PLAYGROUND_TLS_CA`: The custom certificate authority (CA) certificate in PEM format. This must be a file path (for a mounted volume). Use this to mount CAs only if you're using a helm version below `0.11.9`; otherwise, use the [Mount internal CAs for TLS](./self-host-custom-tls-certificates#mount-internal-cas-for-tls) section above.

Once you have set these environment variables, enter the LangSmith Playground **Settings** page and select the **Provider** that requires custom TLS certificates. Set your model provider configuration as usual, and the custom TLS certificates will be used when connecting to the model provider.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-custom-tls-certificates.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
-----BEGIN CERTIFICATE-----
   <PUBLIC_CA>
   -----END CERTIFICATE-----

   -----BEGIN CERTIFICATE-----
   <INTERNAL_CA>
   -----END CERTIFICATE-----

   ...
```

Example 2 (unknown):
```unknown
3. If using custom CA for TLS with your databases and other external services, provide the following values to your LangSmith helm chart:
```

---

## Configure LangSmith for scale

**URL:** llms-txt#configure-langsmith-for-scale

**Contents:**
- Summary
- Trace ingestion (write path)
- Trace querying (read path)
- Example LangSmith configurations for scale
  - Low reads, low writes <a name="low-reads-low-writes" />
  - Low reads, high writes <a name="low-reads-high-writes" />

Source: https://docs.langchain.com/langsmith/self-host-scale

A self-hosted LangSmith instance can handle a large number of traces and users. The default configuration for the self-hosted deployment can handle substantial load, and you can configure your deployment to be able to achieve higher scale. This page describes scaling considerations and provides some examples to help configure your self-hosted instance.

For example configurations, refer to [Example LangSmith configurations for scale](#example-langsmith-configurations-for-scale).

The table below provides an overview comparing different LangSmith configurations for various load patterns (reads / writes):

|                                                                                                                                                                  | **[Low / low](#low-reads-low-writes)**              | **[Low / high](#low-reads-high-writes)**            | **[High / low](#high-reads-low-writes)**                                                                                                                                                                                                 | [Medium / medium](#medium-reads-medium-writes)      | [High / high](#high-reads-high-writes)                                                                                                                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <Tooltip tip="Number of users actively viewing traces on the frontend">Concurrent frontend users</Tooltip>                                                       | 5                                                   | 5                                                   | 50                                                                                                                                                                                                                                       | 20                                                  | 50                                                                                                                                                                                                                                       |
| <Tooltip tip="Number of traces being ingested via SDKs or API endpoints">Traces submitted per second</Tooltip>                                                   | 10                                                  | 1000                                                | 10                                                                                                                                                                                                                                       | 100                                                 | 1000                                                                                                                                                                                                                                     |
| **Frontend replicas**<br />(500m CPU, 1Gi per replica)                                                                                                           | 1 (default)                                         | 4                                                   | 2                                                                                                                                                                                                                                        | 2                                                   | 4                                                                                                                                                                                                                                        |
| **Platform backend replicas**<br />(1 CPU, 2Gi per replica)                                                                                                      | 3 (default)                                         | 20                                                  | 3 (default)                                                                                                                                                                                                                              | 3 (default)                                         | 20                                                                                                                                                                                                                                       |
| **Queue replicas**<br />(1 CPU, 2Gi per replica)                                                                                                                 | 3 (default)                                         | 160                                                 | 6                                                                                                                                                                                                                                        | 10                                                  | 160                                                                                                                                                                                                                                      |
| **Backend replicas**<br />(1 CPU, 2Gi per replica)                                                                                                               | 2 (default)                                         | 5                                                   | 40                                                                                                                                                                                                                                       | 16                                                  | 50                                                                                                                                                                                                                                       |
| **Redis resources**                                                                                                                                              | 8 Gi (default)                                      | 200 Gi external                                     | 8 Gi (default)                                                                                                                                                                                                                           | 13Gi external                                       | 200 Gi external                                                                                                                                                                                                                          |
| **ClickHouse resources**                                                                                                                                         | 4 CPU<br />16 Gi (default)                          | 10 CPU<br />32Gi memory                             | 8 CPU<br />16 Gi per replica                                                                                                                                                                                                             | 16 CPU<br />24Gi memory                             | 14 CPU<br />24 Gi per replica                                                                                                                                                                                                            |
| **ClickHouse setup**                                                                                                                                             | Single instance                                     | Single instance                                     | 3-node <Tooltip tip="Recommended for high read loads to prevent degraded performance. Another option would be [managed clickhouse](/langsmith/self-host-external-clickhouse#langsmith-managed-clickhouse).">replicated cluster</Tooltip> | Single instance                                     | 3-node <Tooltip tip="Recommended for high read loads to prevent degraded performance. Another option would be [managed clickhouse](/langsmith/self-host-external-clickhouse#langsmith-managed-clickhouse).">replicated cluster</Tooltip> |
| <Tooltip tip="We recommend using an external instance and enabling autoexpansion for the disk to handle growing data requirements.">Postgres resources</Tooltip> | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external)                                                                                                                                                                                      | 2 CPU<br />8 GB memory<br />10GB storage (external) | 2 CPU<br />8 GB memory<br />10GB storage (external)                                                                                                                                                                                      |
| **Blob storage**                                                                                                                                                 | Disabled                                            | Enabled                                             | Enabled                                                                                                                                                                                                                                  | Enabled                                             | Enabled                                                                                                                                                                                                                                  |

Below we go into more details about the read and write paths as well as provide a `values.yaml` snippet for you to start with for your self-hosted LangSmith instance.

## Trace ingestion (write path)

Common usage that put load on the write path:

* Ingesting traces via the Python or JavaScript LangSmith SDK
* Ingesting traces via the `@traceable` wrapper
* Submitting traces via the `/runs/multipart` endpoint

Services that play a large role in trace ingestion:

* Platform backend service: Receives initial request to ingest traces and places traces on a Redis queue
* Redis cache: Used to queue traces that need to be persisted
* Queue service: Persists traces for querying
* ClickHouse: Persistent storage used for traces

When scaling up the write path (trace ingestion), it is helpful to monitor the four services/resources listed above. Here are some typical changes that can help increase performance of trace ingestion:

* Give ClickHouse more resources (CPU and memory) if it is approaching resource limits.
* Increase the number of platform-backend pods if ingest requests are taking long to respond.
* Increase queue service pod replicas if traces are not being processed from Redis fast enough.
* Use a larger Redis cache if you notice that the current Redis instance is reaching resource limits. This could also be a reason why ingest requests take a long time.

## Trace querying (read path)

Common usage that puts load on the read path:

* Users on the frontend looking at tracing projects or individual traces
* Scripts used to query for trace info
* Hitting either the `/runs/query` or `/runs/<run-id>` api endpoints

Services that play a large role in querying traces:

* Backend service: Receives the request and submits a query to ClickHouse to then respond to the request
* ClickHouse: Persistent storage for traces. This is the main database that is queried when requesting trace info.

When scaling up the read path (trace querying), it is helpful to monitor the two services/resources listed above. Here are some typical changes that can help improve performance of trace querying:

* Increase the number of backend service pods. This would be most impactful if backend service pods are reaching 1 core CPU usage.
* Give ClickHouse more resources (CPU or Memory). ClickHouse can be very resource intensive, but it should lead to better performance.
* Move to a [replicated ClickHouse cluster](/langsmith/self-host-external-clickhouse#ha-replicated-clickhouse-cluster). Adding replicas of ClickHouse helps with read performance, but we recommend staying below 5 replicas (start with 3).

For more precise guidance on how this translates to helm chart values, refer to the examples the following [section](#example-langsmith-configurations-for-scale). If you are unsure why your LangSmith instance cannot handle a certain load pattern, contact the LangChain team.

## Example LangSmith configurations for scale

Below we provide some example LangSmith configurations based on expected read and write loads.

For read load (trace querying):

* Low means roughly 5 users looking at traces at a time (about 10 requests per second)
* Medium means roughly 20 users looking at traces at a time (about 40 requests per second)
* High means roughly 50 users looking at traces at a time (about 100 requests per second)

For write load (trace ingestion):

* Low means up to 10 traces submitted per second
* Medium means up to 100 traces submitted per second
* High means up to 1000 traces submitted per second

<Note>
  The exact optimal configuration depends on your usage and trace payloads. Use the examples below in combination with the information above and your specific usage to update your LangSmith configuration as you see fit. If you have any questions, please reach out to the LangChain team.
</Note>

### Low reads, low writes <a name="low-reads-low-writes" />

The default LangSmith configuration will handle this load. No custom resource configuration is needed here.

### Low reads, high writes <a name="low-reads-high-writes" />

You have a very high scale of trace ingestions, but single digit number of users on the frontend querying traces at any one time.

For this, we recommend a configuration like this:

```yaml  theme={null}
config:
  blobStorage:
    # Please also set the other keys to connect to your blob storage. See configuration section.
    enabled: true
  settings:
    redisRunsExpirySeconds: "3600"

---

## Configure LangSmith OpenTelemetry export (no OTEL env vars or headers needed)

**URL:** llms-txt#configure-langsmith-opentelemetry-export-(no-otel-env-vars-or-headers-needed)

**Contents:**
  - Add an attachment to a trace

configure(project_name="adk-otel-demo")

async def main():
    agent = LlmAgent(
        name="travel_assistant",
        model="gemini-2.5-flash-lite",
        instruction="You are a helpful travel assistant.",
    )

session_service = InMemorySessionService()
    runner = Runner(app_name="travel_app", agent=agent, session_service=session_service)

user_id = "user_123"
    session_id = "session_abc"
    await session_service.create_session(app_name="travel_app", user_id=user_id, session_id=session_id)

new_message = types.Content(parts=[types.Part(text="Hi! Recommend a weekend trip to Paris.")], role="user")

for event in runner.run(user_id=user_id, session_id=session_id, new_message=new_message):
        print(event)

if __name__ == "__main__":
    asyncio.run(main())
python  theme={null}
import asyncio
import base64
import json
from pathlib import Path
from dotenv import load_dotenv
from google.adk import Runner
from google.adk.agents import LlmAgent
from google.adk.sessions import InMemorySessionService
from google.genai import types
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider, SpanProcessor
from langsmith.integrations.otel import OtelSpanProcessor

class AttachmentSpanProcessor(SpanProcessor):
    """Custom SpanProcessor to add attachments to invocation spans."""

def __init__(self):
        self.attachment_data = None

def set_attachment(self, attachment_data):
        self.attachment_data = attachment_data

def on_end(self, span):
        if span.name == "invocation" and self.attachment_data:
            attachments_json = json.dumps([self.attachment_data])
            span._attributes["langsmith.attachments"] = attachments_json

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You do not need to set OTEL environment variables or exporters. `configure()` wires them for LangSmith automatically; instrumentors (like `GoogleADKInstrumentor`) create the spans.
</Note>

Here is an [example](https://smith.langchain.com/public/d6d47eeb-511e-4fda-ad17-2caa7bd7150b/r) of what the resulting trace looks like in LangSmith.

### Add an attachment to a trace

LangSmith supports [attaching files to traces](/langsmith/upload-files-with-traces). This is useful when building an agent with multimodal inputs or outputs. Attachments are also supported when tracing with OpenTelemetry.

The example below [traces a Google ADK agent](/langsmith/trace-with-google-adk) and adds an attachment to the trace. It uses a combination of LangSmith's `OtelSpanProcessor` and a custom `AttachmentSpanProcessor` that uses [`on_end()`](https://opentelemetry-python.readthedocs.io/en/latest/sdk/trace.export.html#opentelemetry.sdk.trace.export.SimpleSpanProcessor.on_end) to add an image attachment to the parent span.
```

---

## Configure LangSmith tracing

**URL:** llms-txt#configure-langsmith-tracing

configure(project_name="multi-framework-app")

---

## Configure OpenTelemetry

**URL:** llms-txt#configure-opentelemetry

tracer_provider = trace.get_tracer_provider()
if not isinstance(tracer_provider, TracerProvider):
    tracer_provider = TracerProvider()
    trace.set_tracer_provider(tracer_provider)

---

## Configure prompt settings

**URL:** llms-txt#configure-prompt-settings

**Contents:**
- Model configurations
  - Create saved configurations
  - Edit configurations
  - Delete configurations
  - Extra parameters
- Tool settings
- Prompt formatting

Source: https://docs.langchain.com/langsmith/managing-model-configurations

The LangSmith [playground](/langsmith/prompt-engineering-concepts#prompt-playground) enables you to control various settings for your prompts. The **Prompt Settings** window contains:

* [Model configuration](#model-configurations)
* [Tool settings](#tool-settings)
* [Prompt formatting](#prompt-formatting)

To access **Prompt Settings**:

1. Navigate to the **Playground** in the left sidebar.
2. Under the **Prompts** heading select the gear <Icon icon="gear" iconType="solid" /> icon next to the model name, which will launch the **Prompt Settings** window.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=6c0f7d7012b1e5295fe545149f955e6b" alt="Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc." data-og-width="886" width="886" data-og-height="689" height="689" data-path="langsmith/images/model-config-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=280&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=4e3b9ad92f6f14f4e0523bef50199318 280w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=560&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=e538eb740495a8afa8bfc552b13ae294 560w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=840&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=ebe73264e977153c869fd04d1552d09b 840w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1100&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=2eeb01882056046bc73cc019d674af7e 1100w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=1650&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=8f28fe2fe8054cf0623fb9d17f91966f 1650w, https://mintcdn.com/langchain-5e9cc07a/6r3GRtwWCl4ozaHW/langsmith/images/model-config-light.png?w=2500&fit=max&auto=format&n=6r3GRtwWCl4ozaHW&q=85&s=cf9ad39be3623e73322d123699e73f19 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=2e9da272c3fc8f7ac958c6e6d1da85e3" alt="Model Configuration window in the LangSmith UI, settings for Provider, Model, Temperature, Max Output Tokens, Top P, Presence Penalty, Frequency Penalty, Reasoning Effort, etc." data-og-width="881" width="881" data-og-height="732" height="732" data-path="langsmith/images/model-config-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=280&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=652fb75a4682cfc813743a1260764e59 280w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=560&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=02c980a8387f3d69a5870660b1668080 560w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=840&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ee633c06056fa7ad46ea58a179afa169 840w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1100&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=f62a35ed726b5f89c156a40c9ea76f2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=1650&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=18114575db8e6c7ce928763ddcb88c12 1650w, https://mintcdn.com/langchain-5e9cc07a/ppc8uxWc01j4q7Ia/langsmith/images/model-config-dark.png?w=2500&fit=max&auto=format&n=ppc8uxWc01j4q7Ia&q=85&s=ab24dc4975def52db55c4896ead5b77c 2500w" />
   </div>

## Model configurations

Model configurations define the parameters your prompt runs against. In the LangSmith Playground, you can save and manage these configurations, which allows you to reuse your preferred settings across prompts and sessions. For details on specific settings, refer to your model provider’s documentation (for example, [Anthropic](https://platform.claude.com/docs/en/api/messages), [OpenAI](https://platform.openai.com/docs/api-reference/responses/create)).

### Create saved configurations

1. In the **Model Configurations** tab, adjust the model configuration as needed—you can select a [saved configuration to edit](#edit-configurations).
2. Click the **Save As** button in the top bar.
3. Enter a name and optional description for your configuration and confirm.
4. Now that you've saved the configuration, anyone in your organization's [workspace](/langsmith/administration-overview#workspaces) can access it. All saved configurations are available in the **Model Configuration** dropdown.
5. Once you have created a saved configuration, you can set it as your default, so any new prompt you create will automatically use this configuration. To set a configuration as your default, click the **Set as default** <Icon icon="thumbtack" iconType="solid" /> icon next to the model name in the dropdown.

### Edit configurations

1. To rename a saved configuration, or update the description, select the configuration name or description and make the necessary changes.
2. Update the current configuration's parameters as needed and click the **Save** button at the top.

### Delete configurations

1. Select the configuration you want to remove.
2. Click the trash <Icon icon="trash" iconType="solid" /> icon to delete it.

The **Extra Parameters** field allows you to pass additional model parameters that aren't directly supported in the LangSmith interface. This is particularly useful in two scenarios:

1. When model providers release new parameters that haven't yet been integrated into the LangSmith interface. You can specify these parameters in JSON format to use them right away. For example:

2. When troubleshooting parameter-related errors in the playground, such as:

If you receive an error about unnecessary parameters (which is more common when using [LangChain JS](/oss/python/langchain/overview) for run tracing), you can use this field to remove the extra parameters.

[*Tools*](/langsmith/prompt-engineering-concepts#tools) enable your LLM to perform tasks like searching the web, looking up information, and so on. In the **Tools Settings** tab, you can manage the ways your LLM uses and accesses the tools you have defined in your prompt, including:

* **Parallel Tool Calls**: Calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously. (Dependent on model support for parallel execution.)
* **Tool Choice**: Select the tools that the model can access. For more details, refer to [Use tools in a prompt](/langsmith/use-tools).

The **Prompt Format** tab allows you to specify:

* The **Prompt type**. For details on chat and completion prompts, refer to [Prompt engineering](/langsmith/prompt-engineering-concepts#chat-vs-completion) concepts.
* The **Template format**. For details on prompt templating and using variables, refer to [F-string vs. mustache](/langsmith/prompt-engineering-concepts##f-string-vs-mustache).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/managing-model-configurations.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. When troubleshooting parameter-related errors in the playground, such as:
```

---

## Configure Semantic Kernel

**URL:** llms-txt#configure-semantic-kernel

kernel = Kernel()
kernel.add_service(OpenAIChatCompletion())

---

## Configure the OTLP exporter for your custom endpoint

**URL:** llms-txt#configure-the-otlp-exporter-for-your-custom-endpoint

provider = TracerProvider()
otlp_exporter = OTLPSpanExporter(
    # Change to your provider's endpoint
    endpoint="https://otel.your-provider.com/v1/traces",
    # Add any required headers for authentication
    headers={"api-key": "your-api-key"}
)
processor = BatchSpanProcessor(otlp_exporter)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

---

## Configure threads

**URL:** llms-txt#configure-threads

**Contents:**
- Group traces into threads
  - Example
- View threads
  - View a thread
  - View feedback
  - Save thread level filter

Source: https://docs.langchain.com/langsmith/threads

Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the `Threads` feature in LangSmith.

## Group traces into threads

A `Thread` is a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.

To associate traces together, you need to pass in a special `metadata` key where the value is the unique identifier for that thread. The key name should be one of:

* `session_id`
* `thread_id`
* `conversation_id`.

The value can be any string you want, but we recommend using UUIDs, such as `f47ac10b-58cc-4372-a567-0e02b2c3d479`. Check out [this guide](./add-metadata-tags) for instructions on adding metadata to your traces.

This example demonstrates how to log and retrieve conversation history using a structured message format to maintain long-running chats.

After waiting a few seconds, you can make the following calls to continue the conversation. By passing `get_chat_history=True,`/`getChatHistory: true`,
you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,
instead of just responding to the latest message.

Keep the conversation going. Since past messages are included, the LLM will remember the conversation.

You can view threads by clicking on the **Threads** tab in any project details page. You will then see a list of all threads, sorted by the most recent activity.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=45e1c11dce5eaaaf0cf8ae01057647b7" alt="LangSmith UI showing the threads table." data-og-width="1277" width="1277" data-og-height="762" height="762" data-path="langsmith/images/threads-tab-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cb5d147a58a3a9ecbb1c550a3308e871 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=a7cc6f9c8def1e15cc9c93382b82473d 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=23013f31edf91e66da89e102cb6ea302 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1bfbd6daad34b699553a30d8f9663540 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d3fc20ddbe02630ac98a0c336a39caa7 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfa17b6f006faef0a6f8537eedab6532 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=b0ec4964ee49a3ead3a1e8042e406abc" alt="LangSmith UI showing the threads table." data-og-width="1275" width="1275" data-og-height="761" height="761" data-path="langsmith/images/threads-tab-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=98750a8129e2e8283871c096a642f8b8 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=4be143312bdd90ab8318da304c0c1e91 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=c7ace4e000b6a1c925d32b78983fadca 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9ece1b058745be1c3b2dcfffd9a9af58 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=0ab63a1b906fe2a559cfb5772c95dc47 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/threads-tab-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=6093b81ee1f92978880e74a9630f451d 2500w" />
</div>

<Callout type="info" icon="bird">
  Use **[Polly](/langsmith/polly)** in thread views to analyze conversation threads, understand user sentiment, identify pain points, and track whether issues were resolved.
</Callout>

You can then click into a particular thread. This will open the history for a particular thread.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f7af4c3904073d5f58f28c656603ca19" alt="LangSmith UI showing the threads table." data-og-width="1273" width="1273" data-og-height="757" height="757" data-path="langsmith/images/thread-overview-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cd769088ab3ab2dae09982915f23772d 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=70ae6b5a6b8edb83ba3604d4c6e0262e 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=61d89d8077072221373490edac65363c 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=ad8159fe12f056dbc561c612e3797b97 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=3611f7bcc95c45bcb91c093ca36ef348 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1c0426d7e83562e1d76e079959bda186 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f738de4cac932ed2b8657e8f3b706b77" alt="LangSmith UI showing the threads table." data-og-width="1273" width="1273" data-og-height="753" height="753" data-path="langsmith/images/thread-overview-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9bc9dd49c63661dceb981899c5f0332b 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=01713d47cf762f99be1a1143b01582e8 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfc77e449d0ce27cdfa51b2f7c6ed655 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d84f671a8f2c1207dbb98c72a37d1832 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d592bd4c8671b3d7f19a49471888a901 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=2405eaef2af227dd5e5efac85fc9e623 2500w" />
</div>

Threads can be viewed in two different ways:

* [Thread overview](/langsmith/threads#thread-overview)
* [Trace view](/langsmith/threads#trace-view)

You can use the buttons at the top of the page to switch between the two views or use the keyboard shortcut `T` to toggle between the two views.

The thread overview page shows you a chatbot-like UI where you can see the inputs and outputs for each turn of the conversation. You can configure which fields in the inputs and outputs are displayed in the overview, or show multiple fields by clicking the **Configure** button.

The JSON path for the inputs and outputs supports negative indexing, so you can use `-1` to access the last element of an array. For example, `inputs.messages[-1].content` will access the last message in the `messages` array.

The trace view here is similar to the trace view when looking at a single run, except that you have easy access to all the runs for each turn in the thread.

When viewing a thread, across the top of the page you will see a section called `Feedback`. This is where you can see the feedback for each of the runs that make up the thread. This feedback is aggregated, so if you evaluate each run of a thread for the same criteria, you will see the average score across all the runs displayed. You can also see [thread level feedback](/langsmith/online-evaluations#configure-multi-turn-online-evaluators) left here.

### Save thread level filter

Similar to saving filters at the project level, you can also save commonly used filters at the thread level. To save filters on the threads table, set a filter using the filters button and then click the **Save filter** button.

You can open up the trace or annotate the trace in a side panel by clicking on `Annotate` and `Open trace`, respectively.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/threads.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

After waiting a few seconds, you can make the following calls to continue the conversation. By passing `get_chat_history=True,`/`getChatHistory: true`,
you can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,
instead of just responding to the latest message.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

Keep the conversation going. Since past messages are included, the LLM will remember the conversation.

<CodeGroup>
```

---

## Configure webhook notifications for LangSmith alerts

**URL:** llms-txt#configure-webhook-notifications-for-langsmith-alerts

**Contents:**
- Overview
- Prerequisites
- Integration Configuration
  - Step 1: Prepare Your Receiving Endpoint
  - Step 2: Configure Webhook Parameters
  - Step 3: Test the Webhook
- Troubleshooting
- Security Considerations
- Sending alerts to Slack using a webhook
  - Prerequisites

Source: https://docs.langchain.com/langsmith/alerts-webhook

This guide details the process for setting up webhook notifications for [LangSmith alerts](/langsmith/alerts). Before proceeding, make sure you have followed the steps leading up to the notification step of creating the alert by following [this guide](./alerts). Webhooks enable integration with custom services and third-party platforms by sending HTTP POST requests when alert conditions are triggered. Use webhooks to forward alert data to ticketing systems, chat applications, or custom monitoring solutions.

* An endpoint that can receive HTTP POST requests
* Appropriate authentication credentials for your receiving service (if required)

## Integration Configuration

### Step 1: Prepare Your Receiving Endpoint

Before configuring the webhook in LangSmith, ensure your receiving endpoint:

* Accepts HTTP POST requests
* Can process JSON payloads
* Is accessible from external services
* Has appropriate authentication mechanisms (if required)

Additionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.

### Step 2: Configure Webhook Parameters

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fecb6275ad3d576a864d1c6a2771c847" alt="Webhook Setup" data-og-width="754" width="754" data-og-height="523" height="523" data-path="langsmith/images/webhook-setup.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ef03d3ab887113e73dbdc1097076d103 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a25fcaedcbed92c9c3f2e2bddd8d88bd 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4785471ce1e58f3c48ce19b7be3889c5 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8c7dd40aeb5635cdf4ddf207d0dfe7c7 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aff125529b9db8fbf861999e70bcdb26 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-setup.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e9de7c4f0dcc440d734f4f3d09d2abf4 2500w" />

In the notification section of your alert complete the webhook configuration with the following parameters:

* **URL**: The complete URL of your receiving endpoint
  * Example: `https://api.example.com/incident-webhook`

* **Headers**: JSON Key-value pairs sent with the webhook request

* Common headers include:

* `Authorization`: For authentication tokens
    * `Content-Type`: Usually set to `application/json` (default)
    * `X-Source`: To identify the source as LangSmith

* If no headers, then simply use `{}`

* **Request Body Template**: Customize the JSON payload sent to your endpoint

* Default: LangSmith sends the payload defined and the following additonal key-value pairs appended to the payload:

* `project_name`: Name of the triggered alert
    * `alert_rule_id`: A UUID to identify the LangSmith alert. This can be used as a de-duplication key in the webhook service.
    * `alert_rule_name`: The name of the alert rule.
    * `alert_rule_type`: The type of alert (as of 04/01/2025 all alerts are of type `threshold`).
    * `alert_rule_attribute`: The attribute associated with the alert rule - `error_count`, `feedback_score` or `latency`.
    * `triggered_metric_value`: The value of the metric at the time the threshold was triggered.
    * `triggered_threshold`: The threshold that triggered the alert.
    * `timestamp`: The timestamp that triggered the alert.

### Step 3: Test the Webhook

Click **Send Test Alert** to send the webhook notification to ensure the notification works as intended.

If webhook notifications aren't being delivered:

* Verify the webhook URL is correct and accessible
* Ensure any authentication headers are properly formatted
* Check that your receiving endpoint accepts POST requests
* Examine your endpoint's logs for received but rejected requests
* Verify your custom payload template is valid JSON format

## Security Considerations

* Use HTTPS for your webhook endpoints
* Implement authentication for your webhook endpoint
* Consider adding a shared secret in your headers to verify webhook sources
* Validate incoming webhook requests before processing them

## Sending alerts to Slack using a webhook

Here is an example for configuring LangSmith alerts to send notifications to Slack channels using the [`chat.postMessage`](https://api.slack.com/methods/chat.postMessage) API.

* Access to a Slack workspace
* A LangSmith project to set up alerts
* Permissions to create Slack applications

### Step 1: Create a Slack App

1. Visit the [Slack API Applications page](https://api.slack.com/apps)
2. Click **Create New App**
3. Select **From scratch**
4. Provide an **App Name** (e.g., "LangSmith Alerts")
5. Select the workspace where you want to install the app
6. Click **Create App**

### Step 2: Configure Bot Permissions

1. In the left sidebar of your Slack app configuration, click **OAuth & Permissions**

2. Scroll down to **Bot Token Scopes** under **Scopes** and click **Add an OAuth Scope**

3. Add the following scopes:

* `chat:write` (Send messages as the app)
   * `chat:write.public` (Send messages to channels the app isn't in)
   * `channels:read` (View basic channel information)

### Step 3: Install the App to Your Workspace

1. Scroll up to the top of the **OAuth & Permissions** page
2. Click **Install to Workspace**
3. Review the permissions and click **Allow**
4. Copy the **Bot User OAuth Token** that appears (begins with `xoxb-`)

### Step 4: Add the Bot to a Slack Channel

Add the bot to the specific channel you want to receive alerts in. You can add a bot to a Slack channel by mentioning it in the message field (e.g., `@botname`).

You also need the channel ID to configure the webhook alert in LangSmith. You can find the channel ID by opening channel details > About

### Step 5: Configure the Webhook Alert in LangSmith

1. In LangSmith, navigate to your project
2. Select **Alerts → Create Alert**
3. Define your alert metrics and conditions
4. In the notification section, select **Webhook**
5. Configure the webhook with the following settings:

**Headers**
<Note>Replace `xoxb-your-token-here` with your Bot's User OAuth Token</Note>

**Request Body Template**
<Note>It is required to fill in the `{channel_id}` from the value found in Step 4. <br /><br />The remaining fields: `alert_name`, `project_name` and `project_url` optionally add additional context to the alert message. You can find your `project_url` in the browser's URL bar. Copy the portion up to but not including any query parameters.</Note>

6. Click **Save** to activate the webhook configuration

### Step 6: Test the Integration

1. In the LangSmith alert configuration, click **Test Alert**
2. Check your specified Slack channel for the test notification
3. Verify that the message contains the expected alert information

### (Optional) Step 7: Link to the Alert Preview in the Request Body

After creating an alert, you can optionally link to its preview in the webhook's request body.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=286ebb8f90bafbdcacf9a0602aaf749c" alt="Alert Preview Pane" data-og-width="832" width="832" data-og-height="773" height="773" data-path="langsmith/images/alert-preview-pane.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=20a409a30bff44a1a8bb1b79a6a2216b 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=414bb4719617bd23452273c73327d601 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6bc7bc7aaee65f7f4afac42102047ad2 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=491244ac56f6f4bcbb64419b267df0fe 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d47f5ba127c3f61e3cb7498f8b7568fe 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/alert-preview-pane.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a70706db839b2d211024116ba19acef 2500w" />

1. Save your alert
2. Find your saved alert in the alerts table and click it
3. Copy the displayed URL
4. Click "Edit Alert"
5. Replace the existing project URL with the copied alert preview URL

## Additional Resources

* [LangSmith Alerts Documentation](/langsmith/alerts)
* [Slack chat.postMessage API Documentation](https://api.slack.com/methods/chat.postMessage)
* [Slack Block Kit Builder](https://app.slack.com/block-kit-builder/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/alerts-webhook.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Headers**
<Note>Replace `xoxb-your-token-here` with your Bot's User OAuth Token</Note>
```

Example 2 (unknown):
```unknown
**Request Body Template**
<Note>It is required to fill in the `{channel_id}` from the value found in Step 4. <br /><br />The remaining fields: `alert_name`, `project_name` and `project_url` optionally add additional context to the alert message. You can find your `project_url` in the browser's URL bar. Copy the portion up to but not including any query parameters.</Note>
```

---

## Configure webhook notifications for rules

**URL:** llms-txt#configure-webhook-notifications-for-rules

**Contents:**
- Webhook payload
- Security
  - Webhook custom HTTP headers
  - Webhook Delivery
- Example with Modal
  - Setup
  - Secrets
  - Service

Source: https://docs.langchain.com/langsmith/webhooks

When you add a webhook URL on an automation action, we will make a POST request to your webhook endpoint any time the rules you defined match any new runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=da310e976aa8824071d65b8fb44b9123" alt="Webhook" data-og-width="872" width="872" data-og-height="991" height="991" data-path="langsmith/images/webhook.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6f30f7cd2de82b0ccb826d257b933f12 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0fce81ff2661e8944ebfb781a07017fe 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5799cf7458a15ac99579ba273d0b875e 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aa958b944f848f3a64ce068a64bc8433 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a3e94463ac9d8fa498accb27124785ab 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a5db18db5cf7097fdba6d6b7162ba6d 2500w" />

The payload we send to your webhook endpoint contains:

* `"rule_id"`: this is the ID of the automation that sent this payload
* `"start_time"` and `"end_time"`: these are the time boundaries where we found matching runs
* `"runs"`: this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.
* `"feedback_stats"`: this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below.

<Note>
  **fetching from S3 URLs**

Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.

The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.
</Note>

This is an example of the entire payload we send to your webhook endpoint:

We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.

### Webhook custom HTTP headers

If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.

<Note>
  Headers are stored in encrypted format.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8d6fde711d74784b803c13aba4b38837" alt="Webhook headers" data-og-width="848" width="848" data-og-height="1004" height="1004" data-path="langsmith/images/webhook-headers.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1d6c9f67920f0de5bc4b440593b87116 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a62d07be3f38e9c659faadb091f8a23e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5a6c128eb91f899f9213fd0f8999a11f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a9dd4e7f434f2f2806df60232f34ac3 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=315d0e900bd0c65dd5264869bd545351 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=31ce3f7628c5c3ba3916b77d906ff0c5 2500w" />

When delivering events to your webhook endpoint we follow these guidelines

* If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
* If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
* If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
* If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
* Anything your endpoint returns in the body will be ignored

## Example with Modal

For an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.

First, create a Modal account. Then, locally install the Modal SDK:

To finish setting up your account, run the command:

and follow the instructions

Next, you will need to set up some secrets in Modal.

First, LangSmith will need to authenticate to Modal by passing in a secret.
The easiest way to do this is to pass in a secret in the query parameters.
To validate this secret, we will need to add a secret in *Modal* to validate it.
We will do that by creating a Modal secret.
You can see instructions for secrets [here](https://modal.com/docs/guide/secrets).
For this purpose, let's call our secret `ls-webhook` and have it set an environment variable with the name `LS_WEBHOOK`.

We can also set up a LangSmith secret - luckily there is already an integration template for this!

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0c3209b59cb36273d82fb44383efa1d5" alt="LangSmith Modal Template" data-og-width="1229" width="1229" data-og-height="779" height="779" data-path="langsmith/images/modal-langsmith-secret.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c2ff70b647c04bb6a45a08de537b4d22 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=02181b882935f45339d31f48adeed1e9 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=52119f475cca739369cebb71bfefafae 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=aa8a1fc73b2e0b7f27c3186732e3bde9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=91d36950d22b86f7ad790a61957cbad7 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/modal-langsmith-secret.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=efcdfe7d54ca30079b147e00a0d9e934 2500w" />

After that, you can create a Python file that will serve as your endpoint.
An example is below, with comments explaining what is going on:

```python  theme={null}
from fastapi import HTTPException, status, Request, Query
from modal import Secret, Stub, web_endpoint, Image

stub = Stub("auth-example", image=Image.debian_slim().pip_install("langsmith"))

@stub.function(
    secrets=[Secret.from_name("ls-webhook"), Secret.from_name("my-langsmith-secret")]
)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  **fetching from S3 URLs**

  Depending on how recent your runs are, the `inputs_s3_urls` and `outputs_s3_urls` fields may contain S3 URLs to the actual data instead of the data itself.

  The `inputs` and `outputs` can be fetched by the `ROOT.presigned_url` provided in `inputs_s3_urls` and `outputs_s3_urls` respectively.
</Note>

This is an example of the entire payload we send to your webhook endpoint:
```

Example 2 (unknown):
```unknown
## Security

We strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications.

An example would be
```

Example 3 (unknown):
```unknown
### Webhook custom HTTP headers

If you'd like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on the `Headers` option next to the URL field and add your headers.

<Note>
  Headers are stored in encrypted format.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8d6fde711d74784b803c13aba4b38837" alt="Webhook headers" data-og-width="848" width="848" data-og-height="1004" height="1004" data-path="langsmith/images/webhook-headers.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1d6c9f67920f0de5bc4b440593b87116 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a62d07be3f38e9c659faadb091f8a23e 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5a6c128eb91f899f9213fd0f8999a11f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6a9dd4e7f434f2f2806df60232f34ac3 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=315d0e900bd0c65dd5264869bd545351 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/webhook-headers.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=31ce3f7628c5c3ba3916b77d906ff0c5 2500w" />

### Webhook Delivery

When delivering events to your webhook endpoint we follow these guidelines

* If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.
* If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .
* If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.
* If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.
* Anything your endpoint returns in the body will be ignored

## Example with Modal

### Setup

For an example of how to set this up, we will use [Modal](https://modal.com/). Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We'll focus on the web endpoints here.

First, create a Modal account. Then, locally install the Modal SDK:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configure your collector for LangSmith telemetry

**URL:** llms-txt#configure-your-collector-for-langsmith-telemetry

Source: https://docs.langchain.com/langsmith/langsmith-collector

The various services in a LangSmith deployment emit telemetry data in the form of logs, metrics, and traces. You may already have telemetry collectors set up in your Kubernetes cluster, or would like to deploy one to monitor your application.

This page describes how to configure an [OTel Collector](https://opentelemetry.io/docs/collector/configuration/) to gather telemetry data from LangSmith. Note that all of the concepts discussed below can be translated to other collectors such as [Fluentd](https://www.fluentd.org/) or [FluentBit](https://fluentbit.io/).

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

---

## Connect an authentication provider

**URL:** llms-txt#connect-an-authentication-provider

**Contents:**
- Background
- Prerequisites
- 1. Install dependencies
- 2. Set up the authentication provider
- 3. Implement token validation

Source: https://docs.langchain.com/langsmith/add-auth-server

In [the last tutorial](/langsmith/resource-auth), you added resource authorization to give users private conversations. However, you are still using hard-coded tokens for authentication, which is not secure. Now you'll replace those tokens with real user accounts using [OAuth2](/langsmith/deployment-quickstart).

You'll keep the same [`Auth`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) object and [resource-level access control](/langsmith/auth#single-owner-resources), but upgrade authentication to use Supabase as your identity provider. While Supabase is used in this tutorial, the concepts apply to any OAuth2 provider. You'll learn how to:

1. Replace test tokens with real JWT tokens
2. Integrate with OAuth2 providers for secure user authentication
3. Handle user sessions and metadata while maintaining our existing authorization logic

OAuth2 involves three main roles:

1. **Authorization server**: The identity provider (e.g., Supabase, Auth0, Google) that handles user authentication and issues tokens
2. **Application backend**: Your LangGraph application. This validates tokens and serves protected resources (conversation data)
3. **Client application**: The web or mobile app where users interact with your service

A standard OAuth2 flow works something like this:

Before you start this tutorial, ensure you have:

* The [bot from the second tutorial](/langsmith/resource-auth) running without errors.
* A [Supabase project](https://supabase.com/dashboard) to use its authentication server.

## 1. Install dependencies

Install the required dependencies. Start in your `custom-auth` directory and ensure you have the `langgraph-cli` installed:

<a id="setup-auth-provider" />

## 2. Set up the authentication provider

Next, fetch the URL of your auth server and the private key for authentication.
Since you're using Supabase for this, you can do this in the Supabase dashboard:

1. In the left sidebar, click on t️⚙ Project Settings" and then click "API"
2. Copy your project URL and add it to your `.env` file

3. Copy your service role secret key and add it to your `.env` file:

4. Copy your "anon public" key and note it down. This will be used later when you set up our client code.

## 3. Implement token validation

In the previous tutorials, you used the [`Auth`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) object to [validate hard-coded tokens](/langsmith/set-up-custom-auth) and [add resource ownership](/langsmith/resource-auth).

Now you'll upgrade your authentication to validate real JWT tokens from Supabase. The main changes will all be in the [`@auth.authenticate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) decorated function:

* Instead of checking against a hard-coded list of tokens, you'll make an HTTP request to Supabase to validate the token.
* You'll extract real user information (ID, email) from the validated token.
* The existing resource authorization logic remains unchanged.

Update `src/security/auth.py` to implement this:

```python {highlight={8-9,20-30}} title="src/security/auth.py" theme={null}
import os
import httpx
from langgraph_sdk import Auth

**Examples:**

Example 1 (unknown):
```unknown
## Prerequisites

Before you start this tutorial, ensure you have:

* The [bot from the second tutorial](/langsmith/resource-auth) running without errors.
* A [Supabase project](https://supabase.com/dashboard) to use its authentication server.

## 1. Install dependencies

Install the required dependencies. Start in your `custom-auth` directory and ensure you have the `langgraph-cli` installed:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

<a id="setup-auth-provider" />

## 2. Set up the authentication provider

Next, fetch the URL of your auth server and the private key for authentication.
Since you're using Supabase for this, you can do this in the Supabase dashboard:

1. In the left sidebar, click on t️⚙ Project Settings" and then click "API"
2. Copy your project URL and add it to your `.env` file
```

Example 4 (unknown):
```unknown
3. Copy your service role secret key and add it to your `.env` file:
```

---

## Connect nodes in a sequence

**URL:** llms-txt#connect-nodes-in-a-sequence

---

## Connect to an external ClickHouse database

**URL:** llms-txt#connect-to-an-external-clickhouse-database

**Contents:**
- Requirements
- HA Replicated Clickhouse Cluster
- LangSmith-managed ClickHouse
- Parameters
- Configuration
- TLS with ClickHouse
  - Server TLS (one-way)
  - Mutual TLS with client auth (mTLS)

Source: https://docs.langchain.com/langsmith/self-host-external-clickhouse

ClickHouse is a high-performance, column-oriented database system. It allows for fast ingestion of data and is optimized for analytical queries.

LangSmith uses ClickHouse as the primary data store for traces and feedback. By default, self-hosted LangSmith will use an internal ClickHouse database that is bundled with the LangSmith instance. This is run as a stateful set in the same Kubernetes cluster as the LangSmith application or as a Docker container on the same host as the LangSmith application.

However, you can configure LangSmith to use an external ClickHouse database for easier management and scaling. By configuring an external ClickHouse database, you can manage backups, scaling, and other operational tasks for your database. While Clickhouse is not yet a native service in Azure, AWS, or Google Cloud, you can run LangSmith with an external ClickHouse database in the following ways:

* [LangSmith-managed ClickHouse](/langsmith/langsmith-managed-clickhouse)

* Provision a [ClickHouse Cloud](https://clickhouse.cloud/) either directly or through a cloud provider marketplace:

* [Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/clickhouse.clickhouse_cloud?tab=Overview)
  * [Google Cloud Marketplace](https://console.cloud.google.com/marketplace/product/clickhouse-public/clickhouse-cloud)
  * [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-jettukeanwrfc)

* On a VM in your cloud provider

<Note>
  Using the first two options (LangSmith-managed ClickHouse or ClickHouse Cloud) will provision a Clickhouse service OUTSIDE of your VPC. However, both options support private endpoints, meaning that you can direct traffic to the ClickHouse service without exposing it to the public internet (eg via AWS PrivateLink, or GCP Private Service Connect).

Additionally, sensitive information can be configured to be not stored in Clickhouse. Please contact support via [support.langchain.com](https://support.langchain.com) for more information.
</Note>

* A provisioned ClickHouse instance that your LangSmith application will have network access to (see above for options).
* A user with admin access to the ClickHouse database. This user will be used to create the necessary tables, indexes, and views.
* We support both standalone ClickHouse and externally managed clustered deployments. For clustered deployments, ensure all nodes are running the same version. Note that clustered setups are not supported with bundled ClickHouse installations.
* We only support ClickHouse versions >= 23.9. Use of ClickHouse versions >= 24.2 requires LangSmith v0.6 or later.
* We rely on a few configuration parameters to be set on your ClickHouse instance. These are detailed below:

<Warning>
  Our system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.
</Warning>

## HA Replicated Clickhouse Cluster

<Warning>
  By default, the setup process above will only work with a single node Clickhouse cluster.
</Warning>

If you would like to use a multi-node Clickhouse cluster for HA, we support this with additional required configuration. This setup can use a Clickhouse cluster with multiple nodes where data replicated via Zookeeper or Clickhouse Keeper. For more information on Clickhouse replication, see [Clickhouse Data Replication Docs](https://clickhouse.com/docs/architecture/replication).

In order to setup LangSmith with a replicated multi-node Clickhouse setup:

* You need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See [Clickhouse Replication Setup Docs](https://clickhouse.com/docs/architecture/replication).
* You need to set the cluster setting in the [LangSmith Configuration](#configuration) section, specifically the `cluster` settings to match your Clickhouse Cluster name. This will use the `Replicated` table engines when running the Clickhouse migrations.
* If in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.
* **Note**: You will need to enable your `cluster` setting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as a `Replicated` table engine vs the non replicated engine type.

When running migrations with `cluster` enabled, the migration will create the `Replicated` table engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.

<Note>
  For an example setup of a replicated ClickHouse cluster, refer to the [replicated ClickHouse section](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/replicated-clickhouse/README.md) in the LangSmith Helm chart repo, under examples.
</Note>

## LangSmith-managed ClickHouse

* If using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please contact support via [support.langchain.com](https://support.langchain.com) for more information.
* You will also need to set up Blob Storage. You can read more about Blob Storage in the [Blob Storage documentation](/langsmith/self-host-blob-storage).

<Note>
  ClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.
</Note>

For more information, refer to the [managed ClickHouse](/langsmith/langsmith-managed-clickhouse) page.

You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:

* **Host**: The hostname or IP address of the ClickHouse database
* **HTTP Port**: The port that the ClickHouse database listens on for HTTP connections
* **Native Port**: The port that the ClickHouse database listens on for [native connections](https://clickhouse.com/docs/en/interfaces/tcp)
* **Database**: The name of the ClickHouse database that LangSmith should use
* **Username**: The username to use to connect to the ClickHouse database
* **Password**: The password to use to connect to the ClickHouse database
* **Cluster (Optional)**: The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.

<Warning>
  Important considerations for clustered deployments:

* Clustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.

* Clustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.

* When using a clustered deployment, LangSmith will automatically:

* Run database migrations across all nodes in the cluster
    * Configure tables for data replication across the cluster

Note that while data is replicated across nodes, LangSmith does not configure distributed tables or handle query routing - queries will be directed to the specified host. You will need to handle any load balancing or query distribution at the infrastructure level if desired.
</Warning>

With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external ClickHouse database.

## TLS with ClickHouse

Use this section to configure TLS for ClickHouse connections. For mounting internal/public CAs so LangSmith trusts your ClickHouse server certificate, see [Configure custom TLS certificates](/langsmith/self-host-custom-tls-certificates#mount-internal-cas-for-tls).

### Server TLS (one-way)

To enable TLS for ClickHouse connections:

* Set `tls: true` in your configuration (or use `tlsSecretKey` with an external secret).
* Use the appropriate TLS ports (typically `8443` for HTTP and `9440` for native TCP connections).
* Provide a CA bundle using `config.customCa.secretName` and `config.customCa.secretKey` if using an internal CA.

<Warning>
  Mount a custom CA only when your ClickHouse server uses an internal or private CA. Publicly trusted CAs do not require this configuration.
</Warning>

### Mutual TLS with client auth (mTLS)

As of LangSmith helm chart version **0.12.29**, we support mTLS for ClickHouse clients. For server-side authentication in mTLS, use the [Server TLS steps](#server-tls-one-way) (custom CA) in addition to the following client certificate configuration.

If your ClickHouse server requires client certificate authentication:

* Provide a Secret with your client certificate and key.
* Reference it via `clickhouse.external.clientCert.secretName` and specify the keys with `certSecretKey` and `keySecretKey`.

#### Non-TLS native port for migrations

<Warning>
  When using mTLS with ClickHouse, you must **keep a non-TLS native (TCP) port** open for our migrations job, which runs on helm install and upgrade. The application itself will not communicate through this port, it is **only used by the migration job**.
</Warning>

By default, the migration job connects to port `9000` for migrations. If your ClickHouse instance uses a different non-TLS native port, you can configure it using the `CLICKHOUSE_MIGRATE_NATIVE_PORT` environment variable:

#### Pod security context for certificate volumes

The certificate volumes mounted for mTLS are protected by file access restrictions. To ensure all LangSmith pods can read the certificate files, you must set `fsGroup: 1000` in the pod security context.

You can configure this in one of two ways:

**Option 1: Use `commonPodSecurityContext`**

Set the `fsGroup` at the top level to apply it to all pods:

**Option 2: Add to individual pod security contexts**

If you need more granular control, add the `fsGroup` to each pod's security context individually. See the [mTLS configuration example](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/mtls_config.yaml) for a complete reference.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-clickhouse.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  Our system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.
</Warning>

## HA Replicated Clickhouse Cluster

<Warning>
  By default, the setup process above will only work with a single node Clickhouse cluster.
</Warning>

If you would like to use a multi-node Clickhouse cluster for HA, we support this with additional required configuration. This setup can use a Clickhouse cluster with multiple nodes where data replicated via Zookeeper or Clickhouse Keeper. For more information on Clickhouse replication, see [Clickhouse Data Replication Docs](https://clickhouse.com/docs/architecture/replication).

In order to setup LangSmith with a replicated multi-node Clickhouse setup:

* You need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See [Clickhouse Replication Setup Docs](https://clickhouse.com/docs/architecture/replication).
* You need to set the cluster setting in the [LangSmith Configuration](#configuration) section, specifically the `cluster` settings to match your Clickhouse Cluster name. This will use the `Replicated` table engines when running the Clickhouse migrations.
* If in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.
* **Note**: You will need to enable your `cluster` setting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as a `Replicated` table engine vs the non replicated engine type.

When running migrations with `cluster` enabled, the migration will create the `Replicated` table engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.

<Note>
  For an example setup of a replicated ClickHouse cluster, refer to the [replicated ClickHouse section](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/replicated-clickhouse/README.md) in the LangSmith Helm chart repo, under examples.
</Note>

## LangSmith-managed ClickHouse

* If using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please contact support via [support.langchain.com](https://support.langchain.com) for more information.
* You will also need to set up Blob Storage. You can read more about Blob Storage in the [Blob Storage documentation](/langsmith/self-host-blob-storage).

<Note>
  ClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.
</Note>

For more information, refer to the [managed ClickHouse](/langsmith/langsmith-managed-clickhouse) page.

## Parameters

You will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:

* **Host**: The hostname or IP address of the ClickHouse database
* **HTTP Port**: The port that the ClickHouse database listens on for HTTP connections
* **Native Port**: The port that the ClickHouse database listens on for [native connections](https://clickhouse.com/docs/en/interfaces/tcp)
* **Database**: The name of the ClickHouse database that LangSmith should use
* **Username**: The username to use to connect to the ClickHouse database
* **Password**: The password to use to connect to the ClickHouse database
* **Cluster (Optional)**: The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.

<Warning>
  Important considerations for clustered deployments:

  * Clustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.

  * Clustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.

  * When using a clustered deployment, LangSmith will automatically:

    * Run database migrations across all nodes in the cluster
    * Configure tables for data replication across the cluster

  Note that while data is replicated across nodes, LangSmith does not configure distributed tables or handle query routing - queries will be directed to the specified host. You will need to handle any load balancing or query distribution at the infrastructure level if desired.
</Warning>

## Configuration

With these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying the `config.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external ClickHouse database.

## TLS with ClickHouse

Use this section to configure TLS for ClickHouse connections. For mounting internal/public CAs so LangSmith trusts your ClickHouse server certificate, see [Configure custom TLS certificates](/langsmith/self-host-custom-tls-certificates#mount-internal-cas-for-tls).

### Server TLS (one-way)

To enable TLS for ClickHouse connections:

* Set `tls: true` in your configuration (or use `tlsSecretKey` with an external secret).
* Use the appropriate TLS ports (typically `8443` for HTTP and `9440` for native TCP connections).
* Provide a CA bundle using `config.customCa.secretName` and `config.customCa.secretKey` if using an internal CA.

<Warning>
  Mount a custom CA only when your ClickHouse server uses an internal or private CA. Publicly trusted CAs do not require this configuration.
</Warning>

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Connect to an external PostgreSQL database

**URL:** llms-txt#connect-to-an-external-postgresql-database

**Contents:**
- Requirements
- Connection String
- Configuration
- TLS with PostgreSQL
  - Server TLS (one-way)
  - Mutual TLS with Client Auth (mTLS)

Source: https://docs.langchain.com/langsmith/self-host-external-postgres

LangSmith uses a PostgreSQL database as the primary data store for transactional workloads and operational data (almost everything besides runs). By default, LangSmith Self-Hosted will use an internal PostgreSQL database. However, you can configure LangSmith to use an external PostgreSQL database. By configuring an external PostgreSQL database, you can more easily manage backups, scaling, and other operational tasks for your database.

* A provisioned PostgreSQL database that your LangSmith instance will have network access to. We recommend using a managed PostgreSQL service like:

* [Amazon RDS](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_GettingStarted.CreatingConnecting.PostgreSQL.html)
  * [Google Cloud SQL](https://cloud.google.com/curated-resources/cloud-sql#section-1)
  * [Azure Database for PostgreSQL](https://azure.microsoft.com/en-us/products/postgresql#features)

* Note: We only officially support PostgreSQL versions >= 14.

* A user with admin access to the PostgreSQL database. This user will be used to create the necessary tables, indexes, and schemas.

* This user will also need to have the ability to create extensions in the database. We use/will try to install the `btree_gin`, `btree_gist`, `pgcrypto`, `citext`, `ltree`, and `pg_trgm` extensions.

* If using a schema other than public, ensure that you do not have any other schemas with the extensions enabled, or you must include that in your search path.

* Support for pgbouncer and other connection poolers is community-based. Community members have reported that pgbouncer has worked with `pool_mode` = `session` and a suitable setting for `ignore_startup_parameters` (as of writing, `search_path` and `lock_timeout` need to be ignored). Care is needed to avoid polluting connection pools; some level of PostgreSQL expertise is advisable. LangChain Inc currently does not have roadmap plans for formal test coverage or commercial support of pgbouncer or amazon rds proxy or any other poolers, but the community is welcome to discuss and collaborate on support through GitHub issues.

* By default, we recommend an instance with **at least 2 vCPUs and 8GB of memory**. However, the actual requirements will depend on your workload and the number of users you have. We recommend monitoring your PostgreSQL instance and scaling up as needed.

You will need to provide a connection string to your PostgreSQL database. This connection string should include the following information:

* Host
* Port
* Database
* Username
* Password (Make sure to url encode this if there are any special characters)
* URL params

This will take the form of:

An example connection string might look like:

Without url parameters, the connection string would look like:

With your connection string in hand, you can configure your LangSmith instance to use an external PostgreSQL database. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external PostgreSQL database.

## TLS with PostgreSQL

Use this section to configure TLS for PostgreSQL connections. For mounting internal/public CAs so LangSmith trusts your PostgreSQL server certificate, see [Configure custom TLS certificates](/langsmith/self-host-custom-tls-certificates#mount-internal-cas-for-tls).

### Server TLS (one-way)

To validate the PostgreSQL server certificate:

* Provide a CA bundle using `config.customCa.secretName` and `config.customCa.secretKey`.
* Use `sslmode=require` or `sslmode=verify-full`, as well as `sslrootcert=system` to your connection URL.

<Warning>
  Mount a custom CA only when your PostgreSQL server uses an internal or private CA. Publicly trusted CAs do not require this configuration.
</Warning>

### Mutual TLS with Client Auth (mTLS)

As of LangSmith helm chart version **0.12.29**, we support mTLS for PostgreSQL clients. For server-side authentication in mTLS, use the [Server TLS steps](#server-tls-one-way) (custom CA) in addition to the following client certificate configuration.

If your PostgreSQL server requires client certificate authentication:

* Provide a Secret with your client certificate and key.
* Reference it via `postgres.external.clientCert.secretName` and specify the keys with `certSecretKey` and `keySecretKey`.
* Use `sslmode=verify-full` and `sslrootcert=system` in your connection URL.

#### Pod security context for certificate volumes

The certificate volumes mounted for mTLS are protected by file access restrictions. To ensure all LangSmith pods can read the certificate files, you must set `fsGroup: 1000` in the pod security context.

You can configure this in one of two ways:

**Option 1: Use `commonPodSecurityContext`**

Set the `fsGroup` at the top level to apply it to all pods:

**Option 2: Add to individual pod security contexts**

If you need more granular control, add the `fsGroup` to each pod's security context individually. See the [mTLS configuration example](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/mtls_config.yaml) for a complete reference.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-postgres.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
username:password@host:port/database?<url_params>
```

Example 2 (unknown):
```unknown
myuser:mypassword@myhost:5432/mydatabase?sslmode=disable
```

Example 3 (unknown):
```unknown
myuser:mypassword@myhost:5432/mydatabase
```

Example 4 (unknown):
```unknown

```

---

## Connect to an external Redis database

**URL:** llms-txt#connect-to-an-external-redis-database

**Contents:**
- Requirements
- Standalone Redis
  - Connection String
  - Configuration
- Redis Cluster
  - Host Names
  - Configuration
- TLS with Redis
  - Server TLS (one-way)
  - Mutual TLS with Client Auth (mTLS)

Source: https://docs.langchain.com/langsmith/self-host-external-redis

LangSmith uses Redis to back our queuing/caching operations. By default, LangSmith Self-Hosted will use an internal Redis instance. However, you can configure LangSmith to use an external Redis instance. By configuring an external Redis instance, you can more easily manage backups, scaling, and other operational tasks for your Redis instance.

* A provisioned Redis instance that your LangSmith instance will have network access to. We recommend using a managed Redis service like:

* [Amazon ElastiCache](https://aws.amazon.com/elasticache/redis/)
  * [Google Cloud Memorystore](https://cloud.google.com/memorystore)
  * [Azure Cache for Redis](https://azure.microsoft.com/en-us/services/cache/)

* Note: We only officially support Redis versions >= 5.

* We support both Standalone and Redis Cluster. See the appropriate sections for deployment instructions.

* By default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your tracing workload. We recommend monitoring your Redis instance and scaling up as needed.

### Connection String

You will need to assemble the connection string for your Redis instance. This connection string should include the following information:

* Host
* Database
* Port
* URL params

This will take the form of:

An example connection string might look like:

Note: If your Standalone Redis requires authentication or TLS, include these directly in the connection URL:

* Use `rediss://` when TLS is enabled on your Redis server.
* Provide the password in the connection string.

With your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

You can also store the connection URL in an existing Kubernetes Secret and reference it in your Helm values.

Once configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external Redis instance.

As of LangSmith helm version **0.12.25**, we officially support **Redis Cluster**.

When using Redis Cluster, provide a list of node hostnames and ports. Each node URI must be in the form:

Do not include a password in these URIs, and do not use `rediss` here. For Redis Cluster:

* Provide the password separately via `redis.external.cluster.password` or through a Secret using `passwordSecretKey`.
* Enable TLS separately with `redis.external.cluster.tlsEnabled: true`.

When connecting to an external Redis Cluster, configure the Helm values under `redis.external.cluster`. You can either:

* Provide node URIs and (optionally) a password directly in `values.yaml`.
* Or reference an existing Kubernetes `Secret` containing node URIs and password.

If using an existing Secret, it should contain:

<CodeGroup>
  
</CodeGroup>

Use this section to configure TLS for Redis connections. For mounting internal/public CAs so LangSmith trusts your Redis server certificate, see [Configure custom TLS certificates](/langsmith/self-host-custom-tls-certificates#mount-internal-cas-for-tls).

### Server TLS (one-way)

To validate the Redis server certificate:

* Provide a CA bundle using `config.customCa.secretName` and `config.customCa.secretKey`.
* For Standalone Redis, use `rediss://` in the connection URL.
* For Redis Cluster, set `redis.external.cluster.tlsEnabled: true`.

<Warning>
  Mount a custom CA only when your Redis server uses an internal or private CA. Publicly trusted CAs do not require this configuration.
</Warning>

### Mutual TLS with Client Auth (mTLS)

As of LangSmith helm chart version **0.12.29**, we support mTLS for Redis clients. For server-side authentication in mTLS, use the [Server TLS steps](#server-tls-one-way) (custom CA) in addition to the following client certificate configuration.

If your Redis server requires client certificate authentication:

* Provide a Secret with your client certificate and key.
* Reference it via `redis.external.clientCert.secretName` and specify the keys with `certSecretKey` and `keySecretKey`.
* For Standalone Redis, keep using `rediss://` in the connection URL.
* For Redis Cluster, set `redis.external.cluster.tlsEnabled: true`.

#### Pod security context for certificate volumes

The certificate volumes mounted for mTLS are protected by file access restrictions. To ensure all LangSmith pods can read the certificate files, you must set `fsGroup: 1000` in the pod security context.

You can configure this in one of two ways:

**Option 1: Use `commonPodSecurityContext`**

Set the `fsGroup` at the top level to apply it to all pods:

**Option 2: Add to individual pod security contexts**

If you need more granular control, add the `fsGroup` to each pod's security context individually. See the [mtls configuration example](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/examples/mtls_config.yaml) for a complete reference.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-external-redis.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
"redis://host:port/db?<url_params>"
```

Example 2 (unknown):
```unknown
"redis://langsmith-redis:6379/0"
```

Example 3 (unknown):
```unknown
### Configuration

With your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying the `values` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Connect to an OpenAI compliant model provider/proxy

**URL:** llms-txt#connect-to-an-openai-compliant-model-provider/proxy

**Contents:**
- Deploy an OpenAI compliant model
- Use the model in the LangSmith Playground

Source: https://docs.langchain.com/langsmith/custom-openai-compliant-model

The LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for  in the playground.

## Deploy an OpenAI compliant model

Many providers offer OpenAI compliant models or proxy services. Some examples of this include:

* [LiteLLM Proxy](https://github.com/BerriAI/litellm?tab=readme-ov-file#quick-start-proxy---cli)
* [Ollama](https://ollama.com/)

You can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.

Take a look at the full [specification](https://platform.openai.com/docs/api-reference/chat) for more information.

## Use the model in the LangSmith Playground

Once you have deployed a model server, you can use it in the LangSmith [Playground](/langsmith/prompt-engineering-concepts#prompt-playground).

To access the **Prompt Settings** menu:

1. Under the **Prompts** heading select the gear <Icon icon="gear" iconType="solid" /> icon next to the model name.
2. In the **Model Configuration** tab, select the model to edit in the dropdown.
3. For the **Provider** dropdown, select **OpenAI Compatible Endpoint**.
4. Add your OpenAI Compatible Endpoint to the **Base URL** input.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=fdbe548e512ed40fb512578d02986b45" alt="Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected." data-og-width="897" width="897" data-og-height="572" height="572" data-path="langsmith/images/openai-compatible-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=75921e7b30edbac5263ee10178977383 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=468265c2ad0a6c1740eb18590dab27a5 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=9f9c3f68df77205264eb9221373790f2 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=ccb9820653e1e57b529bc46ac7d20e40 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c320b3f4051643b71fba4faa350daf9b 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=e9d1be7c69021b7fa575a2a466dbfe58 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=97459563da21d17228a1bb94a1b9edf3" alt="Model Configuration window in the LangSmith UI with a model selected and the Provider dropdown with OpenAI Compatible Endpoint selected." data-og-width="896" width="896" data-og-height="552" height="552" data-path="langsmith/images/openai-compatible-endpoint-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=280&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=c3e8e46813ec673fbc3ac4e4748a4ab6 280w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=560&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=381a567b022c71ed6f74abbb7e3cecbd 560w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=840&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=7df617e39d5bd098f4e80d523ef85778 840w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1100&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=06408cd348f56fcc409af8273c799a97 1100w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=1650&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=6b79b55e2868ed11e9d2c2eb396b004f 1650w, https://mintcdn.com/langchain-5e9cc07a/cemWY9w7h0W8uMbk/langsmith/images/openai-compatible-endpoint-dark.png?w=2500&fit=max&auto=format&n=cemWY9w7h0W8uMbk&q=85&s=4e0439d889aa2175c0802e9bf5db399b 2500w" />
   </div>

If everything is set up correctly, you should see the model's response in the playground. You can also use this functionality to invoke downstream pipelines as well.

For information on how to store your model configuration , refer to [Configure prompt settings](/langsmith/managing-model-configurations).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-openai-compliant-model.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Connect to a custom model

**URL:** llms-txt#connect-to-a-custom-model

**Contents:**
- Deploy a custom model server
- Adding configurable fields
- Use the model in the LangSmith Playground

Source: https://docs.langchain.com/langsmith/custom-endpoint

The LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model's API via [LangServe](https://github.com/langchain-ai/langserve), an open source library for serving LangChain applications. Behind the scenes, the playground will interact with your model server to generate responses.

## Deploy a custom model server

For your convenience, we have provided a sample model server that you can use as a reference. You can find the sample model server [here](https://github.com/langchain-ai/langsmith-model-server) We highly recommend using the sample model server as a starting point.

Depending on your model is an instruct-style or chat-style model, you will need to implement either `custom_model.py` or `custom_chat_model.py` respectively.

## Adding configurable fields

It is often useful to configure your model with different parameters. These might include temperature, model\_name, max\_tokens, etc.

To make your model configurable in the LangSmith playground, you need to add configurable fields to your model server. These fields can be used to change model parameters from the playground.

You can add configurable fields by implementing the `with_configurable_fields` function in the `config.py` file. You can

## Use the model in the LangSmith Playground

Once you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select either the `ChatCustomModel` or the `CustomModel` provider for chat-style model or instruct-style models.

Enter the `URL`. The playground will automatically detect the available endpoints and configurable fields. You can then invoke the model with the desired parameters.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7a2889af5f55cc73661033837a50fad6" alt="ChatCustomModel in Playground" data-og-width="2816" width="2816" data-og-height="1676" height="1676" data-path="langsmith/images/playground-custom-model.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c6509706fee0c85205e039f6868a5ead 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=deafe903353d9bec02143ebd578d5599 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=928818d42fc58d83e1b5a04ecaa36630 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=552046bb4c04947154a2c8fa3457beca 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2735d4eed015cafa0861079133c5220c 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-custom-model.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f59ef79d897acce3ae4835ce949d61b6 2500w" />

If everything is set up correctly, you should see the model's response in the playground as well as the configurable fields specified in the `with_configurable_fields`.

See how to store your model configuration for later use [here](/langsmith/managing-model-configurations).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-endpoint.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Connect turn tracker after task creation

**URL:** llms-txt#connect-turn-tracker-after-task-creation

if task.turn_tracking_observer:
    turn_audio_recorder.connect_to_turn_tracker(task.turn_tracking_observer)

---

## console.log(result);

**URL:** llms-txt#console.log(result);

**Contents:**
  - Error handling

/**
 * {
 *   messages: [
 *     ...
 *     { role: "tool", content: "Returning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'}", tool_call_id: "call_456", name: "MeetingAction" }
 *   ],
 *   structuredResponse: { task: "update the project timeline", assignee: "Sarah", priority: "high" }
 * }
 */
ts  theme={null}
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ContactInfo = z.object({
    name: z.string().describe("Person's name"),
    email: z.string().describe("Email address"),
});

const EventDetails = z.object({
    event_name: z.string().describe("Name of the event"),
    date: z.string().describe("Event date"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy([ContactInfo, EventDetails]),
});

const result = await agent.invoke({
    messages: [
        {
        role: "user",
        content:
            "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th",
        },
    ],
});

/**
 * {
 *   messages: [
 *     { role: "user", content: "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_1" }, { name: "EventDetails", args: { event_name: "Tech Conference", date: "March 15th" }, id: "call_2" } ] },
 *     { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_1", name: "ContactInfo" },
 *     { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_2", name: "EventDetails" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_3" } ] },
 *     { role: "tool", content: "Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}", tool_call_id: "call_3", name: "ContactInfo" }
 *   ],
 *   structuredResponse: { name: "John Doe", email: "john@email.com" }
 * }
 */
ts  theme={null}
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductRating = z.object({
    rating: z.number().min(1).max(5).describe("Rating from 1-5"),
    comment: z.string().describe("Review comment"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy(ProductRating),
});

const result = await agent.invoke({
    messages: [
        {
        role: "user",
        content: "Parse this: Amazing product, 10/10!",
        },
    ],
});

/**
 * {
 *   messages: [
 *     { role: "user", content: "Parse this: Amazing product, 10/10!" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ProductRating", args: { rating: 10, comment: "Amazing product" }, id: "call_1" } ] },
 *     { role: "tool", content: "Error: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating\nrating\n  Input should be less than or equal to 5 [type=less_than_equal, input_value=10, input_type=int].\n Please fix your mistakes.", tool_call_id: "call_1", name: "ProductRating" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ProductRating", args: { rating: 5, comment: "Amazing product" }, id: "call_2" } ] },
 *     { role: "tool", content: "Returning structured response: {'rating': 5, 'comment': 'Amazing product'}", tool_call_id: "call_2", name: "ProductRating" }
 *   ],
 *   structuredResponse: { rating: 5, comment: "Amazing product" }
 * }
 */
ts  theme={null}
const responseFormat = toolStrategy(ProductRating, {
    handleError: "Please provide a valid rating between 1-5 and include a comment."
)

// Error message becomes:
// { role: "tool", content: "Please provide a valid rating between 1-5 and include a comment." }
ts  theme={null}
import { ToolInputParsingException } from "@langchain/core/tools";

const responseFormat = toolStrategy(ProductRating, {
    handleError: (error: ToolStrategyError) => {
        if (error instanceof ToolInputParsingException) {
        return "Please provide a valid rating between 1-5 and include a comment.";
        }
        return error.message;
    }
)

// Only validation errors get retried with default message:
// { role: "tool", content: "Error: Failed to parse structured output for tool 'ProductRating': ...\n Please fix your mistakes." }
ts  theme={null}
const responseFormat = toolStrategy(ProductRating, {
    handleError: (error: ToolStrategyError) => {
        if (error instanceof ToolInputParsingException) {
        return "Please provide a valid rating between 1-5 and include a comment.";
        }
        if (error instanceof CustomUserError) {
        return "This is a custom user error.";
        }
        return error.message;
    }
)
ts  theme={null}
const responseFormat = toolStrategy(ProductRating, {
    handleError: false  // All errors raised
)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/structured-output.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Error handling

Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.

#### Multiple structured outputs error

When a model incorrectly calls multiple structured output tools, the agent provides error feedback in a [`ToolMessage`](https://reference.langchain.com/javascript/classes/_langchain_core.messages.ToolMessage.html) and prompts the model to retry:
```

Example 2 (unknown):
```unknown
#### Schema validation error

When structured output doesn't match the expected schema, the agent provides specific error feedback:
```

Example 3 (unknown):
```unknown
#### Error handling strategies

You can customize how errors are handled using the `handleErrors` parameter:

**Custom error message:**
```

Example 4 (unknown):
```unknown
**Handle specific exceptions only:**
```

---

## Continue conversation

**URL:** llms-txt#continue-conversation

**Contents:**
- Message content

messages = [
    HumanMessage("What's the weather in San Francisco?"),
    ai_message,  # Model's tool call
    tool_message,  # Tool execution result
]
response = model.invoke(messages)  # Model processes the result
python  theme={null}
    from langchain.messages import ToolMessage

# Sent to model
    message_content = "It was the best of times, it was the worst of times."

# Artifact available downstream
    artifact = {"document_id": "doc_123", "page": 0}

tool_message = ToolMessage(
        content=message_content,
        tool_call_id="call_123",
        name="search_books",
        artifact=artifact,
    )
    python  theme={null}
from langchain.messages import HumanMessage

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Attributes">
  <ParamField path="content" type="string" required>
    The stringified output of the tool call.
  </ParamField>

  <ParamField path="tool_call_id" type="string" required>
    The ID of the tool call that this message is responding to. Must match the ID of the tool call in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage).
  </ParamField>

  <ParamField path="name" type="string" required>
    The name of the tool that was called.
  </ParamField>

  <ParamField path="artifact" type="dict">
    Additional data not sent to the model but can be accessed programmatically.
  </ParamField>
</Accordion>

<Note>
  The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.

  <Accordion title="Example: Using artifact for retrieval metadata">
    For example, a [retrieval](/oss/python/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:
```

Example 2 (unknown):
```unknown
See the [RAG tutorial](/oss/python/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/python/langchain/agents) with LangChain.
  </Accordion>
</Note>

***

## Message content

You can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data.

Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below.

LangChain chat models accept message content in the `content` attribute.

This may contain either:

1. A string
2. A list of content blocks in a provider-native format
3. A list of [LangChain's standard content blocks](#standard-content-blocks)

See below for an example using [multimodal](#multimodal) inputs:
```

---

## Continue execution

**URL:** llms-txt#continue-execution

**Contents:**
  - Review tool calls
- Short-term memory
  - Manage checkpoints
  - Decouple return value from saved value
  - Chatbot example
- Long-term memory
- Workflows
- Integrate with other libraries

for event in graph.stream(Command(resume="baz"), config):
    print(event)
    print("\n")
python  theme={null}
from typing import Union

def review_tool_call(tool_call: ToolCall) -> Union[ToolCall, ToolMessage]:
    """Review a tool call, returning a validated version."""
    human_review = interrupt(
        {
            "question": "Is this correct?",
            "tool_call": tool_call,
        }
    )
    review_action = human_review["action"]
    review_data = human_review.get("data")
    if review_action == "continue":
        return tool_call
    elif review_action == "update":
        updated_tool_call = {**tool_call, **{"args": review_data}}
        return updated_tool_call
    elif review_action == "feedback":
        return ToolMessage(
            content=review_data, name=tool_call["name"], tool_call_id=tool_call["id"]
        )
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph.message import add_messages
from langgraph.types import Command, interrupt

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def agent(messages, previous):
    if previous is not None:
        messages = add_messages(previous, messages)

model_response = call_model(messages).result()
    while True:
        if not model_response.tool_calls:
            break

# Review tool calls
        tool_results = []
        tool_calls = []
        for i, tool_call in enumerate(model_response.tool_calls):
            review = review_tool_call(tool_call)
            if isinstance(review, ToolMessage):
                tool_results.append(review)
            else:  # is a validated tool call
                tool_calls.append(review)
                if review != tool_call:
                    model_response.tool_calls[i] = review  # update message

# Execute remaining tool calls
        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]
        remaining_tool_results = [fut.result() for fut in tool_result_futures]

# Append to message list
        messages = add_messages(
            messages,
            [model_response, *tool_results, *remaining_tool_results],
        )

# Call model again
        model_response = call_model(messages).result()

# Generate final response
    messages = add_messages(messages, model_response)
    return entrypoint.final(value=model_response, save=messages)
python  theme={null}
config = {
    "configurable": {
        "thread_id": "1",  # [!code highlight]
        # optionally provide an ID for a specific checkpoint,
        # otherwise the latest checkpoint is shown
        # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
}
graph.get_state(config)  # [!code highlight]

StateSnapshot(
    values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]}, next=(),
    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
    metadata={
        'source': 'loop',
        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
        'step': 4,
        'parents': {},
        'thread_id': '1'
    },
    created_at='2025-05-05T16:01:24.680462+00:00',
    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
    tasks=(),
    interrupts=()
)
python  theme={null}
config = {
    "configurable": {
        "thread_id": "1"  # [!code highlight]
    }
}
list(graph.get_state_history(config))  # [!code highlight]

[
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
        next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:24.680462+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        tasks=(),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")]},
        next=('call_model',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.863421+00:00',
        parent_config={...}
        tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
        next=('__start__',),
        config={...},
        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.863173+00:00',
        parent_config={...}
        tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "what's my name?"}]}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
        next=(),
        config={...},
        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:23.862295+00:00',
        parent_config={...}
        tasks=(),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob")]},
        next=('call_model',),
        config={...},
        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:22.278960+00:00',
        parent_config={...}
        tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),
        interrupts=()
    ),
    StateSnapshot(
        values={'messages': []},
        next=('__start__',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
        created_at='2025-05-05T16:01:22.277497+00:00',
        parent_config=None,
        tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}),),
        interrupts=()
    )
]
python  theme={null}
from langgraph.func import entrypoint
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def accumulate(n: int, *, previous: int | None) -> entrypoint.final[int, int]:
    previous = previous or 0
    total = previous + n
    # Return the *previous* value to the caller but save the *new* total to the checkpoint.
    return entrypoint.final(value=previous, save=total)

config = {"configurable": {"thread_id": "my-thread"}}

print(accumulate.invoke(1, config=config))  # 0
print(accumulate.invoke(2, config=config))  # 1
print(accumulate.invoke(3, config=config))  # 3
python  theme={null}
from langchain.messages import BaseMessage
from langgraph.graph import add_messages
from langgraph.func import entrypoint, task
from langgraph.checkpoint.memory import InMemorySaver
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")

@task
def call_model(messages: list[BaseMessage]):
    response = model.invoke(messages)
    return response

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):
    if previous:
        inputs = add_messages(previous, inputs)

response = call_model(inputs).result()
    return entrypoint.final(value=response, save=add_messages(inputs, response))

config = {"configurable": {"thread_id": "1"}}
input_message = {"role": "user", "content": "hi! I'm bob"}
for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()

input_message = {"role": "user", "content": "what's my name?"}
for chunk in workflow.stream([input_message], config, stream_mode="values"):
    chunk.pretty_print()
```

[long-term memory](/oss/python/concepts/memory#long-term-memory) allows storing information across different **thread ids**. This could be useful for learning information about a given user in one conversation and using it in another.

* [Workflows and agent](/oss/python/langgraph/workflows-agents) guide for more examples of how to build workflows using the Functional API.

## Integrate with other libraries

* [Add LangGraph's features to other frameworks using the functional API](/langsmith/deploy-other-frameworks): Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-functional-api.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
After resuming, the run proceeds through the remaining step and terminates as expected.

### Review tool calls

To review tool calls before execution, we add a `review_tool_call` function that calls [`interrupt`](/oss/python/langgraph/interrupts#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it.

Given a tool call, our function will [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) for human review. At that point we can either:

* Accept the tool call
* Revise the tool call and continue
* Generate a custom tool message (e.g., instructing the model to re-format its tool call)
```

Example 2 (unknown):
```unknown
We can now update our [entrypoint](/oss/python/langgraph/functional-api#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) supplied by the human. The results of prior tasks — in this case the initial model call — are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).
```

Example 3 (unknown):
```unknown
## Short-term memory

Short-term memory allows storing information across different **invocations** of the same **thread id**. See [short-term memory](/oss/python/langgraph/functional-api#short-term-memory) for more details.

### Manage checkpoints

You can view and delete the information stored by the checkpointer.

<a id="checkpoint" />

#### View thread state
```

Example 4 (unknown):
```unknown

```

---

## Contributing

**URL:** llms-txt#contributing

**Contents:**
- Ways to Contribute
- Acceptable uses of LLMs

Source: https://docs.langchain.com/oss/python/contributing/overview

**Welcome! Thank you for your interest in contributing.**

LangChain has helped form the largest developer community in generative AI, and we're always open to new contributors. Whether you're fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone 🦜❤️

## Ways to Contribute

<AccordionGroup>
  <Accordion title="Report bugs" icon="bug">
    Found a bug? Please help us fix it by following these steps:

<Steps>
      <Step title="Search">
        Check if the issue already exists in our GitHub Issues for the respective repo:

<Columns cols={2}>
          <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchain/issues">Issues</Card>
          <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraph/issues">Issues</Card>
        </Columns>
      </Step>

<Step title="Create issue">
        If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example). Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.
      </Step>

<Step title="Wait">
        A project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.
      </Step>
    </Steps>

If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please [link them](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) rather than combining them. For example,

<Accordion title="Suggest features" icon="wand-magic-sparkles">
    Have an idea for a new feature or enhancement?

<Steps>
      <Step title="Search">
        Search the issues for the respective repository for existing feature requests:

<Columns cols={2}>
          <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchain/issues?q=state%3Aopen%20label%3A%22feature%20request%22">Issues</Card>
          <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Aenhancement">Issues</Card>
        </Columns>
      </Step>

<Step title="Discuss">
        If no requests exist, start a new discussion under the [relevant category](https://forum.langchain.com/c/help/langchain/14) so that project maintainers and the community can provide feedback.
      </Step>

<Step title="Describe">
        Be sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.
      </Step>
    </Steps>
  </Accordion>

<Accordion title="Improve documentation" icon="book">
    Documentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference.

<Card title="How to propose changes to the documentation" href="/oss/python/contributing/documentation" arrow>Guide</Card>
  </Accordion>

<Accordion title="Contribute code" icon="code">
    With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help!

<Card title="How to make your first Pull Request" href="/oss/python/contributing/code" arrow>Guide</Card>

If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.

If you are looking for something to work on, check out the issues labeled "good first issue" or "help wanted" in our repos:

<Columns cols={2}>
      <Card title="LangChain" icon="link" href="https://github.com/langchain-ai/langchain/labels">Labels</Card>
      <Card title="LangGraph" icon="circle-nodes" href="https://github.com/langchain-ai/langgraph/labels">Labels</Card>
    </Columns>
  </Accordion>

<Accordion title="Add a new integration" icon="plug-circle-plus">
    <Card title="LangChain" icon="link" href="/oss/python/contributing/integrations-langchain" arrow>Guide to adding a new LangChain integration</Card>
  </Accordion>
</AccordionGroup>

## Acceptable uses of LLMs

Generative AI can be a useful tool for contributors, but like any tool should be used with critical thinking and good judgement.

We struggle when contributors' entire work (code changes, documentation update, pull request descriptions) are LLM-generated. These drive-by contributions often mean well but often miss the mark in terms of contextual relevance, accuracy, and quality.

We will close those pull requests and issues that are unproductive, so we can focus our maintainer capacity elsewhere.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Contributing integrations

**URL:** llms-txt#contributing-integrations

**Contents:**
- Why contribute an integration to LangChain?
- Components to integrate
- How to contribute an integration

Source: https://docs.langchain.com/oss/python/contributing/integrations-langchain

**Integrations are a core component of LangChain.**

LangChain provides standard interfaces for several different components (language models, vector stores, etc) that are crucial when building LLM applications. Contributing an integration helps expand LangChain's ecosystem and makes your service discoverable to millions of developers.

## Why contribute an integration to LangChain?

<Card title="Discoverability" icon="magnifying-glass">
  LangChain is the most used framework for building LLM applications, with over 20 million monthly downloads.
</Card>

<Card title="Interoperability" icon="arrows-rotate">
  LangChain components expose a standard interface, allowing developers to easily swap them for each other. If you implement a LangChain integration, any developer using a different component will easily be able to swap yours in.
</Card>

<Card title="Best Practices" icon="star">
  Through their standard interface, LangChain components encourage and facilitate best practices (streaming, async, etc.) that improve developer experience and application performance.
</Card>

## Components to integrate

While any component can be integrated into LangChain, there are specific types of integrations we encourage more:

**Integrate these ✅**:

* [**Chat Models**](/oss/python/integrations/chat): Most actively used component type
* [**Tools/Toolkits**](/oss/python/integrations/tools): Enable agent capabilities
* [**Retrievers**](/oss/python/integrations/retrievers): Core to RAG applications
* [**Embedding Models**](/oss/python/integrations/text_embedding): Foundation for vector operations
* [**Vector Stores**](/oss/python/integrations/vectorstores): Essential for semantic search

* **LLMs (Text-Completion Models)**: Deprecated in favor of [Chat Models](/oss/python/integrations/chat)
* [**Document Loaders**](/oss/python/integrations/document_loaders): High maintenance burden
* [**Key-Value Stores**](/oss/python/integrations/stores): Limited usage
* **Document Transformers**: Niche use cases
* **Model Caches**: Infrastructure concerns
* **Graphs**: Complex abstractions
* **Message Histories**: Storage abstractions
* **Callbacks**: System-level components
* **Chat Loaders**: Limited demand
* **Adapters**: Edge case utilities

## How to contribute an integration

<Steps>
  <Step title="Confirm eligibility">
    Verify that your integration is in the list of [encouraged components](#components-to-integrate) we are currently accepting.
  </Step>

<Step title="Implement your package">
    <Card title="How to implement a LangChain integration" icon="link" href="/oss/python/contributing/implement-langchain" arrow />
  </Step>

<Step title="Pass standard tests">
    If applicable, implement support for LangChain's [standard test](/oss/python/contributing/standard-tests-langchain) suite for your integration and successfully run them.
  </Step>

<Step title="Publish integration">
    <Card title="How to publish an integration" icon="upload" href="/oss/python/contributing/publish-langchain" arrow />
  </Step>

<Step title="Add documentation">
    Open a PR to add documentation for your integration to the official LangChain docs.

<Accordion title="Integration documentation guide" icon="book">
      An integration is only as useful as its documentation. To ensure a consistent experience for users, docs are required for all new integrations. We have a standard starting-point template for each type of integration for you to copy and modify.

In a new PR to the LangChain [docs repo](https://github.com/langchain-ai/docs), create a new file in the relevant directory under `src/oss/python/integrations/<component_type>/integration_name.mdx` using the appropriate template file:

* [Chat models](https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/chat/TEMPLATE.mdx)
      * [Tools and toolkits](https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/tools/TEMPLATE.mdx)
      * [Retrievers](https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/retrievers/TEMPLATE.mdx)
      * Text splitters - Coming soon
      * Embedding models - Coming soon
      * [Vector stores](https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/vectorstores/TEMPLATE.mdx)
      * Document loaders - Coming soon
      * Key-value stores - Coming soon

For reference docs, please open an issue on the repo so that a maintainer can add them.
    </Accordion>
  </Step>

<Step title="Co-marketing" icon="megaphone">
    (Optional) Engage with the LangChain team for joint [co-marketing](/oss/python/contributing/comarketing).
  </Step>
</Steps>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/integrations-langchain.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Contributing to code

**URL:** llms-txt#contributing-to-code

**Contents:**
- Getting started
  - Quick fix: submit a bugfix
  - Full development setup
- Contribution guidelines
  - Backwards compatibility
  - New features
  - Security guidelines
- Development environment
- Repository structure
- Development workflow

Source: https://docs.langchain.com/oss/python/contributing/code

Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers.

<Note>
  Before submitting large **new features or refactors**, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work.

This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.
</Note>

### Quick fix: submit a bugfix

For simple bugfixes, you can get started immediately:

<Steps>
  <Step title="Reproduce the issue">
    Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification
  </Step>

<Step title="Fork the repository">
    Fork the [LangChain](https://github.com/langchain-ai/langchain) or [LangGraph](https://github.com/langchain-ai/langgraph) repo to your <Tooltip tip="If you fork to an organization account, maintainers will be unable to make edits.">personal GitHub account</Tooltip>
  </Step>

<Step title="Clone and setup">

You will need to install [`uv`](https://docs.astral.sh/uv/) if you haven't previously
  </Step>

<Step title="Create a branch">
    Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.

<Step title="Write failing tests">
    Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions
  </Step>

<Step title="Make your changes">
    Fix the bug while following our [code quality standards](#code-quality-standards). Make the **minimal change necessary** to resolve the issue. We strongly encourage contributors to comment on the issue before they start coding. For example:

*"I'd like to work on this. My intended approach would be to \[...brief description...]. Does this align with maintainer expectations?"*

A 30-second comment often prevents wasted effort if your initial approach is wrong.
  </Step>

<Step title="Verify the fix">
    Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR

<Step title="Document the change">
    Update docstrings if behavior changes, add comments for complex logic
  </Step>

<Step title="Submit a pull request">
    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.
  </Step>
</Steps>

### Full development setup

For ongoing development or larger contributions:

1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations
2. Set up your environment following our [setup guide](#development-environment) below
3. Understand the [repository structure](#repository-structure) and package organization
4. Learn our [development workflow](#development-workflow) including testing and linting

## Contribution guidelines

Before you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a "first contribution" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.

Contributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well.

### Backwards compatibility

<Warning>
  Breaking changes to public APIs are not allowed except for critical security fixes.

See our [versioning policy](/oss/python/versioning) for details on major version releases.
</Warning>

Maintain compatibility via:

<AccordionGroup>
  <Accordion title="Stable interfaces">
    **Always preserve**:

* Function signatures and parameter names
    * Class interfaces and method names
    * Return value structure and types
    * Import paths for public APIs
  </Accordion>

<Accordion title="Safe changes">
    **Acceptable modifications**:

* Adding new optional parameters

* Adding new methods to classes

* Improving performance without changing behavior

* Adding new modules or functions
  </Accordion>

<Accordion title="Before making changes">
    * **Would this break existing user code?**

* Check if your target is public

* If needed, is it exported in `__init__.py`?

* Are there existing usage patterns in tests?
  </Accordion>
</AccordionGroup>

We aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.

In general, feature contribution requirements include:

<Steps>
  <Step title="Design discussion">
    Open an issue describing:

* The problem you're solving
    * Proposed API design
    * Expected usage patterns
  </Step>

<Step title="Implementation">
    * Follow existing code patterns
    * Include comprehensive tests and documentation
    * Consider security implications
  </Step>

<Step title="Integration considerations">
    * How does this interact with existing features?
    * Are there performance implications?
    * Does this introduce new dependencies?

We will reject features that are likely to lead to security vulnerabilities or reports.
  </Step>
</Steps>

### Security guidelines

<Warning>
  Security is paramount. Never introduce vulnerabilities or unsafe patterns.
</Warning>

<AccordionGroup>
  <Accordion title="Input validation">
    * Validate and sanitize all user inputs
    * Properly escape data in templates and queries
    * Never use `eval()`, `exec()`, or `pickle` on user data, as this can lead to arbitrary code execution vulnerabilities
  </Accordion>

<Accordion title="Error handling">
    * Use specific exception types
    * Don't expose sensitive information in error messages
    * Implement proper resource cleanup
  </Accordion>

<Accordion title="Dependencies">
    * Avoid adding hard dependencies
    * Keep optional dependencies minimal
    * Review third-party packages for security issues
  </Accordion>
</AccordionGroup>

## Development environment

<Warning>
  Our Python projects use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management. Make sure you have the latest version installed.
</Warning>

Once you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.

<Tabs>
  <Tab title="LangChain" icon="link">
    <AccordionGroup>
      <Accordion title="Core abstractions">
        For changes to `langchain-core`:

<Accordion title="Main package">
        For changes to `langchain`:

<Accordion title="Partner packages">
        For changes to [partner integrations](/oss/python/integrations/providers/overview):

<Accordion title="Community packages">
        For changes to community integrations (located in a [separate repo](https://github.com/langchain-ai/langchain-community)):

</Accordion>
    </AccordionGroup>
  </Tab>

<Tab title="LangGraph" icon="circle-nodes">
    WIP - coming soon! In the meantime, follow instructions for LangChain.
  </Tab>
</Tabs>

## Repository structure

<Tabs>
  <Tab title="LangChain" icon="link">
    LangChain is organized as a monorepo with multiple packages:

<AccordionGroup>
      <Accordion title="Core packages" defaultOpen>
        * **[`langchain`](https://github.com/langchain-ai/langchain/tree/master/libs/langchain#readme)** (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic
        * **[`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core#readme)** (located in `libs/core/`): Base interfaces and core abstractions
      </Accordion>

<Accordion title="Partner packages">
        Located in `libs/partners/`, these are independently versioned packages for specific integrations. For example:

* **[`langchain-openai`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai#readme)**: [OpenAI](/oss/python/integrations/providers/openai) integrations
        * **[`langchain-anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic#readme)**: [Anthropic](/oss/python/integrations/providers/anthropic) integrations
        * **[`langchain-google-genai`](https://github.com/langchain-ai/langchain-google/)**: [Google Generative AI](/oss/python/integrations/chat/google_generative_ai) integrations

Many partner packages are in external repositories. Please check the [list of integrations](/oss/python/integrations/providers/overview) for details.
      </Accordion>

<Accordion title="Supporting packages">
        * **[`langchain-text-splitters`](https://github.com/langchain-ai/langchain/tree/master/libs/text-splitters#readme)**: Text splitting utilities
        * **[`langchain-standard-tests`](https://github.com/langchain-ai/langchain/tree/master/libs/standard-tests#readme)**: Standard test suites for integrations
        * **[`langchain-cli`](https://github.com/langchain-ai/langchain/tree/master/libs/cli#readme)**: Command line interface
        * **[`langchain-community`](https://github.com/langchain-ai/langchain-community)**: Community maintained integrations (located in a separate repo)
      </Accordion>
    </AccordionGroup>
  </Tab>

<Tab title="LangGraph" icon="circle-nodes">
    WIP - coming soon! In the meantime, follow instructions for LangChain.
  </Tab>
</Tabs>

## Development workflow

### Testing requirements

<Info>
  Directories are relative to the package you're working in.
</Info>

Every code change must include comprehensive tests.

**Location**: `tests/unit_tests/`

* No network calls allowed
* Test all code paths including edge cases
* Use mocks for external dependencies

```bash  theme={null}
make test

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
You will need to install [`uv`](https://docs.astral.sh/uv/) if you haven't previously
  </Step>

  <Step title="Create a branch">
    Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later.
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Write failing tests">
    Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions
  </Step>

  <Step title="Make your changes">
    Fix the bug while following our [code quality standards](#code-quality-standards). Make the **minimal change necessary** to resolve the issue. We strongly encourage contributors to comment on the issue before they start coding. For example:

    *"I'd like to work on this. My intended approach would be to \[...brief description...]. Does this align with maintainer expectations?"*

    A 30-second comment often prevents wasted effort if your initial approach is wrong.
  </Step>

  <Step title="Verify the fix">
    Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Document the change">
    Update docstrings if behavior changes, add comments for complex logic
  </Step>

  <Step title="Submit a pull request">
    Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE_NUMBER`) so that the issue is automatically closed when your PR is merged.
  </Step>
</Steps>

### Full development setup

For ongoing development or larger contributions:

1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations
2. Set up your environment following our [setup guide](#development-environment) below
3. Understand the [repository structure](#repository-structure) and package organization
4. Learn our [development workflow](#development-workflow) including testing and linting

***

## Contribution guidelines

Before you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a "first contribution" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial.

Contributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well.

### Backwards compatibility

<Warning>
  Breaking changes to public APIs are not allowed except for critical security fixes.

  See our [versioning policy](/oss/python/versioning) for details on major version releases.
</Warning>

Maintain compatibility via:

<AccordionGroup>
  <Accordion title="Stable interfaces">
    **Always preserve**:

    * Function signatures and parameter names
    * Class interfaces and method names
    * Return value structure and types
    * Import paths for public APIs
  </Accordion>

  <Accordion title="Safe changes">
    **Acceptable modifications**:

    * Adding new optional parameters

    * Adding new methods to classes

    * Improving performance without changing behavior

    * Adding new modules or functions
  </Accordion>

  <Accordion title="Before making changes">
    * **Would this break existing user code?**

    * Check if your target is public

    * If needed, is it exported in `__init__.py`?

    * Are there existing usage patterns in tests?
  </Accordion>
</AccordionGroup>

### New features

We aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies.

In general, feature contribution requirements include:

<Steps>
  <Step title="Design discussion">
    Open an issue describing:

    * The problem you're solving
    * Proposed API design
    * Expected usage patterns
  </Step>

  <Step title="Implementation">
    * Follow existing code patterns
    * Include comprehensive tests and documentation
    * Consider security implications
  </Step>

  <Step title="Integration considerations">
    * How does this interact with existing features?
    * Are there performance implications?
    * Does this introduce new dependencies?

    We will reject features that are likely to lead to security vulnerabilities or reports.
  </Step>
</Steps>

### Security guidelines

<Warning>
  Security is paramount. Never introduce vulnerabilities or unsafe patterns.
</Warning>

Security checklist:

<AccordionGroup>
  <Accordion title="Input validation">
    * Validate and sanitize all user inputs
    * Properly escape data in templates and queries
    * Never use `eval()`, `exec()`, or `pickle` on user data, as this can lead to arbitrary code execution vulnerabilities
  </Accordion>

  <Accordion title="Error handling">
    * Use specific exception types
    * Don't expose sensitive information in error messages
    * Implement proper resource cleanup
  </Accordion>

  <Accordion title="Dependencies">
    * Avoid adding hard dependencies
    * Keep optional dependencies minimal
    * Review third-party packages for security issues
  </Accordion>
</AccordionGroup>

***

## Development environment

<Warning>
  Our Python projects use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management. Make sure you have the latest version installed.
</Warning>

Once you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.

<Tabs>
  <Tab title="LangChain" icon="link">
    <AccordionGroup>
      <Accordion title="Core abstractions">
        For changes to `langchain-core`:
```

---

## Contributing to documentation

**URL:** llms-txt#contributing-to-documentation

**Contents:**
- Contribute
  - Quick edits
  - Larger edits and additions

Source: https://docs.langchain.com/oss/python/contributing/documentation

Accessible documentation is a vital part of LangChain. We welcome both documentation for new features and [integrations](/oss/python/contributing/publish-langchain#adding-documentation), as well as community improvements to existing docs.

<Note>
  These are contribution guidelines for our open source projects, but they also apply to the [LangSmith documentation](/langsmith/home).
</Note>

For quick changes like fixing typos or changing a link, you can edit directly on GitHub without setting up a local development environment:

<Info>
  **Prerequisites:**

* A [GitHub](https://github.com/) account
  * Basic familiarity of the [fork-and-pull workflow](https://graphite.dev/guides/understanding-git-fork-pull-request-workflow) for contributing
</Info>

1. At the bottom of the page you want to edit, click the link **Edit this page on GitHub**.
2. GitHub will prompt you to fork the repository to your account. Make sure to fork into your <Tooltip tip="If you clone to an organization, maintainers are unable to make edits, which may delay acceptance.">personal account</Tooltip>.
3. Make the changes directly in GitHub's web editor.
4. Click **Commit changes...** and give your commit a descriptive title like `fix(docs): summary of change`. If applicable, add an [extended description](https://www.gitkraken.com/learn/git/best-practices/git-commit-message#git-commit-message-structure).
5. GitHub will redirect you to create a pull request. Give it a title (often the same as the commit) and follow the PR template checklist.

<Note>
  Docs PRs are typically reviewed within a few days. Keep an eye on your PR to address any feedback from maintainers.

Do not bump the PR unless you have new information to provide – maintainers will address it as their availability permits.
</Note>

### Larger edits and additions

For larger changes, additions, or ongoing contributions, it's necessary to set up a local development environment on your machine. Our documentation build pipeline offers local preview, important for ensuring your changes appear as intended before submitting.

#### Set up local environment

Before you can work on this project, ensure you have the following installed:

* `python >= 3.13, < 4.0`
* [**`uv`**](https://docs.astral.sh/uv/) - Python package manager (used for dependency management)
* [**Node.js**](https://nodejs.org/en) and [**`npm`**](https://www.npmjs.com/) - For Mintlify CLI and reference documentation builds
* [**Make**](https://www.gnu.org/software/make/) - For running build commands
* [**Git**](https://git-scm.com/) - For version control

**Optional but recommended:**

* **[`markdownlint-cli`](https://github.com/igorshubovych/markdownlint-cli)** - For linting markdown files

* **[`pnpm`](https://pnpm.io/)** - Required only if you're working on reference documentation

* **[Mintlify MDX VSCode extension](https://www.mintlify.com/blog/mdx-vscode-extension)**

1. Clone the [`langchain-ai/docs`](https://github.com/langchain-ai/docs) repo. Follow the steps outlined in [`IDE_SETUP.md`](https://github.com/langchain-ai/docs/blob/main/IDE_SETUP.md).

2. Install dependencies:

* Install Python dependencies using `uv sync --all-groups`
   * Install Mintlify CLI globally via npm

3. Verify your setup:

This should build the documentation without errors.

After install, you'll have access to the `docs` command:

* `docs dev` - Start development mode with file watching and hot reload
* `docs build` - Build documentation

See [Available commands](#available-commands) for more details.

#### Edit documentation

<Note>
  **Only edit files in `src/`** – The `build/` directory is automatically generated.
</Note>

1. Ensure your [environment is set up](#set-up-local-environment) and that you have followed the steps in [`IDE_SETUP.md`](https://github.com/langchain-ai/docs/blob/main/IDE_SETUP.md) to configure your IDE/editor to automatically apply the correct settings.

2. Edit files in `src/`
   * Make changes to markdown files and the build system will automatically detect changes and rebuild affected files.
   * If OSS content varies between Python and JavaScript/TypeScript, add content for [both in the same file](#co-locate-python-and-javascripttypescript-oss-content). Otherwise, content will be identical for both languages.
   * Use [Mintlify syntax](https://mintlify.com/docs) for formatting.

3. Start development mode to preview changes locally:

This starts a development server with hot reload at `http://localhost:3000`.

* Continue editing and see changes reflected immediately.
   * The development server rebuilds only changed files for faster feedback.

5. Run the [quality checks](#run-quality-checks) to ensure your changes are valid.

6. Get approval from the relevant reviewers.

LangChain team members can [generate a sharable preview build](#create-a-sharable-preview-build)

7. [Publish to production](#publish-to-prod) (team members only).

#### Create a sharable preview build

<Note>
  Only LangChain team members can create sharable preview builds.
</Note>

<Accordion title="Instructions">
  Previews are useful for sharing work-in-progress changes with others.

When you create or update a PR, a [preview branch/ID](https://github.com/langchain-ai/docs/actions/workflows/create-preview-branch.yml) is automatically generated for you. A comment will be left on the PR with the ID, which you can then use to generate a preview. (You can also run this workflow manually if needed.)

1. Copy the preview branch's ID from the comment.
  2. In the [Mintlify dashboard](https://dashboard.mintlify.com/langchain-5e9cc07a/langchain-5e9cc07a?section=previews), click **Create preview deployment**.
  3. Enter the preview branch's ID.
  4. Click **Create deployment**.
     A **Manual update** will display in the **Previews** table.
  5. Select the preview and click **Visit** to view the preview build.

To redeploy the preview build with the latest changes, click **Redeploy** on the Mintlify dashboard.
</Accordion>

#### Run quality checks

Before submitting changes, ensure your code passes formatting and linting checks:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
* **[`pnpm`](https://pnpm.io/)** - Required only if you're working on reference documentation
```

Example 2 (unknown):
```unknown
* **[Mintlify MDX VSCode extension](https://www.mintlify.com/blog/mdx-vscode-extension)**

**Setup steps:**

1. Clone the [`langchain-ai/docs`](https://github.com/langchain-ai/docs) repo. Follow the steps outlined in [`IDE_SETUP.md`](https://github.com/langchain-ai/docs/blob/main/IDE_SETUP.md).

2. Install dependencies:
```

Example 3 (unknown):
```unknown
This command will:

   * Install Python dependencies using `uv sync --all-groups`
   * Install Mintlify CLI globally via npm

3. Verify your setup:
```

Example 4 (unknown):
```unknown
This should build the documentation without errors.

After install, you'll have access to the `docs` command:
```

---

## Control plane API reference for LangSmith Deployment

**URL:** llms-txt#control-plane-api-reference-for-langsmith-deployment

**Contents:**
- Host
- Authentication
- Versioning
- Quick Start
- Example Code

Source: https://docs.langchain.com/langsmith/api-ref-control-plane

The control plane API is part of [LangSmith Deployment](/langsmith/deployments). With the control plane API, you can programmatically create, manage, and automate your [Agent Server](/langsmith/agent-server) deployments—for example, as part of a custom CI/CD workflow.

Browse the full API reference in the **Control Plane API** section in the sidebar, or refer to the endpoint groups:

* [Integrations (v1)](/api-reference/integrations-v1/list-github-integrations): GitHub integrations and repository listings
* [Deployments (v2)](/api-reference/deployments-v2): Create, manage, and update Agent Server deployments
* [Listeners (v2)](/api-reference/listeners-v2): Listener resources for self-hosted enterprise organizations
* [Auth Service (v2)](/api-reference/auth-service-v2): OAuth provider configuration and authentication flows

The control plane hosts for Cloud data regions:

| US                               | EU                                  |
| -------------------------------- | ----------------------------------- |
| `https://api.host.langchain.com` | `https://eu.api.host.langchain.com` |

**Note**: Self-hosted deployments of LangSmith will have a custom host for the control plane. The control plane APIs can be accessed at the path `/api-host`. For example, `http(s)://<host>/api-host/v2/deployments`. See [here](../langsmith/self-host-usage#configuring-the-application-you-want-to-use-with-langsmith) for more details.

To authenticate with the control plane API, set the `X-Api-Key` header to a valid LangSmith API key and set the `X-Tenant-Id` header to a valid workspace ID to target.

Example `curl` command:

Each endpoint path is prefixed with a version (e.g. `v1`, `v2`).

1. Call `POST /v2/deployments` to create a new Deployment. The response body contains the Deployment ID (`id`) and the ID of the latest (and first) revision (`latest_revision_id`).
2. Call `GET /v2/deployments/{deployment_id}` to retrieve the Deployment. Set `deployment_id` in the URL to the value of Deployment ID (`id`).
3. Poll for revision `status` until `status` is `DEPLOYED` by calling `GET /v2/deployments/{deployment_id}/revisions/{latest_revision_id}`.
4. Call `PATCH /v2/deployments/{deployment_id}` to update the deployment.

Below is example Python code that demonstrates how to orchestrate the control plane APIs to create a deployment, update the deployment, and delete the deployment.

```python  theme={null}
import os
import time

import requests
from dotenv import load_dotenv

**Examples:**

Example 1 (unknown):
```unknown
## Versioning

Each endpoint path is prefixed with a version (e.g. `v1`, `v2`).

## Quick Start

1. Call `POST /v2/deployments` to create a new Deployment. The response body contains the Deployment ID (`id`) and the ID of the latest (and first) revision (`latest_revision_id`).
2. Call `GET /v2/deployments/{deployment_id}` to retrieve the Deployment. Set `deployment_id` in the URL to the value of Deployment ID (`id`).
3. Poll for revision `status` until `status` is `DEPLOYED` by calling `GET /v2/deployments/{deployment_id}/revisions/{latest_revision_id}`.
4. Call `PATCH /v2/deployments/{deployment_id}` to update the deployment.

## Example Code

Below is example Python code that demonstrates how to orchestrate the control plane APIs to create a deployment, update the deployment, and delete the deployment.
```

---

## Conversation 1: Learn about a project

**URL:** llms-txt#conversation-1:-learn-about-a-project

agent.invoke({
    "messages": [{"role": "user", "content": "We're building a web app with React. Save project notes."}]
})

---

## Conversation 2: Use that knowledge

**URL:** llms-txt#conversation-2:-use-that-knowledge

agent.invoke({
    "messages": [{"role": "user", "content": "What framework are we using?"}]
})

---

## Copy Thread

**URL:** llms-txt#copy-thread

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/copy-thread

langsmith/agent-server-openapi.json post /threads/{thread_id}/copy
Create a new thread with a copy of the state and checkpoints from an existing thread.

---

## Count Assistants

**URL:** llms-txt#count-assistants

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/count-assistants

langsmith/agent-server-openapi.json post /assistants/count
Get the count of assistants matching the specified criteria.

---

## Count Crons

**URL:** llms-txt#count-crons

Source: https://docs.langchain.com/langsmith/agent-server-api/crons-plus-tier/count-crons

langsmith/agent-server-openapi.json post /runs/crons/count
Get the count of crons matching the specified criteria.

---

## Count Threads

**URL:** llms-txt#count-threads

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/count-threads

langsmith/agent-server-openapi.json post /threads/count
Get the count of threads matching the specified criteria.

---

## Co-marketing

**URL:** llms-txt#co-marketing

**Contents:**
  - Content we're excited to promote

Source: https://docs.langchain.com/oss/python/contributing/comarketing

With over 60 million monthly downloads, LangChain has a large audience of developers building LLM applications. Beyond just listing integrations, we aim to highlight high-quality, educational examples that inspire developers and advance the ecosystem.

<Note>
  While we occasionally share integrations, we prioritize content that provides
  meaningful insights and best practices. Our main social channels are [Twitter](https://x.com/LangChainAI) and
  [LinkedIn](https://www.linkedin.com/company/langchain/), where we highlight the best examples.
</Note>

### Content we're excited to promote

<AccordionGroup>
  <Accordion title="Educational content" icon="graduation-cap">
    Blogs, YouTube videos and other media showcasing educational content. Note that we prefer content that is NOT framed as "here's how to use integration XYZ", but rather "here's how to do ABC", as we find that is more educational and helpful for developers.
  </Accordion>

<Accordion title="End-to-end applications" icon="cube">
    End-to-end applications are great resources for developers looking to build. We prefer to highlight applications that are more complex/agentic in nature, and that use [LangGraph](https://github.com/langchain-ai/langgraph) as the orchestration framework. We get particularly excited about anything involving:

* Long-term memory systems
    * Human-in-the-loop interaction patterns
    * Multi-agent architectures
  </Accordion>

<Accordion title="Research" icon="flask">
    We love highlighting novel research! Whether it is research built on top of LangChain or that integrates with it.
  </Accordion>
</AccordionGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/comarketing.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Create and manage datasets in the UI

**URL:** llms-txt#create-and-manage-datasets-in-the-ui

**Contents:**
- Create a dataset and add examples
  - Manually from a tracing project
  - Automatically from a tracing project
  - From examples in an Annotation Queue
  - From the Prompt Playground
  - Import a dataset from a CSV or JSONL file
  - Create a new dataset from the Datasets & Experiments page
  - Add synthetic examples created by an LLM
- Manage a dataset
  - Create a dataset schema

Source: https://docs.langchain.com/langsmith/manage-datasets-in-application

[*Datasets*](/langsmith/evaluation-concepts#datasets) enable you to perform repeatable evaluations over time using consistent data. Datasets are made up of [*examples*](/langsmith/evaluation-concepts#examples), which store inputs, outputs, and optionally, reference outputs.

This page outlines the various methods for [creating](#create-a-dataset-and-add-examples) and [managing](#manage-a-dataset) datasets in the [LangSmith UI](https://smith.langchain.com).

## Create a dataset and add examples

The following sections explain the different ways you can create a dataset in LangSmith and add examples to it. Depending on your workflow, you can manually curate examples, automatically capture them from tracing, import files, or even generate synthetic data:

* [Manually from a tracing project](#manually-from-a-tracing-project)
* [Automatically from a tracing project](#automatically-from-a-tracing-project)
* [From examples in an Annotation Queue](#from-examples-in-an-annotation-queue)
* [From the Prompt Playground](#from-the-prompt-playground)
* [Import a dataset from a CSV or JSONL file](#import-a-dataset-from-a-csv-or-jsonl-file)
* [Create a new dataset from the dataset page](#create-a-new-dataset-from-the-dataset-page)
* [Add synthetic examples created by an LLM via the Datasets UI](#add-synthetic-examples-created-by-an-llm-via-the-datasets-ui)

### Manually from a tracing project

A common pattern for constructing datasets is to convert notable traces from your application into dataset examples. This approach requires that you have [configured tracing to LangSmith](/langsmith/observability-concepts#tracing-configuration).

<Check>
  A technique to build datasets is to filter the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset. For tips on how to filter traces, refer to [Filter traces](/langsmith/filter-traces-in-application) guide.
</Check>

There are two ways to add data manually from a tracing project to datasets. Navigate to **Tracing Projects** and select a project.

1. Multi-select runs from the runs table. On the **Runs** tab, multi-select runs. At the bottom of the page, click <Icon icon="database" /> **Add to Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fe666b6e1888554d0573df770bd9cda2" alt="The Runs table with a run selected and the Add to Dataset button visible at the bottom of the page." data-og-width="2912" width="2912" data-og-height="1464" height="1464" data-path="langsmith/images/multiselect-add-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2a5526d48109c7e33cb5b72f19c0ae2d 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0434d04eb3c301ea49a369e5def47701 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3f0d3184b1dea71c3794ba653637a513 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=69e347a4a8f26bf7343ba05e88d9fe24 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e4187008b9cc8f5936b7d64251d4641b 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiselect-add-to-dataset.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9419ca3c63abd40cfb899c5cffc36e33 2500w" />

2. On the **Runs** tab, select a run from the table. On the individual run details page, select  **Add to** -> **Dataset** in the top right corner.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fd03b2cf578c3e524223afc5b09d0589" alt="Add to dataset" data-og-width="2898" width="2898" data-og-height="1462" height="1462" data-path="langsmith/images/add-to..dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=70baaf45d7471f218c95af5ee77530d8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=dd8fcd49d9b39e3937c3abcb0b2afc31 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9477bdbe20caa5ccdb50ea1e4e18f234 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8c527bf07a2e032583496c9ba4ddf0c5 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1c538e79a32bc8bb0e6458f1bf7a2276 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to..dataset.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7a739ceda9692ae6661ce365a9ca46ce 2500w" />

When you select a dataset from the run details page, a modal will pop up letting you know if any [transformations](/langsmith/dataset-transformations) were applied or if schema validation failed. For example, the screenshot below shows a dataset that is using transformations to optimize for collecting LLM runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4ac9ca81489294ac40bf4b88a68ba1c9" alt="Confirmation" data-og-width="2898" width="2898" data-og-height="1452" height="1452" data-path="langsmith/images/confirmation.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3c09ffaacb03ec55aeddda61a3a58111 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9146d5498be48425a6de3caf28db394f 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=299ac17e36643502a8ddc0a7d4d49780 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7607f473d263d7a9d55e1720571d3755 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=deb6514e2202d020882f4fc810207795 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/confirmation.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4141f028af5084d808297d936f168121 2500w" />

You can then optionally edit the run before adding it to the dataset.

### Automatically from a tracing project

You can use [run rules](/langsmith/rules) to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that are [tagged](/langsmith/observability-concepts#tags) with a specific use case or have a [low feedback score](/langsmith/observability-concepts#feedback).

### From examples in an Annotation Queue

<Check>
  If you rely on subject matter experts to build meaningful datasets, use [annotation queues](/langsmith/annotation-queues) to provide a streamlined view for reviewers. Human reviewers can optionally modify the inputs/outputs/reference outputs from a trace before it is added to the dataset.
</Check>

Annotation queues can be optionally configured with a default dataset, though you can add runs to any dataset by using the dataset switcher on the bottom of the screen. Once you select the right dataset, click **Add to Dataset** or hit the hot key `D` to add the run to it.

Any modifications you make to the run in your annotation queue will carry over to the dataset, and all metadata associated with the run will also be copied.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=885c58ab30d94b2371b79730468e0be3" alt="Add to dataset from annotation queue" data-og-width="2290" width="2290" data-og-height="1468" height="1468" data-path="langsmith/images/add-to-dataset-from-aq.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=90c1279193bf00acf6112980ebd94558 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9cb62b05c7d90e5904573d7992a141cc 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5c547cc7f74a3b3b34eb7788ad324fcd 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6c623a6015dcb818677b6af9d41ad923 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=47c9a570a9c1e568c0a035aa82adc5d0 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-dataset-from-aq.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c76a247a72ca269c29c99c1da46852e6 2500w" />

Note you can also set up rules to add runs that meet specific criteria to an annotation queue using [automation rules](/langsmith/rules).

### From the Prompt Playground

On the [**Prompt Playground**](/langsmith/observability-concepts#prompt-playground) page, select **Set up Evaluation**, click **+New** if you're starting a new dataset or select from an existing dataset.

<Note>
  Creating datasets inline in the playground is not supported for datasets that have nested keys. In order to add/edit examples with nested keys, you must edit [from the datasets page](/langsmith/manage-datasets-in-application#from-the-datasets-page).
</Note>

To edit the examples:

* Use **+Row** to add a new example to the dataset
* Delete an example using the **⋮** dropdown on the right hand side of the table
* If you're creating a reference-free dataset remove the "Reference Output" column using the **x** button in the column. Note: this action is not reversible.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=583ce80f84902ccb5ccab36a44dddb9b" alt="Create a dataset in the playground" data-og-width="1318" width="1318" data-og-height="981" height="981" data-path="langsmith/images/playground-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=31fe00c30e17e901334f46845dba1464 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6e63d19dd4be4761d1f6c0d4746395cc 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f625e8480f96e644b4ff2205a2905c9d 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5126df423f04d152ad638f70f1a7765d 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=53a13fe730bbe7c2393049dac76266db 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-dataset.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=998729ce541d92f078382984c76c2501 2500w" />

### Import a dataset from a CSV or JSONL file

On the **Datasets & Experiments** page, click **+New Dataset**, then **Import** an existing dataset from CSV or JSONL file.

### Create a new dataset from the Datasets & Experiments page

1. Navigate to the **Datasets & Experiments** page from the left-hand menu.
2. Click **+ New Dataset**.
3. On the **New Dataset** page, select the **Create from scratch** tab.
4. Add a name and description for the dataset.
5. (Optional) Create a [dataset schema](#create-a-dataset-schema) to validate your dataset.
6. Click **Create**, which will create an empty dataset.
7. To add examples inline, on the dataset's page, go to the **Examples** tab. Click **+ Example**.
8. Define examples in JSON and click **Submit**. For more details on dataset splits, refer to [Create and manage dataset splits](#create-and-manage-dataset-splits).

### Add synthetic examples created by an LLM

If you have existing examples and a [schema](#create-a-dataset-schema) defined on your dataset, when you click **+ Example** there is an option to <Icon icon="sparkles" /> **Add AI-Generated Examples**. This will use an LLM to create [synthetic](/langsmith/evaluation-concepts#synthetic-data) examples.

In **Generate examples**, do the following:

1. Click **API Key** in the top right of the pane to set your OpenAI API key as a [workspace secret](/langsmith/administration-overview#workspaces). If your workspace already has an OpenAI API key set, you can skip this step.

2. Select <Tooltip tip="A few sample input–output pairs that guide the model on how to perform a task.">few-shot examples</Tooltip>: Toggle **Automatic** or **Manual** reference examples. You can select these examples manually from your dataset or use the automatic selection option.

3. Enter the number of synthetic examples you want to generate.

4. Click **Generate**.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=4ec726f80ee38a829ade96caedb61925" alt="The AI-Generated Examples configuration window. Selections for manual and automatic and number of examples to generate." data-og-width="689" width="689" data-og-height="383" height="383" data-path="langsmith/images/generate-synthetic-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=280&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=2a49e15cf0be32a284ab6749941367d4 280w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=560&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=c6d6ee9674352b0bc75573cd7f4a5151 560w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=840&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=782626122e3c9ebef0cf67c0cd2060cf 840w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=1100&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6e1a7c8b25bc8d15324dfd7c50a8bd5c 1100w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=1650&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=bb757aa11517627441a4c8317f7fb313 1650w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-light.png?w=2500&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6bab6b7f6f500ac9044ab3b816484af5 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=6c0ba9da5bf342e702c23406bdfdf18c" alt="The AI-Generated Examples configuration window. Selections for manual and automatic and number of examples to generate." data-og-width="674" width="674" data-og-height="361" height="361" data-path="langsmith/images/generate-synthetic-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=280&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=f4f82b37bd0e5b33bb7d113f5d67de38 280w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=560&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=b07f30b95f9c7dd7c3c21919d2a6ee86 560w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=840&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=64a4cbcd28b292e109f2807125516981 840w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=1100&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=fd764de7b0999e27b1b336260f206cd8 1100w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=1650&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=77c5081c2232e183b372a418bffce330 1650w, https://mintcdn.com/langchain-5e9cc07a/4E7JL9dL7Pg6moF1/langsmith/images/generate-synthetic-dark.png?w=2500&fit=max&auto=format&n=4E7JL9dL7Pg6moF1&q=85&s=3be24e6515512dda957b4ce6ba2cd732 2500w" />
   </div>

5. The examples will appear on the **Select generated examples** page. Choose which examples to add to your dataset, with the option to edit them before finalizing. Click **Save Examples**.

6. Each example will be validated against your specified dataset schema and tagged as **synthetic** in the source metadata.

<div style={{ textAlign: 'center' }}>
     <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=146c5f6238415bb8d77da15a8a17c839" alt="Select generated examples page with generated examples selected and Save examples button." data-og-width="1781" width="1781" data-og-height="856" height="856" data-path="langsmith/images/select-generated-examples-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=280&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1175f7421bb4c5456ebccccddcc2bd56 280w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=560&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=d4d85335f1ec8826acce6be4a56d20d2 560w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=840&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=167488b8860682ee6b3ceb6dee4e8604 840w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=1100&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=468acc35c4f46ce0997f1c552504c3ff 1100w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=1650&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=3131cd4260d4cff89da156c774243500 1650w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-light.png?w=2500&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=23b27fab3ff759c7f7d48d7057b19409 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1f1235b31b2d86cf5c7c615c84061e9c" alt="Select generated examples page with generated examples selected and Save examples button." data-og-width="1779" width="1779" data-og-height="838" height="838" data-path="langsmith/images/select-generated-examples-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=280&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=704f33356493a5913bae1c6a744acc2d 280w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=560&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=9222e34f0b8693ecfde55d33378c887a 560w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=840&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=8e96a9e18d8142347fbcd3e6ecbb64c5 840w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=1100&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=3e6c57060fb9bb371fafbe272ed83381 1100w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=1650&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=a81b2b4775f89ce9004a3cc1ea38ae57 1650w, https://mintcdn.com/langchain-5e9cc07a/mw9POU1xwbwaPxuQ/langsmith/images/select-generated-examples-dark.png?w=2500&fit=max&auto=format&n=mw9POU1xwbwaPxuQ&q=85&s=1917622069d093a1f43388bac001a050 2500w" />
   </div>

### Create a dataset schema

LangSmith datasets store arbitrary JSON objects. We recommend (but do not require) that you define a schema for your dataset to ensure that they conform to a specific JSON schema. Dataset schemas are defined with standard [JSON schema](https://json-schema.org/), with the addition of a few [prebuilt types](/langsmith/dataset-json-types) that make it easier to type common primitives like messages and tools.

Certain fields in your schema have a `+ Transformations` option. Transformations are preprocessing steps that, if enabled, update your examples when you add them to the dataset. For example the `convert to OpenAI messages` transformation will convert message-like objects, like LangChain messages, to OpenAI message format.

For the full list of available transformations, see [our reference](/langsmith/dataset-transformations).

<Note>
  If you plan to collect production traces in your dataset from LangChain [ChatModels](https://python.langchain.com/do/langsmith/observability-concepts/chat_models/) or from OpenAI calls using the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client), we offer a prebuilt Chat Model schema that converts messages and tools into industry standard openai formats that can be used downstream with any model for testing. You can also customize the template settings to match your use case.

Please see the [dataset transformations reference](/langsmith/dataset-transformations) for more information.
</Note>

### Create and manage dataset splits

Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common in machine learning workflows to split datasets into training, validation, and test sets. This can be useful to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas metadata would be used more for storing information on your examples like tags and information about its origin.

In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split). However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for some evaluation workflows - for example, if an example falls into multiple categories on which you may want to evaluate your application.

In order to create and manage splits in the app, you can select some examples in your dataset and click "Add to Split". From the resulting popup menu, you can select and unselect splits for the selected examples, or create a new split.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=014aa1fdc735f055c9e66a2a18720d4c" alt="Add to Split" data-og-width="1309" width="1309" data-og-height="915" height="915" data-path="langsmith/images/add-to-split2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0c1ef18d892f91c218d51d47fb313d81 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1168e3dd272772dde9cd92d767b832f8 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=be237dec928908095462f5314bb795fd 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6d78b720e3dd1e98bb4880be2f18b4e1 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d68fd43c3f0e68fad3cc47f3c80d4f04 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-split2.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=95711a7ca8fe56f1a724568454f6fde5 2500w" />

### Edit example metadata

You can add metadata to your examples by clicking on an example and then clicking "Edit" on the top righthand side of the popover. From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about your examples, such as tags or version info, which you can then [group by](/langsmith/analyze-an-experiment#group-results-by-metadata) when analyzing experiment results or [filter by](/langsmith/manage-datasets-programmatically#list-examples-by-metadata) when you call `list_examples` in the SDK.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-metadata.gif?s=d7235cc2b83913561faccb5083780f17" alt="Add Metadata" data-og-width="1010" width="1010" data-og-height="720" height="720" data-path="langsmith/images/add-metadata.gif" data-optimize="true" data-opv="3" />

You can filter examples by split, metadata key/value or perform full-text search over examples. These filtering options are available to the top left of the examples table.

* **Filter by split**: Select split > Select a split to filter by
* **Filter by metadata**: Filters > Select "Metadata" from the dropdown > Select the metadata key and value to filter on
* **Full-text search**: Filters > Select "Full Text" from the dropdown > Enter your search criteria

You may add multiple filters, and only examples that satisfy all of the filters will be displayed in the table.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2d1f300884d5e886267a137a3cb3e4c7" alt="Filters Applied to Examples" data-og-width="1307" width="1307" data-og-height="370" height="370" data-path="langsmith/images/filters-applied.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=31bd615f72fbf783850caac0b6f06bb7 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4fcef71fff0a7251a559f3d275f527da 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=591c3a65c909f386e745e6f76107722f 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1bc1167f20d8a0b80ce88f3001735a4d 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=769ab5f104184d1f5841185666c50dbf 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filters-applied.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7130de5c7f60a1e597ddbffa8575f056 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets-in-application.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Create an account and API key

**URL:** llms-txt#create-an-account-and-api-key

**Contents:**
- API keys
- Create an API key
- Delete an API key
- Configure the SDK
- Using API keys outside of the SDK

Source: https://docs.langchain.com/langsmith/create-account-api-key

To get started with LangSmith, you need to create an account. You can sign up for a free account in the [LangSmith UI](https://smith.langchain.com). LangSmith supports sign in with Google, GitHub, and email.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2a18e674c9b6e96dd0d5af16ddeeaf1a" alt="Create account" data-og-width="1768" width="1768" data-og-height="1252" height="1252" data-path="langsmith/images/create-account.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=45abdaeb33706739a680080f52a5457c 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=991caa9cc07e229e8a6db0a2ac38fdb9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0ff78a93a7e28cfe9fccbfd5a7d54ec5 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=04f93900e425e47461c882b25e5298f7 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=866363d551391983b36edfc481d67404 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-account.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=186e16f729e0a637c0eaab35002aa4ad 2500w" />

LangSmith supports two types of API keys: Service Keys and Personal Access Tokens. Both types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases.

For more details on Service Keys and Personal Access Tokens, refer to the [Administration overview page](/langsmith/administration-overview).

To log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests. API keys can be scoped to a set of [workspaces](/langsmith/administration-overview#workspaces), or the entire [organization](/langsmith/administration-overview#organizations).

To create either type of API key:

1. Navigate to the [Settings page](https://smith.langchain.com/settings) and scroll to the **API Keys** section.
2. For service keys, choose between an organization-scoped and workspace-scoped key. If the key is workspace-scoped, the workspaces must then be specified.

Enterprise users are also able to [assign specific roles](/langsmith/administration-overview#workspace-roles-rbac) to the key, which adjusts its permissions.
3. Set the key's expiration; the key will become unusable after the number of days chosen, or never, if that is selected.
4. Click **Create API Key.**

<Note>
  The API key will be shown only once, so make sure to copy it and store it in a safe place.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=e27b419a9c317a78f8a98ff5024e1235" alt="Create API key" data-og-width="1224" width="1224" data-og-height="1137" height="1137" data-path="langsmith/images/create-api-key.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=280&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=b0148d07161d0f214a9e6442297e83a3 280w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=560&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=305cf6904089edcee1db749552e41b5f 560w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=840&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=88ead9f8f16475135f43f32ecdfae35a 840w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=1100&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=0803eebefa356974e485217545b9cf13 1100w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=1650&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=fe671c360990ca8e4e86f5749a7f59db 1650w, https://mintcdn.com/langchain-5e9cc07a/RZqwlMMHZpKJks4w/langsmith/images/create-api-key.png?w=2500&fit=max&auto=format&n=RZqwlMMHZpKJks4w&q=85&s=02431a31fddd5ee58d30296fed75a238 2500w" />

To delete an API key:

1. Navigate to the [Settings page](https://smith.langchain.com/settings) and scroll to the **API Keys** section.
2. Find the API key you need to delete from the table. Toggle **Personal** or **Service** as needed.
3. Select the trash icon <Icon icon="trash" iconType="solid" /> in the **Actions** column and confirm deletion.

You may set the following environment variables in addition to `LANGSMITH_API_KEY`.

This is only required if using the EU instance.

`LANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com`

This is only required for keys scoped to more than one workspace.

`LANGSMITH_WORKSPACE_ID=<Workspace ID>`

## Using API keys outside of the SDK

See [instructions for managing your organization via API](/langsmith/manage-organization-by-api).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-account-api-key.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Create an AI message manually (e.g., for conversation history)

**URL:** llms-txt#create-an-ai-message-manually-(e.g.,-for-conversation-history)

ai_msg = AIMessage("I'd be happy to help you with that question!")

---

## Create Assistant

**URL:** llms-txt#create-assistant

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/create-assistant

langsmith/agent-server-openapi.json post /assistants
Create an assistant.

An initial version of the assistant will be created and the assistant is set to that version. To change versions, use the `POST /assistants/{assistant_id}/latest` endpoint.

---

## Create audio recorder

**URL:** llms-txt#create-audio-recorder

audio_recorder = AudioRecorder(str(recording_path))

---

## Create a child run, linked to the parent

**URL:** llms-txt#create-a-child-run,-linked-to-the-parent

child_run = construct_run(
    name="Child Run",
    run_type="llm",
    inputs={"question": "What is the capital of France?"},
    parent_dotted_order=parent_run["dotted_order"],
)

---

## Create a code analysis prompt template

**URL:** llms-txt#create-a-code-analysis-prompt-template

code_analysis_prompt = """
Analyze the following code and provide insights:

Please provide:
1. A brief summary of what the code does
2. Any potential improvements
3. Code quality assessment
"""

prompt_template_config = PromptTemplateConfig(
    template=code_analysis_prompt,
    name="code_analyzer",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(name="code", description="The code to analyze", is_required=True),
    ],
)

---

## Create a custom LangGraph graph

**URL:** llms-txt#create-a-custom-langgraph-graph

def create_weather_graph():
    workflow = StateGraph(...)
    # Build your custom graph
    return workflow.compile()

weather_graph = create_weather_graph()

---

## Create a dataset

**URL:** llms-txt#create-a-dataset

**Contents:**
- Run a single experiment

examples = [
    {
        "inputs": {"text": "Shut up, idiot"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "You're a wonderful person"},
        "outputs": {"label": "Not toxic"},
    },
    {
        "inputs": {"text": "This is the worst thing ever"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "I had a great day today"},
        "outputs": {"label": "Not toxic"},
    },
    {
        "inputs": {"text": "Nobody likes you"},
        "outputs": {"label": "Toxic"},
    },
    {
        "inputs": {"text": "This is unacceptable. I want to speak to the manager."},
        "outputs": {"label": "Not toxic"},
    },
]

dataset_name = "Toxic Queries - API Example"
dataset = client.create_dataset(dataset_name=dataset_name)
client.create_examples(dataset_id=dataset.id, examples=examples)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Run a single experiment

To run an experiment via the API, you'll need to:

1. Fetch the examples from your dataset.
2. Create an experiment (also called a "session" in the API).
3. For each example, create runs that reference both the example and the experiment.
4. Close the experiment by setting its `end_time`.

First, pull all of the examples you'd want to use in your experiment using the `/examples` endpoint:
```

---

## Create a documentation generator

**URL:** llms-txt#create-a-documentation-generator

**Contents:**
- Advanced usage
  - Custom metadata and tags

doc_prompt = """
Generate comprehensive documentation for the following function:

Include:
- Purpose and functionality
- Parameters and return values
- Usage examples
- Any important notes
"""

doc_template_config = PromptTemplateConfig(
    template=doc_prompt,
    name="doc_generator",
    template_format="semantic-kernel",
    input_variables=[
        InputVariable(name="function_code", description="The function code to document", is_required=True),
    ],
)

doc_generator = kernel.add_function(
    function_name="generateDocs",
    plugin_name="documentationPlugin",
    prompt_template_config=doc_template_config,
)

async def main():
    # Example code to analyze
    sample_code = """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
    """

# Analyze the code
    analysis_result = await kernel.invoke(code_analyzer, code=sample_code)
    print("Code Analysis:")
    print(analysis_result)
    print("\n" + "="*50 + "\n")

# Generate documentation
    doc_result = await kernel.invoke(doc_generator, function_code=sample_code)
    print("Generated Documentation:")
    print(doc_result)

return {"analysis": str(analysis_result), "documentation": str(doc_result)}

if __name__ == "__main__":
    asyncio.run(main())
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes:
```

---

## Create a function that will take in a list of examples and format them into a string

**URL:** llms-txt#create-a-function-that-will-take-in-a-list-of-examples-and-format-them-into-a-string

**Contents:**
  - NEW CODE ###
- Semantic search over examples

def create_example_string(examples):
    final_strings = []
    for e in examples:
        final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")
    return "\n\n".join(final_strings)
### NEW CODE ###

client = openai.Client()

available_topics = [
    "bug",
    "improvement",
    "new_feature",
    "documentation",
    "integration",
]

prompt_template = """Classify the type of the issue as one of {topics}.

Here are some examples:
{examples}

Begin!
Issue: {text}
>"""

@traceable(
    run_type="chain",
    name="Classifier",
)
def topic_classifier(
    topic: str):
    # We can now pull down the examples from the dataset
    # We do this inside the function so it always get the most up-to-date examples,
    # But this can be done outside and cached for speed if desired
    examples = list(ls_client.list_examples(dataset_name="classifier-github-issues"))  # <- New Code
    example_string = create_example_string(examples)
    return client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {
                "role": "user",
                "content": prompt_template.format(
                    topics=','.join(available_topics),
                    text=topic,
                    examples=example_string,
                )
            }
        ],
    ).choices[0].message.content
python  theme={null}
ls_client = Client()
run_id = uuid7()
topic_classifier(
    "address bug in documentation",
    langsmith_extra={"run_id": run_id})
python  theme={null}
import numpy as np

def find_similar(examples, topic, k=5):
    inputs = [e.inputs['topic'] for e in examples] + [topic]
    vectors = client.embeddings.create(input=inputs, model="text-embedding-3-small")
    vectors = [e.embedding for e in vectors.data]
    vectors = np.array(vectors)
    args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5]
    examples = [examples[i] for i in args]
    return examples
python  theme={null}
ls_client = Client()

def create_example_string(examples):
    final_strings = []
    for e in examples:
        final_strings.append(f"Input: {e.inputs['topic']}\n> {e.outputs['output']}")
    return "\n\n".join(final_strings)

client = openai.Client()

available_topics = [
    "bug",
    "improvement",
    "new_feature",
    "documentation",
    "integration",
]

prompt_template = """Classify the type of the issue as one of {topics}.

Here are some examples:
{examples}

Begin!
Issue: {text}
>"""

@traceable(
    run_type="chain",
    name="Classifier",
)
def topic_classifier(
    topic: str):
    examples = list(ls_client.list_examples(dataset_name="classifier-github-issues"))
    examples = find_similar(examples, topic)
    example_string = create_example_string(examples)
    return client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {
                "role": "user",
                "content": prompt_template.format(
                    topics=','.join(available_topics),
                    text=topic,
                    examples=example_string,
                )
            }
        ],
    ).choices[0].message.content
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/optimize-classifier.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as `documentation`
```

Example 2 (unknown):
```unknown
## Semantic search over examples

One additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.

In order to do this, we can first define an example to find the `k` most similar examples:
```

Example 3 (unknown):
```unknown
We can then use that in the application
```

---

## Create a new experiment using the /sessions endpoint

**URL:** llms-txt#create-a-new-experiment-using-the-/sessions-endpoint

---

## Create a parent run

**URL:** llms-txt#create-a-parent-run

parent_run = construct_run(
    name="Parent Run",
    run_type="chain",
    inputs={"main_question": "Tell me about France"},
)

---

## Create a prompt

**URL:** llms-txt#create-a-prompt

**Contents:**
- Compose your prompt
  - Template format
  - Add a template variable
  - Structured output
  - Tools
- Run the prompt
- Save your prompt
- View your prompts
- Add metadata

Source: https://docs.langchain.com/langsmith/create-a-prompt

Navigate to the  in the left-hand sidebar or from the application homepage.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2bede4ae9332bdf43ae20580d5bb957d" alt="Empty playground" data-og-width="1747" width="1747" data-og-height="1285" height="1285" data-path="langsmith/images/empty-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=20602d3b2e7b4219a8dc3612fee194b7 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=141b0ab234c54970f4e86f24ed13a954 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1fa1d0cb9075f4fbdcae487ea4348116 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bb3583d9357a597754396ee94e52c0da 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c21b3e9881e95a0b954d578fa1d8aa47 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/empty-playground.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3dcdde6a3fd4f3365059e181f53f68a0 2500w" />

## Compose your prompt

On the left is an editable view of the prompt.

The prompt is made up of messages, each of which has a "role" - including `system`, `human`, and `ai`.

The default template format is `f-string`, but you can change the prompt template format to `mustache` by clicking on the settings icon next to the model -> prompt format -> template format. Learn more about template formats [here](/langsmith/prompt-engineering-concepts#f-string-vs-mustache).
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=417fa567135babb46d2bf080b7eb44f0" alt="Template format" data-og-width="938" width="938" data-og-height="352" height="352" data-path="langsmith/images/template-format.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5bbcf83ad9e251ea9fdeb6c0f7dd49eb 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=02776bb45c6ffe0df98775886e75eaeb 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c63b67b6afe893f7944ee2fb8b76bbaf 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9df27878d7eaf60e953b9438fcf3f8c4 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a8b2e1b3fd0977e5b6beb984119bc6fd 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-format.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d30b1d76394200f9473f0b8fb08d2a5e 2500w" />

### Add a template variable

The power of prompts comes from the ability to use variables in your prompt. You can use variables to add dynamic content to your prompt. Add a template variable in one of two ways:

1. Add `{{variable_name}}` to your prompt (with one curly brace on each side for `f-string` and two for `mustache`). <img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b250a57ef0e0a40a56822af750d52810" alt="Prompt with variable" data-og-width="726" width="726" data-og-height="169" height="169" data-path="langsmith/images/prompt-with-variable.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ea82252d99c7ee63c15d1c1036db8c55 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fc56d047cf631b1e66ecfd09ab4c03a5 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7cc8cb861e9361445dcee545cbac84b7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a7ed018754ebb8dad675a75c4aa638cb 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5d360aac28b60689789e8d3274103611 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-with-variable.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b9b98555c36f929004d234cc7fadaea5 2500w" />

2. Highlight text you want to templatize and click the tooltip button that shows up. Enter a name for your variable, and convert. <img src="https://mintlify.s3.us-west-1.amazonaws.com/langchain-5e9cc07a/langsmith/images/convert-to-variable.gif" alt="Convert to variable" />

When we add a variable, we see a place to enter sample inputs for our prompt variables. Fill these in with values to test the prompt. <img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=35674518c53e340d02719cbb7b5fd782" alt="Prompt inputs" data-og-width="775" width="775" data-og-height="134" height="134" data-path="langsmith/images/prompt-inputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=59775af2fccde924b7ad7657db2b4656 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=df9b24f1eb306578590ee772720ced08 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=df549771817326a72a9387d0133ea590 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4bc399d02630e4873ae34afbbb60057e 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=78917ac5c4c938d926a5fe420d4d2780 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-inputs.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=89368aa76e0764d3f3fcb90623b2f28b 2500w" />

### Structured output

Adding an output schema to your prompt will get output in a structured format. Learn more about structured output [here](/langsmith/prompt-engineering-concepts#structured-output). <img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=51cdef35a620c225896dbf2f3ab07528" alt="Structured output" data-og-width="814" width="814" data-og-height="574" height="574" data-path="langsmith/images/structured-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7af3797e6106f2c7858cc67dac7cfe60 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7a3b8bb3d32eda103213856e4e05e74c 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e973d61622e18ab678631c734c4b16a8 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c63196b1604baef88db3f56375458005 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e672a8f8af25d414d61592a1b22205c0 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/structured-output.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d57196b5940203b66a31196ea464243 2500w" />

You can also add a tool by clicking the `+ Tool` button at the bottom of the prompt editor. See [here](/langsmith/use-tools) for more information on how to use tools.

<Callout type="info" icon="bird">
  Use **[Polly](/langsmith/polly)** in the Playground to generate tools, create output schemas, and optimize your prompts with AI assistance.
</Callout>

Click "Start" to run the prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9dec20b94326e5c8b11775b56eca55b4" alt="Create a prompt run" data-og-width="1525" width="1525" data-og-height="766" height="766" data-path="langsmith/images/create-a-prompt-run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=10cdcbb52db991d37478cdd199f51baf 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=14986d2ef1ad2aafd69d5005413604c8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=20cc523bb2ffa9756d95908b0b434d91 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5bdcb8118c13d9e9710ccc663058f47c 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1f6760ed7f81ce2d9c7907c8539d6122 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-a-prompt-run.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c47787a81ad1ffed842e630ec80a362 2500w" />

To save your prompt, click the "Save" button, name your prompt, and decide if you want it to be "private" or "public". Private prompts are only visible to your workspace, while public prompts are discoverable to anyone.

The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version. <img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=180e2c79fb9d1ee8d7869fc279e2d94a" alt="Save prompt" data-og-width="465" width="465" data-og-height="306" height="306" data-path="langsmith/images/save-prompt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2a2ea17f2ffb787ce2bbbfc88302636e 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7120851531e79e13d8717ba14eb64483 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6b7d47e08120d2329fa923be8c19a612 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b6a4ed33c82c54f093ad4f07db1b7a1c 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5746af64213ab0bc08d7315a895a0d28 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-prompt.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=470e4700c6ed64348f25ae23a8d462c5 2500w" />

<Check>
  The first time you create a public prompt, you'll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace.
</Check>

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=56dbd809c50fb2abc816c73c599f0baf" alt="Public handle" data-og-width="575" width="575" data-og-height="357" height="357" data-path="langsmith/images/public-handle.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e87d4538d4413b9ce13aed26f43e075 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=df539980aec2f06214ee13668d271e38 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0b19f257c2d9eb78b88c0df6e7920d8d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=06c1efd1463437913cbdd6e19274f354 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=648394098abe45d6d916ee1fe3f9066b 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/public-handle.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=844bf001cfa48ba449c873d228cc5fa3 2500w" />

You've just created your first prompt! View a table of your prompts in the prompts tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9f8f5567bb93a0add181a51531474796" alt="Prompt table" data-og-width="1508" width="1508" data-og-height="309" height="309" data-path="langsmith/images/prompt-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6421b33ff02f9af3f994e665fbcddf96 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bf3d973fd239d93c0c4b0dd45e2e7129 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d2d60a1b18eb7baaeaa5a04fa67d4ac7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9d11895d724a60910231494927f86e24 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=52afb51d257f64c5e59ad77e3a3cb67a 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-table.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=15e2b24c349a9f21729b572fdb23734f 2500w" />

To add metadata to your prompt, click the prompt and then click the "Edit" pencil icon next to the name. This brings you to where you can add additional information about the prompt, including a description, a README, and use cases. For public prompts this information will be visible to anyone who views your prompt in the LangChain Hub.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=e54ca6c0c8283b027f5848f79d1cf064" alt="Pencil" data-og-width="1167" width="1167" data-og-height="1067" height="1067" data-path="langsmith/images/pencil.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0336448912188bb85366b0c49556d207 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6456cdae3e5743b770ca530748efd21b 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7f16e9adaa3e2d3063623317344fb7d7 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=12e5f2c3a2cc2f91d66d467cff910809 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ce6cb1b3011638d9bec990c1e8131837 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pencil.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b61eb5331a47c9e6998261799729fdbf 2500w" /> <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=febb5f53b2f917cf3e5a2ff2566eaef4" alt="Edit prompt" data-og-width="1508" width="1508" data-og-height="1084" height="1084" data-path="langsmith/images/edit-prompt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=440242ac2e20e092c50810ca84d37286 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=46ad1078bdd4342cc49ecd1a70837149 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f4998ff77574085ec2eb2fdc8c755fdc 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2065392afa627416297cc8dc5a10780d 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bda747e312cbabdd107a7b54b4e1c03e 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-prompt.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=846eaa58fed8f3a14aa76dbb610a0e41 2500w" />

---

## Create a thread and chat

**URL:** llms-txt#create-a-thread-and-chat

**Contents:**
- Next steps

thread = await client.threads.create()
print(f"✅ Created thread as Alice: {thread['thread_id']}")

response = await client.runs.create(
    thread_id=thread["thread_id"],
    assistant_id="agent",
    input={"messages": [{"role": "user", "content": "Hello!"}]},
)
print("✅ Bot responded:")
print(response)
```

1. Without a valid token, we can't access the bot
2. With a valid token, we can create threads and chat

Congratulations! You've built a chatbot that only lets "authenticated" users access it. While this system doesn't (yet) implement a production-ready security scheme, we've learned the basic mechanics of how to control access to our bot. In the next tutorial, we'll learn how to give each user their own private conversations.

Now that you can control who accesses your bot, you might want to:

1. Continue the tutorial by going to [Make conversations private](/langsmith/resource-auth) to learn about resource authorization.
2. Read more about [authentication concepts](/langsmith/auth).
3. Check out the API reference for [Auth](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth), [Auth.authenticate](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate), and [MinimalUserDict](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.MinimalUserDict) for more authentication details.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-custom-auth.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Create a thread as user 1

**URL:** llms-txt#create-a-thread-as-user-1

thread = await user1_client.threads.create()
print(f"✅ User 1 created thread: {thread['thread_id']}")

---

## Create Background Run

**URL:** llms-txt#create-background-run

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/create-background-run

langsmith/agent-server-openapi.json post /threads/{thread_id}/runs
Create a run in existing thread, return the run ID immediately. Don't wait for the final run output.

---

## Create child run

**URL:** llms-txt#create-child-run

child_run_id = uuid7()
post_run(child_run_id, "OpenAI Call", "llm", {"messages": messages}, parent_run_id)

---

## Create clients for both users

**URL:** llms-txt#create-clients-for-both-users

alice = get_client(
    url="http://localhost:2024",
    headers={"Authorization": "Bearer user1-token"}
)

bob = get_client(
    url="http://localhost:2024",
    headers={"Authorization": "Bearer user2-token"}
)

---

## Create clients with different sampling rates

**URL:** llms-txt#create-clients-with-different-sampling-rates

client_1 = Client(tracing_sampling_rate=0.5)  # 50% sampling
client_2 = Client(tracing_sampling_rate=0.25)  # 25% sampling
client_no_trace = Client(tracing_sampling_rate=0.0)  # No tracing

---

## Create config with thread_id for state persistence

**URL:** llms-txt#create-config-with-thread_id-for-state-persistence

config = {"configurable": {"thread_id": str(uuid.uuid4())}}

---

## Create Cron

**URL:** llms-txt#create-cron

Source: https://docs.langchain.com/langsmith/agent-server-api/crons-plus-tier/create-cron

langsmith/agent-server-openapi.json post /runs/crons
Create a cron to schedule runs on new threads.

---

## Create dataset

**URL:** llms-txt#create-dataset

examples = [
    {
        "inputs": {"messages": [{"role": "user", "content": "i bought some tracks recently and i dont like them"}]},
        "outputs": {"route": "refund_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "I was thinking of purchasing some Rolling Stones tunes, any recommendations?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "i want a refund on purchase 237"}, {"role": "assistant", "content": "I've refunded you a total of $1.98. How else can I help you today?"}, {"role": "user", "content": "did prince release any albums in 2000?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
    {
        "inputs": {"messages": [{"role": "user", "content": "i purchased a cover of Yesterday recently but can't remember who it was by, which versions of it do you have?"}]},
        "outputs": {"route": "question_answering_agent"},
    },
]

dataset_name = "Chinook Customer Service Bot: Intent Classifier"
if not client.has_dataset(dataset_name=dataset_name):
    dataset = client.create_dataset(dataset_name=dataset_name)
    client.create_examples(
        dataset_id=dataset.id,
        examples=examples
    )

---

## Create Deployment

**URL:** llms-txt#create-deployment

Source: https://docs.langchain.com/api-reference/deployments-v2/create-deployment

https://api.host.langchain.com/openapi.json post /v2/deployments
Create a new deployment.

---

## Create Listener

**URL:** llms-txt#create-listener

Source: https://docs.langchain.com/api-reference/listeners-v2/create-listener

https://api.host.langchain.com/openapi.json post /v2/listeners
Create a listener.<br>
<br>
Creating a listener is only allowed for LangSmith organizations with self-hosted enterprise plans.

---

## Create Oauth Provider

**URL:** llms-txt#create-oauth-provider

Source: https://docs.langchain.com/api-reference/auth-service-v2/create-oauth-provider

https://api.host.langchain.com/openapi.json post /v2/auth/providers
Create a new OAuth provider.

---

## Create parent run

**URL:** llms-txt#create-parent-run

parent_run_id = uuid7()
post_run(parent_run_id, "Chat Pipeline", "chain", {"question": question})

---

## Create Run Batch

**URL:** llms-txt#create-run-batch

Source: https://docs.langchain.com/langsmith/agent-server-api/stateless-runs/create-run-batch

langsmith/agent-server-openapi.json post /runs/batch
Create a batch of runs and return immediately.

---

## Create Run, Stream Output

**URL:** llms-txt#create-run,-stream-output

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/create-run-stream-output

langsmith/agent-server-openapi.json post /threads/{thread_id}/runs/stream
Create a run in existing thread. Stream the output.

---

## Create Run, Wait for Output

**URL:** llms-txt#create-run,-wait-for-output

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/create-run-wait-for-output

langsmith/agent-server-openapi.json post /threads/{thread_id}/runs/wait
Create a run in existing thread. Wait for the final output and then return it.

---

## Create store with semantic search enabled

**URL:** llms-txt#create-store-with-semantic-search-enabled

**Contents:**
- Manage short-term memory
  - Trim messages
  - Delete messages
  - Summarize messages
  - Manage checkpoints
- Prebuilt memory tools
- Database management

embeddings = init_embeddings("openai:text-embedding-3-small")
store = InMemoryStore(
    index={
        "embed": embeddings,
        "dims": 1536,
    }
)

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

items = store.search(
    ("user_123", "memories"), query="I'm hungry", limit=1
)
python  theme={null}

from langchain.embeddings import init_embeddings
  from langchain.chat_models import init_chat_model
  from langgraph.store.base import BaseStore
  from langgraph.store.memory import InMemoryStore
  from langgraph.graph import START, MessagesState, StateGraph

model = init_chat_model("gpt-4o-mini")

# Create store with semantic search enabled
  embeddings = init_embeddings("openai:text-embedding-3-small")
  store = InMemoryStore(
      index={
          "embed": embeddings,
          "dims": 1536,
      }
  )

store.put(("user_123", "memories"), "1", {"text": "I love pizza"})
  store.put(("user_123", "memories"), "2", {"text": "I am a plumber"})

def chat(state, *, store: BaseStore):
      # Search based on user's last message
      items = store.search(
          ("user_123", "memories"), query=state["messages"][-1].content, limit=2
      )
      memories = "\n".join(item.value["text"] for item in items)
      memories = f"## Memories of user\n{memories}" if memories else ""
      response = model.invoke(
          [
              {"role": "system", "content": f"You are a helpful assistant.\n{memories}"},
              *state["messages"],
          ]
      )
      return {"messages": [response]}

builder = StateGraph(MessagesState)
  builder.add_node(chat)
  builder.add_edge(START, "chat")
  graph = builder.compile(store=store)

for message, metadata in graph.stream(
      input={"messages": [{"role": "user", "content": "I'm hungry"}]},
      stream_mode="messages",
  ):
      print(message.content, end="")
  python  theme={null}
from langchain_core.messages.utils import (  # [!code highlight]
    trim_messages,  # [!code highlight]
    count_tokens_approximately  # [!code highlight]
)  # [!code highlight]

def call_model(state: MessagesState):
    messages = trim_messages(  # [!code highlight]
        state["messages"],
        strategy="last",
        token_counter=count_tokens_approximately,
        max_tokens=128,
        start_on="human",
        end_on=("human", "tool"),
    )
    response = model.invoke(messages)
    return {"messages": [response]}

builder = StateGraph(MessagesState)
builder.add_node(call_model)
...
python  theme={null}
  from langchain_core.messages.utils import (
      trim_messages,  # [!code highlight]
      count_tokens_approximately  # [!code highlight]
  )
  from langchain.chat_models import init_chat_model
  from langgraph.graph import StateGraph, START, MessagesState

model = init_chat_model("claude-sonnet-4-5-20250929")
  summarization_model = model.bind(max_tokens=128)

def call_model(state: MessagesState):
      messages = trim_messages(  # [!code highlight]
          state["messages"],
          strategy="last",
          token_counter=count_tokens_approximately,
          max_tokens=128,
          start_on="human",
          end_on=("human", "tool"),
      )
      response = model.invoke(messages)
      return {"messages": [response]}

checkpointer = InMemorySaver()
  builder = StateGraph(MessagesState)
  builder.add_node(call_model)
  builder.add_edge(START, "call_model")
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}
  graph.invoke({"messages": "hi, my name is bob"}, config)
  graph.invoke({"messages": "write a short poem about cats"}, config)
  graph.invoke({"messages": "now do the same but for dogs"}, config)
  final_response = graph.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
  
  ================================== Ai Message ==================================

Your name is Bob, as you mentioned when you first introduced yourself.
  python  theme={null}
from langchain.messages import RemoveMessage  # [!code highlight]

def delete_messages(state):
    messages = state["messages"]
    if len(messages) > 2:
        # remove the earliest two messages
        return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]
python  theme={null}
from langgraph.graph.message import REMOVE_ALL_MESSAGES  # [!code highlight]

def delete_messages(state):
    return {"messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  # [!code highlight]
python  theme={null}
  from langchain.messages import RemoveMessage  # [!code highlight]

def delete_messages(state):
      messages = state["messages"]
      if len(messages) > 2:
          # remove the earliest two messages
          return {"messages": [RemoveMessage(id=m.id) for m in messages[:2]]}  # [!code highlight]

def call_model(state: MessagesState):
      response = model.invoke(state["messages"])
      return {"messages": response}

builder = StateGraph(MessagesState)
  builder.add_sequence([call_model, delete_messages])
  builder.add_edge(START, "call_model")

checkpointer = InMemorySaver()
  app = builder.compile(checkpointer=checkpointer)

for event in app.stream(
      {"messages": [{"role": "user", "content": "hi! I'm bob"}]},
      config,
      stream_mode="values"
  ):
      print([(message.type, message.content) for message in event["messages"]])

for event in app.stream(
      {"messages": [{"role": "user", "content": "what's my name?"}]},
      config,
      stream_mode="values"
  ):
      print([(message.type, message.content) for message in event["messages"]])
  
  [('human', "hi! I'm bob")]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', "what's my name?")]
  [('human', "hi! I'm bob"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', "what's my name?"), ('ai', 'Your name is Bob.')]
  [('human', "what's my name?"), ('ai', 'Your name is Bob.')]
  python  theme={null}
from langgraph.graph import MessagesState
class State(MessagesState):
    summary: str
python  theme={null}
def summarize_conversation(state: State):

# First, we get any existing summary
    summary = state.get("summary", "")

# Create our summarization prompt
    if summary:

# A summary already exists
        summary_message = (
            f"This is a summary of the conversation to date: {summary}\n\n"
            "Extend the summary by taking into account the new messages above:"
        )

else:
        summary_message = "Create a summary of the conversation above:"

# Add prompt to our history
    messages = state["messages"] + [HumanMessage(content=summary_message)]
    response = model.invoke(messages)

# Delete all but the 2 most recent messages
    delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
    return {"summary": response.content, "messages": delete_messages}
python  theme={null}
  from typing import Any, TypedDict

from langchain.chat_models import init_chat_model
  from langchain.messages import AnyMessage
  from langchain_core.messages.utils import count_tokens_approximately
  from langgraph.graph import StateGraph, START, MessagesState
  from langgraph.checkpoint.memory import InMemorySaver
  from langmem.short_term import SummarizationNode, RunningSummary  # [!code highlight]

model = init_chat_model("claude-sonnet-4-5-20250929")
  summarization_model = model.bind(max_tokens=128)

class State(MessagesState):
      context: dict[str, RunningSummary]  # [!code highlight]

class LLMInputState(TypedDict):  # [!code highlight]
      summarized_messages: list[AnyMessage]
      context: dict[str, RunningSummary]

summarization_node = SummarizationNode(  # [!code highlight]
      token_counter=count_tokens_approximately,
      model=summarization_model,
      max_tokens=256,
      max_tokens_before_summary=256,
      max_summary_tokens=128,
  )

def call_model(state: LLMInputState):  # [!code highlight]
      response = model.invoke(state["summarized_messages"])
      return {"messages": [response]}

checkpointer = InMemorySaver()
  builder = StateGraph(State)
  builder.add_node(call_model)
  builder.add_node("summarize", summarization_node)  # [!code highlight]
  builder.add_edge(START, "summarize")
  builder.add_edge("summarize", "call_model")
  graph = builder.compile(checkpointer=checkpointer)

# Invoke the graph
  config = {"configurable": {"thread_id": "1"}}
  graph.invoke({"messages": "hi, my name is bob"}, config)
  graph.invoke({"messages": "write a short poem about cats"}, config)
  graph.invoke({"messages": "now do the same but for dogs"}, config)
  final_response = graph.invoke({"messages": "what's my name?"}, config)

final_response["messages"][-1].pretty_print()
  print("\nSummary:", final_response["context"]["running_summary"].summary)
  
  ================================== Ai Message ==================================

From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.

Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled "The Mystery of Cats" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote "The Joy of Dogs," which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.
  python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1",  # [!code highlight]
            # optionally provide an ID for a specific checkpoint,
            # otherwise the latest checkpoint is shown
            # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
    }
    graph.get_state(config)  # [!code highlight]
    
    StateSnapshot(
        values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]}, next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        metadata={
            'source': 'loop',
            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
            'step': 4,
            'parents': {},
            'thread_id': '1'
        },
        created_at='2025-05-05T16:01:24.680462+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        tasks=(),
        interrupts=()
    )
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1",  # [!code highlight]
            # optionally provide an ID for a specific checkpoint,
            # otherwise the latest checkpoint is shown
            # "checkpoint_id": "1f029ca3-1f5b-6704-8004-820c16b69a5a"  # [!code highlight]

}
    }
    checkpointer.get_tuple(config)  # [!code highlight]
    
    CheckpointTuple(
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
        checkpoint={
            'v': 3,
            'ts': '2025-05-05T16:01:24.680462+00:00',
            'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',
            'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
            'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
        },
        metadata={
            'source': 'loop',
            'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},
            'step': 4,
            'parents': {},
            'thread_id': '1'
        },
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
        pending_writes=[]
    )
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1"  # [!code highlight]
        }
    }
    list(graph.get_state_history(config))  # [!code highlight]
    
    [
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
            next=(),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:24.680462+00:00',
            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            tasks=(),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")]},
            next=('call_model',),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.863421+00:00',
            parent_config={...}
            tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
            next=('__start__',),
            config={...},
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.863173+00:00',
            parent_config={...}
            tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "what's my name?"}]}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},
            next=(),
            config={...},
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:23.862295+00:00',
            parent_config={...}
            tasks=(),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': [HumanMessage(content="hi! I'm bob")]},
            next=('call_model',),
            config={...},
            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:22.278960+00:00',
            parent_config={...}
            tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),
            interrupts=()
        ),
        StateSnapshot(
            values={'messages': []},
            next=('__start__',),
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
            created_at='2025-05-05T16:01:22.277497+00:00',
            parent_config=None,
            tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}),),
            interrupts=()
        )
    ]
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "1"  # [!code highlight]
        }
    }
    list(checkpointer.list(config))  # [!code highlight]
    
    [
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:24.680462+00:00',
                'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',
                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?"), AIMessage(content='Your name is Bob.')]},
            },
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},
            parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            pending_writes=[]
        ),
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.863421+00:00',
                'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f',
                'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content="what's my name?")], 'branch:to:call_model': None}
            },
            metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.863173+00:00',
                'id': '1f029ca3-1790-616e-8002-9e021694a0cd',
                'channel_versions': {'__start__': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},
                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}, 'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}
            },
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "what's my name?"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': "what's my name?"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call_model', None)]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:23.862295+00:00',
                'id': '1f029ca3-178d-6f54-8001-d7b180db0c89',
                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}
            },
            metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[]
        ),
        CheckpointTuple(
            config={...},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:22.278960+00:00',
                'id': '1f029ca3-0874-6612-8000-339f2abc83b1',
                'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'},
                'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}},
                'channel_values': {'messages': [HumanMessage(content="hi! I'm bob")], 'branch:to:call_model': None}
            },
            metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},
            parent_config={...},
            pending_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))]
        ),
        CheckpointTuple(
            config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},
            checkpoint={
                'v': 3,
                'ts': '2025-05-05T16:01:22.277497+00:00',
                'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565',
                'channel_versions': {'__start__': '00000000000000000000000000000001.0.7040775356287469'},
                'versions_seen': {'__input__': {}},
                'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}
            },
            metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': "hi! I'm bob"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},
            parent_config=None,
            pending_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': "hi! I'm bob"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call_model', None)]
        )
    ]
    python  theme={null}
thread_id = "1"
checkpointer.delete_thread(thread_id)
```

## Prebuilt memory tools

**LangMem** is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the [LangMem documentation](https://langchain-ai.github.io/langmem/) for usage examples.

## Database management

If you are using any database-backed persistence implementation (such as Postgres or Redis) to store short and/or long-term memory, you will need to run migrations to set up the required schema before you can use it with your database.

By convention, most database-specific libraries define a `setup()` method on the checkpointer or store instance that runs the required migrations. However, you should check with your specific implementation of [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) or [`BaseStore`](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) to confirm the exact method name and usage.

We recommend running migrations as a dedicated deployment step, or you can ensure they're run as part of server startup.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Long-term memory with semantic search">
```

Example 2 (unknown):
```unknown
</Accordion>

## Manage short-term memory

With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:

* [Trim messages](#trim-messages): Remove first or last N messages (before calling LLM)
* [Delete messages](#delete-messages) from LangGraph state permanently
* [Summarize messages](#summarize-messages): Summarize earlier messages in the history and replace them with a summary
* [Manage checkpoints](#manage-checkpoints) to store and retrieve message history
* Custom strategies (e.g., message filtering, etc.)

This allows the agent to keep track of the conversation without exceeding the LLM's context window.

### Trim messages

Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.

To trim message history, use the [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function:
```

Example 3 (unknown):
```unknown
<Accordion title="Full example: trim messages">
```

Example 4 (unknown):
```unknown

```

---

## Create task

**URL:** llms-txt#create-task

task = PipelineTask(
    pipeline,
    params=PipelineParams(enable_metrics=True),
    enable_tracing=True,
    enable_turn_tracking=True,  # Required for turn audio recording
    conversation_id=conversation_id,
)

---

## Create the dataset

**URL:** llms-txt#create-the-dataset

ls_client = Client()
dataset_name = "attachment-test-dataset"
dataset = ls_client.create_dataset(
  dataset_name=dataset_name,
  description="Test dataset for evals with publicly available attachments",
)

inputs = {
  "audio_question": "What is in this audio clip?",
  "image_question": "What is in this image?",
}

outputs = {
  "audio_answer": "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",
  "image_answer": "A mug with a blanket over it.",
}

---

## Create the example

**URL:** llms-txt#create-the-example

**Contents:**
- 2. Run evaluations
  - Define a target function

ls_client.create_examples(
  dataset_id=dataset.id,
  examples=[example],
  # Uncomment this flag if you'd like to upload attachments from local files:
  # dangerously_allow_filesystem=True
)
typescript  theme={null}
import { Client } from "langsmith";
import { v4 as uuid4 } from "uuid";

// Publicly available test files
const pdfUrl = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf";
const wavUrl = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav";
const pngUrl = "https://www.w3.org/Graphics/PNG/nurbcup2si.png";

// Helper function to fetch file as ArrayBuffer
async function fetchArrayBuffer(url: string): Promise<ArrayBuffer> {
  const response = await fetch(url);
  if (!response.ok) {
    throw new Error(`Failed to fetch ${url}: ${response.statusText}`);
  }
  return response.arrayBuffer();
}

// Fetch files as ArrayBuffer
const pdfArrayBuffer = await fetchArrayBuffer(pdfUrl);
const wavArrayBuffer = await fetchArrayBuffer(wavUrl);
const pngArrayBuffer = await fetchArrayBuffer(pngUrl);

// Create the LangSmith client (Ensure LANGSMITH_API_KEY is set in env)
const langsmithClient = new Client();

// Create a unique dataset name
const datasetName = "attachment-test-dataset:" + uuid4().substring(0, 8);

// Create the dataset
const dataset = await langsmithClient.createDataset(datasetName, {
  description: "Test dataset for evals with publicly available attachments",
});

// Define the example with attachments
const exampleId = uuid4();
const example = {
  id: exampleId,
  inputs: {
      audio_question: "What is in this audio clip?",
      image_question: "What is in this image?",
  },
  outputs: {
      audio_answer: "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",
      image_answer: "A mug with a blanket over it.",
  },
  attachments: {
    my_pdf: {
      mimeType: "application/pdf",
      data: pdfArrayBuffer
    },
    my_wav: {
      mimeType: "audio/wav",
      data: wavArrayBuffer
    },
    my_img: {
      mimeType: "image/png",
      data: pngArrayBuffer
    },
  },
};

// Upload the example with attachments to the dataset
await langsmithClient.uploadExamplesMultipart(dataset.id, [example]);
python  theme={null}
  client.create_examples(..., dangerously_allow_filesystem=True)
  python  theme={null}
{
    "presigned_url": str,
    "mime_type": str,
    "reader": BinaryIO
}
python  theme={null}
from langsmith.wrappers import wrap_openai
import base64
from openai import OpenAI

client = wrap_openai(OpenAI())

**Examples:**

Example 1 (unknown):
```unknown
#### TypeScript

Requires version >= 0.2.13

You can use the `uploadExamplesMultipart` method to upload examples with attachments.

Note that this is a different method from the standard `createExamples` method, which currently does not support attachments. Each attachment requires either a `Uint8Array` or an `ArrayBuffer` as the data type.

* `Uint8Array`: Useful for handling binary data directly.
* `ArrayBuffer`: Represents fixed-length binary data, which can be converted to `Uint8Array` as needed.

Note that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.
```

Example 2 (unknown):
```unknown
<Info>
  Along with being passed in as bytes, attachments can be specified as paths to local files. To do so pass in a path for the attachment `data` value and specify arg `dangerously_allow_filesystem=True`:
```

Example 3 (unknown):
```unknown
</Info>

## 2. Run evaluations

### Define a target function

Now that we have a dataset that includes examples with attachments, we can define a target function to run over these examples. The following example simply uses OpenAI's GPT-4o model to answer questions about an image and an audio clip.

#### Python

The target function you are evaluating must have two positional arguments in order to consume the attachments associated with the example, the first must be called `inputs` and the second must be called `attachments`.

* The `inputs` argument is a dictionary that contains the input data for the example, excluding the attachments.
* The `attachments` argument is a dictionary that maps the attachment name to a dictionary containing a presigned url, mime\_type, and a reader of the bytes content of the file. You can use either the presigned url or the reader to get the file contents. Each value in the attachments dictionary is a dictionary with the following structure:
```

Example 4 (unknown):
```unknown

```

---

## Create Thread

**URL:** llms-txt#create-thread

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/create-thread

langsmith/agent-server-openapi.json post /threads
Create a thread.

---

## Create Thread Cron

**URL:** llms-txt#create-thread-cron

Source: https://docs.langchain.com/langsmith/agent-server-api/crons-plus-tier/create-thread-cron

langsmith/agent-server-openapi.json post /threads/{thread_id}/runs/crons
Create a cron to schedule runs on a thread.

---

## Create turn audio recorder

**URL:** llms-txt#create-turn-audio-recorder

turn_audio_recorder = TurnAudioRecorder(
    span_processor=span_processor,
    conversation_id=conversation_id,
    recordings_dir=recordings_dir,
    turn_tracker=None,  # Will be set after task creation
)

---

## Create two test users

**URL:** llms-txt#create-two-test-users

print(f"Creating test users: {email1} and {email2}")
await sign_up(email1, password)
await sign_up(email2, password)
python  theme={null}
async def login(email: str, password: str):
    """Get an access token for an existing user."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{SUPABASE_URL}/auth/v1/token?grant_type=password",
            json={
                "email": email,
                "password": password
            },
            headers={
                "apikey": SUPABASE_ANON_KEY,
                "Content-Type": "application/json"
            },
        )
        assert response.status_code == 200
        return response.json()["access_token"]

**Examples:**

Example 1 (unknown):
```unknown
⚠️ Before continuing: Check your email and click both confirmation links. Supabase will reject `/login` requests until after you have confirmed your users' email.

Now test that users can only see their own data. Make sure the server is running (run `langgraph dev`) before proceeding. The following snippet requires the "anon public" key that you copied from the Supabase dashboard while [setting up the auth provider](#setup-auth-provider) previously.
```

---

## Create your underlying embeddings model

**URL:** llms-txt#create-your-underlying-embeddings-model

underlying_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc.

---

## Customize user management

**URL:** llms-txt#customize-user-management

**Contents:**
- Features
  - Workspace level invites to an organization
  - SSO New Member Login Flow
  - Disabling Organization Creating
  - Disabling Personal Organizations

Source: https://docs.langchain.com/langsmith/self-host-user-management

<Note>
  This guide assumes you have read the [admin guide](/langsmith/administration-overview) and [organization setup guide](/langsmith/set-up-a-workspace#set-up-an-organization).
</Note>

LangSmith offers additional customization features for user management using feature flags.

### Workspace level invites to an organization

The default behavior in LangSmith requires a user to be an Organization Admin in order to invite new users to an organization. For self-hosted customers that would like to delegate this responsibility to workspace Admins, a feature flag may be set that enables workspace Admins to invite new users to the organization as well as their specific workspace **at the workspace level**.

Once this feature is enabled via the configuration option below, workspace Admins may add new users in the `Workspace members` tab under `Settings` > `Workspaces`. Both of the following cases are supported when inviting at the workspace level, while the organization level invite functions the same as before.

1. Invite users who are NOT already active in the organization: this will add the users as pending to the organization and specific workspace
2. Invite users who ARE already active in the organization: adds the users directly to the workspace as an active member (no pending state).

Admins may invite users for both cases at the same time.

### SSO New Member Login Flow

As of helm **v0.11.10**, self-hosted deployments using OAuth SSO will no longer need to manually add members in LangSmith settings for them to join. Deployments will have a <b>default</b> organization, to which new users will automatically be added upon their first login to LangSmith.

For your **default** organization, you can set which workspace(s) and workspace role is assigned to new members. For **non-default** organizations, the invitation flow remains the same.
Once a user joins an organization, any changes to their workspaces or roles beyond the default organization settings must be managed either through LangSmith settings (as before) or via SCIM.

<Note>
  By default, all new users are added to the organization’s initially provisioned workspace (**Workspace 1** by default) with the **Workspace Editor** role.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=e7274ed7fdd47fe7c4c1f514d78f3ac7" alt="Update SSO Member Settings" data-og-width="1769" width="1769" data-og-height="1251" height="1251" data-path="langsmith/images/sso-member-settings-update.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=280&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=2acde74eb4c622771decfe6d750f7c35 280w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=560&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=47bcef317de7189eda2743a66ead2070 560w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=840&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=eaeb449a5dc84d481fee92b7bdd0e163 840w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1100&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=dd18c7a8aa8ee1fc2cce7d418334c713 1100w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1650&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=a7d778198cdb93c3813b92341fc08b70 1650w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=2500&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=6029314be79253612b1465581144b170 2500w" />

<Note>
  To change your default organization, use **Set Default Organization** in the organization selector dropdown. (Org Admin permissions required in both the source and target organization.)
</Note>

### Disabling Organization Creating

By default, any user can create an organization in LangSmith. For self-hosted customers, an admin may want to restrict this ability after setting up initial organizations. This feature flag allows an admin to disable the ability for users to create new organizations.

<Note>
  The `userOrgCreationDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

### Disabling Personal Organizations

By default, any user who logs in to LangSmith will have a personal organization created for them. For self-hosted customers, an admin may want to restrict this ability. This feature flag allows an admin to disable the ability for users to create personal organizations.

<Note>
  The `personalOrgsDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-user-management.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### SSO New Member Login Flow

As of helm **v0.11.10**, self-hosted deployments using OAuth SSO will no longer need to manually add members in LangSmith settings for them to join. Deployments will have a <b>default</b> organization, to which new users will automatically be added upon their first login to LangSmith.

For your **default** organization, you can set which workspace(s) and workspace role is assigned to new members. For **non-default** organizations, the invitation flow remains the same.
Once a user joins an organization, any changes to their workspaces or roles beyond the default organization settings must be managed either through LangSmith settings (as before) or via SCIM.

<Note>
  By default, all new users are added to the organization’s initially provisioned workspace (**Workspace 1** by default) with the **Workspace Editor** role.
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=e7274ed7fdd47fe7c4c1f514d78f3ac7" alt="Update SSO Member Settings" data-og-width="1769" width="1769" data-og-height="1251" height="1251" data-path="langsmith/images/sso-member-settings-update.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=280&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=2acde74eb4c622771decfe6d750f7c35 280w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=560&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=47bcef317de7189eda2743a66ead2070 560w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=840&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=eaeb449a5dc84d481fee92b7bdd0e163 840w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1100&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=dd18c7a8aa8ee1fc2cce7d418334c713 1100w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=1650&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=a7d778198cdb93c3813b92341fc08b70 1650w, https://mintcdn.com/langchain-5e9cc07a/QEp_iTXiY5U9rQvE/langsmith/images/sso-member-settings-update.png?w=2500&fit=max&auto=format&n=QEp_iTXiY5U9rQvE&q=85&s=6029314be79253612b1465581144b170 2500w" />

<Note>
  To change your default organization, use **Set Default Organization** in the organization selector dropdown. (Org Admin permissions required in both the source and target organization.)
</Note>

### Disabling Organization Creating

By default, any user can create an organization in LangSmith. For self-hosted customers, an admin may want to restrict this ability after setting up initial organizations. This feature flag allows an admin to disable the ability for users to create new organizations.

#### Configuration

<Note>
  The `userOrgCreationDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Disabling Personal Organizations

By default, any user who logs in to LangSmith will have a personal organization created for them. For self-hosted customers, an admin may want to restrict this ability. This feature flag allows an admin to disable the ability for users to create personal organizations.

#### Configuration

<Note>
  The `personalOrgsDisabled` feature flag is set to `true` by default for organizations using [basic auth](/langsmith/self-host-basic-auth) or [SSO](/langsmith/self-host-sso).
</Note>

<CodeGroup>
```

---

## Custom instrumentation

**URL:** llms-txt#custom-instrumentation

**Contents:**
- Use `@traceable` / `traceable`
- Use the `trace` context manager (Python only)
- Use the `RunTree` API
- Example usage

Source: https://docs.langchain.com/langsmith/annotate-code

<Note>
  If you've decided you no longer want to trace your runs, you can remove the `LANGSMITH_TRACING` environment variable. Note that this does not affect the `RunTree` objects or API users, as these are meant to be low-level and not affected by the tracing toggle.
</Note>

There are several ways to log traces to LangSmith.

<Check>
  If you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the [LangChain-specific instructions](/langsmith/trace-with-langchain).
</Check>

## Use `@traceable` / `traceable`

LangSmith makes it easy to log traces with minimal changes to your existing code with the `@traceable` decorator in Python and `traceable` function in TypeScript.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `@traceable` or `traceable`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

The `@traceable` decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with `@traceable`.

Note that when wrapping a sync function with `traceable`, (e.g. `formatPrompt` in the example below), you should use the `await` keyword when calling it to
ensure the trace is logged correctly.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-code-trace.gif?s=bb81d0cb45382f2d793d43624db6e9ba" alt="Annotate code trace" data-og-width="822" width="822" data-og-height="480" height="480" data-path="langsmith/images/annotate-code-trace.gif" data-optimize="true" data-opv="3" />

## Use the `trace` context manager (Python only)

In Python, you can use the `trace` context manager to log traces to LangSmith. This is useful in situations where:

1. You want to log traces for a specific block of code.
2. You want control over the inputs, outputs, and other attributes of the trace.
3. It is not feasible to use a decorator or wrapper.
4. Any or all of the above.

The context manager integrates seamlessly with the `traceable` decorator and `wrap_openai` wrapper, so you can use them together in the same application.

## Use the `RunTree` API

Another, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method.

This method is not recommended, as it's easier to make mistakes in propagating trace context.

You can extend the utilities above to conveniently trace any code. Below are some example extensions:

Trace any public method in a class:

```python  theme={null}
from typing import Any, Callable, Type, TypeVar

def traceable_cls(cls: Type[T]) -> Type[T]:
    """Instrument all public methods in a class."""
    def wrap_method(name: str, method: Any) -> Any:
        if callable(method) and not name.startswith("__"):
            return traceable(name=f"{cls.__name__}.{name}")(method)
        return method

# Handle __dict__ case
    for name in dir(cls):
        if not name.startswith("_"):
            try:
                method = getattr(cls, name)
                setattr(cls, name, wrap_method(name, method))
            except AttributeError:
                # Skip attributes that can't be set (e.g., some descriptors)
                pass

# Handle __slots__ case
    if hasattr(cls, "__slots__"):
        for slot in cls.__slots__:  # type: ignore[attr-defined]
            if not slot.startswith("__"):
                try:
                    method = getattr(cls, slot)
                    setattr(cls, slot, wrap_method(slot, method))
                except AttributeError:
                    # Skip slots that don't have a value yet
                    pass

@traceable_cls
class MyClass:
    def __init__(self, some_val: int):
        self.some_val = some_val

def combine(self, other_val: int):
        return self.some_val + other_val

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-code-trace.gif?s=bb81d0cb45382f2d793d43624db6e9ba" alt="Annotate code trace" data-og-width="822" width="822" data-og-height="480" height="480" data-path="langsmith/images/annotate-code-trace.gif" data-optimize="true" data-opv="3" />

## Use the `trace` context manager (Python only)

In Python, you can use the `trace` context manager to log traces to LangSmith. This is useful in situations where:

1. You want to log traces for a specific block of code.
2. You want control over the inputs, outputs, and other attributes of the trace.
3. It is not feasible to use a decorator or wrapper.
4. Any or all of the above.

The context manager integrates seamlessly with the `traceable` decorator and `wrap_openai` wrapper, so you can use them together in the same application.
```

Example 3 (unknown):
```unknown
## Use the `RunTree` API

Another, more explicit way to log traces to LangSmith is via the `RunTree` API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method.

This method is not recommended, as it's easier to make mistakes in propagating trace context.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Custom middleware

**URL:** llms-txt#custom-middleware

**Contents:**
- Hooks
  - Node-style hooks
  - Wrap-style hooks
- Create middleware
  - Decorator-based middleware
  - Class-based middleware
- Custom state schema
- Execution order
- Agent jumps
- Best practices

Source: https://docs.langchain.com/oss/python/langchain/middleware/custom

Build custom middleware by implementing hooks that run at specific points in the agent execution flow.

Middleware provides two styles of hooks to intercept agent execution:

<CardGroup cols={2}>
  <Card title="Node-style hooks" icon="share-nodes" href="#node-style-hooks">
    Run sequentially at specific execution points.
  </Card>

<Card title="Wrap-style hooks" icon="container-storage" href="#wrap-style-hooks">
    Run around each model or tool call.
  </Card>
</CardGroup>

Run sequentially at specific execution points. Use for logging, validation, and state updates.

* `before_agent` - Before agent starts (once per invocation)
* `before_model` - Before each model call
* `after_model` - After each model response
* `after_agent` - After agent completes (once per invocation)

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

Intercept execution and control when the handler is called. Use for retries, caching, and transformation.

You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).

* `wrap_model_call` - Around each model call
* `wrap_tool_call` - Around each tool call

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

You can create middleware in two ways:

<CardGroup cols={2}>
  <Card title="Decorator-based middleware" icon="at" href="#decorator-based-middleware">
    Quick and simple for single-hook middleware. Use decorators to wrap individual functions.
  </Card>

<Card title="Class-based middleware" icon="brackets-curly" href="#class-based-middleware">
    More powerful for complex middleware with multiple hooks or configuration.
  </Card>
</CardGroup>

### Decorator-based middleware

Quick and simple for single-hook middleware. Use decorators to wrap individual functions.

**Available decorators:**

* [`@before_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_agent) - Runs before agent starts (once per invocation)
* [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) - Runs before each model call
* [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model) - Runs after each model response
* [`@after_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_agent) - Runs after agent completes (once per invocation)

* [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) - Wraps each model call with custom logic
* [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call) - Wraps each tool call with custom logic

* [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) - Generates dynamic system prompts

**When to use decorators:**

* Single hook needed
* No complex configuration
* Quick prototyping

### Class-based middleware

More powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.

**When to use classes:**

* Defining both sync and async implementations for the same hook
* Multiple hooks needed in a single middleware
* Complex configuration required (e.g., configurable thresholds, custom models)
* Reuse across projects with init-time configuration

## Custom state schema

Middleware can extend the agent's state with custom properties. This enables middleware to:

* **Track state across execution**: Maintain counters, flags, or other values that persist throughout the agent's execution lifecycle

* **Share data between hooks**: Pass information from `before_model` to `after_model` or between different middleware instances

* **Implement cross-cutting concerns**: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic

* **Make conditional decisions**: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

When using multiple middleware, understand how they execute:

<Accordion title="Execution flow">
  **Before hooks run in order:**

1. `middleware1.before_agent()`
  2. `middleware2.before_agent()`
  3. `middleware3.before_agent()`

**Agent loop starts**

4. `middleware1.before_model()`
  5. `middleware2.before_model()`
  6. `middleware3.before_model()`

**Wrap hooks nest like function calls:**

7. `middleware1.wrap_model_call()` → `middleware2.wrap_model_call()` → `middleware3.wrap_model_call()` → model

**After hooks run in reverse order:**

8. `middleware3.after_model()`
  9. `middleware2.after_model()`
  10. `middleware1.after_model()`

11. `middleware3.after_agent()`
  12. `middleware2.after_agent()`
  13. `middleware1.after_agent()`
</Accordion>

* `before_*` hooks: First to last
* `after_*` hooks: Last to first (reverse)
* `wrap_*` hooks: Nested (first middleware wraps all others)

To exit early from middleware, return a dictionary with `jump_to`:

**Available jump targets:**

* `'end'`: Jump to the end of the agent execution (or the first `after_agent` hook)
* `'tools'`: Jump to the tools node
* `'model'`: Jump to the model node (or the first `before_model` hook)

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

1. Keep middleware focused - each should do one thing well
2. Handle errors gracefully - don't let middleware errors crash the agent
3. **Use appropriate hook types**:
   * Node-style for sequential logic (logging, validation)
   * Wrap-style for control flow (retry, fallback, caching)
4. Clearly document any custom state properties
5. Unit test middleware independently before integrating
6. Consider execution order - place critical middleware first in the list
7. Use built-in middleware when possible

### Dynamic model selection

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

### Tool call monitoring

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

### Dynamically selecting tools

Select relevant tools at runtime to improve performance and accuracy.

* **Shorter prompts** - Reduce complexity by exposing only relevant tools
* **Better accuracy** - Models choose correctly from fewer options
* **Permission control** - Dynamically filter tools based on user access

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

### Working with system messages

Modify system messages in middleware using the `system_message` field on `ModelRequest`. The `system_message` field contains a [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) object (even if the agent was created with a string `system_prompt`).

**Example: Adding context to system message**

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

**Example: Working with cache control (Anthropic)**

When working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:

<Tabs>
  <Tab title="Decorator">
    
  </Tab>

<Tab title="Class">
    
  </Tab>
</Tabs>

* `ModelRequest.system_message` is always a [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) object, even if the agent was created with `system_prompt="string"`
* Use `SystemMessage.content_blocks` to access content as a list of blocks, regardless of whether the original content was a string or list
* When modifying system messages, use `content_blocks` and append new blocks to preserve existing structure
* You can pass [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) objects directly to `create_agent`'s `system_prompt` parameter for advanced use cases like cache control

## Additional resources

* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/)
* [Built-in middleware](/oss/python/langchain/middleware/built-in)
* [Testing agents](/oss/python/langchain/test)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/custom.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Class">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

### Wrap-style hooks

Intercept execution and control when the handler is called. Use for retries, caching, and transformation.

You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).

**Available hooks:**

* `wrap_model_call` - Around each model call
* `wrap_tool_call` - Around each tool call

**Example:**

<Tabs>
  <Tab title="Decorator">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Class">
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

## Create middleware

You can create middleware in two ways:

<CardGroup cols={2}>
  <Card title="Decorator-based middleware" icon="at" href="#decorator-based-middleware">
    Quick and simple for single-hook middleware. Use decorators to wrap individual functions.
  </Card>

  <Card title="Class-based middleware" icon="brackets-curly" href="#class-based-middleware">
    More powerful for complex middleware with multiple hooks or configuration.
  </Card>
</CardGroup>

### Decorator-based middleware

Quick and simple for single-hook middleware. Use decorators to wrap individual functions.

**Available decorators:**

**Node-style:**

* [`@before_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_agent) - Runs before agent starts (once per invocation)
* [`@before_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before_model) - Runs before each model call
* [`@after_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_model) - Runs after each model response
* [`@after_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after_agent) - Runs after agent completes (once per invocation)

**Wrap-style:**

* [`@wrap_model_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_model_call) - Wraps each model call with custom logic
* [`@wrap_tool_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap_tool_call) - Wraps each tool call with custom logic

**Convenience:**

* [`@dynamic_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic_prompt) - Generates dynamic system prompts

**Example:**
```

---

## Custom output rendering

**URL:** llms-txt#custom-output-rendering

**Contents:**
- Configure custom output rendering
  - For tracing projects
  - For datasets
  - For annotation queues
- Build a custom renderer
  - Understand the message format
  - Example implementation
- Where custom rendering appears

Source: https://docs.langchain.com/langsmith/custom-output-rendering

Custom output rendering allows you to visualize run outputs and dataset reference outputs using your own custom HTML pages. This is particularly useful for:

* **Domain-specific formatting**: Display medical records, legal documents, or other specialized data types in their native format.
* **Custom visualizations**: Create charts, graphs, or diagrams from numeric or structured output data.

In this page you'll learn how to:

* **[Configure custom rendering](#configure-custom-output-rendering)** in the LangSmith UI.
* **[Build a custom renderer](#build-a-custom-renderer)** to display output data.
* **[Understand where custom rendering appears](#where-custom-rendering-appears)** in LangSmith.

## Configure custom output rendering

Configure custom rendering at two levels:

* **For datasets**: Apply custom rendering to all runs associated with that dataset, wherever they appear—in experiments, run detail panes, or annotation queues.
* **For annotation queues**: Apply custom rendering to all runs within a specific annotation queue, regardless of which dataset they come from. This takes precedence over dataset-level configuration.

### For tracing projects

To configure custom output rendering for a tracing project:

<img src="https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=034f982fa3174c1649e5188bbb11ca03" alt="Tracing project settings showing custom output rendering configuration" data-og-width="1325" width="1325" data-og-height="1207" height="1207" data-path="langsmith/images/tracing-project-custom-output-rendering-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=280&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=9192b74a8df6dac268f17acee40817ab 280w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=560&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=106cf65662788e212505ac39d46d60ab 560w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=840&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=247529f2b0a3326a346145b9476cb709 840w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=1100&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=efcd6238cd47a17055a7ebcc214b3b30 1100w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=1650&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=4e0a42d7a9d939b0cd2731814116cba9 1650w, https://mintcdn.com/langchain-5e9cc07a/oyRHf9tRXOU-EPbv/langsmith/images/tracing-project-custom-output-rendering-settings.png?w=2500&fit=max&auto=format&n=oyRHf9tRXOU-EPbv&q=85&s=2519e4389455995b458e317fc23caa83 2500w" />

1. Navigate to the **Tracing Projects** page.
2. Click on an existing tracing project or create a new one.
3. In the edit tracing project pane, scroll to the **Custom Output Rendering** section.
4. Toggle **Enable custom output rendering**.
5. Enter the webpage URL in the **URL** field.
6. Click **Save**.

To configure custom output rendering for a dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=7daf042ebae80eec20cd90a25c1d6087" alt="Dataset page with three-dot menu showing Custom Output Rendering option" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-menu.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=432f6a5bb6c79797c17fa8faa74169b1 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c77ec3be9ea681180d5c82d2588814b5 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3f8fdb966f809ad049585efe3271182b 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3bdd2738b8dc1d17eeea3951536dedc8 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=5428362058ab8e8f4f71009779e3134f 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-menu.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a31530b7bd16f2fe6f8c6b622f55d9de 2500w" />

1. Navigate to your dataset in the **Datasets & Experiments** page.
2. Click **⋮** (three-dot menu) in the top right corner.
3. Select **Custom Output Rendering**.
4. Toggle **Enable custom output rendering**.
5. Enter the webpage URL in the **URL** field.
6. Click **Save**.

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=bffd3b40ca14bbebc05c998d1cb5fa7e" alt="Custom Output Rendering modal with fields filled in" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-modal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=2fd8bdfcce6fdf5e9bd865590a8e0f79 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c110a35a03909e146be961ac5386c888 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1959a6a045a04e0d3290de4379a358bc 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=4a154ab114547ff7e243acb36c9bd9a3 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=c2eae83b1d6ec160a37fd998511e9794 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-modal.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=025b243cf994c130439ef55f6dc5e856 2500w" />

### For annotation queues

To configure custom output rendering for an annotation queue:

<img src="https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=579aad04fa6990b220514280eef799f4" alt="Annotation queue settings showing custom output rendering configuration" data-og-width="3456" width="3456" data-og-height="1914" height="1914" data-path="langsmith/images/annotation-queue-custom-output-rendering-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=280&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=525d5f11afb5bf8a5fd86f5de5063d72 280w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=560&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=0d58600ddf45ff1e7bbad5cc31489f24 560w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=840&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=d5aaa11700f5f58bcf157dc0fd32877a 840w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=1100&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=51794d75925516c728177e91689dbdcd 1100w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=1650&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=cf7c47a38a18a548f627dc56e94abfde 1650w, https://mintcdn.com/langchain-5e9cc07a/optUJrLvYf4z4j5I/langsmith/images/annotation-queue-custom-output-rendering-settings.png?w=2500&fit=max&auto=format&n=optUJrLvYf4z4j5I&q=85&s=bcd50a9547635188f2e7e458af0baa46 2500w" />

1. Navigate to the **Annotation Queues** page.
2. Click on an existing annotation queue or create a new one.
3. In the annotation queue settings pane, scroll to the **Custom Output Rendering** section.
4. Toggle **Enable custom output rendering**.
5. Enter the webpage URL in the **URL** field.
6. Click **Save** or **Create**.

<Info>When custom rendering settings are applied at multiple levels, the precedence is as follows: annotation queue > dataset > tracing project.</Info>

## Build a custom renderer

### Understand the message format

Your HTML page will receive output data via the [postMessage API](https://developer.mozilla.org/en-US/docs/Web/API/Window/postMessage). LangSmith sends messages with the following structure:

* `type`: Indicates whether this is an actual output (`"output"`) or a reference output (`"reference"`).
* `data`: The output data itself.
* `metadata.inputs`: The input data that generated this output, provided for context.

<Note>**Message delivery timing**: LangSmith uses an exponential backoff retry mechanism to ensure your page receives the data even if it loads slowly. Messages are sent up to 6 times with increasing delays (100ms, 200ms, 400ms, 800ms, 1600ms, 3200ms).</Note>

### Example implementation

This example listens for incoming postMessage events and displays them on the page. Each message is numbered and formatted as JSON, making it easy to inspect the data structure LangSmith sends to your renderer.

## Where custom rendering appears

When enabled, your custom rendering will replace the default output view in:

* **Experiment comparison view**: When comparing outputs across multiple experiments:

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=6f1fd9d3ca4be55aa9a0b40140771e08" alt="Experiment comparison view showing custom rendering" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-experiment-comparison.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=4010749197ec99216b6cbc004f8e2aed 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=3eb5a8825d76c92f531c34912c476fb5 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1977c3020b1b2150beefc4980db857f9 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=2f8f86c1ad7ff02b65f78cd43bead446 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=979e5426ee86673cd088aef614c52a6e 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-experiment-comparison.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=292b4ac1322414a059b2ba5a4285caf3 2500w" />

* **Run detail panes**: When viewing runs that are associated with a dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=abec759e27bb3dfa827354d13746cf61" alt="Run detail pane showing custom rendering" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-run-details.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=9d4d8e9e4d3e2f9e856d9da23e16d805 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=1adfc78a0ba06a3b9b18433fc2e1f82c 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a882e8fb03c81526070c0e9f49de157b 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=39d6a85d5578c58cf2c6853ce1d5e704 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=d9c37910f147b76685b018a620aa9d40 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-run-details.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=24538e055a2363976a7f3ce5d0111356 2500w" />

* **Annotation queues**: When reviewing runs in annotation queues:

<img src="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=8d1b66541ea7dcd0246354fca1568719" alt="Annotation queue showing custom rendering" data-og-width="3456" width="3456" data-og-height="2156" height="2156" data-path="langsmith/images/custom-output-rendering-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=280&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a42373180e5ab88ee80c3a670102f290 280w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=560&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=47be4504dd9d4a0c191cc426d7eaa7c4 560w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=840&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=36eda0a27d923057fbb4f0fc661e311e 840w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=1100&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=82593a5f663686eb61d9b90d9b1b8c39 1100w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=1650&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=dec8e45c1c77bd1cdd9b3aec093bf566 1650w, https://mintcdn.com/langchain-5e9cc07a/l7rhdSRpjWBkaCke/langsmith/images/custom-output-rendering-annotation-queue.png?w=2500&fit=max&auto=format&n=l7rhdSRpjWBkaCke&q=85&s=a8a978b3ec3d119fb16002d1a432c91a 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-output-rendering.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
* `type`: Indicates whether this is an actual output (`"output"`) or a reference output (`"reference"`).
* `data`: The output data itself.
* `metadata.inputs`: The input data that generated this output, provided for context.

<Note>**Message delivery timing**: LangSmith uses an exponential backoff retry mechanism to ensure your page receives the data even if it loads slowly. Messages are sent up to 6 times with increasing delays (100ms, 200ms, 400ms, 800ms, 1600ms, 3200ms).</Note>

### Example implementation

This example listens for incoming postMessage events and displays them on the page. Each message is numbered and formatted as JSON, making it easy to inspect the data structure LangSmith sends to your renderer.
```

---

## Custom workflow

**URL:** llms-txt#custom-workflow

**Contents:**
- Key characteristics
- When to use
- Basic implementation

Source: https://docs.langchain.com/oss/python/langchain/multi-agent/custom-workflow

In the **custom workflow** architecture, you define your own bespoke execution flow using [LangGraph](/oss/python/langgraph/overview). You have complete control over the graph structure—including sequential steps, conditional branches, loops, and parallel execution.

## Key characteristics

* Complete control over graph structure
* Mix deterministic logic with agentic behavior
* Support for sequential steps, conditional branches, loops, and parallel execution
* Embed other patterns as nodes in your workflow

Use custom workflows when standard patterns (subagents, skills, etc.) don't fit your requirements, you need to mix deterministic logic with agentic behavior, or your use case requires complex routing or multi-stage processing.

Each node in your workflow can be a simple function, an LLM call, or an entire [agent](/oss/python/langchain/agents) with [tools](/oss/python/langchain/tools). You can also compose other architectures within a custom workflow—for example, embedding a multi-agent system as a single node.

For a complete example of a custom workflow, see the tutorial below.

<Card title="Tutorial: Build a multi-source knowledge base with routing" icon="book" href="/oss/python/langchain/multi-agent/router-knowledge-base" arrow cta="Learn more">
  The [router pattern](/oss/python/langchain/multi-agent/router) is an example of a custom workflow. This tutorial walks through building a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results.

## Basic implementation

The core insight is that you can call a LangChain agent directly inside any LangGraph node, combining the flexibility of custom workflows with the convenience of pre-built agents:

```python  theme={null}
from langchain.agents import create_agent
from langgraph.graph import StateGraph, START, END

agent = create_agent(model="openai:gpt-4o", tools=[...])

def agent_node(state: State) -> dict:
    """A LangGraph node that invokes a LangChain agent."""
    result = agent.invoke({
        "messages": [{"role": "user", "content": state["query"]}]
    })
    return {"answer": result["messages"][-1].content}

**Examples:**

Example 1 (unknown):
```unknown
## Key characteristics

* Complete control over graph structure
* Mix deterministic logic with agentic behavior
* Support for sequential steps, conditional branches, loops, and parallel execution
* Embed other patterns as nodes in your workflow

## When to use

Use custom workflows when standard patterns (subagents, skills, etc.) don't fit your requirements, you need to mix deterministic logic with agentic behavior, or your use case requires complex routing or multi-stage processing.

Each node in your workflow can be a simple function, an LLM call, or an entire [agent](/oss/python/langchain/agents) with [tools](/oss/python/langchain/tools). You can also compose other architectures within a custom workflow—for example, embedding a multi-agent system as a single node.

For a complete example of a custom workflow, see the tutorial below.

<Card title="Tutorial: Build a multi-source knowledge base with routing" icon="book" href="/oss/python/langchain/multi-agent/router-knowledge-base" arrow cta="Learn more">
  The [router pattern](/oss/python/langchain/multi-agent/router) is an example of a custom workflow. This tutorial walks through building a router that queries GitHub, Notion, and Slack in parallel, then synthesizes results.

  >
</Card>

## Basic implementation

The core insight is that you can call a LangChain agent directly inside any LangGraph node, combining the flexibility of custom workflows with the convenience of pre-built agents:
```

---

## Dataset transformations

**URL:** llms-txt#dataset-transformations

**Contents:**
- Transformation types
- Chat Model prebuilt schema
  - Compatibility
  - Enablement
  - Specs

Source: https://docs.langchain.com/langsmith/dataset-transformations

LangSmith allows you to attach transformations to fields in your dataset's schema that apply to your data before it is added to your dataset, whether that be from UI, API, or run rules.

Coupled with [LangSmith's prebuilt JSON schema types](/langsmith/dataset-json-types), these allow you to do easy preprocessing of your data before saving it into your datasets.

## Transformation types

| Transformation Type         | Target Types                                                               | Functionality                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| --------------------------- | -------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `remove_system_messages`    | `Array[Message]`                                                           | Filters a list of messages to remove any system messages.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| `convert_to_openai_message` | Message `Array[Message]`                                                   | Converts any incoming data from LangChain's internal serialization format to OpenAI's standard message format using langchain's [`convert_to_openai_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.convert_to_openai_messages.html). If the target field is marked as required, and no matching message is found upon entry, it will attempt to extract a message (or list of messages) from several well-known LangSmith tracing formats (e.g., any traced LangChain [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel) run or traced run from the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client)), and remove the original key containing the message. |
| `convert_to_openai_tool`    | `Array[Tool]` Only available on top level fields in the inputs dictionary. | Converts any incoming data into OpenAI standard tool formats here using langchain's [`convert_to_openai_tool`](https://reference.langchain.com/python/langchain_core/utils/#langchain_core.utils.function_calling.convert_to_openai_tool) Will extract tool definitions from a run's invocation parameters if present / no tools are found at the specified key. This is useful because LangChain chat models trace tool definitions to the `extra.invocation_params` field of the run rather than inputs.                                                                                                                                                                                                                                                                                                                               |
| `remove_extra_fields`       | `Object`                                                                   | Removes any field not defined in the schema for this target object.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |

## Chat Model prebuilt schema

The main use case for transformations is to simplify collecting production traces into datasets in a format that can be standardized across model providers for usage in evaluations / few shot prompting / etc downstream.

To simplify setup of transformations for our end users, LangSmith offers a pre-defined schema that will do the following:

* Extract messages from your collected runs and transform them into the openai standard format, which makes them compatible all LangChain ChatModels and most model providers' SDK for downstream evaluation and experimentation
* Extract any tools used by your LLM and add them to your example's input to be used for reproducability in downstream evaluation

<Check>
  Users who want to iterate on their system prompts often also add the Remove System Messages transformation on their input messages when using our Chat Model schema, which will prevent you from saving the system prompt to your dataset.
</Check>

The LLM run collection schema is built to collect data from LangChain [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel) runs or traced runs from the [LangSmith OpenAI wrapper](/langsmith/annotate-code#wrap-the-openai-client).

Please contact support via [support.langchain.com](https://support.langchain.com) if you have an LLM run you are tracing that is not compatible and we can extend support.

If you want to apply transformations to other sorts of runs (for example, representing LangGraph state with message history), please define your schema directly and manually add the relevant transformations.

When adding a run from a tracing project or annotation queue to a dataset, if it has the LLM run type, we will apply the Chat Model schema by default.

For enablement on new datasets, see our [dataset management how-to guide](/langsmith/manage-datasets-in-application).

For the full API specs of the prebuilt schema, see the below sections:

And the transformations look as follows:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/dataset-transformations.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Output schema
```

Example 2 (unknown):
```unknown
#### Transformations

And the transformations look as follows:
```

---

## Data purging for compliance

**URL:** llms-txt#data-purging-for-compliance

**Contents:**
- Data retention
- Trace deletes
  - Deletion timeline
  - Delete specific traces
  - Delete by metadata
- Example deletes
  - Deleting examples is a two-step process
  - Deletion types

Source: https://docs.langchain.com/langsmith/data-purging-compliance

This guide covers the various features available after data reaches LangSmith Cloud servers to help you achieve your privacy goals.

LangSmith provides automatic data retention capabilities to help with compliance and storage management. Data retention policies can be configured at the organization and project levels.

For detailed information about data retention configuration and management, please refer to the [Data Retention concepts](/langsmith/administration-overview#data-retention) documentation.

You can use the API to complete trace deletes. The API supports two methods for deleting traces:

1. **By trace IDs and session ID**: Delete specific traces by providing a list of trace IDs and their corresponding session ID (up to 1000 traces per request)
2. **By metadata**: Delete traces across a workspace that match any of the specified metadata key-value pairs

For more details, refer to the [API spec](https://api.smith.langchain.com/redoc#tag/run/operation/delete_runs_api_v1_runs_delete_post).

<Warning>
  All trace deletions will delete related entities like feedbacks, aggregations, and stats across all data storages.
</Warning>

### Deletion timeline

Trace deletions are processed during non-peak usage times and are not instant. LangChain runs the delete job on the weekend. There is no confirmation of deletion - you'll need to query the data again to verify it has been removed.

### Delete specific traces

To delete specific traces by their trace IDs from a single session:

<Note>
  The `session_id` is the project ID for the trace you are trying to delete. You can find it on the tracing project page in the LangSmith UI.
</Note>

### Delete by metadata

When deleting by metadata:

* Accepts a `metadata` object of key/value pairs. KV pair matching uses an **or** condition. A trace will match if it has **any** of the key-value pairs specified in metadata (not all)
* You don't need to specify a session id when deleting by metadata. Deletes will apply across the workspace.

To delete traces based on metadata across a workspace (matches **any** of the metadata key-value pairs):

This will delete traces that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata.

<Warning>
  Remember that you can only schedule up to 1000 traces per session per request. For larger deletions, you'll need to make multiple requests.
</Warning>

You can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.

<Warning>
  Hard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.
</Warning>

### Deleting examples is a two-step process

For bulk operations, example deletion follows a two-step process:

#### 1. Search for examples by metadata

Find all examples with matching metadata across all datasets in a workspace.

[GET /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get)

* `as_of` must be explicitly specified as a timestamp. Only examples created before the `as_of` date will be returned

This will return examples that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata across all datasets in your workspace.

#### 2. Hard delete examples

Once you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example.

[POST /v1/platform/datasets/examples/delete/](https://api.smith.langchain.com/redoc?#tag/examples/paths/~1v1~1platform~1datasets~1examples~1delete/post)

* Specify `example_ids` (list of example IDs) and `hard_delete` (boolean) in the request body

#### Soft delete (default)

* Creates tombstoned entries with NULL inputs/outputs in the dataset
* Preserves historical data and maintains dataset versioning
* Only affects the current version of the dataset

* Permanently removes inputs, outputs, and metadata from ALL dataset versions
* Complete data removal when compliance requires zero-out across all versions
* Set `"hard_delete": true` in the request body

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-purging-compliance.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Delete by metadata

When deleting by metadata:

* Accepts a `metadata` object of key/value pairs. KV pair matching uses an **or** condition. A trace will match if it has **any** of the key-value pairs specified in metadata (not all)
* You don't need to specify a session id when deleting by metadata. Deletes will apply across the workspace.

To delete traces based on metadata across a workspace (matches **any** of the metadata key-value pairs):
```

Example 2 (unknown):
```unknown
This will delete traces that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata.

<Warning>
  Remember that you can only schedule up to 1000 traces per session per request. For larger deletions, you'll need to make multiple requests.
</Warning>

## Example deletes

You can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.

<Warning>
  Hard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.
</Warning>

### Deleting examples is a two-step process

For bulk operations, example deletion follows a two-step process:

#### 1. Search for examples by metadata

Find all examples with matching metadata across all datasets in a workspace.

[GET /examples](https://api.smith.langchain.com/redoc#tag/examples/operation/read_examples_api_v1_examples_get)

* `as_of` must be explicitly specified as a timestamp. Only examples created before the `as_of` date will be returned
```

Example 3 (unknown):
```unknown
This will return examples that have either `user_id: "user123"` **or** `environment: "staging"` in their metadata across all datasets in your workspace.

#### 2. Hard delete examples

Once you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example.

[POST /v1/platform/datasets/examples/delete/](https://api.smith.langchain.com/redoc?#tag/examples/paths/~1v1~1platform~1datasets~1examples~1delete/post)

* Specify `example_ids` (list of example IDs) and `hard_delete` (boolean) in the request body
```

---

## Data storage and privacy

**URL:** llms-txt#data-storage-and-privacy

**Contents:**
- CLI
- Agent Server
  - LangSmith Tracing
  - In-memory development server
  - Standalone Server
- Studio
- Quick reference

Source: https://docs.langchain.com/langsmith/data-storage-and-privacy

This document describes how data is processed in the LangGraph CLI and the Agent Server for both the in-memory server (`langgraph dev`) and the local Docker server (`langgraph up`). It also describes what data is tracked when interacting with the hosted Studio frontend.

LangGraph **CLI** is the command-line interface for building and running LangGraph applications; see the [CLI guide](/langsmith/cli) to learn more.

By default, calls to most CLI commands log a single analytics event upon invocation. This helps us better prioritize improvements to the CLI experience. Each telemetry event contains the calling process's OS, OS version, Python version, the CLI version, the command name (`dev`, `up`, `run`, etc.), and booleans representing whether a flag was passed to the command. You can see the full analytics logic [here](https://github.com/langchain-ai/langgraph/blob/main/libs/cli/langgraph-cli/analytics.py).

You can disable all CLI telemetry by setting `LANGGRAPH_CLI_NO_ANALYTICS=1`.

<a id="in-memory-docker" />

The [Agent Server](/langsmith/agent-server) provides a durable execution runtime that relies on persisting checkpoints of your application state, long-term memories, thread metadata, assistants, and similar resources to the local file system or a database. Unless you have deliberately customized the storage location, this information is either written to local disk (for `langgraph dev`) or a PostgreSQL database (for `langgraph up` and in all deployments).

### LangSmith Tracing

When running the Agent server (either in-memory or in Docker), LangSmith tracing may be enabled to facilitate faster debugging and offer observability of graph state and LLM prompts in production. You can always disable tracing by setting `LANGSMITH_TRACING=false` in your server's runtime environment.

<a id="langgraph-dev" />

### In-memory development server

`langgraph dev` runs an [in-memory development server](/langsmith/local-server) as a single Python process, designed for quick development and testing. It saves all checkpointing and memory data to disk within a `.langgraph_api` directory in the current working directory. Apart from the telemetry data described in the [CLI](#cli) section, no data leaves the machine unless you have enabled tracing or your graph code explicitly contacts an external service.

<a id="langgraph-up" />

### Standalone Server

`langgraph up` builds your local package into a Docker image and runs the server as the [data plane](/langsmith/self-hosted) consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid `LANGGRAPH_AES_KEY` environment variable. You can also specify [TTLs](/langsmith/configure-ttl) for checkpoints and cross-thread memories in `langgraph.json` to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.

Additional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).

If you've disabled [tracing](#langsmith-tracing), no user data is persisted externally unless your graph code explicitly contacts an external service.

[Studio](/langsmith/studio) is a graphical interface for interacting with your Agent Server. It does not persist any private data (the data you send to your server is not sent to LangSmith). Though the Studio interface is served at [smith.langchain.com](https://smith.langchain.com), it is run in your browser and connects directly to your local Agent Server so that no data needs to be sent to LangSmith.

If you are logged in, LangSmith does collect some usage analytics to help improve the debugging user experience. This includes:

* Page visits and navigation patterns
* User actions (button clicks)
* Browser type and version
* Screen resolution and viewport size

Importantly, no application data or code (or other sensitive configuration details) are collected. All of that is stored in the persistence layer of your Agent Server. When using Studio anonymously, no account creation is required and usage analytics are not collected.

In summary, you can opt-out of server-side telemetry by turning off CLI analytics and disabling tracing.

| Variable                       | Purpose                   | Default                |
| ------------------------------ | ------------------------- | ---------------------- |
| `LANGGRAPH_CLI_NO_ANALYTICS=1` | Disable CLI analytics     | Analytics enabled      |
| `LANGSMITH_API_KEY`            | Enable LangSmith tracing  | Tracing disabled       |
| `LANGSMITH_TRACING=false`      | Disable LangSmith tracing | Depends on environment |

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-storage-and-privacy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Decide whether to retrieve

**URL:** llms-txt#decide-whether-to-retrieve

workflow.add_conditional_edges(
    "generate_query_or_respond",
    # Assess LLM decision (call `retriever_tool` tool or respond to the user)
    tools_condition,
    {
        # Translate the condition outputs to nodes in our graph
        "tools": "retrieve",
        END: END,
    },
)

---

## Decline (user doesn't want to provide info)

**URL:** llms-txt#decline-(user-doesn't-want-to-provide-info)

ElicitResult(action="decline")

---

## Define an example with attachments

**URL:** llms-txt#define-an-example-with-attachments

example_id = uuid.uuid4()
example = {
  "id": example_id,
  "inputs": inputs,
  "outputs": outputs,
  "attachments": {
      "my_pdf": {"mime_type": "application/pdf", "data": pdf_bytes},
      "my_wav": {"mime_type": "audio/wav", "data": wav_bytes},
      "my_img": {"mime_type": "image/png", "data": img_bytes},
      # Example of an attachment specified via a local file path:
      # "my_local_img": {"mime_type": "image/png", "data": Path(__file__).parent / "my_local_img.png"},
  },
}

---

## Define a new graph

**URL:** llms-txt#define-a-new-graph

workflow = StateGraph(State)

---

## Define a tool

**URL:** llms-txt#define-a-tool

def multiply(a: int, b: int) -> int:
    return a * b

---

## Define dataset: these are your test cases

**URL:** llms-txt#define-dataset:-these-are-your-test-cases

**Contents:**
- Define metrics
- Run Evaluations
- Comparing results
- Set up automated testing to run in CI/CD
- Track results over time
- Conclusion
- Reference code

dataset_name = "QA Example Dataset"
dataset = client.create_dataset(dataset_name)

client.create_examples(
    dataset_id=dataset.id,
    examples=[
        {
            "inputs": {"question": "What is LangChain?"},
            "outputs": {"answer": "A framework for building LLM applications"},
        },
        {
            "inputs": {"question": "What is LangSmith?"},
            "outputs": {"answer": "A platform for observing and evaluating LLM applications"},
        },
        {
            "inputs": {"question": "What is OpenAI?"},
            "outputs": {"answer": "A company that creates Large Language Models"},
        },
        {
            "inputs": {"question": "What is Google?"},
            "outputs": {"answer": "A technology company known for search"},
        },
        {
            "inputs": {"question": "What is Mistral?"},
            "outputs": {"answer": "A company that creates Large Language Models"},
        }
    ]
)
python  theme={null}
import openai
from langsmith import wrappers

openai_client = wrappers.wrap_openai(openai.OpenAI())

eval_instructions = "You are an expert professor specialized in grading students' answers to questions."

def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    user_content = f"""You are grading the following question:
{inputs['question']}
Here is the real answer:
{reference_outputs['answer']}
You are grading the following predicted answer:
{outputs['response']}
Respond with CORRECT or INCORRECT:
Grade:"""
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        temperature=0,
        messages=[
            {"role": "system", "content": eval_instructions},
            {"role": "user", "content": user_content},
        ],
    ).choices[0].message.content
    return response == "CORRECT"
python  theme={null}
def concision(outputs: dict, reference_outputs: dict) -> bool:
    return int(len(outputs["response"]) < 2 * len(reference_outputs["answer"]))
python  theme={null}
default_instructions = "Respond to the users question in a short, concise manner (one short sentence)."

def my_app(question: str, model: str = "gpt-4o-mini", instructions: str = default_instructions) -> str:
    return openai_client.chat.completions.create(
        model=model,
        temperature=0,
        messages=[
            {"role": "system", "content": instructions},
            {"role": "user", "content": question},
        ],
    ).choices[0].message.content
python  theme={null}
def ls_target(inputs: str) -> dict:
    return {"response": my_app(inputs["question"])}
python  theme={null}
experiment_results = client.evaluate(
    ls_target, # Your AI system
    data=dataset_name, # The data to predict and grade over
    evaluators=[concision, correctness], # The evaluators to score the results
    experiment_prefix="openai-4o-mini", # A prefix for your experiment names to easily identify them
)
python  theme={null}
def ls_target_v2(inputs: str) -> dict:
    return {"response": my_app(inputs["question"], model="gpt-4-turbo")}

experiment_results = client.evaluate(
    ls_target_v2,
    data=dataset_name,
    evaluators=[concision, correctness],
    experiment_prefix="openai-4-turbo",
)
python  theme={null}
instructions_v3 = "Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."

def ls_target_v3(inputs: str) -> dict:
    response = my_app(
        inputs["question"],
        model="gpt-4-turbo",
        instructions=instructions_v3
    )
    return {"response": response}

experiment_results = client.evaluate(
    ls_target_v3,
    data=dataset_name,
    evaluators=[concision, correctness],
    experiment_prefix="strict-openai-4-turbo",
)
python  theme={null}
def test_length_score() -> None:
    """Test that the length score is at least 80%."""
    experiment_results = evaluate(
        ls_target, # Your AI system
        data=dataset_name, # The data to predict and grade over
        evaluators=[concision, correctness], # The evaluators to score the results
    )
    # This will be cleaned up in the next release:
    feedback = client.list_feedback(
        run_ids=[r.id for r in client.list_runs(project_name=experiment_results.experiment_name)],
        feedback_key="concision"
    )
    scores = [f.score for f in feedback]
    assert sum(scores) / len(scores) >= 0.8, "Aggregate score should be at least .8"
python  theme={null}
  import openai
  from langsmith import Client, wrappers

# Application code
  openai_client = wrappers.wrap_openai(openai.OpenAI())

default_instructions = "Respond to the users question in a short, concise manner (one short sentence)."

def my_app(question: str, model: str = "gpt-4o-mini", instructions: str = default_instructions) -> str:
      return openai_client.chat.completions.create(
          model=model,
          temperature=0,
          messages=[
              {"role": "system", "content": instructions},
              {"role": "user", "content": question},
          ],
      ).choices[0].message.content

# Define dataset: these are your test cases
  dataset_name = "QA Example Dataset"
  dataset = client.create_dataset(dataset_name)

client.create_examples(
      dataset_id=dataset.id,
      examples=[
          {
              "inputs": {"question": "What is LangChain?"},
              "outputs": {"answer": "A framework for building LLM applications"},
          },
          {
              "inputs": {"question": "What is LangSmith?"},
              "outputs": {"answer": "A platform for observing and evaluating LLM applications"},
          },
          {
              "inputs": {"question": "What is OpenAI?"},
              "outputs": {"answer": "A company that creates Large Language Models"},
          },
          {
              "inputs": {"question": "What is Google?"},
              "outputs": {"answer": "A technology company known for search"},
          },
          {
              "inputs": {"question": "What is Mistral?"},
              "outputs": {"answer": "A company that creates Large Language Models"},
          }
      ]
  )

# Define evaluators
  eval_instructions = "You are an expert professor specialized in grading students' answers to questions."

def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
      user_content = f"""You are grading the following question:
  {inputs['question']}
  Here is the real answer:
  {reference_outputs['answer']}
  You are grading the following predicted answer:
  {outputs['response']}
  Respond with CORRECT or INCORRECT:
  Grade:"""
      response = openai_client.chat.completions.create(
          model="gpt-4o-mini",
          temperature=0,
          messages=[
              {"role": "system", "content": eval_instructions},
              {"role": "user", "content": user_content},
          ],
      ).choices[0].message.content
      return response == "CORRECT"

def concision(outputs: dict, reference_outputs: dict) -> bool:
      return int(len(outputs["response"]) < 2 * len(reference_outputs["answer"]))

# Run evaluations
  def ls_target(inputs: str) -> dict:
      return {"response": my_app(inputs["question"])}

experiment_results_v1 = client.evaluate(
      ls_target, # Your AI system
      data=dataset_name, # The data to predict and grade over
      evaluators=[concision, correctness], # The evaluators to score the results
      experiment_prefix="openai-4o-mini", # A prefix for your experiment names to easily identify them
  )

def ls_target_v2(inputs: str) -> dict:
      return {"response": my_app(inputs["question"], model="gpt-4-turbo")}

experiment_results_v2 = client.evaluate(
      ls_target_v2,
      data=dataset_name,
      evaluators=[concision, correctness],
      experiment_prefix="openai-4-turbo",
  )

instructions_v3 = "Respond to the users question in a short, concise manner (one short sentence). Do NOT use more than ten words."

def ls_target_v3(inputs: str) -> dict:
      response = my_app(
          inputs["question"],
          model="gpt-4-turbo",
          instructions=instructions_v3
      )
      return {"response": response}

experiment_results_v3 = client.evaluate(
      ls_target_v3,
      data=dataset_name,
      evaluators=[concision, correctness],
      experiment_prefix="strict-openai-4-turbo",
  )
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-chatbot-tutorial.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Now, if we go the LangSmith UI and look for `QA Example Dataset` in the `Datasets & Testing` page, when we click into it we should see that we have five new examples.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9ab5110714d009d5865ba0e2d8ee0ffa" alt="Testing tutorial dataset" data-og-width="1251" width="1251" data-og-height="560" height="560" data-path="langsmith/images/testing-tutorial-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e4b38ded6968e649ed8ab507f63f1f3e 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f7aee5327f8058dd99684cd43e44c791 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e853ed05b0a2ad40f9e4d0403e7004c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=331654a31885b89a93924eaac4fa95da 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=833bf2a60b392323bba47fbe42655537 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/testing-tutorial-dataset.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3410e4bc7ac5c28f8838fc5fb88026bd 2500w" />

## Define metrics

After creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those **exact** answers, but rather something that is similar. This makes our evaluation a little trickier.

In addition to evaluating correctness, let's also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response.

Let's go ahead and define these two metrics.

For the first, we will use an LLM to **judge** whether the output is correct (with respect to the expected output). This **LLM-as-a-judge** is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:
```

Example 2 (unknown):
```unknown
For evaluating the length of the response, this is a lot easier! We can just define a simple function that checks whether the actual output is less than 2x the length of the expected result.
```

Example 3 (unknown):
```unknown
## Run Evaluations

Great! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:
```

Example 4 (unknown):
```unknown
Before running this through LangSmith evaluations, we need to define a simple wrapper that maps the input keys from our dataset to the function we want to call, and then also maps the output of the function to the output key we expect.
```

---

## Define edges

**URL:** llms-txt#define-edges

**Contents:**
  - Impose a recursion limit
- Async
- Combine control flow and state updates with `Command`

def route(state: State) -> Literal["b", END]:
    if len(state["aggregate"]) < 7:
        return "b"
    else:
        return END

builder.add_edge(START, "a")
builder.add_conditional_edges("a", route)
builder.add_edge("b", "a")
graph = builder.compile()
python  theme={null}
from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"aggregate": []})

Node A sees []
Node B sees ['A']
Node A sees ['A', 'B']
Node B sees ['A', 'B', 'A']
Node A sees ['A', 'B', 'A', 'B']
Node B sees ['A', 'B', 'A', 'B', 'A']
Node A sees ['A', 'B', 'A', 'B', 'A', 'B']
python  theme={null}
from langgraph.errors import GraphRecursionError

try:
    graph.invoke({"aggregate": []}, {"recursion_limit": 4})
except GraphRecursionError:
    print("Recursion Error")

Node A sees []
Node B sees ['A']
Node C sees ['A', 'B']
Node D sees ['A', 'B']
Node A sees ['A', 'B', 'C', 'D']
Recursion Error
python  theme={null}
  import operator
  from typing import Annotated, Literal
  from typing_extensions import TypedDict
  from langgraph.graph import StateGraph, START, END
  from langgraph.managed.is_last_step import RemainingSteps

class State(TypedDict):
      aggregate: Annotated[list, operator.add]
      remaining_steps: RemainingSteps

def a(state: State):
      print(f'Node A sees {state["aggregate"]}')
      return {"aggregate": ["A"]}

def b(state: State):
      print(f'Node B sees {state["aggregate"]}')
      return {"aggregate": ["B"]}

# Define nodes
  builder = StateGraph(State)
  builder.add_node(a)
  builder.add_node(b)

# Define edges
  def route(state: State) -> Literal["b", END]:
      if state["remaining_steps"] <= 2:
          return END
      else:
          return "b"

builder.add_edge(START, "a")
  builder.add_conditional_edges("a", route)
  builder.add_edge("b", "a")
  graph = builder.compile()

# Test it out
  result = graph.invoke({"aggregate": []}, {"recursion_limit": 4})
  print(result)
  
  Node A sees []
  Node B sees ['A']
  Node A sees ['A', 'B']
  {'aggregate': ['A', 'B', 'A']}
  python  theme={null}
  import operator
  from typing import Annotated, Literal
  from typing_extensions import TypedDict
  from langgraph.graph import StateGraph, START, END

class State(TypedDict):
      aggregate: Annotated[list, operator.add]

def a(state: State):
      print(f'Node A sees {state["aggregate"]}')
      return {"aggregate": ["A"]}

def b(state: State):
      print(f'Node B sees {state["aggregate"]}')
      return {"aggregate": ["B"]}

def c(state: State):
      print(f'Node C sees {state["aggregate"]}')
      return {"aggregate": ["C"]}

def d(state: State):
      print(f'Node D sees {state["aggregate"]}')
      return {"aggregate": ["D"]}

# Define nodes
  builder = StateGraph(State)
  builder.add_node(a)
  builder.add_node(b)
  builder.add_node(c)
  builder.add_node(d)

# Define edges
  def route(state: State) -> Literal["b", END]:
      if len(state["aggregate"]) < 7:
          return "b"
      else:
          return END

builder.add_edge(START, "a")
  builder.add_conditional_edges("a", route)
  builder.add_edge("b", "c")
  builder.add_edge("b", "d")
  builder.add_edge(["c", "d"], "a")
  graph = builder.compile()
  python  theme={null}
  from IPython.display import Image, display

display(Image(graph.get_graph().draw_mermaid_png()))
  python  theme={null}
  result = graph.invoke({"aggregate": []})
  
  Node A sees []
  Node B sees ['A']
  Node D sees ['A', 'B']
  Node C sees ['A', 'B']
  Node A sees ['A', 'B', 'C', 'D']
  Node B sees ['A', 'B', 'C', 'D', 'A']
  Node D sees ['A', 'B', 'C', 'D', 'A', 'B']
  Node C sees ['A', 'B', 'C', 'D', 'A', 'B']
  Node A sees ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D']
  python  theme={null}
  from langgraph.errors import GraphRecursionError

try:
      result = graph.invoke({"aggregate": []}, {"recursion_limit": 4})
  except GraphRecursionError:
      print("Recursion Error")
  
  Node A sees []
  Node B sees ['A']
  Node C sees ['A', 'B']
  Node D sees ['A', 'B']
  Node A sees ['A', 'B', 'C', 'D']
  Recursion Error
  shell  theme={null}
    pip install -U "langchain[openai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["OPENAI_API_KEY"] = "sk-..."

model = init_chat_model("gpt-4.1")
      python Model Class theme={null}
      import os
      from langchain_openai import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "sk-..."

model = ChatOpenAI(model="gpt-4.1")
      shell  theme={null}
    pip install -U "langchain[anthropic]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = init_chat_model("claude-sonnet-4-5-20250929")
      python Model Class theme={null}
      import os
      from langchain_anthropic import ChatAnthropic

os.environ["ANTHROPIC_API_KEY"] = "sk-..."

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
      shell  theme={null}
    pip install -U "langchain[openai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["AZURE_OPENAI_API_KEY"] = "..."
      os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
      os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = init_chat_model(
          "azure_openai:gpt-4.1",
          azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"],
      )
      python Model Class theme={null}
      import os
      from langchain_openai import AzureChatOpenAI

os.environ["AZURE_OPENAI_API_KEY"] = "..."
      os.environ["AZURE_OPENAI_ENDPOINT"] = "..."
      os.environ["OPENAI_API_VERSION"] = "2025-03-01-preview"

model = AzureChatOpenAI(
          model="gpt-4.1",
          azure_deployment=os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]
      )
      shell  theme={null}
    pip install -U "langchain[google-genai]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["GOOGLE_API_KEY"] = "..."

model = init_chat_model("google_genai:gemini-2.5-flash-lite")
      python Model Class theme={null}
      import os
      from langchain_google_genai import ChatGoogleGenerativeAI

os.environ["GOOGLE_API_KEY"] = "..."

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite")
      shell  theme={null}
    pip install -U "langchain[aws]"
    python init_chat_model theme={null}
      from langchain.chat_models import init_chat_model

# Follow the steps here to configure your credentials:
      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

model = init_chat_model(
          "anthropic.claude-3-5-sonnet-20240620-v1:0",
          model_provider="bedrock_converse",
      )
      python Model Class theme={null}
      from langchain_aws import ChatBedrock

model = ChatBedrock(model="anthropic.claude-3-5-sonnet-20240620-v1:0")
      shell  theme={null}
    pip install -U "langchain[huggingface]"
    python init_chat_model theme={null}
      import os
      from langchain.chat_models import init_chat_model

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

model = init_chat_model(
          "microsoft/Phi-3-mini-4k-instruct",
          model_provider="huggingface",
          temperature=0.7,
          max_tokens=1024,
      )
      python Model Class theme={null}
      import os
      from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint

os.environ["HUGGINGFACEHUB_API_TOKEN"] = "hf_..."

llm = HuggingFaceEndpoint(
          repo_id="microsoft/Phi-3-mini-4k-instruct",
          temperature=0.7,
          max_length=1024,
      )
      model = ChatHuggingFace(llm=llm)
      python  theme={null}
from langchain.chat_models import init_chat_model
from langgraph.graph import MessagesState, StateGraph

async def node(state: MessagesState):  # [!code highlight]
    new_message = await llm.ainvoke(state["messages"])  # [!code highlight]
    return {"messages": [new_message]}

builder = StateGraph(MessagesState).add_node(node).set_entry_point("node")
graph = builder.compile()

input_message = {"role": "user", "content": "Hello"}
result = await graph.ainvoke({"messages": [input_message]})  # [!code highlight]
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        # state update
        update={"foo": "bar"},
        # control flow
        goto="my_other_node"
    )
python  theme={null}
import random
from typing_extensions import TypedDict, Literal
from langgraph.graph import StateGraph, START
from langgraph.types import Command

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e1b99e7efe45b1fdc5836d590d5fbbc3" alt="Simple loop graph" data-og-width="188" width="188" data-og-height="249" height="249" data-path="oss/images/graph_api_image_7.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a443c1ddc2f6a4e7c73f4482c7d63912 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=f65d82d8aaeb024beb5da1aa2948bcdb 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=b95f4df2fb69f28779a1d8dd113409d0 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=bdb4011d05756c10a1c7b5dea683fdb7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=dde791caa4279a6248b59b70df99dd2c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/graph_api_image_7.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e4d568719f1761ff3a3d2ea9175241d8 2500w" />

This architecture is similar to a [ReAct agent](/oss/python/langgraph/workflows-agents) in which node `"a"` is a tool-calling model, and node `"b"` represents the tools.

In our `route` conditional edge, we specify that we should end after the `"aggregate"` list in the state passes a threshold length.

Invoking the graph, we see that we alternate between nodes `"a"` and `"b"` before terminating once we reach the termination condition.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
### Impose a recursion limit

In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's [recursion limit](/oss/python/langgraph/graph-api#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](/oss/python/langgraph/graph-api#graphs). We can then catch and handle this exception:
```

---

## Define graph state

**URL:** llms-txt#define-graph-state

class State(TypedDict):
    foo: str

---

## Define input schema

**URL:** llms-txt#define-input-schema

class InputState(TypedDict):
    question: str

---

## Define nodes

**URL:** llms-txt#define-nodes

builder = StateGraph(State)
builder.add_node(a)
builder.add_node(b)

---

## Define other parameters

**URL:** llms-txt#define-other-parameters

val = 42
text = "Hello, world!"

---

## Define output schema

**URL:** llms-txt#define-output-schema

class OutputState(TypedDict):
    answer: str

---

## Define regex patterns for various PII

**URL:** llms-txt#define-regex-patterns-for-various-pii

SSN_PATTERN = re.compile(r'\b\d{3}-\d{2}-\d{4}\b')
CREDIT_CARD_PATTERN = re.compile(r'\b(?:\d[ -]*?){13,16}\b')
EMAIL_PATTERN = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,7}\b')
PHONE_PATTERN = re.compile(r'\b(?:\+?1[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}\b')
FULL_NAME_PATTERN = re.compile(r'\b([A-Z][a-z]*\s[A-Z][a-z]*)\b')

def regex_anonymize(text):
    """
    Anonymize sensitive information in the text using regex patterns.
    Args:
        text (str): The input text to be anonymized.
    Returns:
        str: The anonymized text.
    """
    # Replace sensitive information with placeholders
    text = SSN_PATTERN.sub('[REDACTED SSN]', text)
    text = CREDIT_CARD_PATTERN.sub('[REDACTED CREDIT CARD]', text)
    text = EMAIL_PATTERN.sub('[REDACTED EMAIL]', text)
    text = PHONE_PATTERN.sub('[REDACTED PHONE]', text)
    text = FULL_NAME_PATTERN.sub('[REDACTED NAME]', text)
    return text

def recursive_anonymize(data, depth=10):
    """
    Recursively traverse the data structure and anonymize sensitive information.
    Args:
        data (any): The input data to be anonymized.
        depth (int): The current recursion depth to prevent excessive recursion.
    Returns:
        any: The anonymized data.
    """
    if depth == 0:
        return data
    if isinstance(data, dict):
        anonymized_dict = {}
        for k, v in data.items():
            anonymized_value = recursive_anonymize(v, depth - 1)
            anonymized_dict[k] = anonymized_value
        return anonymized_dict
    elif isinstance(data, list):
        anonymized_list = []
        for item in data:
            anonymized_item = recursive_anonymize(item, depth - 1)
            anonymized_list.append(anonymized_item)
        return anonymized_list
    elif isinstance(data, str):
        anonymized_data = regex_anonymize(data)
        return anonymized_data
    else:
        return data

openai_client = wrap_openai(openai.Client())

---

## Define target function that uses attachments

**URL:** llms-txt#define-target-function-that-uses-attachments

**Contents:**
  - Define custom evaluators
- Update examples with attachments
- UI
  - 1. Create examples with attachments
  - 2. Create a multimodal prompt
  - Define custom evaluators
  - Update examples with attachments

def file_qa(inputs, attachments):
    # Read the audio bytes from the reader and encode them in base64
    audio_reader = attachments["my_wav"]["reader"]
    audio_b64 = base64.b64encode(audio_reader.read()).decode('utf-8')

audio_completion = client.chat.completions.create(
        model="gpt-4o-audio-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": inputs["audio_question"]
                    },
                    {
                        "type": "input_audio",
                        "input_audio": {
                            "data": audio_b64,
                            "format": "wav"
                        }
                    }
                ]
            }
        ]
    )

# Most models support taking in an image URL directly in addition to base64 encoded images
    # You can pipe the image pre-signed URL directly to the model
    image_url = attachments["my_img"]["presigned_url"]
    image_completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
          {
            "role": "user",
            "content": [
              {"type": "text", "text": inputs["image_question"]},
              {
                "type": "image_url",
                "image_url": {
                  "url": image_url,
                },
              },
            ],
          }
        ],
    )

return {
        "audio_answer": audio_completion.choices[0].message.content,
        "image_answer": image_completion.choices[0].message.content,
    }
typescript  theme={null}
{
  presigned_url: string,
  mime_type: string,
}
typescript  theme={null}
import OpenAI from "openai";
import { wrapOpenAI } from "langsmith/wrappers";

const client: any = wrapOpenAI(new OpenAI());

async function fileQA(inputs: Record<string, any>, config?: Record<string, any>) {
  const presignedUrl = config?.attachments?.["my_wav"]?.presigned_url;
  if (!presignedUrl) {
    throw new Error("No presigned URL provided for audio.");
  }

const response = await fetch(presignedUrl);
  if (!response.ok) {
    throw new Error(`Failed to fetch audio: ${response.statusText}`);
  }

const arrayBuffer = await response.arrayBuffer();
  const uint8Array = new Uint8Array(arrayBuffer);
  const audioB64 = Buffer.from(uint8Array).toString("base64");

const audioCompletion = await client.chat.completions.create({
    model: "gpt-4o-audio-preview",
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: inputs["audio_question"] },
          {
            type: "input_audio",
            input_audio: {
              data: audioB64,
              format: "wav",
            },
          },
        ],
      },
    ],
  });

const imageUrl = config?.attachments?.["my_img"]?.presigned_url
  const imageCompletion = await client.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: inputs["image_question"] },
          {
            type: "image_url",
            image_url: {
              url: imageUrl,
            },
          },
        ],
      },
    ],
  });

return {
    audio_answer: audioCompletion.choices[0].message.content,
    image_answer: imageCompletion.choices[0].message.content,
  };
}
python Python theme={null}
  # Assumes you've installed pydantic
  from pydantic import BaseModel

def valid_image_description(outputs: dict, attachments: dict) -> bool:
    """Use an LLM to judge if the image description and images are consistent."""
    instructions = """
    Does the description of the following image make sense?
    Please carefully review the image and the description to determine if the description is valid.
    """

class Response(BaseModel):
        description_is_valid: bool

image_url = attachments["my_img"]["presigned_url"]
    response = client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": instructions
            },
            {
                "role": "user",
                "content": [
                    {"type": "image_url", "image_url": {"url": image_url}},
                    {"type": "text", "text": outputs["image_answer"]}
                ]
            }
        ],
        response_format=Response
    )
    return response.choices[0].message.parsed.description_is_valid

ls_client.evaluate(
    file_qa,
    data=dataset_name,
    evaluators=[valid_image_description],
  )
  typescript TypeScript theme={null}
  import { zodResponseFormat } from 'openai/helpers/zod';
  import { z } from 'zod';
  import { evaluate } from "langsmith/evaluation";

const DescriptionResponse = z.object({
    description_is_valid: z.boolean(),
  });

async function validImageDescription({
    outputs,
    attachments,
  }: {
    outputs?: any;
    attachments?: any;
  }): Promise<{ key: string; score: boolean}> {
    const instructions = `Does the description of the following image make sense?
  Please carefully review the image and the description to determine if the description is valid.`;

const imageUrl = attachments?.["my_img"]?.presigned_url
    const completion = await client.beta.chat.completions.parse({
        model: "gpt-4o",
        messages: [
            {
                role: "system",
                content: instructions,
            },
            {
                role: "user",
                content: [
                    { type: "image_url", image_url: { url: imageUrl } },
                    { type: "text", text: outputs?.image_answer },
                ],
            },
        ],
        response_format: zodResponseFormat(DescriptionResponse, 'imageResponse'),
    });

const score: boolean = completion.choices[0]?.message?.parsed?.description_is_valid ?? false;
    return { key: "valid_image_description", score };
  }

const resp = await evaluate(fileQA, {
    data: datasetName,
    // Need to pass flag to include attachments
    includeAttachments: true,
    evaluators: [validImageDescription],
    client: langsmithClient
  });
  python Python theme={null}
  example_update = {
    "id": example_id,
    "attachments": {
        # These are net new attachments
        "my_new_file": ("text/plain", b"foo bar"),
    },
    "inputs": inputs,
    "outputs": outputs,
    # Any attachments not in rename/retain will be deleted.
    # In this case, that would be "my_img" if we uploaded it.
    "attachments_operations": {
        # Retained attachments will stay exactly the same
        "retain": ["my_pdf"],
        # Renaming attachments preserves the original data
        "rename": {
            "my_wav": "my_new_wav",
        }
    },
  }

ls_client.update_examples(dataset_id=dataset.id, updates=[example_update])
  typescript TypeScript theme={null}
  import { ExampleUpdateWithAttachments } from "langsmith/schemas";

const exampleUpdate: ExampleUpdateWithAttachments = {
    id: exampleId,
    attachments: {
      // These are net new attachments
      "my_new_file": {
        mimeType: "text/plain",
        data: Buffer.from("foo bar")
      },
    },
    attachments_operations: {
      // Retained attachments will stay exactly the same
      retain: ["my_img"],
      // Renaming attachments preserves the original data
      rename: {
        "my_wav": "my_new_wav",
      },
      // Any attachments not in rename/retain will be deleted
      // In this case, that would be "my_pdf"
    },
  };

await langsmithClient.updateExamplesMultipart(dataset.id, [exampleUpdate]);
  ```
</CodeGroup>

### 1. Create examples with attachments

You can add examples with attachments to a dataset in a few different ways.

#### From existing runs

When adding runs to a LangSmith dataset, attachments can be selectively propagated from the source run to the destination example. To learn more, please see [this guide](/langsmith/manage-datasets-in-application#add-runs-from-the-tracing-project-ui).

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b8fa62cb39c4f1fc67d9b24fa78d1653" alt="Add trace with attachments to dataset" data-og-width="1662" width="1662" data-og-height="679" height="679" data-path="langsmith/images/add-trace-with-attachments-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=72ee339359616ed8f03f4ddbfe86bc23 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cb6f558f8a0391588583a7b5d520a27f 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=bc51ed2e68af972e488051fc1ae01caf 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d58d097b70ad0f6b7a327058c659d8d9 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cbac645e6534290036d24963c231a878 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-trace-with-attachments-to-dataset.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b4ff5c59dd6c2b23d97bfd5e34206a50 2500w" />

You can create examples with attachments directly from the LangSmith UI. Click the `+ Example` button in the `Examples` tab of the dataset UI. Then upload attachments using the "Upload Files" button:

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=183f929c807f59157e93d40354057933" alt="Create example with attachments" data-og-width="3456" width="3456" data-og-height="1856" height="1856" data-path="langsmith/images/create-example-with-attachments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c80316a80e8359d14aa42aac6767b677 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1921a813aea4e6fa62bcf7cfd169662e 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=35d6882ec8757e56d81eba2033220cf7 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0b93d237b11ab6bf8693b287ee19dc4b 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=38384c5a8166b41535657ec8a6337b49 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-example-with-attachments.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0f8622286209391451895c2e4c7b03b 2500w" />

Once uploaded, you can view examples with attachments in the LangSmith UI. Each attachment will be rendered with a preview for easy inspection. <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8f813bdf7d3bfd5a840e5f8c47693ed3" alt="Attachments with examples" data-og-width="1331" width="1331" data-og-height="593" height="593" data-path="langsmith/images/attachments-with-examples.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0ecc6a1a59da5972731c4f36fc0154d8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f7985b2778973b594e8522964eb13770 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f062086524f7aff66c094ac0e259c04e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=970e61cff6a2cfc749c14481afe2a3f9 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4c34be3eb64547daf4879b2f2dab3ec4 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachments-with-examples.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=700119fd1a262ce0f85d924b684141a4 2500w" />

### 2. Create a multimodal prompt

The LangSmith UI allows you to include attachments in your prompts when evaluating multimodal models:

First, click the file icon in the message where you want to add multimodal content. Next, add a template variable for the attachment(s) you want to include for each example.

* For a single attachment type: Use the suggested variable name. Note: all examples must have an attachment with this name.
* For multiple attachments or if your attachments have varying names from one example to another: Use the `All attachments` variable to include all available attachments for each example.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/adding-multimodal-variable.gif?s=07a15c9fc5e6fc743f92b6b41ab8c9e0" alt="Adding multimodal variable" data-og-width="1700" width="1700" data-og-height="1080" height="1080" data-path="langsmith/images/adding-multimodal-variable.gif" data-optimize="true" data-opv="3" />

### Define custom evaluators

<Note>
  The LangSmith playground does not currently support pulling multimodal content into evaluators. If this would be helpful for your use case, please let us know in the [LangChain Forum](https://forum.langchain.com/) (sign up [here](https://www.langchain.com/join-community) if you're not already a member)!
</Note>

You can evaluate a model's text output by adding an evaluator that takes in the example's inputs and outputs. Even without multimodal support in your evaluators, you can still run text-only evaluations. For example:

* OCR → text correction: Use a vision model to extract text from a document, then evaluate the accuracy of the extracted output.
* Speech-to-text → transcription quality: Use a voice model to transcribe audio to text, then evaluate the transcription against your reference.

For more information on defining custom evaluators, see the [LLM as Judge](/langsmith/llm-as-judge) guide.

### Update examples with attachments

<Note>
  Attachments are limited to 20MB in size in the UI.
</Note>

When editing an example in the UI, you can:

* Upload new attachments
* Rename and delete attachments
* Reset attachments to their previous state using the quick reset button

Changes are not saved until you click submit.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/attachment-editing.gif?s=4f165ed98fe81722961778ebbe1691ed" alt="Attachment editing" data-og-width="1204" width="1204" data-og-height="720" height="720" data-path="langsmith/images/attachment-editing.gif" data-optimize="true" data-opv="3" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-with-attachments.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### TypeScript

In the TypeScript SDK, the `config` argument is used to pass in the attachments to the target function if `includeAttachments` is set to `true`.

The `config` will contain `attachments` which is an object mapping the attachment name to an object of the form:
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
### Define custom evaluators

The exact same rules apply as above to determine whether the evaluator should receive attachments.

The evaluator below uses an LLM to judge if the reasoning and the answer are consistent. To learn more about how to define llm-based evaluators, please see [this guide](/langsmith/llm-as-judge).

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define the function that calls the model

**URL:** llms-txt#define-the-function-that-calls-the-model

def call_model(state: State):
    messages = state['messages']
    response = model.invoke(messages)

# We return a list, because this will get added to the existing list
    return {"messages": [response]}

---

## Define the function that determines whether to continue or not

**URL:** llms-txt#define-the-function-that-determines-whether-to-continue-or-not

def should_continue(state: State) -> Literal["tools", END]:
    messages = state['messages']
    last_message = messages[-1]

# If the LLM makes a tool call, then we route to the "tools" node
    if last_message.tool_calls:
        return "tools"

# Otherwise, we stop (reply to the user)
    return END

---

## Define the graph

**URL:** llms-txt#define-the-graph

graph = (
    StateGraph(MessagesState)
    ...
    .compile()
    .with_config({'callbacks': [tracer]})
)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/observability.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Define the nodes

**URL:** llms-txt#define-the-nodes

def node_a(state: State) -> Command[Literal["node_b", "node_c"]]:
    print("Called A")
    value = random.choice(["b", "c"])
    # this is a replacement for a conditional edge function
    if value == "b":
        goto = "node_b"
    else:
        goto = "node_c"

# note how Command allows you to BOTH update the graph state AND route to the next node
    return Command(
        # this is the state update
        update={"foo": value},
        # this is a replacement for an edge
        goto=goto,
    )

def node_b(state: State):
    print("Called B")
    return {"foo": state["foo"] + "b"}

def node_c(state: State):
    print("Called C")
    return {"foo": state["foo"] + "c"}
python  theme={null}
builder = StateGraph(State)
builder.add_edge(START, "node_a")
builder.add_node(node_a)
builder.add_node(node_b)
builder.add_node(node_c)

**Examples:**

Example 1 (unknown):
```unknown
We can now create the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with the above nodes. Notice that the graph doesn't have [conditional edges](/oss/python/langgraph/graph-api#conditional-edges) for routing! This is because control flow is defined with [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) inside `node_a`.
```

---

## Define the nodes we will cycle between

**URL:** llms-txt#define-the-nodes-we-will-cycle-between

workflow.add_node(generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([retriever_tool]))
workflow.add_node(rewrite_question)
workflow.add_node(generate_answer)

workflow.add_edge(START, "generate_query_or_respond")

---

## Define the node that processes the input and generates an answer

**URL:** llms-txt#define-the-node-that-processes-the-input-and-generates-an-answer

def answer_node(state: InputState):
    # Example answer and an extra key
    return {"answer": "bye", "question": state["question"]}

---

## Define the overall schema, combining both input and output

**URL:** llms-txt#define-the-overall-schema,-combining-both-input-and-output

class OverallState(InputState, OutputState):
    pass

---

## Define the processing node

**URL:** llms-txt#define-the-processing-node

def answer_node(state: InputState):
    # Replace with actual logic and do something useful
    return {"answer": "bye", "question": state["question"]}

---

## Define the schema for the input

**URL:** llms-txt#define-the-schema-for-the-input

class InputState(TypedDict):
    question: str

---

## Define the schema for the output

**URL:** llms-txt#define-the-schema-for-the-output

class OutputState(TypedDict):
    answer: str

---

## Define the two nodes we will cycle between

**URL:** llms-txt#define-the-two-nodes-we-will-cycle-between

workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)

---

## Define tools

**URL:** llms-txt#define-tools

@tool
def multiply(a: int, b: int) -> int:
    """Multiply `a` and `b`.

Args:
        a: First int
        b: Second int
    """
    return a * b

@tool
def add(a: int, b: int) -> int:
    """Adds `a` and `b`.

Args:
        a: First int
        b: Second int
    """
    return a + b

@tool
def divide(a: int, b: int) -> float:
    """Divide `a` and `b`.

Args:
        a: First int
        b: Second int
    """
    return a / b

---

## Define your graph

**URL:** llms-txt#define-your-graph

builder = StateGraph(MessagesState)

---

## Define your tasks

**URL:** llms-txt#define-your-tasks

research_task = Task(
    description="""Conduct comprehensive research on the current state of AI adoption
    in small to medium businesses. Focus on:
    1. Current adoption rates and trends
    2. Main barriers to adoption
    3. Most popular AI tools and use cases
    4. ROI and business impact metrics

Provide a detailed analysis with supporting data and statistics.""",
    agent=market_researcher,
    expected_output="A comprehensive market research report on AI adoption in SMBs with data, trends, and insights.",
)

analysis_task = Task(
    description="""Analyze the research findings and identify key statistical patterns.
    Create data visualizations and provide quantitative insights on:
    1. Adoption rate trends over time
    2. Industry-specific adoption patterns
    3. ROI correlation analysis
    4. Barrier impact assessment

Present findings in a clear, data-driven format.""",
    agent=data_analyst,
    expected_output="Statistical analysis report with key metrics, trends, and data-driven insights.",
    context=[research_task],
)

content_task = Task(
    description="""Based on the research and analysis, create a compelling marketing
    strategy document that includes:
    1. Executive summary of key findings
    2. Target audience personas based on adoption patterns
    3. Key messaging framework addressing main barriers
    4. Content recommendations for different business segments
    5. Campaign strategy to drive AI adoption

Make the content actionable and business-focused.""",
    agent=content_strategist,
    expected_output="Complete marketing strategy document with personas, messaging, and campaign recommendations.",
    context=[research_task, analysis_task],
)

---

## Define your tools

**URL:** llms-txt#define-your-tools

**Contents:**
- View traces in LangSmith
- Advanced usage
  - Custom metadata and tags

def get_flight_info(destination: str, departure_date: str) -> dict:
    """Get flight information for a destination."""
    return {
        "destination": destination,
        "departure_date": departure_date,
        "price": "$450",
        "duration": "5h 30m",
        "airline": "Example Airways"
    }

def get_hotel_recommendations(city: str, check_in: str) -> dict:
    """Get hotel recommendations for a city."""
    return {
        "city": city,
        "check_in": check_in,
        "hotels": [
            {"name": "Grand Plaza Hotel", "rating": 4.5, "price": "$120/night"},
            {"name": "City Center Inn", "rating": 4.2, "price": "$95/night"}
        ]
    }

async def main():
    # Create your ADK agent
    agent = LlmAgent(
        name="travel_assistant",
        tools=[get_flight_info, get_hotel_recommendations],
        model="gemini-2.5-flash-lite",
        instruction="You are a helpful travel assistant that can help with flights and hotels.",
    )

# Set up session service and runner
    session_service = InMemorySessionService()
    runner = Runner(
        app_name="travel_app",
        agent=agent,
        session_service=session_service
    )

# Create a session
    user_id = "traveler_456"
    session_id = "session_789"
    await session_service.create_session(
        app_name="travel_app",
        user_id=user_id,
        session_id=session_id
    )

# Send a message to the agent
    new_message = types.Content(
        parts=[types.Part(text="I need to book a flight to Paris for March 15th and find a good hotel.")],
        role="user",
    )

# Run the agent and process events
    events = runner.run(
        user_id=user_id,
        session_id=session_id,
        new_message=new_message,
    )

for event in events:
        print(event)

if __name__ == "__main__":
    asyncio.run(main())
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## View traces in LangSmith

* **Agent conversations**: Complete conversation flows between users and your ADK agents.
* **Tool calls**: Individual function calls made by your agents.
* **Model interactions**: LLM requests and responses using Gemini models.
* **Session information**: User and session context for organizing related traces.
* **Model interactions**: LLM requests and responses using Gemini models

<img src="https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=3495c7838ba7467b905a180fc9ce477b" alt="LangSmith dashboard with raw input from run and trace information." data-og-width="3022" width="3022" data-og-height="1444" height="1444" data-path="langsmith/images/adk.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=280&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=b6db8e92abed624dc492ee12d217b5d3 280w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=560&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=1caecf02d785375c0cae8c97cccf093c 560w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=840&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=3b7fe88b62d4c4bffa889bd79fa5fefd 840w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=1100&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=d7a152d9255fc6d42f946b3601e79ecc 1100w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=1650&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=1eb5ea91226c92628f474254eb177f80 1650w, https://mintcdn.com/langchain-5e9cc07a/OEEzzB__isjPfBRD/langsmith/images/adk.png?w=2500&fit=max&auto=format&n=OEEzzB__isjPfBRD&q=85&s=a98880df67b4dae4c56de2a3d27fefa7 2500w" />

## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes in your ADK application:
```

---

## Delete an item.

**URL:** llms-txt#delete-an-item.

Source: https://docs.langchain.com/langsmith/agent-server-api/store/delete-an-item

langsmith/agent-server-openapi.json delete /store/items

---

## Delete Assistant

**URL:** llms-txt#delete-assistant

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/delete-assistant

langsmith/agent-server-openapi.json delete /assistants/{assistant_id}
Delete an assistant by ID.

All versions of the assistant will be deleted as well.

---

## Delete Cron

**URL:** llms-txt#delete-cron

Source: https://docs.langchain.com/langsmith/agent-server-api/crons-plus-tier/delete-cron

langsmith/agent-server-openapi.json delete /runs/crons/{cron_id}
Delete a cron by ID.

---

## Delete Deployment

**URL:** llms-txt#delete-deployment

Source: https://docs.langchain.com/api-reference/deployments-v2/delete-deployment

https://api.host.langchain.com/openapi.json delete /v2/deployments/{deployment_id}
Delete a deployment by ID.

---

## Delete Listener

**URL:** llms-txt#delete-listener

Source: https://docs.langchain.com/api-reference/listeners-v2/delete-listener

https://api.host.langchain.com/openapi.json delete /v2/listeners/{listener_id}
Delete a listener by ID.

---

## Delete Oauth Provider

**URL:** llms-txt#delete-oauth-provider

Source: https://docs.langchain.com/api-reference/auth-service-v2/delete-oauth-provider

https://api.host.langchain.com/openapi.json delete /v2/auth/providers/{provider_id}
Delete an OAuth provider.

---

## Delete organizations

**URL:** llms-txt#delete-organizations

**Contents:**
  - Prerequisites
  - Running the deletion script for a single organization

Source: https://docs.langchain.com/langsmith/script-delete-an-organization

The LangSmith UI does not currently support the deletion of an individual organization from a self-hosted instance of LangSmith. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedbacks tables and then removing the Organization from the Postgres tenants table.

This command using the Organization ID as an argument.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

5. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

6. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
   * If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port `8443`

7. The script to delete an organization

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_organization_sh)

### Running the deletion script for a single organization

Run the following command to run the organization removal script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see organization is no longer present.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-an-organization.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Delete Run

**URL:** llms-txt#delete-run

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/delete-run

langsmith/agent-server-openapi.json delete /threads/{thread_id}/runs/{run_id}
Delete a run by ID.

---

## Delete Thread

**URL:** llms-txt#delete-thread

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/delete-thread

langsmith/agent-server-openapi.json delete /threads/{thread_id}
Delete a thread by ID.

---

## Delete traces

**URL:** llms-txt#delete-traces

**Contents:**
  - Prerequisites
  - Running the deletion script for a single trace
  - Running the deletion script for a multiple traces from a file with one trace ID per line
- Troubleshooting
  - "Could not find trace IDs" error

Source: https://docs.langchain.com/langsmith/script-delete-traces

The LangSmith UI does not currently support the deletion of an individual trace. This, however, can be accomplished by directly removing the trace from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedback table themselves.

This command can either be run using a trace ID as an argument or using a file that is a list of trace IDs.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `delete_trace_by_id` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to delete a trace

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_trace_by_id.sh)

### Running the deletion script for a single trace

Run the following command to run the trace deletion script using a single trace ID:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see specified trace ID is no longer present nor reflected in stats.

### Running the deletion script for a multiple traces from a file with one trace ID per line

Run the following command to run the trace deletion script using a list of trace IDs:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see all the specified traces have been removed.

### "Could not find trace IDs" error

If you receive an error message stating that trace IDs could not be found, add the `--ssl` flag to your command. Without this flag, the script may not be able to properly connect to ClickHouse, resulting in false "trace ID not found" errors.

Example with SSL flag:

You can also verify that traces exist by connecting to ClickHouse directly using `clickhouse-cli` and querying for the trace IDs before running the deletion script.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-traces.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

Example 2 (unknown):
```unknown
If you visit the LangSmith UI, you should now see specified trace ID is no longer present nor reflected in stats.

### Running the deletion script for a multiple traces from a file with one trace ID per line

Run the following command to run the trace deletion script using a list of trace IDs:
```

Example 3 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

Example 4 (unknown):
```unknown
If you visit the LangSmith UI, you should now see all the specified traces have been removed.

## Troubleshooting

### "Could not find trace IDs" error

If you receive an error message stating that trace IDs could not be found, add the `--ssl` flag to your command. Without this flag, the script may not be able to properly connect to ClickHouse, resulting in false "trace ID not found" errors.

Example with SSL flag:
```

---

## Delete workspaces

**URL:** llms-txt#delete-workspaces

**Contents:**
  - Prerequisites
  - Running the deletion script for a single workspace

Source: https://docs.langchain.com/langsmith/script-delete-a-workspace

<Note>
  Deleting a workspace is supported **nativley in LangSmith Self-Hosted v0.10**. View [instructions for deleting a workspace](/langsmith/set-up-a-workspace#delete-a-workspace).

Follow the guide below for Self-Hosted versions before v0.10.
</Note>

The LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs\_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table.

This command using the Workspace ID as an argument.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

5. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

6. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
     * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.
   * If you are using Clickhouse Cloud you will want to specify the --ssl flag and use port `8443`

7. The script to delete a workspace

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/delete_workspace.sh)

### Running the deletion script for a single workspace

Run the following command to run the workspace removal script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

If you visit the LangSmith UI, you should now see workspace is deleted.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-delete-a-workspace.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Deploy an observability stack for your LangSmith deployment

**URL:** llms-txt#deploy-an-observability-stack-for-your-langsmith-deployment

Source: https://docs.langchain.com/langsmith/observability-stack

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

LangSmith applications expose telemetry data that can be sent to the backend of your choice. If you don’t already have an observability stack, or prefer to keep LangSmith telemetry separate from your main application, you can use the LangSmith Observability Helm chart to deploy a basic observability stack.

---

## Deploy with control plane

**URL:** llms-txt#deploy-with-control-plane

**Contents:**
- Overview
- Prerequisites
- Step 1. Test locally
- Step 2. Build Docker image
- Step 3. Push to container registry
- Step 4. Deploy with the control plane UI
- Update deployment
- Private registry authentication
- Next steps

Source: https://docs.langchain.com/langsmith/deploy-with-control-plane

This guide shows you how to deploy your applications to [hybrid](/langsmith/hybrid) or [self-hosted](/langsmith/self-hosted) instances with a [control plane](/langsmith/control-plane). With a control plane, you build Docker images locally, push them to a registry that your Kubernetes cluster has access to, and deploy them with the [LangSmith UI](https://smith.langchain.com).

<Note>
  **This guide is for deploying applications, not setting up infrastructure.**

Before using this guide, you must have already completed infrastructure setup:

* **[Hybrid setup](/langsmith/deploy-hybrid)**: For hybrid hosting.
  * **[Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform)**: For self-hosted with control plane.

If you haven't set up your infrastructure yet, start with the [Platform setup section](/langsmith/platform-setup).
</Note>

Applications deployed to hybrid or self-hosted LangSmith instances with control plane use Docker images. In this guide, the application deployment workflow is:

1. Test your application locally using `langgraph dev` or [Studio](/langsmith/studio).
2. Build a Docker image using the `langgraph build` command.
3. Push the image to a container registry accessible by your infrastructure.
4. Deploy from the [control plane UI](/langsmith/control-plane#control-plane-ui) by specifying the image URL.

Before completing this guide, you'll need the following:

* Completed infrastructure setup to enable your [data plane](/langsmith/data-plane) to receive application deployments:
  * [Hybrid setup](/langsmith/deploy-hybrid): Installs data plane components (listener, operator, CRDs) in your Kubernetes cluster that connect to LangChain's managed control plane.
  * [Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform): Enables LangSmith Deployment on your self-hosted LangSmith instance.
* Access to the [LangSmith UI](https://smith.langchain.com) with LangSmith Deployment enabled.
* A container registry accessible by your Kubernetes cluster. If using a private registry that requires authentication, you must configure image pull secrets as part of your infrastructure setup. Refer to [Private registry authentication](#private-registry-authentication).

## Step 1. Test locally

Before deploying, test your application locally. You can use the [LangGraph CLI](/langsmith/cli#dev) to run an Agent server in development mode:

For a full guide local testing, refer to the [Local server quickstart](/langsmith/local-server).

## Step 2. Build Docker image

Build a Docker image of your application using the [`langgraph build`](/langsmith/cli#build) command:

Build command options include:

| Option               | Default          | Description                                                       |
| -------------------- | ---------------- | ----------------------------------------------------------------- |
| `-t, --tag TEXT`     | Required         | Tag for the Docker image                                          |
| `--platform TEXT`    |                  | Target platform(s) to build for (e.g., `linux/amd64,linux/arm64`) |
| `--pull / --no-pull` | `--pull`         | Build with latest remote Docker image                             |
| `-c, --config FILE`  | `langgraph.json` | Path to configuration file                                        |

Example with platform specification:

For full details, see the [CLI reference](/langsmith/cli#build).

## Step 3. Push to container registry

Push your image to a container registry accessible by your Kubernetes cluster. The specific commands depend on your registry provider.

<Tip>
  Tag your images with version information (e.g., `my-registry.com/my-app:v1.0.0`) to make rollbacks easier.
</Tip>

## Step 4. Deploy with the control plane UI

The [control plane UI](/langsmith/control-plane#control-plane-ui) allows you to create and manage deployments, view logs and metrics, and update configurations. To create a new deployment in the [LangSmith UI](https://smith.langchain.com):

1. In the left-hand navigation panel, select **Deployments**.
2. In the top-right corner, select **+ New Deployment**.
3. In the deployment configuration panel, provide:
   * **Image URL**: The full image URL you pushed in [Step 3](#step-3-push-to-container-registry).
   * **Listener/Compute ID**: Select the listener configured for your infrastructure.
   * **Namespace**: The Kubernetes namespace to deploy to.
   * **Environment variables**: Any required configuration (API keys, etc.).
   * Other deployment settings as needed.
4. Select **Submit**.

The control plane will coordinate with your [data plane](/langsmith/data-plane) listener to deploy your application.

After creating a deployment, the infrastructure is [provisioned asynchronously](/langsmith/control-plane#asynchronous-deployment). Deployment can take up to several minutes, with initial deployments taking longer due to database creation.

From the control plane UI, you can view build logs, server logs, and deployment metrics including CPU/memory usage, replicas, and API performance. For more details, refer to the [control plane monitoring documentation](/langsmith/control-plane#monitoring).

<Note>
  A [LangSmith Observability tracing project](/langsmith/observability) is automatically created for each deployment with the same name as the deployment. Tracing environment variables are set automatically by the control plane.
</Note>

To deploy a new version of your application, create a [new revision](/langsmith/control-plane#revisions):

Starting from the LangSmith UI:

1. In the left-hand navigation panel, select **Deployments**.
2. Select an existing deployment.
3. In the Deployment view, select **+ New Revision** in the top-right corner.
4. Update the configuration:
   * Update the **Image URL** to your new image version.
   * Update environment variables if needed.
   * Adjust other settings as needed.
5. Select **Submit**.

## Private registry authentication

If your container registry requires authentication (e.g., AWS ECR, Azure ACR, GCP Artifact Registry, private Docker registry), you must configure Kubernetes image pull secrets before deploying applications. This is a one-time infrastructure configuration.

<Note>
  **This configuration is done at the infrastructure level, not per-deployment.** Once configured, all deployments automatically inherit the registry credentials.
</Note>

The configuration steps depend on your deployment type:

* **Self-hosted with control plane**: Configure `imagePullSecrets` in your LangSmith Helm chart's `values.yaml` file. See the detailed steps in the [Enable LangSmith Deployment guide](/langsmith/deploy-self-hosted-full-platform#setup).
* **Hybrid**: Configure `imagePullSecrets` in your `langgraph-dataplane-values.yaml` file using the same format.

For detailed steps on creating image pull secrets for different registry providers, refer to the [Kubernetes documentation on pulling images from private registries](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).

* **[Control plane](/langsmith/control-plane)**: Learn more about control plane features.
* **[Data plane](/langsmith/data-plane)**: Understand data plane architecture.
* **[Observability](/langsmith/observability)**: Monitor your deployments with automatic tracing.
* **[Studio](/langsmith/studio)**: Test and debug deployed applications.
* **[LangGraph CLI](/langsmith/cli)**: Full CLI reference documentation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-with-control-plane.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For a full guide local testing, refer to the [Local server quickstart](/langsmith/local-server).

## Step 2. Build Docker image

Build a Docker image of your application using the [`langgraph build`](/langsmith/cli#build) command:
```

Example 2 (unknown):
```unknown
Build command options include:

| Option               | Default          | Description                                                       |
| -------------------- | ---------------- | ----------------------------------------------------------------- |
| `-t, --tag TEXT`     | Required         | Tag for the Docker image                                          |
| `--platform TEXT`    |                  | Target platform(s) to build for (e.g., `linux/amd64,linux/arm64`) |
| `--pull / --no-pull` | `--pull`         | Build with latest remote Docker image                             |
| `-c, --config FILE`  | `langgraph.json` | Path to configuration file                                        |

Example with platform specification:
```

---

## Deploy your app to Cloud

**URL:** llms-txt#deploy-your-app-to-cloud

**Contents:**
- Prerequisites
- 1. Create a repository on GitHub
- 2. Deploy to LangSmith
- 3. Test your application in Studio
- 4. Get the API URL for your deployment
- 5. Test the API
- Next steps

Source: https://docs.langchain.com/langsmith/deployment-quickstart

This is a quickstart guide for deploying your first application to LangSmith Cloud.

<Tip>
  For a comprehensive Cloud deployment guide with all configuration options, refer to the [Cloud deployment setup guide](/langsmith/deploy-to-cloud).
</Tip>

Before you begin, ensure you have the following:

* A [GitHub account](https://github.com/)
* A [LangSmith account](https://smith.langchain.com/) (free to sign up)

## 1. Create a repository on GitHub

To deploy an application to **LangSmith**, your application code must reside in a GitHub repository. Both public and private repositories are supported. For this quickstart, use the [`new-langgraph-project` template](https://github.com/langchain-ai/react-agent) for your application:

1. Go to the [`new-langgraph-project` repository](https://github.com/langchain-ai/new-langgraph-project) or [`new-langgraphjs-project` template](https://github.com/langchain-ai/new-langgraphjs-project).
2. Click the `Fork` button in the top right corner to fork the repository to your GitHub account.
3. Click **Create fork**.

## 2. Deploy to LangSmith

1. Log in to [LangSmith](https://smith.langchain.com/).
2. In the left sidebar, select **Deployments**.
3. Click the **+ New Deployment** button. A pane will open where you can fill in the required fields.
4. If you are a first time user or adding a private repository that has not been previously connected, click the **Import from GitHub** button and follow the instructions to connect your GitHub account.
5. Select your New LangGraph Project repository.
6. Click **Submit** to deploy.
   This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.

## 3. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. [Studio](/langsmith/studio) will open to display your graph.

## 4. Get the API URL for your deployment

1. In the **Deployment details** view, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

You can now test the API:

<Tabs>
  <Tab title="Python SDK (Async)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="Python SDK (Sync)">
    1. Install the LangGraph Python SDK:

2. Send a message to the assistant (threadless run):

<Tab title="JavaScript SDK">
    1. Install the LangGraph JS SDK

2. Send a message to the assistant (threadless run):

<Tab title="Rest API">
    
  </Tab>
</Tabs>

You've successfully deployed your application to LangSmith Cloud. Here are some next steps:

* **Explore Studio**: Use [Studio](/langsmith/studio) to visualize and debug your graph interactively.
* **Monitor your app**: Set up [observability](/langsmith/observability) with traces, dashboards, and alerts.
* **Learn more about Cloud**: See the [complete Cloud setup guide](/langsmith/deploy-to-cloud) for all configuration options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deployment-quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. Send a message to the assistant (threadless run):
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Python SDK (Sync)">
    1. Install the LangGraph Python SDK:
```

Example 3 (unknown):
```unknown
2. Send a message to the assistant (threadless run):
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript SDK">
    1. Install the LangGraph JS SDK
```

---

## Deprecated method call

**URL:** llms-txt#deprecated-method-call

**Contents:**
  - `example` parameter removed from `AIMessage`
- Minor changes
- Archived docs

text = response.text()
```

Existing usage patterns (i.e., `.text()`) will continue to function but now emit a warning. The method form will be removed in v2.

### `example` parameter removed from `AIMessage`

The `example` parameter has been removed from [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects. We recommend migrating to use `additional_kwargs` for passing extra metadata as needed.

* `AIMessageChunk` objects now include a `chunk_position` attribute with position `'last'` to indicate the final chunk in a stream. This allows for clearer handling of streamed messages. If the chunk is not the final one, `chunk_position` will be `None`.
* `LanguageModelOutputVar` is now typed to [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) instead of [`BaseMessage`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage).
* The logic for merging message chunks (`AIMessageChunk.add`) has been updated with more sophisticated selection handling for the final id for the merged chunk. It prioritizes provider-assigned IDs over LangChain-generated IDs.
* We now open files with `utf-8` encoding by default.
* Standard tests now use multimodal content blocks.

Old docs are archived for reference:

* [v0.3 docs content](https://github.com/langchain-ai/langchain/tree/v0.3/docs/docs)
* [v0.3 API reference](https://reference.langchain.com/v0.3/python/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langchain-v1.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Document API authentication in OpenAPI

**URL:** llms-txt#document-api-authentication-in-openapi

**Contents:**
- Default Schema
- Custom Security Schema
- Testing

Source: https://docs.langchain.com/langsmith/openapi-security

This guide shows how to customize the OpenAPI security schema for your LangSmith API documentation. A well-documented security schema helps API consumers understand how to authenticate with your API and even enables automatic client generation. See the [Authentication & Access Control conceptual guide](/langsmith/auth) for more details about LangGraph's authentication system.

<Note>
  **Implementation vs Documentation**
  This guide only covers how to document your security requirements in OpenAPI. To implement the actual authentication logic, see [How to add custom authentication](/langsmith/custom-auth).
</Note>

This guide applies to all LangSmith deployments (Cloud and self-hosted). It does not apply to usage of the LangGraph open source library if you are not using LangSmith.

The default security scheme varies by deployment type:

<Tabs>
  <Tab title="LangSmith" />
</Tabs>

By default, LangSmith requires a LangSmith API key in the `x-api-key` header:

When using one of the LangGraph SDK's, this can be inferred from environment variables.

<Tabs>
  <Tab title="Self-hosted" />
</Tabs>

By default, self-hosted deployments have no security scheme. This means they are to be deployed only on a secured network or with authentication. To add custom authentication, see [How to add custom authentication](/langsmith/custom-auth).

## Custom Security Schema

To customize the security schema in your OpenAPI documentation, add an `openapi` field to your `auth` configuration in `langgraph.json`. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in [How to add custom authentication](/langsmith/custom-auth).

Note that LangSmith does not provide authentication endpoints - you'll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.

<Tabs>
  <Tab title="OAuth2 with Bearer Token">
    
  </Tab>

<Tab title="API Key">
    
  </Tab>
</Tabs>

After updating your configuration:

1. Deploy your application
2. Visit `/docs` to see the updated OpenAPI documentation
3. Try out the endpoints using credentials from your authentication server (make sure you've implemented the authentication logic first)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/openapi-security.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
When using one of the LangGraph SDK's, this can be inferred from environment variables.

<Tabs>
  <Tab title="Self-hosted" />
</Tabs>

By default, self-hosted deployments have no security scheme. This means they are to be deployed only on a secured network or with authentication. To add custom authentication, see [How to add custom authentication](/langsmith/custom-auth).

## Custom Security Schema

To customize the security schema in your OpenAPI documentation, add an `openapi` field to your `auth` configuration in `langgraph.json`. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in [How to add custom authentication](/langsmith/custom-auth).

Note that LangSmith does not provide authentication endpoints - you'll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.

<Tabs>
  <Tab title="OAuth2 with Bearer Token">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="API Key">
```

---

## Document transformers

**URL:** llms-txt#document-transformers

Source: https://docs.langchain.com/oss/javascript/integrations/document_transformers/index

<Columns cols={3}>
  <Card title="html-to-text" icon="link" href="/oss/javascript/integrations/document_transformers/html-to-text" arrow="true" cta="View guide" />

<Card title="mozilla/readability" icon="link" href="/oss/javascript/integrations/document_transformers/mozilla_readability" arrow="true" cta="View guide" />

<Card title="OpenAI functions metadata tagger" icon="link" href="/oss/javascript/integrations/document_transformers/openai_metadata_tagger" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/document_transformers/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Double texting

**URL:** llms-txt#double-texting

**Contents:**
- Enqueue (default)
- Reject
- Interrupt
- Rollback

Source: https://docs.langchain.com/langsmith/double-texting

<Info>
  **Prerequisites**

* [Agent Server](/langsmith/agent-server)
</Info>

Many times users might interact with your graph in unintended ways.
For instance, a user may send one message and before the graph has finished running send a second message.
More generally, users may invoke the graph a second time before the first run has finished.
We call this "double texting".

[Enqueue](#enqueue-default) is the default double texting (multi-tasking) strategy when creating runs in the [Agent Server](/langsmith/agent-server).

<Note>
  Double texting is a feature of LangSmith Deployment. It is not available in the [LangGraph open source framework](/oss/python/langgraph/overview).
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=1cae1e8cd4920872e7992460b081f76d" alt="Double-text strategies across first vs. second run: Reject keeps only the first; Enqueue runs the second afterward; Interrupt halts the first to run the second; Rollback reverts the first and reruns with the second." data-og-width="1886" width="1886" data-og-height="648" height="648" data-path="langsmith/images/double-texting.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=280&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=67fc4d3817141da00d0f0e0b5c6de093 280w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=560&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=2c9cf620db602c51a7e3804cb0815058 560w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=840&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=7e16e946f3c616476fd99b40aa731a3c 840w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=1100&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=98032b626677cf73744a3922112abda4 1100w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=1650&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=2537ca1524e871001cd454f41dca6597 1650w, https://mintcdn.com/langchain-5e9cc07a/Hucw5hmCzWXDanL-/langsmith/images/double-texting.png?w=2500&fit=max&auto=format&n=Hucw5hmCzWXDanL-&q=85&s=d37e7e8cd1c4a5bad8cc399d0b878382 2500w" />

This option allows the current run to finish before processing any new input. Incoming requests are queued and executed sequentially once prior runs complete.

For configuring the enqueue double text option, refer to the [how-to guide](/langsmith/enqueue-concurrent).

This option rejects any additional incoming runs while a current run is in progress and prevents concurrent execution or double texting.

For configuring the reject double text option, refer to the [how-to guide](/langsmith/reject-concurrent).

This option halts the current execution and preserves the progress made up to the interruption point. The new user input is then inserted, and execution continues from that state.

When using this option, your graph must account for potential edge cases. For example, a tool call may have been initiated but not yet completed at the time of interruption. In these cases, handling or removing partial tool calls may be necessary to avoid unresolved operations.

For configuring the interrupt double text option, refer to the [how-to guide](/langsmith/interrupt-concurrent).

This option halts the current execution and reverts all progress—including the initial run input—before processing the new user input. The new input is treated as a fresh run, starting from the initial state.

For configuring the rollback double text option, refer to the [how-to guide](/langsmith/rollback-concurrent).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/double-texting.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Do NOT mistake this for the secret service role key

**URL:** llms-txt#do-not-mistake-this-for-the-secret-service-role-key

SUPABASE_ANON_KEY = os.environ.get("SUPABASE_ANON_KEY")
if not SUPABASE_ANON_KEY:
    SUPABASE_ANON_KEY = getpass("Enter your public Supabase anon  key: ")

async def sign_up(email: str, password: str):
    """Create a new user account."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{SUPABASE_URL}/auth/v1/signup",
            json={"email": email, "password": password},
            headers={"apiKey": SUPABASE_ANON_KEY},
        )
        assert response.status_code == 200
        return response.json()

---

## Durable execution

**URL:** llms-txt#durable-execution

**Contents:**
- Requirements
- Determinism and Consistent Replay
- Durability modes
- Using tasks in nodes
- Resuming Workflows
- Starting Points for Resuming Workflows

Source: https://docs.langchain.com/oss/python/langgraph/durable-execution

**Durable execution** is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require [human-in-the-loop](/oss/python/langgraph/interrupts), where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later).

LangGraph's built-in [persistence](/oss/python/langgraph/persistence) layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for [human-in-the-loop](/oss/python/langgraph/interrupts) interactions -- it can be resumed from its last recorded state.

<Tip>
  If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.
  To make the most of durable execution, ensure that your workflow is designed to be [deterministic](#determinism-and-consistent-replay) and [idempotent](#determinism-and-consistent-replay) and wrap any side effects or non-deterministic operations inside [tasks](/oss/python/langgraph/functional-api#task). You can use [tasks](/oss/python/langgraph/functional-api#task) from both the [StateGraph (Graph API)](/oss/python/langgraph/graph-api) and the [Functional API](/oss/python/langgraph/functional-api).
</Tip>

To leverage durable execution in LangGraph, you need to:

1. Enable [persistence](/oss/python/langgraph/persistence) in your workflow by specifying a [checkpointer](/oss/python/langgraph/persistence#checkpointer-libraries) that will save workflow progress.

2. Specify a [thread identifier](/oss/python/langgraph/persistence#threads) when executing a workflow. This will track the execution history for a particular instance of the workflow.

3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside [`task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task) to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see [Determinism and Consistent Replay](#determinism-and-consistent-replay).

## Determinism and Consistent Replay

When you resume a workflow run, the code does **NOT** resume from the **same line of code** where execution stopped; instead, it will identify an appropriate [starting point](#starting-points-for-resuming-workflows) from which to pick up where it left off. This means that the workflow will replay all steps from the [starting point](#starting-points-for-resuming-workflows) until it reaches the point where it was stopped.

As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside [tasks](/oss/python/langgraph/functional-api#task) or [nodes](/oss/python/langgraph/graph-api#nodes).

To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:

* **Avoid Repeating Work**: If a [node](/oss/python/langgraph/graph-api#nodes) contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate **task**. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.
* **Encapsulate Non-Deterministic Operations:** Wrap any code that might yield non-deterministic results (e.g., random number generation) inside **tasks** or **nodes**. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.
* **Use Idempotent Operations**: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a **task** starts but fails to complete successfully, the workflow's resumption will re-run the **task**, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.

For some examples of pitfalls to avoid, see the [Common Pitfalls](/oss/python/langgraph/functional-api#common-pitfalls) section in the functional API, which shows
how to structure your code using **tasks** to avoid these issues. The same principles apply to the [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph).

LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application's requirements. A higher durability mode adds more overhead to the workflow execution. You can specify the durability mode when calling any graph execution method:

The durability modes, from least to most durable, are as follows:

* `"exit"`: Changes are persisted only when graph execution exits (either successfully, with an error, or due to an interrupt). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from system failures (e.g., process crashes) that occur mid-execution.
* `"async"`: Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there's a small risk that checkpoints might not be written if the process crashes during execution.
* `"sync"`: Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.

## Using tasks in nodes

If a [node](/oss/python/langgraph/graph-api#nodes) contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.

<Tabs>
  <Tab title="Original">
    
  </Tab>

<Tab title="With task">
    
  </Tab>
</Tabs>

## Resuming Workflows

Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:

* **Pausing and Resuming Workflows:** Use the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function to pause a workflow at specific points and the [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) primitive to resume it with updated state. See [**Interrupts**](/oss/python/langgraph/interrupts) for more details.
* **Recovering from Failures:** Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `None` as the input value (see this [example](/oss/python/langgraph/use-functional-api#resuming-after-an-error) with the functional API).

## Starting Points for Resuming Workflows

* If you're using a [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph), the starting point is the beginning of the [**node**](/oss/python/langgraph/graph-api#nodes) where execution stopped.
* If you're making a subgraph call inside a node, the starting point will be the **parent** node that called the subgraph that was halted.
  Inside the subgraph, the starting point will be the specific [**node**](/oss/python/langgraph/graph-api#nodes) where execution stopped.
* If you're using the Functional API, the starting point is the beginning of the [**entrypoint**](/oss/python/langgraph/functional-api#entrypoint) where execution stopped.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/durable-execution.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The durability modes, from least to most durable, are as follows:

* `"exit"`: Changes are persisted only when graph execution exits (either successfully, with an error, or due to an interrupt). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from system failures (e.g., process crashes) that occur mid-execution.
* `"async"`: Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there's a small risk that checkpoints might not be written if the process crashes during execution.
* `"sync"`: Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.

## Using tasks in nodes

If a [node](/oss/python/langgraph/graph-api#nodes) contains multiple operations, you may find it easier to convert each operation into a **task** rather than refactor the operations into individual nodes.

<Tabs>
  <Tab title="Original">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="With task">
```

---

## Dynamic few shot example selection

**URL:** llms-txt#dynamic-few-shot-example-selection

**Contents:**
- Pre-conditions
- Index your dataset for few shot search
- Test search quality in the few shot playground
- Adding few shot search to your application
  - Code snippets

Source: https://docs.langchain.com/langsmith/index-datasets-for-dynamic-few-shot-example-selection

<Note>
  This feature is in open beta. It is only available to paid team plans. Please contact support via [support.langchain.com](https://support.langchain.com) if you have questions about enablement.
</Note>

Configure your datasets so that you can search for few shot examples based on an incoming request.

1. Your dataset must use the KV store data type (we do not currently support chat model or LLM type datasets)
2. You must have an input schema defined for your dataset. See our docs on setting up schema validation [in our UI](/langsmith/manage-datasets-in-application#dataset-schema-validation) for details.
3. You must be on a paid team plan (e.g. Plus plan)
4. You must be on LangSmith cloud

## Index your dataset for few shot search

Navigate to the datasets UI, and click the new `Few-Shot search` tab. Hit the `Start sync` button, which will create a new index on your dataset to make it searchable.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7bc50f1a3d4ea7b9e7458f3ed9770179" alt="Few shot tab unsynced" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-tab-unsynced.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c2ee3688a24b8d142d4a5dd89c8797c1 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=44a4d8b59948893b4bd20d4c1b62198c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=16a303eab8e2aede574c12353f782789 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fbcf4566ec48c219bb7828bdc2cfe6b9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=15da2b6b3e4a29ac3a2b4e139e2cf345 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-tab-unsynced.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6bd99520313da2fedbbbaf388c956ba7 2500w" />

By default, we sync to the latest version of your dataset. That means when new examples are added to your dataset, they will automatically be added to your index. This process runs every few minutes, so there should be a very short delay for indexing new examples. You can see whether your index is up to date under `Few-shot index` on the lefthand side of the screen in the next section.

## Test search quality in the few shot playground

Now that you have turned on indexing for your dataset, you will see the new few shot playground.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b2b714b5a281ab8f39761566afd452f7" alt="Few shot synced empty state" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-synced-empty-state.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b8e4e648ce36f12a141f98815fd2e63d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=49044eef3af8c08b60bcfeabadde6d16 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b0fb897716bfac9521d0490c11eb006e 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f21c2ad1fc5177af66c5f0c5ac59f660 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=dd9e2f9aa42366063ca14ad2b365e6fc 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-synced-empty-state.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=72158d44bcf916f2548bc5c7a6dabbce 2500w" />

You can type in a sample input, and check which results would be returned by our search API.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f64d39010f85b68d4d9ae3febc2b59d7" alt="Few shot search results" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-search-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cbbd01172c4514eb3d775a4c0f3ca2e4 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4a15ea167365f8d9105ab5aab50ed241 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=94ac208df6e860b9c194a646e3183779 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec68d9679e4cc86eebe352a9364e8894 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5650c218963b5e8bd8d56e0bf886cd3e 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-search-results.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=89e90c66c065bee2a728cd5dad69432a 2500w" />

Each result will have a score and a link to the example in the dataset. The scoring system works such that 0 is a completely random result, and higher scores are better. Results will be sorted in descending order according to score.

<Note>
  Search uses a BM25-like algorithm for keyword based similarity scores. The actual score is subject to change as we improve the search algorithm, so we recommend not relying on the scores themselves, as their meaning may evolve over time. They are simply used for convenience in vibe-testing outputs in the playground.
</Note>

## Adding few shot search to your application

Click the `Get Code Snippet` button in the previous diagram, you'll be taken to a screen that has code snippets from our LangSmith SDK in different languages.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=015d9416ef4f2708a6f2dfedceb1ea07" alt="Few shot code snippet" data-og-width="3208" width="3208" data-og-height="1902" height="1902" data-path="langsmith/images/few-shot-code-snippet.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a5b15cf813569f4f218d972f2dbe89a5 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=231a3aad9937aab1fa92e65839bb5869 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8aa86a24694342bca8ab387c83338afe 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d92247c22e6fb2fd85e80713616b7fba 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=28f6e81c67c52687cd24431b83145675 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-code-snippet.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=aeafca83ae2489c43aa38d24165e5d62 2500w" />

For code samples on using few shot search in LangChain python applications, please see our [how-to guide in the LangChain docs](https://python.langchain.com/v0.2/docs/how_to/example_selectors_langsmith/).

<Note>
  Please ensure you are using the python SDK with version >= 1.101 or the typescript SDK with version >= 1.43
</Note>

For copy and paste convenience, you can find the similar code snippets to the ones shown in the screenshot above here:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/index-datasets-for-dynamic-few-shot-example-selection.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

---

## Dynamic prompts

**URL:** llms-txt#dynamic-prompts

@dynamic_prompt
def dynamic_system_prompt(request: ModelRequest) -> str:
    user_name = request.runtime.context.user_name  # [!code highlight]
    system_prompt = f"You are a helpful assistant. Address the user as {user_name}."
    return system_prompt

---

## Edges taken after the `action` node is called.

**URL:** llms-txt#edges-taken-after-the-`action`-node-is-called.

workflow.add_conditional_edges(
    "retrieve",
    # Assess agent decision
    grade_documents,
)
workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

---

## Egress for subscription metrics and operational metadata

**URL:** llms-txt#egress-for-subscription-metrics-and-operational-metadata

**Contents:**
- LangSmith Telemetry
  - What we use it for
  - What we collect
  - How to disable
- Example payloads
  - License Verification
  - Usage Reporting
  - Telemetry: Operational LangSmith Metrics
  - Telemetry: Operational LangSmith Traces
- Our Commitment

Source: https://docs.langchain.com/langsmith/self-host-egress

<Info>
  This section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance serving version 0.9.0 or later. Previous versions of LangSmith did not have this feature.
</Info>

Self-Hosted LangSmith instances store all information locally and will never send sensitive information outside of your network. We currently only track platform usage for billing purposes according to the entitlements in your order. In order to better remotely support our customers, we do require egress to `https://beacon.langchain.com`.

In the future, we will be introducing support diagnostics to help us ensure that LangSmith is running at an optimal level within your environment.

<Warning>
  **This will require egress to `https://beacon.langchain.com` from your network. Refer to the [allowlisting IP section](/langsmith/cloud#allowlisting-ip-addresses) for static IP addresses, if needed.**
</Warning>

Generally, data that we send to Beacon can be categorized as follows:

* Subscription Metrics

* Subscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:

* Number of traces
    * Seats allocated per contract
    * Seats in currently use

* Operational Metadata
  * This metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.

## LangSmith Telemetry

As of version ***0.11***, LangSmith deployments will by default send telemetry data back to our backend. All telemetry data is associated with an organization and deployment, but never identified with individual users. We ***do not collect PII*** (personally identifiable information) in any form.

### What we use it for

* To provide more proactive support and faster troubleshooting of self-hosted instances.
* Assisting with performance tuning.
* Understanding real-world usage to prioritize improvements.

* **Request metadata**: anonymized request counts, sizes, and durations.
* **Database metrics**: query durations, error rates, and performance counters.
* **Operational LangSmith traces**: traces with timing and error information for high-latency or failed requests. These are **not** customer traces, these are operational traces about the functioning of the LangSmith instance.

<Info>
  We do not collect actual payload contents, database records, or any data that can identify your end users or customers.
</Info>

Set the following values in your `langsmith_config.yaml` file:

In an effort to maximize transparency, we provide sample payloads here:

### License Verification

`POST beacon.langchain.com/v1/beacon/verify`

`POST beacon.langchain.com/v1/beacon/ingest-traces`

### Telemetry: Operational LangSmith Metrics

`POST beacon.langchain.com/v1/beacon/v1/metrics`

### Telemetry: Operational LangSmith Traces

`POST beacon.langchain.com/v1/beacon/v1/traces`

LangChain will not store any sensitive information in the Subscription Metrics or Operational Metadata. Any data collected will not be shared with a third party. If you have any concerns about the data being sent, please reach out to your account team.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-egress.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Example payloads

In an effort to maximize transparency, we provide sample payloads here:

### License Verification

**Endpoint:**

`POST beacon.langchain.com/v1/beacon/verify`

**Request:**
```

Example 2 (unknown):
```unknown
**Response:**
```

Example 3 (unknown):
```unknown
### Usage Reporting

**Endpoint:**

`POST beacon.langchain.com/v1/beacon/ingest-traces`

**Request:**
```

Example 4 (unknown):
```unknown
**Response:**
```

---

## Embedding models

**URL:** llms-txt#embedding-models

**Contents:**
- Overview
  - How it works
  - Similarity metrics
- Interface
- Top integrations
- Caching

Source: https://docs.langchain.com/oss/python/integrations/text_embedding/index

<Note>
  This overview covers **text-based embedding models**. LangChain does not currently support multimodal embeddings.

See [top embedding models](#top-integrations).
</Note>

Embedding models transform raw text—such as a sentence, paragraph, or tweet—into a fixed-length vector of numbers that captures its **semantic meaning**. These vectors allow machines to compare and search text based on meaning rather than exact words.

In practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase *"machine learning"*, embeddings can surface documents that discuss related concepts even when different wording is used.

1. **Vectorization** — The model encodes each input string as a high-dimensional vector.
2. **Similarity scoring** — Vectors are compared using mathematical metrics to measure how closely related the underlying texts are.

### Similarity metrics

Several metrics are commonly used to compare embeddings:

* **Cosine similarity** — measures the angle between two vectors.
* **Euclidean distance** — measures the straight-line distance between points.
* **Dot product** — measures how much one vector projects onto another.

Here's an example of computing cosine similarity between two vectors:

LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the [Embeddings](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) interface.

Two main methods are available:

* `embed_documents(texts: List[str]) → List[List[float]]`: Embeds a list of documents.
* `embed_query(text: str) → List[float]`: Embeds a single query.

<Note>
  The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.
</Note>

| Model                                                                                          | Package                                                                                                                                                            |
| ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [`OpenAIEmbeddings`](/oss/python/integrations/text_embedding/openai)                           | [`langchain-openai`](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)                              |
| [`AzureOpenAIEmbeddings`](/oss/python/integrations/text_embedding/azure_openai)                | [`langchain-openai`](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html)                    |
| [`GoogleGenerativeAIEmbeddings`](/oss/python/integrations/text_embedding/google_generative_ai) | [`langchain-google-genai`](https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html) |
| [`OllamaEmbeddings`](/oss/python/integrations/text_embedding/ollama)                           | [`langchain-ollama`](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)                               |
| [`TogetherEmbeddings`](/oss/python/integrations/text_embedding/together)                       | [`langchain-together`](https://python.langchain.com/api_reference/together/embeddings/langchain_together.embeddings.TogetherEmbeddings.html)                       |
| [`FireworksEmbeddings`](/oss/python/integrations/text_embedding/fireworks)                     | [`langchain-fireworks`](https://python.langchain.com/api_reference/fireworks/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html)                   |
| [`MistralAIEmbeddings`](/oss/python/integrations/text_embedding/mistralai)                     | [`langchain-mistralai`](https://python.langchain.com/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.MistralAIEmbeddings.html)                   |
| [`CohereEmbeddings`](/oss/python/integrations/text_embedding/cohere)                           | [`langchain-cohere`](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.cohere.Cohere.html)                                        |
| [`NomicEmbeddings`](/oss/python/integrations/text_embedding/nomic)                             | [`langchain-nomic`](https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html)                                   |
| [`FakeEmbeddings`](/oss/python/integrations/text_embedding/fake)                               | [`langchain-core`](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.fake.FakeEmbeddings.html)                                  |
| [`DatabricksEmbeddings`](/oss/python/integrations/text_embedding/databricks)                   | [`databricks-langchain`](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.DatabricksEmbeddings)   |
| [`WatsonxEmbeddings`](/oss/python/integrations/text_embedding/ibm_watsonx)                     | [`langchain-ibm`](https://python.langchain.com/api_reference/ibm/embeddings/langchain_ibm.embeddings.WatsonxEmbeddings.html)                                       |
| [`NVIDIAEmbeddings`](/oss/python/integrations/text_embedding/nvidia_ai_endpoints)              | [`langchain-nvidia`](https://python.langchain.com/api_reference/nvidia_ai_endpoints/embeddings/langchain_nvidia_ai_endpoints.embeddings.NVIDIAEmbeddings.html)     |
| [`AimlapiEmbeddings`](/oss/python/integrations/text_embedding/aimlapi)                         | [`langchain-aimlapi`](https://python.langchain.com/api_reference/aimlapi/embeddings/langchain_aimlapi.embeddings.AimlapiEmbeddings.html)                           |

Embeddings can be stored or temporarily cached to avoid needing to recompute them.

Caching embeddings can be done using a `CacheBackedEmbeddings`. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.

The main supported way to initialize a `CacheBackedEmbeddings` is `from_bytes_store`. It takes the following parameters:

* **`underlying_embedder`**: The embedder to use for embedding.
* **`document_embedding_cache`**: Any [`ByteStore`](/oss/python/integrations/stores/) for caching document embeddings.
* **`batch_size`**: (optional, defaults to `None`) The number of documents to embed between store updates.
* **`namespace`**: (optional, defaults to `""`) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).
* **`query_embedding_cache`**: (optional, defaults to `None`) A [`ByteStore`](/oss/python/integrations/stores/) for caching query embeddings, or `True` to reuse the same store as `document_embedding_cache`.

<Important>
  - Always set the `namespace` parameter to avoid collisions when using different embedding models.
  - `CacheBackedEmbeddings` does not cache query embeddings by default. To enable this, specify a `query_embedding_cache`.
</Important>

```python  theme={null}
import time
from langchain_classic.embeddings import CacheBackedEmbeddings  # [!code highlight]
from langchain_classic.storage import LocalFileStore # [!code highlight]
from langchain_core.vectorstores import InMemoryVectorStore

**Examples:**

Example 1 (unknown):
```unknown
## Interface

LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the [Embeddings](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) interface.

Two main methods are available:

* `embed_documents(texts: List[str]) → List[List[float]]`: Embeds a list of documents.
* `embed_query(text: str) → List[float]`: Embeds a single query.

<Note>
  The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.
</Note>

## Top integrations

| Model                                                                                          | Package                                                                                                                                                            |
| ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [`OpenAIEmbeddings`](/oss/python/integrations/text_embedding/openai)                           | [`langchain-openai`](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)                              |
| [`AzureOpenAIEmbeddings`](/oss/python/integrations/text_embedding/azure_openai)                | [`langchain-openai`](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.azure.AzureOpenAIEmbeddings.html)                    |
| [`GoogleGenerativeAIEmbeddings`](/oss/python/integrations/text_embedding/google_generative_ai) | [`langchain-google-genai`](https://python.langchain.com/api_reference/google_genai/embeddings/langchain_google_genai.embeddings.GoogleGenerativeAIEmbeddings.html) |
| [`OllamaEmbeddings`](/oss/python/integrations/text_embedding/ollama)                           | [`langchain-ollama`](https://python.langchain.com/api_reference/ollama/embeddings/langchain_ollama.embeddings.OllamaEmbeddings.html)                               |
| [`TogetherEmbeddings`](/oss/python/integrations/text_embedding/together)                       | [`langchain-together`](https://python.langchain.com/api_reference/together/embeddings/langchain_together.embeddings.TogetherEmbeddings.html)                       |
| [`FireworksEmbeddings`](/oss/python/integrations/text_embedding/fireworks)                     | [`langchain-fireworks`](https://python.langchain.com/api_reference/fireworks/embeddings/langchain_fireworks.embeddings.FireworksEmbeddings.html)                   |
| [`MistralAIEmbeddings`](/oss/python/integrations/text_embedding/mistralai)                     | [`langchain-mistralai`](https://python.langchain.com/api_reference/mistralai/embeddings/langchain_mistralai.embeddings.MistralAIEmbeddings.html)                   |
| [`CohereEmbeddings`](/oss/python/integrations/text_embedding/cohere)                           | [`langchain-cohere`](https://python.langchain.com/api_reference/community/llms/langchain_community.llms.cohere.Cohere.html)                                        |
| [`NomicEmbeddings`](/oss/python/integrations/text_embedding/nomic)                             | [`langchain-nomic`](https://python.langchain.com/api_reference/nomic/embeddings/langchain_nomic.embeddings.NomicEmbeddings.html)                                   |
| [`FakeEmbeddings`](/oss/python/integrations/text_embedding/fake)                               | [`langchain-core`](https://python.langchain.com/api_reference/core/embeddings/langchain_core.embeddings.fake.FakeEmbeddings.html)                                  |
| [`DatabricksEmbeddings`](/oss/python/integrations/text_embedding/databricks)                   | [`databricks-langchain`](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks_langchain.html#databricks_langchain.DatabricksEmbeddings)   |
| [`WatsonxEmbeddings`](/oss/python/integrations/text_embedding/ibm_watsonx)                     | [`langchain-ibm`](https://python.langchain.com/api_reference/ibm/embeddings/langchain_ibm.embeddings.WatsonxEmbeddings.html)                                       |
| [`NVIDIAEmbeddings`](/oss/python/integrations/text_embedding/nvidia_ai_endpoints)              | [`langchain-nvidia`](https://python.langchain.com/api_reference/nvidia_ai_endpoints/embeddings/langchain_nvidia_ai_endpoints.embeddings.NVIDIAEmbeddings.html)     |
| [`AimlapiEmbeddings`](/oss/python/integrations/text_embedding/aimlapi)                         | [`langchain-aimlapi`](https://python.langchain.com/api_reference/aimlapi/embeddings/langchain_aimlapi.embeddings.AimlapiEmbeddings.html)                           |

## Caching

Embeddings can be stored or temporarily cached to avoid needing to recompute them.

Caching embeddings can be done using a `CacheBackedEmbeddings`. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.

The main supported way to initialize a `CacheBackedEmbeddings` is `from_bytes_store`. It takes the following parameters:

* **`underlying_embedder`**: The embedder to use for embedding.
* **`document_embedding_cache`**: Any [`ByteStore`](/oss/python/integrations/stores/) for caching document embeddings.
* **`batch_size`**: (optional, defaults to `None`) The number of documents to embed between store updates.
* **`namespace`**: (optional, defaults to `""`) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).
* **`query_embedding_cache`**: (optional, defaults to `None`) A [`ByteStore`](/oss/python/integrations/stores/) for caching query embeddings, or `True` to reuse the same store as `document_embedding_cache`.

<Important>
  - Always set the `namespace` parameter to avoid collisions when using different embedding models.
  - `CacheBackedEmbeddings` does not cache query embeddings by default. To enable this, specify a `query_embedding_cache`.
</Important>
```

---

## enabled: true

**URL:** llms-txt#enabled:-true

---

## Enable blob storage

**URL:** llms-txt#enable-blob-storage

**Contents:**
- Requirements
- Authentication
  - Amazon S3
  - Google Cloud Storage
  - Azure Blob Storage
- CH Search
- Configuration
- TTL Configuration
  - Amazon S3
  - Google Cloud Storage

Source: https://docs.langchain.com/langsmith/self-host-blob-storage

By default, LangSmith stores run inputs, outputs, errors, manifests, extras, and events in ClickHouse. If you so choose, you can instead store this information in blob storage, which has a couple of notable benefits. For the best results in production deployments, we **strongly** recommend using blob storage, which offers the following benefits:

1. In high trace environments, inputs, outputs, errors, manifests, extras, and events may balloon the size of your databases.
2. If using LangSmith Managed ClickHouse, you may want sensitive information in blob storage that resides in your environment. To alleviate this, LangSmith supports storing run inputs, outputs, errors, manifests, extras, events, and attachments in an external blob storage system.

<Note>
  Azure blob storage is available in Helm chart versions 0.8.9 and greater. [Deleting trace projects](/langsmith/observability-concepts#deleting-traces-from-langsmith) is supported in Azure starting in Helm chart version 0.10.43.
</Note>

* Access to a valid blob storage service

* [Amazon S3](https://aws.amazon.com/s3/)
    * [Google Cloud Storage (GCS)](https://cloud.google.com/storage?hl=en)
  * [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs)

* A bucket/directory in your blob storage to store the data. We highly recommend creating a separate bucket/directory for LangSmith data.
  * **If you are using TTLs**, you will need to set up a lifecycle policy to delete old data. You can find more information on configuring TTLs [here](/langsmith/self-host-ttl). These policies should mirror the TTLs you have set in your LangSmith configuration, or you may experience data loss. See [here](#ttl-configuration) on how to setup the lifecycle rules for TTLs for blob storage.

* Credentials to permit LangSmith Services to access the bucket/directory
  * You will need to provide your LangSmith instance with the necessary credentials to access the bucket/directory. Read the authentication [section](#authentication) below for more information.

* If using S3 or GCS, an API url for your blob storage service

* This will be the URL that LangSmith uses to access your blob storage system
  * For Amazon S3, this will be the URL of the S3 endpoint. Something like: `https://s3.amazonaws.com` or `https://s3.us-west-1.amazonaws.com` if using a regional endpoint.
  * For Google Cloud Storage, this will be the URL of the GCS endpoint. Something like: `https://storage.googleapis.com`

To authenticate to [Amazon S3](https://aws.amazon.com/s3/), you will need to create an IAM policy granting the following permissions on your bucket.

Once you have the correct policy, there are three ways to authenticate with Amazon S3:

1. [**(Recommended) IAM Role for Service Account**](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html): You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.

1. You will need to create an IAM role with the policy attached.
   2. You will need to allow LangSmith service accounts to assume the role. The `langsmith-queue`, `langsmith-backend`, and `langsmith-platform-backend` service accounts will need to be able to assume the role.
      <Warning>
        The service account names will be different if you are using a custom release name. You can find the service account names by running `kubectl get serviceaccounts` in your cluster.
      </Warning>
   3. You will need to provide the role ARN to LangSmith. You can do this by adding the `eks.amazonaws.com/role-arn: "<role_arn>"` annotation to the `queue`, `backend`, and `platform-backend` services in your Helm Chart installation.

2. [**Access Key and Secret Key**](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html): You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.
   1. You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user.

3. [**VPC Endpoint Access**](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html): You can enable access to your S3 bucket via a VPC endpoint, which allows traffic to flow securely from your VPC to your S3 bucket.

1. You'll need to provision a VPC endpoint and configure it to allow access to your S3 bucket.
   2. You can refer to our [public Terraform modules](https://github.com/langchain-ai/terraform/blob/main/modules/aws/s3/main.tf#L12) for guidance and an example of configuring this.

#### KMS encryption header support for S3

Starting with LangSmith Helm chart version **0.11.24**, you can pass a KMS encryption key header and enforce a specific KMS key for writes by providing its ARN. To enable this, set the following values in your Helm chart:

### Google Cloud Storage

To authenticate with [Google Cloud Storage](https://cloud.google.com/storage?hl=en), you will need to create a [`service account`](https://cloud.google.com/iam/docs/service-account-overview) with the necessary permissions to access your bucket.

Your service account will need the `Storage Admin` role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using.

Once you have a provisioned service account, you will need to generate a [`HMAC key`](https://cloud.google.com/storage/docs/authentication/hmackeys) for that service account. This key and secret will be used to authenticate with Google Cloud Storage.

### Azure Blob Storage

To authenticate with [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), you will need to use one of the following methods to grant LangSmith workloads permission to access your [container](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#containers) (listed in order of precedence):

1. [Storage account and access key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage)
2. [Connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string)
3. [Workload identity](https://azure.github.io/azure-workload-identity/docs/introduction.html) (recommended), managed identity, or environment variables supported by [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication?tabs=bash#2-authenticate-with-azure). This is the default authentication method when configuration for either option above is not present.
   1. To use workload identity, add the label `azure.workload.identity/use: true` to the `queue`, `backend`, and `platform-backend` deployments. Additionally, add the `azure.workload.identity/client-id` annotation to the corresponding service accounts, which should be an existing Azure AD Application's client ID or user-assigned managed identity's client ID. See [Azure's documentation](https://azure.github.io/azure-workload-identity/docs/topics/service-account-labels-and-annotations.html) for additional details.

<Note>
  Some deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL (`https://<storage_account_name>.blob.core.windows.net/`). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).
</Note>

By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.

After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.

<Note>
  Google Cloud Storage (GCS) exposes an S3-compatible API. When using GCS, set the blob storage engine to "S3", configure the <code>apiURL</code> to your GCS endpoint (for example, <code>[https://storage.googleapis.com](https://storage.googleapis.com)</code>), and authenticate using a service account HMAC access key and secret (using `accessKey` and `accessKeySecret`).

You must use **HMAC** access key and secret for GCS. We do not support service account annotations.
</Note>

<Note>
  If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the [generated secret template](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml) for the expected secret keys.
</Note>

If using the [TTL](/langsmith/self-host-ttl) feature with LangSmith, you'll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace's retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention.

The following TTL prefix are used:

* `ttl_s/`: Short term TTL, configured for 14 days.
* `ttl_l/`: Long term TTL, configured for 400 days.

If you have customized the TTLs in your LangSmith configuration, you will need to adjust the TTLs in your blob storage configuration to match.

If using S3 for your blob storage, you will need to setup a filter lifecycle configuration that matches the prefixes above. You can find information for this [in the Amazon Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-filter).

As an example, if you are using Terraform to manage your S3 bucket, you would setup something like this:

### Google Cloud Storage

You will need to setup lifecycle conditions for your GCS buckets that you are using. You can find information for this [in the Google Documentation](https://cloud.google.com/storage/docs/lifecycle#conditions), specifically using matchesPrefix.

As an example, if you are using Terraform to manage your GCS bucket, you would setup something like this:

### Azure blob storage

You will need to configure a [lifecycle management policy](https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-policy-configure) on the container in order to expire objects matching the prefixes above.

As an example, if you are [using Terraform to manage your blob storage container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_management_policy), you would setup something like this:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-blob-storage.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Once you have the correct policy, there are three ways to authenticate with Amazon S3:

1. [**(Recommended) IAM Role for Service Account**](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html): You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production.

   1. You will need to create an IAM role with the policy attached.
   2. You will need to allow LangSmith service accounts to assume the role. The `langsmith-queue`, `langsmith-backend`, and `langsmith-platform-backend` service accounts will need to be able to assume the role.
      <Warning>
        The service account names will be different if you are using a custom release name. You can find the service account names by running `kubectl get serviceaccounts` in your cluster.
      </Warning>
   3. You will need to provide the role ARN to LangSmith. You can do this by adding the `eks.amazonaws.com/role-arn: "<role_arn>"` annotation to the `queue`, `backend`, and `platform-backend` services in your Helm Chart installation.

2. [**Access Key and Secret Key**](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html): You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure.
   1. You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user.

3. [**VPC Endpoint Access**](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html): You can enable access to your S3 bucket via a VPC endpoint, which allows traffic to flow securely from your VPC to your S3 bucket.

   1. You'll need to provision a VPC endpoint and configure it to allow access to your S3 bucket.
   2. You can refer to our [public Terraform modules](https://github.com/langchain-ai/terraform/blob/main/modules/aws/s3/main.tf#L12) for guidance and an example of configuring this.

#### KMS encryption header support for S3

Starting with LangSmith Helm chart version **0.11.24**, you can pass a KMS encryption key header and enforce a specific KMS key for writes by providing its ARN. To enable this, set the following values in your Helm chart:
```

Example 2 (unknown):
```unknown
### Google Cloud Storage

To authenticate with [Google Cloud Storage](https://cloud.google.com/storage?hl=en), you will need to create a [`service account`](https://cloud.google.com/iam/docs/service-account-overview) with the necessary permissions to access your bucket.

Your service account will need the `Storage Admin` role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using.

Once you have a provisioned service account, you will need to generate a [`HMAC key`](https://cloud.google.com/storage/docs/authentication/hmackeys) for that service account. This key and secret will be used to authenticate with Google Cloud Storage.

### Azure Blob Storage

To authenticate with [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), you will need to use one of the following methods to grant LangSmith workloads permission to access your [container](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#containers) (listed in order of precedence):

1. [Storage account and access key](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage)
2. [Connection string](https://learn.microsoft.com/en-us/azure/storage/common/storage-configure-connection-string)
3. [Workload identity](https://azure.github.io/azure-workload-identity/docs/introduction.html) (recommended), managed identity, or environment variables supported by [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/azure/developer/go/azure-sdk-authentication?tabs=bash#2-authenticate-with-azure). This is the default authentication method when configuration for either option above is not present.
   1. To use workload identity, add the label `azure.workload.identity/use: true` to the `queue`, `backend`, and `platform-backend` deployments. Additionally, add the `azure.workload.identity/client-id` annotation to the corresponding service accounts, which should be an existing Azure AD Application's client ID or user-assigned managed identity's client ID. See [Azure's documentation](https://azure.github.io/azure-workload-identity/docs/topics/service-account-labels-and-annotations.html) for additional details.

<Note>
  Some deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL (`https://<storage_account_name>.blob.core.windows.net/`). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).
</Note>

## CH Search

By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.

## Configuration

After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.

<Note>
  Google Cloud Storage (GCS) exposes an S3-compatible API. When using GCS, set the blob storage engine to "S3", configure the <code>apiURL</code> to your GCS endpoint (for example, <code>[https://storage.googleapis.com](https://storage.googleapis.com)</code>), and authenticate using a service account HMAC access key and secret (using `accessKey` and `accessKeySecret`).

  You must use **HMAC** access key and secret for GCS. We do not support service account annotations.
</Note>

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

<Note>
  If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the [generated secret template](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/templates/secrets.yaml) for the expected secret keys.
</Note>

## TTL Configuration

If using the [TTL](/langsmith/self-host-ttl) feature with LangSmith, you'll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace's retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention.

The following TTL prefix are used:

* `ttl_s/`: Short term TTL, configured for 14 days.
* `ttl_l/`: Long term TTL, configured for 400 days.

If you have customized the TTLs in your LangSmith configuration, you will need to adjust the TTLs in your blob storage configuration to match.

### Amazon S3

If using S3 for your blob storage, you will need to setup a filter lifecycle configuration that matches the prefixes above. You can find information for this [in the Amazon Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-filter).

As an example, if you are using Terraform to manage your S3 bucket, you would setup something like this:
```

---

## Enable TTL and data retention

**URL:** llms-txt#enable-ttl-and-data-retention

**Contents:**
- Requirements
- ClickHouse TTL Cleanup Job
  - Default Schedule
  - Disabling the Job
  - Configuring the Schedule
  - Configuring Minimum Expired Rows Per Part
  - Configuring Maximum Active Mutations
  - Emergency: Stopping Running Mutations
  - Backups and Data Retention

Source: https://docs.langchain.com/langsmith/self-host-ttl

LangSmith Self-Hosted allows enablement of automatic TTL and Data Retention of traces. This can be useful if you're complying with data privacy regulations, or if you want to have more efficient space usage and auto cleanup of your traces. Traces will also have their data retention period automatically extended based on certain actions or run rule applications.

You can configure retention through helm or environment variable settings. There are a few options that are configurable:

* *Enabled:* Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see [data retention guide](/langsmith/administration-overview#data-retention) for details).
* *Retention Periods:* You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects.

## ClickHouse TTL Cleanup Job

As of version **0.11**, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse's built-in TTL mechanism.

<Warning>
  This job uses potentially long running **mutations** (`ALTER TABLE DELETE`), which are expensive operations that can impact ClickHouse's performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with **1 concurrent active** mutation (default), we did not observe significant CPU, memory, or latency increases.
</Warning>

By default, the cleanup job runs:

* **Saturday**: 8pm and 10pm UTC
* **Sunday**: 12am, 2am, and 4am UTC

### Disabling the Job

To disable the cleanup job entirely:

### Configuring the Schedule

You can customize when the cleanup job runs by modifying the cron expressions:

<Tip>
  To run the job on a single cron schedule, set both `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING` and `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING` to the same value. Job locking prevents overlapping executions.
</Tip>

### Configuring Minimum Expired Rows Per Part

The job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:

* **Too low**: Job scans entire parts to clear minimal data (inefficient)
* **Too high**: Job misses parts with significant expired data

#### Checking Expired Rows

Use this query to analyze expired rows in your tables, and tweak your minimum value accordingly:

### Configuring Maximum Active Mutations

Delete operations can be time-consuming (\~50 minutes for a 100GB part). You can increase concurrent mutations to speed up the process:

<Warning>
  Increasing concurrent DELETE operations can severely impact system performance. Monitor your system carefully and only increase this value if you can tolerate potentially slower insert and read latencies.
</Warning>

### Emergency: Stopping Running Mutations

If you experience latency spikes and need to terminate a running mutation:

1. **Find active mutations**:

Look for the `mutation_id` where the `command` column contains a `DELETE` statement.

2. **Kill the mutation**:

### Backups and Data Retention

If disk space does not decrease after running this job, or if it continues to increase, backups may be causing the issue by creating file system hard links. These links prevent ClickHouse from cleaning up the data.

To verify, check the following directories inside your ClickHouse pod:

* `/var/lib/clickhouse/backup`
* `/var/lib/clickhouse/shadow`

If backups are present, copy them to an external filesystem or blob storage (e.g., S3), then clear the directories. Within a few minutes, you will notice disk space releasing.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-ttl.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## ClickHouse TTL Cleanup Job

As of version **0.11**, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse's built-in TTL mechanism.

<Warning>
  This job uses potentially long running **mutations** (`ALTER TABLE DELETE`), which are expensive operations that can impact ClickHouse's performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with **1 concurrent active** mutation (default), we did not observe significant CPU, memory, or latency increases.
</Warning>

### Default Schedule

By default, the cleanup job runs:

* **Saturday**: 8pm and 10pm UTC
* **Sunday**: 12am, 2am, and 4am UTC

### Disabling the Job

To disable the cleanup job entirely:
```

Example 3 (unknown):
```unknown
### Configuring the Schedule

You can customize when the cleanup job runs by modifying the cron expressions:
```

Example 4 (unknown):
```unknown
<Tip>
  To run the job on a single cron schedule, set both `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING` and `CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING` to the same value. Job locking prevents overlapping executions.
</Tip>

### Configuring Minimum Expired Rows Per Part

The job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:

* **Too low**: Job scans entire parts to clear minimal data (inefficient)
* **Too high**: Job misses parts with significant expired data
```

---

## End runs

**URL:** llms-txt#end-runs

**Contents:**
- Batch Ingestion

patch_run(child_run_id, chat_completion.dict())
patch_run(parent_run_id, {"answer": chat_completion.choices[0].message.content})
python  theme={null}
import json
import os
import uuid
from datetime import datetime, timezone
from typing import Dict, List
import requests
from requests_toolbelt import MultipartEncoder
from uuid_utils.compat import uuid7

def create_dotted_order(
    start_time: datetime | None = None,
    run_id: uuid.UUID | None = None
) -> str:
    """Create a dotted order string for run ordering and hierarchy.

The dotted order is used to establish the sequence and relationships between runs.
    It combines a timestamp with a unique identifier to ensure proper ordering and tracing.
    """
    st = start_time or datetime.now(timezone.utc)
    id_ = run_id or uuid7()
    return f"{st.strftime('%Y%m%dT%H%M%S%fZ')}{id_}"

def create_run_base(
    name: str,
    run_type: str,
    inputs: dict,
    start_time: datetime
) -> dict:
    """Create the base structure for a run."""
    run_id = uuid7()
    return {
        "id": str(run_id),
        "trace_id": str(run_id),
        "name": name,
        "start_time": start_time.isoformat(),
        "inputs": inputs,
        "run_type": run_type,
    }

def construct_run(
    name: str,
    run_type: str,
    inputs: dict,
    parent_dotted_order: str | None = None,
) -> dict:
    """Construct a run dictionary with the given parameters.

This function creates a run with a unique ID and dotted order, establishing its place
    in the trace hierarchy if it's a child run.
    """
    start_time = datetime.now(timezone.utc)
    run = create_run_base(name, run_type, inputs, start_time)
    current_dotted_order = create_dotted_order(start_time, uuid.UUID(run["id"]))

if parent_dotted_order:
        current_dotted_order = f"{parent_dotted_order}.{current_dotted_order}"
        run["trace_id"] = parent_dotted_order.split(".")[0].split("Z")[1]
        run["parent_run_id"] = parent_dotted_order.split(".")[-1].split("Z")[1]

run["dotted_order"] = current_dotted_order
    return run

def serialize_run(operation: str, run_data: dict) -> List[tuple]:
    """Serialize a run for the multipart request.

This function separates the run data into parts for efficient transmission and storage.
    The main run data and optional fields (inputs, outputs, events) are serialized separately.
    """
    run_id = run_data.get("id", str(uuid7()))

# Separate optional fields
    inputs = run_data.pop("inputs", None)
    outputs = run_data.pop("outputs", None)
    events = run_data.pop("events", None)

# Serialize main run data
    run_data_json = json.dumps(run_data).encode("utf-8")
    parts.append(
        (
            f"{operation}.{run_id}",
            (
                None,
                run_data_json,
                "application/json",
                {"Content-Length": str(len(run_data_json))},
            ),
        )
    )

# Serialize optional fields
    for key, value in [("inputs", inputs), ("outputs", outputs), ("events", events)]:
        if value:
            serialized_value = json.dumps(value).encode("utf-8")
            parts.append(
                (
                    f"{operation}.{run_id}.{key}",
                    (
                        None,
                        serialized_value,
                        "application/json",
                        {"Content-Length": str(len(serialized_value))},
                    ),
                )
            )

def batch_ingest_runs(
    api_url: str,
    api_key: str,
    posts: list[dict] | None = None,
    patches: list[dict] | None = None,
) -> None:
    """Ingest multiple runs in a single batch request.

This function handles both creating new runs (posts) and updating existing runs (patches).
    It's more efficient for ingesting multiple runs compared to individual API calls.
    """
    boundary = uuid.uuid4().hex
    all_parts = []

for operation, runs in zip(("post", "patch"), (posts, patches)):
        if runs:
            all_parts.extend(
                [part for run in runs for part in serialize_run(operation, run)]
            )

encoder = MultipartEncoder(fields=all_parts, boundary=boundary)
    headers = {"Content-Type": encoder.content_type, "x-api-key": api_key}

try:
        response = requests.post(
            f"{api_url}/runs/multipart",
            data=encoder,
            headers=headers
        )
        response.raise_for_status()
        print("Successfully ingested runs.")
    except requests.RequestException as e:
        print(f"Error ingesting runs: {e}")
        # In a production environment, you might want to log this error or handle it more robustly

**Examples:**

Example 1 (unknown):
```unknown
See the doc on the [Run (span) data format](/langsmith/run-data-format) for more information.

## Batch Ingestion

For faster ingestion of runs and higher rate limits, you can use the POST `/runs/multipart` [link](https://api.smith.langchain.com/redoc#tag/run/operation/multipart_ingest_runs_api_v1_runs_multipart_post) endpoint. Below is an example. It requires `orjson` (for fast json ), [`requests-toolbelt`](https://pypi.org/project/requests-toolbelt/) and [`uuid-utils`](https://pypi.org/project/uuid-utils/) to run
```

---

## Enforce previous behavior with output_version flag

**URL:** llms-txt#enforce-previous-behavior-with-output_version-flag

**Contents:**
  - Default `max_tokens` in `langchain-anthropic`
  - Legacy code moved to `langchain-classic`
  - Removal of deprecated APIs
  - Text property

model = ChatOpenAI(model="gpt-4o-mini", output_version="v0")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Default `max_tokens` in `langchain-anthropic`

The `max_tokens` parameter in `langchain-anthropic` now defaults to higher values based on the model chosen, rather than the previous default of `1024`. If you relied on the old default, explicitly set `max_tokens=1024`.

### Legacy code moved to `langchain-classic`

Existing functionality outside the focus of standard interfaces and agents has been moved to the [`langchain-classic`](https://pypi.org/project/langchain-classic) package. See the [Simplified namespace](#simplified-package) section for details on what's available in the core `langchain` package and what moved to `langchain-classic`.

### Removal of deprecated APIs

Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the [deprecation notices](https://python.langchain.com/docs/versions/migrating_chains) from previous versions for replacement APIs.

### Text property

Use of the `.text()` method on message objects should drop the parentheses, as it is now a property:
```

---

## Enqueue concurrent

**URL:** llms-txt#enqueue-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/enqueue-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `enqueue` option for double texting, which adds the interruptions to a queue and executes them in the order they are received by the client. Below is a quick example of using the `enqueue` option.

Enqueue is the default double texting (multi-tasking) strategy when creating runs in the [Agent Server](/langsmith/agent-server).

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Then, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now let's start two runs, with the second interrupting the first one with a multitask strategy of "enqueue":

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the thread has data from both runs:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/enqueue-concurrent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Then, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Environment variables

**URL:** llms-txt#environment-variables

**Contents:**
- `BG_JOB_ISOLATED_LOOPS`
- `BG_JOB_SHUTDOWN_GRACE_PERIOD_SECS`
- `BG_JOB_TIMEOUT_SECS`
- `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`

Source: https://docs.langchain.com/langsmith/env-var

The Agent Server supports specific environment variables for configuring a deployment.

## `BG_JOB_ISOLATED_LOOPS`

Set `BG_JOB_ISOLATED_LOOPS` to `True` to execute background runs in an isolated event loop separate from the serving API event loop.

This environment variable should be set to `True` if the implementation of a graph/node contains synchronous code. In this situation, the synchronous code will block the serving API event loop, which may cause the API to be unavailable. A symptom of an unavailable API is continuous application restarts due to failing health checks.

## `BG_JOB_SHUTDOWN_GRACE_PERIOD_SECS`

Specifies, in seconds, how long the server will wait for background jobs to finish after the queue receives a shutdown signal. After this period, the server will force termination. Defaults to `180` seconds. The maximum value is `3600` seconds. Set this to ensure jobs have enough time to complete cleanly during shutdown. Added in `langgraph-api==0.2.16`.

## `BG_JOB_TIMEOUT_SECS`

The timeout of a background run can be increased. However, the infrastructure for a Cloud deployment enforces a 1 hour timeout limit for API requests. This means the connection between client and server will timeout after 1 hour. This is not configurable.

A background run can execute for longer than 1 hour, but a client must reconnect to the server (e.g. join stream via `POST /threads/{thread_id}/runs/{run_id}/stream`) to retrieve output from the run if the run is taking longer than 1 hour.

## `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT`

Specify `OTEL_EXPORTER_OTLP_TRACES_ENDPOINT` to configure OpenTelemetry APM tracing for the deployment. Specify other [`OTEL_*` environment variables](https://opentelemetry.io/docs/collector/configuration/) to configure tracing, logging, and other instrumentation.

```shell  theme={null}

---

## Errors

**URL:** llms-txt#errors

Source: https://docs.langchain.com/oss/python/common-errors

This page contains guides around resolving common errors you may find while building with LangChain and LangGraph.

Errors referenced below will have an `lc_error_code` property corresponding to one of the below codes when they are thrown in code.

| Error code                                                                                          |
| --------------------------------------------------------------------------------------------------- |
| [GRAPH\_RECURSION\_LIMIT](/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT)                       |
| [INVALID\_CHAT\_HISTORY](/oss/python/langgraph/errors/INVALID_CHAT_HISTORY)                         |
| [INVALID\_CONCURRENT\_GRAPH\_UPDATE](/oss/python/langgraph/errors/INVALID_CONCURRENT_GRAPH_UPDATE)  |
| [INVALID\_GRAPH\_NODE\_RETURN\_VALUE](/oss/python/langgraph/errors/INVALID_GRAPH_NODE_RETURN_VALUE) |
| [INVALID\_PROMPT\_INPUT](/oss/python/langchain/errors/INVALID_PROMPT_INPUT)                         |
| [INVALID\_TOOL\_RESULTS](/oss/python/langchain/errors/INVALID_TOOL_RESULTS)                         |
| [MESSAGE\_COERCION\_FAILURE](/oss/python/langchain/errors/MESSAGE_COERCION_FAILURE)                 |
| [MISSING\_CHECKPOINTER](/oss/python/langgraph/errors/MISSING_CHECKPOINTER)                          |
| [MODEL\_AUTHENTICATION](/oss/python/langchain/errors/MODEL_AUTHENTICATION)                          |
| [MODEL\_NOT\_FOUND](/oss/python/langchain/errors/MODEL_NOT_FOUND)                                   |
| [MODEL\_RATE\_LIMIT](/oss/python/langchain/errors/MODEL_RATE_LIMIT)                                 |
| [MULTIPLE\_SUBGRAPHS](/oss/python/langgraph/errors/MULTIPLE_SUBGRAPHS)                              |
| [OUTPUT\_PARSING\_FAILURE](/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE)                     |

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/common-errors.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Evaluate a chatbot

**URL:** llms-txt#evaluate-a-chatbot

**Contents:**
- Setup
- Create a dataset

Source: https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial

In this guide we will set up evaluations for a chatbot. These allow you to measure how well your application is performing over a set of data. Being able to get this insight quickly and reliably will allow you to iterate with confidence.

At a high level, in this tutorial we will:

* *Create an initial golden dataset to measure performance*
* *Define metrics to use to measure performance*
* *Run evaluations on a few different prompts or models*
* *Compare results manually*
* *Track results over time*
* *Set up automated testing to run in CI/CD*

For more information on the evaluation workflows LangSmith supports, check out the [how-to guides](/langsmith/evaluation), or see the reference docs for [evaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) and its asynchronous [aevaluate](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) counterpart.

Lots to cover, let's dive in!

First install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:

And set environment variables to enable LangSmith tracing:

The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:

* What should the schema of each datapoint be?
* How many datapoints should I gather?
* How should I gather those datapoints?

**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.

**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!

**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally *living* constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling \~10-20 examples.

Once you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).

For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!

```python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

And set environment variables to enable LangSmith tracing:
```

Example 3 (unknown):
```unknown
## Create a dataset

The first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:

* What should the schema of each datapoint be?
* How many datapoints should I gather?
* How should I gather those datapoints?

**Schema:** Each datapoint should consist of, at the very least, the inputs to the application. If you are able, it is also very helpful to define the expected outputs - these represent what you would expect a properly functioning application to output. Often times you cannot define the perfect output - that's okay! Evaluation is an iterative process. Sometimes you may also want to define more information for each example - like the expected documents to fetch in RAG, or the expected steps to take as an agent. LangSmith datasets are very flexible and allow you to define arbitrary schemas.

**How many:** There's no hard and fast rule for how many you should gather. The main thing is to make sure you have proper coverage of edge cases you may want to guard against. Even 10-50 examples can provide a lot of value! Don't worry about getting a large number to start - you can (and should) always add over time!

**How to get:** This is maybe the trickiest part. Once you know you want to gather a dataset... how do you actually go about it? For most teams that are starting a new project, we generally see them start by collecting the first 10-20 datapoints by hand. After starting with these datapoints, these datasets are generally *living* constructs and grow over time. They generally grow after seeing how real users will use your application, seeing the pain points that exist, and then moving a few of those datapoints into this set. There are also methods like synthetically generating data that can be used to augment your dataset. To start, we recommend not worrying about those and just hand labeling \~10-20 examples.

Once you've got your dataset, there are a few different ways to upload them to LangSmith. For this tutorial, we will use the client, but you can also upload via the UI (or even create them in the UI).

For this tutorial, we will create 5 datapoints to evaluate on. We will be evaluating a question-answering application. The input will be a question, and the output will be an answer. Since this is a question-answering application, we can define the expected answer. Let's show how to create and upload this dataset to LangSmith!
```

---

## Evaluate a RAG application

**URL:** llms-txt#evaluate-a-rag-application

**Contents:**
- Overview
- Setup
  - Environment
  - Application
- Dataset
- Evaluators
  - Correctness: Response vs reference answer
  - Relevance: Response vs input
  - Groundedness: Response vs retrieved docs
  - Retrieval relevance: Retrieved docs vs input

Source: https://docs.langchain.com/langsmith/evaluate-rag-tutorial

<Info>
  [RAG evaluation](/langsmith/evaluation-concepts#retrieval-augmented-generation-rag) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [LLM-as-judge evaluators](/langsmith/evaluation-concepts#llm-as-judge)
</Info>

Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.

This tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:

1. How to create test datasets
2. How to run your RAG application on those datasets
3. How to measure your application's performance using different evaluation metrics

A typical RAG evaluation workflow consists of three main steps:

1. Creating a dataset with questions and their expected answers

2. Running your RAG application on those questions

3. Using evaluators to measure how well your application performed, looking at factors like:

* Answer relevance
   * Answer accuracy
   * Retrieval quality

For this tutorial, we'll create and evaluate a bot that answers questions about a few of [Lilian Weng's](https://lilianweng.github.io/) insightful blog posts.

First, let's set our environment variables:

And install the dependencies we'll need:

<Info>
  While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.
</Info>

In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.

We'll stick to a simple implementation that:

* Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store
* Retrieval: retrieves those chunks based on the user question
* Generation: passes the question and retrieved docs to an LLM.

#### Indexing and retrieval

First, lets load the blog posts we want to build a chatbot for and index them.

We can now define the generative pipeline.

Now that we've got our application, let's build a dataset to evaluate it. Our dataset will be very simple in this case: we'll have example questions and reference answers.

One way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:

1. **Correctness**: Response vs reference answer

* `Goal`: Measure "*how similar/correct is the RAG chain answer, relative to a ground-truth answer*"
* `Mode`: Requires a ground truth (reference) answer supplied through a dataset
* `Evaluator`: Use LLM-as-judge to assess answer correctness.

2. **Relevance**: Response vs input

* `Goal`: Measure "*how well does the generated response address the initial user input*"
* `Mode`: Does not require reference answer, because it will compare the answer to the input question
* `Evaluator`: Use LLM-as-judge to assess answer relevance, helpfulness, etc.

3. **Groundedness**: Response vs retrieved docs

* `Goal`: Measure "*to what extent does the generated response agree with the retrieved context*"
* `Mode`: Does not require reference answer, because it will compare the answer to the retrieved context
* `Evaluator`: Use LLM-as-judge to assess faithfulness, hallucinations, etc.

4. **Retrieval relevance**: Retrieved docs vs input

* `Goal`: Measure "*how relevant are my retrieved results for this query*"
* `Mode`: Does not require reference answer, because it will compare the question to the retrieved context
* `Evaluator`: Use LLM-as-judge to assess relevance

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6f303c2a284296b42e7d2d2e658f5171" alt="Rag eval overview" data-og-width="1252" width="1252" data-og-height="547" height="547" data-path="langsmith/images/rag-eval-overview.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b5531116cdd61ca9e8ea6fcd760643db 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8f83816ac849c05dd8d3dee4c9670729 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8656d8f8af7ffa7f2684376cf2f70874 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4299367332fbefd15e938aefc23ca6fe 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0bc19645279f119031a5cb8ca32f2f7d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rag-eval-overview.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=185b7c4ba4f4127f780d5fa17b96c752 2500w" />

### Correctness: Response vs reference answer

### Relevance: Response vs input

The flow is similar to above, but we simply look at the `inputs` and `outputs` without needing the `reference_outputs`. Without a reference answer we can't grade accuracy, but can still grade relevance—as in, did the model address the user's question or not.

### Groundedness: Response vs retrieved docs

Another useful way to evaluate responses without needing reference answers is to check if the response is justified by (or "grounded in") the retrieved documents.

### Retrieval relevance: Retrieved docs vs input

We can now kick off our evaluation job with all of our different evaluators.

You can see an example of what these results look like here: [LangSmith link](https://smith.langchain.com/public/302573e2-20bf-4f8c-bdad-e97c20f33f1b/d)

<Accordion title="Here's a consolidated script with all the above code:">
  <CodeGroup>

</CodeGroup>
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-rag-tutorial.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

And install the dependencies we'll need:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Application

<Info>
  While this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.
</Info>

In this section, we'll build a basic Retrieval-Augmented Generation (RAG) application.

We'll stick to a simple implementation that:

* Indexing: chunks and indexes a few of Lilian Weng's blogs in a vector store
* Retrieval: retrieves those chunks based on the user question
* Generation: passes the question and retrieved docs to an LLM.

#### Indexing and retrieval

First, lets load the blog posts we want to build a chatbot for and index them.

<CodeGroup>
```

---

## Evaluate each run by comparing outputs to expected values

**URL:** llms-txt#evaluate-each-run-by-comparing-outputs-to-expected-values

**Contents:**
- Run a pairwise experiment

for run in runs:
    # Get the expected output from the original example
    example_id = run["reference_example_id"]
    expected_output = next(
        ex["outputs"]["label"]
        for ex in examples
        if ex["id"] == example_id
    )

# Compare the model output to the expected output
    actual_output = run["outputs"].get("label", "")
    is_correct = expected_output.lower() == actual_output.lower()

# Post feedback score
    # API Reference: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post
    feedback = {
        "run_id": str(run["id"]),
        "key": "correctness",  # The name of your evaluation metric
        "score": 1.0 if is_correct else 0.0,
        "comment": f"Expected: {expected_output}, Got: {actual_output}",  # Optional
    }

resp = requests.post(
        "https://api.smith.langchain.com/api/v1/feedback",
        json=feedback,
        headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
    )
    resp.raise_for_status()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can add multiple feedback scores with different keys to track various metrics. For example, you might add both a "correctness" score and a "toxicity\_detected" score.

## Run a pairwise experiment

Next, we'll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other.

For more information, check out [this guide](/langsmith/evaluate-pairwise).
```

---

## Evaluation concepts

**URL:** llms-txt#evaluation-concepts

**Contents:**
- What to evaluate
- Offline and online evaluations
  - Offline evaluations
  - Online evaluations
- Evaluation lifecycle
  - 1. Development with offline evaluation
  - 2. Initial deployment with online evaluation
  - 3. Continuous improvement
- Core evaluation targets
  - Targets for offline evaluation

Source: https://docs.langchain.com/langsmith/evaluation-concepts

LLM outputs are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what "good" looks like and measure it. LangSmith Evaluation provides a framework for measuring quality throughout the application lifecycle, from pre-deployment testing to production monitoring.

Before building evaluations, identify what matters for your application. Break down your system into its critical components—LLM calls, retrieval steps, tool invocations, output formatting—and determine quality criteria for each.

**Start with manually curated examples.** Create 5-10 examples of what "good" looks like for each critical component. These examples serve as your ground truth and inform which evaluation approaches to use. For instance:

* **RAG system**: Examples of good retrievals (relevant documents) and good answers (accurate, complete).
* **Agent**: Examples of correct tool selection and proper argument formatting or trajectory that the agent took.
* **Chatbot**: Examples of helpful, on-brand responses that address user intent.

Once you've defined "good" through examples, you can measure how often your system produces similar quality outputs.

## Offline and online evaluations

LangSmith supports two types of evaluations that serve different purposes in your development workflow:

### Offline evaluations

<Icon icon="flask" iconType="solid" /> Use offline evaluations for **pre-deployment testing**:

* **Benchmarking**: Compare multiple versions to find the best performer.
* **Regression testing**: Ensure new versions don't degrade quality.
* **Unit testing**: Verify correctness of individual components.
* **Backtesting**: Test new versions against historical data.

Offline evaluations target [*examples*](#examples) from [*datasets*](#datasets)—curated test cases with reference outputs that define what "good" looks like.

### Online evaluations

<Icon icon="radar" iconType="solid" /> Use online evaluations for **production monitoring**:

* **Real-time monitoring**: Track quality continuously on live traffic.
* **Anomaly detection**: Flag unusual patterns or edge cases.
* **Production feedback**: Identify issues to add to offline datasets.

Online evaluations target [*runs*](#runs) and [*threads*](#threads) from [tracing](/langsmith/observability-quickstart)—real production traces without reference outputs.

This difference in targets determines what you can evaluate: offline evaluations can check correctness against expected answers, while online evaluations focus on quality patterns, safety, and real-world behavior.

## Evaluation lifecycle

As you develop and [deploy your application](/langsmith/deployments), your evaluation strategy evolves from pre-deployment testing to production monitoring. During development and testing, offline evaluations validate functionality against curated datasets. After deployment, online evaluations monitor production behavior on live traffic. As applications mature, both evaluation types work together in an iterative feedback loop to improve quality continuously.

### 1. Development with offline evaluation

Before production deployment, use offline evaluations to validate functionality, benchmark different approaches, and build confidence.

Follow the [quickstart](/langsmith/evaluation-quickstart) to run your first offline evaluation.

### 2. Initial deployment with online evaluation

After deployment, use online evaluations to monitor production quality, detect unexpected issues, and collect real-world data.

Learn how to [configure online evaluations](/langsmith/online-evaluations) for production monitoring.

### 3. Continuous improvement

Use both evaluation types together in an iterative feedback loop. Online evaluations surface issues that become offline test cases, offline evaluations validate fixes, and online evaluations confirm production improvements.

## Core evaluation targets

Evaluations run on different targets depending on whether they are offline or online.

### Targets for offline evaluation

Offline evaluations run on datasets and examples. The presence of reference outputs enables comparison between expected and actual results.

A dataset is a *collection of examples* used for evaluating an application. An example is a test input, reference output pair.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=be2adaa8535dbb253bc0f199895da2e1" alt="Dataset" data-og-width="1279" width="1279" data-og-height="495" height="495" data-path="langsmith/images/dataset-concept.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b86cc15fe64ec6fdac0a14472b84c1e0 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b388730259225fbf466b9a8682303330 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bf8930b4598fbc88212e6e3dde2d0950 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=10220cebf437b481fd655d8e1ddca492 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=16e3d46679352d87e6358acc155d0bd9 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/dataset-concept.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8b987fee69c354d78e7182a685ac38ed 2500w" />

Each example consists of:

* **Inputs**: a dictionary of input variables to pass to your application.
* **Reference outputs** (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.
* **Metadata** (optional): a dictionary of additional information that can be used to create filtered views of a dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0c25674bd30e502e3034b754b8649d66" alt="Example" data-og-width="1281" width="1281" data-og-height="406" height="406" data-path="langsmith/images/example-concept.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3f49d9e298b397039ae97266ad8eda6e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=eb14858aac0f28b26cb1e6ee9ca920f4 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ae707f4b153d7c6e77215012018483c7 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9791ddf51677e2e4f046f32e46304e3d 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=02b7df4950f45c8d4460b0b0cfad88c0 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/example-concept.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=db7340730c6025b03bcb96339fdc980e 2500w" />

Learn more about [managing datasets](/langsmith/manage-datasets).

An *experiment* represents the results of evaluating a specific application version on a dataset. Each experiment captures outputs, evaluator scores, and execution traces for every example in the dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=89c78822157136d0e28e9a110dbdbfd5" alt="Experiment view" data-og-width="1633" width="1633" data-og-height="942" height="942" data-path="langsmith/images/experiment-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f911d77a92d1fb020d6ddea937bd224e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=11c80e9c9aad7df87ea97a540170809a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5b3722ed4fa19b7fcd7d73454dfd342a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=97d5e419a79137652aeac9139473414a 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4a79633866510d7f8ea1d92fa6f58138 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=53012b35d0f323587d5956b3a800818b 2500w" />

Multiple experiments typically run on a given dataset to test different application configurations (e.g., different prompts or LLMs). LangSmith displays all experiments associated with a dataset and supports [comparing multiple experiments](/langsmith/compare-experiment-results) side-by-side.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c4ed751dc3e74f5ad16634dff061bd77" alt="Comparison view" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b6c96f4480262f4db39b7a06a971ca8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=568b65404d8bbe7444d742fbf131bb4d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=36c750c22fcc302d617f1e6e8a5014a0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe9225cdcf381e2b6ef17c0bf99b3004 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac42818b24a858252bb320a23e06b1c2 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bd4214772a68d401b48b704b56a35e4a 2500w" />

Learn [how to analyze experiment results](/langsmith/analyze-an-experiment).

### Targets for online evaluation

Online evaluations run on runs and threads from production traffic. Without reference outputs, evaluators focus on detecting issues, anomalies, and quality degradation in real-time.

A *run* is a single execution trace from your [deployed application](/langsmith/deployments). Each run contains:

* **Inputs**: The actual user inputs your application received.
* **Outputs**: What your application actually returned.
* **Intermediate steps**: All the child runs (tool calls, LLM calls, and so on).
* **Metadata**: Tags, user feedback, latency metrics, etc.

Unlike examples in datasets, runs do not include reference outputs. Online evaluators must assess quality without knowing what the "correct" answer should be, relying instead on quality heuristics, safety checks, and reference-free evaluation techniques.

Learn more about [runs and traces in the Observability concepts](/langsmith/observability-concepts#runs).

*Threads* are collections of related runs representing multi-turn conversations. Online evaluators can run at the thread level to evaluate entire conversations rather than individual turns. This enables assessment of conversation-level properties like coherence across turns, topic maintenance, and user satisfaction throughout an interaction.

*Evaluators* are functions that score application performance. They provide the measurement layer for both offline and online evaluation, adapting their inputs based on what data is available.

Run evaluators using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) and [TypeScript](https://docs.smith.langchain.com/reference/js)), via the [Prompt Playground](/langsmith/observability-concepts#prompt-playground), or by configuring [rules](/langsmith/rules) to run them automatically on tracing projects or datasets.

Evaluator inputs differ based on evaluation type:

**Offline evaluators** receive:

* [Example](#examples): The example from your [dataset](#datasets), containing inputs, reference outputs, and metadata.
* [Run](/langsmith/observability-concepts#runs): The actual outputs and intermediate steps from running the application on the example inputs.

**Online evaluators** receive:

* [Run](/langsmith/observability-concepts#runs): The production trace containing inputs, outputs, and intermediate steps (no reference outputs available).

### Evaluator outputs

Evaluators return **feedback**, which is the scores from evaluation. Feedback is a dictionary or list of dictionaries. Each dictionary contains:

* `key`: The metric name.
* `score` | `value`: The metric value (`score` for numerical metrics, `value` for categorical metrics).
* `comment` (optional): Additional reasoning or explanation for the score.

### Evaluation techniques

LangSmith supports several evaluation approaches:

* [Human](#human)
* [Code](#code)
* [LLM-as-judge](#llm-as-judge)
* [Pairwise](#pairwise)

*Human evaluation* involves manual review of application outputs and execution traces. This approach is [often an effective starting point for evaluation](https://hamel.dev/blog/posts/evals/#looking-at-your-traces). LangSmith provides tools to review application outputs and traces (all intermediate steps).

[Annotation queues](/langsmith/annotation-queues) streamline the process of collecting human feedback on application outputs.

*Code evaluators* are deterministic, rule-based functions. They work well for checks such as verifying the structure of a chatbot's response is not empty, that generated code compiles, or that a classification matches exactly.

*LLM-as-judge evaluators* use LLMs to score application outputs. The grading rules and criteria are typically encoded in the LLM prompt. These evaluators can be:

* **Reference-free**: Check if output contains offensive content or adheres to specific criteria.
* **Reference-based**: Compare output to a reference (e.g., check factual accuracy relative to the reference).

LLM-as-judge evaluators require careful review of scores and prompt tuning. Few-shot evaluators, which include examples of inputs, outputs, and expected grades in the grader prompt, often improve performance.

Learn about [how to define an LLM-as-a-judge evaluator](/langsmith/llm-as-judge).

*Pairwise evaluators* compare outputs from two application versions using heuristics (e.g., which response is longer), LLMs (with pairwise prompts), or human reviewers.

Pairwise evaluation works well when directly scoring an output is difficult but comparing two outputs is straightforward. For example, in summarization tasks, choosing the more informative of two summaries is often easier than assigning an absolute score to a single summary.

Learn [how run pairwise evaluations](/langsmith/evaluate-pairwise).

### Reference-free vs reference-based evaluators

Understanding whether an evaluator requires reference outputs is essential for determining when it can be used.

**Reference-free evaluators** assess quality without comparing to expected outputs. These work for both offline and online evaluation:

* **Safety checks**: Toxicity detection, PII detection, content policy violations
* **Format validation**: JSON structure, required fields, schema compliance
* **Quality heuristics**: Response length, latency, specific keywords
* **Reference-free LLM-as-judge**: Clarity, coherence, helpfulness, tone

**Reference-based evaluators** require reference outputs and only work for offline evaluation:

* **Correctness**: Semantic similarity to reference answer
* **Factual accuracy**: Fact-checking against ground truth
* **Exact match**: Classification tasks with known labels
* **Reference-based LLM-as-judge**: Comparing output quality to a reference

When designing an evaluation strategy, reference-free evaluators provide consistency across both offline testing and online monitoring, while reference-based evaluators enable more precise correctness checks during development.

LangSmith supports various evaluation approaches for different stages of development and deployment. Understanding when to use each type helps build a comprehensive evaluation strategy.

Offline and online evaluations serve different purposes:

* **Offline evaluation types** test pre-deployment on curated datasets with reference outputs
* **Online evaluation types** monitor production behavior on live traffic without reference outputs

Learn more about [evaluation types and when to use each](/langsmith/evaluation-types).

### Building datasets

There are various strategies for building datasets:

**Manually curated examples**

This is the recommended starting point. Create 10–20 high-quality examples covering common scenarios and edge cases. These examples define what "good" looks like for your application.

**Historical traces**

Once in production, convert real traces into examples. For high-traffic applications:

* **User feedback**: Add runs that received negative feedback to test against.
* **Heuristics**: Identify interesting runs (e.g., long latency, errors).
* **LLM feedback**: Use LLMs to detect noteworthy conversations.

Generate additional examples from existing ones. Works best when starting with several high-quality, hand-crafted examples as templates.

### Dataset organization

Partition datasets into subsets for targeted evaluation. Use splits for performance optimization (smaller splits for rapid iteration) and interpretability (evaluate different input types separately).

Learn how to [create and manage dataset splits](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits).

LangSmith automatically creates dataset [versions](/langsmith/manage-datasets#version-a-dataset) when examples change. [Tag versions](/langsmith/manage-datasets#tag-a-version) to mark important milestones. Target specific versions in CI pipelines to ensure dataset updates don't break workflows.

### Human feedback collection

Human feedback often provides the most valuable assessment, particularly for subjective quality dimensions.

**Annotation queues**

[Annotation queues](/langsmith/annotation-queues) enable structured collection of human feedback. Flag specific runs for review, collect annotations in a streamlined interface, and transfer annotated runs to datasets for future evaluations.

Annotation queues complement [inline annotation](/langsmith/annotate-traces-inline) by offering additional capabilities: grouping runs, specifying criteria, and configuring reviewer permissions.

### Evaluations vs testing

Testing and evaluation are similar but distinct concepts.

**Evaluation measures performance according to metrics.** Metrics can be fuzzy or subjective, and prove more useful in relative terms. They typically compare systems against each other.

**Testing asserts correctness.** A system can only be deployed if it passes all tests.

Evaluation metrics can be converted into tests. For example, regression tests can assert that new versions must outperform baseline versions on relevant metrics. Run tests and evaluations together for efficiency when systems are expensive to run.

Evaluations can be written using standard testing tools like [pytest](/langsmith/pytest) or [Vitest/Jest](/langsmith/vitest-jest).

## Quick reference: Offline vs online evaluation

The following table summarizes the key differences between offline and online evaluations:

|                       | **Offline Evaluation**                                      | **Online Evaluation**                                                |
| --------------------- | ----------------------------------------------------------- | -------------------------------------------------------------------- |
| **Runs on**           | Dataset (Examples)                                          | Tracing Project (Runs/Threads)                                       |
| **Data access**       | Inputs, Outputs, Reference Outputs                          | Inputs, Outputs only                                                 |
| **When to use**       | Pre-deployment, during development                          | Production, post-deployment                                          |
| **Primary use cases** | Benchmarking, unit testing, regression testing, backtesting | Real-time monitoring, production feedback, anomaly detection         |
| **Evaluation timing** | Batch processing on curated test sets                       | Real-time or near real-time on live traffic                          |
| **Setup location**    | Evaluation tab (SDK, UI, Prompt Playground)                 | [Observability tab](/langsmith/online-evaluations) (automated rules) |
| **Data requirements** | Requires dataset curation                                   | No dataset needed, evaluates live traces                             |

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-concepts.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Evaluation job and results

**URL:** llms-txt#evaluation-job-and-results

**Contents:**
  - Trajectory evaluator
  - Single step evaluators

experiment_results = await client.aevaluate(
    run_graph,
    data=dataset_name,
    evaluators=[final_answer_correct],
    experiment_prefix="sql-agent-gpt4o-e2e",
    num_repetitions=1,
    max_concurrency=4,
)
experiment_results.to_pandas()
python  theme={null}
def trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float:
    """Check how many of the desired steps the agent took."""
    if len(reference_outputs['trajectory']) > len(outputs['trajectory']):
        return False

i = j = 0
    while i < len(reference_outputs['trajectory']) and j < len(outputs['trajectory']):
        if reference_outputs['trajectory'][i] == outputs['trajectory'][j]:
            i += 1
        j += 1

return i / len(reference_outputs['trajectory'])
python  theme={null}
async def run_graph(inputs: dict) -> dict:
    """Run graph and track the trajectory it takes along with the final response."""
    trajectory = []
    # Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/
    # Set stream_mode="debug" to stream all possible events: https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming
    async for namespace, chunk in graph.astream({"messages": [
            {
                "role": "user",
                "content": inputs['question'],
            }
        ]}, subgraphs=True, stream_mode="debug"):
        # Event type for entering a node
        if chunk['type'] == 'task':
            # Record the node name
            trajectory.append(chunk['payload']['name'])
            # Given how we defined our dataset, we also need to track when specific tools are
            # called by our question answering ReACT agent. These tool calls can be found
            # when the ToolsNode (named "tools") is invoked by looking at the AIMessage.tool_calls
            # of the latest input message.
            if chunk['payload']['name'] == 'tools' and chunk['type'] == 'task':
                for tc in chunk['payload']['input']['messages'][-1].tool_calls:
                    trajectory.append(tc['name'])
    return {"trajectory": trajectory}

experiment_results = await client.aevaluate(
    run_graph,
    data=dataset_name,
    evaluators=[trajectory_subsequence],
    experiment_prefix="sql-agent-gpt4o-trajectory",
    num_repetitions=1,
    max_concurrency=4,
)
experiment_results.to_pandas()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/708d08f4-300e-4c75-9677-c6b71b0d28c9/d).

### Trajectory evaluator

As agents become more complex, they have more potential points of failure. Rather than using simple pass/fail evaluations, it's often better to use evaluations that can give partial credit when an agent takes some correct steps, even if it doesn't reach the right final answer.

This is where trajectory evaluations come in. A trajectory evaluation:

1. Compares the actual sequence of steps the agent took against an expected sequence
2. Calculates a score based on how many of the expected steps were completed correctly

For this example, our end-to-end dataset contains an ordered list of steps that we expect the agent to take. Let's create an evaluator that checks the agent's actual trajectory against these expected steps and calculates what percentage were completed:
```

Example 2 (unknown):
```unknown
Now we can run our evaluation. Our evaluator assumes that our target function returns a 'trajectory' key, so lets define a target function that does so. We'll need to usage [LangGraph's streaming capabilities](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming/) to record the trajectory.

Note that we are reusing the same dataset as for our final response evaluation, so we could have run both evaluators together and defined a target function that returns both "response" and "trajectory". In practice it's often useful to have separate datasets for each type of evaluation, which is why we show them separately here:
```

Example 3 (unknown):
```unknown
You can see what these results look like here: [LangSmith link](https://smith.langchain.com/public/708d08f4-300e-4c75-9677-c6b71b0d28c9/d).

### Single step evaluators

While end-to-end tests give you the most signal about your agents performance, for the sake of debugging and iterating on your agent it can be helpful to pinpoint specific steps that are difficult and evaluate them directly.

In our case, a crucial part of our agent is that it routes the user's intention correctly into either the "refund" path or the "question answering" path. Let's create a dataset and run some evaluations to directly stress test this one component.
```

---

## Evaluation types

**URL:** llms-txt#evaluation-types

**Contents:**
- Offline evaluation types
  - Benchmarking
  - Unit tests
  - Regression tests
  - Backtesting
  - Pairwise evaluation
- Online evaluation types
  - Real-time monitoring
  - Anomaly detection
  - Production feedback loop

Source: https://docs.langchain.com/langsmith/evaluation-types

LangSmith supports various evaluation types for different stages of development and deployment. Understanding when to use each helps build a comprehensive evaluation strategy.

## Offline evaluation types

Offline evaluation tests applications on curated datasets before deployment. By running evaluations on examples with reference outputs, teams can compare versions, validate functionality, and build confidence before exposing changes to users.

Run offline evaluations client-side using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) and [TypeScript](https://docs.smith.langchain.com/reference/js)) or server-side via the [Prompt Playground](/langsmith/observability-concepts#prompt-playground) or [automations](/langsmith/rules).

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=879e4ee3616cecd7cff39879cfc6ec7b" alt="Offline" data-og-width="1581" width="1581" data-og-height="477" height="477" data-path="langsmith/images/offline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ba01953933bebf30c6dc5d8112a3b3db 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3424f1efa82db871cba04c9a4bcac188 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ad0eca755f778a844465976b00a3efb6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cddc86584ad7d9e82a60fc219cff886b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e8cf5a07175523921ee1595e36ea1d73 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/offline.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0f94c55d22273e06d6cde301b4a0a3f3 2500w" />

*Benchmarking* compares multiple application versions on a curated dataset to identify the best performer. This process involves creating a dataset of representative inputs, defining performance metrics, and testing each version.

Benchmarking requires dataset curation with gold-standard reference outputs and well-designed comparison metrics. Examples:

* **RAG Q\&A bot**: Dataset of questions and reference answers, with an LLM-as-judge evaluator checking semantic equivalence between actual and reference answers.
* **ReACT agent**: Dataset of user requests and reference tool calls, with a heuristic evaluator verifying all expected tool calls were made.

*Unit tests* verify the correctness of individual system components. In LLM contexts, [unit tests are often rule-based assertions](https://hamel.dev/blog/posts/evals/#level-1-unit-tests) on inputs or outputs (e.g., verifying LLM-generated code compiles, JSON loads successfully) that validate basic functionality.

Unit tests typically expect consistent passing results, making them suitable for CI pipelines. When running in CI, configure caching to minimize LLM API calls and associated costs.

*Regression tests* measure performance consistency across application versions over time. They ensure new versions do not degrade performance on cases the current version handles correctly, and ideally demonstrate improvements over the baseline. These tests typically run when making updates expected to affect user experience (e.g., model or architecture changes).

LangSmith's comparison view highlights regressions (red) and improvements (green) relative to the baseline, enabling quick identification of changes.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c4ed751dc3e74f5ad16634dff061bd77" alt="Comparison view" data-og-width="3018" width="3018" data-og-height="1532" height="1532" data-path="langsmith/images/comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b6c96f4480262f4db39b7a06a971ca8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=568b65404d8bbe7444d742fbf131bb4d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=36c750c22fcc302d617f1e6e8a5014a0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe9225cdcf381e2b6ef17c0bf99b3004 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac42818b24a858252bb320a23e06b1c2 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bd4214772a68d401b48b704b56a35e4a 2500w" />

*Backtesting* evaluates new application versions against historical production data. Production logs are converted into a dataset, then newer versions process these examples to assess performance on past, realistic user inputs.

This approach is commonly used for evaluating new model releases. For example, when a new model becomes available, test it on the most recent production runs and compare results to actual production outcomes.

### Pairwise evaluation

*Pairwise evaluation* compares outputs from two versions by determining relative quality rather than assigning absolute scores. For some tasks, [determining "version A is better than B"](https://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/) is easier than scoring each version independently.

This approach proves particularly useful for LLM-as-judge evaluations on subjective tasks. For example, in summarization, determining "Which summary is clearer and more concise?" is often simpler than assigning numeric clarity scores.

Learn [how run pairwise evaluations](/langsmith/evaluate-pairwise).

## Online evaluation types

Online evaluation assesses production application outputs in near real-time. Without reference outputs, these evaluations focus on detecting issues, monitoring quality trends, and identifying edge cases that inform future offline testing.

Online evaluators typically run server-side. LangSmith provides built-in [LLM-as-judge evaluators](/langsmith/llm-as-judge) for configuration, and supports custom code evaluators that run within LangSmith.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8d6c1b932e5487c4c01d84ae4f984240" alt="Online" data-og-width="1474" width="1474" data-og-height="521" height="521" data-path="langsmith/images/online.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fd658bcfa1357196dd87ab6263a4896d 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=371e394c68e91e93efe1c80fb85d5484 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5c65ce1c6487a959ce54bffcf8155bb8 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b8e1815f6df4419f65294d2a658bc9f0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f737a486fdb5232e8db121a760075dd8 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0ad2d3d938b8048c21f23f0052904940 2500w" />

### Real-time monitoring

Monitor application quality continuously as users interact with the system. Online evaluations run automatically on production traffic, providing immediate feedback on each interaction. This enables detection of quality degradation, unusual patterns, or unexpected behaviors before they impact significant user populations.

### Anomaly detection

Identify outliers and edge cases that deviate from expected patterns. Online evaluators can flag runs with unusual characteristics—extremely long or short responses, unexpected error rates, or outputs that fail safety checks—for human review and potential addition to offline datasets.

### Production feedback loop

Use insights from production to improve offline evaluation. Online evaluations surface real-world issues and usage patterns that may not appear in curated datasets. Failed production runs become candidates for dataset examples, creating an iterative cycle where production experience continuously refines testing coverage.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation-types.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Evaluator

**URL:** llms-txt#evaluator

def correct(outputs: dict, reference_outputs: dict) -> bool:
    """Check if the agent chose the correct route."""
    return outputs["route"] == reference_outputs["route"]

---

## Evaluator functions can be sync or async

**URL:** llms-txt#evaluator-functions-can-be-sync-or-async

def concise(inputs: dict, outputs: dict) -> bool:
    return len(outputs["output"]) < 3 * len(inputs["idea"])

ls_client = Client()
ideas = [
    "universal basic income",
    "nuclear fusion",
    "hyperloop",
    "nuclear powered rockets",
]
dataset = ls_client.create_dataset("research ideas")
ls_client.create_examples(
    dataset_name=dataset.name,
    examples=[{"inputs": {"idea": i}} for i in ideas],
)

---

## Evaluator function

**URL:** llms-txt#evaluator-function

async def final_answer_correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:
    """Evaluate if the final response is equivalent to reference response."""

# Note that we assume the outputs has a 'response' dictionary. We'll need to make sure
    # that the target function we define includes this key.
    user = f"""QUESTION: {inputs['question']}
    GROUND TRUTH RESPONSE: {reference_outputs['response']}
    STUDENT RESPONSE: {outputs['response']}"""

grade = await grader_llm.ainvoke([{"role": "system", "content": grader_instructions}, {"role": "user", "content": user}])
    return grade["is_correct"]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Now we can run our evaluation. Our evaluator assumes that our target function returns a 'response' key, so lets define a target function that does so.

Also remember that in our refund graph we made the refund node configurable, so that if we specified `config={"env": "test"}`, we would mock out the refunds without actually updating the DB. We'll use this configurable variable in our target `run_graph` method when invoking our graph:
```

---

## Example: caching a query embedding

**URL:** llms-txt#example:-caching-a-query-embedding

tic = time.time()
print(cached_embedder.embed_query("Hello, world!"))
print(f"First call took: {time.time() - tic:.2f} seconds")

---

## Example Collector Configuration: Logs Sidecar

**URL:** llms-txt#example-collector-configuration:-logs-sidecar

---

## Example Collector Configuration: Metrics and Traces Gateway

**URL:** llms-txt#example-collector-configuration:-metrics-and-traces-gateway

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langsmith-collector.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Example configuration for high reads, high writes (500 read/500 write requests per second)

**URL:** llms-txt#example-configuration-for-high-reads,-high-writes-(500-read/500-write-requests-per-second)

**Contents:**
  - Autoscaling

api:
  replicas: 15
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

queue:
  replicas: 10
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

config:
  numberOfJobsPerWorker: 50

redis:
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"

postgres:
  resources:
    requests:
      cpu: "8"
      memory: "32Gi"
    limits:
      cpu: "16"
      memory: "64Gi"
yaml  theme={null}
api:
  autoscaling:
    enabled: true
    minReplicas: 15
    maxReplicas: 25

queue:
  autoscaling:
    enabled: true
    minReplicas: 10
    maxReplicas: 20
```

<Note>
  Ensure that your deployment environment has sufficient resources to scale to the recommended size. Monitor your applications and infrastructure to ensure optimal performance. Consider implementing monitoring and alerting to track resource usage and application performance.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-server-scale.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Autoscaling

If your deployment experiences bursty traffic, you can enable autoscaling to scale the number of API servers and queue workers to handle the load.

Here is a sample configuration for autoscaling for high reads and high writes:
```

---

## Example configuration for high reads, low writes (500 read/5 write requests per second)

**URL:** llms-txt#example-configuration-for-high-reads,-low-writes-(500-read/5-write-requests-per-second)

**Contents:**
  - Medium reads, medium writes <a name="medium-reads-medium-writes" />

api:
  replicas: 10
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

queue:
  replicas: 1  # Default, minimal write load
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

redis:
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"

postgres:
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
    limits:
      cpu: "8"
      memory: "32Gi"
  # Consider read replicas for high read scenarios
  readReplicas: 2
yaml  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Medium reads, medium writes <a name="medium-reads-medium-writes" />

This is a balanced configuration that should handle moderate read and write loads (50 read/50 write requests per second).

For this, we recommend a configuration like this:
```

---

## Example configuration for low reads, high writes (5 read/500 write requests per second)

**URL:** llms-txt#example-configuration-for-low-reads,-high-writes-(5-read/500-write-requests-per-second)

**Contents:**
  - High reads, low writes <a name="high-reads-low-writes" />

api:
  replicas: 6
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

queue:
  replicas: 10
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

config:
  numberOfJobsPerWorker: 50

redis:
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"

postgres:
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
    limits:
      cpu: "8"
      memory: "32Gi"
yaml  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### High reads, low writes <a name="high-reads-low-writes" />

You have a high volume of read requests (500 per second) but relatively few write requests (5 per second).

For this, we recommend a configuration like this:
```

---

## Example configuration for medium reads, medium writes (50 read/50 write requests per second)

**URL:** llms-txt#example-configuration-for-medium-reads,-medium-writes-(50-read/50-write-requests-per-second)

**Contents:**
  - High reads, high writes <a name="high-reads-high-writes" />

api:
  replicas: 3
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

queue:
  replicas: 5
  resources:
    requests:
      cpu: "1"
      memory: "2Gi"
    limits:
      cpu: "2"
      memory: "4Gi"

redis:
  resources:
    requests:
      memory: "2Gi"
    limits:
      memory: "2Gi"

postgres:
  resources:
    requests:
      cpu: "4"
      memory: "16Gi"
    limits:
      cpu: "8"
      memory: "32Gi"
yaml  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### High reads, high writes <a name="high-reads-high-writes" />

You have high volumes of both read and write requests (500 read/500 write requests per second).

For this, we recommend a configuration like this:
```

---

## Example: create a predetermined tool call

**URL:** llms-txt#example:-create-a-predetermined-tool-call

def list_tables(state: MessagesState):
    tool_call = {
        "name": "sql_db_list_tables",
        "args": {},
        "id": "abc123",
        "type": "tool_call",
    }
    tool_call_message = AIMessage(content="", tool_calls=[tool_call])

list_tables_tool = next(tool for tool in tools if tool.name == "sql_db_list_tables")
    tool_message = list_tables_tool.invoke(tool_call)
    response = AIMessage(f"Available tables: {tool_message.content}")

return {"messages": [tool_call_message, tool_message, response]}

---

## Example data format

**URL:** llms-txt#example-data-format

Source: https://docs.langchain.com/langsmith/example-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on evaluation](/langsmith/evaluation-concepts)
</Check>

LangSmith stores examples in datasets as follows:

| Field Name          | Type     | Description                                                                                          |
| ------------------- | -------- | ---------------------------------------------------------------------------------------------------- |
| **id**              | UUID     | Unique identifier for the example.                                                                   |
| **name**            | string   | The name of the example.                                                                             |
| **created\_at**     | datetime | The time this example was created                                                                    |
| **modified\_at**    | datetime | The last time this example was modified                                                              |
| **inputs**          | object   | A map of inputs for the example.                                                                     |
| **outputs**         | object   | A map or set of outputs generated by the run.                                                        |
| **dataset\_id**     | UUID     | The dataset the example belongs to                                                                   |
| **source\_run\_id** | UUID     | If this example was created from a LangSmith [`Run`](/langsmith/run-data-format), the ID of said run |
| **metadata**        | object   | A map of additional, user or SDK defined information that can be stored on an example.               |

To learn more about how examples are used in evaluation, read our how-to guide on [evaluating LLM applications](/langsmith/evaluate-llm-application).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/example-data-format.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Example: force a model to create a tool call

**URL:** llms-txt#example:-force-a-model-to-create-a-tool-call

**Contents:**
- 5. Implement the agent
- 6. Implement human-in-the-loop review

def call_get_schema(state: MessagesState):
    # Note that LangChain enforces that all models accept `tool_choice="any"`
    # as well as `tool_choice=<string name of tool>`.
    llm_with_tools = model.bind_tools([get_schema_tool], tool_choice="any")
    response = llm_with_tools.invoke(state["messages"])

return {"messages": [response]}

generate_query_system_prompt = """
You are an agent designed to interact with a SQL database.
Given an input question, create a syntactically correct {dialect} query to run,
then look at the results of the query and return the answer. Unless the user
specifies a specific number of examples they wish to obtain, always limit your
query to at most {top_k} results.

You can order the results by a relevant column to return the most interesting
examples in the database. Never query for all the columns from a specific table,
only ask for the relevant columns given the question.

DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.
""".format(
    dialect=db.dialect,
    top_k=5,
)

def generate_query(state: MessagesState):
    system_message = {
        "role": "system",
        "content": generate_query_system_prompt,
    }
    # We do not force a tool call here, to allow the model to
    # respond naturally when it obtains the solution.
    llm_with_tools = model.bind_tools([run_query_tool])
    response = llm_with_tools.invoke([system_message] + state["messages"])

return {"messages": [response]}

check_query_system_prompt = """
You are a SQL expert with a strong attention to detail.
Double check the {dialect} query for common mistakes, including:
- Using NOT IN with NULL values
- Using UNION when UNION ALL should have been used
- Using BETWEEN for exclusive ranges
- Data type mismatch in predicates
- Properly quoting identifiers
- Using the correct number of arguments for functions
- Casting to the correct data type
- Using the proper columns for joins

If there are any of the above mistakes, rewrite the query. If there are no mistakes,
just reproduce the original query.

You will call the appropriate tool to execute the query after running this check.
""".format(dialect=db.dialect)

def check_query(state: MessagesState):
    system_message = {
        "role": "system",
        "content": check_query_system_prompt,
    }

# Generate an artificial user message to check
    tool_call = state["messages"][-1].tool_calls[0]
    user_message = {"role": "user", "content": tool_call["args"]["query"]}
    llm_with_tools = model.bind_tools([run_query_tool], tool_choice="any")
    response = llm_with_tools.invoke([system_message, user_message])
    response.id = state["messages"][-1].id

return {"messages": [response]}
python  theme={null}
def should_continue(state: MessagesState) -> Literal[END, "check_query"]:
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return END
    else:
        return "check_query"

builder = StateGraph(MessagesState)
builder.add_node(list_tables)
builder.add_node(call_get_schema)
builder.add_node(get_schema_node, "get_schema")
builder.add_node(generate_query)
builder.add_node(check_query)
builder.add_node(run_query_node, "run_query")

builder.add_edge(START, "list_tables")
builder.add_edge("list_tables", "call_get_schema")
builder.add_edge("call_get_schema", "get_schema")
builder.add_edge("get_schema", "generate_query")
builder.add_conditional_edges(
    "generate_query",
    should_continue,
)
builder.add_edge("check_query", "run_query")
builder.add_edge("run_query", "generate_query")

agent = builder.compile()
python  theme={null}
from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles

display(Image(agent.get_graph().draw_mermaid_png()))
python  theme={null}
question = "Which genre on average has the longest tracks?"

for step in agent.stream(
    {"messages": [{"role": "user", "content": question}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()

================================ Human Message =================================

Which genre on average has the longest tracks?
================================== Ai Message ==================================

Available tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track
================================== Ai Message ==================================
Tool Calls:
  sql_db_schema (call_yzje0tj7JK3TEzDx4QnRR3lL)
 Call ID: call_yzje0tj7JK3TEzDx4QnRR3lL
  Args:
    table_names: Genre, Track
================================= Tool Message =================================
Name: sql_db_schema

CREATE TABLE "Genre" (
	"GenreId" INTEGER NOT NULL,
	"Name" NVARCHAR(120),
	PRIMARY KEY ("GenreId")
)

/*
3 rows from Genre table:
GenreId	Name
1	Rock
2	Jazz
3	Metal
*/

CREATE TABLE "Track" (
	"TrackId" INTEGER NOT NULL,
	"Name" NVARCHAR(200) NOT NULL,
	"AlbumId" INTEGER,
	"MediaTypeId" INTEGER NOT NULL,
	"GenreId" INTEGER,
	"Composer" NVARCHAR(220),
	"Milliseconds" INTEGER NOT NULL,
	"Bytes" INTEGER,
	"UnitPrice" NUMERIC(10, 2) NOT NULL,
	PRIMARY KEY ("TrackId"),
	FOREIGN KEY("MediaTypeId") REFERENCES "MediaType" ("MediaTypeId"),
	FOREIGN KEY("GenreId") REFERENCES "Genre" ("GenreId"),
	FOREIGN KEY("AlbumId") REFERENCES "Album" ("AlbumId")
)

/*
3 rows from Track table:
TrackId	Name	AlbumId	MediaTypeId	GenreId	Composer	Milliseconds	Bytes	UnitPrice
1	For Those About To Rock (We Salute You)	1	1	1	Angus Young, Malcolm Young, Brian Johnson	343719	11170334	0.99
2	Balls to the Wall	2	2	1	U. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann	342562	5510424	0.99
3	Fast As a Shark	3	2	1	F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman	230619	3990994	0.99
*/
================================== Ai Message ==================================
Tool Calls:
  sql_db_query (call_cb9ApLfZLSq7CWg6jd0im90b)
 Call ID: call_cb9ApLfZLSq7CWg6jd0im90b
  Args:
    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;
================================== Ai Message ==================================
Tool Calls:
  sql_db_query (call_DMVALfnQ4kJsuF3Yl6jxbeAU)
 Call ID: call_DMVALfnQ4kJsuF3Yl6jxbeAU
  Args:
    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query

[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]
================================== Ai Message ==================================

The genre with the longest tracks on average is "Sci Fi & Fantasy," with an average track length of approximately 2,911,783 milliseconds. Other genres with relatively long tracks include "Science Fiction," "Drama," "TV Shows," and "Comedy."
python  theme={null}
from langchain_core.runnables import RunnableConfig
from langchain.tools import tool
from langgraph.types import interrupt

@tool(
    run_query_tool.name,
    description=run_query_tool.description,
    args_schema=run_query_tool.args_schema
)
def run_query_tool_with_interrupt(config: RunnableConfig, **tool_input):
    request = {
        "action": run_query_tool.name,
        "args": tool_input,
        "description": "Please review the tool call"
    }
    response = interrupt([request]) # [!code highlight]
    # approve the tool call
    if response["type"] == "accept":
        tool_response = run_query_tool.invoke(tool_input, config)
    # update tool call args
    elif response["type"] == "edit":
        tool_input = response["args"]["args"]
        tool_response = run_query_tool.invoke(tool_input, config)
    # respond to the LLM with user feedback
    elif response["type"] == "response":
        user_feedback = response["args"]
        tool_response = user_feedback
    else:
        raise ValueError(f"Unsupported interrupt response type: {response['type']}")

**Examples:**

Example 1 (unknown):
```unknown
## 5. Implement the agent

We can now assemble these steps into a workflow using the [Graph API](/oss/python/langgraph/graph-api). We define a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) at the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query.
```

Example 2 (unknown):
```unknown
We visualize the application below:
```

Example 3 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=1ddd4aae369fb8c143edaccb0a09c81f" alt="SQL agent graph" style={{ height: "800px" }} data-og-width="308" width="308" data-og-height="645" height="645" data-path="oss/images/sql-agent-langgraph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=280&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=e5d3e67f17d65e438370f7d771e3ba7d 280w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=560&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=dbcb80fdb2d00a6dc33dc90f05d100b5 560w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=840&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=72be69a1e7ac39afad3d0aa03ecffffa 840w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=1100&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=5ad351b8b6641defe17882f5e102cab0 1100w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=1650&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=8a5cefc8ac6938d0b4b0946e0522ffaa 1650w, https://mintcdn.com/langchain-5e9cc07a/aAi4RLdXQAh8fThS/oss/images/sql-agent-langgraph.png?w=2500&fit=max&auto=format&n=aAi4RLdXQAh8fThS&q=85&s=0b5b7711b4b2ece3a3ccb10a2b012166 2500w" />

We can now invoke the graph:
```

Example 4 (unknown):
```unknown

```

---

## Example usage

**URL:** llms-txt#example-usage

**Contents:**
- Advanced usage
  - Custom metadata and tags

if __name__ == "__main__":
    task = """
    Create a Python function that implements a binary search algorithm.
    The function should:
    - Take a sorted list and a target value as parameters
    - Return the index of the target if found, or -1 if not found
    - Include proper error handling and documentation
    """

result = run_code_review_session(task)
    print(f"Result: {result}")
python  theme={null}
from opentelemetry import trace

**Examples:**

Example 1 (unknown):
```unknown
## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces by setting span attributes in your AutoGen application:
```

---

## Execute tool and create result message

**URL:** llms-txt#execute-tool-and-create-result-message

weather_result = "Sunny, 72°F"
tool_message = ToolMessage(
    content=weather_result,
    tool_call_id="call_123"  # Must match the call ID
)

---

## Execution order:

**URL:** llms-txt#execution-order:

---

## [{'expensive_node': {'result': 10}}]

**URL:** llms-txt#[{'expensive_node':-{'result':-10}}]

print(graph.invoke({"x": 5}, stream_mode='updates'))    # [!code highlight]

---

## [{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]

**URL:** llms-txt#[{'expensive_node':-{'result':-10},-'__metadata__':-{'cached':-true}}]

**Contents:**
- Edges
  - Normal Edges
  - Conditional Edges
  - Entry point
  - Conditional entry point
- `Send`
- `Command`
  - When should I use Command instead of conditional edges?
  - Navigating to a node in a parent graph
  - Using inside tools

python  theme={null}
graph.add_edge("node_a", "node_b")
python  theme={null}
graph.add_conditional_edges("node_a", routing_function)
python  theme={null}
graph.add_conditional_edges("node_a", routing_function, {True: "node_b", False: "node_c"})
python  theme={null}
from langgraph.graph import START

graph.add_edge(START, "node_a")
python  theme={null}
from langgraph.graph import START

graph.add_conditional_edges(START, routing_function)
python  theme={null}
graph.add_conditional_edges(START, routing_function, {True: "node_b", False: "node_c"})
python  theme={null}
def continue_to_jokes(state: OverallState):
    return [Send("generate_joke", {"subject": s}) for s in state['subjects']]

graph.add_conditional_edges("node_a", continue_to_jokes)
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        # state update
        update={"foo": "bar"},
        # control flow
        goto="my_other_node"
    )
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    if state["foo"] == "bar":
        return Command(update={"foo": "baz"}, goto="my_other_node")
python  theme={null}
def my_node(state: State) -> Command[Literal["other_subgraph"]]:
    return Command(
        update={"foo": "bar"},
        goto="other_subgraph",  # where `other_subgraph` is a node in the parent graph
        graph=Command.PARENT
    )
python  theme={null}
@dataclass
class ContextSchema:
    llm_provider: str = "openai"

graph = StateGraph(State, context_schema=ContextSchema)
python  theme={null}
graph.invoke(inputs, context={"llm_provider": "anthropic"})
python  theme={null}
from langgraph.runtime import Runtime

def node_a(state: State, runtime: Runtime[ContextSchema]):
    llm = get_llm(runtime.context.llm_provider)
    # ...
python  theme={null}
graph.invoke(inputs, config={"recursion_limit": 5}, context={"llm": "anthropic"})
python  theme={null}
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

def my_node(state: dict, config: RunnableConfig) -> dict:
    current_step = config["metadata"]["langgraph_step"]
    print(f"Currently on step: {current_step}")
    return state
python  theme={null}
from typing import Annotated, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.managed import RemainingSteps

class State(TypedDict):
    messages: Annotated[list, lambda x, y: x + y]
    remaining_steps: RemainingSteps  # Managed value - tracks steps until limit

def reasoning_node(state: State) -> dict:
    # RemainingSteps is automatically populated by LangGraph
    remaining = state["remaining_steps"]

# Check if we're running low on steps
    if remaining <= 2:
        return {"messages": ["Approaching limit, wrapping up..."]}

# Normal processing
    return {"messages": ["thinking..."]}

def route_decision(state: State) -> Literal["reasoning_node", "fallback_node"]:
    """Route based on remaining steps"""
    if state["remaining_steps"] <= 2:
        return "fallback_node"
    return "reasoning_node"

def fallback_node(state: State) -> dict:
    """Handle cases where recursion limit is approaching"""
    return {"messages": ["Reached complexity limit, providing best effort answer"]}

**Examples:**

Example 1 (unknown):
```unknown
1. First run takes two seconds to run (due to mocked expensive computation).
2. Second run utilizes cache and returns quickly.

## Edges

Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:

* Normal Edges: Go directly from one node to the next.
* Conditional Edges: Call a function to determine which node(s) to go to next.
* Entry Point: Which node to call first when user input arrives.
* Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.

A node can have multiple outgoing edges. If a node has multiple outgoing edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.

### Normal Edges

If you **always** want to go from node A to node B, you can use the [`add_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly.
```

Example 2 (unknown):
```unknown
### Conditional Edges

If you want to **optionally** route to one or more edges (or optionally terminate), you can use the [`add_conditional_edges`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a "routing function" to call after that node is executed:
```

Example 3 (unknown):
```unknown
Similar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value.

By default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.

You can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node.
```

Example 4 (unknown):
```unknown
<Tip>
  Use [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function.
</Tip>

### Entry point

The entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph.
```

---

## Experiment configuration

**URL:** llms-txt#experiment-configuration

**Contents:**
  - Repetitions
  - Concurrency
  - Caching

Source: https://docs.langchain.com/langsmith/experiment-configuration

LangSmith supports several configuration options for experiments:

* [Repetitions](#repetitions)
* [Concurrency](#concurrency)
* [Caching](#caching)

*Repetitions* run an experiment multiple times to account for LLM output variability. Since LLM outputs are non-deterministic, multiple repetitions provide a more accurate performance estimate.

Configure repetitions by passing the `num_repetitions` argument to `evaluate` / `aevaluate` ([Python](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate), [TypeScript](https://docs.smith.langchain.com/reference/js/interfaces/evaluation.EvaluateOptions#numrepetitions)). Each repetition re-runs both the target function and all evaluators.

Learn more in the [repetitions how-to guide](/langsmith/repetition).

*Concurrency* controls how many examples run simultaneously during an experiment. Configure it by passing the `max_concurrency` argument to `evaluate` / `aevaluate`. The semantics differ between the two functions:

The `max_concurrency` argument specifies the maximum number of concurrent threads for running both the target function and evaluators.

The `max_concurrency` argument uses a semaphore to limit concurrent tasks. `aevaluate` creates a task for each example, where each task runs the target function and all evaluators for that example. The `max_concurrency` argument specifies the maximum number of concurrent examples to process.

*Caching* stores API call results to disk to speed up future experiments. Set the `LANGSMITH_TEST_CACHE` environment variable to a valid folder path with write access. Future experiments that make identical API calls will reuse cached results instead of making new requests.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/experiment-configuration.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Explore the failed inputs and outputs.

**URL:** llms-txt#explore-the-failed-inputs-and-outputs.

for r in failed:
    print(r["example"].inputs)
    print(r["run"].outputs)

---

## Explore the results as a Pandas DataFrame.

**URL:** llms-txt#explore-the-results-as-a-pandas-dataframe.

---

## export LANGSMITH_API_KEY="your-api-key"

**URL:** llms-txt#export-langsmith_api_key="your-api-key"

**Contents:**
  - Installation
- Instantiation
- Indexing and Retrieval
- Direct Usage
  - Embed single texts
  - Embed multiple texts
- Specifying dimensions
- Custom URLs
- API reference

bash npm theme={null}
  npm install @langchain/openai @langchain/core
  bash yarn theme={null}
  yarn add @langchain/openai @langchain/core
  bash pnpm theme={null}
  pnpm add @langchain/openai @langchain/core
  typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  apiKey: "YOUR-API-KEY", // In Node.js defaults to process.env.OPENAI_API_KEY
  batchSize: 512, // Default value if omitted is 512. Max is 2048
  model: "text-embedding-3-large",
});
typescript  theme={null}
// Create a vector store with a sample text
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const text = "LangChain is the framework for building context-aware reasoning applications";

const vectorstore = await MemoryVectorStore.fromDocuments(
  [{ pageContent: text, metadata: {} }],
  embeddings,
);

// Use the vector store as a retriever that returns a single document
const retriever = vectorstore.asRetriever(1);

// Retrieve the most similar text
const retrievedDocuments = await retriever.invoke("What is LangChain?");

retrievedDocuments[0].pageContent;
text  theme={null}
LangChain is the framework for building context-aware reasoning applications
typescript  theme={null}
const singleVector = await embeddings.embedQuery(text);

console.log(singleVector.slice(0, 100));
text  theme={null}
[
    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,
   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,
    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,
   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,
   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,
   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,
    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,
   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,
  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,
  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,
  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,
    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,
    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,
   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,
    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,
    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,
     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,
   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,
   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,
     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397
]
typescript  theme={null}
const text2 = "LangGraph is a library for building stateful, multi-actor applications with LLMs";

const vectors = await embeddings.embedDocuments([text, text2]);

console.log(vectors[0].slice(0, 100));
console.log(vectors[1].slice(0, 100));
text  theme={null}
[
    -0.01927683,  0.0037708976,  -0.032942563,  0.0037671267,  0.008175306,
   -0.012511838,  -0.009713832,   0.021403614,  -0.015377721, 0.0018684798,
    0.020574018,   0.022399133,   -0.02322873,   -0.01524951,  -0.00504169,
   -0.007375876,   -0.03448109, 0.00015130726,   0.021388533, -0.012564631,
   -0.020031009,   0.027406884,  -0.039217334,    0.03036327,  0.030393435,
   -0.021750538,   0.032610722,  -0.021162277,  -0.025898525,  0.018869571,
    0.034179416,  -0.013371604,  0.0037652412,   -0.02146395, 0.0012641934,
   -0.055688616,    0.05104287,  0.0024982197,  -0.019095825, 0.0037369595,
  0.00088757504,   0.025189597,  -0.018779071,   0.024978427,  0.016833287,
  -0.0025868358,  -0.011727491, -0.0021154736,  -0.017738303, 0.0013839195,
  -0.0131151825,   -0.05405959,   0.029729757,  -0.003393808,  0.019774588,
    0.028885076,   0.004355387,   0.026094612,    0.06479911,  0.038040817,
    -0.03478276,  -0.012594799,  -0.024767255, -0.0031430433,  0.017874055,
   -0.015294761,   0.005709139,   0.025355516,   0.044798266,   0.02549127,
    -0.02524993, 0.00014553308,  -0.019427665,  -0.023545485,  0.008748483,
    0.019850006,  -0.028417485,  -0.001860938,   -0.02318348, -0.010799851,
     0.04793565, -0.0048983963,    0.02193154,  -0.026411368,  0.026426451,
   -0.012149832,   0.035355937,  -0.047814984,  -0.027165547, -0.008228099,
   -0.007737882,   0.023726488,  -0.046487626,  -0.007783133, -0.019638835,
     0.01793439,  -0.018024892,  0.0030336871,  -0.019578502, 0.0042837397
]
[
   -0.010181213,   0.023419594,   -0.04215527, -0.0015320902,  -0.023573855,
  -0.0091644935,  -0.014893179,   0.019016149,  -0.023475688,  0.0010219777,
    0.009255648,    0.03996757,   -0.04366983,   -0.01640774,  -0.020194141,
    0.019408813,  -0.027977299,  -0.022017224,   0.013539891,  -0.007769135,
    0.032647192,  -0.015089511,  -0.022900717,   0.023798235,   0.026084099,
   -0.024625633,   0.035003178,  -0.017978394,  -0.049615882,   0.013364594,
    0.031132633,   0.019142363,   0.023195215,  -0.038396914,   0.005584942,
   -0.031946007,   0.053682756, -0.0036356465,   0.011240003,  0.0056690844,
  -0.0062791156,   0.044146635,  -0.037387207,    0.01300699,   0.018946031,
   0.0050415234,   0.029618073,  -0.021750772,  -0.000649473, 0.00026951815,
   -0.014710871,  -0.029814405,    0.04204308,  -0.014710871,  0.0039616977,
   -0.021512369,   0.054608323,   0.021484323,    0.02790718,  -0.010573876,
   -0.023952495,  -0.035143413,  -0.048802506, -0.0075798146,   0.023279356,
   -0.022690361,  -0.016590048,  0.0060477243,   0.014100839,   0.005476258,
   -0.017221114, -0.0100059165,  -0.017922299,  -0.021989176,    0.01830094,
     0.05516927,   0.001033372,  0.0017310516,   -0.00960624,  -0.037864015,
    0.013063084,   0.006591143,  -0.010160177,  0.0011394264,    0.04953174,
    0.004806626,   0.029421741,  -0.037751824,   0.003618117,   0.007162609,
    0.027696826, -0.0021070621,  -0.024485396, -0.0042141243,   -0.02801937,
   -0.019605145,   0.016281527,  -0.035143413,    0.01640774,   0.042323552
]
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddingsDefaultDimensions = new OpenAIEmbeddings({
  model: "text-embedding-3-large",
});

const vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments(["some text"]);
console.log(vectorsDefaultDimensions[0].length);
text  theme={null}
3072
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings1024 = new OpenAIEmbeddings({
  model: "text-embedding-3-large",
  dimensions: 1024,
});

const vectors1024 = await embeddings1024.embedDocuments(["some text"]);
console.log(vectors1024[0].length);
text  theme={null}
1024
typescript  theme={null}
import { OpenAIEmbeddings } from "@langchain/openai";

const model = new OpenAIEmbeddings({
  configuration: {
    baseURL: "https://your_custom_url.com",
  },
});
```

You can also pass other `ClientOptions` parameters accepted by the official SDK.

If you are hosting on Azure OpenAI, see the [dedicated page instead](/oss/javascript/integrations/text_embedding/azure_openai).

For detailed documentation of all OpenAIEmbeddings features and configurations head to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/text_embedding/openai.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Installation

The LangChain OpenAIEmbeddings integration lives in the `@langchain/openai` package:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Instantiation

Now we can instantiate our model object and generate chat completions:
```

---

## Export LangSmith telemetry to your observability backend

**URL:** llms-txt#export-langsmith-telemetry-to-your-observability-backend

Source: https://docs.langchain.com/langsmith/export-backend

<Warning>
  **This section is only applicable for Kubernetes deployments.**
</Warning>

Self-Hosted LangSmith instances produce telemetry data in the form of logs, metrics and traces. This section will show you how to access and export that data to an observability collector or backend.

This section assumes that you have monitoring infrastructure set up already, or you will set up this infrastructure and want to know how to configure LangSmith to collect data from it.

Infrastructure refers to:

* Collectors, such as [OpenTelemetry](https://opentelemetry.io/docs/collector/), [FluentBit](https://docs.fluentbit.io/manual) or [Prometheus](https://prometheus.io/).
* Observability backends, such as [Datadog](https://www.datadoghq.com/) or the [Grafana](https://grafana.com/) ecosystem.

---

## export LANGSMITH_TRACING="true"

**URL:** llms-txt#export-langsmith_tracing="true"

---

## Extraction schema, mirrors the graph state.

**URL:** llms-txt#extraction-schema,-mirrors-the-graph-state.

class PurchaseInformation(TypedDict):
    """All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don't know their value."""

invoice_id: int | None
    invoice_line_ids: list[int] | None
    customer_first_name: str | None
    customer_last_name: str | None
    customer_phone: str | None
    track_name: str | None
    album_title: str | None
    artist_name: str | None
    purchase_date_iso_8601: str | None
    followup: Annotated[
        str | None,
        ...,
        "If the user hasn't enough identifying information, please tell them what the required information is and ask them to specify it.",
    ]

---

## Extract customer_id and customer_name using jq

**URL:** llms-txt#extract-customer_id-and-customer_name-using-jq

export CUSTOMER_ID=$(echo "$response" | jq -r '.customer_info.customer_id')
export CUSTOMER_NAME=$(echo "$response" | jq -r '.customer_info.customer_name')

---

## Extract structured content from tool messages

**URL:** llms-txt#extract-structured-content-from-tool-messages

for message in result["messages"]:
    if isinstance(message, ToolMessage) and message.artifact:
        structured_content = message.artifact["structured_content"]
python  theme={null}
import json

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.interceptors import MCPToolCallRequest
from mcp.types import TextContent

async def append_structured_content(request: MCPToolCallRequest, handler):
    """Append structured content from artifact to tool message."""
    result = await handler(request)
    if result.structuredContent:
        result.content += [
            TextContent(type="text", text=json.dumps(result.structuredContent)),
        ]
    return result

client = MultiServerMCPClient({...}, tool_interceptors=[append_structured_content])
python  theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import create_agent

client = MultiServerMCPClient({...})
tools = await client.get_tools()
agent = create_agent("claude-sonnet-4-5-20250929", tools)

result = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "Take a screenshot of the current page"}]}
)

**Examples:**

Example 1 (unknown):
```unknown
**Appending structured content via interceptor**

If you want structured content to be visible in the conversation history (visible to the model), you can use an [interceptor](#tool-interceptors) to automatically append structured content to the tool result:
```

Example 2 (unknown):
```unknown
#### Multimodal tool content

MCP tools can return [multimodal content](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#tool-result) (images, text, etc.) in their responses. When an MCP server returns content with multiple parts (e.g., text and images), the adapter converts them to LangChain's [standard content blocks](/oss/python/langchain/messages#standard-content-blocks). You can access the standardized representation via the `content_blocks` property on the `ToolMessage`:
```

---

## Fail the build if accuracy is too low

**URL:** llms-txt#fail-the-build-if-accuracy-is-too-low

**Contents:**
  - Batch processing with blocking=True

if average_accuracy < 0.85:
    print("❌ Evaluation failed! Accuracy below 85% threshold.")
    sys.exit(1)

print("✅ Evaluation passed!")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Batch processing with blocking=True

When you need to perform operations that require the complete dataset (like calculating percentiles, sorting by score, or generating summary reports), use `blocking=True` to wait for all evaluations to complete before processing:
```

---

## Feedback data format

**URL:** llms-txt#feedback-data-format

Source: https://docs.langchain.com/langsmith/feedback-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
</Check>

**Feedback** is LangSmith's way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span). Feedback can be produced from a variety of ways, such as:

1. [Sent up along with a trace](/langsmith/attach-user-feedback) from the LLM application
2. Generated by a user in the app [inline](/langsmith/annotate-traces-inline) or in an [annotation queue](/langsmith/annotation-queues)
3. Generated by an automatic evaluator during [offline evaluation](/langsmith/evaluate-llm-application)
4. Generated by an [online evaluator](/langsmith/online-evaluations)

Feedback is stored in a simple format with the following fields:

| Field Name                | Type     | Description                                                                                            |
| ------------------------- | -------- | ------------------------------------------------------------------------------------------------------ |
| id                        | UUID     | Unique identifier for the record itself                                                                |
| created\_at               | datetime | Timestamp when the record was created                                                                  |
| modified\_at              | datetime | Timestamp when the record was last modified                                                            |
| session\_id               | UUID     | Unique identifier for the experiment or tracing project the run was a part of                          |
| run\_id                   | UUID     | Unique identifier for a specific run within a session                                                  |
| key                       | string   | A key describing the criteria of the feedback, eg "correctness"                                        |
| score                     | number   | Numerical score associated with the feedback key                                                       |
| value                     | string   | Reserved for storing a value associated with the score. Useful for categorical feedback.               |
| comment                   | string   | Any comment or annotation associated with the record. This can be a justification for the score given. |
| correction                | object   | Reserved for storing correction details, if any                                                        |
| feedback\_source          | object   | Object containing information about the feedback source                                                |
| feedback\_source.type     | string   | The type of source where the feedback originated, eg "api", "app", "evaluator"                         |
| feedback\_source.metadata | object   | Reserved for additional metadata, currently                                                            |
| feedback\_source.user\_id | UUID     | Unique identifier for the user providing feedback                                                      |

Here is an example JSON representation of a feedback record in the above format:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/feedback-data-format.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Fetch the comparative experiment

**URL:** llms-txt#fetch-the-comparative-experiment

resp = requests.get(
    f"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative",
    params={"id": comparative_experiment_id},
    headers={"x-api-key": os.environ["LANGSMITH_API_KEY"]}
)

comparative_experiment = resp.json()[0]
experiment_ids = [info["id"] for info in comparative_experiment["experiments_info"]]

from collections import defaultdict
example_id_to_runs_map = defaultdict(list)

---

## Fetch the files as bytes

**URL:** llms-txt#fetch-the-files-as-bytes

pdf_bytes = requests.get(pdf_url).content
wav_bytes = requests.get(wav_url).content
img_bytes = requests.get(img_url).content

---

## Fetch the runs from one of the experiments

**URL:** llms-txt#fetch-the-runs-from-one-of-the-experiments

---

## Fetch the runs we want to convert to a dataset/experiment

**URL:** llms-txt#fetch-the-runs-we-want-to-convert-to-a-dataset/experiment

---

## Filter traces

**URL:** llms-txt#filter-traces

**Contents:**
- Creating and Applying Filters
  - Filtering by run attributes
  - Filtering by time range
  - Filter operators
- Specific Filtering Techniques
  - Filter for intermediate runs (spans)
  - Filter based on inputs and outputs
  - Filter based on input / output key-value pairs
  - Example: Filtering for tool calls
  - Negative filtering on key-value pairs

Source: https://docs.langchain.com/langsmith/filter-traces-in-application

<Tip>**Recommended reading**: It might be helpful to read the [Conceptual guide on tracing](/langsmith/observability-concepts) to gain familiarity with the concepts mentioned on this page.</Tip>

Tracing projects can contain a significant amount of data. Filters are used for effectively navigating and analyzing this data, allowing you to:

* **Have focused investigations**: Quickly narrow down to specific runs for ad-hoc analysis
* **Debug and analyze**: Identify and examine errors, failed runs, and performance bottlenecks

This page contains a series of guides for how to filter runs in a tracing project. If you are programmatically exporting runs for analysis via the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or [SDK](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs), please refer to the [exporting traces guide](./export-traces) for more information.

## Creating and Applying Filters

### Filtering by run attributes

There are two ways to filter runs in a tracing project:

1. **Filters**: Located towards the top-left of the tracing projects page. This is where you construct and manage detailed filter criteria.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d3751c19f28b7dffecec87e27d6c6d53" alt="Filtering" data-og-width="1156" width="1156" data-og-height="551" height="551" data-path="langsmith/images/filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f03bd4a75a5b8b0cc2cb2d96b0908000 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=68f5583f17be5060e8815ae14edd6619 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=35af2365bef2b299b70e336209c54797 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=fc2bc02daf5f56c914cd0cf7f274d200 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e45165a47a471aaf0e94dcb5e2abbb7d 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=98ee26df269a2bdabad8a3f5bea1211a 2500w" />

2. **Filter Shortcuts**: Positioned on the right sidebar of the tracing projects page. The filter shortcuts bar provides quick access to filters based on the most frequently occurring attributes in your project's runs.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c70333d17e9f38f813a6ddef017c7a29" alt="Filter Shortcuts" data-og-width="1330" width="1330" data-og-height="1078" height="1078" data-path="langsmith/images/filter-shortcuts.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b2e073e3f070674704f3f6506e851bc2 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=dbdcf6272d1f115a158f4b1dad02f4b7 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=808645fd9ebcd95e424b8b0a47a75cde 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=de2a53f560cfaecb13673b400a46de55 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=73ac5c2913b9e13ae99822c2a80b290e 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-shortcuts.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f126680bfdf0fe958a7cc06429387582 2500w" />

<Info>
  **Default filter**

By default, the `IsTrace` is `true` filter is applied. This displays only top-level traces. Removing this filter will show all runs, including intermediate spans, in the project.
</Info>

### Filtering by time range

In addition to filtering by run attributes, you can also filter runs within a specific time range. This option is available towards the top-left of the tracing projects page.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=214ec0a0bcec5e5bf07a54becec35a80" alt="Filtering on time" data-og-width="1325" width="1325" data-og-height="680" height="680" data-path="langsmith/images/filter-time.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cf9a2351d13ebfcada25090fc8a56f6e 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=35ef8e40fcb515e8b9b9b98d49312521 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7c69f28b9a1debb36dd1929292d0dca3 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a538ee5b3b711f1d257cc733e701ef0c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=acb075b10621f69b098b929fc64d54d3 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-time.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e11f9d866b5ad3b89a29fad8167f5aac 2500w" />

The available filter operators depend on the data type of the attribute you are filtering on. Here's an overview of common operators:

* **is**: Exact match on the filter value
* **is not**: Negative match on the filter value
* **contains**: Partial match on the filter value
* **does not contain**: Negative partial match on the filter value
* **is one of**: Match on any of the values in the list
* `>` / `<`: Available for numeric fields

## Specific Filtering Techniques

### Filter for intermediate runs (spans)

In order to filter for intermediate runs (spans), you first need to remove the default `IsTrace` is `true` filter. For example, you would do this if you wanted to filter by `run name` for sub runs or filter by `run type`.

Run metadata and tags are also powerful to filter on. These rely on good tagging across all parts of your pipeline. To learn more, you can check out [this guide](./add-metadata-tags).

### Filter based on inputs and outputs

You can filter runs based on the content in the inputs and outputs of the run.

To filter either inputs or outputs, you can use the `Full-Text Search` filter which will match keywords in either field. For more targeted search, you can use the `Input` or `Output` filters which will only match content based on the respective field.

<Note>
  For performance, we index up to 250 characters of data for full-text search. If your search query exceeds this limit, we recommend using [Input/Output key-value search](/langsmith/filter-traces-in-application#filter-based-on-input-%2F-output-key-value-pairs) instead.
</Note>

You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided.

Note that keyword search is done by splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common JSON keywords).

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9d233d67218491a50cc759335a8ce6fa" alt="Filtering" data-og-width="368" width="368" data-og-height="301" height="301" data-path="langsmith/images/filter-full-text.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bf7a2397bf1fda0c41a460bd8c2ae9f2 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2ca993100e989cdfe8628b0d788fbd37 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1c379c825a1759dc3e13f7c182475159 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7e1eab353b75b25d22a901a01caff403 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8fd894a5a8725b5f048bcc9d03b27d07 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-full-text.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f3a68f1e2dc9ab3451f1d5e10733322c 2500w" />

Based on the filters above, the system will search for `python` and `tensorflow` in either inputs or outputs, and `embedding` in the inputs along with `fine` and `tune` in the outputs.

### Filter based on input / output key-value pairs

In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.

<Note>
  We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won't be indexed. This helps us ensure fast, reliable performance.
</Note>

To filter based on key-value pairs, select the `Input Key` or `Output Key` filter from the filters dropdown.

For example, to match the following input:

Select `Filters`, `Add Filter` to bring up the filtering options. Then select `Input Key`, enter `input` as the key and enter `What is the capital of France?` as the value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fef9d6d6bb8c6d285df898ce9c93f192" alt="Filtering" data-og-width="575" width="575" data-og-height="132" height="132" data-path="langsmith/images/search-kv-input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6625f651706eae30b9f1ce9fcbfdb9b2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=184a5582ff5deba63ca77aa203af7fbc 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d7c647a7249e766a43b9f0995a60ab1 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=901d0c7b928a44a753d42058aad52a8e 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cbb901c4a190590ebc194999cf525260 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7614d23d5ba9e06b8673905ef93e6fb5 2500w" />

You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output:

Select `Output Key`, enter `documents.page_content` as the key and enter `The capital of France is Paris` as the value. This will match the nested key `documents.page_content` with the specified value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6d66e1d62691463e05a7933bb3b2c0ce" alt="Filtering" data-og-width="708" width="708" data-og-height="95" height="95" data-path="langsmith/images/search-kv-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7f623484a5b5b5ab4c8a83f1288bed0 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1756043b13d848b8b61043a88c06aa43 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eef83852d80520b9941d72fe41cc4d64 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=683073a1b186796b7b9892748c1fbd94 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1fdf9cf2ef7eaf699e0d7212ef2ea2ea 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e8ab34552881a97fe451bfc3794b6cbb 2500w" />

You can add multiple key-value filters to create more complex queries. You can also use the `Filter Shortcuts` on the right side to quickly filter based on common key-value pairs as shown below:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cec4279c975c71e67710c606a2dc700" alt="Filtering" data-og-width="637" width="637" data-og-height="702" height="702" data-path="langsmith/images/search-kv-filter-shortcut.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=667fbfc50987837e0f0188b8d1dbf1ca 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef422e47304d43568ae954e35c7b1764 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c84e0935eb05be46679d57bb86219d07 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ee8dcaf98f8fd9dc65d5b7f17df1804a 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d0b79382681256ab905af3fc0d7c5660 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8da0c834fad83b17d5cdf69bdd1ccb64 2500w" />

### Example: Filtering for tool calls

It's common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use the `Output Key` filter.

While this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output.

In this case, let's assume this is the output you want to filter for:

With the example above, the KV search will map each nested JSON path as a key-value pair that you can use to search and filter.

LangSmith will break it into the following set of searchable key-value pairs:

| Key                                                | Value                                                                        |
| -------------------------------------------------- | ---------------------------------------------------------------------------- |
| `generations.type`                                 | `ChatGeneration`                                                             |
| `generations.message.type`                         | `constructor`                                                                |
| `generations.message.kwargs.type`                  | `ai`                                                                         |
| `generations.message.kwargs.id`                    | `run-ca7f7531-f4de-4790-9c3e-960be7f8b109`                                   |
| `generations.message.kwargs.tool_calls.name`       | `Plan`                                                                       |
| `generations.message.kwargs.tool_calls.args.steps` | `Research LangGraph's node configuration capabilities`                       |
| `generations.message.kwargs.tool_calls.args.steps` | `Investigate how to add a Python code execution node`                        |
| `generations.message.kwargs.tool_calls.args.steps` | `Find an example or create a sample implementation of a code execution node` |
| `generations.message.kwargs.tool_calls.id`         | `toolu_01XexPzAVknT3gRmUB5PK5BP`                                             |
| `generations.message.kwargs.tool_calls.type`       | `tool_call`                                                                  |
| `type`                                             | `LLMResult`                                                                  |

To search for a specific tool call, you can use the following Output Key search while removing the root runs filter:

`generations.message.kwargs.tool_calls.name` = `Plan`

This will match root and non-root runs where the `tool_calls` name is `Plan`.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d763b8572d14db7f1bc59a3cf4b08a5" alt="Filtering" data-og-width="629" width="629" data-og-height="98" height="98" data-path="langsmith/images/search-kv-tool.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cd9ec450a02572f4bdcec3850bbf4c2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a0988bd110eb51fc73f7e5e2875117b2 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=21710580f012a7a62eeaf528f735f4c9 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5778f4ef1f156e7d2d0a04f45339aa72 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae736245285787c485a6a085e5eec65e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=178a7e4dec57f7260393ee8d0b16030a 2500w" />

### Negative filtering on key-value pairs

Different types of negative filtering can be applied to `Metadata`, `Input Key`, and `Output Key` fields to exclude specific runs from your results.

For example, to find all runs where the metadata key `phone` is not equal to `1234567890`, set the `Metadata` `Key` operator to `is` and `Key` field to `phone`, then set the `Value` operator to `is not` and the `Value` field to `1234567890`. This will match all runs that have a metadata key `phone` with any value except `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=acbd683725e073e1ac78b8ba132e6d43" alt="Filtering" data-og-width="549" width="549" data-og-height="100" height="100" data-path="langsmith/images/negative-filtering-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a0fd6f85165cae0ca7a7d96be57805e1 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=714cdc9f0ad4e6c2b67d49d79740e9b2 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf74951b9cf760cbd6c5653b6d15b95a 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4ae07131bdb12d5e6c5676e51f54ee7c 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=bfb45574bec84ad88d9866b025ac61ba 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-1.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e4a74fca06b1bf2940f15528b04f1b21 2500w" />

To find runs that don't have a specific metadata key, set the `Key` operator to `is not`. For example, setting the `Key` operator to `is not` with `phone` as the key will match all runs that don't have a `phone` field in their metadata.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2c298181ed0aef202ca52f5c30b3d62b" alt="Filtering" data-og-width="419" width="419" data-og-height="128" height="128" data-path="langsmith/images/negative-filtering-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb11546a30cf22c9653c05519b263409 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6a7f5a2ac54f187e449f24011af6f160 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=161ed2a1433d0fa695ef527b32292ee6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d7f8b99d1af28d2e27cdab3c3b03fdad 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ef65a43c85924cf25da164bc049e7029 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-2.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5c79162abd3187782b27f06f28a9d590 2500w" />

You can also filter for runs that neither have a specific key nor a specific value. To find runs where the metadata has neither the key `phone` nor any field with the value `1234567890`, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is not` with value `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=18ca1b82babfda201b184a8884d120eb" alt="Filtering" data-og-width="571" width="571" data-og-height="125" height="125" data-path="langsmith/images/negative-filtering-3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=50c9bee0afc47df246ddb3313e1b7784 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5bbf4d4da70355e861ce7b7adf5ca4b5 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ba52269765451c26687cc7b74a572a68 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6b2b21485dc7073c4edc8b0d1b6718b1 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=18b134952892cc7e5203a5dfaa25eda4 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-3.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f546a96e2b6b229cc593e8a31ec7572f 2500w" />

Finally, you can also filter for runs that do not have a specific key but have a specific value. To find runs where there is no `phone` key but there is a value of `1234567890` for some other key, set the `Key` operator to `is not` with key `phone`, and the `Value` operator to `is` with value `1234567890`.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=60e118205cc6524432058ce968aad478" alt="Filtering" data-og-width="546" width="546" data-og-height="126" height="126" data-path="langsmith/images/negative-filtering-4.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=edd2fee83ba3d63bd8a44f82cfc2c66c 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=20bfead7865f62ee4db7890734c811ac 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=42d6832b0db4843441c5a8dcd95c88b7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6361c16f6321f6f47ca83867f3f5bfbd 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6fcbe01d2be9672356101c4b9a585e78 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/negative-filtering-4.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4dcb53d4331f8a1802ec49b1c3d1971a 2500w" />

Note that you can use the `does not contain` operator instead of `is not` to perform a substring match.

Saving filters allows you to store and reuse frequently used filter configurations. Saved filters are specific to a tracing project.

In the filter box, click the **Save filter** button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=27236a7b0e60a352d16b7affd9b913e5" alt="Filtering" data-og-width="854" width="854" data-og-height="775" height="775" data-path="langsmith/images/save-a-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8bf93a5ab643e0f0ef5194529a084879 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6cb4d039bf74e97e65688465707560ab 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=381acf9bb9f3c2443faeb4dc0c84506f 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fd1660d33217beb5031de6546ff2cdf7 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4b4bc3898549d5f505d0e8989307d5f4 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/save-a-filter.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a9927179e81ace45255aef6a99c77aa3 2500w" />

#### Use a saved filter

After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than three saved filters, only two will be displayed directly, with the rest accessible via a "more" menu. You can use the settings icon in the saved filter bar to optionally hide default saved filters.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=89774c393f6303c981adeba6750f7cad" alt="Filtering" data-og-width="1256" width="1256" data-og-height="404" height="404" data-path="langsmith/images/selecting-a-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a204feab24664b271ddc70ea8dd60698 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4806fcac0a82770fccdb66b06181736e 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9600c7b41c39c9759347a7a5f6ba7d4d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2f4a06c4a4dd87c995b54cae9f177e23 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5f5df78152c343362d6f3ba0c7a9a82a 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/selecting-a-filter.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9d681b4c6a42c7c4a9440525bb1a3557 2500w" />

#### Update a saved filter

With the filter selected, make any changes to filter parameters. Then click **Update filter** > **Update** to update the filter.

In the same menu, you can also create a new saved filter by clicking **Update filter** > **Create new**.

#### Delete a saved filter

Click the settings icon in the saved filter bar, and delete a filter using the trash icon.

You can copy a constructed filter to share it with colleagues, reuse it later, or query runs programmatically in the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or [SDK](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs).

In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those.

This will give you a string representing the filter in the LangSmith query language. For example: `and(eq(is_root, true), and(eq(feedback_key, "user_score"), eq(feedback_score, 1)))`. For more information on the query language syntax, please refer to [this reference](/langsmith/trace-query-syntax#filter-query-language).

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f9f60cca965c1466fe58ec4e8cc52ea5" alt="Copy Filter" data-og-width="1123" width="1123" data-og-height="382" height="382" data-path="langsmith/images/copy-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8966107385058659add5005faa0570d5 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=54cdbad3c852e011f1bbf9bf59cd553d 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=198de895cd5c70de24e72b9eb5f8975d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b500fb05f2705eb74157d5a1ba2bebbe 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67cdd3110def30e481ec0266eaffac66 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/copy-filter.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f2366d6a3ab857b74bcbd551122ec350 2500w" />

## Filtering runs within the trace view

You can also apply filters directly within the trace view, which is useful for sifting through traces with a large number of runs. The same filters available in the main runs table view can be applied here.

By default, only the runs that match the filters will be shown. To see the matched runs within the broader context of the trace tree, switch the view option from "Filtered Only" to "Show All" or "Most relevant".

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=13ea9f30d6efd9bb3f96c8c2376a6858" alt="Filtering within trace view" data-og-width="1341" width="1341" data-og-height="764" height="764" data-path="langsmith/images/filter-runs-in-trace-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=506069ddcc28d8959213f1bfa5d4d2eb 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8b4e2a130a76953f7291e983fb13c86d 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f702ad52060f28de4febdc556daafc6e 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4c6506211f69e03b54793fce9c85760b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ad5849855b303c6357705708e038a9d6 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-runs-in-trace-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6fd17200291f3c6eb680861aa1ab7d80 2500w" />

## Manually specify a raw query in LangSmith query language

If you have [copied a previously constructed filter](/langsmith/filter-traces-in-application#copy-the-filter), you may want to manually apply this raw query in a future session.

In order to do this, you can click on **Advanced filters** on the bottom of the filters popover. From there you can paste a raw query into the text box.

Note that this will add that query to the existing queries, not overwrite it.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0657c7420e760fb57782531d039c0b0d" alt="Raw Query" data-og-width="495" width="495" data-og-height="570" height="570" data-path="langsmith/images/raw-query.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8ebe5718f7e50c0e5186d23ea6467620 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=121dab45e0a3f1599ad9dd26132f2c59 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=0d3ca66c6498f2c1d4e33bf4b63068e8 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef61bc19b9fe41eb0e5d80566049e288 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ccaeac2952f3a2fde4d99c5d69c8055d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/raw-query.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f15ec18a7e3cb748defc6d10c3033d28 2500w" />

## Use an AI Query to auto-generate a query (Experimental)

Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we've added an `AI Query` functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query.

For example: "All runs longer than 10 seconds"

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ef4266d159753dacafbd5af2bcad0ab8" alt="AI Query" data-og-width="464" width="464" data-og-height="319" height="319" data-path="langsmith/images/ai-query.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9589aa2ef6d4fd16533c0b9123ab552e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f03177102e2c6fa2ec2043bce0d3aa7d 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3f6c3c22f37565790e77fa8a55e459a6 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8ba2568d2044c01364206560d7dd8efc 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8bf30e3c64545cca142bf0a6745e19f6 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/ai-query.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c38f6e1b2e00ef271d25f9f674aa27a4 2500w" />

### Filter for intermediate runs (spans) on properties of the root

A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it.

In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Trace filters`. These filters will apply to the traces of all the parent runs of the individual runs you've already filtered for.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a520558efb53e4dd3d6751db76739f90" alt="Filtering" data-og-width="551" width="551" data-og-height="542" height="542" data-path="langsmith/images/trace-filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5303c1b8db95a047593d52ff52ebc55f 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d21e37d06556b6cd20d88c6b72293f68 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=700e831f6c8d473bb9eb3beca7131b4e 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=43467c081d7cfc83582220fe6b7266fe 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9cda650d2069d852c95d8e51893e424d 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-filter.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f0fa57df70f665ef672c917fe6231c15 2500w" />

### Filter for runs (spans) whose child runs have some attribute

This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with name `Foo`. This is useful when `Foo` is not always called, but you want to analyze the cases where it is.

In order to do this, you can click on the `Advanced Filters` link all the way at the bottom of the filter. This will open up a new modal where you can add `Tree filters`. This will make the rule you specify apply to all child runs of the individual runs you've already filtered for.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6b70fc03e9f46333d7af3566619204f0" alt="Filtering" data-og-width="471" width="471" data-og-height="602" height="602" data-path="langsmith/images/child-runs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e7f54072ed1f198804ffca3e073710c 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2c9e7b6627d5edece1b4f90879c03310 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=265d349fc921cf06be7910f20f275a5e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8445bba9df0671da9e718e63c4e2cc48 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cfd020e86ca4b5e2cc77ee43a0c35746 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/child-runs.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=199dab9b8f3adbaecadf3c2206ab008c 2500w" />

### Example: Filtering on all runs whose tree contains the tool call filter

Extending the [tool call filtering example](/langsmith/filter-traces-in-application#example-filtering-for-tool-calls) from above, if you would like to filter for all runs *whose tree contains* the tool filter call, you can use the tree filter in the [advanced filters](/langsmith/filter-traces-in-application#advanced-filters) setting:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b11deac10cd42be7efff94134658b25b" alt="Filtering" data-og-width="669" width="669" data-og-height="462" height="462" data-path="langsmith/images/search-kv-tool-tree.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3f487a02af0891c8c97935b8674d8aba 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4334fbf4536432920e35d1df4f5cff2e 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c7b2edb49f520ebe3e2f03651c8d9d39 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6c107e48006c1ee4d2246935b1259595 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=be785994ef014f06767d1001f5d5dbf1 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-tool-tree.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c686bc45b1d33a12ddcc89aa9c5c0a35 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/filter-traces-in-application.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Select `Filters`, `Add Filter` to bring up the filtering options. Then select `Input Key`, enter `input` as the key and enter `What is the capital of France?` as the value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fef9d6d6bb8c6d285df898ce9c93f192" alt="Filtering" data-og-width="575" width="575" data-og-height="132" height="132" data-path="langsmith/images/search-kv-input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6625f651706eae30b9f1ce9fcbfdb9b2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=184a5582ff5deba63ca77aa203af7fbc 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d7c647a7249e766a43b9f0995a60ab1 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=901d0c7b928a44a753d42058aad52a8e 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cbb901c4a190590ebc194999cf525260 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-input.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7614d23d5ba9e06b8673905ef93e6fb5 2500w" />

You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output:
```

Example 2 (unknown):
```unknown
Select `Output Key`, enter `documents.page_content` as the key and enter `The capital of France is Paris` as the value. This will match the nested key `documents.page_content` with the specified value.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6d66e1d62691463e05a7933bb3b2c0ce" alt="Filtering" data-og-width="708" width="708" data-og-height="95" height="95" data-path="langsmith/images/search-kv-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7f623484a5b5b5ab4c8a83f1288bed0 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1756043b13d848b8b61043a88c06aa43 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eef83852d80520b9941d72fe41cc4d64 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=683073a1b186796b7b9892748c1fbd94 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=1fdf9cf2ef7eaf699e0d7212ef2ea2ea 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-output.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e8ab34552881a97fe451bfc3794b6cbb 2500w" />

You can add multiple key-value filters to create more complex queries. You can also use the `Filter Shortcuts` on the right side to quickly filter based on common key-value pairs as shown below:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9cec4279c975c71e67710c606a2dc700" alt="Filtering" data-og-width="637" width="637" data-og-height="702" height="702" data-path="langsmith/images/search-kv-filter-shortcut.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=667fbfc50987837e0f0188b8d1dbf1ca 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ef422e47304d43568ae954e35c7b1764 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c84e0935eb05be46679d57bb86219d07 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ee8dcaf98f8fd9dc65d5b7f17df1804a 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d0b79382681256ab905af3fc0d7c5660 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/search-kv-filter-shortcut.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8da0c834fad83b17d5cdf69bdd1ccb64 2500w" />

### Example: Filtering for tool calls

It's common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use the `Output Key` filter.

While this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output.

In this case, let's assume this is the output you want to filter for:
```

---

## Finally, we compile it!

**URL:** llms-txt#finally,-we-compile-it!

---

## Find memories about food preferences

**URL:** llms-txt#find-memories-about-food-preferences

---

## First call

**URL:** llms-txt#first-call

config = {"configurable": {"thread_id": "my-thread"}}
result = agent.invoke(input, config=config)

---

## First invocation

**URL:** llms-txt#first-invocation

agent.invoke(HumanMessage(content="I live in Sydney, Australia."))

---

## First let's just say hi to the AI

**URL:** llms-txt#first-let's-just-say-hi-to-the-ai

for update in graph.stream(
    {"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates"
):
    print(update)
python  theme={null}
def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):

# Get the user id from the config
    user_id = config["configurable"]["user_id"]

# Namespace the memory
    namespace = (user_id, "memories")

# ... Analyze conversation and create a new memory

# Create a new memory ID
    memory_id = str(uuid.uuid4())

# We create a new memory
    store.put(namespace, memory_id, {"memory": memory})

python  theme={null}
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
 'namespace': ['1', 'memories'],
 'created_at': '2024-10-02T17:22:31.590602+00:00',
 'updated_at': '2024-10-02T17:22:31.590605+00:00'}
python  theme={null}
def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):
    # Get the user id from the config
    user_id = config["configurable"]["user_id"]

# Namespace the memory
    namespace = (user_id, "memories")

# Search based on the most recent message
    memories = store.search(
        namespace,
        query=state["messages"][-1].content,
        limit=3
    )
    info = "\n".join([d.value["memory"] for d in memories])

# ... Use memories in the model call
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
We can access the `in_memory_store` and the `user_id` in *any node* by passing `store: BaseStore` and `config: RunnableConfig` as node arguments. Here's how we might use semantic search in a node to find relevant memories:
```

Example 2 (unknown):
```unknown
As we showed above, we can also access the store in any node and use the `store.search` method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.
```

Example 3 (unknown):
```unknown
We can access the memories and use them in our model call.
```

Example 4 (unknown):
```unknown
If we create a new thread, we can still access the same memories so long as the `user_id` is the same.
```

---

## First, post the runs to create them

**URL:** llms-txt#first,-post-the-runs-to-create-them

posts = [parent_run, child_run]
batch_ingest_runs(api_url, api_key, posts=posts)

---

## First session: save user info

**URL:** llms-txt#first-session:-save-user-info

agent.invoke({
    "messages": [{"role": "user", "content": "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev"}]
})

---

## Fix markdown issues

**URL:** llms-txt#fix-markdown-issues

---

## Format code automatically

**URL:** llms-txt#format-code-automatically

---

## For certain unit tests, you may need to set certain flags and environment variables:

**URL:** llms-txt#for-certain-unit-tests,-you-may-need-to-set-certain-flags-and-environment-variables:

TIKTOKEN_CACHE_DIR=tiktoken_cache uv run --group test pytest --disable-socket --allow-unix-socket tests/unit_tests/

---

## for every request. This will determine whether the request is allowed or not

**URL:** llms-txt#for-every-request.-this-will-determine-whether-the-request-is-allowed-or-not

**Contents:**
- 3. Test your bot
- 4. Chat with your bot

@auth.authenticate
async def get_current_user(authorization: str | None) -> Auth.types.MinimalUserDict:
    """Check if the user's token is valid."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"
    # Check if token is valid
    if token not in VALID_TOKENS:
        raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid token")

# Return user info if valid
    user_data = VALID_TOKENS[token]
    return {
        "identity": user_data["id"],
    }
json {highlight={7-9}} title="langgraph.json" theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "auth": {
    "path": "src/security/auth.py:auth"
  }
}
bash  theme={null}
langgraph dev --no-browser
json  theme={null}
{
    "auth": {
        "path": "src/security/auth.py:auth",
        "disable_studio_auth": true
    }
}
python  theme={null}
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
Notice that your [Auth.authenticate](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth.authenticate) handler does two important things:

1. Checks if a valid token is provided in the request's [Authorization header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization)
2. Returns the user's [MinimalUserDict](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.MinimalUserDict)

Now tell LangGraph to use authentication by adding the following to the [langgraph.json](https://reference.langchain.com/python/cloud/reference/cli/#configuration-file) configuration:
```

Example 2 (unknown):
```unknown
## 3. Test your bot

Start the server again to test everything out:
```

Example 3 (unknown):
```unknown
If you didn't add the `--no-browser`, the Studio UI will open in the browser. By default, we also permit access from Studio, even when using custom auth. This makes it easier to develop and test your bot in Studio. You can remove this alternative authentication option by setting `disable_studio_auth: true` in your auth configuration:
```

Example 4 (unknown):
```unknown
## 4. Chat with your bot

You should now only be able to access the bot if you provide a valid token in the request header. Users will still, however, be able to access each other's resources until you add [resource authorization handlers](/langsmith/auth#resource-specific-handlers) in the next section of the tutorial.

<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=3ccfa86789baea630b8f418e9eb5b648" alt="Auth gate passes requests with a valid token, but no per-resource filters are applied yet—so users share visibility until authorization handlers are added in the next step." data-og-width="2617" width="2617" data-og-height="1673" height="1673" data-path="langsmith/images/authentication.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=0216c0cf0cb74f67f43e65561a787c96 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=c4a9ab37e2413a38dc61ef6f4288c1b1 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=d1146dee549ab13c949efae260706988 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=e430d8ebe4534d20c1d4d3d887f0c938 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=3aeaa334e3ff1457c165821966721f94 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authentication.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=74d6dd4c73baa0182624085d13f08530 2500w" />

Run the following code in a file or notebook:
```

---

## For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.

**URL:** llms-txt#for-langsmith-api-keys-linked-to-multiple-workspaces,-set-the-langsmith_workspace_id-environment-variable-to-specify-which-workspace-to-use.

**Contents:**
  - 3. Log a trace
- Without LangChain
  - 1. Installation
  - 2. Configure your environment

export LANGSMITH_WORKSPACE_ID=<your-workspace-id>
python Python theme={null}
  from typing import Literal
  from langchain.messages import HumanMessage
  from langchain_openai import ChatOpenAI
  from langchain.tools import tool
  from langgraph.prebuilt import ToolNode
  from langgraph.graph import StateGraph, MessagesState

@tool
  def search(query: str):
      """Call to surf the web."""
      if "sf" in query.lower() or "san francisco" in query.lower():
          return "It's 60 degrees and foggy."
      return "It's 90 degrees and sunny."

tools = [search]
  tool_node = ToolNode(tools)

model = ChatOpenAI(model="gpt-4o", temperature=0).bind_tools(tools)

def should_continue(state: MessagesState) -> Literal["tools", "__end__"]:
      messages = state['messages']
      last_message = messages[-1]
      if last_message.tool_calls:
          return "tools"
      return "__end__"

def call_model(state: MessagesState):
      messages = state['messages']
      # Invoking `model` will automatically infer the correct tracing context
      response = model.invoke(messages)
      return {"messages": [response]}

workflow = StateGraph(MessagesState)
  workflow.add_node("agent", call_model)
  workflow.add_node("tools", tool_node)
  workflow.add_edge("__start__", "agent")
  workflow.add_conditional_edges(
      "agent",
      should_continue,
  )
  workflow.add_edge("tools", 'agent')

app = workflow.compile()

final_state = app.invoke(
      {"messages": [HumanMessage(content="what is the weather in sf")]},
      config={"configurable": {"thread_id": 42}}
  )

final_state["messages"][-1].content
  typescript TypeScript theme={null}
  import { HumanMessage, AIMessage } from "@langchain/core/messages";
  import { tool } from "@langchain/core/tools";
  import { z } from "zod";
  import { ChatOpenAI } from "@langchain/openai";
  import { StateGraph, StateGraphArgs } from "@langchain/langgraph";
  import { ToolNode } from "@langchain/langgraph/prebuilt";

interface AgentState {
    messages: HumanMessage[];
  }

const graphState: StateGraphArgs<AgentState>["channels"] = {
    messages: {
      reducer: (x: HumanMessage[], y: HumanMessage[]) => x.concat(y),
    },
  };

const searchTool = tool(async ({ query }: { query: string }) => {
    if (query.toLowerCase().includes("sf") || query.toLowerCase().includes("san francisco")) {
      return "It's 60 degrees and foggy."
    }
    return "It's 90 degrees and sunny."
  }, {
    name: "search",
    description:
      "Call to surf the web.",
    schema: z.object({
      query: z.string().describe("The query to use in your search."),
    }),
  });

const tools = [searchTool];
  const toolNode = new ToolNode<AgentState>(tools);

const model = new ChatOpenAI({
    model: "gpt-4o",
    temperature: 0,
  }).bindTools(tools);

function shouldContinue(state: AgentState) {
    const messages = state.messages;
    const lastMessage = messages[messages.length - 1] as AIMessage;
    if (lastMessage.tool_calls?.length) {
      return "tools";
    }
    return "__end__";
  }

async function callModel(state: AgentState) {
    const messages = state.messages;
    // Invoking `model` will automatically infer the correct tracing context
    const response = await model.invoke(messages);
    return { messages: [response] };
  }

const workflow = new StateGraph<AgentState>({ channels: graphState })
    .addNode("agent", callModel)
    .addNode("tools", toolNode)
    .addEdge("__start__", "agent")
    .addConditionalEdges("agent", shouldContinue)
    .addEdge("tools", "agent");

const app = workflow.compile();

const finalState = await app.invoke(
    { messages: [new HumanMessage("what is the weather in sf")] },
    { configurable: { thread_id: "42" } }
  );

finalState.messages[finalState.messages.length - 1].content;
  bash pip theme={null}
  pip install openai langsmith langgraph
  bash yarn theme={null}
  yarn add openai langsmith @langchain/langgraph
  bash npm theme={null}
  npm install openai langsmith @langchain/langgraph
  bash pnpm theme={null}
  pnpm add openai langsmith @langchain/langgraph
  bash wrap theme={null}
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:

  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`

  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:

  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`

  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
</Info>

### 3. Log a trace

Once you've set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

An example trace from running the above code [looks like this](https://smith.langchain.com/public/10863294-ee79-484a-927f-0558230f1547/r):

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a589f14351fb48e721205d1e363753ea" alt="Trace tree for a LangGraph run with LangChain" data-og-width="3314" width="3314" data-og-height="1766" height="1766" data-path="langsmith/images/langgraph-with-langchain-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01650d2547ba6e440a66ceb0bdeb566a 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c0d7d3f04d58edd25e9b99d2a61890ce 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf074976fbc12b0baad0e8320fd7fa47 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=627f66d7b9e2b8a054e66e8dbc7afa73 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7946baa3ad54fa851cd8a34eb31b47b0 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-with-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6b8e6a216b312f5ae3c74503fabba63 2500w" />

## Without LangChain

If you are using other SDKs or custom functions within LangGraph, you will need to [wrap or decorate them appropriately](/langsmith/annotate-code#use-traceable--traceable) (with the `@traceable` decorator in Python or the `traceable` function in JS, or something like e.g. `wrap_openai` for SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.

Here's an example. You can also see this page for more information.

### 1. Installation

Install the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## For large datasets, lazily load documents

**URL:** llms-txt#for-large-datasets,-lazily-load-documents

**Contents:**
- By category
  - Webpages
  - PDFs
  - Cloud Providers
  - Social Platforms
  - Messaging Services
  - Productivity tools
  - Common file types
- All document loaders

for document in loader.lazy_load():
    print(document)
```

The below document loaders allow you to load webpages.

| Document Loader                                                             | Description                                                                                                          | Package/API |
| --------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------- |
| [Web](/oss/python/integrations/document_loaders/web_base)                   | Uses urllib and BeautifulSoup to load and parse HTML web pages                                                       | Package     |
| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file) | Uses Unstructured to load and parse web pages                                                                        | Package     |
| [RecursiveURL](/oss/python/integrations/document_loaders/recursive_url)     | Recursively scrapes all child links from a root URL                                                                  | Package     |
| [Sitemap](/oss/python/integrations/document_loaders/sitemap)                | Scrapes all pages on a given sitemap                                                                                 | Package     |
| [Spider](/oss/python/integrations/document_loaders/spider)                  | Crawler and scraper that returns LLM-ready data                                                                      | API         |
| [Firecrawl](/oss/python/integrations/document_loaders/firecrawl)            | API service that can be deployed locally                                                                             | API         |
| [Docling](/oss/python/integrations/document_loaders/docling)                | Uses Docling to load and parse web pages                                                                             | Package     |
| [Hyperbrowser](/oss/python/integrations/document_loaders/hyperbrowser)      | Platform for running and scaling headless browsers, can be used to scrape/crawl any site                             | API         |
| [AgentQL](/oss/python/integrations/document_loaders/agentql)                | Web interaction and structured data extraction from any web page using an AgentQL query or a Natural Language prompt | API         |

The below document loaders allow you to load PDF documents.

| Document Loader                                                                    | Description                                          | Package/API |
| ---------------------------------------------------------------------------------- | ---------------------------------------------------- | ----------- |
| [PyPDF](/oss/python/integrations/document_loaders/pypdfloader)                     | Uses `pypdf` to load and parse PDFs                  | Package     |
| [Unstructured](/oss/python/integrations/document_loaders/unstructured_file)        | Uses Unstructured's open source library to load PDFs | Package     |
| [Amazon Textract](/oss/python/integrations/document_loaders/amazon_textract)       | Uses AWS API to load PDFs                            | API         |
| [MathPix](/oss/python/integrations/document_loaders/mathpix)                       | Uses MathPix to load PDFs                            | Package     |
| [PDFPlumber](/oss/python/integrations/document_loaders/pdfplumber)                 | Load PDF files using PDFPlumber                      | Package     |
| [PyPDFDirectry](/oss/python/integrations/document_loaders/pypdfdirectory)          | Load a directory with PDF files                      | Package     |
| [PyPDFium2](/oss/python/integrations/document_loaders/pypdfium2)                   | Load PDF files using PyPDFium2                       | Package     |
| [PyMuPDF](/oss/python/integrations/document_loaders/pymupdf)                       | Load PDF files using PyMuPDF                         | Package     |
| [PyMuPDF4LLM](/oss/python/integrations/document_loaders/pymupdf4llm)               | Load PDF content to Markdown using PyMuPDF4LLM       | Package     |
| [PDFMiner](/oss/python/integrations/document_loaders/pdfminer)                     | Load PDF files using PDFMiner                        | Package     |
| [Upstage Document Parse Loader](/oss/python/integrations/document_loaders/upstage) | Load PDF files using UpstageDocumentParseLoader      | Package     |
| [Docling](/oss/python/integrations/document_loaders/docling)                       | Load PDF files using Docling                         | Package     |
| [UnDatasIO](/oss/python/integrations/document_loaders/undatasio)                   | Load PDF files using UnDatasIO                       | Package     |
| [OpenDataLoader PDF](/oss/python/integrations/document_loaders/opendataloader_pdf) | Load PDF files using OpenDataLoader PDF              | Package     |

The below document loaders allow you to load documents from your favorite cloud providers.

| Document Loader                                                                                            | Description                                                 | Partner Package | API reference                                                                                                                                                                                  |
| ---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- | --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [AWS S3 Directory](/oss/python/integrations/document_loaders/aws_s3_directory)                             | Load documents from an AWS S3 directory                     | ❌               | [`S3DirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.s3_directory.S3DirectoryLoader.html)                          |
| [AWS S3 File](/oss/python/integrations/document_loaders/aws_s3_file)                                       | Load documents from an AWS S3 file                          | ❌               | [`S3FileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.s3_file.S3FileLoader.html)                                         |
| [Azure AI Data](/oss/python/integrations/document_loaders/azure_ai_data)                                   | Load documents from Azure AI services                       | ❌               | [`AzureAIDataLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.azure_ai_data.AzureAIDataLoader.html)                         |
| [Azure Blob Storage](/oss/python/integrations/document_loaders/azure_blob_storage)                         | Load documents from Azure Blob Storage                      | ✅               | [`AzureBlobStorageLoader`](https://reference.langchain.com/python/integrations/langchain_azure/storage/)                                                                                       |
| [Dropbox](/oss/python/integrations/document_loaders/dropbox)                                               | Load documents from Dropbox                                 | ❌               | [`DropboxLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.dropbox.DropboxLoader.html)                                       |
| [Google Cloud Storage Directory](/oss/python/integrations/document_loaders/google_cloud_storage_directory) | Load documents from GCS bucket                              | ✅               | [`GCSDirectoryLoader`](https://python.langchain.com/api_reference/google_community/gcs_directory/langchain_google_community.gcs_directory.GCSDirectoryLoader.html)                             |
| [Google Cloud Storage File](/oss/python/integrations/document_loaders/google_cloud_storage_file)           | Load documents from GCS file object                         | ✅               | [`GCSFileLoader`](https://python.langchain.com/api_reference/google_community/gcs_file/langchain_google_community.gcs_file.GCSFileLoader.html)                                                 |
| [Google Drive](/oss/python/integrations/document_loaders/google_drive)                                     | Load documents from Google Drive (Google Docs only)         | ✅               | [`GoogleDriveLoader`](https://python.langchain.com/api_reference/google_community/drive/langchain_google_community.drive.GoogleDriveLoader.html)                                               |
| [Huawei OBS Directory](/oss/python/integrations/document_loaders/huawei_obs_directory)                     | Load documents from Huawei Object Storage Service Directory | ❌               | [`OBSDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.obs_directory.OBSDirectoryLoader.html)                       |
| [Huawei OBS File](/oss/python/integrations/document_loaders/huawei_obs_file)                               | Load documents from Huawei Object Storage Service File      | ❌               | [`OBSFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.obs_file.OBSFileLoader.html)                                      |
| [Microsoft OneDrive](/oss/python/integrations/document_loaders/microsoft_onedrive)                         | Load documents from Microsoft OneDrive                      | ❌               | [`OneDriveLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.onedrive.OneDriveLoader.html)                                    |
| [Microsoft SharePoint](/oss/python/integrations/document_loaders/microsoft_sharepoint)                     | Load documents from Microsoft SharePoint                    | ❌               | [`SharePointLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.sharepoint.SharePointLoader.html)                              |
| [Tencent COS Directory](/oss/python/integrations/document_loaders/tencent_cos_directory)                   | Load documents from Tencent Cloud Object Storage Directory  | ❌               | [`TencentCOSDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.tencent_cos_directory.TencentCOSDirectoryLoader.html) |
| [Tencent COS File](/oss/python/integrations/document_loaders/tencent_cos_file)                             | Load documents from Tencent Cloud Object Storage File       | ❌               | [`TencentCOSFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.tencent_cos_file.TencentCOSFileLoader.html)                |

The below document loaders allow you to load documents from different social media platforms.

| Document Loader                                              | API reference                                                                                                                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Twitter](/oss/python/integrations/document_loaders/twitter) | [`TwitterTweetLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.twitter.TwitterTweetLoader.html) |
| [Reddit](/oss/python/integrations/document_loaders/reddit)   | [`RedditPostsLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.reddit.RedditPostsLoader.html)    |

### Messaging Services

The below document loaders allow you to load data from different messaging platforms.

| Document Loader                                                          | API reference                                                                                                                                                               |
| ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Telegram](/oss/python/integrations/document_loaders/telegram)           | [`TelegramChatFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.telegram.TelegramChatFileLoader.html) |
| [WhatsApp](/oss/python/integrations/document_loaders/whatsapp_chat)      | [`WhatsAppChatLoader`](https://python.langchain.com/api_reference/community/chat_loaders/langchain_community.chat_loaders.whatsapp.WhatsAppChatLoader.html)                 |
| [Discord](/oss/python/integrations/document_loaders/discord)             | [`DiscordChatLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.discord.DiscordChatLoader.html)            |
| [Facebook Chat](/oss/python/integrations/document_loaders/facebook_chat) | [`FacebookChatLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.facebook_chat.FacebookChatLoader.html)    |
| [Mastodon](/oss/python/integrations/document_loaders/mastodon)           | [`MastodonTootsLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.mastodon.MastodonTootsLoader.html)       |

### Productivity tools

The below document loaders allow you to load data from commonly used productivity tools.

| Document Loader                                            | API reference                                                                                                                                                                  |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [Figma](/oss/python/integrations/document_loaders/figma)   | [`FigmaFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.figma.FigmaFileLoader.html)                     |
| [Notion](/oss/python/integrations/document_loaders/notion) | [`NotionDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.notion.NotionDirectoryLoader.html)        |
| [Slack](/oss/python/integrations/document_loaders/slack)   | [`SlackDirectoryLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.slack_directory.SlackDirectoryLoader.html) |
| [Quip](/oss/python/integrations/document_loaders/quip)     | [`QuipLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.quip.QuipLoader.html)                                |
| [Trello](/oss/python/integrations/document_loaders/trello) | [`TrelloLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.trello.TrelloLoader.html)                          |
| [Roam](/oss/python/integrations/document_loaders/roam)     | [`RoamLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.roam.RoamLoader.html)                                |
| [GitHub](/oss/python/integrations/document_loaders/github) | [`GithubFileLoader`](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.github.GithubFileLoader.html)                  |

### Common file types

The below document loaders allow you to load data from common data formats.

| Document Loader                                                                                  | Data Type                                                                                                                                                                    |
| ------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`CSVLoader`](/oss/python/integrations/document_loaders/csv)                                     | CSV files                                                                                                                                                                    |
| [`Unstructured`](/oss/python/integrations/document_loaders/unstructured_file)                    | Many file types (see [https://docs.unstructured.io/platform/supported-file-types](https://docs.unstructured.io/platform/supported-file-types))                               |
| [`JSONLoader`](/oss/python/integrations/document_loaders/json)                                   | JSON files                                                                                                                                                                   |
| [`BSHTMLLoader`](/oss/python/integrations/document_loaders/bshtml)                               | HTML files                                                                                                                                                                   |
| [`DoclingLoader`](/oss/python/integrations/document_loaders/docling)                             | Various file types (see [https://ds4sd.github.io/docling/](https://ds4sd.github.io/docling/))                                                                                |
| [`PolarisAIDataInsightLoader`](/oss/python/integrations/document_loaders/polaris_ai_datainsight) | Various file types (see [https://datainsight.polarisoffice.com/documentation?docType=doc\_extract](https://datainsight.polarisoffice.com/documentation?docType=doc_extract)) |

## All document loaders

<Columns cols={3}>
  <Card title="acreom" icon="link" href="/oss/python/integrations/document_loaders/acreom" arrow="true" cta="View guide" />

<Card title="AgentQLLoader" icon="link" href="/oss/python/integrations/document_loaders/agentql" arrow="true" cta="View guide" />

<Card title="AirbyteLoader" icon="link" href="/oss/python/integrations/document_loaders/airbyte" arrow="true" cta="View guide" />

<Card title="Airtable" icon="link" href="/oss/python/integrations/document_loaders/airtable" arrow="true" cta="View guide" />

<Card title="Alibaba Cloud MaxCompute" icon="link" href="/oss/python/integrations/document_loaders/alibaba_cloud_maxcompute" arrow="true" cta="View guide" />

<Card title="Amazon Textract" icon="link" href="/oss/python/integrations/document_loaders/amazon_textract" arrow="true" cta="View guide" />

<Card title="Apify Dataset" icon="link" href="/oss/python/integrations/document_loaders/apify_dataset" arrow="true" cta="View guide" />

<Card title="ArxivLoader" icon="link" href="/oss/python/integrations/document_loaders/arxiv" arrow="true" cta="View guide" />

<Card title="AssemblyAI Audio Transcripts" icon="link" href="/oss/python/integrations/document_loaders/assemblyai" arrow="true" cta="View guide" />

<Card title="AstraDB" icon="link" href="/oss/python/integrations/document_loaders/astradb" arrow="true" cta="View guide" />

<Card title="Async Chromium" icon="link" href="/oss/python/integrations/document_loaders/async_chromium" arrow="true" cta="View guide" />

<Card title="AsyncHtml" icon="link" href="/oss/python/integrations/document_loaders/async_html" arrow="true" cta="View guide" />

<Card title="Athena" icon="link" href="/oss/python/integrations/document_loaders/athena" arrow="true" cta="View guide" />

<Card title="AWS S3 Directory" icon="link" href="/oss/python/integrations/document_loaders/aws_s3_directory" arrow="true" cta="View guide" />

<Card title="AWS S3 File" icon="link" href="/oss/python/integrations/document_loaders/aws_s3_file" arrow="true" cta="View guide" />

<Card title="AZLyrics" icon="link" href="/oss/python/integrations/document_loaders/azlyrics" arrow="true" cta="View guide" />

<Card title="Azure AI Data" icon="link" href="/oss/python/integrations/document_loaders/azure_ai_data" arrow="true" cta="View guide" />

<Card title="Azure Blob Storage" icon="link" href="/oss/python/integrations/document_loaders/azure_blob_storage" arrow="true" cta="View guide" />

<Card title="Azure AI Document Intelligence" icon="link" href="/oss/python/integrations/document_loaders/azure_document_intelligence" arrow="true" cta="View guide" />

<Card title="BibTeX" icon="link" href="/oss/python/integrations/document_loaders/bibtex" arrow="true" cta="View guide" />

<Card title="BiliBili" icon="link" href="/oss/python/integrations/document_loaders/bilibili" arrow="true" cta="View guide" />

<Card title="Blackboard" icon="link" href="/oss/python/integrations/document_loaders/blackboard" arrow="true" cta="View guide" />

<Card title="Blockchain" icon="link" href="/oss/python/integrations/document_loaders/blockchain" arrow="true" cta="View guide" />

<Card title="Box" icon="link" href="/oss/python/integrations/document_loaders/box" arrow="true" cta="View guide" />

<Card title="Brave Search" icon="link" href="/oss/python/integrations/document_loaders/brave_search" arrow="true" cta="View guide" />

<Card title="Browserbase" icon="link" href="/oss/python/integrations/document_loaders/browserbase" arrow="true" cta="View guide" />

<Card title="Browserless" icon="link" href="/oss/python/integrations/document_loaders/browserless" arrow="true" cta="View guide" />

<Card title="BSHTMLLoader" icon="link" href="/oss/python/integrations/document_loaders/bshtml" arrow="true" cta="View guide" />

<Card title="Cassandra" icon="link" href="/oss/python/integrations/document_loaders/cassandra" arrow="true" cta="View guide" />

<Card title="ChatGPT Data" icon="link" href="/oss/python/integrations/document_loaders/chatgpt_loader" arrow="true" cta="View guide" />

<Card title="College Confidential" icon="link" href="/oss/python/integrations/document_loaders/college_confidential" arrow="true" cta="View guide" />

<Card title="Concurrent Loader" icon="link" href="/oss/python/integrations/document_loaders/concurrent" arrow="true" cta="View guide" />

<Card title="Confluence" icon="link" href="/oss/python/integrations/document_loaders/confluence" arrow="true" cta="View guide" />

<Card title="CoNLL-U" icon="link" href="/oss/python/integrations/document_loaders/conll-u" arrow="true" cta="View guide" />

<Card title="Copy Paste" icon="link" href="/oss/python/integrations/document_loaders/copypaste" arrow="true" cta="View guide" />

<Card title="Couchbase" icon="link" href="/oss/python/integrations/document_loaders/couchbase" arrow="true" cta="View guide" />

<Card title="CSV" icon="link" href="/oss/python/integrations/document_loaders/csv" arrow="true" cta="View guide" />

<Card title="Cube Semantic Layer" icon="link" href="/oss/python/integrations/document_loaders/cube_semantic" arrow="true" cta="View guide" />

<Card title="Datadog Logs" icon="link" href="/oss/python/integrations/document_loaders/datadog_logs" arrow="true" cta="View guide" />

<Card title="Dedoc" icon="link" href="/oss/python/integrations/document_loaders/dedoc" arrow="true" cta="View guide" />

<Card title="Diffbot" icon="link" href="/oss/python/integrations/document_loaders/diffbot" arrow="true" cta="View guide" />

<Card title="Discord" icon="link" href="/oss/python/integrations/document_loaders/discord" arrow="true" cta="View guide" />

<Card title="Docling" icon="link" href="/oss/python/integrations/document_loaders/docling" arrow="true" cta="View guide" />

<Card title="Docugami" icon="link" href="/oss/python/integrations/document_loaders/docugami" arrow="true" cta="View guide" />

<Card title="Docusaurus" icon="link" href="/oss/python/integrations/document_loaders/docusaurus" arrow="true" cta="View guide" />

<Card title="Dropbox" icon="link" href="/oss/python/integrations/document_loaders/dropbox" arrow="true" cta="View guide" />

<Card title="Email" icon="link" href="/oss/python/integrations/document_loaders/email" arrow="true" cta="View guide" />

<Card title="EPub" icon="link" href="/oss/python/integrations/document_loaders/epub" arrow="true" cta="View guide" />

<Card title="Etherscan" icon="link" href="/oss/python/integrations/document_loaders/etherscan" arrow="true" cta="View guide" />

<Card title="EverNote" icon="link" href="/oss/python/integrations/document_loaders/evernote" arrow="true" cta="View guide" />

<Card title="Facebook Chat" icon="link" href="/oss/python/integrations/document_loaders/facebook_chat" arrow="true" cta="View guide" />

<Card title="Fauna" icon="link" href="/oss/python/integrations/document_loaders/fauna" arrow="true" cta="View guide" />

<Card title="Figma" icon="link" href="/oss/python/integrations/document_loaders/figma" arrow="true" cta="View guide" />

<Card title="FireCrawl" icon="link" href="/oss/python/integrations/document_loaders/firecrawl" arrow="true" cta="View guide" />

<Card title="Geopandas" icon="link" href="/oss/python/integrations/document_loaders/geopandas" arrow="true" cta="View guide" />

<Card title="Git" icon="link" href="/oss/python/integrations/document_loaders/git" arrow="true" cta="View guide" />

<Card title="GitBook" icon="link" href="/oss/python/integrations/document_loaders/gitbook" arrow="true" cta="View guide" />

<Card title="GitHub" icon="link" href="/oss/python/integrations/document_loaders/github" arrow="true" cta="View guide" />

<Card title="Glue Catalog" icon="link" href="/oss/python/integrations/document_loaders/glue_catalog" arrow="true" cta="View guide" />

<Card title="Google AlloyDB for PostgreSQL" icon="link" href="/oss/python/integrations/document_loaders/google_alloydb" arrow="true" cta="View guide" />

<Card title="Google BigQuery" icon="link" href="/oss/python/integrations/document_loaders/google_bigquery" arrow="true" cta="View guide" />

<Card title="Google Bigtable" icon="link" href="/oss/python/integrations/document_loaders/google_bigtable" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for SQL Server" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_mssql" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for MySQL" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_mysql" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for PostgreSQL" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_sql_pg" arrow="true" cta="View guide" />

<Card title="Google Cloud Storage Directory" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_storage_directory" arrow="true" cta="View guide" />

<Card title="Google Cloud Storage File" icon="link" href="/oss/python/integrations/document_loaders/google_cloud_storage_file" arrow="true" cta="View guide" />

<Card title="Google Firestore in Datastore Mode" icon="link" href="/oss/python/integrations/document_loaders/google_datastore" arrow="true" cta="View guide" />

<Card title="Google Drive" icon="link" href="/oss/python/integrations/document_loaders/google_drive" arrow="true" cta="View guide" />

<Card title="Google El Carro for Oracle Workloads" icon="link" href="/oss/python/integrations/document_loaders/google_el_carro" arrow="true" cta="View guide" />

<Card title="Google Firestore (Native Mode)" icon="link" href="/oss/python/integrations/document_loaders/google_firestore" arrow="true" cta="View guide" />

<Card title="Google Memorystore for Redis" icon="link" href="/oss/python/integrations/document_loaders/google_memorystore_redis" arrow="true" cta="View guide" />

<Card title="Google Spanner" icon="link" href="/oss/python/integrations/document_loaders/google_spanner" arrow="true" cta="View guide" />

<Card title="Google Speech-to-Text" icon="link" href="/oss/python/integrations/document_loaders/google_speech_to_text" arrow="true" cta="View guide" />

<Card title="Grobid" icon="link" href="/oss/python/integrations/document_loaders/grobid" arrow="true" cta="View guide" />

<Card title="Gutenberg" icon="link" href="/oss/python/integrations/document_loaders/gutenberg" arrow="true" cta="View guide" />

<Card title="Hacker News" icon="link" href="/oss/python/integrations/document_loaders/hacker_news" arrow="true" cta="View guide" />

<Card title="Huawei OBS Directory" icon="link" href="/oss/python/integrations/document_loaders/huawei_obs_directory" arrow="true" cta="View guide" />

<Card title="Huawei OBS File" icon="link" href="/oss/python/integrations/document_loaders/huawei_obs_file" arrow="true" cta="View guide" />

<Card title="HuggingFace Dataset" icon="link" href="/oss/python/integrations/document_loaders/hugging_face_dataset" arrow="true" cta="View guide" />

<Card title="HyperbrowserLoader" icon="link" href="/oss/python/integrations/document_loaders/hyperbrowser" arrow="true" cta="View guide" />

<Card title="iFixit" icon="link" href="/oss/python/integrations/document_loaders/ifixit" arrow="true" cta="View guide" />

<Card title="Images" icon="link" href="/oss/python/integrations/document_loaders/image" arrow="true" cta="View guide" />

<Card title="Image Captions" icon="link" href="/oss/python/integrations/document_loaders/image_captions" arrow="true" cta="View guide" />

<Card title="IMSDb" icon="link" href="/oss/python/integrations/document_loaders/imsdb" arrow="true" cta="View guide" />

<Card title="Iugu" icon="link" href="/oss/python/integrations/document_loaders/iugu" arrow="true" cta="View guide" />

<Card title="Joplin" icon="link" href="/oss/python/integrations/document_loaders/joplin" arrow="true" cta="View guide" />

<Card title="JSONLoader" icon="link" href="/oss/python/integrations/document_loaders/json" arrow="true" cta="View guide" />

<Card title="Jupyter Notebook" icon="link" href="/oss/python/integrations/document_loaders/jupyter_notebook" arrow="true" cta="View guide" />

<Card title="Kinetica" icon="link" href="/oss/python/integrations/document_loaders/kinetica" arrow="true" cta="View guide" />

<Card title="lakeFS" icon="link" href="/oss/python/integrations/document_loaders/lakefs" arrow="true" cta="View guide" />

<Card title="LangSmith" icon="link" href="/oss/python/integrations/document_loaders/langsmith" arrow="true" cta="View guide" />

<Card title="LarkSuite (FeiShu)" icon="link" href="/oss/python/integrations/document_loaders/larksuite" arrow="true" cta="View guide" />

<Card title="LLM Sherpa" icon="link" href="/oss/python/integrations/document_loaders/llmsherpa" arrow="true" cta="View guide" />

<Card title="Mastodon" icon="link" href="/oss/python/integrations/document_loaders/mastodon" arrow="true" cta="View guide" />

<Card title="MathPixPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/mathpix" arrow="true" cta="View guide" />

<Card title="MediaWiki Dump" icon="link" href="/oss/python/integrations/document_loaders/mediawikidump" arrow="true" cta="View guide" />

<Card title="Merge Documents Loader" icon="link" href="/oss/python/integrations/document_loaders/merge_doc" arrow="true" cta="View guide" />

<Card title="MHTML" icon="link" href="/oss/python/integrations/document_loaders/mhtml" arrow="true" cta="View guide" />

<Card title="Microsoft Excel" icon="link" href="/oss/python/integrations/document_loaders/microsoft_excel" arrow="true" cta="View guide" />

<Card title="Microsoft OneDrive" icon="link" href="/oss/python/integrations/document_loaders/microsoft_onedrive" arrow="true" cta="View guide" />

<Card title="Microsoft OneNote" icon="link" href="/oss/python/integrations/document_loaders/microsoft_onenote" arrow="true" cta="View guide" />

<Card title="Microsoft PowerPoint" icon="link" href="/oss/python/integrations/document_loaders/microsoft_powerpoint" arrow="true" cta="View guide" />

<Card title="Microsoft SharePoint" icon="link" href="/oss/python/integrations/document_loaders/microsoft_sharepoint" arrow="true" cta="View guide" />

<Card title="Microsoft Word" icon="link" href="/oss/python/integrations/document_loaders/microsoft_word" arrow="true" cta="View guide" />

<Card title="Near Blockchain" icon="link" href="/oss/python/integrations/document_loaders/mintbase" arrow="true" cta="View guide" />

<Card title="Modern Treasury" icon="link" href="/oss/python/integrations/document_loaders/modern_treasury" arrow="true" cta="View guide" />

<Card title="MongoDB" icon="link" href="/oss/python/integrations/document_loaders/mongodb" arrow="true" cta="View guide" />

<Card title="Needle Document Loader" icon="link" href="/oss/python/integrations/document_loaders/needle" arrow="true" cta="View guide" />

<Card title="News URL" icon="link" href="/oss/python/integrations/document_loaders/news" arrow="true" cta="View guide" />

<Card title="Notion DB" icon="link" href="/oss/python/integrations/document_loaders/notion" arrow="true" cta="View guide" />

<Card title="Nuclia" icon="link" href="/oss/python/integrations/document_loaders/nuclia" arrow="true" cta="View guide" />

<Card title="Obsidian" icon="link" href="/oss/python/integrations/document_loaders/obsidian" arrow="true" cta="View guide" />

<Card title="OpenDataLoader PDF" icon="link" href="/oss/python/integrations/document_loaders/opendataloader_pdf" arrow="true" cta="View guide" />

<Card title="Open Document Format (ODT)" icon="link" href="/oss/python/integrations/document_loaders/odt" arrow="true" cta="View guide" />

<Card title="Open City Data" icon="link" href="/oss/python/integrations/document_loaders/open_city_data" arrow="true" cta="View guide" />

<Card title="Oracle Autonomous Database" icon="link" href="/oss/python/integrations/document_loaders/oracleadb_loader" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/document_loaders/oracleai" arrow="true" cta="View guide" />

<Card title="Org-mode" icon="link" href="/oss/python/integrations/document_loaders/org_mode" arrow="true" cta="View guide" />

<Card title="Outline Document Loader" icon="link" href="/oss/python/integrations/document_loaders/outline" arrow="true" cta="View guide" />

<Card title="Pandas DataFrame" icon="link" href="/oss/python/integrations/document_loaders/pandas_dataframe" arrow="true" cta="View guide" />

<Card title="PDFMinerLoader" icon="link" href="/oss/python/integrations/document_loaders/pdfminer" arrow="true" cta="View guide" />

<Card title="PDFPlumber" icon="link" href="/oss/python/integrations/document_loaders/pdfplumber" arrow="true" cta="View guide" />

<Card title="Pebblo Safe DocumentLoader" icon="link" href="/oss/python/integrations/document_loaders/pebblo" arrow="true" cta="View guide" />

<Card title="Polaris AI DataInsight" icon="link" href="/oss/python/integrations/document_loaders/polaris_ai_datainsight" arrow="true" cta="View guide" />

<Card title="Polars DataFrame" icon="link" href="/oss/python/integrations/document_loaders/polars_dataframe" arrow="true" cta="View guide" />

<Card title="Dell PowerScale" icon="link" href="/oss/python/integrations/document_loaders/powerscale" arrow="true" cta="View guide" />

<Card title="Psychic" icon="link" href="/oss/python/integrations/document_loaders/psychic" arrow="true" cta="View guide" />

<Card title="PubMed" icon="link" href="/oss/python/integrations/document_loaders/pubmed" arrow="true" cta="View guide" />

<Card title="PullMdLoader" icon="link" href="/oss/python/integrations/document_loaders/pull_md" arrow="true" cta="View guide" />

<Card title="PyMuPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/pymupdf" arrow="true" cta="View guide" />

<Card title="PyMuPDF4LLM" icon="link" href="/oss/python/integrations/document_loaders/pymupdf4llm" arrow="true" cta="View guide" />

<Card title="PyPDFDirectoryLoader" icon="link" href="/oss/python/integrations/document_loaders/pypdfdirectory" arrow="true" cta="View guide" />

<Card title="PyPDFium2Loader" icon="link" href="/oss/python/integrations/document_loaders/pypdfium2" arrow="true" cta="View guide" />

<Card title="PyPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/pypdfloader" arrow="true" cta="View guide" />

<Card title="PySpark" icon="link" href="/oss/python/integrations/document_loaders/pyspark_dataframe" arrow="true" cta="View guide" />

<Card title="Quip" icon="link" href="/oss/python/integrations/document_loaders/quip" arrow="true" cta="View guide" />

<Card title="ReadTheDocs Documentation" icon="link" href="/oss/python/integrations/document_loaders/readthedocs_documentation" arrow="true" cta="View guide" />

<Card title="Recursive URL" icon="link" href="/oss/python/integrations/document_loaders/recursive_url" arrow="true" cta="View guide" />

<Card title="Reddit" icon="link" href="/oss/python/integrations/document_loaders/reddit" arrow="true" cta="View guide" />

<Card title="Roam" icon="link" href="/oss/python/integrations/document_loaders/roam" arrow="true" cta="View guide" />

<Card title="Rockset" icon="link" href="/oss/python/integrations/document_loaders/rockset" arrow="true" cta="View guide" />

<Card title="rspace" icon="link" href="/oss/python/integrations/document_loaders/rspace" arrow="true" cta="View guide" />

<Card title="RSS Feeds" icon="link" href="/oss/python/integrations/document_loaders/rss" arrow="true" cta="View guide" />

<Card title="RST" icon="link" href="/oss/python/integrations/document_loaders/rst" arrow="true" cta="View guide" />

<Card title="scrapfly" icon="link" href="/oss/python/integrations/document_loaders/scrapfly" arrow="true" cta="View guide" />

<Card title="ScrapingAnt" icon="link" href="/oss/python/integrations/document_loaders/scrapingant" arrow="true" cta="View guide" />

<Card title="SingleStore" icon="link" href="/oss/python/integrations/document_loaders/singlestore" arrow="true" cta="View guide" />

<Card title="Sitemap" icon="link" href="/oss/python/integrations/document_loaders/sitemap" arrow="true" cta="View guide" />

<Card title="Slack" icon="link" href="/oss/python/integrations/document_loaders/slack" arrow="true" cta="View guide" />

<Card title="Snowflake" icon="link" href="/oss/python/integrations/document_loaders/snowflake" arrow="true" cta="View guide" />

<Card title="Source Code" icon="link" href="/oss/python/integrations/document_loaders/source_code" arrow="true" cta="View guide" />

<Card title="Spider" icon="link" href="/oss/python/integrations/document_loaders/spider" arrow="true" cta="View guide" />

<Card title="Spreedly" icon="link" href="/oss/python/integrations/document_loaders/spreedly" arrow="true" cta="View guide" />

<Card title="Stripe" icon="link" href="/oss/python/integrations/document_loaders/stripe" arrow="true" cta="View guide" />

<Card title="Subtitle" icon="link" href="/oss/python/integrations/document_loaders/subtitle" arrow="true" cta="View guide" />

<Card title="SurrealDB" icon="link" href="/oss/python/integrations/document_loaders/surrealdb" arrow="true" cta="View guide" />

<Card title="Telegram" icon="link" href="/oss/python/integrations/document_loaders/telegram" arrow="true" cta="View guide" />

<Card title="Tencent COS Directory" icon="link" href="/oss/python/integrations/document_loaders/tencent_cos_directory" arrow="true" cta="View guide" />

<Card title="Tencent COS File" icon="link" href="/oss/python/integrations/document_loaders/tencent_cos_file" arrow="true" cta="View guide" />

<Card title="TensorFlow Datasets" icon="link" href="/oss/python/integrations/document_loaders/tensorflow_datasets" arrow="true" cta="View guide" />

<Card title="TiDB" icon="link" href="/oss/python/integrations/document_loaders/tidb" arrow="true" cta="View guide" />

<Card title="2Markdown" icon="link" href="/oss/python/integrations/document_loaders/tomarkdown" arrow="true" cta="View guide" />

<Card title="TOML" icon="link" href="/oss/python/integrations/document_loaders/toml" arrow="true" cta="View guide" />

<Card title="Trello" icon="link" href="/oss/python/integrations/document_loaders/trello" arrow="true" cta="View guide" />

<Card title="TSV" icon="link" href="/oss/python/integrations/document_loaders/tsv" arrow="true" cta="View guide" />

<Card title="Twitter" icon="link" href="/oss/python/integrations/document_loaders/twitter" arrow="true" cta="View guide" />

<Card title="UnDatasIO" icon="link" href="/oss/python/integrations/document_loaders/undatasio" arrow="true" cta="View guide" />

<Card title="Unstructured" icon="link" href="/oss/python/integrations/document_loaders/unstructured_file" arrow="true" cta="View guide" />

<Card title="UnstructuredMarkdownLoader" icon="link" href="/oss/python/integrations/document_loaders/unstructured_markdown" arrow="true" cta="View guide" />

<Card title="UnstructuredPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/unstructured_pdfloader" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/document_loaders/upstage" arrow="true" cta="View guide" />

<Card title="URL" icon="link" href="/oss/python/integrations/document_loaders/url" arrow="true" cta="View guide" />

<Card title="Vsdx" icon="link" href="/oss/python/integrations/document_loaders/vsdx" arrow="true" cta="View guide" />

<Card title="Weather" icon="link" href="/oss/python/integrations/document_loaders/weather" arrow="true" cta="View guide" />

<Card title="WebBaseLoader" icon="link" href="/oss/python/integrations/document_loaders/web_base" arrow="true" cta="View guide" />

<Card title="WhatsApp Chat" icon="link" href="/oss/python/integrations/document_loaders/whatsapp_chat" arrow="true" cta="View guide" />

<Card title="Wikipedia" icon="link" href="/oss/python/integrations/document_loaders/wikipedia" arrow="true" cta="View guide" />

<Card title="UnstructuredXMLLoader" icon="link" href="/oss/python/integrations/document_loaders/xml" arrow="true" cta="View guide" />

<Card title="Xorbits Pandas DataFrame" icon="link" href="/oss/python/integrations/document_loaders/xorbits" arrow="true" cta="View guide" />

<Card title="YouTube Audio" icon="link" href="/oss/python/integrations/document_loaders/youtube_audio" arrow="true" cta="View guide" />

<Card title="YouTube Transcripts" icon="link" href="/oss/python/integrations/document_loaders/youtube_transcript" arrow="true" cta="View guide" />

<Card title="YoutubeLoaderDL" icon="link" href="/oss/python/integrations/document_loaders/yt_dlp" arrow="true" cta="View guide" />

<Card title="Yuque" icon="link" href="/oss/python/integrations/document_loaders/yuque" arrow="true" cta="View guide" />

<Card title="ZeroxPDFLoader" icon="link" href="/oss/python/integrations/document_loaders/zeroxpdfloader" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/document_loaders/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## For production use, consider using a configuration file or environment variables

**URL:** llms-txt#for-production-use,-consider-using-a-configuration-file-or-environment-variables

api_url = "https://api.smith.langchain.com"
api_key = os.environ.get("LANGSMITH_API_KEY")

if not api_key:
    raise ValueError("LANGSMITH_API_KEY environment variable is not set")

---

## Frequently Asked Questions

**URL:** llms-txt#frequently-asked-questions

**Contents:**
- Questions and Answers
  - I've been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?
  - Which plan is right for me?
  - What is a seat?
  - What is a trace?
  - What is an Agent Run?
  - What is an ingested event?
  - I've hit my rate or usage limits. What can I do?
  - I have a developer account, can I upgrade my account to the Plus or Enterprise plan?
  - How does billing work?

Source: https://docs.langchain.com/langsmith/pricing-faq

## Questions and Answers

### I've been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?

If you've been using LangSmith already, your usage will be billable starting in July 2024. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by [contacting our sales team](https://www.langchain.com/contact-sales).

### Which plan is right for me?

If you're an individual developer, the Developer plan is a great choice for small projects.

For teams that want to collaborate in LangSmith, check out the Plus plan. **If you are an early-stage startup building an AI application**, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our [Startup Contact Form](https://airtable.com/app8ZrGLtHAtFVO1o/pagfLAmdTz4ep7TGu/form) for more details.

If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our [Sales Contact Form](https://www.langchain.com/contact-sales) for more details.

A seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.

A trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an [example](https://smith.langchain.com/public/17c24270-9f74-47e7-b70c-d508afc448fa/r) of a single trace.

### What is an Agent Run?

An Agent Run is one end-to-end invocation of a LangGraph agent deployed via LangSmith Deployment. Nodes and subgraphs within a single agent execution are not charged separately. Calls to other LangGraph agents (through RemoteGraph or the LangGraph SDK or the API directly) are charged separately, to the deployment that hosts the agent being called. An interrupt for human-in-the-loop creates a separate Agent Run when resuming.

Agent Runs are billed at \$0.005 each. For high-volume usage, please [contact our sales team](https://www.langchain.com/contact-sales) to discuss custom pricing options.

### What is an ingested event?

An ingested event is any distinct, trace-related data sent to LangSmith. This includes:

* Inputs, outputs and metadata sent at the start of a run step within a trace
* Inputs, outputs and metadata sent at the end of a run step within a trace
* Feedback on run steps or traces

### I've hit my rate or usage limits. What can I do?

When you first sign up for a LangSmith account, you get a Personal organization that is limited to 5000 monthly traces. To continue sending traces after reaching this limit, upgrade to the Developer or Plus plans by adding a credit card. Head to [Plans and Billing](https://smith.langchain.com/settings/payments) to upgrade.

Similarly, if you've hit the rate limits on your current plan, you can upgrade to a higher plan to get higher limits, or contact support via [support.langchain.com](https://support.langchain.com) with questions.

### I have a developer account, can I upgrade my account to the Plus or Enterprise plan?

Yes, Developer plan users can easily upgrade to the Plus plan on the [Plans and Billing](https://smith.langchain.com/settings/payments) page. For the Enterprise plan, please [contact our sales team](https://www.langchain.com/contact-sales) to discuss your needs.

### How does billing work?

Seats are billed monthly on the first of the month. Additional seats purchased mid-month are pro-rated and billed within one day of the purchase. Seats removed mid-month will not be credited.

As long as you have a card on file in your account, we'll service your traces and bill you on the first of the month for traces that you submitted in the previous month. You will be able to set usage limits if you so choose to limit the maximum charges you could incur in any given month.

### Can I limit how much I spend on tracing?

You can set limits on the number of traces that can be sent to LangSmith per month on the [Usage configuration](https://smith.langchain.com/settings/payments) page.

<Note>
  While we do show you the dollar value of your usage limit for convenience, this limit evaluated in terms of number of traces instead of dollar amount. For example, if you are approved for our startup plan tier where you are given a generous allotment of free traces, your usage limit will not automatically change.

You are not currently able to set a spend limit in the product.
</Note>

### How can I track my usage so far this month?

Under the Settings section for your Organization you will see subsection for **Usage**. There, you will be able to see a graph of the daily number of billable LangSmith traces from the last 30, 60, or 90 days. Note that this data is delayed by 1-2 hours and so may trail your actual number of runs slightly for the current day.

### I have a question about my bill...

Customers on the Developer and Plus plan tiers should contact support via [support.langchain.com](https://support.langchain.com). Customers on the Enterprise plan should contact their sales representative directly.

Enterprise plan customers are billed annually by invoice.

### What can I expect from Support?

On the Developer plan, community-based support is available on [LangChain community Slack](https://www.langchain.com/join-community).

On the Plus plan, you will also receive preferential support via [support.langchain.com](https://support.langchain.com) for LangSmith-related questions only and we'll do our best to respond within the next business day.

On the Enterprise plan, you'll get white-glove support with a Slack channel, a dedicated customer success manager, and monthly check-ins to go over LangSmith and LangChain questions. We can help with anything from debugging, agent and RAG techniques, evaluation approaches, and cognitive architecture reviews. If you purchase the add-on to run LangSmith in your environment, we'll also support deployments and new releases with our infra engineering team on-call.

### Where is my data stored?

You may choose to sign up in either the US or EU region. See the [cloud architecture reference](/langsmith/cloud#cloud-architecture-and-scalability) for more details. If you're on the Enterprise plan, we can deliver LangSmith to run on your kubernetes cluster in AWS, GCP, or Azure so that data never leaves your environment.

### Which security frameworks is LangSmith compliant with?

We are SOC 2 Type II, GDPR, and HIPAA compliant.

You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). Please note we only enter into BAAs with customers on our Enterprise plan.

### Will you train on the data that I send LangSmith?

We will not train on your data, and you own all rights to your data. See [LangSmith Terms of Service](https://langchain.dev/terms-of-service) for more information.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pricing-faq.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## from langsmith import aevaluate

**URL:** llms-txt#from-langsmith-import-aevaluate

---

## Functional API: Minimal changes to existing code

**URL:** llms-txt#functional-api:-minimal-changes-to-existing-code

**Contents:**
- Combining both APIs

from langgraph.func import entrypoint, task

@task
def process_user_input(user_input: str) -> dict:
    # Existing function with minimal changes
    return {"processed": user_input.lower().strip()}

@entrypoint(checkpointer=checkpointer)
def workflow(user_input: str) -> str:
    # Standard Python control flow
    processed = process_user_input(user_input).result()

if "urgent" in processed["processed"]:
        response = handle_urgent_request(processed).result()
    else:
        response = handle_normal_request(processed).result()

return response
python  theme={null}
@entrypoint(checkpointer=checkpointer)
def essay_workflow(topic: str) -> dict:
    # Linear flow with simple branching
    outline = create_outline(topic).result()

if len(outline["points"]) < 3:
        outline = expand_outline(outline).result()

draft = write_draft(outline).result()

# Human review checkpoint
    feedback = interrupt({"draft": draft, "action": "Please review"})

if feedback == "approve":
        final_essay = draft
    else:
        final_essay = revise_essay(draft, feedback).result()

return {"essay": final_essay}
python  theme={null}
@entrypoint(checkpointer=checkpointer)
def quick_prototype(data: dict) -> dict:
    # Fast iteration - no state schema needed
    step1_result = process_step1(data).result()
    step2_result = process_step2(step1_result).result()

return {"final_result": step2_result}
python  theme={null}
@task
def analyze_document(document: str) -> dict:
    # Local state management within function
    sections = extract_sections(document)
    summaries = [summarize(section) for section in sections]
    key_points = extract_key_points(summaries)

return {
        "sections": len(sections),
        "summaries": summaries,
        "key_points": key_points
    }

@entrypoint(checkpointer=checkpointer)
def document_processor(document: str) -> dict:
    analysis = analyze_document(document).result()
    # State is passed between functions as needed
    return generate_report(analysis).result()
python  theme={null}
from langgraph.graph import StateGraph
from langgraph.func import entrypoint

**Examples:**

Example 1 (unknown):
```unknown
**2. Linear workflows with simple logic**

When your workflow is primarily sequential with straightforward conditional logic.
```

Example 2 (unknown):
```unknown
**3. Rapid prototyping**

When you want to quickly test ideas without the overhead of defining state schemas and graph structures.
```

Example 3 (unknown):
```unknown
**4. Function-scoped state management**

When your state is naturally scoped to individual functions and doesn't need to be shared broadly.
```

Example 4 (unknown):
```unknown
## Combining both APIs

You can use both APIs together in the same application. This is useful when different parts of your system have different requirements.
```

---

## Generate a completion

**URL:** llms-txt#generate-a-completion

client = openai.Client()
chat_completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages
)

---

## Generate ClickHouse stats

**URL:** llms-txt#generate-clickhouse-stats

**Contents:**
  - Prerequisites
  - Running the clickhouse stats generation script

Source: https://docs.langchain.com/langsmith/script-generate-clickhouse-stats

As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate Clickhouse statistics that will help us understand memory and CPU consumption and connection concurrency.

This command will generate a CSV that can be shared with the LangChain team.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `get_clickhouse_stats` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to generate ClickHouse stats

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_clickhouse_stats.sh)

### Running the clickhouse stats generation script

Run the following command to run the stats generation script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

and after running this command you should see a file, clickhouse\_stats.csv, has been created with Clickhouse statistics.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-generate-clickhouse-stats.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Generate query stats

**URL:** llms-txt#generate-query-stats

**Contents:**
  - Prerequisites
  - Running the query stats generation script

Source: https://docs.langchain.com/langsmith/script-generate-query-stats

As part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience.

This command will generate a CSV that can be shared with the LangChain team.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the `get_query_stats` script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to generate query stats

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_query_stats.sh)

### Running the query stats generation script

Run the following command to run the stats generation script:

For example, if you are using the bundled version with port-forwarding, the command would look like:

and after running this command you should see a file, query\_stats.csv, has been created with LangSmith query statistics.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-generate-query-stats.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command would look like:
```

---

## Generic / global handler catches calls that aren't handled by more specific handlers

**URL:** llms-txt#generic-/-global-handler-catches-calls-that-aren't-handled-by-more-specific-handlers

@auth.on
async def reject_unhandled_requests(ctx: Auth.types.AuthContext, value: Any) -> False:
    print(f"Request to {ctx.path} by {ctx.user.identity}")
    raise Auth.exceptions.HTTPException(
        status_code=403,
        detail="Forbidden"
    )

---

## Get Assistant

**URL:** llms-txt#get-assistant

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/get-assistant

langsmith/agent-server-openapi.json get /assistants/{assistant_id}
Get an assistant by ID.

---

## Get Assistant Graph

**URL:** llms-txt#get-assistant-graph

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/get-assistant-graph

langsmith/agent-server-openapi.json get /assistants/{assistant_id}/graph
Get an assistant by ID.

---

## Get Assistant Schemas

**URL:** llms-txt#get-assistant-schemas

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/get-assistant-schemas

langsmith/agent-server-openapi.json get /assistants/{assistant_id}/schemas
Get an assistant by ID.

---

## Get Assistant Subgraphs

**URL:** llms-txt#get-assistant-subgraphs

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/get-assistant-subgraphs

langsmith/agent-server-openapi.json get /assistants/{assistant_id}/subgraphs
Get an assistant's subgraphs.

---

## Get Assistant Subgraphs by Namespace

**URL:** llms-txt#get-assistant-subgraphs-by-namespace

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/get-assistant-subgraphs-by-namespace

langsmith/agent-server-openapi.json get /assistants/{assistant_id}/subgraphs/{namespace}
Get an assistant's subgraphs filtered by namespace.

---

## Get Assistant Versions

**URL:** llms-txt#get-assistant-versions

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/get-assistant-versions

langsmith/agent-server-openapi.json post /assistants/{assistant_id}/versions
Get all versions of an assistant.

---

## get a state snapshot for a specific checkpoint_id

**URL:** llms-txt#get-a-state-snapshot-for-a-specific-checkpoint_id

**Contents:**
  - Get state history
  - Replay
  - Update state
- Memory Store
  - Basic Usage
  - Semantic Search

config = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}}
graph.get_state(config)

StateSnapshot(
    values={'foo': 'b', 'bar': ['a', 'b']},
    next=(),
    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
    created_at='2024-08-29T19:19:38.821749+00:00',
    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()
)
python  theme={null}
config = {"configurable": {"thread_id": "1"}}
list(graph.get_state_history(config))

[
    StateSnapshot(
        values={'foo': 'b', 'bar': ['a', 'b']},
        next=(),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},
        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},
        created_at='2024-08-29T19:19:38.821749+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
        tasks=(),
    ),
    StateSnapshot(
        values={'foo': 'a', 'bar': ['a']},
        next=('node_b',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},
        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},
        created_at='2024-08-29T19:19:38.819946+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},
        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),
    ),
    StateSnapshot(
        values={'foo': '', 'bar': []},
        next=('node_a',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},
        metadata={'source': 'loop', 'writes': None, 'step': 0},
        created_at='2024-08-29T19:19:38.817813+00:00',
        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},
        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),
    ),
    StateSnapshot(
        values={'bar': []},
        next=('__start__',),
        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},
        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},
        created_at='2024-08-29T19:19:38.816205+00:00',
        parent_config=None,
        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),
    )
]
python  theme={null}
config = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}}
graph.invoke(None, config=config)
python  theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]

{"foo": 1, "bar": ["a"]}
python  theme={null}
graph.update_state(config, {"foo": 2, "bar": ["b"]})

{"foo": 2, "bar": ["a", "b"]}
python  theme={null}
from langgraph.store.memory import InMemoryStore
in_memory_store = InMemoryStore()
python  theme={null}
user_id = "1"
namespace_for_memory = (user_id, "memories")
python  theme={null}
memory_id = str(uuid.uuid4())
memory = {"food_preference" : "I like pizza"}
in_memory_store.put(namespace_for_memory, memory_id, memory)
python  theme={null}
memories = in_memory_store.search(namespace_for_memory)
memories[-1].dict()
{'value': {'food_preference': 'I like pizza'},
 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',
 'namespace': ['1', 'memories'],
 'created_at': '2024-10-02T17:22:31.590602+00:00',
 'updated_at': '2024-10-02T17:22:31.590605+00:00'}
python  theme={null}
from langchain.embeddings import init_embeddings

store = InMemoryStore(
    index={
        "embed": init_embeddings("openai:text-embedding-3-small"),  # Embedding provider
        "dims": 1536,                              # Embedding dimensions
        "fields": ["food_preference", "$"]              # Fields to embed
    }
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
In our example, the output of `get_state` will look like this:
```

Example 2 (unknown):
```unknown
### Get state history

You can get the full history of the graph execution for a given thread by calling [`graph.get_state_history(config)`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get_state_history). This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list.
```

Example 3 (unknown):
```unknown
In our example, the output of [`get_state_history`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get_state_history) will look like this:
```

Example 4 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=38ffff52be4d8806b287836295a3c058" alt="State" data-og-width="2692" width="2692" data-og-height="1056" height="1056" data-path="oss/images/get_state.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e932acac5021614d0eb99b90e54be004 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=2eaf153fd49ba728e1d679c12bb44b6f 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=0ac091c7dbe8b1f0acff97615a3683ee 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9921a482f1c4f86316fca23a5150b153 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9412cd906f6d67a9fe1f50a5d4f4c674 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/get_state.jpg?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ccc5118ed85926bda3715c81ce728fcc 2500w" />

### Replay

It's also possible to play-back a prior graph execution. If we `invoke` a graph with a `thread_id` and a `checkpoint_id`, then we will *re-play* the previously executed steps *before* a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps *after* the checkpoint.

* `thread_id` is the ID of a thread.
* `checkpoint_id` is an identifier that refers to a specific checkpoint within a thread.

You must pass these when invoking the graph as part of the `configurable` portion of the config:
```

---

## Get customer information from the API

**URL:** llms-txt#get-customer-information-from-the-api

export LANGSMITH_URL="<your_langsmith_url>"
export response=$(curl -s $LANGSMITH_URL/api/v1/info)
export CUSTOMER_ID=$(echo "$response" | jq -r '.customer_info.customer_id') && echo "Customer ID: $CUSTOMER_ID"
export CUSTOMER_NAME=$(echo "$response" | jq -r '.customer_info.customer_name') && echo "Customer name: $CUSTOMER_NAME"

---

## Get Deployment

**URL:** llms-txt#get-deployment

Source: https://docs.langchain.com/api-reference/deployments-v2/get-deployment

https://api.host.langchain.com/openapi.json get /v2/deployments/{deployment_id}
Get a deployment by ID.

---

## Get email from command line

**URL:** llms-txt#get-email-from-command-line

email = getpass("Enter your email: ")
base_email = email.split("@")
password = "secure-password"  # CHANGEME
email1 = f"{base_email[0]}+1@{base_email[1]}"
email2 = f"{base_email[0]}+2@{base_email[1]}"

SUPABASE_URL = os.environ.get("SUPABASE_URL")
if not SUPABASE_URL:
    SUPABASE_URL = getpass("Enter your Supabase project URL: ")

---

## Get help

**URL:** llms-txt#get-help

**Contents:**
- Learning resources
- Community support
- Professional support
- Contribute
- Stay connected

Source: https://docs.langchain.com/oss/python/langchain/get-help

Connect with the LangChain community, access learning resources, and get the support you need to build with confidence.

## Learning resources

Start your journey or deepen your knowledge with our comprehensive learning materials.

* **[Chat LangChain](https://chat.langchain.com/)**: Ask the docs anything about LangChain, powered by real-time docs
* **[API Reference](https://reference.langchain.com/python/)**: Complete documentation for all LangChain packages

Get help from fellow developers and the LangChain team through our active community channels.

* **[Community Forum](https://forum.langchain.com/)**: Ask questions, share solutions, and discuss best practices
* **[Community Slack](https://www.langchain.com/join-community)**: Connect with other builders and get quick help

## Professional support

For enterprise needs and critical applications, access dedicated support channels.

* **[Support portal](https://support.langchain.com/)**: Submit tickets and track support requests
* **[LangSmith status](https://status.smith.langchain.com/)**: Real-time status of LangSmith services and APIs

Help us improve LangChain for everyone. Whether you're fixing bugs, adding features, or improving documentation, we welcome your contributions.

* **[Contributing Guide](/oss/python/contributing/overview)**: Everything you need to know about contributing to LangChain

Follow us for the latest updates, announcements, and community highlights.

* **[X (Twitter)](https://twitter.com/langchainai)**: Daily updates and community spotlights
* **[LinkedIn](https://www.linkedin.com/company/langchain/)**: Professional network and company updates

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/get-help.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Get Listener

**URL:** llms-txt#get-listener

Source: https://docs.langchain.com/api-reference/listeners-v2/get-listener

https://api.host.langchain.com/openapi.json get /v2/listeners/{listener_id}
Get a listener by ID.

---

## Get Oauth Provider

**URL:** llms-txt#get-oauth-provider

Source: https://docs.langchain.com/api-reference/auth-service-v2/get-oauth-provider

https://api.host.langchain.com/openapi.json get /v2/auth/providers/{provider_id}
Get a specific OAuth provider.

---

## Get or create tracer provider

**URL:** llms-txt#get-or-create-tracer-provider

tracer_provider = trace.get_tracer_provider()
if not isinstance(tracer_provider, TracerProvider):
    tracer_provider = TracerProvider()
    trace.set_tracer_provider(tracer_provider)

---

## Get Revision

**URL:** llms-txt#get-revision

Source: https://docs.langchain.com/api-reference/deployments-v2/get-revision

https://api.host.langchain.com/openapi.json get /v2/deployments/{deployment_id}/revisions/{revision_id}
Get a revision by ID for a deployment.

---

## Get Run

**URL:** llms-txt#get-run

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/get-run

langsmith/agent-server-openapi.json get /threads/{thread_id}/runs/{run_id}
Get a run by ID.

---

## Get the API response and extract customer information

**URL:** llms-txt#get-the-api-response-and-extract-customer-information

export LANGSMITH_URL="<your_langsmith_url>"
response=$(curl -s $LANGSMITH_URL/api/v1/info)

---

## Get the current tracer

**URL:** llms-txt#get-the-current-tracer

**Contents:**
  - Combining with other instrumentors

tracer = trace.get_tracer(__name__)

async def main():
    with tracer.start_as_current_span("semantic_kernel_workflow") as span:
        # Add custom metadata
        span.set_attribute("langsmith.metadata.workflow_type", "code_analysis")
        span.set_attribute("langsmith.metadata.user_id", "developer_123")
        span.set_attribute("langsmith.span.tags", "semantic-kernel,code-analysis")

# Your Semantic Kernel code here
        result = await kernel.invoke(code_analyzer, code=sample_code)
        return result
python  theme={null}
from langsmith.integrations.otel import configure
from openinference.instrumentation.openai import OpenAIInstrumentor
from openinference.instrumentation.dspy import DSPyInstrumentor

**Examples:**

Example 1 (unknown):
```unknown
### Combining with other instrumentors

You can combine Semantic Kernel instrumentation with other instrumentors (e.g., DSPy, AutoGen) by adding them and initializing them as instrumentors:
```

---

## get the latest state snapshot

**URL:** llms-txt#get-the-latest-state-snapshot

config = {"configurable": {"thread_id": "1"}}
graph.get_state(config)

---

## get the "memory" by ID

**URL:** llms-txt#get-the-"memory"-by-id

item = store.get(namespace, "a-memory") # [!code highlight]

---

## Get the tool call

**URL:** llms-txt#get-the-tool-call

**Contents:**
- Prompt chaining
- Parallelization
- Routing
- Orchestrator-worker
  - Creating workers in LangGraph

msg.tool_calls
python Graph API theme={null}
  from typing_extensions import TypedDict
  from langgraph.graph import StateGraph, START, END
  from IPython.display import Image, display

# Graph state
  class State(TypedDict):
      topic: str
      joke: str
      improved_joke: str
      final_joke: str

# Nodes
  def generate_joke(state: State):
      """First LLM call to generate initial joke"""

msg = llm.invoke(f"Write a short joke about {state['topic']}")
      return {"joke": msg.content}

def check_punchline(state: State):
      """Gate function to check if the joke has a punchline"""

# Simple check - does the joke contain "?" or "!"
      if "?" in state["joke"] or "!" in state["joke"]:
          return "Pass"
      return "Fail"

def improve_joke(state: State):
      """Second LLM call to improve the joke"""

msg = llm.invoke(f"Make this joke funnier by adding wordplay: {state['joke']}")
      return {"improved_joke": msg.content}

def polish_joke(state: State):
      """Third LLM call for final polish"""
      msg = llm.invoke(f"Add a surprising twist to this joke: {state['improved_joke']}")
      return {"final_joke": msg.content}

# Build workflow
  workflow = StateGraph(State)

# Add nodes
  workflow.add_node("generate_joke", generate_joke)
  workflow.add_node("improve_joke", improve_joke)
  workflow.add_node("polish_joke", polish_joke)

# Add edges to connect nodes
  workflow.add_edge(START, "generate_joke")
  workflow.add_conditional_edges(
      "generate_joke", check_punchline, {"Fail": "improve_joke", "Pass": END}
  )
  workflow.add_edge("improve_joke", "polish_joke")
  workflow.add_edge("polish_joke", END)

# Compile
  chain = workflow.compile()

# Show workflow
  display(Image(chain.get_graph().draw_mermaid_png()))

# Invoke
  state = chain.invoke({"topic": "cats"})
  print("Initial joke:")
  print(state["joke"])
  print("\n--- --- ---\n")
  if "improved_joke" in state:
      print("Improved joke:")
      print(state["improved_joke"])
      print("\n--- --- ---\n")

print("Final joke:")
      print(state["final_joke"])
  else:
      print("Final joke:")
      print(state["joke"])
  python Functional API theme={null}
  from langgraph.func import entrypoint, task

# Tasks
  @task
  def generate_joke(topic: str):
      """First LLM call to generate initial joke"""
      msg = llm.invoke(f"Write a short joke about {topic}")
      return msg.content

def check_punchline(joke: str):
      """Gate function to check if the joke has a punchline"""
      # Simple check - does the joke contain "?" or "!"
      if "?" in joke or "!" in joke:
          return "Fail"

@task
  def improve_joke(joke: str):
      """Second LLM call to improve the joke"""
      msg = llm.invoke(f"Make this joke funnier by adding wordplay: {joke}")
      return msg.content

@task
  def polish_joke(joke: str):
      """Third LLM call for final polish"""
      msg = llm.invoke(f"Add a surprising twist to this joke: {joke}")
      return msg.content

@entrypoint()
  def prompt_chaining_workflow(topic: str):
      original_joke = generate_joke(topic).result()
      if check_punchline(original_joke) == "Pass":
          return original_joke

improved_joke = improve_joke(original_joke).result()
      return polish_joke(improved_joke).result()

# Invoke
  for step in prompt_chaining_workflow.stream("cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Graph API theme={null}
  # Graph state
  class State(TypedDict):
      topic: str
      joke: str
      story: str
      poem: str
      combined_output: str

# Nodes
  def call_llm_1(state: State):
      """First LLM call to generate initial joke"""

msg = llm.invoke(f"Write a joke about {state['topic']}")
      return {"joke": msg.content}

def call_llm_2(state: State):
      """Second LLM call to generate story"""

msg = llm.invoke(f"Write a story about {state['topic']}")
      return {"story": msg.content}

def call_llm_3(state: State):
      """Third LLM call to generate poem"""

msg = llm.invoke(f"Write a poem about {state['topic']}")
      return {"poem": msg.content}

def aggregator(state: State):
      """Combine the joke and story into a single output"""

combined = f"Here's a story, joke, and poem about {state['topic']}!\n\n"
      combined += f"STORY:\n{state['story']}\n\n"
      combined += f"JOKE:\n{state['joke']}\n\n"
      combined += f"POEM:\n{state['poem']}"
      return {"combined_output": combined}

# Build workflow
  parallel_builder = StateGraph(State)

# Add nodes
  parallel_builder.add_node("call_llm_1", call_llm_1)
  parallel_builder.add_node("call_llm_2", call_llm_2)
  parallel_builder.add_node("call_llm_3", call_llm_3)
  parallel_builder.add_node("aggregator", aggregator)

# Add edges to connect nodes
  parallel_builder.add_edge(START, "call_llm_1")
  parallel_builder.add_edge(START, "call_llm_2")
  parallel_builder.add_edge(START, "call_llm_3")
  parallel_builder.add_edge("call_llm_1", "aggregator")
  parallel_builder.add_edge("call_llm_2", "aggregator")
  parallel_builder.add_edge("call_llm_3", "aggregator")
  parallel_builder.add_edge("aggregator", END)
  parallel_workflow = parallel_builder.compile()

# Show workflow
  display(Image(parallel_workflow.get_graph().draw_mermaid_png()))

# Invoke
  state = parallel_workflow.invoke({"topic": "cats"})
  print(state["combined_output"])
  python Functional API theme={null}
  @task
  def call_llm_1(topic: str):
      """First LLM call to generate initial joke"""
      msg = llm.invoke(f"Write a joke about {topic}")
      return msg.content

@task
  def call_llm_2(topic: str):
      """Second LLM call to generate story"""
      msg = llm.invoke(f"Write a story about {topic}")
      return msg.content

@task
  def call_llm_3(topic):
      """Third LLM call to generate poem"""
      msg = llm.invoke(f"Write a poem about {topic}")
      return msg.content

@task
  def aggregator(topic, joke, story, poem):
      """Combine the joke and story into a single output"""

combined = f"Here's a story, joke, and poem about {topic}!\n\n"
      combined += f"STORY:\n{story}\n\n"
      combined += f"JOKE:\n{joke}\n\n"
      combined += f"POEM:\n{poem}"
      return combined

# Build workflow
  @entrypoint()
  def parallel_workflow(topic: str):
      joke_fut = call_llm_1(topic)
      story_fut = call_llm_2(topic)
      poem_fut = call_llm_3(topic)
      return aggregator(
          topic, joke_fut.result(), story_fut.result(), poem_fut.result()
      ).result()

# Invoke
  for step in parallel_workflow.stream("cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Graph API theme={null}
  from typing_extensions import Literal
  from langchain.messages import HumanMessage, SystemMessage

# Schema for structured output to use as routing logic
  class Route(BaseModel):
      step: Literal["poem", "story", "joke"] = Field(
          None, description="The next step in the routing process"
      )

# Augment the LLM with schema for structured output
  router = llm.with_structured_output(Route)

# State
  class State(TypedDict):
      input: str
      decision: str
      output: str

# Nodes
  def llm_call_1(state: State):
      """Write a story"""

result = llm.invoke(state["input"])
      return {"output": result.content}

def llm_call_2(state: State):
      """Write a joke"""

result = llm.invoke(state["input"])
      return {"output": result.content}

def llm_call_3(state: State):
      """Write a poem"""

result = llm.invoke(state["input"])
      return {"output": result.content}

def llm_call_router(state: State):
      """Route the input to the appropriate node"""

# Run the augmented LLM with structured output to serve as routing logic
      decision = router.invoke(
          [
              SystemMessage(
                  content="Route the input to story, joke, or poem based on the user's request."
              ),
              HumanMessage(content=state["input"]),
          ]
      )

return {"decision": decision.step}

# Conditional edge function to route to the appropriate node
  def route_decision(state: State):
      # Return the node name you want to visit next
      if state["decision"] == "story":
          return "llm_call_1"
      elif state["decision"] == "joke":
          return "llm_call_2"
      elif state["decision"] == "poem":
          return "llm_call_3"

# Build workflow
  router_builder = StateGraph(State)

# Add nodes
  router_builder.add_node("llm_call_1", llm_call_1)
  router_builder.add_node("llm_call_2", llm_call_2)
  router_builder.add_node("llm_call_3", llm_call_3)
  router_builder.add_node("llm_call_router", llm_call_router)

# Add edges to connect nodes
  router_builder.add_edge(START, "llm_call_router")
  router_builder.add_conditional_edges(
      "llm_call_router",
      route_decision,
      {  # Name returned by route_decision : Name of next node to visit
          "llm_call_1": "llm_call_1",
          "llm_call_2": "llm_call_2",
          "llm_call_3": "llm_call_3",
      },
  )
  router_builder.add_edge("llm_call_1", END)
  router_builder.add_edge("llm_call_2", END)
  router_builder.add_edge("llm_call_3", END)

# Compile workflow
  router_workflow = router_builder.compile()

# Show the workflow
  display(Image(router_workflow.get_graph().draw_mermaid_png()))

# Invoke
  state = router_workflow.invoke({"input": "Write me a joke about cats"})
  print(state["output"])
  python Functional API theme={null}
  from typing_extensions import Literal
  from pydantic import BaseModel
  from langchain.messages import HumanMessage, SystemMessage

# Schema for structured output to use as routing logic
  class Route(BaseModel):
      step: Literal["poem", "story", "joke"] = Field(
          None, description="The next step in the routing process"
      )

# Augment the LLM with schema for structured output
  router = llm.with_structured_output(Route)

@task
  def llm_call_1(input_: str):
      """Write a story"""
      result = llm.invoke(input_)
      return result.content

@task
  def llm_call_2(input_: str):
      """Write a joke"""
      result = llm.invoke(input_)
      return result.content

@task
  def llm_call_3(input_: str):
      """Write a poem"""
      result = llm.invoke(input_)
      return result.content

def llm_call_router(input_: str):
      """Route the input to the appropriate node"""
      # Run the augmented LLM with structured output to serve as routing logic
      decision = router.invoke(
          [
              SystemMessage(
                  content="Route the input to story, joke, or poem based on the user's request."
              ),
              HumanMessage(content=input_),
          ]
      )
      return decision.step

# Create workflow
  @entrypoint()
  def router_workflow(input_: str):
      next_step = llm_call_router(input_)
      if next_step == "story":
          llm_call = llm_call_1
      elif next_step == "joke":
          llm_call = llm_call_2
      elif next_step == "poem":
          llm_call = llm_call_3

return llm_call(input_).result()

# Invoke
  for step in router_workflow.stream("Write me a joke about cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Graph API theme={null}
  from typing import Annotated, List
  import operator

# Schema for structured output to use in planning
  class Section(BaseModel):
      name: str = Field(
          description="Name for this section of the report.",
      )
      description: str = Field(
          description="Brief overview of the main topics and concepts to be covered in this section.",
      )

class Sections(BaseModel):
      sections: List[Section] = Field(
          description="Sections of the report.",
      )

# Augment the LLM with schema for structured output
  planner = llm.with_structured_output(Sections)
  python Functional API theme={null}
  from typing import List

# Schema for structured output to use in planning
  class Section(BaseModel):
      name: str = Field(
          description="Name for this section of the report.",
      )
      description: str = Field(
          description="Brief overview of the main topics and concepts to be covered in this section.",
      )

class Sections(BaseModel):
      sections: List[Section] = Field(
          description="Sections of the report.",
      )

# Augment the LLM with schema for structured output
  planner = llm.with_structured_output(Sections)

@task
  def orchestrator(topic: str):
      """Orchestrator that generates a plan for the report"""
      # Generate queries
      report_sections = planner.invoke(
          [
              SystemMessage(content="Generate a plan for the report."),
              HumanMessage(content=f"Here is the report topic: {topic}"),
          ]
      )

return report_sections.sections

@task
  def llm_call(section: Section):
      """Worker writes a section of the report"""

# Generate section
      result = llm.invoke(
          [
              SystemMessage(content="Write a report section."),
              HumanMessage(
                  content=f"Here is the section name: {section.name} and description: {section.description}"
              ),
          ]
      )

# Write the updated section to completed sections
      return result.content

@task
  def synthesizer(completed_sections: list[str]):
      """Synthesize full report from sections"""
      final_report = "\n\n---\n\n".join(completed_sections)
      return final_report

@entrypoint()
  def orchestrator_worker(topic: str):
      sections = orchestrator(topic).result()
      section_futures = [llm_call(section) for section in sections]
      final_report = synthesizer(
          [section_fut.result() for section_fut in section_futures]
      ).result()
      return final_report

# Invoke
  report = orchestrator_worker.invoke("Create a report on LLM scaling laws")
  from IPython.display import Markdown
  Markdown(report)
  python  theme={null}
from langgraph.types import Send

**Examples:**

Example 1 (unknown):
```unknown
## Prompt chaining

Prompt chaining is when each LLM call processes the output of the previous call. It's often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:

* Translating documents into different languages
* Verifying generated content for consistency

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=762dec147c31b8dc6ebb0857e236fc1f" alt="Prompt chaining" data-og-width="1412" width="1412" data-og-height="444" height="444" data-path="oss/images/prompt_chain.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=fda27cf4f997e350d4ce48be16049c47 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1374b6de11900d394fc73722a3a6040e 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=25246c7111a87b5df5a2af24a0181efe 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=0c57da86a49cf966cc090497ade347f1 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a1b5c8fc644d7a80c0792b71769c97da 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8a3f66f0e365e503a85b30be48bc1a76 2500w" />

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Parallelization

With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:

* Split up subtasks and run them in parallel, which increases speed
* Run tasks multiple times to check for different outputs, which increases confidence

Some examples include:

* Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors
* Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8afe3c427d8cede6fed1e4b2a5107b71" alt="parallelization.png" data-og-width="1020" width="1020" data-og-height="684" height="684" data-path="oss/images/parallelization.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=88e51062b14d9186a6f0ea246bc48635 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=934941ca52019b7cbce7fbdd31d00f0f 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=30b5c86c545d0e34878ff0a2c367dd0a 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6227d2c39f332eaeda23f7db66871dd7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=283f3ee2924a385ab88f2cbfd9c9c48c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=69f6a97716b38998b7b399c3d8ac7d9c 2500w" />

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get Thread

**URL:** llms-txt#get-thread

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/get-thread

langsmith/agent-server-openapi.json get /threads/{thread_id}
Get a thread by ID.

---

## Get Thread History

**URL:** llms-txt#get-thread-history

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/get-thread-history

langsmith/agent-server-openapi.json get /threads/{thread_id}/history
Get all past states for a thread.

---

## Get Thread History Post

**URL:** llms-txt#get-thread-history-post

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/get-thread-history-post

langsmith/agent-server-openapi.json post /threads/{thread_id}/history
Get all past states for a thread.

---

## Get Thread State

**URL:** llms-txt#get-thread-state

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/get-thread-state

langsmith/agent-server-openapi.json get /threads/{thread_id}/state
Get state for a thread.

The latest state of the thread (i.e. latest checkpoint) is returned.

---

## Get Thread State At Checkpoint

**URL:** llms-txt#get-thread-state-at-checkpoint

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/get-thread-state-at-checkpoint-1

langsmith/agent-server-openapi.json post /threads/{thread_id}/state/checkpoint
Get state for a thread at a specific checkpoint.

---

## Google

**URL:** llms-txt#google

**Contents:**
- Google Generative AI
  - Chat models
  - LLMs
  - Embedding models
- Google Cloud
  - Chat models
  - LLMs
  - Embedding models
  - Document loaders
  - Document transformers

Source: https://docs.langchain.com/oss/python/integrations/providers/google

This page covers all LangChain integrations with [Google Gemini](https://ai.google.dev/gemini-api/docs), [Google Cloud](https://cloud.google.com/), and other Google products (such as Google Maps, YouTube, and [more](#other-google-products)).

<Note>
  **Unified SDK & Package Consolidation**

As of `langchain-google-genai` 4.0.0, this package uses the consolidated [`google-genai`](https://googleapis.github.io/python-genai/) SDK and now supports **both the Gemini Developer API and Vertex AI** backends.

The `langchain-google-vertexai` package remains supported for Vertex AI platform-specific features (Model Garden, Vector Search, evaluation services, etc.).

Read the [full announcement and migration guide](https://github.com/langchain-ai/langchain-google/discussions/1422).
</Note>

Not sure which package to use?

<AccordionGroup>
  <Accordion title="Google Generative AI (Gemini API & Vertex AI)">
    Access Google Gemini models via the **[Gemini Developer API](https://ai.google.dev/)** or **[Vertex AI](https://cloud.google.com/vertex-ai)**. The backend is selected automatically based on your configuration.

* **Gemini Developer API**: Quick setup with API key, ideal for individual developers and rapid prototyping
    * **Vertex AI**: Enterprise features with Google Cloud integration (requires GCP project)

Use the `langchain-google-genai` package for chat models, LLMs, and embeddings.

[See integrations.](#google-generative-ai)
  </Accordion>

<Accordion title="Google Cloud (Vertex AI Platform Services)">
    Access Vertex AI platform-specific services beyond Gemini models: Model Garden (Llama, Mistral, Anthropic), evaluation services, and specialized vision models.

Use the `langchain-google-vertexai` package for platform services and specific packages (e.g., `langchain-google-community`, `langchain-google-cloud-sql-pg`) for other cloud services like databases and storage.

[See integrations.](#google-cloud)
  </Accordion>
</AccordionGroup>

See Google's guide on [migrating from the Gemini API to Vertex AI](https://ai.google.dev/gemini-api/docs/migrate-to-cloud) for more details on the differences.

<Note>
  Integration packages for Gemini models and the Vertex AI platform are maintained in the [`langchain-google`](https://github.com/langchain-ai/langchain-google) repository.

You can find a host of LangChain integrations with other Google APIs and services in the `langchain-google-community` package (listed on this page) and the [`googleapis`](https://github.com/orgs/googleapis/repositories?q=langchain) GitHub organization.
</Note>

## Google Generative AI

Access Google Gemini models via the [Gemini Developer API](https://ai.google.dev/gemini-api/docs) or [Vertex AI](https://cloud.google.com/vertex-ai) using the unified `langchain-google-genai` package.

<Note>
  **Package consolidation**

Certain `langchain-google-vertexai` classes for Gemini models are being deprecated in favor of the unified `langchain-google-genai` package. Please migrate to the new classes.

Read the [full announcement and migration guide](https://github.com/langchain-ai/langchain-google/discussions/1422).
</Note>

<Columns cols={1}>
  <Card title="ChatGoogleGenerativeAI" href="/oss/python/integrations/chat/google_generative_ai" cta="Get started" icon="message" arrow>
    Google Gemini chat models via **Gemini Developer API** or **Vertex AI**.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="GoogleGenerativeAI" href="/oss/python/integrations/llms/google_ai" cta="Get started" icon="i-cursor" arrow>
    Access the same Gemini models (via **Gemini Developer API** or **Vertex AI**) using the (legacy) LLM text completion interface.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="GoogleGenerativeAIEmbeddings" href="/oss/python/integrations/text_embedding/google_generative_ai" cta="Get started" icon="layer-group" arrow>
    Gemini embedding models via **Gemini Developer API** or **Vertex AI**.
  </Card>
</Columns>

Access Vertex AI platform-specific services including Model Garden (Llama, Mistral, Anthropic), Vector Search, evaluation services, and specialized vision models.

<Note>
  **For Gemini models**, use [`ChatGoogleGenerativeAI`](/oss/python/integrations/chat/google_generative_ai) from `langchain-google-genai` instead of `ChatVertexAI`. It supports both Gemini Developer API and Vertex AI backends.

The classes below focus on **Vertex AI platform services** that are *not* available in the consolidated SDK.

Read the [full announcement and migration guide](https://github.com/langchain-ai/langchain-google/discussions/1422).
</Note>

<Columns cols={2}>
  <Card title="ChatVertexAI" icon="comments" href="/oss/python/integrations/chat/google_vertex_ai" cta="Get started" arrow>
    **Deprecated** – Use [`ChatGoogleGenerativeAI`](/oss/python/integrations/chat/google_generative_ai) for Gemini models instead.
  </Card>

<Card title="ChatAnthropicVertex" icon="comments" href="/oss/python/integrations/chat/google_anthropic_vertex" cta="Get started" arrow>
    Anthropic on Vertex AI Model Garden
  </Card>
</Columns>

<AccordionGroup>
  <Accordion title="VertexModelGardenLlama">
    Llama on Vertex AI Model Garden

<Accordion title="VertexModelGardenMistral">
    Mistral on Vertex AI Model Garden

<Accordion title="GemmaChatLocalHF">
    Local Gemma model loaded from HuggingFace.

<Accordion title="GemmaChatLocalKaggle">
    Local Gemma model loaded from Kaggle.

<Accordion title="GemmaChatVertexAIModelGarden">
    Gemma on Vertex AI Model Garden

<Accordion title="VertexAIImageCaptioningChat">
    Implementation of the Image Captioning model as a chat.

<Accordion title="VertexAIImageEditorChat">
    Given an image and a prompt, edit the image. Currently only supports mask-free editing.

<Accordion title="VertexAIImageGeneratorChat">
    Generates an image from a prompt.

<Accordion title="VertexAIVisualQnAChat">
    Chat implementation of a visual QnA model.

</Accordion>
</AccordionGroup>

(legacy) string-in, string-out LLM interface.

<Columns cols={2}>
  <Card title="VertexAIModelGarden" icon="i-cursor" href="/oss/python/integrations/llms/google_vertex_ai#vertex-model-garden" cta="Get started" arrow>
    Access Gemini, and hundreds of OSS models via Vertex AI Model Garden service.
  </Card>

<Card title="VertexAI" icon="i-cursor" href="/oss/python/integrations/llms/google_vertex_ai" cta="Get started" arrow>
    **Deprecated** – Use [`GoogleGenerativeAI`](/oss/python/integrations/llms/google_generative_ai) for Gemini models instead.
  </Card>
</Columns>

<AccordionGroup>
  <Accordion title="Gemma local from Hugging Face">
    Local Gemma model loaded from HuggingFace.

<Accordion title="Gemma local from Kaggle">
    Local Gemma model loaded from Kaggle.

<Accordion title="Gemma on Vertex AI Model Garden">
    
  </Accordion>

<Accordion title="Vertex AI image captioning">
    Implementation of the Image Captioning model as an LLM.

</Accordion>
</AccordionGroup>

<Columns cols={2}>
  <Card title="VertexAIEmbeddings" icon="layer-group" href="/oss/python/integrations/text_embedding/google_vertex_ai" cta="Get started" arrow>
    **Deprecated** – Use [`GenerativeAIEmbeddings`](/oss/python/integrations/text_embedding/google_generative_ai) instead.
  </Card>
</Columns>

Load documents from various Google Cloud sources.

<Columns cols={2}>
  <Card title="AlloyDB for PostgreSQL" href="/oss/python/integrations/document_loaders/google_alloydb" cta="Get started" arrow>
    Google Cloud AlloyDB is a fully managed PostgreSQL-compatible database service.
  </Card>

<Card title="BigQuery" href="/oss/python/integrations/document_loaders/google_bigquery" cta="Get started" arrow>
    Google Cloud BigQuery is a serverless data warehouse.
  </Card>

<Card title="Bigtable" href="/oss/python/integrations/document_loaders/google_bigtable" cta="Get started" arrow>
    Google Cloud Bigtable is a fully managed NoSQL Big Data database service.
  </Card>

<Card title="Cloud SQL for MySQL" href="/oss/python/integrations/document_loaders/google_cloud_sql_mysql" cta="Get started" arrow>
    Google Cloud SQL for MySQL is a fully-managed MySQL database service.
  </Card>

<Card title="Cloud SQL for SQL Server" href="/oss/python/integrations/document_loaders/google_cloud_sql_mssql" cta="Get started" arrow>
    Google Cloud SQL for SQL Server is a fully-managed SQL Server database service.
  </Card>

<Card title="Cloud SQL for PostgreSQL" href="/oss/python/integrations/document_loaders/google_cloud_sql_pg" cta="Get started" arrow>
    Google Cloud SQL for PostgreSQL is a fully-managed PostgreSQL database service.
  </Card>

<Card title="Cloud Storage (directory)" href="/oss/python/integrations/document_loaders/google_cloud_storage_directory" cta="Get started" arrow>
    Google Cloud Storage is a managed service for storing unstructured data.
  </Card>

<Card title="Cloud Storage (file)" href="/oss/python/integrations/document_loaders/google_cloud_storage_file" cta="Get started" arrow>
    Google Cloud Storage is a managed service for storing unstructured data.
  </Card>

<Card title="El Carro for Oracle Workloads" href="/oss/python/integrations/document_loaders/google_el_carro" cta="Get started" arrow>
    Google El Carro Oracle Operator runs Oracle databases in Kubernetes.
  </Card>

<Card title="Firestore (Native Mode)" href="/oss/python/integrations/document_loaders/google_firestore" cta="Get started" arrow>
    Google Cloud Firestore is a NoSQL document database.
  </Card>

<Card title="Firestore (Datastore Mode)" href="/oss/python/integrations/document_loaders/google_datastore" cta="Get started" arrow>
    Google Cloud Firestore in Datastore mode
  </Card>

<Card title="Memorystore for Redis" href="/oss/python/integrations/document_loaders/google_memorystore_redis" cta="Get started" arrow>
    Google Cloud Memorystore for Redis is a fully managed Redis service.
  </Card>

<Card title="Spanner" href="/oss/python/integrations/document_loaders/google_spanner" cta="Get started" arrow>
    Google Cloud Spanner is a fully managed, globally distributed relational database service.
  </Card>

<Card title="Speech-to-Text" href="/oss/python/integrations/document_loaders/google_speech_to_text" cta="Get started" arrow>
    Google Cloud Speech-to-Text transcribes audio files.
  </Card>
</Columns>

<Card title="Cloud Vision loader">
  Load data using Google Cloud Vision API.

### Document transformers

Transform documents using Google Cloud services.

<Columns cols={2}>
  <Card title="Document AI" href="/oss/python/integrations/document_transformers/google_docai" cta="Get started" arrow>
    Transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume.
  </Card>

<Card title="Google Translate" href="/oss/python/integrations/document_transformers/google_translate" cta="Get started" arrow>
    Translate text and HTML with the Google Cloud Translation API.
  </Card>
</Columns>

Store and search vectors using Google Cloud databases and Vertex AI Vector Search.

<Columns cols={2}>
  <Card title="AlloyDB for PostgreSQL" href="/oss/python/integrations/vectorstores/google_alloydb" cta="Get started" arrow>
    Google Cloud AlloyDB is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.
  </Card>

<Card title="BigQuery Vector Search" href="/oss/python/integrations/vectorstores/google_bigquery_vector_search" cta="Get started" arrow>
    BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.
  </Card>

<Card title="Memorystore for Redis" href="/oss/python/integrations/vectorstores/google_memorystore_redis" cta="Get started" arrow>
    Vector store using Memorystore for Redis
  </Card>

<Card title="Spanner" href="/oss/python/integrations/vectorstores/google_spanner" cta="Get started" arrow>
    Vector store using Cloud Spanner
  </Card>

<Card title="Firestore (Native Mode)" href="/oss/python/integrations/vectorstores/google_firestore" cta="Get started" arrow>
    Vector store using Firestore
  </Card>

<Card title="Cloud SQL for MySQL" href="/oss/python/integrations/vectorstores/google_cloud_sql_mysql" cta="Get started" arrow>
    Vector store using Cloud SQL for MySQL
  </Card>

<Card title="Cloud SQL for PostgreSQL" href="/oss/python/integrations/vectorstores/google_cloud_sql_pg" cta="Get started" arrow>
    Vector store using Cloud SQL for PostgreSQL.
  </Card>

<Card title="Vertex AI Vector Search" href="/oss/python/integrations/vectorstores/google_vertex_ai_vector_search" cta="Get started" arrow>
    Formerly known as Vertex AI Matching Engine, provides a low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.
  </Card>

<Card title="With DataStore Backend" href="/oss/python/integrations/vectorstores/google_vertex_ai_vector_search/#optional--you-can-also-create-vectore-and-store-chunks-in-a-datastore" cta="Get started" arrow>
    Vector search using Datastore for document storage.
  </Card>
</Columns>

Retrieve information using Google Cloud services.

<Columns cols={2}>
  <Card title="Vertex AI Search" icon="magnifying-glass" href="/oss/python/integrations/retrievers/google_vertex_ai_search" cta="Get started" arrow>
    Build generative AI powered search engines using Vertex AI Search
  </Card>

<Card title="Document AI Warehouse" icon="warehouse" href="https://cloud.google.com/document-ai-warehouse" cta="Get started" arrow>
    Search, store, and manage documents using Document AI Warehouse.
  </Card>
</Columns>

Integrate agents with various Google Cloud services.

<Columns cols={2}>
  <Card title="Text-to-Speech" icon="volume-high" href="/oss/python/integrations/tools/google_cloud_texttospeech" cta="Get started" arrow>
    Google Cloud Text-to-Speech synthesizes natural-sounding speech with 100+ voices in multiple languages.
  </Card>
</Columns>

Track LLM/Chat model usage.

<AccordionGroup>
  <Accordion title="Vertex AI callback handler">
    Callback Handler that tracks `VertexAI` usage info.

</Accordion>
</AccordionGroup>

Evaluate model outputs using Vertex AI.

<AccordionGroup>
  <Accordion title="VertexPairWiseStringEvaluator">
    Pair-wise evaluation using Vertex AI models.

<Accordion title="VertexStringEvaluator">
    Evaluate a single prediction string using Vertex AI models.

</Accordion>
</AccordionGroup>

## Other Google products

Integrations with various Google services beyond the core Cloud Platform.

<Columns cols={1}>
  <Card title="Google Drive" href="/oss/python/integrations/document_loaders/google_drive" cta="Get started" arrow>
    Google Drive file storage. Currently supports Google Docs.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="ScaNN (Local Index)" href="/oss/python/integrations/vectorstores/google_scann" cta="Get started" arrow>
    ScaNN is a method for efficient vector similarity search at scale.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="Google Drive" href="/oss/python/integrations/retrievers/google_drive" cta="Get started" arrow>
    Retrieve documents from Google Drive.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="Google Search" href="/oss/python/integrations/tools/google_search" cta="Get started" arrow>
    Perform web searches using Google Custom Search Engine (CSE).
  </Card>

<Card title="Google Drive" href="/oss/python/integrations/tools/google_drive" cta="Get started" arrow>
    Tools for interacting with Google Drive.
  </Card>

<Card title="Google Finance" href="/oss/python/integrations/tools/google_finance" cta="Get started" arrow>
    Query financial data.
  </Card>

<Card title="Google Jobs" href="/oss/python/integrations/tools/google_jobs" cta="Get started" arrow>
    Query job listings.
  </Card>

<Card title="Google Lens" href="/oss/python/integrations/tools/google_lens" cta="Get started" arrow>
    Perform visual searches.
  </Card>

<Card title="Google Places" href="/oss/python/integrations/tools/google_places" cta="Get started" arrow>
    Search for places information.
  </Card>

<Card title="Google Scholar" href="/oss/python/integrations/tools/google_scholar" cta="Get started" arrow>
    Search academic papers.
  </Card>

<Card title="Google Trends" href="/oss/python/integrations/tools/google_trends" cta="Get started" arrow>
    Query Google Trends data.
  </Card>
</Columns>

<Columns cols={1}>
  <Card title="MCP Toolbox" href="/oss/python/integrations/tools/mcp_toolbox" cta="Get started" arrow>
    Simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB
  </Card>
</Columns>

Collections of tools for specific Google services.

<Columns cols={2}>
  <Card title="Gmail" icon="envelope" href="/oss/python/integrations/tools/google_gmail" cta="Get started" arrow>
    Toolkit to create, get, search, and send emails using the Gmail API.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="Gmail" icon="envelope" href="/oss/python/integrations/chat_loaders/google_gmail" cta="Get started" arrow>
    Load chat history from Gmail threads.
  </Card>
</Columns>

## 3rd party integrations

Access Google services via unofficial third-party APIs.

<Columns cols={2}>
  <Card title="SearchApi" icon="magnifying-glass" href="/oss/python/integrations/tools/searchapi" cta="Get started" arrow>
    searchapi.io provides API access to Google search results, YouTube, and more.
  </Card>

<Card title="SerpApi" icon="magnifying-glass" href="/oss/python/integrations/tools/serpapi" cta="Get started" arrow>
    SerpApi provides API access to Google search results.
  </Card>

<Card title="Serper.dev" icon="magnifying-glass" href="/oss/python/integrations/tools/google_serper" cta="Get started" arrow>
    serper.dev provides API access to Google search results.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="Search tool" icon="youtube" href="/oss/python/integrations/tools/youtube" cta="Get started" arrow>
    Search YouTube videos without the official API.
  </Card>

<Card title="Audio loader" icon="youtube" href="/oss/python/integrations/document_loaders/youtube_audio" cta="Get started" arrow>
    Download audio from YouTube videos.
  </Card>

<Card title="Transcripts loader" icon="youtube" href="/oss/python/integrations/document_loaders/youtube_transcript" cta="Get started" arrow>
    Load video transcripts.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/google.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

  <Accordion title="VertexModelGardenMistral">
    Mistral on Vertex AI Model Garden
```

Example 2 (unknown):
```unknown
</Accordion>

  <Accordion title="GemmaChatLocalHF">
    Local Gemma model loaded from HuggingFace.
```

Example 3 (unknown):
```unknown
</Accordion>

  <Accordion title="GemmaChatLocalKaggle">
    Local Gemma model loaded from Kaggle.
```

Example 4 (unknown):
```unknown
</Accordion>

  <Accordion title="GemmaChatVertexAIModelGarden">
    Gemma on Vertex AI Model Garden
```

---

## Graph node for executing the refund.

**URL:** llms-txt#graph-node-for-executing-the-refund.

---

## Graph node for extracting user info and routing to lookup/refund/END.

**URL:** llms-txt#graph-node-for-extracting-user-info-and-routing-to-lookup/refund/end.

async def gather_info(state: State) -> Command[Literal["lookup", "refund", END]]:
    info = await info_llm.ainvoke(
        [
            {"role": "system", "content": gather_info_instructions},
            *state["messages"],
        ]
    )
    parsed = info["parsed"]
    if any(parsed[k] for k in ("invoice_id", "invoice_line_ids")):
        goto = "refund"
    elif all(
        parsed[k]
        for k in ("customer_first_name", "customer_last_name", "customer_phone")
    ):
        goto = "lookup"
    else:
        goto = END
    update = {"messages": [info["raw"]], **parsed}
    return Command(update=update, goto=goto)

---

## Graph node for looking up the users purchases

**URL:** llms-txt#graph-node-for-looking-up-the-users-purchases

def lookup(state: State) -> dict:
    args = (
        state[k]
        for k in (
            "customer_first_name",
            "customer_last_name",
            "customer_phone",
            "track_name",
            "album_title",
            "artist_name",
            "purchase_date_iso_8601",
        )
    )
    results = _lookup(*args)
    if not results:
        response = "We did not find any purchases associated with the information you've provided. Are you sure you've entered all of your information correctly?"
        followup = response
    else:
        response = f"Which of the following purchases would you like to be refunded for?\n\n"
        followup = f"Which of the following purchases would you like to be refunded for?\n\n{tabulate(results, headers='keys')}"
    return {
        "messages": [{"role": "assistant", "content": response}],
        "followup": followup,
        "invoice_line_ids": [res["invoice_line_id"] for res in results],
    }

---

## {'graph_output': 'My name is Lance'}

**URL:** llms-txt#{'graph_output':-'my-name-is-lance'}

**Contents:**
  - Reducers
  - Working with Messages in Graph State

python  theme={null}
   StateGraph(
       OverallState,
       input_schema=InputState,
       output_schema=OutputState
   )
   python Example A theme={null}
from typing_extensions import TypedDict

class State(TypedDict):
    foo: int
    bar: list[str]
python Example B theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from operator import add

class State(TypedDict):
    foo: int
    bar: Annotated[list[str], add]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
There are two subtle and important points to note here:

1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node *can write to any state channel in the graph state.* The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.

2. We initialize the graph with:
```

Example 2 (unknown):
```unknown
So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization?

   We can do this because `_nodes` can also declare additional state `channels_` as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.

### Reducers

Reducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:

#### Default Reducer

These two examples show how to use the default reducer:
```

Example 3 (unknown):
```unknown
In this example, no reducer functions are specified for any key. Let's assume the input to the graph is:

`{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["bye"]}`
```

Example 4 (unknown):
```unknown
In this example, we've used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{"foo": 1, "bar": ["hi"]}`. Let's then assume the first `Node` returns `{"foo": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{"foo": 2, "bar": ["hi"]}`. If the second node returns `{"bar": ["bye"]}` then the `State` would then be `{"foo": 2, "bar": ["hi", "bye"]}`. Notice here that the `bar` key is updated by adding the two lists together.

#### Overwrite

<Tip>
  In some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the [`Overwrite`](https://reference.langchain.com/python/langgraph/types/) type for this purpose. [Learn how to use `Overwrite` here](/oss/python/langgraph/use-graph-api#bypass-reducers-with-overwrite).
</Tip>

### Working with Messages in Graph State

#### Why use messages?

Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [chat model interface](/oss/python/langchain/models) in particular accepts a list of message objects as inputs. These messages come in a variety of forms such as [`HumanMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.HumanMessage) (user input) or [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) (LLM response).

To read more about what message objects are, please refer to the [Messages conceptual guide](/oss/python/langchain/messages).

#### Using Messages in your Graph

In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer.

However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.

#### Serialization

In addition to keeping track of message IDs, the [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel.

See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format:
```

---

## Graph state

**URL:** llms-txt#graph-state

class State(TypedDict):
    topic: str  # Report topic
    sections: list[Section]  # List of report sections
    completed_sections: Annotated[
        list, operator.add
    ]  # All workers write to this key in parallel
    final_report: str  # Final report

---

## Graph state.

**URL:** llms-txt#graph-state.

class State(TypedDict):
    """Agent state."""
    messages: Annotated[list[AnyMessage], add_messages]
    followup: str | None

invoice_id: int | None
    invoice_line_ids: list[int] | None
    customer_first_name: str | None
    customer_last_name: str | None
    customer_phone: str | None
    track_name: str | None
    album_title: str | None
    artist_name: str | None
    purchase_date_iso_8601: str | None

---

## Health Check

**URL:** llms-txt#health-check

Source: https://docs.langchain.com/langsmith/agent-server-api/system/health-check

langsmith/agent-server-openapi.json get /ok
Check the health status of the server. Optionally check database connectivity.

---

## Helper function to load files as bytes

**URL:** llms-txt#helper-function-to-load-files-as-bytes

def load_file(file_path: str) -> bytes:
    with open(file_path, "rb") as f:
        return f.read()

---

## Here is the user info for user with ID "abc123":

**URL:** llms-txt#here-is-the-user-info-for-user-with-id-"abc123":

---

## (Here, we demonstrate manually creating the messages for brevity)

**URL:** llms-txt#(here,-we-demonstrate-manually-creating-the-messages-for-brevity)

ai_message = AIMessage(
    content=[],
    tool_calls=[{
        "name": "get_weather",
        "args": {"location": "San Francisco"},
        "id": "call_123"
    }]
)

---

## How to add TTLs to your application

**URL:** llms-txt#how-to-add-ttls-to-your-application

**Contents:**
- Configuring checkpoint TTL
- Configuring store item TTL
- Combining TTL configurations
- Configure per-thread TTL
- Runtime overrides
- Deployment process

Source: https://docs.langchain.com/langsmith/configure-ttl

<Tip>
  **Prerequisites**
  This guide assumes familiarity with [LangSmith](/langsmith/home), [Persistence](/oss/python/langgraph/persistence), and [Cross-thread persistence](/oss/python/langgraph/persistence#memory-store) concepts.
</Tip>

LangSmith persists both [checkpoints](/oss/python/langgraph/persistence#checkpoints) (thread state) and [cross-thread memories](/oss/python/langgraph/persistence#memory-store) (store items). Configure Time-to-Live (TTL) policies in `langgraph.json` to automatically manage the lifecycle of this data, preventing indefinite accumulation.

## Configuring checkpoint TTL

Checkpoints capture the state of conversation threads. Setting a TTL ensures old checkpoints and threads are automatically deleted.

Add a `checkpointer.ttl` configuration to your `langgraph.json` file:

* `strategy`: Specifies the action taken on expiration. Currently, only `"delete"` is supported, which deletes all checkpoints in the thread upon expiration.
* `sweep_interval_minutes`: Defines how often, in minutes, the system checks for expired checkpoints.
* `default_ttl`: Sets the default lifespan of threads (and corresponding checkpoints) in minutes (e.g., 43200 minutes = 30 days). Applies only to checkpoints created after this configuration is deployed; existing checkpoints/threads are not changed. To clear older data, delete it explicitly.

## Configuring store item TTL

Store items allow cross-thread data persistence. Configuring TTL for store items helps manage memory by removing stale data.

Add a `store.ttl` configuration to your `langgraph.json` file:

* `refresh_on_read`: (Optional, default `true`) If `true`, accessing an item via `get` or `search` resets its expiration timer. If `false`, TTL only refreshes on `put`.
* `sweep_interval_minutes`: (Optional) Defines how often, in minutes, the system checks for expired items. If omitted, no sweeping occurs.
* `default_ttl`: (Optional) Sets the default lifespan of store items in minutes (e.g., 10080 minutes = 7 days). Applies only to items created after this configuration is deployed; existing items are not changed. If you need to clear older items, delete them manually. If omitted, items do not expire by default.

## Combining TTL configurations

You can configure TTLs for both checkpoints and store items in the same `langgraph.json` file to set different policies for each data type. Here is an example:

## Configure per-thread TTL

You can apply [TTL configurations per-thread](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.create).

The default `store.ttl` settings from `langgraph.json` can be overridden at runtime by providing specific TTL values in SDK method calls like `get`, `put`, and `search`.

## Deployment process

After configuring TTLs in `langgraph.json`, deploy or restart your LangGraph application for the changes to take effect. Use `langgraph dev` for local development or `langgraph up` for Docker deployment.

See the [langgraph.json CLI reference](https://langchain-ai.github.io/langgraph/reference/configuration/#configuration-file) for more details on the other configurable options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configure-ttl.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
* `strategy`: Specifies the action taken on expiration. Currently, only `"delete"` is supported, which deletes all checkpoints in the thread upon expiration.
* `sweep_interval_minutes`: Defines how often, in minutes, the system checks for expired checkpoints.
* `default_ttl`: Sets the default lifespan of threads (and corresponding checkpoints) in minutes (e.g., 43200 minutes = 30 days). Applies only to checkpoints created after this configuration is deployed; existing checkpoints/threads are not changed. To clear older data, delete it explicitly.

## Configuring store item TTL

Store items allow cross-thread data persistence. Configuring TTL for store items helps manage memory by removing stale data.

Add a `store.ttl` configuration to your `langgraph.json` file:
```

Example 2 (unknown):
```unknown
* `refresh_on_read`: (Optional, default `true`) If `true`, accessing an item via `get` or `search` resets its expiration timer. If `false`, TTL only refreshes on `put`.
* `sweep_interval_minutes`: (Optional) Defines how often, in minutes, the system checks for expired items. If omitted, no sweeping occurs.
* `default_ttl`: (Optional) Sets the default lifespan of store items in minutes (e.g., 10080 minutes = 7 days). Applies only to items created after this configuration is deployed; existing items are not changed. If you need to clear older items, delete them manually. If omitted, items do not expire by default.

## Combining TTL configurations

You can configure TTLs for both checkpoints and store items in the same `langgraph.json` file to set different policies for each data type. Here is an example:
```

Example 3 (unknown):
```unknown
## Configure per-thread TTL

You can apply [TTL configurations per-thread](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.create).
```

---

## How to audit evaluator scores

**URL:** llms-txt#how-to-audit-evaluator-scores

**Contents:**
- In the comparison view
- In the runs table
- In the SDK

Source: https://docs.langchain.com/langsmith/audit-evaluator-scores

LLM-as-a-judge evaluators don't always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK.

## In the comparison view

In the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the "edit" icon on the right to bring up the corrections view. You may then type in your desired score in the text box under "Make correction". If you would like, you may also attach an explanation to your correction. This is useful if you are using a [few-shot evaluator](/langsmith/create-few-shot-evaluators) and will be automatically inserted into your few-shot examples in place of the `few_shot_explanation` prompt variable.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5b815b771c18f291a9ef1b7defb9feb3" alt="Audit Evaluator Comparison View" data-og-width="3426" width="3426" data-og-height="1878" height="1878" data-path="langsmith/images/corrections-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4840ceb8c340713fef6a7999c5d9c6cb 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=08b128d085701f17e20fdc6d314253a8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7d6300071894c9ff3f1fc80c6954c13d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ffe472be6a4b33d741782a1bc3269c60 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=454422187b095a4ad0ec3ad9074d4301 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-comparison-view.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=93b0585577eb98e6d76db3cba6868473 2500w" />

In the runs table, find the "Feedback" column and click on the feedback tag to bring up the feedback details. Again, click the "edit" icon on the right to bring up the corrections view.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5e64530681ac9125751af2383b67ba35" alt="Audit Evaluator Runs Table" data-og-width="1734" width="1734" data-og-height="1002" height="1002" data-path="langsmith/images/corrections-runs-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=46a1a8328ad238d876d3b003a7ab836a 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=60183b8a46938ccfe97a694cb941e7e3 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7a2b5008a78d18d283e81eae9a8e23c0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e58f3c26472e5e78209927d662ab72c1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=eeb374392b18fa613e43564269cd8ff8 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/corrections-runs-table.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=117673d7c311c45f0b954313a35efb32 2500w" />

Corrections can be made via the SDK's `update_feedback` function, with the `correction` dict. You must specify a `score` key which corresponds to a number for it to be rendered in the UI.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/audit-evaluator-scores.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## How to compare experiment results

**URL:** llms-txt#how-to-compare-experiment-results

**Contents:**
- Open the comparison view
- Adjust the table display
- View regressions and improvements
- Update baseline experiment and metric
- Open a trace
- Expand detailed view
- View summary charts
- Use experiment metadata as chart labels

Source: https://docs.langchain.com/langsmith/compare-experiment-results

When you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different [*experiments*](/langsmith/evaluation-concepts#experiment).

LangSmith supports a comparison view that lets you hone in on key differences, regressions, and improvements between different experiments.

## Open the comparison view

1. To access the experiment comaprison view, navigate to the **Datasets & Experiments** page.
2. Select a dataset, which will open the **Experiments** tab.
3. Select two or more experiments abd then click **Compare**.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67d4d6068012e92b101f900595734977" alt="The Experiments view in the UI with 3 experiments selected and the Compare button highlighted." data-og-width="1626" width="1626" data-og-height="966" height="966" data-path="langsmith/images/compare-select.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8e520c4ec316531d45a9f538c3f36f78 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0aed7d0b1cb5d70321ea536ce1decea9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=71ed0d1faed09fe9bd845e8049327948 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b8ccc3f4aa28630b633021b18eb64dd0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6e8b473e44e88e18aa743d1f75b9f6b5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-select.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c0784778d3b730d9e8b34400c852462b 2500w" />

## Adjust the table display

You can toggle between different views by clicking **Full** or **Compact** at the top of the **Comparing Experiments** page.

Toggling **Full** will show the full text of the input, output, and reference output for each run. If the reference output is too long to display in the table, you can click on **Expand detailed view** to view the full content.

You can also select and hide individual feedback keys or individual metrics in the **Display** settings dropdown to isolate the information you need in the comparison view.

## View regressions and improvements

In the comparison view, runs that *regressed* on your specified feedback key against your baseline experiment will be highlighted in red, while runs that *improved* will be highlighted in green. At the top of each column, you can find how many runs in that experiment did better and how many did worse than your baseline experiment.

Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=14f3a9b65dec55c9e4a0f5688d9e8f43" alt="The comparison view comparing 2 experiments with the regressions and improvements highlighted in red and green respectively." data-og-width="1632" width="1632" data-og-height="739" height="739" data-path="langsmith/images/regression-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c7256046a4a6c5d28f350dd8d26bb7e3 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=78c6a2cb4baa784abeb3336d5bea94c4 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9772371f026939748db446fff60fe19b 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d5639c88a1a0214a0955ffbcda23faf0 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e4f232fbef17ff53192d8ac6db3913e 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/regression-view.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d55c8e280732ba5d9e3866564af29de9 2500w" />

## Update baseline experiment and metric

In order to track regressions, you need to:

1. In the **Baseline** dropdown at the top of the comparison view, select a **Baseline experiment** against which to compare. By default, the newest experiment is selected as the baseline.
2. Select a  **Feedback key** (evaluation metric) you want to focus compare against. One will be assigned by default, but you can adjust as needed.
3. Configure whether a higher score is better for the selected feedback key. This preference will be stored.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=3df57789664fd92aba18ea2f438934bd" alt="The Baseline dropdown highlighted with a selected experiment and feedback key of &#x22;hallucination&#x22;." data-og-width="1627" width="1627" data-og-height="898" height="898" data-path="langsmith/images/select-baseline.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=173b5ead77441998fc19669be3290aff 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=46f3bd52bad1fe8fc11ea269b37b15f9 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a7cf328e494a3b674ad60f8a082d6b44 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae7ec9831c8c36855d085f48a8d8e6b3 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4ef21b8740df8f02f7579239e9a9792a 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-baseline.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c87682dbb3438f9c59a837a740d56979 2500w" />

If the example you're evaluating is from an ingested [run](/langsmith/observability-concepts#runs), you can hover over the output cell and click on the trace icon to open the trace view for that run. This will open up a trace in the side panel.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b5feaa0894a645f4642c7422de937c7d" alt="The View trace icon highlighted from an ingested run." data-og-width="1632" width="1632" data-og-height="662" height="662" data-path="langsmith/images/open-source-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=49a42505f2992ab6caeab6f1c52c8e81 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c05cf153d430c062d42d1bf18ef73d6a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=734ad1f9a66a03615b259305298480f0 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3ad016ed31dc9c43eaa74cdffcc66a5e 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=991be6dc57ad88ef21aaba9c90ea24fa 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/open-source-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c4110db6d5e60b4e8df840c409cb5c38 2500w" />

## Expand detailed view

From any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1ff7f02d5ba6ea89902b4de0e37967e7" alt="An example in the Comparing Experiments view of a expanded view of the repetitions." data-og-width="1643" width="1643" data-og-height="926" height="926" data-path="langsmith/images/expanded-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4803efab7ae4a4363145857941d50055 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=98f67feac8ad53ddb18be06955ab2895 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=628927173d27260eb7f76d82e66ff2d4 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6de1f01362d8658d9f673e91deec77b6 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8f68caed39717af5314beb187c3f542f 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-view.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=00fa65a0580a10973a91d506b7cdbc09 2500w" />

## View summary charts

View summary charts by clicking on the **Charts** tab at the top of the page.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ada485c324964c8df96caa566ad11b1d" alt="The Charts summary page with 8 summary charts for the comparison." data-og-width="1639" width="1639" data-og-height="1147" height="1147" data-path="langsmith/images/charts-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e21f174643b26892cae4280962653263 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=40f98d83faf7c13a05a9b410787c9a75 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6a49ddad4c2c02bc14a0d765dc7dc261 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=8aec566c2fd07b7d6e12a3e639cf6660 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=716edd3ad3626831a88c6db07f0fa566 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/charts-tab.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=08e8ea763e825413a3aff09c061593ca 2500w" />

## Use experiment metadata as chart labels

You can configure the x-axis labels for the charts based on [experiment metadata](/langsmith/filter-experiments-ui#background-add-metadata-to-your-experiments).

Select a metadata key in the **x-axis** dropdown to change the chart labels.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb175478369f9a7a2314e44f6becc9e4" alt="x-axis dropdown highlighted with a list of the metadata attached to the experiment." data-og-width="1637" width="1637" data-og-height="1141" height="1141" data-path="langsmith/images/metadata-in-charts.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b34f45a5f0736b6b83a73a00f94212f6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7084f0126711a63d6f539904f4e9091f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dee53003f1c958d09f839d24cb54eb63 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7a0323fef549e2aef3fcf22295d40d19 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=81c37649a07ffa594bad4fd57cc8fb32 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata-in-charts.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=70d700760e7ba9dd0edce54429d11f95 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/compare-experiment-results.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## How to create and manage datasets programmatically

**URL:** llms-txt#how-to-create-and-manage-datasets-programmatically

**Contents:**
- Create a dataset
  - Create a dataset from list of values
  - Create a dataset from traces
  - Create a dataset from a CSV file
  - Create a dataset from pandas DataFrame (Python only)
- Fetch datasets
  - Query all datasets
  - List datasets by name
  - List datasets by type
- Fetch examples

Source: https://docs.langchain.com/langsmith/manage-datasets-programmatically

You can use the Python and TypeScript SDK to manage datasets programmatically. This includes creating, updating, and deleting datasets, as well as adding examples to them.

### Create a dataset from list of values

The most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example.

Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary.

<Check>
  If you have many examples to create, consider using the `create_examples`/`createExamples` method to create multiple examples in a single request. If creating a single example, you can use the `create_example`/`createExample` method.
</Check>

### Create a dataset from traces

To create datasets from the runs (spans) of your traces, you can use the same approach. For **many** more examples of how to fetch and filter runs, see the [export traces](/langsmith/export-traces) guide. Below is an example:

### Create a dataset from a CSV file

In this section, we will demonstrate how you can create a dataset by uploading a CSV file.

First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.

### Create a dataset from pandas DataFrame (Python only)

The python client offers an additional convenience method to upload a dataset from a pandas dataframe.

You can programmatically fetch datasets from LangSmith using the `list_datasets`/`listDatasets` method in the Python and TypeScript SDKs. Below are some common calls.

<Info>
  Initialize the client before running the below code snippets.
</Info>

### Query all datasets

### List datasets by name

If you want to search by the exact name, you can do the following:

If you want to do a case-invariant substring search, try the following:

### List datasets by type

You can filter datasets by type. Below is an example querying for chat datasets.

You can programmatically fetch examples from LangSmith using the `list_examples`/`listExamples` method in the Python and TypeScript SDKs. Below are some common calls.

<Info>
  Initialize the client before running the below code snippets.
</Info>

### List all examples for a dataset

You can filter by dataset ID:

Or you can filter by dataset name (this must exactly match the dataset name you want to query)

### List examples by id

You can also list multiple examples all by ID.

### List examples by metadata

You can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair. Under the hood, we check to see if the example's metadata contains the key-value pair(s) you specify.

For example, if you have an example with metadata `{"foo": "bar", "baz": "qux"}`, both `{foo: bar}` and `{baz: qux}` would match, as would `{foo: bar, baz: qux}`.

### List examples by structured filter

Similar to how you can use the structured filter query language to [fetch runs](/langsmith/export-traces#use-filter-query-language), you can use it to fetch examples.

<Note>
  This is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.

Additionally, the structured filter query language is only supported for `metadata` fields.
</Note>

You can use the `has` operator to fetch examples with metadata fields that contain specific key/value pairs and the `exists` operator to fetch examples with metadata fields that contain a specific key. Additionally, you can also chain multiple filters together using the `and` operator and negate a filter using the `not` operator.

### Update single example

You can programmatically update examples from LangSmith using the `update_example`/`updateExample` method in the Python and TypeScript SDKs. Below is an example.

### Bulk update examples

You can also programmatically update multiple examples in a single request with the `update_examples`/`updateExamples` method in the Python and TypeScript SDKs. Below is an example.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets-programmatically.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Create a dataset from traces

To create datasets from the runs (spans) of your traces, you can use the same approach. For **many** more examples of how to fetch and filter runs, see the [export traces](/langsmith/export-traces) guide. Below is an example:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Create a dataset from a CSV file

In this section, we will demonstrate how you can create a dataset by uploading a CSV file.

First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.

<CodeGroup>
```

---

## How to customize the Dockerfile

**URL:** llms-txt#how-to-customize-the-dockerfile

Source: https://docs.langchain.com/langsmith/custom-docker

Users can add an array of additional lines to add to the Dockerfile following the import from the parent LangGraph image. In order to do this, you simply need to modify your `langgraph.json` file by passing in the commands you want run to the `dockerfile_lines` key. For example, if we wanted to use `Pillow` in our graph you would need to add the following dependencies:

This would install the system packages required to use Pillow if we were working with `jpeg` or `png` image formats.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-docker.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
{
    "dependencies": ["."],
    "graphs": {
        "openai_agent": "./openai_agent.py:agent",
    },
    "env": "./.env",
    "dockerfile_lines": [
        "RUN apt-get update && apt-get install -y libjpeg-dev zlib1g-dev libpng-dev",
        "RUN pip install Pillow"
    ]
}
```

---

## How to define an LLM-as-a-judge evaluator

**URL:** llms-txt#how-to-define-an-llm-as-a-judge-evaluator

**Contents:**
- SDK
  - Pre-built evaluators
  - Create your own LLM-as-a-judge evaluator

Source: https://docs.langchain.com/langsmith/llm-as-judge

<Info>
  * [LLM-as-a-judge evaluator](/langsmith/evaluation-concepts#llm-as-judge)
</Info>

LLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.

This guide shows you how to define an LLM-as-a-judge evaluator for [offline evaluation](/langsmith/evaluation-concepts#offline-evaluation) using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to [setting up online evaluations](/langsmith/online-evaluations#configure-llm-as-judge-evaluators).

### Pre-built evaluators

Pre-built evaluators are a useful starting point for setting up evaluations. Refer to [pre-built evaluators](/langsmith/prebuilt-evaluators) for how to use pre-built evaluators with LangSmith.

### Create your own LLM-as-a-judge evaluator

For complete control of evaluator logic, create your own LLM-as-a-judge evaluator and run it using the LangSmith SDK ([Python](https://docs.smith.langchain.com/reference/python/reference) / [TypeScript](https://docs.smith.langchain.com/reference/js)).

Requires `langsmith>=0.2.0`

```python  theme={null}
from langsmith import evaluate, traceable, wrappers, Client
from openai import OpenAI

---

## How to define a code evaluator

**URL:** llms-txt#how-to-define-a-code-evaluator

**Contents:**
- Basic example
- Evaluator args
- Evaluator output
- Additional examples
- Related

Source: https://docs.langchain.com/langsmith/code-evaluator

<Info>
  * [Evaluators](/langsmith/evaluation-concepts#evaluators)
</Info>

Code evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate).

code evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).
* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.
* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

When using JS/TS these should all be passed in as part of a single object argument.

Code evaluators are expected to return one of the following types:

* `dict`: dicts of the form `{"score" | "value": ..., "key": ...}` allow you to customize the metric type ("score" for numerical and "value" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.
* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.
* `list[dict]`: return multiple metrics using a single function.

## Additional examples

Requires `langsmith>=0.2.0`

* [Evaluate aggregate experiment results](/langsmith/summary): Define summary evaluators, which compute metrics for an entire experiment.
* [Run an evaluation comparing two experiments](/langsmith/evaluate-pairwise): Define pairwise evaluators, which compute metrics by comparing two (or more) experiments against each other.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/code-evaluator.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Evaluator args

code evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `run: Run`: The full [Run](/langsmith/run-data-format) object generated by the application on the given example.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metdata (if available).
* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: dict`: A dictionary of the outputs generated by the application on the given `inputs`.
* `reference_outputs/referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs`. `run` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

When using JS/TS these should all be passed in as part of a single object argument.

## Evaluator output

Code evaluators are expected to return one of the following types:

Python and JS/TS

* `dict`: dicts of the form `{"score" | "value": ..., "key": ...}` allow you to customize the metric type ("score" for numerical and "value" for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.

Python only

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.
* `str`: this is intepreted as a categorical metric. The function name is used as the name of the metric.
* `list[dict]`: return multiple metrics using a single function.

## Additional examples

Requires `langsmith>=0.2.0`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## How to define a summary evaluator

**URL:** llms-txt#how-to-define-a-summary-evaluator

**Contents:**
- Basic example
- Summary evaluator args
- Summary evaluator output

Source: https://docs.langchain.com/langsmith/summary

Some metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset. These are called summary evaluators.

Here, we'll compute the f1-score, which is a combination of precision and recall.

This sort of metric can only be computed over all of the examples in our experiment, so our evaluator takes in a list of outputs, and a list of reference\_outputs.

You can then pass this evaluator to the `evaluate` method as follows:

In the LangSmith UI, you'll the summary evaluator's score displayed with the corresponding key.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d0f259baa7d7467bf172ef8197c3bb17" alt="summary_eval.png" data-og-width="1535" width="1535" data-og-height="122" height="122" data-path="langsmith/images/summary-eval.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=076d830ea3952a4a598d25a2830232e0 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a5f96c7cf258a92be14f489bc1a05f8c 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4a2a77c6e0ae855a7027888591733e13 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=62eee3f7f104ae12e97ba22828a8bb2c 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=48d6045a20e1021aceadc98554e39e9e 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/summary-eval.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d72ea1cfc84b0cb33461eb36ea47c696 2500w" />

## Summary evaluator args

Summary evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `inputs: list[dict]`: A list of the inputs corresponding to a single example in a dataset.
* `outputs: list[dict]`: A list of the dict outputs produced by each experiment on the given inputs.
* `reference_outputs/referenceOutputs: list[dict]`: A list of the reference outputs associated with the example, if available.
* `runs: list[Run]`: A list of the full [Run](/langsmith/run-data-format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
* `examples: list[Example]`: All of the dataset [Example](/langsmith/example-data-format) objects, including the example inputs, outputs (if available), and metdata (if available).

## Summary evaluator output

Summary evaluators are expected to return one of the following types:

* `dict`: dicts of the form `{"score": ..., "name": ...}` allow you to pass a numeric or boolean score and metric name.

Currently Python only

* `int | float | bool`: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/summary.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

You can then pass this evaluator to the `evaluate` method as follows:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## How to define a target function to evaluate

**URL:** llms-txt#how-to-define-a-target-function-to-evaluate

**Contents:**
- Target function signature

Source: https://docs.langchain.com/langsmith/define-target-function

There are three main pieces need to run an evaluation:

1. A [dataset](/langsmith/evaluation-concepts#datasets) of test inputs and expected outputs.
2. A target function which is what you're evaluating.
3. [Evaluators](/langsmith/evaluation-concepts#evaluators) that score your target function's outputs.

This guide shows you how to define the target function depending on the part of your application you are evaluating. See here for [how to create a dataset](/langsmith/manage-datasets-programmatically) and [how to define evaluators](/langsmith/code-evaluator), and here for an [end-to-end example of running an evaluation](/langsmith/evaluate-llm-application).

## Target function signature

In order to evaluate an application in code, we need a way to run the application. When using `evaluate()` ([Python](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.evaluate)/[TypeScript](https://docs.smith.langchain.com/reference/js/functions/evaluation.evaluate))we'll do this by passing in a *target function* argument. This is a function that takes in a dataset [Example's](/langsmith/evaluation-concepts#examples) inputs and returns the application output as a dict. Within this function we can call our application however we'd like. We can also format the output however we'd like. The key is that any evaluator functions we define should work with the output format we return in our target function.

```python  theme={null}
from langsmith import Client

---

## How to evaluate an application's intermediate steps

**URL:** llms-txt#how-to-evaluate-an-application's-intermediate-steps

**Contents:**
- 1. Define your LLM pipeline
- 2. Create a dataset and examples to evaluate the pipeline
- 3. Define your custom evaluators
- 4. Evaluate the pipeline
- Related

Source: https://docs.langchain.com/langsmith/evaluate-on-intermediate-steps

While, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline.

For example, for retrieval-augmented generation (RAG), you might want to

1. Evaluate the retrieval step to ensure that the correct documents are retrieved w\.r.t the input query.
2. Evaluate the generation step to ensure that the correct answer is generated w\.r.t the retrieved documents.

In this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios.

In order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the `run`/`rootRun` argument, which is a `Run` object that contains the intermediate steps of your pipeline.

## 1. Define your LLM pipeline

The below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents.

Requires `langsmith>=0.3.13`

This pipeline will produce a trace that looks something like: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3b691ca56f9d60035dcba2c248692fa1" alt="evaluation_intermediate_trace.png" data-og-width="2586" width="2586" data-og-height="1676" height="1676" data-path="langsmith/images/evaluation-intermediate-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=23a9b9abdb3e43e0f6326f0d4293ab7d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4b771691bd1afffe9f371a105f7eaebe 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=beb01776de9a5fa663c82d4380bc78cd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec84bd3345df3d2cef38878b902c355b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=043a7f22da6b158e070c853d67bacd69 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=42a9ea799157fee30a6b243c02615a02 2500w" />

## 2. Create a dataset and examples to evaluate the pipeline

We are building a very simple dataset with a couple of examples to evaluate the pipeline.

Requires `langsmith>=0.3.13`

## 3. Define your custom evaluators

As mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w\.r.t the input query and another that evaluates the hallucination of the generated answer w\.r.t the retrieved documents. We will be using LangChain LLM wrappers, along with [`with_structured_output`](https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/) to define the evaluator for hallucination.

The key here is that the evaluator function should traverse the `run` / `rootRun` argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria.

Example uses `langchain` for convenience, this is not required.

## 4. Evaluate the pipeline

Finally, we'll run `evaluate` with the custom evaluators defined above.

The experiment will contain the results of the evaluation, including the scores and comments from the evaluators: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e926744573c6b9757ba22ff245a3da2c" alt="evaluation_intermediate_experiment.png" data-og-width="2446" width="2446" data-og-height="1244" height="1244" data-path="langsmith/images/evaluation-intermediate-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7b6e321b15a06b2adc7f1cacb8e07a35 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c677007bcc1e2af4b3767d6b44fcb327 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a39153399b6721b7c51693f5a59cf2b0 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1132228eba6761a724ae98d85fcf536c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5d74785384737df0cf67145b397b1934 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-experiment.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bef00e4bdc12289d9f1e4b77ed8489cf 2500w" />

* [Evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-on-intermediate-steps.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Requires `langsmith>=0.3.13`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

This pipeline will produce a trace that looks something like: <img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3b691ca56f9d60035dcba2c248692fa1" alt="evaluation_intermediate_trace.png" data-og-width="2586" width="2586" data-og-height="1676" height="1676" data-path="langsmith/images/evaluation-intermediate-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=23a9b9abdb3e43e0f6326f0d4293ab7d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4b771691bd1afffe9f371a105f7eaebe 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=beb01776de9a5fa663c82d4380bc78cd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ec84bd3345df3d2cef38878b902c355b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=043a7f22da6b158e070c853d67bacd69 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluation-intermediate-trace.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=42a9ea799157fee30a6b243c02615a02 2500w" />

## 2. Create a dataset and examples to evaluate the pipeline

We are building a very simple dataset with a couple of examples to evaluate the pipeline.

Requires `langsmith>=0.3.13`

<CodeGroup>
```

---

## How to evaluate an existing experiment (Python only)

**URL:** llms-txt#how-to-evaluate-an-existing-experiment-(python-only)

Source: https://docs.langchain.com/langsmith/evaluate-existing-experiment

Evaluation of existing experiments is currently only supported in the Python SDK.

If you have already run an experiment and want to add additional evaluation metrics, you can apply any evaluators to the experiment using the `evaluate()` / `aevaluate()` methods as before. Just pass in the experiment name / ID instead of a target function:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-existing-experiment.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## How to evaluate an LLM application

**URL:** llms-txt#how-to-evaluate-an-llm-application

**Contents:**
- Define an application
- Create or select a dataset
- Define an evaluator
- Run the evaluation
- Explore the results[​](#explore-the-results "Direct link to Explore the results")
- Reference code[​](#reference-code "Direct link to Reference code")
- Related[​](#related "Direct link to Related")

Source: https://docs.langchain.com/langsmith/evaluate-llm-application

This guide shows you how to run an evaluation on an LLM application using the LangSmith SDK.

<Info>
  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets)
</Info>

In this guide we'll go over how to evaluate an application using the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) method in the LangSmith SDK.

<Check>
  For larger evaluation jobs in Python we recommend using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), the asynchronous version of [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on [running an evaluation asynchronously](/langsmith/evaluation-async).

In JS/TS evaluate() is already asynchronous so no separate method is needed.

It is also important to configure the `max_concurrency`/`maxConcurrency` arg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.
</Check>

## Define an application

First we need an application to evaluate. Let's create a simple toxicity classifier for this example.

We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to [this guide](/langsmith/annotate-code).

## Create or select a dataset

We need a [Dataset](/langsmith/evaluation-concepts#datasets) to evaluate our application on. Our dataset will contain labeled [examples](/langsmith/evaluation-concepts#examples) of toxic and non-toxic text.

Requires `langsmith>=0.3.13`

For more details on datasets, refer to the [Manage datasets](/langsmith/manage-datasets) page.

## Define an evaluator

<Check>
  You can also check out LangChain's open source evaluation package [openevals](https://github.com/langchain-ai/openevals) for common pre-built evaluators.
</Check>

[Evaluators](/langsmith/evaluation-concepts#evaluators) are functions for scoring your application's outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.

* Python: Requires `langsmith>=0.3.13`
* TypeScript: Requires `langsmith>=0.2.9`

## Run the evaluation

We'll use the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) methods to run the evaluation.

The key arguments are:

* a target function that takes an input dictionary and returns an output dictionary. The `example.inputs` field of each [Example](/langsmith/example-data-format) is what gets passed to the target function. In this case our `toxicity_classifier` is already set up to take in example inputs so we can use it directly.
* `data` - the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examples
* `evaluators` - a list of evaluators to score the outputs of the function

Python: Requires `langsmith>=0.3.13`

## Explore the results[​](#explore-the-results "Direct link to Explore the results")

Each invocation of `evaluate()` creates an [Experiment](/langsmith/evaluation-concepts#experiments) which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback.

*If you've annotated your code for tracing, you can open the trace of each row in a side panel view.*

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-experiment.gif?s=252d96dbd2100a691f1d3b61716fde38" alt="View experiment" data-og-width="1132" width="1132" data-og-height="720" height="720" data-path="langsmith/images/view-experiment.gif" data-optimize="true" data-opv="3" />

## Reference code[​](#reference-code "Direct link to Reference code")

<Accordion title="Click to see a consolidated code snippet">
  <CodeGroup>

</CodeGroup>
</Accordion>

## Related[​](#related "Direct link to Related")

* [Run an evaluation asynchronously](/langsmith/evaluation-async)
* [Run an evaluation via the REST API](/langsmith/run-evals-api-only)
* [Run an evaluation from the prompt playground](/langsmith/run-evaluation-from-prompt-playground)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-llm-application.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

We've optionally enabled tracing to capture the inputs and outputs of each step in the pipeline. To understand how to annotate your code for tracing, please refer to [this guide](/langsmith/annotate-code).

## Create or select a dataset

We need a [Dataset](/langsmith/evaluation-concepts#datasets) to evaluate our application on. Our dataset will contain labeled [examples](/langsmith/evaluation-concepts#examples) of toxic and non-toxic text.

Requires `langsmith>=0.3.13`

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

For more details on datasets, refer to the [Manage datasets](/langsmith/manage-datasets) page.

## Define an evaluator

<Check>
  You can also check out LangChain's open source evaluation package [openevals](https://github.com/langchain-ai/openevals) for common pre-built evaluators.
</Check>

[Evaluators](/langsmith/evaluation-concepts#evaluators) are functions for scoring your application's outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.

* Python: Requires `langsmith>=0.3.13`
* TypeScript: Requires `langsmith>=0.2.9`

<CodeGroup>
```

---

## How to evaluate a graph

**URL:** llms-txt#how-to-evaluate-a-graph

**Contents:**
- End-to-end evaluations
  - Define a graph

Source: https://docs.langchain.com/langsmith/evaluate-graph

<Info>
  [langgraph](https://langchain-ai.github.io/langgraph/)
</Info>

`langgraph` is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Evaluating `langgraph` graphs can be challenging because a single invocation can involve many LLM calls, and which LLM calls are made may depend on the outputs of preceding calls. In this guide we will focus on the mechanics of how to pass graphs and graph nodes to `evaluate()` / `aevaluate()`. For evaluation techniques and best practices when building agents head to the [langgraph docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation).

## End-to-end evaluations

The most common type of evaluation is an end-to-end one, where we want to evaluate the final graph output for each example input.

Lets construct a simple ReACT agent to start:

```python  theme={null}
from typing import Annotated, Literal, TypedDict
from langchain.chat_models import init_chat_model
from langchain.tools import tool
from langgraph.prebuilt import ToolNode
from langgraph.graph import END, START, StateGraph
from langgraph.graph.message import add_messages

class State(TypedDict):
    # Messages have the type "list". The 'add_messages' function
    # in the annotation defines how this state key should be updated
    # (in this case, it appends messages to the list, rather than overwriting them)
    messages: Annotated[list, add_messages]

---

## How to evaluate a runnable

**URL:** llms-txt#how-to-evaluate-a-runnable

**Contents:**
- Setup
- Evaluate
- Related

Source: https://docs.langchain.com/langsmith/langchain-runnable

<Info>
  * `langchain`: [Python](https://python.langchain.com) and [JS/TS](https://js.langchain.com)
  * Runnable: [Python](https://python.langchain.com/docs/concepts/runnables/) and [JS/TS](https://js.langchain.com/docs/concepts/runnables/)
</Info>

`langchain` [Runnable](https://python.langchain.com/docs/concepts/runnables/) objects (such as chat models, retrievers, chains, etc.) can be passed directly into `evaluate()` / `aevaluate()`.

Let's define a simple chain to evaluate. First, install all the required packages:

To evaluate our chain we can pass it directly to the `evaluate()` / `aevaluate()` method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form `{"text": "..."}`.

The runnable is traced appropriately for each output.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b9dac41dafb9a1cbb3b90fc508f212f7" alt="Runnable Evaluation" data-og-width="2288" width="2288" data-og-height="1052" height="1052" data-path="langsmith/images/runnable-eval.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=39f7bda57df5d29c72729390065342c2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2bbfa58f877541adff85056d2d4910c7 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=198967ebb494d0577fac294f879f348c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=dd0758a55517d6899d445bd203bc7d03 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6fa8f6a044a0b978ef727390f18f5ce3 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/runnable-eval.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=40dad8febfdaf0756c90b6326e2c4415 2500w" />

* [How to evaluate a `langgraph` graph](/langsmith/evaluate-on-intermediate-steps)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langchain-runnable.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Now define a chain:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Evaluate

To evaluate our chain we can pass it directly to the `evaluate()` / `aevaluate()` method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form `{"text": "..."}`.

<CodeGroup>
```

---

## How to fetch performance metrics for an experiment

**URL:** llms-txt#how-to-fetch-performance-metrics-for-an-experiment

Source: https://docs.langchain.com/langsmith/fetch-perf-metrics-experiment

<Check>
  Tracing projects and experiments use the same underlying data structure in our backend, which is called a "session."

You might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.

We are working on unifying the terminology across our documentation and APIs.
</Check>

When you run an experiment using `evaluate` with the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the `read_project`/`readProject` methods.

The payload for experiment details includes the following values:

From here, you can extract performance metrics such as:

* `latency_p50`: The 50th percentile latency in seconds.
* `latency_p99`: The 99th percentile latency in seconds.
* `total_tokens`: The total number of tokens used.
* `prompt_tokens`: The number of prompt tokens used.
* `completion_tokens`: The number of completion tokens used.
* `total_cost`: The total cost of the experiment.
* `prompt_cost`: The cost of the prompt tokens.
* `completion_cost`: The cost of the completion tokens.
* `feedback_stats`: The feedback statistics for the experiment.
* `error_rate`: The error rate for the experiment.
* `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).
* `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).

Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.

First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](/langsmith/evaluate-llm-application) on evaluation for more details.

```python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown
From here, you can extract performance metrics such as:

* `latency_p50`: The 50th percentile latency in seconds.
* `latency_p99`: The 99th percentile latency in seconds.
* `total_tokens`: The total number of tokens used.
* `prompt_tokens`: The number of prompt tokens used.
* `completion_tokens`: The number of completion tokens used.
* `total_cost`: The total cost of the experiment.
* `prompt_cost`: The cost of the prompt tokens.
* `completion_cost`: The cost of the completion tokens.
* `feedback_stats`: The feedback statistics for the experiment.
* `error_rate`: The error rate for the experiment.
* `first_token_p50`: The 50th percentile latency for the time to generate the first token (if using streaming).
* `first_token_p99`: The 99th percentile latency for the time to generate the first token (if using streaming).

Here is an example of how you can fetch the performance metrics for an experiment using the Python and TypeScript SDKs.

First, as a prerequisite, we will create a trivial dataset. Here, we only demonstrate this in Python, but you can do the same in TypeScript. Please view the [how-to guide](/langsmith/evaluate-llm-application) on evaluation for more details.
```

---

## How to filter experiments in the UI

**URL:** llms-txt#how-to-filter-experiments-in-the-ui

**Contents:**
- Background: add metadata to your experiments
- Filter experiments in the UI

Source: https://docs.langchain.com/langsmith/filter-experiments-ui

LangSmith lets you filter your previous experiments by feedback scores and metadata to make it easy to find only the experiments you care about.

## Background: add metadata to your experiments

When you run an experiment in the SDK, you can attach metadata to make it easier to filter in UI. This is helpful if you know what axes you want to drill down into when running experiments.

In our example, we are going to attach metadata to our experiment around the model used, the model provider, and a known ID of the prompt:

## Filter experiments in the UI

In the UI, we see all experiments that have been run by default.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0cb5b29f405286dadb8b6491709eb789" alt="Filter all experiments" data-og-width="2900" width="2900" data-og-height="1370" height="1370" data-path="langsmith/images/filter-all-experiments.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=45a66945b4ca67d1dc3c29890b5d5feb 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1d8fdbdf637f750dcef092a49b45e0ae 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7177578edd6e6bcfef63d7a778ded723 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f0c6bdae3afabe2b179a74450adbc508 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8dd72a9e30b093a58819d79f40f69b65 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-all-experiments.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e65610094b48a0f81ac75a49be573382 2500w" />

If we, say, have a preference for openai models, we can easily filter down and see scores within just openai models first:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e3a0c42e4eb37cae68d367dec75d0df1" alt="Filter openai" data-og-width="2910" width="2910" data-og-height="1130" height="1130" data-path="langsmith/images/filter-openai.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=96ef35ffdaf371b2819da5a3f4bdf7aa 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b44c6d30e0e03560d26719facf1dfc0c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a737dceb8e7a2c89b9d0dfc008352afd 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=cd3adf4b2d249bde3646810d3783b931 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7dc39864577acc20bb9229de313cbe33 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-openai.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7d5392a8453b2f416f795834eb02cc6e 2500w" />

We can stack filters, allowing us to filter out low scores on correctness to make sure we only compare relevant experiments:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c0e223bbe637a03a4c251896a5662f52" alt="Filter feedback" data-og-width="2912" width="2912" data-og-height="826" height="826" data-path="langsmith/images/filter-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=476871f1d9569046084b380b3fa50ba3 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c8f24775e8f736a2cfd6b1092106a200 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c014f713cb651fa61d13dc8fe11055e2 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f8c9ad41ba9ba73ee43df8c0c740b53e 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4e8105316812331bf3d865c15028e3e7 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-feedback.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=45c2cd7461fde3670a814ae0915a3584 2500w" />

Finally, we can clear and reset filters. For example, if we see there is clear there's a winner with the `singleminded` prompt, we can change filtering settings to see if any other model providers' models work as well with it:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7a30d214745fcddc022e3c715267dfd2" alt="Filter singleminded" data-og-width="2904" width="2904" data-og-height="832" height="832" data-path="langsmith/images/filter-singleminded.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bcf8d6a876b76137cc5682308f9e2f74 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=4c1e77dc12de1b65183d26877104a384 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3aa8585e58f88e5e7eac344ae386808f 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=613558759a127d7003d3ae653e83298b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2ba32a223d7012904c0eb63fe7c1fdb7 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-singleminded.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=c643c61125d409c706ea607b1504740c 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/filter-experiments-ui.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## How to handle model rate limits

**URL:** llms-txt#how-to-handle-model-rate-limits

**Contents:**
- Using `langchain` RateLimiters (Python only)
- Retrying with exponential backoff
- Limiting `max_concurrency`

Source: https://docs.langchain.com/langsmith/rate-limiting

A common issue when running large evaluation jobs is running into third-party API rate limits, usually from model providers. There are a few ways to deal with rate limits.

## Using `langchain` RateLimiters (Python only)

If you're using `langchain` Python chat models in your application or evaluators, you can add rate limiters to your model(s) that will add client-side control of the frequency with which requests are sent to the model provider API to avoid rate limit errors.

See the [`langchain`](/oss/python/langchain/models#rate-limiting) documentation for more on how to configure rate limiters.

## Retrying with exponential backoff

A very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.

#### With `langchain`

If you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:

See the `langchain` [Python](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.

#### Without `langchain`

If you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).

## Limiting `max_concurrency`

Limiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rate-limiting.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
See the [`langchain`](/oss/python/langchain/models#rate-limiting) documentation for more on how to configure rate limiters.

## Retrying with exponential backoff

A very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.

#### With `langchain`

If you're using `langchain` components you can add retries to all model calls with the `.with_retry(...)` / `.withRetry()` method:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

See the `langchain` [Python](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.BaseChatModel.with_retry) and [JS](https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html#withRetry) API references for more.

#### Without `langchain`

If you're not using `langchain` you can use other libraries like `tenacity` (Python) or `backoff` (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the [OpenAI docs](https://platform.openai.com/docs/guides/rate-limits#retrying-with-exponential-backoff).

## Limiting `max_concurrency`

Limiting the number of concurrent calls you're making to your application and evaluators is another way to decrease the frequency of model calls you're making, and in that way avoid rate limit errors. `max_concurrency` can be set directly on the [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) / [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate) functions. This parallelizes evaluation by effectively splitting the dataset across threads.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How to improve your evaluator with few-shot examples

**URL:** llms-txt#how-to-improve-your-evaluator-with-few-shot-examples

**Contents:**
- How few-shot examples work
- Configure your evaluator
  - 1. Configure variable mapping
  - 2. Specify the number of few-shot examples to use
- Make corrections
- View your corrections dataset

Source: https://docs.langchain.com/langsmith/create-few-shot-evaluators

Using LLM-as-a-judge evaluators can be very helpful when you can't evaluate your system programmatically. However, their effectiveness depends on their quality and how well they align with human reviewer feedback. LangSmith provides the ability to improve the alignment of LLM-as-a-judge evaluator to human preferences using few-shot examples.

Human corrections are automatically inserted into your evaluator prompt using few-shot examples. Few-shot examples is a technique inspired by [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot) that guides the models output with a few high-quality examples.

This guide covers how to set up few-shot examples as part of your LLM-as-a-judge evaluator and apply corrections to feedback scores.

## How few-shot examples work

* Few-shot examples are added to your evaluator prompt using the `{{Few-shot examples}}` variable
* Creating an evaluator with few-shot examples, will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections
* At runtime, these examples will inserted into the evaluator to serve as a guide for its outputs - this will help the evaluator to better align with human preferences

## Configure your evaluator

<Note>
  Few-shot examples are not currently supported in LLM-as-a-judge evaluators that use the prompt hub and are only compatible with prompts that use mustache formatting.
</Note>

Before enabling few-shot examples, set up your LLM-as-a-judge evaluator. If you haven't done this yet, follow the steps in the [LLM-as-a-judge evaluator guide](/langsmith/llm-as-judge).

### 1. Configure variable mapping

Each few-shot example is formatted according to the variable mapping specified in the configuration. The variable mapping for few-shot examples, should contain the same variables as your main prompt, plus a `few_shot_explanation` and a `score` variable which should have the same name as your feedback key.

For example, if your main prompt has variables `question` and `response`, and your evaluator outputs a `correctness` score, then your few-shot prompt should have the vartiables `question`, `response`, `few_shot_explanation`, and `correctness`.

### 2. Specify the number of few-shot examples to use

You may also specify the number of few-shot examples to use. The default is 5. If your examples are very long, you may want to set this number lower to save tokens - whereas if your examples tend to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you.

<Info>
  [Audit evaluator scores](/langsmith/audit-evaluator-scores)
</Info>

As you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you [make corrections to these scores](/langsmith/audit-evaluator-scores), you will begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of the `few_shot_explanation` variable.

The inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset. The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8c7bfcc6cc4ab86c18240c3cbf2ea44c" alt="Few-shot example" data-og-width="1572" width="1572" data-og-height="790" height="790" data-path="langsmith/images/few-shot-example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=91f4e17fd853ba23c1b04934144dfa77 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e0a5e2a026e4166c341900dd49316f35 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=88aec2ef5c37c16c67e0eefecd3fbc0a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bfbbd35cf503ce2f3dbf743fab8fb75b 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bf0765bfeabc8d34ef49626dca0135ae 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/few-shot-example.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f9d2cf9437ee0e160a511903bd88238a 2500w" />

Note that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!

## View your corrections dataset

In order to view your corrections dataset:

* **Online evaluators**: Select your run rule and click **Edit Rule**
* **Offline evaluators**: Select your evaluator and click **Edit Evaluator**

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=03453ef08f1c272d5d9aaf71d1fb7301" alt="Edit Evaluator" data-og-width="800" width="800" data-og-height="284" height="284" data-path="langsmith/images/edit-evaluator.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d495c791e6c8ae9d241085795d4b67b5 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b1ddde8054744862494e4d3f02a460b0 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9838e073d1d7e61c6b79d8f35ba1a1b3 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=67322be4166f4479a466427a9b270ca1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4cd490404d1616498fed810b3ce75a21 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/edit-evaluator.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4c8a87ad332661a0b8b472dd34f1f4ab 2500w" />

Head to your dataset of corrections linked in the the **Improve evaluator accuracy using few-shot examples** section. You can view and update your few-shot examples in the dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3215f3f24a08186fd76c6dbad18a3cf5" alt="View few-shot dataset" data-og-width="1470" width="1470" data-og-height="478" height="478" data-path="langsmith/images/view-few-shot-ds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ad702a532a8f083c71056baff4370f30 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d45ebd4263adc9c10598fad633167ca3 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fe060c8d000a41566949ff35d6c62135 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f735943da46a1e57328b86246f5da25f 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9861b9651d5d63a07662e7aa1bc68491 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-few-shot-ds.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6084c3697ffd582e30301540906a5698 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-few-shot-evaluators.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## How to integrate LangGraph into your React application

**URL:** llms-txt#how-to-integrate-langgraph-into-your-react-application

**Contents:**
- Install the SDK
- Example
- Customizing your UI
  - Loading states
  - Resume a stream after page refresh
  - Thread management
  - Messages handling
  - Accessing full graph state
  - Interrupts
  - Branching

Source: https://docs.langchain.com/langsmith/use-stream-react

<Info>
  **Prerequisites**

* [LangSmith](/langsmith/home)
  * [Agent Server](/langsmith/agent-server)
</Info>

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) React hook provides a seamless way to integrate LangGraph into your React applications. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great chat experiences.

* Messages streaming: Handle a stream of message chunks to form a complete message
* Automatic state management for messages, interrupts, loading states, and errors
* Conversation branching: Create alternate conversation paths from any point in the chat history
* UI-agnostic design: bring your own components and styling

Let's explore how to use [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) in your React application.

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) provides a solid foundation for creating bespoke chat experiences. For pre-built chat components and interfaces, we also recommend checking out [CopilotKit](https://docs.copilotkit.ai/coagents/quickstart/langgraph) and [assistant-ui](https://www.assistant-ui.com/docs/runtimes/langgraph).

## Customizing your UI

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) hook takes care of all the complex state management behind the scenes, providing you with simple interfaces to build your UI. Here's what you get out of the box:

* Thread state management
* Loading and error states
* Interrupts
* Message handling and updates
* Branching support

Here are some examples on how to use these features effectively:

The `isLoading` property tells you when a stream is active, enabling you to:

* Show a loading indicator
* Disable input fields during processing
* Display a cancel button

### Resume a stream after page refresh

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.

By default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage in `reconnectOnMount` instead. The storage is used to persist the in-flight run ID for a thread (under `lg:stream:${threadId}` key).

You can also manually manage the resuming process by using the run callbacks to persist the run metadata and the `joinStream` function to resume the stream. Make sure to pass `streamResumable: true` when creating the run; otherwise some events might be lost.

### Thread management

Keep track of conversations with built-in thread management. You can access the current thread ID and get notified when new threads are created:

We recommend storing the `threadId` in your URL's query parameters to let users resume conversations after page refreshes.

### Messages handling

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) hook will keep track of the message chunks received from the server and concatenate them together to form a complete message. The completed message chunks can be retrieved via the `messages` property.

By default, the `messagesKey` is set to `messages`, where it will append the new messages chunks to `values["messages"]`. If you store messages in a different key, you can change the value of `messagesKey`.

Under the hood, [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) automatically subscribes to multiple [stream modes](/langsmith/streaming#supported-stream-modes) to provide a complete picture of your graph's execution. The `messages` property specifically uses `messages-tuple` mode to receive individual LLM tokens from chat model invocations. Learn more about messages streaming in the [streaming](/langsmith/streaming#messages) guide.

### Accessing full graph state

Beyond messages, you can access the complete graph state via the `values` property. This includes any state your graph maintains, not just the conversation history:

This is powered by the `values` stream mode under the hood, which streams the full state after each graph step.

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) hook exposes the `interrupt` property, which will be filled with the last interrupt from the thread. You can use interrupts to:

* Render a confirmation UI before executing a node
* Wait for human input, allowing agent to ask the user with clarifying questions

Learn more about interrupts in the [How to handle interrupts](/oss/python/langgraph/interrupts#pause-using-interrupt) guide.

For each message, you can use `getMessagesMetadata()` to get the first checkpoint from which the message has been first seen. You can then create a new run from the checkpoint preceding the first seen checkpoint to create a new branch in a thread.

A branch can be created in following ways:

1. Edit a previous user message.
2. Request a regeneration of a previous assistant message.

For advanced use cases you can use the `experimental_branchTree` property to get the tree representation of the thread, which can be used to render branching controls for non-message based graphs.

### Optimistic Updates

You can optimistically update the client state before performing a network request to the agent, allowing you to provide immediate feedback to the user, such as showing the user message immediately before the agent has seen the request.

### Cached Thread Display

Use the `initialValues` option to display cached thread data immediately while the history is being loaded from the server. This improves user experience by showing cached data instantly when navigating to existing threads.

### Optimistic thread creation

Use the `threadId` option in `submit` function to enable optimistic UI patterns where you need to know the thread ID before the thread is actually created.

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) hook is friendly for apps written in TypeScript and you can specify types for the state to get better type safety and IDE support.

You can also optionally specify types for different scenarios, such as:

* `ConfigurableType`: Type for the `config.configurable` property (default: `Record<string, unknown>`)
* `InterruptType`: Type for the interrupt value - i.e. contents of `interrupt(...)` function (default: `unknown`)
* `CustomEventType`: Type for the custom events (default: `unknown`)
* `UpdateType`: Type for the submit function (default: `Partial<State>`)

If you're using LangGraph.js, you can also reuse your graph's annotation types. However, make sure to only import the types of the annotation schema in order to avoid importing the entire LangGraph.js runtime (i.e. via `import type { ... }` directive).

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) hook provides callback options that give you access to different types of streaming events beyond just messages. You don't need to explicitly configure stream modes— just pass callbacks for the event types you want to handle:

### Available callbacks

| Callback          | Description                                                                                                                            | Stream mode |
| ----------------- | -------------------------------------------------------------------------------------------------------------------------------------- | ----------- |
| `onUpdateEvent`   | Called when a state update is received after each graph step                                                                           | `updates`   |
| `onCustomEvent`   | Called when a custom event is received from your graph. See the [streaming](/oss/python/langgraph/streaming#stream-custom-data) guide. | `custom`    |
| `onMetadataEvent` | Called with run and thread metadata                                                                                                    | `metadata`  |
| `onError`         | Called when an error occurs                                                                                                            | -           |
| `onFinish`        | Called when the stream completes                                                                                                       | -           |

This design means you can access rich streaming data (state updates, custom events, metadata) without manually configuring stream modes—`useStream` handles the subscription for you.

* [useStream API Reference](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-stream-react.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

Example 2 (unknown):
```unknown
## Customizing your UI

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) hook takes care of all the complex state management behind the scenes, providing you with simple interfaces to build your UI. Here's what you get out of the box:

* Thread state management
* Loading and error states
* Interrupts
* Message handling and updates
* Branching support

Here are some examples on how to use these features effectively:

### Loading states

The `isLoading` property tells you when a stream is active, enabling you to:

* Show a loading indicator
* Disable input fields during processing
* Display a cancel button
```

Example 3 (unknown):
```unknown
### Resume a stream after page refresh

The [`useStream()`](https://reference.langchain.com/javascript/functions/_langchain_langgraph-sdk.react.useStream.html) hook can automatically resume an ongoing run upon mounting by setting `reconnectOnMount: true`. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost.
```

Example 4 (unknown):
```unknown
By default the ID of the created run is stored in `window.sessionStorage`, which can be swapped by passing a custom storage in `reconnectOnMount` instead. The storage is used to persist the in-flight run ID for a thread (under `lg:stream:${threadId}` key).
```

---

## How to interact with a deployment using RemoteGraph

**URL:** llms-txt#how-to-interact-with-a-deployment-using-remotegraph

**Contents:**
- Prerequisites
- Initialize the graph
  - Use a URL
  - Use a client
- Invoke the graph
  - Asynchronously
  - Synchronously
- Persist state at the thread level
- Use as a subgraph

Source: https://docs.langchain.com/langsmith/use-remote-graph

[`RemoteGraph`](https://reference.langchain.com/python/langsmith/deployment/remote_graph/) is a client-side interface that allows you to interact with your [deployment](/langsmith/deployments) as if it were a local graph. It provides API parity with [`CompiledGraph`](/oss/python/langgraph/graph-api#compiling-your-graph), which means that you can use the same methods (`invoke()`, `stream()`, `get_state()`, etc.) in your development and production environments. This page describes how to initialize a `RemoteGraph` and interact with it.

`RemoteGraph` is useful for the following:

* Separation of development and deployment: Build and test a graph locally with `CompiledGraph`, deploy it to LangSmith, and then [use `RemoteGraph`](#initialize-the-graph) to call it in production while working with the same API interface.
* Thread-level persistence: [Persist and fetch the state](#persist-state-at-the-thread-level) of a conversation across calls with a thread ID.
* Subgraph embedding: Compose modular graphs for a multi-agent workflow by embedding a `RemoteGraph` as a [subgraph](#use-as-a-subgraph) within another graph.
* Reusable workflows: Use deployed graphs as nodes or [tools](https://reference.langchain.com/python/langsmith/deployment/remote_graph/#langgraph.pregel.remote.RemoteGraph.as_tool), so that you can reuse and expose complex logic.

<Warning>
  **Important: Avoid calling the same deployment**

`RemoteGraph` is designed to call graphs on other deployments. Do not use `RemoteGraph` to call itself or another graph on the same deployment, as this can lead to deadlocks and resource exhaustion. Instead, use local graph composition or [subgraphs](/oss/python/langgraph/use-subgraphs) for graphs within the same deployment.
</Warning>

Before getting started with `RemoteGraph`, make sure you have:

* Access to [LangSmith](/langsmith/home), where your graphs are developed and managed.
* A running [Agent Server](/langsmith/agent-server), which hosts your deployed graphs for remote interaction.

## Initialize the graph

When initializing a `RemoteGraph`, you must always specify:

* `name`: The name of the graph you want to interact with **or** an assistant ID. If you specify a graph name, the default assistant will be used. If you specify an assistant ID, that specific assistant will be used. The graph name is the same name you use in the `langgraph.json` configuration file for your deployment.
* `api_key`: A valid [LangSmith API key](/langsmith/create-account-api-key). You can set as an environment variable (`LANGSMITH_API_KEY`) or pass directly in the `api_key` argument. You can also provide the API key in the `client` / `sync_client` arguments, if `LangGraphClient` / `SyncLangGraphClient` was initialized with the `api_key` argument.

Additionally, you have to provide one of the following:

* [`url`](#use-a-url): The URL of the deployment you want to interact with. If you pass the `url` argument, both sync and async clients will be created using the provided URL, headers (if provided), and default configuration values (e.g., timeout).
* [`client`](#use-a-client): A `LangGraphClient` instance for interacting with the deployment asynchronously (e.g., using `.astream()`, `.ainvoke()`, `.aget_state()`, `.aupdate_state()`).
* `sync_client`: A `SyncLangGraphClient` instance for interacting with the deployment synchronously (e.g., using `.stream()`, `.invoke()`, `.get_state()`, `.update_state()`).

<Note>
  If you pass both `client` or `sync_client` as well as the `url` argument, they will take precedence over the `url` argument. If none of the `client` / `sync_client` / `url` arguments are provided, `RemoteGraph` will raise a `ValueError` at runtime.
</Note>

`RemoteGraph` implements the same Runnable interface as `CompiledGraph`, so you can use it in the same way as a compiled graph. It supports the full set of standard methods, including `.invoke()`, `.stream()`, `.get_state()`, and `.update_state()`, as well as their asynchronous variants.

<Note>
  To use the graph asynchronously, you must provide either the `url` or `client` when initializing the `RemoteGraph`.
</Note>

<Note>
  To use the graph synchronously, you must provide either the `url` or `sync_client` when initializing the `RemoteGraph`.
</Note>

<CodeGroup>
  
</CodeGroup>

## Persist state at the thread level

By default, graph runs (for example, calls made with `.invoke()` or `.stream()`) are stateless, which means that intermediate checkpoints and the final state are not persisted after a run.

If you want to preserve the outputs of a run—for example, to support human-in-the-loop workflows—you can create a thread and pass its ID through the `config` argument. This works the same way as with a regular compiled graph:

<Note>
  If you need to use a `checkpointer` with a graph that has a `RemoteGraph` subgraph node, make sure to use UUIDs as thread IDs.
</Note>

A graph can also call out to multiple `RemoteGraph` instances as [*subgraph*](/oss/python/langgraph/use-subgraphs) nodes. This allows for modular, scalable workflows where different responsibilities are split across separate graphs.

`RemoteGraph` exposes the same interface as a regular `CompiledGraph`, so you can use it directly as a subgraph inside another graph. For example:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-remote-graph.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Use a client

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Invoke the graph

`RemoteGraph` implements the same Runnable interface as `CompiledGraph`, so you can use it in the same way as a compiled graph. It supports the full set of standard methods, including `.invoke()`, `.stream()`, `.get_state()`, and `.update_state()`, as well as their asynchronous variants.

### Asynchronously

<Note>
  To use the graph asynchronously, you must provide either the `url` or `client` when initializing the `RemoteGraph`.
</Note>

<CodeGroup>
```

---

## How to kick off background runs

**URL:** llms-txt#how-to-kick-off-background-runs

**Contents:**
- Setup
- Check runs on thread
- Start runs on thread

Source: https://docs.langchain.com/langsmith/background-run

This guide covers how to kick off background runs for your agent.
This can be useful for long running jobs.

First let's set up our client and thread:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Check runs on thread

If we list the current runs on this thread, we will see that it's empty:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Start runs on thread

Now let's kick off a run:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

The first time we poll it, we can see `status=pending`:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can join the run, wait for it to finish and check that status again:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Perfect! The run succeeded as we would expect. We can double check that the run worked as expected by printing out the final state:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can also just print the content of the last AIMessage:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/background-run.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Output:
```

Example 4 (unknown):
```unknown
## Check runs on thread

If we list the current runs on this thread, we will see that it's empty:

<Tabs>
  <Tab title="Python">
```

---

## How to read experiment results locally

**URL:** llms-txt#how-to-read-experiment-results-locally

**Contents:**
- Iterate over evaluation results

Source: https://docs.langchain.com/langsmith/read-local-experiment-results

When running [evaluations](/langsmith/evaluation-concepts), you may want to process results programmatically in your script rather than viewing them in the [LangSmith UI](https://smith.langchain.com). This is useful for scenarios like:

* **CI/CD pipelines**: Implement quality gates that fail builds if evaluation scores drop below a threshold.
* **Local debugging**: Inspect and analyze results without API calls.
* **Custom aggregations**: Calculate metrics and statistics using your own logic.
* **Integration testing**: Use evaluation results to gate merges or deployments.

This guide shows you how to iterate over and process [experiment](/langsmith/evaluation-concepts#experiment) results from the [`ExperimentResults`](https://reference.langchain.com/python/langsmith/observability/sdk/evaluation/#langsmith.evaluation._runner.ExperimentResults) object returned by [`Client.evaluate()`](https://reference.langchain.com/python/langsmith/observability/sdk/client/#langsmith.client.Client.evaluate).

<Note>
  This page focuses on processing results programmatically while still uploading them to LangSmith.

If you want to run evaluations locally **without** recording anything to LangSmith (for quick testing or validation), refer to [Run an evaluation locally](/langsmith/local) which uses `upload_results=False`.
</Note>

## Iterate over evaluation results

The [`evaluate()`](https://reference.langchain.com/python/langsmith/observability/sdk/client/#langsmith.client.Client.evaluate) function returns an [`ExperimentResults`](https://reference.langchain.com/python/langsmith/observability/sdk/evaluation/#langsmith.evaluation._runner.ExperimentResults) object that you can iterate over. The `blocking` parameter controls when results become available:

* `blocking=False`: Returns immediately with an iterator that yields results as they're produced. This allows you to process results in real-time as the evaluation runs.
* `blocking=True` (default): Blocks until all evaluations complete before returning. When you iterate over the results, all data is already available.

Both modes return the same `ExperimentResults` type; the difference is whether the function waits for completion before returning. Use `blocking=False` for streaming and real-time debugging, or `blocking=True` for batch processing when you need the complete dataset.

The following example demonstrates `blocking=False`. It iterates over results as they stream in, collects them in a list, then processes them in a separate loop:

```python  theme={null}
from langsmith import Client
import random

def target(inputs):
    """Your application or LLM chain"""
    return {"output": "MY OUTPUT"}

def evaluator(run, example):
    """Your evaluator function"""
    return {"key": "randomness", "score": random.randint(0, 1)}

---

## How to return categorical vs numerical metrics

**URL:** llms-txt#how-to-return-categorical-vs-numerical-metrics

**Contents:**
- Related

Source: https://docs.langchain.com/langsmith/metric-type

LangSmith supports both categorical and numerical metrics, and you can return either when writing a custom evaluator.

For an evaluator result to be logged as a numerical metric, it must returned as:

* (Python only) an `int`, `float`, or `bool`
* a dict of the form `{"key": "metric_name", "score": int | float | bool}`

For an evaluator result to be logged as a categorical metric, it must be returned as:

* (Python only) a `str`
* a dict of the form `{"key": "metric_name", "value": str | int | float | bool}`

Here are some examples:

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

* [Return multiple metrics in one evaluator](/langsmith/multiple-scores)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/metric-type.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## How to return multiple scores in one evaluator

**URL:** llms-txt#how-to-return-multiple-scores-in-one-evaluator

**Contents:**
- Related

Source: https://docs.langchain.com/langsmith/multiple-scores

Sometimes it is useful for a custom evaluator or summary evaluator to return multiple metrics. For example, if you have multiple metrics being generated by an LLM judge, you can save time and money by making a single LLM call that generates multiple metrics instead of making multiple LLM calls.

To return multiple scores using the Python SDK, simply return a list of dictionaries/objects of the following form:

To do so with the JS/TS SDK, return an object with a 'results' key and then a list of the above form

Each of these dictionaries can contain any or all of the [feedback fields](/langsmith/feedback-data-format); check out the linked document for more information.

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

Rows from the resulting experiment will display each of the scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7f0a67189b7202a46d5e093cce9ea283" alt="multiple_scores.png" data-og-width="1622" width="1622" data-og-height="1020" height="1020" data-path="langsmith/images/multiple-scores.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb2f322c66eadd3ef0eef99a5c11063f 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=05a52f0d760f2e079c6701e83c48fb73 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=147f1a6046e9dabac499f68ec6abf68f 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0a807e000d2e2d7cd18b6af3e83e8a59 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f46442dafae69f9039fe1c9def5a1a01 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-scores.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c182c52b39a42dbcab618fc8b2a0c525 2500w" />

* [Return categorical vs numerical metrics](/langsmith/metric-type)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multiple-scores.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To do so with the JS/TS SDK, return an object with a 'results' key and then a list of the above form
```

Example 2 (unknown):
```unknown
Each of these dictionaries can contain any or all of the [feedback fields](/langsmith/feedback-data-format); check out the linked document for more information.

Example:

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Support for multiple scores is available in `langsmith@0.1.32` and higher

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## How to run an evaluation asynchronously

**URL:** llms-txt#how-to-run-an-evaluation-asynchronously

**Contents:**
- Use `aevaluate()`

Source: https://docs.langchain.com/langsmith/evaluation-async

<Info>
  [Evaluations](/langsmith/evaluation-concepts#applying-evaluations) | [Evaluators](/langsmith/evaluation-concepts#evaluators) | [Datasets](/langsmith/evaluation-concepts#datasets) | [Experiments](/langsmith/evaluation-concepts#experiments)
</Info>

We can run evaluations asynchronously via the SDK using [aevaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._arunner.aevaluate), which accepts all of the same arguments as [evaluate()](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) but expects the application function to be asynchronous. You can learn more about how to use the `evaluate()` function [here](/langsmith/evaluate-llm-application).

<Info>
  This guide is only relevant when using the Python SDK. In JS/TS the `evaluate()` function is already async. You can see how to use it [here](/langsmith/evaluate-llm-application).
</Info>

Requires `langsmith>=0.3.13`

```python  theme={null}
from langsmith import wrappers, Client
from openai import AsyncOpenAI

---

## How to run an evaluation locally (Python only)

**URL:** llms-txt#how-to-run-an-evaluation-locally-(python-only)

**Contents:**
- Example

Source: https://docs.langchain.com/langsmith/local

Sometimes it is helpful to run an evaluation locally without uploading any results to LangSmith. For example, if you're quickly iterating on a prompt and want to smoke test it on a few examples, or if you're validating that your target and evaluator functions are defined correctly, you may not want to record these evaluations.

You can do this by using the LangSmith Python SDK and passing `upload_results=False` to `evaluate()` / `aevaluate()`.

This will run you application and evaluators exactly as it always does and return the same output, but nothing will be recorded to LangSmith. This includes not just the experiment results but also the application and evaluator traces.

<Note>
  If you want to upload results to LangSmith but also need to process them in your script (for quality gates, custom aggregations, etc.), refer to [Read experiment results locally](/langsmith/read-local-experiment-results).
</Note>

Let's take a look at an example:

Requires `langsmith>=0.2.0`. Example also uses `pandas`.

```python  theme={null}
from langsmith import Client

---

## How to run a pairwise evaluation

**URL:** llms-txt#how-to-run-a-pairwise-evaluation

**Contents:**
- Prerequisites
- `evaluate()` comparative args
- Define a pairwise evaluator
  - Evaluator args
  - Evaluator output
- Run a pairwise evaluation
- View pairwise experiments

Source: https://docs.langchain.com/langsmith/evaluate-pairwise

<Info>
  Concept: [Pairwise evaluations](/langsmith/evaluation-concepts#pairwise)
</Info>

LangSmith supports evaluating **existing** experiments in a comparative manner. Instead of evaluating one output at a time, you can score the output from multiple experiments against each other. In this guide, you'll use [`evaluate()`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate) with two existing experiments to [define an evaluator](#define-a-pairwise-evaluator) and [run a pairwise evaluation](#run-a-pairwise-evaluation). Finally, you'll use the LangSmith UI to [view the pairwise experiments](#view-pairwise-experiments).

* If you haven't already created experiments to compare, check out the [quick start](/langsmith/evaluation-quickstart) or the [how-to guide](/langsmith/evaluate-llm-application) to get started with evaluations.
* This guide requires `langsmith` Python version `>=0.2.0` or JS version `>=0.2.9`.

<Info>
  You can also use [`evaluate_comparative()`](https://docs.smith.langchain.com/reference/python/evaluation/langsmith.evaluation._runner.evaluate_comparative) with more than two existing experiments.
</Info>

## `evaluate()` comparative args

At its simplest, `evaluate` / `aevaluate` function takes the following arguments:

| Argument     | Description                                                                                                                        |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------------- |
| `target`     | A list of the two **existing experiments** you would like to evaluate against each other. These can be uuids or experiment names.  |
| `evaluators` | A list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. |

Along with these, you can also pass in the following optional args:

| Argument                                 | Description                                                                                                                                                                                                                                                                                                                                                                    |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `randomize_order` / `randomizeOrder`     | An optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False. |
| `experiment_prefix` / `experimentPrefix` | A prefix to be attached to the beginning of the pairwise experiment name. Defaults to None.                                                                                                                                                                                                                                                                                    |
| `description`                            | A description of the pairwise experiment. Defaults to None.                                                                                                                                                                                                                                                                                                                    |
| `max_concurrency` / `maxConcurrency`     | The maximum number of concurrent evaluations to run. Defaults to 5.                                                                                                                                                                                                                                                                                                            |
| `client`                                 | The LangSmith client to use. Defaults to None.                                                                                                                                                                                                                                                                                                                                 |
| `metadata`                               | Metadata to attach to your pairwise experiment. Defaults to None.                                                                                                                                                                                                                                                                                                              |
| `load_nested` / `loadNested`             | Whether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False.                                                                                                                                                                                                                                        |

## Define a pairwise evaluator

Pairwise evaluators are just functions with an expected signature.

Custom evaluator functions must have specific argument names. They can take any subset of the following arguments:

* `inputs: dict`: A dictionary of the inputs corresponding to a single example in a dataset.
* `outputs: list[dict]`: A two-item list of the dict outputs produced by each experiment on the given inputs.
* `reference_outputs` / `referenceOutputs: dict`: A dictionary of the reference outputs associated with the example, if available.
* `runs: list[Run]`: A two-item list of the full [Run](/langsmith/run-data-format) objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.
* `example: Example`: The full dataset [Example](/langsmith/example-data-format), including the example inputs, outputs (if available), and metadata (if available).

For most use cases you'll only need `inputs`, `outputs`, and `reference_outputs` / `referenceOutputs`. `runs` and `example` are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.

Custom evaluators are expected to return one of the following types:

* `dict`: dictionary with keys:

* `key`, which represents the feedback key that will be logged
  * `scores`, which is a mapping from run ID to score for that run.
  * `comment`, which is a string. Most commonly used for model reasoning.

Currently Python only

* `list[int | float | bool]`: a two-item list of scores. The list is assumed to have the same order as the `runs` / `outputs` evaluator args. The evaluator function name is used for the feedback key.

Note that you should choose a feedback key that is distinct from standard feedbacks on your run. We recommend prefixing pairwise feedback keys with `pairwise_` or `ranked_`.

## Run a pairwise evaluation

The following example uses [a prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI's response: 0, 1, or 2.

<Info>
  In the Python example below, we are pulling [this structured prompt](https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2) from the [LangChain Hub](/langsmith/manage-prompts#public-prompt-hub) and using it with a LangChain chat model wrapper.

**Usage of LangChain is totally optional.** To illustrate this point, the TypeScript example uses the OpenAI SDK directly.
</Info>

* Python: Requires `langsmith>=0.2.0`
* TypeScript: Requires `langsmith>=0.2.9`

## View pairwise experiments

Navigate to the "Pairwise Experiments" tab from the dataset page:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=dddf35fd971055d0d94ae4184c91dea3" alt="Pairwise Experiments Tab" data-og-width="3454" width="3454" data-og-height="1912" height="1912" data-path="langsmith/images/pairwise-from-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=4c1677867b832da9c3b4338a210570f8 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=80d4795bc999156850eb8092e8267c9f 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ed2e5fb624828fb649bf33473e7dc797 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=05c2248284b4efb2f5a9f38cffef0b9b 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=3014131b2f5ae730aa354afaa7312316 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-from-dataset.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=38c4f707158930cf7d1d155db4021362 2500w" />

Click on a pairwise experiment that you would like to inspect, and you will be brought to the Comparison View:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8afa7467faf707c0bb5ede23b007beda" alt="Pairwise Comparison View" data-og-width="3430" width="3430" data-og-height="1886" height="1886" data-path="langsmith/images/pairwise-comparison-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9a837cee527a1bf5dda5a77b8ce16ba6 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ebb39f8f2fb7a542d2273cfc64c5b4f4 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2f2de8c570a3e6401ba0220da343b3e0 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cf612b4c6938e856b78c7476f8cc6304 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ff05d71cd12f19d0403e6a1e3e64609a 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/pairwise-comparison-view.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5674d6403a3070935830983b9e36ac2f 2500w" />

You may filter to runs where the first experiment was better or vice versa by clicking the thumbs up/thumbs down buttons in the table header:

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=677c48099cee9848d2119c154c7b0d88" alt="Pairwise Filtering" data-og-width="3454" width="3454" data-og-height="1914" height="1914" data-path="langsmith/images/filter-pairwise.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=1ceff9156ccfdb48f246f41c7e0d16ab 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=745f3dc2bed9e3e2d8333df0ff57a43e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e81f10f544953ee39366866c1f4a5d71 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3f4af39e4da50d0ad081d03aaf7b238e 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=63688564868c7a0989c429c6e740e014 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-pairwise.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=527537f492665790aa8380e0d75e7fb3 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-pairwise.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## How to run evaluations with pytest (beta)

**URL:** llms-txt#how-to-run-evaluations-with-pytest-(beta)

**Contents:**
- Installation
- Define and run tests
- Log inputs, outputs, and reference outputs
- Log feedback
- Trace intermediate calls
- Grouping tests into a test suite
- Naming experiments
- Caching
- pytest features
  - Parametrize with `pytest.mark.parametrize`

Source: https://docs.langchain.com/langsmith/pytest

The LangSmith pytest plugin lets Python developers define their datasets and evaluations as pytest test cases.

Compared to the standard evaluation flow, this is useful when:

* **Each example requires different evaluation logic**: Standard evaluation flows assume consistent application and evaluator execution across all dataset examples. For more complex systems or comprehensive evaluations, specific system subsets may require evaluation with particular input types and metrics. These heterogeneous evaluations are simpler to write as distinct test case suites that track together.
* **You want to assert binary expectations**: Track assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines). Testing tools help when both evaluating system outputs and asserting basic properties about them.
* **You want pytest-like terminal outputs**: Get familiar pytest output formatting
* **You already use pytest to test your app**: Add LangSmith tracking to existing pytest workflows

<Warning>
  The pytest integration is in beta and is subject to change in upcoming releases.
</Warning>

<Info>
  The JS/TS SDK has an analogous [Vitest/Jest integration](/langsmith/vitest-jest).
</Info>

This functionality requires Python SDK version `langsmith>=0.3.4`.

For extra features like [rich terminal outputs](#rich-outputs) and [test caching](#caching) install:

## Define and run tests

The pytest integration lets you define datasets and evaluators as test cases.

To track a test in LangSmith add the `@pytest.mark.langsmith` decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.

<CodeGroup>
  
</CodeGroup>

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.

Use `pytest` as you normally would to run the tests:

In most cases we recommend setting a test suite name:

Each time you run this test suite, LangSmith:

* creates a [dataset](/langsmith/evaluation-concepts#datasets) for each test file. If a dataset for this test file already exists it will be updated
* creates an [experiment](/langsmith/evaluation-concepts#experiment) in each created/updated dataset
* creates an experiment row for each test case, with the inputs, outputs, reference outputs and feedback you've logged
* collects the pass/fail rate under the `pass` feedback key for each test case

Here's what a test suite dataset looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f40d29e4260eebc87838ea7be78bd08d" alt="Dataset" data-og-width="1078" width="1078" data-og-height="437" height="437" data-path="langsmith/images/simple-pytest-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ac625f42dd28d99e4fedaf193421d7f5 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=169fc45042f5c75d61e5c9a0dc9117cf 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b63c8752c2356d297cc44969da407673 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f9a727e2fed9369844b4e6de59d72c0f 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e5d14240d3d0df568adb1655a28dbc58 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest-dataset.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9adad58f251fc1dba96c9b6f4092a11f 2500w" />

And what an experiment against that test suite looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=09776ac389e88f7f5058f4f7cf44dc72" alt="Experiment" data-og-width="1077" width="1077" data-og-height="444" height="444" data-path="langsmith/images/simple-pytest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=250e774ba91ea54112577a40466fdf51 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e77b0e75d32f47f9b9ab86df83f0ac99 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1b1bd6106aaed9412a2e56b52598e412 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=65630e87475c1c8570e0d47d18fd1629 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a4507629b61206359c2546afb17ac77a 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-pytest.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a5ee150a76f3e40e704c7de4b2d74fdf 2500w" />

## Log inputs, outputs, and reference outputs

Every time we run a test we're syncing it to a dataset example and tracing it as a run. There's a few different ways that we can trace the example inputs and reference outputs and the run outputs. The simplest is to use the `log_inputs`, `log_outputs`, and `log_reference_outputs` methods. You can run these any time in a test to update the example and run for that test:

Running this test will create/update an example with name "test\_foo", inputs `{"a": 1, "b": 2}`, reference outputs `{"foo": "bar"}` and trace a run with outputs `{"foo": "baz"}`.

**NOTE**: If you run `log_inputs`, `log_outputs`, or `log_reference_outputs` twice, the previous values will be overwritten.

Another way to define example inputs and reference outputs is via pytest fixtures/parametrizations. By default any arguments to your test function will be logged as inputs on the corresponding example. If certain arguments are meant to represet reference outputs, you can specify that they should be logged as such using `@pytest.mark.langsmith(output_keys=["name_of_ref_output_arg"])`:

This will create/sync an example with name "test\_cd", inputs `{"c": 5}` and reference outputs `{"d": 6}`, and run output `{"d": 10}`.

By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with `log_feedback`.

Note the use of the `trace_feedback()` context manager. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case. Instead of showing up in the main test case run it will instead show up in the trace for the `correct` feedback key.

**NOTE**: Make sure that the `log_feedback` call associated with the feedback trace occurs inside the `trace_feedback` context. This way we'll be able to associate the feedback with the trace, and when seeing the feedback in the UI you'll be able to click on it to see the trace that generated it.

## Trace intermediate calls

LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.

## Grouping tests into a test suite

By default, all tests within a given file will be grouped as a single "test suite" with a corresponding dataset. You can configure which test suite a test belongs to by passing the `test_suite_name` parameter to `@pytest.mark.langsmith` for case-by-case grouping, or you can set the `LANGSMITH_TEST_SUITE` env var to group all tests from an execution into a single test suite:

We generally recommend setting `LANGSMITH_TEST_SUITE` to get a consolidated view of all of your results.

## Naming experiments

You can name an experiment using the `LANGSMITH_EXPERIMENT` env var:

LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache HTTP requests to disk. To enable caching, install with `langsmith[pytest]` and set an env var: `LANGSMITH_TEST_CACHE=/my/cache/path`:

All requests will be cached to `tests/cassettes` and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well.

In `langsmith>=0.4.10`, you may selectively enable caching for requests to individual URLs or hostnames like this:

`@pytest.mark.langsmith` is designed to stay out of your way and works well with familiar `pytest` features.

### Parametrize with `pytest.mark.parametrize`

You can use the `parametrize` decorator as before. This will create a new test case for each parametrized instance of the test.

**Note:** as the parametrized list grows, you may consider using `evaluate()` instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset.

### Parallelize with `pytest-xdist`

You can use [pytest-xdist](https://pytest-xdist.readthedocs.io/en/stable/) as you normally would to parallelize test execution:

### Async tests with `pytest-asyncio`

`@pytest.mark.langsmith` works with sync or async tests, so you can run async tests exactly as before.

### Watch mode with `pytest-watch`

Use watch mode to quickly iterate on your tests. We *highly* recommend ony using this with test caching (see below) enabled to avoid unnecessary LLM calls:

If you'd like to see a rich display of the LangSmith results of your test run you can specify `--langsmith-output`:

**Note:** This flag used to be `--output=langsmith` in `langsmith<=0.3.3` but was updated to avoid collisions with other pytest plugins.

You'll get a nice table per test suite that updates live as the results are uploaded to LangSmith:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=10712bc97e37900ca83cb70df1c9357d" alt="Rich pytest outputs" data-og-width="1340" width="1340" data-og-height="548" height="548" data-path="langsmith/images/rich-pytest-outputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b689a6512f89045fdda11112f344c0aa 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=be71e1c2abb5e14616ab8f9c4cf05912 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4d7beb30f4e5f72bc30def219831184d 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=85478a1e6830a5bc01b35eda991e86c7 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2980269c433c3a30c0da34e57a6232ba 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rich-pytest-outputs.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fc52452a3b17850b27d4922b392185f8 2500w" />

Some important notes for using this feature:

* Make sure you've installed `pip install -U "langsmith[pytest]"`
* Rich outputs do not currently work with `pytest-xdist`

**NOTE**: The custom output removes all the standard pytest outputs. If you're trying to debug some unexpected behavior it's often better to show the regular pytest outputs so to get full error traces.

If you want to run the tests without syncing the results to LangSmith, you can set `LANGSMITH_TEST_TRACKING=false` in your environment.

The tests will run as normal, but the experiment logs will not be sent to LangSmith.

LangSmith provides an [expect](https://docs.smith.langchain.com/reference/python/_expect/langsmith._expect._Expect#langsmith._expect._Expect) utility to help define expectations about your LLM output. For example:

This will log the binary "expectation" score to the experiment results, additionally `assert`ing that the expectation is met possibly triggering a test failure.

`expect` also provides "fuzzy match" methods. For example:

This test case will be assigned 4 scores:

1. The `embedding_distance` between the prediction and the expectation
2. The binary `expectation` score (1 if cosine distance is less than 0.5, 0 if not)
3. The `edit_distance` between the prediction and the expectation
4. The overall test pass/fail score (binary)

The `expect` utility is modeled off of [Jest](https://jestjs.io/docs/expect)'s expect API, with some off-the-shelf functionality to make it easier to grade your LLMs.

#### `@test` / `@unit` decorator

The legacy method for marking test cases is using the `@test` or `@unit` decorators:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pytest.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Define and run tests

The pytest integration lets you define datasets and evaluators as test cases.

To track a test in LangSmith add the `@pytest.mark.langsmith` decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case.

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log.

Use `pytest` as you normally would to run the tests:
```

Example 4 (unknown):
```unknown
In most cases we recommend setting a test suite name:
```

---

## How to run evaluations with Vitest/Jest (beta)

**URL:** llms-txt#how-to-run-evaluations-with-vitest/jest-(beta)

**Contents:**
- Setup
  - Vitest
  - Jest
- Define and run evals
- Trace feedback
- Running multiple examples against a test case
- Log outputs
- Trace intermediate calls
- Focusing or skipping tests
- Configuring test suites

Source: https://docs.langchain.com/langsmith/vitest-jest

LangSmith provides integrations with Vitest and Jest that allow JavaScript and TypeScript developers define their datasets and evaluate using familiar syntax.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=94fd2a6f61c9dc386002fadbab7024a8" alt="Jest/Vitest reporter output" data-og-width="2200" width="2200" data-og-height="564" height="564" data-path="langsmith/images/jest-vitest-reporter-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf56669ba6d6ab79ed6237424f163fa7 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=af88b4a6c4d31520b783336f311f56fc 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=32ee63fc2b8923236850f9b2a1fb1775 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e1f396ae3e1b68e358efc599f700e0c3 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=77b9a394525825d5f9395cdb22a5c8b4 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/jest-vitest-reporter-output.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2d22bf302eedc50c70280ce2d8bc7d79 2500w" />

Compared to the `evaluate()` evaluation flow, this is useful when:

* **Each example requires different evaluation logic**: Standard evaluation flows assume consistent application and evaluator execution across all dataset examples. For more complex systems or comprehensive evaluations, specific system subsets may require evaluation with particular input types and metrics. These heterogeneous evaluations are simpler to write as distinct test case suites that track together.
* **You want to assert binary expectations**: Track assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines). Testing tools help when both evaluating system outputs and asserting basic properties about them.
* **You want to take advantage of mocks, watch mode, local results, or other features of the Vitest/Jest ecosystems**

<Info>
  Requires JS/TS SDK version `langsmith>=0.3.1`.
</Info>

<Warning>
  The Vitest/Jest integrations are in beta and are subject to change in upcoming releases.
</Warning>

<Info>
  The Python SDK has an analogous [pytest integration](/langsmith/pytest).
</Info>

Set up the integrations as follows. Note that while you can add LangSmith evals alongside your other unit tests (as standard `*.test.ts` files) using your existing test config files, the below examples will also set up a separate test config file and command to run your evals. It will assume you end your test files with `.eval.ts`.

This ensures that the custom test reporter and other LangSmith touchpoints do not modify your existing test outputs.

Install the required development dependencies if you have not already:

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

Then create a separate `ls.vitest.config.ts` file with the following base config:

* `include` ensures that only files ending with some variation of `eval.ts` in your project are run
* `reporters` is responsible for nicely formatting your output as shown above
* `setupFiles` runs `dotenv` to load environment variables before running your evals

<Warning>
  JSDom environments are not supported at this time. You should either omit the `"environment"` field from your config or set it to `"node"`.
</Warning>

Finally, add the following to the `scripts` field in your `package.json` to run Vitest with the config you just created:

Note that the above script disables Vitest's default watch mode for running evals since many evaluators may include longer running LLM calls.

Install the required development dependencies if you have not already:

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

<Info>
  The setup instructions below are for basic JS files and CJS. To add support for TypeScript and ESM, see Jest's official docs or use [Vitest](#vitest).
</Info>

Then create a separate config file named `ls.jest.config.cjs`:

* `testMatch` ensures that only files ending with some variation of `eval.js` in your project are run
* `reporters` is responsible for nicely formatting your output as shown above
* `setupFiles` runs `dotenv` to load environment variables before running your evals

<Warning>
  JSDom environments are not supported at this time. You should either omit the `"testEnvironment"` field from your config or set it to `"node"`.
</Warning>

Finally, add the following to the `scripts` field in your `package.json` to run Jest with the config you just created:

## Define and run evals

You can now define evals as tests using familiar Vitest/Jest syntax, with a few caveats:

* You should import `describe` and `test` from the `langsmith/jest` or `langsmith/vitest` entrypoint
* You must wrap your test cases in a `describe` block
* When declaring tests, the signature is slightly different - there is an extra argument containing example inputs and expected outputs

Try it out by creating a file named `sql.eval.ts` (or `sql.eval.js` if you are using Jest without TypeScript) and pasting the below contents into it:

You can think of each `ls.test()` case as corresponding to a dataset example, and `ls.describe()` as defining a LangSmith dataset. If you have LangSmith [tracing environment variables](/#3-set-up-your-environment) set when you run the test suite, the SDK does the following:

* creates a [dataset](/langsmith/evaluation-concepts#datasets) with the same name as the name passed to `ls.describe()` in LangSmith if it does not exist
* creates an example in the dataset for each input and expected output passed into a test case if a matching one does not already exist
* creates a new [experiment](/langsmith/evaluation-concepts#experiment) with one result for each test case
* collects the pass/fail rate under the `pass` feedback key for each test case

When you run this test it will have a default `pass` boolean feedback key based on the test case passing / failing. It will also track any outputs that you log with the `ls.logOutputs()` or return from the test function as "actual" result values from your app for the experiment.

Create a `.env` file with your `OPENAI_API_KEY` and LangSmith credentials if you don't already have one:

Now use the `eval` script we set up in the previous step to run the test:

And your declared test should run!

Once it finishes, if you've set your LangSmith environment variables, you should see a link directing you to an experiment created in LangSmith alongside the test results.

Here's what an experiment against that test suite looks like:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9cde688950dd2fc454a8514b02ed7268" alt="Experiment" data-og-width="2752" width="2752" data-og-height="902" height="902" data-path="langsmith/images/simple-vitest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e583f4ee7179018b026ce9c037a05702 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3d87bf9ced639e2deb375f0638b1912e 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e1f92efbbffa880300575043180eb107 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=90a9e603ea1b613b6a95f4a686cb954b 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=335d4ba669f1a75d6c8171ce2d7cbb99 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/simple-vitest.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9330a3bc998e80851ce3a162272b037d 2500w" />

By default LangSmith collects the pass/fail rate under the `pass` feedback key for each test case. You can add additional feedback with either `ls.logFeedback()` or `wrapEvaluator()`. To do so, try the following as your `sql.eval.ts` file (or `sql.eval.js` if you are using Jest without TypeScript):

Note the use of `ls.wrapEvaluator()` around the `myEvaluator` function. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case to avoid clutter, and conveniently creates feedback if the return value from the wrapped function matches `{ key: string; score: number | boolean }`. In this case, instead of showing up in the main test case run, the evaluator trace will instead show up in a trace associated with the `correctness` feedback key.

You can see the evaluator runs in LangSmith by clicking their corresponding feedback chips in the UI.

## Running multiple examples against a test case

You can run the same test case over multiple examples and parameterize your tests using `ls.test.each()`. This is useful when you want to evaluate your app the same way against different inputs:

If you have tracking enabled, each example in the local dataset will be synced to the one created in LangSmith.

Every time we run a test we're syncing it to a dataset example and tracing it as a run. To trace final outputs for the run, you can use `ls.logOutputs()` like this:

The logged outputs will appear in your reporter summary and in LangSmith.

You can also directly return a value from your test function:

However keep in mind if you do this that if your test fails to complete due to a failed assertion or other error, your output will not appear.

## Trace intermediate calls

LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.

## Focusing or skipping tests

You can chain the Vitest/Jest `.skip` and `.only` methods on `ls.test()` and `ls.describe()`:

## Configuring test suites

You can configure test suites with values like metadata or a custom client by passing an extra argument to `ls.describe()` for the full suite or by passing a `config` field into `ls.test()` for individual tests:

The test suite will also automatically extract environment variables from `process.env.ENVIRONMENT`, `process.env.NODE_ENV` and `process.env.LANGSMITH_ENVIRONMENT` and set them as metadata on created experiments. You can then filter experiments by metadata in LangSmith's UI.

See [the API refs](https://docs.smith.langchain.com/reference/js/functions/vitest.describe) for a full list of configuration options.

If you want to run the tests without syncing the results to LangSmith, you can set omit your LangSmith tracing environment variables or set `LANGSMITH_TEST_TRACKING=false` in your environment.

The tests will run as normal, but the experiment logs will not be sent to LangSmith.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/vitest-jest.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

The examples below also require `openai` (and of course `langsmith`!) as a dependency:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How to set up an application with pyproject.toml

**URL:** llms-txt#how-to-set-up-an-application-with-pyproject.toml

**Contents:**
- Specify dependencies
- Specify environment variables
- Define graphs

Source: https://docs.langchain.com/langsmith/setup-pyproject

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up an application for deployment using `pyproject.toml` to define your package's dependencies.

This example is based on [this repository](https://github.com/langchain-ai/langgraph-example-pyproject), which uses the LangGraph framework.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you implement your core application logic without using additional LangGraph OSS APIs while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability). For more details, refer to [Use any framework with LangSmith Deployment](/langsmith/application-structure#use-any-framework-with-langsmith-deployment).
</Tip>

You can also set up with:

* `requirements.txt`: for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `requirements.txt` for LangSmith.
* a monorepo: To deploy a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:

Example `pyproject.toml` file:

Example file directory:

## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

<Tip>
  By default, LangSmith follows the `uv`/`pip` behavior of **not** installing prerelease versions unless explicitly allowed. If want to use prereleases, you have the following options:

* With `pyproject.toml`: add `allow-prereleases = true` to your `[tool.uv]` section.
  * With `requirements.txt` or `setup.py`: you must explicitly specify every prerelease dependency, including transitive ones. For example, if you declare `a==0.0.1a1` and `a` depends on `b==0.0.1a1`, then you must also explicitly include `b==0.0.1a1` in your dependencies.
</Tip>

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each [CompiledStateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) to be included in the application. The variable names will be used later when creating the [configuration file](/langsmith/cli#configuration-file).

Example `agent.py` file, which shows how to import from other modules you define (code for the modules is not shown here, please see [this repository](https://github.com/langchain-ai/langgraph-example-pyproject) to see their implementation):

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you implement your core application logic without using additional LangGraph OSS APIs while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability). For more details, refer to [Use any framework with LangSmith Deployment](/langsmith/application-structure#use-any-framework-with-langsmith-deployment).
</Tip>

You can also set up with:

* `requirements.txt`: for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `requirements.txt` for LangSmith.
* a monorepo: To deploy a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:
```

Example 2 (unknown):
```unknown
Example `pyproject.toml` file:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## How to set up an application with requirements.txt

**URL:** llms-txt#how-to-set-up-an-application-with-requirements.txt

**Contents:**
- Specify dependencies
- Specify environment variables
- Define graphs

Source: https://docs.langchain.com/langsmith/setup-app-requirements-txt

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up an application for deployment using `requirements.txt` to specify project dependencies.

This example is based on [this repository](https://github.com/langchain-ai/langgraph-example), which uses the LangGraph framework.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you implement your core application logic without using additional LangGraph OSS APIs while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability). For more details, refer to [Use any framework with LangSmith Deployment](/langsmith/application-structure#use-any-framework-with-langsmith-deployment).
</Tip>

You can also set up with:

* `pyproject.toml`: If you prefer using poetry for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `pyproject.toml` for LangSmith.
* a monorepo: If you are interested in deploying a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:

Example `requirements.txt` file:

Example file directory:

## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

<Tip>
  By default, LangSmith follows the `uv`/`pip` behavior of **not** installing prerelease versions unless explicitly allowed. If want to use prereleases, you have the following options:

* With `pyproject.toml`: add `allow-prereleases = true` to your `[tool.uv]` section.
  * With `requirements.txt` or `setup.py`: you must explicitly specify every prerelease dependency, including transitive ones. For example, if you declare `a==0.0.1a1` and `a` depends on `b==0.0.1a1`, then you must also explicitly include `b==0.0.1a1` in your dependencies.
</Tip>

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each [CompiledStateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) to be included in the application. The variable names will be used later when creating the [LangGraph configuration file](/langsmith/cli#configuration-file).

Example `agent.py` file, which shows how to import from other modules you define (code for the modules is not shown here, please see [this repository](https://github.com/langchain-ai/langgraph-example) to see their implementation):

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you implement your core application logic without using additional LangGraph OSS APIs while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability). For more details, refer to [Use any framework with LangSmith Deployment](/langsmith/application-structure#use-any-framework-with-langsmith-deployment).
</Tip>

You can also set up with:

* `pyproject.toml`: If you prefer using poetry for dependency management, check out [this how-to guide](/langsmith/setup-app-requirements-txt) on using `pyproject.toml` for LangSmith.
* a monorepo: If you are interested in deploying a graph located inside a monorepo, take a look at [this repository](https://github.com/langchain-ai/langgraph-example-monorepo) for an example of how to do so.

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can optionally be specified in one of the following files: `pyproject.toml`, `setup.py`, or `requirements.txt`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-configuration-file).

The dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:
```

Example 2 (unknown):
```unknown
Example `requirements.txt` file:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## How to set up a JavaScript application

**URL:** llms-txt#how-to-set-up-a-javascript-application

**Contents:**
- Specify dependencies
- Specify environment variables
- Define graphs
- Create the API config
- Next

Source: https://docs.langchain.com/langsmith/setup-javascript

An application must be configured with a [configuration file](/langsmith/cli#configuration-file) in order to be deployed to LangSmith (or to be self-hosted). This how-to guide discusses the basic steps to set up a JavaScript application for deployment using `package.json` to specify project dependencies.

This walkthrough is based on [this repository](https://github.com/langchain-ai/langgraphjs-studio-starter), which you can play around with to learn more about how to set up your application for deployment.

The final repository structure will look something like this:

<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you implement your core application logic without using additional LangGraph OSS APIs while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability). For more details, refer to [Use any framework with LangSmith Deployment](/langsmith/application-structure#use-any-framework-with-langsmith-deployment).
</Tip>

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can be specified in a `package.json`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-api-config).

Example `package.json` file:

When deploying your app, the dependencies will be installed using the package manager of your choice, provided they adhere to the compatible version ranges listed below:

Example file directory:

## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example file directory:

Implement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each compiled graph to be included in the application. The variable names will be used later when creating the [configuration file](/langsmith/cli#configuration-file).

Here is an example `agent.ts`:

Example file directory:

## Create the API config

Create a [configuration file](/langsmith/cli#configuration-file) called `langgraph.json`. See the [configuration file reference](/langsmith/cli#configuration-file) for detailed explanations of each key in the JSON object of the configuration file.

Example `langgraph.json` file:

Note that the variable name of the `CompiledGraph` appears at the end of the value of each subkey in the top-level `graphs` key (i.e. `:<variable_name>`).

<Info>
  **Configuration Location**
  The configuration file must be placed in a directory that is at the same level or higher than the TypeScript files that contain compiled graphs and associated dependencies.
</Info>

After you setup your project and place it in a GitHub repository, it's time to [deploy your app](/langsmith/deployment-quickstart).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/setup-javascript.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  LangSmith Deployment supports deploying a [LangGraph](/oss/python/langgraph/overview) *graph*. However, the implementation of a *node* of a graph can contain arbitrary code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you implement your core application logic without using additional LangGraph OSS APIs while still using LangSmith for [deployment](/langsmith/deployments), scaling, and [observability](/langsmith/observability). For more details, refer to [Use any framework with LangSmith Deployment](/langsmith/application-structure#use-any-framework-with-langsmith-deployment).
</Tip>

After each step, an example file directory is provided to demonstrate how code can be organized.

## Specify dependencies

Dependencies can be specified in a `package.json`. If none of these files is created, then dependencies can be specified later in the [configuration file](#create-the-api-config).

Example `package.json` file:
```

Example 2 (unknown):
```unknown
When deploying your app, the dependencies will be installed using the package manager of your choice, provided they adhere to the compatible version ranges listed below:
```

Example 3 (unknown):
```unknown
Example file directory:
```

Example 4 (unknown):
```unknown
## Specify environment variables

Environment variables can optionally be specified in a file (e.g. `.env`). See the [Environment Variables reference](/langsmith/env-var) to configure additional variables for a deployment.

Example `.env` file:
```

---

## How to simulate multi-turn interactions

**URL:** llms-txt#how-to-simulate-multi-turn-interactions

**Contents:**
- Setup
- Running a simulation
- Running in LangSmith experiments
  - Using `pytest` or `Vitest/Jest`
  - Using `evaluate`
- Modifying the simulated user persona
- Next steps

Source: https://docs.langchain.com/langsmith/multi-turn-simulation

<Info>
  * [Multi-turn interactions](/langsmith/evaluation-concepts#multi-turn-interactions)
  * [Evaluators](/langsmith/evaluation-concepts#evaluators)
  * [LLM-as-judge](/langsmith/evaluation-concepts#llm-as-judge)
  * [OpenEvals](https://github.com/langchain-ai/openevals)
</Info>

AI applications with conversational interfaces, like chatbots, operate over multiple interactions with a user, also called conversation *turns*. When evaluating the performance of such applications, core concepts such as [building a dataset](/langsmith/evaluation-concepts#datasets) and defining [evaluators](/langsmith/evaluation-concepts#evaluators) and metrics to judge your app outputs remain useful. However, you may also find it useful to run a *simulation* between your app and a user, then evaluate this dynamically created trajectory.

Some advantages of doing this are:

* Ease of getting started vs. an evaluation over a full dataset of pre-existing trajectories
* End-to-end coverage from an initial query until a successful or unsuccessful resolution
* The ability to detect repetitive behavior or context loss over several iterations of your app

The downside is that because you are broadening your evaluation surface area to contain multiple turns, there is less consistency than evaluating a single output from your app given a static input from a dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e" alt="Multi turn trace" data-og-width="2952" width="2952" data-og-height="1790" height="1790" data-path="langsmith/images/multi-turn-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w" />

This guide will show you how to simulate multi-turn interactions and evaluate them using the open-source [`openevals`](https://github.com/langchain-ai/openevals) package, which contains prebuilt evaluators and other convenient resources for evaluating your AI apps. It will also use OpenAI models, though you can use other providers as well.

First, ensure you have the required dependencies installed:

<Info>
  If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general.
</Info>

And set up your environment variables:

## Running a simulation

There are two primary components you'll need to get started:

* `app`: Your application, or a function wrapping it. Must accept a single chat message (dict with "role" and "content" keys) as an input arg and a `thread_id` as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.
* `user`: The simulated user. In this guide, we will use an imported prebuilt function named `create_llm_simulated_user` which uses an LLM to generate user responses, though you can [create your own too](https://github.com/langchain-ai/openevals?tab=readme-ov-file#custom-simulated-users).

The simulator in `openevals` passes a single chat message to your `app` from the `user` for each turn. Therefore you should statefully track the current history internally based on `thread_id` if needed.

Here's an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:

The response looks like this:

The simulation first generates an initial query from the simulated `user`, then passes response chat messages back and forth until it reaches `max_turns` (you can alternatively pass a `stopping_condition` that takes the current trajectory and returns `True` or `False` - [see the OpenEvals README for more information](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation)). The return value is the final list of chat messages that make up the converation's **trajectory**.

<Info>
  There are several ways to configure the simulated user, such as having it return fixed responses for the first turns of your simulation, as well as the simulation as a whole. For full details, check out [the OpenEvals README](https://github.com/langchain-ai/openevals?tab=readme-ov-file#multiturn-simulation).
</Info>

The final trace will look something [like this](https://smith.langchain.com/public/648ca37d-1c4d-4f7b-9b6a-89e35dc5d4f0/r) with responses from your `app` and `user` interleaved:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c903f388600ab575e70edb92209c6b2e" alt="Multi turn trace" data-og-width="2952" width="2952" data-og-height="1790" height="1790" data-path="langsmith/images/multi-turn-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a5113736dc83834150a2b414619626b2 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f0bfc1f1764c80efcdebfcc07149ef8a 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53e53c0f05f5638b5e17576c0f37d195 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=dc05ed0dcfe9d6da5b8872df189f6ed9 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7fbd598ec4a113bb143d0a0a1ca68a91 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a9dcabf49b84ba2ad8aac4159b5b0657 2500w" />

Congrats! You just ran your first multi-turn simulation. Next, we'll cover how to run it in a LangSmith experiment.

## Running in LangSmith experiments

You can use the results of multi-turn simulations as part of a LangSmith experiment to track performance and progress over time. For these sections, it helps to be familiar with at least one of LangSmith's [`pytest`](/langsmith/pytest) (Python-only), [`Vitest`/`Jest`](/langsmith/vitest-jest) (JS only), or [`evaluate`](/langsmith/evaluate-llm-application) runners.

### Using `pytest` or `Vitest/Jest`

<Check>
  See the following guides to learn how to set up evals using LangSmith's integrations with test frameworks:

* [`pytest`](https://docs.smith.langchain.com/langsmith/pytest)
  * [`Vitest` or `Jest`](https://docs.smith.langchain.com/langsmith/vitest-jest)
</Check>

If you are using one of the [LangSmith test framework integrations](/langsmith/pytest), you can pass in an array of OpenEvals evaluators as a `trajectory_evaluators` param when running the simulation. These evaluators will run at the end of the simulation, taking the final list of chat messages as an `outputs` kwarg. Your passed `trajectory_evaluator` must therefore accept this kwarg.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=04c56e67e7bb9f01cb905d8a184d62d5" alt="Multi turn vitest" data-og-width="3448" width="3448" data-og-height="1128" height="1128" data-path="langsmith/images/multi-turn-vitest.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=41ac969c6ceb99ac0976ab3027b00e89 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c10eece031225173dc0ded446e3e2e3c 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=90bda951b2cfa02bde0c8ad204a7dac7 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2ca95c911f68412eb09e2f8a0a6b42e4 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=12be96d7f216cd8ce664c01f61f45288 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-turn-vitest.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1faa3a34b3daf67705e2afeca748e353 2500w" />

LangSmith will automatically detect and log the feedback returned from the passed `trajectory_evaluators`, adding it to the experiment. Note also that the test case uses the `fixed_responses` param on the simulated user to start the conversation with a specific input, which you can log and make part of your stored dataset.

You may also find it convenient to have the simulated user's system prompt to be part of your logged dataset as well.

You can also use the [`evaluate`](/langsmith/evaluate-llm-application) runner to evaluate simulated multi-turn interactions. This will be a little bit different from the `pytest`/`Vitest`/`Jest` example in the following ways:

* The simulation should be part of your `target` function, and your target function should return the final trajectory.
  * This will make the trajectory the `outputs` that LangSmith will pass to your evaluators.
* Instead of using the `trajectory_evaluators` param, you should pass your evaluators as a param into the `evaluate()` method.
* You will need an existing dataset of inputs and (optionally) reference trajectories.

## Modifying the simulated user persona

The above examples run using the same simulated user persona for all input examples, defined by the `system` parameter passed into `create_llm_simulated_user`. If you would like to use a different persona for specific items in your dataset, you can update your dataset examples to also contain an extra field with the desired `system` prompt, then pass that field in when creating your simulated user like this:

You've just seen some techniques for simulating multi-turn interactions and running them in LangSmith evals.

Here are some topics you might want to explore next:

* [Trace multiturn conversations across different traces](/langsmith/threads)
* [Use multiple messages in the playground UI](/langsmith/multiple-messages)
* [Return multiple metrics in one evaluator](/langsmith/multiple-scores)

You can also explore the [OpenEvals readme](https://github.com/langchain-ai/openevals) for more on prebuilt evaluators.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multi-turn-simulation.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  If you are using `yarn` as your package manager, you will also need to manually install `@langchain/core` as a peer dependency of `openevals`. This is not required for LangSmith evals in general.
</Info>

And set up your environment variables:
```

Example 3 (unknown):
```unknown
## Running a simulation

There are two primary components you'll need to get started:

* `app`: Your application, or a function wrapping it. Must accept a single chat message (dict with "role" and "content" keys) as an input arg and a `thread_id` as a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.
* `user`: The simulated user. In this guide, we will use an imported prebuilt function named `create_llm_simulated_user` which uses an LLM to generate user responses, though you can [create your own too](https://github.com/langchain-ai/openevals?tab=readme-ov-file#custom-simulated-users).

The simulator in `openevals` passes a single chat message to your `app` from the `user` for each turn. Therefore you should statefully track the current history internally based on `thread_id` if needed.

Here's an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How to sync prompts with GitHub

**URL:** llms-txt#how-to-sync-prompts-with-github

**Contents:**
- Prerequisites
- Understanding LangSmith "Prompt Commits" and webhooks
- Implementing a FastAPI server for webhook reception
- Configuring the webhook in LangSmith
- The workflow in action
- Beyond a simple commit

Source: https://docs.langchain.com/langsmith/prompt-commit

LangSmith provides a collaborative interface to create, test, and iterate on prompts.

While you can [dynamically fetch prompts](/langsmith/manage-prompts-programmatically#pull-a-prompt) from LangSmith into your application at runtime, you may prefer to sync prompts with your own database or version control system. To support this workflow, LangSmith allows you to receive notifications of prompt updates via webhooks.

**Why sync prompts with GitHub?**

* **Version Control:** Keep your prompts versioned alongside your application code in a familiar system.
* **CI/CD Integration:** Trigger automated staging or production deployments when critical prompts change.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a7fd1ae2a70f91c14298803a48785f89" alt="Prompt Webhook Diagram" data-og-width="1336" width="1336" data-og-height="343" height="343" data-path="langsmith/images/prompt-excalidraw.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=02f868ec42337b43a533f23effa76417 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=68806f6d9d1e5b8dbaf49d98294190da 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=133aeb6d3a880b989c2246632b8c0d5d 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fc7091cce5fec3407e005f798db85544 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d789978f57574ec4f0c75beac19fc10c 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-excalidraw.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d6362e5787b7fd1a5db69b3d704912bb 2500w" />

Before we begin, ensure you have the following set up:

1. **GitHub Account:** A standard GitHub account.

2. **GitHub Repository:** Create a new (or choose an existing) repository where your LangSmith prompt manifests will be stored. This could be the same repository as your application code or a dedicated one for prompts.

3. **GitHub Personal Access Token (PAT):**

* LangSmith webhooks don't directly interact with GitHub—they call an intermediary server that *you* create.
   * This server requires a GitHub PAT to authenticate and make commits to your repository.
   * Must include the `repo` scope (`public_repo` is sufficient for public repositories).
   * Go to **GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic)**.
   * Click **Generate new token (classic)**.
   * Name it (e.g., "LangSmith Prompt Sync"), set an expiration, and select the required scopes.
   * Click **Generate token** and **copy it immediately** — it won't be shown again.
   * Store the token securely and provide it as an environment variable to your server.

## Understanding LangSmith "Prompt Commits" and webhooks

In LangSmith, when you save changes to a prompt, you're essentially creating a new version or a "Prompt Commit." These commits are what can trigger webhooks.

The webhook will send a JSON payload containing the new **prompt manifest**.

<Accordion title="Sample Webhook Payload">
  
</Accordion>

<Note>
  It's important to understand that LangSmith webhooks for prompt commits are generally triggered at the **workspace level**. This means if *any* prompt within your LangSmith workspace is modified and a "prompt commit" is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.
</Note>

## Implementing a FastAPI server for webhook reception

To effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.

This publicly accessible server will be responsible for:

1. **Receiving Webhook Requests:** Listening for incoming HTTP POST requests.
2. **Parsing Payloads:** Extracting and interpreting the JSON-formatted prompt manifest from the request body.
3. **Committing to GitHub:** Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.

For deployment, platforms like [Render.com](https://render.com/) (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.

The server's core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.

<Accordion title="Minimal FastAPI Server Code ()">
  `main.py`

This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.

**Key aspects of this server:**

* **Configuration (`.env`):** It expects a `.env` file with your `GITHUB_TOKEN`, `GITHUB_REPO_OWNER`, and `GITHUB_REPO_NAME`. You can also customize `GITHUB_FILE_PATH` (default: `LangSmith_prompt_manifest.json`) and `GITHUB_BRANCH` (default: `main`).
  * **GitHub Interaction:** The `commit_manifest_to_github` function handles the logic of fetching the current file's SHA (to update it) and then committing the new manifest content.
  * **Webhook Endpoint (`/webhook/commit`):** This is the URL path your LangSmith webhook will target.
  * **Error Handling:** Basic error handling for GitHub API interactions is included.

**Deploy this server to your chosen platform (e.g., Render) and note down its public URL (e.g., `https://prompt-commit-webhook.onrender.com`).**
</Accordion>

## Configuring the webhook in LangSmith

Once your FastAPI server is deployed and you have its public URL, you can configure the webhook in LangSmith:

1. Navigate to your LangSmith workspace.

2. Go to the **Prompts** section. Here you'll see a list of your prompts.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7e61c83cdd67749970d8f0e401066d60" alt="LangSmith Prompts section" data-og-width="2996" width="2996" data-og-height="852" height="852" data-path="langsmith/images/prompt-commit-main.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f492027577eacd4131954de447fa77f2 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b5f7ea835f9cddd0724a209869c2512e 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=9aceebb242173d56a79e4974eb0fc7cd 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2ec5c08e0a4682b84a202ebb52bf981d 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7dce8f5f164d40ab0f47dbf7e308382e 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-main.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=03f8d9d0d9368ec370a71f7605f019a2 2500w" />

3. On the top right of the Prompts page, click the **+ Webhook** button.

4. You'll be presented with a form to configure your webhook:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=775cc6392de007e894c42400117d113e" alt="LangSmith Webhook configuration modal" data-og-width="3008" width="3008" data-og-height="1454" height="1454" data-path="langsmith/images/prompt-commit-webhook.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a1d037dbc657bc0758b444d94d458c91 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=708b8a9d27bd584fdd3a4a8df17be36d 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=080f345e1d97aeac9b0d15a0d762fd42 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=387781e1444e51ff64cc2b0f7f02cd26 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c855c18d16698e49737ea6f581162970 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-webhook.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=38e417a519b578f880d59a6c23efb102 2500w" />

* **Webhook URL:** Enter the full public URL of your deployed FastAPI server's endpoint. For our example server, this would be `https://prompt-commit-webhook.onrender.com/webhook/commit`.
   * **Headers (Optional):**
     * You can add custom headers that LangSmith will send with each webhook request.

5. **Test the Webhook:** LangSmith provides a "Send Test Notification" button. Use this to send a sample payload to your server. Check your server logs (e.g., on Render) to ensure it receives the request and processes it successfully (or to debug any issues).

6. **Save** the webhook configuration.

## The workflow in action

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=823988be6300f39a6e9de784b34a2a77" alt="Workflow Diagram showing: User saves prompt in LangSmith, LangSmith sends webhook to FastAPI Server, which interacts with GitHub to update files" data-og-width="2922" width="2922" data-og-height="1014" height="1014" data-path="langsmith/images/prompt-sequence-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=13a26887fccdca822c175912ed7fbd3b 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=07e46cc25aa6d0ebe76b14ce9057936b 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=887758bfcf886c3062177706b7f22fb1 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=a5aa30f606eac545f30a715eccfd80a1 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=32cae9f09ec30759550214f3c3e490ff 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-sequence-diagram.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=bced33a052aa88056de397389f9f7d64 2500w" />

Now, with everything set up, here's what happens:

1. **Prompt Modification:** A user (developer or non-technical team member) modifies a prompt in the LangSmith UI and saves it, creating a new "prompt commit."

2. **Webhook Trigger:** LangSmith detects this new prompt commit and triggers the configured webhook.

3. **HTTP Request:** LangSmith sends an HTTP POST request to the public URL of your FastAPI server (e.g., `https://prompt-commit-webhook.onrender.com/webhook/commit`). The body of this request contains the JSON prompt manifest for the entire workspace.

4. **Server Receives Payload:** Your FastAPI server's endpoint receives the request.

5. **GitHub Commit:** The server parses the JSON manifest from the request body. It then uses the configured GitHub Personal Access Token, repository owner, repository name, file path, and branch to:

* Check if the manifest file already exists in the repository on the specified branch to get its SHA (this is necessary for updating an existing file).
   * Create a new commit with the latest prompt manifest, either creating the file or updating it if it already exists. The commit message will indicate that it's an update from LangSmith.

6. **Confirmation:** You should see the new commit appear in your GitHub repository.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=213d6364ce20e4acf4e3eb7fe8c1b13d" alt="Manifest commited to GitHub" data-og-width="2982" width="2982" data-og-height="1270" height="1270" data-path="langsmith/images/prompt-commit-github.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f03e1a469e28196f66bc1f92993e6045 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=645f3cf20850a03a89a2e56a53d95719 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=418f7716a8521e3d57a4381d4ebd4d08 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=34af751c0c20b176c715f0c6f86654f1 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=13309faa18bde23df5e6df56521823fb 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-commit-github.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=f17d618b6d8e237aa8be023cccea4fd3 2500w" />

You've now successfully synced your LangSmith prompts with GitHub!

## Beyond a simple commit

Our example FastAPI server performs a direct commit of the entire prompt manifest. However, this is just the starting point. You can extend the server's functionality to perform more sophisticated actions:

* **Granular Commits:** Parse the manifest and commit changes to individual prompt files if you prefer a more granular structure in your repository.
* **Trigger CI/CD:** Instead of (or in addition to) committing, have the server trigger a CI/CD pipeline (e.g., Jenkins, GitHub Actions, GitLab CI) to deploy a staging environment, run tests, or build new application versions.
* **Update Databases/Caches:** If your application loads prompts from a database or cache, update these stores directly.
* **Notifications:** Send notifications to Slack, email, or other communication channels about prompt changes.
* **Selective Processing:** Based on metadata within the LangSmith payload (if available, e.g., which specific prompt changed or by whom), you could apply different logic.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-commit.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

<Note>
  It's important to understand that LangSmith webhooks for prompt commits are generally triggered at the **workspace level**. This means if *any* prompt within your LangSmith workspace is modified and a "prompt commit" is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.
</Note>

## Implementing a FastAPI server for webhook reception

To effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.

This publicly accessible server will be responsible for:

1. **Receiving Webhook Requests:** Listening for incoming HTTP POST requests.
2. **Parsing Payloads:** Extracting and interpreting the JSON-formatted prompt manifest from the request body.
3. **Committing to GitHub:** Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.

For deployment, platforms like [Render.com](https://render.com/) (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.

The server's core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.

<Accordion title="Minimal FastAPI Server Code ()">
  `main.py`

  This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.
```

---

## How to upload experiments run outside of LangSmith with the REST API

**URL:** llms-txt#how-to-upload-experiments-run-outside-of-langsmith-with-the-rest-api

**Contents:**
- Request body schema
- Considerations
- Example request
- View the experiment in the UI

Source: https://docs.langchain.com/langsmith/upload-existing-experiments

Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our endpoint.

This guide will show you how to upload evals using the REST API, using the `requests` library in Python as an example. However, the same principles apply to any language.

## Request body schema

Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within the experiment. Each object in the `results` represents a "row" in the experiment - a single dataset example, along with an associated run. Note that `dataset_id` and `dataset_name` refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset in LangSmith (unless that dataset was created via this endpoint).

You may use the following schema to upload experiments to the `/datasets/upload-experiment` endpoint:

The response JSON will be a dict with keys `experiment` and `dataset`, each of which is an object that contains relevant information about the experiment and dataset that was created.

You may upload multiple experiments to the same dataset by providing the same dataset\_id or dataset\_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to [use the comparison view to compare results between experiments](/langsmith/compare-experiment-results).

Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.

You must provide either a dataset\_id or a dataset\_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.

You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.

Below is an example of a simple call to the `/datasets/upload-experiment`. This is a basic example that just uses the most important fields as an illustration.

Below is the response received:

Note that the latency and feedback stats in the experiment results are null because the runs haven't had a chance to be persisted yet, which may take a few seconds. If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don't ask for this information in the request body).

## View the experiment in the UI

Now, login to the UI and click on your newly-created dataset! You should see a single experiment: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=797dd62e7cd3f833cd13bafcedfa5607" alt="Uploaded experiments table" data-og-width="3454" width="3454" data-og-height="1914" height="1914" data-path="langsmith/images/uploaded-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b1209e0ffca0c29ca3e0d7e42f0e8ac8 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a0dc70688d773066f6844e49b0654c5d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=abbe1374d734e276503394edb09aab40 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=22595d2f890ce7918da47809c2ce18cd 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=303c085b3c5e8a5c9e49f5deb54852f0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b5ee82414e1af652674022087d5dc131 2500w" />

Your examples will have been uploaded: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=093061568ece423d5a2c4cb2b5df2721" alt="Uploaded examples" data-og-width="3454" width="3454" data-og-height="1912" height="1912" data-path="langsmith/images/uploaded-dataset-examples.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2112ea321eb2b791f29b2817e4ecfa70 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8fa00a59f3ebe49c8dc413fcb8cdfff6 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3a34d7d6ab89007cfb849b3b1e9fed6e 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4949985fdd2517513234cbec7fa70274 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bdbc201e179f1ba78da86efe792837c9 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-dataset-examples.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fafd5f13e4a508ecc26f370f85a143d2 2500w" />

Clicking on your experiment will bring you to the comparison view: <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d66c9ea1cfbb1acf4f591b11f54a71da" alt="Uploaded experiment comparison view" data-og-width="3452" width="3452" data-og-height="1912" height="1912" data-path="langsmith/images/uploaded-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9839c5164dffd92bb302b1858e6f36e5 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a013685347c8915b42925027212052e5 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=513f29d729a87e70f1b1c0bc0f42c4b5 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=eb4eda99fb12b2ebee2c1f4529fa63a8 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b79aabcad20cca0bfc19cc363c6704d0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/uploaded-experiment.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b78901795d6b7886a0c251a9abba8d52 2500w" />

As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/upload-existing-experiments.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The response JSON will be a dict with keys `experiment` and `dataset`, each of which is an object that contains relevant information about the experiment and dataset that was created.

## Considerations

You may upload multiple experiments to the same dataset by providing the same dataset\_id or dataset\_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to [use the comparison view to compare results between experiments](/langsmith/compare-experiment-results).

Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.

You must provide either a dataset\_id or a dataset\_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.

You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets.

## Example request

Below is an example of a simple call to the `/datasets/upload-experiment`. This is a basic example that just uses the most important fields as an illustration.
```

Example 2 (unknown):
```unknown
Below is the response received:
```

---

## How to use Studio

**URL:** llms-txt#how-to-use-studio

**Contents:**
- Run application
- Manage assistants
- Manage threads
- Next steps

Source: https://docs.langchain.com/langsmith/use-studio

This page describes the core workflows you’ll use in Studio. It explains how to run your application, manage assistant configurations, and work with conversation threads. Each section includes steps in both graph mode (full-featured view of your graph’s execution) and chat mode (lightweight conversational interface):

* [Run application](#run-application): Execute your application or agent and observe its behavior.
* [Manage assistants](#manage-assistants): Create, edit, and select the assistant configuration used by your application.
* [Manage threads](#manage-threads): View and organize the threads, including forking or editing past runs for debugging.

<Tabs>
  <Tab title="Graph">
    ### Specify input

1. Define the input to your graph in the **Input** section on the left side of the page, below the graph interface. Studio will attempt to render a form for your input based on the graph's defined [state schema](/oss/python/langgraph/graph-api/#schema). To disable this, click the **View Raw** button, which will present you with a JSON editor.
    2. Click the up or down arrows at the top of the **Input** section to toggle through and use previously submitted inputs.

To specify the [assistant](/langsmith/assistants) that is used for the run:

1. Click the **Settings** button in the bottom left corner. If an assistant is currently selected the button will also list the assistant name. If no assistant is selected it will say **Manage Assistants**.
    2. Select the assistant to run.
    3. Click the **Active** toggle at the top of the modal to activate it.

For more information, refer to [Manage assistants](#manage-assistants).

Click the dropdown next to **Submit** and click the toggle to enable or disable streaming.

To run your graph with breakpoints:

1. Click **Interrupt**.
    2. Select a node and whether to pause before or after that node has executed.
    3. Click **Continue** in the thread log to resume execution.

For more information on breakpoints, refer to [Human-in-the-loop](/oss/python/langchain/human-in-the-loop).

To submit the run with the specified input and run settings:

1. Click the **Submit** button. This will add a [run](/langsmith/assistants#execution) to the existing selected [thread](/oss/python/langgraph/persistence#threads). If no thread is currently selected, a new one will be created.
    2. To cancel the ongoing run, click the **Cancel** button.
  </Tab>

<Tab title="Chat">
    Specify the input to your chat application in the bottom of the conversation panel.

1. Click the **Send message** button to submit the input as a Human message and have the response streamed back.

To cancel the ongoing run:

1. Click **Cancel**.
    2. Click the **Show tool calls** toggle to hide or show tool calls in the conversation.
  </Tab>
</Tabs>

Studio lets you view, edit, and update your assistants, and allows you to run your graph using these assistant configurations.

For more conceptual details, refer to the [Assistants overview](/langsmith/assistants/).

<Tabs>
  <Tab title="Graph">
    To view your assistants:

1. Click **Manage Assistants** in the bottom left corner. This opens a modal for you to view all the assistants for the selected graph.
    2. Specify the assistant and its version you would like to mark as **Active**. LangSmith will use this assistant when runs are submitted.

The **Default configuration** option will be active, which reflects the default configuration defined in your graph. Edits made to this configuration will be used to update the run-time configuration, but will not update or create a new assistant unless you click **Create new assistant**.
  </Tab>

<Tab title="Chat">
    Chat mode enables you to switch through the different assistants in your graph via the dropdown selector at the top of the page. To create, edit, or delete assistants, use Graph mode.
  </Tab>
</Tabs>

Studio provides tools to view all [threads](/oss/python/langgraph/persistence#threads) saved on the server and edit their state. You can create new threads, switch between threads, and modify past states both in graph mode and chat mode.

<Tabs>
  <Tab title="Graph">
    ### View threads

1. In the top of the right-hand pane, select the dropdown menu to view existing threads.
    2. Select the desired thread, and the thread history will populate in the right-hand side of the page.
    3. To create a new thread, click **+ New Thread** and [submit a run](#run-application).
    4. To view more granular information in the thread, drag the slider at the top of the page to the right. To view less information, drag the slider to the left. Additionally, collapse or expand individual turns, nodes, and keys of the state.
    5. Switch between `Pretty` and `JSON` mode for different rendering formats.

### Edit thread history

To edit the state of the thread:

1. Select <Icon icon="pencil" /> **Edit node state** next to the desired node.
    2. Edit the node's output as desired and click **Fork** to confirm. This will create a new forked run from the checkpoint of the selected node.

If you instead want to re-run the thread from a given checkpoint without editing the state, click **Re-run from here**. This will again create a new forked run from the selected checkpoint. This is useful for re-running with changes that are not specific to the state, such as the selected assistant.
  </Tab>

<Tab title="Chat">
    1. View all threads in the right-hand pane of the page.
    2. Select the desired thread and the thread history will populate in the center panel.
    3. To create a new thread, click **+** and submit a run.

To edit a human message in the thread:

1. Click <Icon icon="pencil" /> **Edit node state** below the human message.
    2. Edit the message as desired and submit. This will create a new fork of the conversation history.
    3. To re-generate an AI message, click the retry icon below the AI message.
  </Tab>
</Tabs>

Refer to the following guides for more detail on tasks you can complete in Studio:

* [Iterate on prompts](/langsmith/observability-studio)
* [Run experiments over datasets](/langsmith/observability-studio#run-experiments-over-a-dataset)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-studio.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## How to use the REST API

**URL:** llms-txt#how-to-use-the-rest-api

**Contents:**
- Create a dataset

Source: https://docs.langchain.com/langsmith/run-evals-api-only

The [Python](https://reference.langchain.com/python/langsmith/) and [TypeScript](https://reference.langchain.com/javascript/modules/langsmith.html) SDKs are the recommended way to run [evaluations](/langsmith/evaluation-concepts) in LangSmith. They include optimizations and features that enhance performance and reliability.

If you cannot use the SDKs—for example, if you are working in a different language or a restricted environment—you can use the REST API directly. This guide demonstrates how to run evaluations using the [REST API](https://api.smith.langchain.com/redoc) with Python's [`requests`](https://requests.readthedocs.io/) library, but the same principles apply to any language.

Before diving into this content, it might be helpful to read the following:

* [Evaluate LLM applications](/langsmith/evaluate-llm-application).
* [LangSmith API Reference](https://api.smith.langchain.com/redoc): Complete API documentation for all endpoints used in this guide.

For this example, we use the Python SDK to create a [dataset](/langsmith/evaluation-concepts#datasets) quickly. To create datasets via the API or UI instead, refer to [Managing datasets](/langsmith/manage-datasets-in-application).

```python  theme={null}
import os
import requests

from datetime import datetime
from langsmith import Client
from openai import OpenAI
from uuid import uuid4

client = Client()
oa_client = OpenAI()

---

## How we are sampling runs to include in our dataset

**URL:** llms-txt#how-we-are-sampling-runs-to-include-in-our-dataset

**Contents:**
  - Convert runs to experiment

end_time = datetime.now(tz=timezone.utc)
start_time = end_time - timedelta(days=1)
run_filter = f'and(gt(start_time, "{start_time.isoformat()}"), lt(end_time, "{end_time.isoformat()}"))'
prod_runs = list(
    client.list_runs(
        project_name=project_name,
        is_root=True,
        filter=run_filter,
    )
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Convert runs to experiment

`convert_runs_to_test` is a function which takes some runs and does the following:

1. The inputs, and optionally the outputs, are saved to a dataset as Examples.
2. The inputs and outputs are stored as an experiment, as if you had run the `evaluate` function and received those outputs.
```

---

## Hugging Face

**URL:** llms-txt#hugging-face

**Contents:**
- Chat models
  - ChatHuggingFace
- LLMs
  - HuggingFaceEndpoint
  - HuggingFacePipeline
- Embedding Models
  - HuggingFaceEmbeddings
  - HuggingFaceEndpointEmbeddings
  - HuggingFaceInferenceAPIEmbeddings
  - HuggingFaceInstructEmbeddings

Source: https://docs.langchain.com/oss/python/integrations/providers/huggingface

This page covers all LangChain integrations with [Hugging Face Hub](https://huggingface.co/) and libraries like [transformers](https://huggingface.co/docs/transformers/index), [sentence transformers](https://sbert.net/), and [datasets](https://huggingface.co/docs/datasets/index).

We can use the `Hugging Face` LLM classes or directly use the `ChatHuggingFace` class.

See a [usage example](/oss/python/integrations/chat/huggingface).

### HuggingFaceEndpoint

We can use the `HuggingFaceEndpoint` class to run open source models via serverless [Inference Providers](https://huggingface.co/docs/inference-providers) or via dedicated [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/llms/huggingface_endpoint).

### HuggingFacePipeline

We can use the `HuggingFacePipeline` class to run open source models locally.

See a [usage example](/oss/python/integrations/llms/huggingface_pipelines).

### HuggingFaceEmbeddings

We can use the `HuggingFaceEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceEndpointEmbeddings

We can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated [Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceInferenceAPIEmbeddings

We can use the `HuggingFaceInferenceAPIEmbeddings` class to run open source embedding models via [Inference Providers](https://huggingface.co/docs/inference-providers).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).

### HuggingFaceInstructEmbeddings

We can use the `HuggingFaceInstructEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/instruct_embeddings).

### HuggingFaceBgeEmbeddings

> [BGE models on the HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5) are one of [the best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard).
> BGE model is created by the [Beijing Academy of Artificial Intelligence (BAAI)](https://en.wikipedia.org/wiki/Beijing_Academy_of_Artificial_Intelligence). `BAAI` is a private non-profit organization engaged in AI research and development.

See a [usage example](/oss/python/integrations/text_embedding/bge_huggingface).

### Hugging Face dataset

> [Hugging Face Hub](https://huggingface.co/docs/hub/index) is home to over 75,000
> [datasets](https://huggingface.co/docs/hub/index#datasets) in more than 100 languages
> that can be used for a broad range of tasks across NLP, Computer Vision, and Audio.
> They used for a diverse range of tasks such as translation, automatic speech
> recognition, and image classification.

We need to install `datasets` python package.

See a [usage example](/oss/python/integrations/document_loaders/hugging_face_dataset).

### Hugging Face model loader

> Load model information from `Hugging Face Hub`, including README content.
>
> This loader interfaces with the `Hugging Face Models API` to fetch
> and load model metadata and README files.
> The API allows you to search and filter models based on
> specific criteria such as model tags, authors, and more.

It uses the Hugging Face models to generate image captions.

We need to install several python packages.

See a [usage example](/oss/python/integrations/document_loaders/image_captions).

### Hugging Face Hub Tools

> [Hugging Face Tools](https://huggingface.co/docs/transformers/v4.29.0/en/custom_tools)
> support text I/O and are loaded using the `load_huggingface_tool` function.

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/huggingface_tools).

### Hugging Face Text-to-Speech Model Inference.

> It is a wrapper around `OpenAI Text-to-Speech API`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/huggingface.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## LLMs

### HuggingFaceEndpoint

We can use the `HuggingFaceEndpoint` class to run open source models via serverless [Inference Providers](https://huggingface.co/docs/inference-providers) or via dedicated [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/llms/huggingface_endpoint).
```

Example 2 (unknown):
```unknown
### HuggingFacePipeline

We can use the `HuggingFacePipeline` class to run open source models locally.

See a [usage example](/oss/python/integrations/llms/huggingface_pipelines).
```

Example 3 (unknown):
```unknown
## Embedding Models

### HuggingFaceEmbeddings

We can use the `HuggingFaceEmbeddings` class to run open source embedding models locally.

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).
```

Example 4 (unknown):
```unknown
### HuggingFaceEndpointEmbeddings

We can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated [Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated).

See a [usage example](/oss/python/integrations/text_embedding/huggingfacehub).
```

---

## Human-in-the-loop

**URL:** llms-txt#human-in-the-loop

**Contents:**
- Interrupt decision types
- Configuring interrupts
- Responding to interrupts

Source: https://docs.langchain.com/oss/python/langchain/human-in-the-loop

The Human-in-the-Loop (HITL) [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) lets you add human oversight to agent tool calls.
When a model proposes an action that might require review — for example, writing to a file or executing SQL — the middleware can pause execution and wait for a decision.

It does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) that halts execution. The graph state is saved using LangGraph's [persistence layer](/oss/python/langgraph/persistence), so execution can pause safely and resume later.

A human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`).

## Interrupt decision types

The [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) defines three built-in ways a human can respond to an interrupt:

| Decision Type | Description                                                               | Example Use Case                                    |
| ------------- | ------------------------------------------------------------------------- | --------------------------------------------------- |
| ✅ `approve`   | The action is approved as-is and executed without changes.                | Send an email draft exactly as written              |
| ✏️ `edit`     | The tool call is executed with modifications.                             | Change the recipient before sending an email        |
| ❌ `reject`    | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |

The available decision types for each tool depend on the policy you configure in `interrupt_on`.
When multiple tool calls are paused at the same time, each action requires a separate decision.
Decisions must be provided in the same order as the actions appear in the interrupt request.

<Tip>
  When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.
</Tip>

## Configuring interrupts

To use HITL, add the [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) to the agent's `middleware` list when creating the agent.

You configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.

<Info>
  You must configure a checkpointer to persist the graph state across interrupts.
  In production, use a persistent checkpointer like [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver). For testing or prototyping, use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver).

When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.
  See the [LangGraph interrupts documentation](/oss/python/langgraph/interrupts) for details.
</Info>

<Accordion title="Configuration options">
  <ParamField body="interrupt_on" type="dict" required>
    Mapping of tool names to approval configs. Values can be `True` (interrupt with default config), `False` (auto-approve), or an `InterruptOnConfig` object.
  </ParamField>

<ParamField body="description_prefix" type="string" default="Tool execution requires approval">
    Prefix for action request descriptions
  </ParamField>

**`InterruptOnConfig` options:**

<ParamField body="allowed_decisions" type="list[string]">
    List of allowed decisions: `'approve'`, `'edit'`, or `'reject'`
  </ParamField>

<ParamField body="description" type="string | callable">
    Static string or callable function for custom description
  </ParamField>
</Accordion>

## Responding to interrupts

When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.

```python  theme={null}
from langgraph.types import Command

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  You must configure a checkpointer to persist the graph state across interrupts.
  In production, use a persistent checkpointer like [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver). For testing or prototyping, use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver).

  When invoking the agent, pass a `config` that includes the **thread ID** to associate execution with a conversation thread.
  See the [LangGraph interrupts documentation](/oss/python/langgraph/interrupts) for details.
</Info>

<Accordion title="Configuration options">
  <ParamField body="interrupt_on" type="dict" required>
    Mapping of tool names to approval configs. Values can be `True` (interrupt with default config), `False` (auto-approve), or an `InterruptOnConfig` object.
  </ParamField>

  <ParamField body="description_prefix" type="string" default="Tool execution requires approval">
    Prefix for action request descriptions
  </ParamField>

  **`InterruptOnConfig` options:**

  <ParamField body="allowed_decisions" type="list[string]">
    List of allowed decisions: `'approve'`, `'edit'`, or `'reject'`
  </ParamField>

  <ParamField body="description" type="string | callable">
    Static string or callable function for custom description
  </ParamField>
</Accordion>

## Responding to interrupts

When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt_on`. In that case, the invocation result will include an `__interrupt__` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.
```

---

## Human-in-the-loop leverages LangGraph's persistence layer.

**URL:** llms-txt#human-in-the-loop-leverages-langgraph's-persistence-layer.

---

## Human-in-the-loop requires a thread ID for persistence

**URL:** llms-txt#human-in-the-loop-requires-a-thread-id-for-persistence

config = {"configurable": {"thread_id": "some_id"}}

---

## Human-in-the-loop using server API

**URL:** llms-txt#human-in-the-loop-using-server-api

**Contents:**
- Dynamic interrupts
- Static interrupts
- Learn more

Source: https://docs.langchain.com/langsmith/add-human-in-the-loop

To review, edit, and approve tool calls in an agent or workflow, use LangGraph's [human-in-the-loop](/oss/python/langgraph/interrupts) features.

## Dynamic interrupts

<Tabs>
  <Tab title="Python">

1. The graph is invoked with some initial state.
    2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
       3\. The graph is resumed with a `Command(resume=...)`, injecting the human's input and continuing execution.
  </Tab>

<Tab title="JavaScript">

1. The graph is invoked with some initial state.
    2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
    3. The graph is resumed with a `{ resume: ... }` command object, injecting the human's input and continuing execution.
  </Tab>

<Tab title="cURL">
    Create a thread:

Run the graph until the interrupt is hit.:

<Accordion title="Extended example: using `interrupt`">
  This is an example graph you can run in the Agent Server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.

1. `interrupt(...)` pauses execution at `human_node`, surfacing the given payload to a human.
  2. Any JSON serializable value can be passed to the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function. Here, a dict containing the text to revise.
  3. Once resumed, the return value of `interrupt(...)` is the human-provided input, which is used to update the state.

Once you have a running Agent Server, you can interact with it using
  [LangGraph SDK](/langsmith/langgraph-python-sdk)

<Tabs>
    <Tab title="Python">

1. The graph is invoked with some initial state.
      2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
         3\. The graph is resumed with a `Command(resume=...)`, injecting the human's input and continuing execution.
    </Tab>

<Tab title="JavaScript">

1. The graph is invoked with some initial state.
      2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
      3. The graph is resumed with a `{ resume: ... }` command object, injecting the human's input and continuing execution.
    </Tab>

<Tab title="cURL">
      Create a thread:

Run the graph until the interrupt is hit:

</Tab>
  </Tabs>
</Accordion>

Static interrupts (also known as static breakpoints) are triggered either before or after a node executes.

<Warning>
  Static interrupts are **not** recommended for human-in-the-loop workflows. They are best used for debugging and testing.
</Warning>

You can set static interrupts by specifying `interrupt_before` and `interrupt_after` at compile time:

1. The breakpoints are set during `compile` time.
2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.

Alternatively, you can set static interrupts at run time:

<Tabs>
  <Tab title="Python">

1. `client.runs.wait` is called with the `interrupt_before` and `interrupt_after` parameters. This is a run-time configuration and can be changed for every invocation.
    2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
    3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.
  </Tab>

<Tab title="JavaScript">

1. `client.runs.wait` is called with the `interruptBefore` and `interruptAfter` parameters. This is a run-time configuration and can be changed for every invocation.
    2. `interruptBefore` specifies the nodes where execution should pause before the node is executed.
    3. `interruptAfter` specifies the nodes where execution should pause after the node is executed.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

The following example shows how to add static interrupts:

<Tabs>
  <Tab title="Python">

1. The graph is run until the first breakpoint is hit.
    2. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>

<Tab title="JavaScript">

1. The graph is run until the first breakpoint is hit.
    2. The graph is resumed by passing in `null` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>

<Tab title="cURL">
    Create a thread:

Run the graph until the breakpoint:

* [Human-in-the-loop conceptual guide](/oss/python/langgraph/interrupts): learn more about LangGraph human-in-the-loop features.
* [Common patterns](/oss/python/langgraph/interrupts#common-patterns): learn how to implement patterns like approving/rejecting actions, requesting user input, tool call review, and validating human input.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/add-human-in-the-loop.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
1. The graph is invoked with some initial state.
    2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
       3\. The graph is resumed with a `Command(resume=...)`, injecting the human's input and continuing execution.
  </Tab>

  <Tab title="JavaScript">
```

Example 2 (unknown):
```unknown
1. The graph is invoked with some initial state.
    2. When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.
    3. The graph is resumed with a `{ resume: ... }` command object, injecting the human's input and continuing execution.
  </Tab>

  <Tab title="cURL">
    Create a thread:
```

Example 3 (unknown):
```unknown
Run the graph until the interrupt is hit.:
```

Example 4 (unknown):
```unknown
Resume the graph:
```

---

## Hybrid

**URL:** llms-txt#hybrid

**Contents:**
  - Workflow
  - Architecture
  - Compute Platforms
  - Egress to LangSmith and the control plane
- Listeners
  - Kubernetes cluster organization
  - LangSmith workspace organization
- Use Cases
  - Each LangSmith workspace → separate Kubernetes cluster
  - One cluster, one namespace per workspace

Source: https://docs.langchain.com/langsmith/hybrid

<Info>
  **Important**
  The hybrid option requires an [Enterprise](https://langchain.com/pricing) plan.
</Info>

The **hybrid** model splits LangSmith infrastructure between LangChain's cloud and yours:

* **Control plane** (LangSmith UI, APIs, and orchestration) runs in LangChain's cloud, managed by LangChain.
* **Data plane** (your <Tooltip tip="The server that runs your applications.">Agent Servers</Tooltip> and agent workloads) runs in your cloud, managed by you.

This combines the convenience of a managed interface with the flexibility of running workloads in your own environment.

<Note>
  Learn more about the [control plane](/langsmith/control-plane), [data plane](/langsmith/data-plane), and [Agent Server](/langsmith/agent-server) architecture concepts.
</Note>

| Component                                                                                                | Responsibilities                                                                                                                                    | Where it runs     | Who manages it |
| -------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------- | -------------- |
| <Tooltip tip="The LangSmith UI and APIs for managing deployments.">Control plane</Tooltip>               | <ul><li>UI for creating deployments and revisions</li><li>APIs for managing deployments</li><li>Observability data storage</li></ul>                | LangChain's cloud | LangChain      |
| <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">Data plane</Tooltip> | <ul><li>Operator/listener to reconcile deployments</li><li>Agent Servers (agents/graphs)</li><li>Backing services (Postgres, Redis, etc.)</li></ul> | Your cloud        | You            |

When running LangSmith in a hybrid model, you authenticate with a [LangSmith API key](/langsmith/create-account-api-key).

1. Use the `langgraph-cli` or [Studio](/langsmith/studio) to test your graph locally.
2. Build a Docker image using the `langgraph build` command.
3. Deploy your Agent Server from the [control plane UI](/langsmith/control-plane#control-plane-ui).

<Note>
  Supported Compute Platforms: [Kubernetes](https://kubernetes.io/).<br />
  For setup, refer to the [Hybrid setup guide](/langsmith/deploy-hybrid).
</Note>

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=86d548632d33be3644bad7213287ac78" alt="Hybrid deployment: LangChain-hosted control plane (LangSmith UI/APIs) manages deployments. Your cloud runs a listener, Agent Server instances, and backing stores (Postgres/Redis) on Kubernetes." data-og-width="1784" width="1784" data-og-height="1782" height="1782" data-path="langsmith/images/hybrid-with-deployment-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=2fe7b82524e32a2ce1e3726ad3bce553 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=807a35d47b9c8e740a96f0a8aa4389a1 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=84333efa9a9e83305b93f4b6e770b2f8 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=1d8bd0547f7814cad914b1ddc6dbfa48 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=09f181972952ab4362b3ac70b7934d59 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-light.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=e2d292d67dbf1fdb68758fac293c0cc7 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=829f0ef40c315c493ef8e30857e9abf5" alt="Hybrid deployment: LangChain-hosted control plane (LangSmith UI/APIs) manages deployments. Your cloud runs a listener, Agent Server instances, and backing stores (Postgres/Redis) on Kubernetes." data-og-width="1784" width="1784" data-og-height="1782" height="1782" data-path="langsmith/images/hybrid-with-deployment-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=bdb7a126e3914a07ed1ff72b66e50e9a 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=14f4f01c71edca5ce1594f3f2145f0e4 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=04f8b60076c4ff6263af77da5a65ccc1 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=50fe58a42273562591bf695d5cdbfe57 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=20025a5634783e2eb1d2ba177724ccc6 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/hybrid-with-deployment-dark.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=79e4e542803c26c38e5fffaf2bc961bf 2500w" />

### Compute Platforms

* **Kubernetes**: Hybrid supports running the data plane on any Kubernetes cluster.

<Tip>
  For setup in Kubernetes, refer to the [Hybrid setup guide](/langsmith/deploy-hybrid)
</Tip>

### Egress to LangSmith and the control plane

In the hybrid deployment model, your self-hosted data plane will send network requests to the control plane to poll for changes that need to be implemented in the data plane. Traces from data plane deployments also get sent to the LangSmith instance integrated with the control plane. This traffic to the control plane is encrypted, over HTTPS. The data plane authenticates with the control plane with a LangSmith API key.

In order to enable this egress, you may need to update internal firewall rules or cloud resources (such as Security Groups) to [allow certain IP addresses](/langsmith/cloud#ingress-into-langchain-saas).

<Warning>
  AWS/Azure PrivateLink or GCP Private Service Connect is currently not supported. This traffic will go over the internet.
</Warning>

In the hybrid option, one or more ["listener" applications](/langsmith/data-plane#”listener”-application) can run depending on how your LangSmith workspaces and Kubernetes clusters are organized.

### Kubernetes cluster organization

* One or more listeners can run in a Kubernetes cluster.
* A listener can deploy into one or more namespaces in that cluster.
* Multiple listeners cannot deploy to the same namespace.
* Cluster owners are responsible for planning listener layout and Agent Server deployments.

### LangSmith workspace organization

* A workspace can be associated with one or more listeners.
* A listener can only be associated with one workspace. LangSmith workspace to listener is a one-to-many relationship.
* A workspace can only deploy to Kubernetes clusters where all of its listeners are deployed.

Here are some common listener configurations (not strict requirements):

### Each LangSmith workspace → separate Kubernetes cluster

* Cluster `alpha` runs workspace `A`
* Cluster `beta` runs workspace `B`

### One cluster, one namespace per workspace

* Cluster `alpha`, namespace `1` runs workspace `A`
* Cluster `alpha`, namespace `2` runs workspace `B`

### Separate clusters, with shared “dev” cluster

* Cluster `alpha` runs workspace `A`
* Cluster `beta` runs workspace `B`
* Cluster `dev` runs workspaces `A` and `B`
* Both workspaces have two listeners; cluster `dev` has two listener deployments

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/hybrid.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## if configured with a subdomain / path prefix:

**URL:** llms-txt#if-configured-with-a-subdomain-/-path-prefix:

**Contents:**
  - Process the API response with jq

curl http://<langsmith_url/prefix/api/v1/info
json  theme={null}
{
  "version": "0.11.4",
  "license_expiration_time": "2026-08-18T19:14:34Z",
  "customer_info": {
    "customer_id": "<id>",
    "customer_name": "<name>"
  }
}
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
This will return a JSON response containing your customer information:
```

Example 2 (unknown):
```unknown
Extract the `customer_id` and `customer_name` from this response to use as input for the export scripts.

### Process the API response with jq

You can use [jq](https://jqlang.org/download) to parse the JSON response and set bash variables for use in your scripts:
```

---

## If desired, specify custom instructions

**URL:** llms-txt#if-desired,-specify-custom-instructions

**Contents:**
  - RAG chains
- Next steps

prompt = (
    "You have access to a tool that retrieves context from a blog post. "
    "Use the tool to help answer user queries."
)
agent = create_agent(model, tools, system_prompt=prompt)
python  theme={null}
query = (
    "What is the standard method for Task Decomposition?\n\n"
    "Once you get the answer, look up common extensions of that method."
)

for event in agent.stream(
    {"messages": [{"role": "user", "content": query}]},
    stream_mode="values",
):
    event["messages"][-1].pretty_print()

================================ Human Message =================================

What is the standard method for Task Decomposition?

Once you get the answer, look up common extensions of that method.
================================== Ai Message ==================================
Tool Calls:
  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)
 Call ID: call_d6AVxICMPQYwAKj9lgH4E337
  Args:
    query: standard method for Task Decomposition
================================= Tool Message =================================
Name: retrieve_context

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Task decomposition can be done...

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Component One: Planning...
================================== Ai Message ==================================
Tool Calls:
  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)
 Call ID: call_0dbMOw7266jvETbXWn4JqWpR
  Args:
    query: common extensions of the standard method for Task Decomposition
================================= Tool Message =================================
Name: retrieve_context

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Task decomposition can be done...

Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}
Content: Component One: Planning...
================================== Ai Message ==================================

The standard method for Task Decomposition often used is the Chain of Thought (CoT)...
python  theme={null}
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt
def prompt_with_context(request: ModelRequest) -> str:
    """Inject context into state messages."""
    last_query = request.state["messages"][-1].text
    retrieved_docs = vector_store.similarity_search(last_query)

docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

system_message = (
        "You are a helpful assistant. Use the following context in your response:"
        f"\n\n{docs_content}"
    )

return system_message

agent = create_agent(model, tools=[], middleware=[prompt_with_context])
python  theme={null}
query = "What is task decomposition?"
for step in agent.stream(
    {"messages": [{"role": "user", "content": query}]},
    stream_mode="values",
):
    step["messages"][-1].pretty_print()

================================ Human Message =================================

What is task decomposition?
================================== Ai Message ==================================

Task decomposition is...
python  theme={null}
  from typing import Any
  from langchain_core.documents import Document
  from langchain.agents.middleware import AgentMiddleware, AgentState

class State(AgentState):
      context: list[Document]

class RetrieveDocumentsMiddleware(AgentMiddleware[State]):
      state_schema = State

def before_model(self, state: AgentState) -> dict[str, Any] | None:
          last_message = state["messages"][-1]
          retrieved_docs = vector_store.similarity_search(last_message.text)

docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

augmented_message_content = (
              f"{last_message.text}\n\n"
              "Use the following context to answer the query:\n"
              f"{docs_content}"
          )
          return {
              "messages": [last_message.model_copy(update={"content": augmented_message_content})],
              "context": retrieved_docs,
          }

agent = create_agent(
      model,
      tools=[],
      middleware=[RetrieveDocumentsMiddleware()],
  )
  ```
</Accordion>

Now that we've implemented a simple RAG application via [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), we can easily incorporate new features and go deeper:

* [Stream](/oss/python/langchain/streaming) tokens and other information for responsive user experiences
* Add [conversational memory](/oss/python/langchain/short-term-memory) to support multi-turn interactions
* Add [long-term memory](/oss/python/langchain/long-term-memory) to support memory across conversational threads
* Add [structured responses](/oss/python/langchain/structured-output)
* Deploy your application with [LangSmith Deployment](/langsmith/deployments)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/rag.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
Note that the agent:

1. Generates a query to search for a standard method for task decomposition;
2. Receiving the answer, generates a second query to search for common extensions of it;
3. Having received all necessary context, answers the question.

We can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).

<Tip>
  You can add a deeper level of control and customization using the [LangGraph](/oss/python/langgraph/overview) framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](/oss/python/langgraph/agentic-rag) for more advanced formulations.
</Tip>

### RAG chains

In the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](/oss/python/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:

| ✅ Benefits                                                                                                                                                 | ⚠️ Drawbacks                                                                                                                                |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **Search only when needed** – The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.                        | **Two inference calls** – When a search is performed, it requires one call to generate the query and another to produce the final response. |
| **Contextual search queries** – By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | **Reduced control** – The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.                    |
| **Multiple searches allowed** – The LLM can execute several searches in support of a single user query.                                                    |                                                                                                                                             |

Another common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.

In this approach we no longer call the model in a loop, but instead make a single pass.

We can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:
```

Example 4 (unknown):
```unknown
Let's try this out:
```

---

## If "env" is set to "test", then we don't actually delete any rows from our database.

**URL:** llms-txt#if-"env"-is-set-to-"test",-then-we-don't-actually-delete-any-rows-from-our-database.

---

## If prod tag points to commit a1b2c3d4, this is equivalent to:

**URL:** llms-txt#if-prod-tag-points-to-commit-a1b2c3d4,-this-is-equivalent-to:

**Contents:**
- Trigger a webhook on prompt commit
  - Configure a webhook
  - Trigger the webhook
- Public prompt hub

prompt = client.pull_prompt("joke-generator:a1b2c3d4")
```

For more information on how to use prompts in code, refer to [Managing prompts programmatically](/langsmith/manage-prompts-programmatically).

## Trigger a webhook on prompt commit

You can configure a webhook to be triggered whenever a commit is made to a prompt.

Some common use cases of this include:

* Triggering a CI/CD pipeline when prompts are updated.
* Synchronizing prompts with a GitHub repository.
* Notifying team members about prompt modifications.

### Configure a webhook

Navigate to the **Prompts** section in the left-hand sidebar or from the application homepage. In the top right corner, click on the `+ Webhook` button.

Add a webhook URL and any required headers.

<Note>
  You can only configure one webhook per workspace. If you want to configure multiple per workspace or set up a different webhook for each prompt, let us know in the [LangChain Forum](https://forum.langchain.com/).
</Note>

To test out your webhook, click the **Send test notification** button. This will send a test notification to the webhook URL you provided with a sample payload.

The sample payload is a JSON object with the following fields:

* `prompt_id`: The ID of the prompt that was committed.
* `prompt_name`: The name of the prompt that was committed.
* `commit_hash`: The commit hash of the prompt.
* `created_at`: The date of the commit.
* `created_by`: The author of the commit.
* `manifest`: The manifest of the prompt.

### Trigger the webhook

Commit to a prompt to trigger the webhook you've configured.

#### Use the Playground

If you do this in the Playground, you'll be prompted to deselect the webhooks you'd like to avoid triggering.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=84f487c929ab7894bbd1e2c8922b6a9e" alt="Commit prompt playground" data-og-width="736" width="736" data-og-height="540" height="540" data-path="langsmith/images/commit-prompt-playground.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=68e38eb869ab7f97b5e53bb793a0ba24 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=84bbdbb22d53ca87b9e49b164d4dd190 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f679941d9c42a52f30c858a00174749d 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c8ba79844ab070cd1a7b443677bbef4a 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d011a2ebe87104d006e71470204d158d 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-prompt-playground.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cdfcfddfdad692fd623b502efdd6589d 2500w" />

If you commit via the API, you can specify to skip triggering the webhook by setting the `skip_webhooks` parameter to `true` or to an array of webhook ids to ignore. Refer to the [API docs](https://api.smith.langchain.com/redoc#tag/commits/operation/create_commit_api_v1_commits__owner___repo__post) for more information.

LangSmith's public prompt hub is a collection of prompts that have been created by the LangChain community that you can use for reference.

<Note>
  Note that prompts are user-generated and unverified. LangChain does not review or endorse public prompts, use these at your own risk. Use of Prompt Hub is subject to our [Terms of Service](https://www.langchain.com/terms-of-service).
</Note>

Navigate to the **Prompts** section of the left-hand sidebar and click on **Browse all Public Prompts in the LangChain Hub**.

Here you'll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the prompt's details, and run the prompt in the Playground. You can [pull any public prompt into your code](/langsmith/manage-prompts-programmatically) using the SDK.

To view prompts tied to your workspace, visit the **Prompts** tab in the sidebar.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d689013c2158309249c547086e145783" alt="Prompts tab" data-og-width="3012" width="3012" data-og-height="1704" height="1704" data-path="langsmith/images/prompts-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ad811ae37a565d22851d6162d2f45e07 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d48f24e8fcc7d5813e7093f117c21cac 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=b93e12cdf73897ab5373eb5019ac58cf 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cb361802a4bb5d823dda53605b647567 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8450e66dcc99a90ea84b1fc66bdfc6ad 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompts-tab.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5de8982f6602902e9fa239ba3f4157d4 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-prompts.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## If you have a higher tiered Tavily API plan you can increase this

**URL:** llms-txt#if-you-have-a-higher-tiered-tavily-api-plan-you-can-increase-this

rate_limiter = InMemoryRateLimiter(requests_per_second=0.08)

---

## If you set OTEL_EXPORTER_OTLP_TRACES_ENDPOINT or OTEL_EXPORTER_OTLP_ENDPOINT,

**URL:** llms-txt#if-you-set-otel_exporter_otlp_traces_endpoint-or-otel_exporter_otlp_endpoint,

---

## Image block

**URL:** llms-txt#image-block

**Contents:**
  - Serialize standard content
- Simplified package
  - Namespace
  - `langchain-classic`
- Breaking changes
  - Dropped Python 3.9 support
  - Updated return type for chat models
  - Default message format for OpenAI Responses API

image_block = {
    "type": "image",
    "url": "https://example.com/image.png",
    "mime_type": "image/png",
}
bash Environment variable theme={null}
  export LC_OUTPUT_VERSION=v1
  python Initialization parameter theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model(
      "gpt-5-nano",
      output_version="v1",
  )
  python v1 (new) theme={null}
  # Chains
  from langchain_classic.chains import LLMChain

# Retrievers
  from langchain_classic.retrievers import ...

# Indexing
  from langchain_classic.indexes import ...

# Hub
  from langchain_classic import hub
  python v0 (old) theme={null}
  # Chains
  from langchain_classic.chains import LLMChain

# Retrievers
  from langchain.retrievers import ...

# Indexing
  from langchain.indexes import ...

# Hub
  from langchain import hub
  bash  theme={null}
uv pip install langchain-classic
python v1 (new) theme={null}
  def bind_tools(
          ...
      ) -> Runnable[LanguageModelInput, AIMessage]:
  python v0 (old) theme={null}
  def bind_tools(
          ...
      ) -> Runnable[LanguageModelInput, BaseMessage]:
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
See the content blocks [reference](/oss/python/langchain/messages#content-block-reference) for more details.

### Serialize standard content

Standard content blocks are **not serialized** into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

<Note>
  Learn more: [Messages](/oss/python/langchain/messages#message-content), [Standard content blocks](/oss/python/langchain/messages#standard-content-blocks), and [Multimodal](/oss/python/langchain/messages#multimodal).
</Note>

***

## Simplified package

The `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality.

### Namespace

| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                             |
| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- |
| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality |
| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from `langchain-core` |
| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from `langchain-core` |
| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization      |
| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                  |

### `langchain-classic`

If you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports:

* Legacy chains (`LLMChain`, `ConversationChain`, etc.)
* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
* The indexing API
* The hub module (for managing prompts programmatically)
* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings)
* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports
* Other deprecated functionality

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "image_inputs": True,

**URL:** llms-txt#"image_inputs":-true,

---

## Implement a CI/CD pipeline using LangSmith Deployment and Evaluation

**URL:** llms-txt#implement-a-ci/cd-pipeline-using-langsmith-deployment-and-evaluation

**Contents:**
- Overview
- Pipeline architecture
  - Trigger sources
  - Testing layers
- GitHub Actions workflow
  - Prerequisites
- Deployment options
  - Prerequisites for manual deployment
  - Local development and testing

Source: https://docs.langchain.com/langsmith/cicd-pipeline-example

This guide demonstrates how to implement a comprehensive CI/CD pipeline for AI agent applications deployed in LangSmith Deployment. In this example, you'll use the [LangGraph](/oss/python/langgraph/overview) open source framework for orchestrating and building the agent, [LangSmith](/langsmith/home) for observability and evaluations. This pipeline is based on the [cicd-pipeline-example repository](https://github.com/langchain-ai/cicd-pipeline-example).

The CI/CD pipeline provides:

* <Icon icon="check-circle" /> **Automated testing**: Unit, integration, and end-to-end tests.
* <Icon icon="chart-line" /> **Offline evaluations**: Performance assessment using [AgentEvals](https://github.com/langchain-ai/agentevals), [OpenEvals](https://github.com/langchain-ai/openevals) and [LangSmith](https://docs.langchain.com/langsmith/home).
* <Icon icon="rocket" /> **Preview and production deployments**: Automated staging and quality-gated production releases using the Control Plane API.
* <Icon icon="eye" /> **Monitoring**: Continuous evaluation and alerting.

## Pipeline architecture

The CI/CD pipeline consists of several key components that work together to ensure code quality and reliable deployments:

There are multiple ways you can trigger this pipeline, either during development or if your application is already live. The pipeline can be triggered by:

* <Icon icon="code-branch" /> **Code changes**: Pushes to main/development branches where you can modify the LangGraph architecture, try different models, update agent logic, or make any code improvements.
* <Icon icon="edit" /> **PromptHub updates**: Changes to prompt templates stored in LangSmith PromptHub—whenever there's a new prompt commit, the system triggers a webhook to run the pipeline.
* <Icon icon="exclamation-triangle" /> **Online evaluation alerts**: Performance degradation notifications from live deployments
* <Icon icon="webhook" /> **LangSmith traces webhooks**: Automated triggers based on trace analysis and performance metrics.
* <Icon icon="play" /> **Manual trigger**: Manual initiation of the pipeline for testing or emergency deployments.

Compared to traditional software, testing AI agent applications also requires assessing response quality, so it is important to test each part of the workflow. The pipeline implements multiple testing layers:

1. <Icon icon="puzzle-piece" /> **Unit tests**: Individual node and utility function testing.
2. <Icon icon="link" /> **Integration tests**: Component interaction testing.
3. <Icon icon="route" /> **End-to-end tests**: Full graph execution testing.
4. <Icon icon="brain" /> **Offline evaluations**: Performance assessment with real-world scenarios including end-to-end evaluations, single-step evaluations, agent trajectory analysis, and multi-turn simulations.
5. <Icon icon="server" /> **LangGraph dev server tests**: Use the [langgraph-cli](/langsmith/cli) tool for spinning up (inside the GitHub Action) a local server to run the LangGraph agent. This polls the `/ok` server API endpoint until it is available and for 30 seconds, after that it throws an error.

## GitHub Actions workflow

The CI/CD pipeline uses GitHub Actions with the [Control Plane API](/langsmith/api-ref-control-plane) and [LangSmith API](https://api.smith.langchain.com/redoc) to automate deployment. A helper script manages API interactions and deployments: [https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph\_api.py](https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph_api.py).

The workflow includes:

* **New agent deployment**: When a new PR is opened and tests pass, a new preview deployment is created in LangSmith Deployment using the [Control Plane API](/langsmith/api-ref-control-plane). This allows you to test the agent in a staging environment before promoting to production.

* **Agent deployment revision**: A revision happens when an existing deployment with the same ID is found, or when the PR is merged into main. In the case of merging to main, the preview deployment is deleted and a production deployment is created. This ensures that any updates to the agent are properly deployed and integrated into the production infrastructure.

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=3ef7d51a322b8b5e2f9c2c70579fcc97" alt="Agent Deployment Revision Workflow" data-og-width="1022" width="1022" data-og-height="196" height="196" data-path="langsmith/images/cicd-new-lgp-revision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=a3d06c339e84a1af99450d23e8bd617f 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=30589c8727af3ecb1d97881fd6692554 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c05ab515ea0901fb2d076dee256ad108 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b939ad6842110227f70cc0526468d21d 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=0559d5b2a85414e954a72377b2eed9ec 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b8b96047a8b37f31b78d793cd7d18f45 2500w" />

* **Testing and evaluation workflow**: In addition to the more traditional testing phases (unit tests, integration tests, end-to-end tests, etc.), the pipeline includes [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) and [Agent dev server testing](/langsmith/local-server) because you want to test the quality of your agent. These evaluations provide comprehensive assessment of the agent's performance using real-world scenarios and data.

<img src="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=477c3f5ec3d9bb9dfc354b9a57860636" alt="Test with Results Workflow" data-og-width="2050" width="2050" data-og-height="996" height="996" data-path="langsmith/images/cicd-test-with-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=280&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=7c5885b5f85c1c408fda449c5a0c706a 280w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=560&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=3b9a25332a9f6b56edfc9fbbfec248c1 560w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=840&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=380cb346fffbaf13365b37c6fa955c05 840w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1100&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=8994d1e816e725865f90a2ac6601f7a4 1100w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1650&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=42b752f1e5f0043dd6998ae372e83874 1650w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=2500&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=043be8ed1ef59cea171f30146790a877 2500w" />

<AccordionGroup>
    <Accordion title="Final Response Evaluation" icon="check-circle">
      Evaluates the final output of your agent against expected results. This is the most common type of evaluation that checks if the agent's final response meets quality standards and answers the user's question correctly.
    </Accordion>

<Accordion title="Single Step Evaluation" icon="step-forward">
      Tests individual steps or nodes within your LangGraph workflow. This allows you to validate specific components of your agent's logic in isolation, ensuring each step functions correctly before testing the full pipeline.
    </Accordion>

<Accordion title="Agent Trajectory Evaluation" icon="route">
      Analyzes the complete path your agent takes through the graph, including all intermediate steps and decision points. This helps identify bottlenecks, unnecessary steps, or suboptimal routing in your agent's workflow. It also evaluates whether your agent invoked the right tools in the right order or at the right time.
    </Accordion>

<Accordion title="Multi-Turn Evaluation" icon="comments">
      Tests conversational flows where the agent maintains context across multiple interactions. This is crucial for agents that handle follow-up questions, clarifications, or extended dialogues with users.
    </Accordion>
  </AccordionGroup>

See the [LangGraph testing documentation](/oss/python/langgraph/test) for specific testing approaches and the [evaluation approaches guide](/langsmith/evaluation-approaches) for a comprehensive overview of offline evaluations.

Before setting up the CI/CD pipeline, ensure you have:

* <Icon icon="robot" /> An AI agent application (in this case built using [LangGraph](/oss/python/langgraph/overview))
* <Icon icon="user" /> A [LangSmith account](https://smith.langchain.com/)
* <Icon icon="key" /> A [LangSmith API key](/langsmith/create-account-api-key) needed to deploy agents and retrieve experiment results
* <Icon icon="cog" /> Project-specific environment variables configured in your repository secrets (e.g., LLM model API keys, vector store credentials, database connections)

<Note>
  While this example uses GitHub, the CI/CD pipeline works with other Git hosting platforms including GitLab, Bitbucket, and others.
</Note>

## Deployment options

LangSmith supports multiple deployment methods, depending on how your [LangSmith instance is hosted](/langsmith/platform-setup):

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration.
* <Icon icon="server" /> **Self-Hosted/Hybrid**: Container registry-based deployments.

The deployment flow starts by modifying your agent implementation. At minimum, you must have a [`langgraph.json`](/langsmith/application-structure) and dependency file in your project (`requirements.txt` or `pyproject.toml`). Use the `langgraph dev` CLI tool to check for errors—fix any errors; otherwise, the deployment will succeed when deployed to LangSmith Deployment.

### Prerequisites for manual deployment

Before deploying your agent, ensure you have:

1. <Icon icon="project-diagram" /> **LangGraph graph**: Your agent implementation (e.g., `./agents/simple_text2sql.py:agent`).
2. <Icon icon="box" /> **Dependencies**: Either `requirements.txt` or `pyproject.toml` with all required packages.
3. <Icon icon="cog" /> **Configuration**: `langgraph.json` file specifying:
   * Path to your agent graph
   * Dependencies location
   * Environment variables
   * Python version

Example `langgraph.json`:

### Local development and testing

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=425460d3401221ab441e21fc706c9cf1" alt="Studio CLI Interface" data-og-width="2972" width="2972" data-og-height="1354" height="1354" data-path="langsmith/images/cicd-studio-cli.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=35e64359dba47f4db4962148073cfadb 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c12eb479d5c46921633c56bdead978bc 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b36efc12f81027b7364cea82a4600fc3 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=131c3fa2e989fbb8ebc4748a5790dc36 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=afa56b4e5ca02495ef5e7cb69d8e1329 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=774ec3dcf76a4b0e61989cd12e41e0c3 2500w" />

First, test your agent locally using [Studio](/langsmith/studio):

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Trigger sources

There are multiple ways you can trigger this pipeline, either during development or if your application is already live. The pipeline can be triggered by:

* <Icon icon="code-branch" /> **Code changes**: Pushes to main/development branches where you can modify the LangGraph architecture, try different models, update agent logic, or make any code improvements.
* <Icon icon="edit" /> **PromptHub updates**: Changes to prompt templates stored in LangSmith PromptHub—whenever there's a new prompt commit, the system triggers a webhook to run the pipeline.
* <Icon icon="exclamation-triangle" /> **Online evaluation alerts**: Performance degradation notifications from live deployments
* <Icon icon="webhook" /> **LangSmith traces webhooks**: Automated triggers based on trace analysis and performance metrics.
* <Icon icon="play" /> **Manual trigger**: Manual initiation of the pipeline for testing or emergency deployments.

### Testing layers

Compared to traditional software, testing AI agent applications also requires assessing response quality, so it is important to test each part of the workflow. The pipeline implements multiple testing layers:

1. <Icon icon="puzzle-piece" /> **Unit tests**: Individual node and utility function testing.
2. <Icon icon="link" /> **Integration tests**: Component interaction testing.
3. <Icon icon="route" /> **End-to-end tests**: Full graph execution testing.
4. <Icon icon="brain" /> **Offline evaluations**: Performance assessment with real-world scenarios including end-to-end evaluations, single-step evaluations, agent trajectory analysis, and multi-turn simulations.
5. <Icon icon="server" /> **LangGraph dev server tests**: Use the [langgraph-cli](/langsmith/cli) tool for spinning up (inside the GitHub Action) a local server to run the LangGraph agent. This polls the `/ok` server API endpoint until it is available and for 30 seconds, after that it throws an error.

## GitHub Actions workflow

The CI/CD pipeline uses GitHub Actions with the [Control Plane API](/langsmith/api-ref-control-plane) and [LangSmith API](https://api.smith.langchain.com/redoc) to automate deployment. A helper script manages API interactions and deployments: [https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph\_api.py](https://github.com/langchain-ai/cicd-pipeline-example/blob/main/.github/scripts/langgraph_api.py).

The workflow includes:

* **New agent deployment**: When a new PR is opened and tests pass, a new preview deployment is created in LangSmith Deployment using the [Control Plane API](/langsmith/api-ref-control-plane). This allows you to test the agent in a staging environment before promoting to production.

* **Agent deployment revision**: A revision happens when an existing deployment with the same ID is found, or when the PR is merged into main. In the case of merging to main, the preview deployment is deleted and a production deployment is created. This ensures that any updates to the agent are properly deployed and integrated into the production infrastructure.

  <img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=3ef7d51a322b8b5e2f9c2c70579fcc97" alt="Agent Deployment Revision Workflow" data-og-width="1022" width="1022" data-og-height="196" height="196" data-path="langsmith/images/cicd-new-lgp-revision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=a3d06c339e84a1af99450d23e8bd617f 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=30589c8727af3ecb1d97881fd6692554 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c05ab515ea0901fb2d076dee256ad108 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b939ad6842110227f70cc0526468d21d 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=0559d5b2a85414e954a72377b2eed9ec 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-new-lgp-revision.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b8b96047a8b37f31b78d793cd7d18f45 2500w" />

* **Testing and evaluation workflow**: In addition to the more traditional testing phases (unit tests, integration tests, end-to-end tests, etc.), the pipeline includes [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) and [Agent dev server testing](/langsmith/local-server) because you want to test the quality of your agent. These evaluations provide comprehensive assessment of the agent's performance using real-world scenarios and data.

  <img src="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=477c3f5ec3d9bb9dfc354b9a57860636" alt="Test with Results Workflow" data-og-width="2050" width="2050" data-og-height="996" height="996" data-path="langsmith/images/cicd-test-with-results.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=280&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=7c5885b5f85c1c408fda449c5a0c706a 280w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=560&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=3b9a25332a9f6b56edfc9fbbfec248c1 560w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=840&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=380cb346fffbaf13365b37c6fa955c05 840w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1100&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=8994d1e816e725865f90a2ac6601f7a4 1100w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=1650&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=42b752f1e5f0043dd6998ae372e83874 1650w, https://mintcdn.com/langchain-5e9cc07a/MrTet_AXQVddxOlO/langsmith/images/cicd-test-with-results.png?w=2500&fit=max&auto=format&n=MrTet_AXQVddxOlO&q=85&s=043be8ed1ef59cea171f30146790a877 2500w" />

  <AccordionGroup>
    <Accordion title="Final Response Evaluation" icon="check-circle">
      Evaluates the final output of your agent against expected results. This is the most common type of evaluation that checks if the agent's final response meets quality standards and answers the user's question correctly.
    </Accordion>

    <Accordion title="Single Step Evaluation" icon="step-forward">
      Tests individual steps or nodes within your LangGraph workflow. This allows you to validate specific components of your agent's logic in isolation, ensuring each step functions correctly before testing the full pipeline.
    </Accordion>

    <Accordion title="Agent Trajectory Evaluation" icon="route">
      Analyzes the complete path your agent takes through the graph, including all intermediate steps and decision points. This helps identify bottlenecks, unnecessary steps, or suboptimal routing in your agent's workflow. It also evaluates whether your agent invoked the right tools in the right order or at the right time.
    </Accordion>

    <Accordion title="Multi-Turn Evaluation" icon="comments">
      Tests conversational flows where the agent maintains context across multiple interactions. This is crucial for agents that handle follow-up questions, clarifications, or extended dialogues with users.
    </Accordion>
  </AccordionGroup>

  See the [LangGraph testing documentation](/oss/python/langgraph/test) for specific testing approaches and the [evaluation approaches guide](/langsmith/evaluation-approaches) for a comprehensive overview of offline evaluations.

### Prerequisites

Before setting up the CI/CD pipeline, ensure you have:

* <Icon icon="robot" /> An AI agent application (in this case built using [LangGraph](/oss/python/langgraph/overview))
* <Icon icon="user" /> A [LangSmith account](https://smith.langchain.com/)
* <Icon icon="key" /> A [LangSmith API key](/langsmith/create-account-api-key) needed to deploy agents and retrieve experiment results
* <Icon icon="cog" /> Project-specific environment variables configured in your repository secrets (e.g., LLM model API keys, vector store credentials, database connections)

<Note>
  While this example uses GitHub, the CI/CD pipeline works with other Git hosting platforms including GitLab, Bitbucket, and others.
</Note>

## Deployment options

LangSmith supports multiple deployment methods, depending on how your [LangSmith instance is hosted](/langsmith/platform-setup):

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration.
* <Icon icon="server" /> **Self-Hosted/Hybrid**: Container registry-based deployments.

The deployment flow starts by modifying your agent implementation. At minimum, you must have a [`langgraph.json`](/langsmith/application-structure) and dependency file in your project (`requirements.txt` or `pyproject.toml`). Use the `langgraph dev` CLI tool to check for errors—fix any errors; otherwise, the deployment will succeed when deployed to LangSmith Deployment.
```

Example 2 (unknown):
```unknown
### Prerequisites for manual deployment

Before deploying your agent, ensure you have:

1. <Icon icon="project-diagram" /> **LangGraph graph**: Your agent implementation (e.g., `./agents/simple_text2sql.py:agent`).
2. <Icon icon="box" /> **Dependencies**: Either `requirements.txt` or `pyproject.toml` with all required packages.
3. <Icon icon="cog" /> **Configuration**: `langgraph.json` file specifying:
   * Path to your agent graph
   * Dependencies location
   * Environment variables
   * Python version

Example `langgraph.json`:
```

Example 3 (unknown):
```unknown
### Local development and testing

<img src="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=425460d3401221ab441e21fc706c9cf1" alt="Studio CLI Interface" data-og-width="2972" width="2972" data-og-height="1354" height="1354" data-path="langsmith/images/cicd-studio-cli.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=280&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=35e64359dba47f4db4962148073cfadb 280w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=560&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=c12eb479d5c46921633c56bdead978bc 560w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=840&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=b36efc12f81027b7364cea82a4600fc3 840w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1100&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=131c3fa2e989fbb8ebc4748a5790dc36 1100w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=1650&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=afa56b4e5ca02495ef5e7cb69d8e1329 1650w, https://mintcdn.com/langchain-5e9cc07a/-UAx6PdOIJpPyTy2/langsmith/images/cicd-studio-cli.png?w=2500&fit=max&auto=format&n=-UAx6PdOIJpPyTy2&q=85&s=774ec3dcf76a4b0e61989cd12e41e0c3 2500w" />

First, test your agent locally using [Studio](/langsmith/studio):
```

---

## Implement a LangChain integration

**URL:** llms-txt#implement-a-langchain-integration

Source: https://docs.langchain.com/oss/python/contributing/implement-langchain

Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.

LangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/python/integrations/chat), [tools](/oss/python/integrations/tools), [retrievers](/oss/python/integrations/retrievers), and more.

Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.

<Tabs>
  <Tab title="Chat Models">
    Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.

<Warning>
      The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/python/langchain/models) for details on how LangChain chat models function.
    </Warning>
  </Tab>

<Tab title="Tools">
    Tools are used in 2 main ways:

1. To define an "input schema" or "args schema" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a "tool call", or parameters to call the tool with.
    2. To take a "tool call" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage.

The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.

<Warning>
      The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/python/langchain/tools) for details on how LangChain tools function.
    </Warning>
  </Tab>

<Tab title="Retrievers">
    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.

<Warning>
      The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/python/integrations/retrievers) for details on how LangChain retrievers function.
    </Warning>
  </Tab>

<Tab title="Vector Stores">
    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store.

See the [vector store integration guide](/oss/python/integrations/vectorstores) for details on implementing a vector store integration.

<Warning>
      The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/python/integrations/vectorstores) for details on how LangChain vector stores function.
    </Warning>
  </Tab>

<Tab title="Embeddings">
    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) class.

<Warning>
      The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/python/integrations/text_embedding) for details on how LangChain embedding models function.
    </Warning>
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Implement distributed tracing

**URL:** llms-txt#implement-distributed-tracing

**Contents:**
- Distributed tracing in Python

Source: https://docs.langchain.com/langsmith/distributed-tracing

Sometimes, you need to trace a request across multiple services.

LangSmith supports distributed tracing out of the box, linking runs within a trace across services using context propagation headers (`langsmith-trace` and optional `baggage` for metadata/tags).

Example client-server setup:

* Trace starts on client
* Continues on server

## Distributed tracing in Python

```python  theme={null}

---

## Import Pipecat components

**URL:** llms-txt#import-pipecat-components

from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.services.whisper.stt import WhisperSTTService
from pipecat.services.openai import OpenAILLMService, OpenAITTSService
from pipecat.transports.local.audio import LocalAudioTransport, LocalAudioTransportParams

---

## Import span processor to enable LangSmith tracing

**URL:** llms-txt#import-span-processor-to-enable-langsmith-tracing

**Contents:**
  - Step 4: Run your agent
- Advanced usage
  - Custom metadata and tags
  - Recording and attaching audio to traces

from langsmith_processor import span_processor
python  theme={null}
async def main():
    # Generate unique conversation ID for LangSmith
    conversation_id = str(uuid.uuid4())
    print(f"Starting conversation: {conversation_id}")

# Configure audio input/output with voice activity detection
    transport = LocalAudioTransport(
        LocalAudioTransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        )
    )

# Initialize AI services
    stt = WhisperSTTService()
    llm = OpenAILLMService(model="gpt-4o-mini")
    tts = OpenAITTSService(voice="alloy")

# Set up conversation context with system prompt
    context = OpenAILLMContext(
        messages=[
            {
                "role": "system",
                "content": "You are a helpful voice assistant. Keep responses concise and conversational."
            }
        ]
    )
    context_aggregator = llm.create_context_aggregator(context)

# Build the processing pipeline
    pipeline = Pipeline([
        transport.input(),           # Capture microphone input
        stt,                         # Convert speech to text
        context_aggregator.user(),   # Add user message to context
        llm,                         # Generate AI response
        tts,                         # Convert response to speech
        transport.output(),          # Play through speakers
        context_aggregator.assistant(),  # Add assistant response to context
    ])

# Create task with tracing enabled
    task = PipelineTask(
        pipeline,
        params=PipelineParams(enable_metrics=True),
        enable_tracing=True,
        enable_turn_tracking=True,
        conversation_id=conversation_id,
    )

# Run the agent
    runner = PipelineRunner()
    await runner.run(task)
python  theme={null}
if __name__ == "__main__":
    asyncio.run(main())
bash  theme={null}
python agent.py
python  theme={null}
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

async def run_voice_session():
    with tracer.start_as_current_span("voice_conversation") as span:
        # Add custom metadata
        span.set_attribute("langsmith.metadata.session_type", "voice_assistant")
        span.set_attribute("langsmith.metadata.user_id", "user_123")
        span.set_attribute("langsmith.span.tags", "pipecat,voice-ai,stt-llm-tts")

# Your Pipecat pipeline code here
        task = PipelineTask(pipeline, enable_tracing=True)
        await task.queue_frames([TextFrame("Hello")])
python  theme={null}
from pathlib import Path
from datetime import datetime
from audio_recorder import AudioRecorder

**Examples:**

Example 1 (unknown):
```unknown
#### Part 2: Define the main function
```

Example 2 (unknown):
```unknown
#### Part 3: Add the entry point
```

Example 3 (unknown):
```unknown
### Step 4: Run your agent

Run your voice agent:
```

Example 4 (unknown):
```unknown
Speak to the agent through your microphone. All traces will automatically appear in LangSmith. Here is an example of a trace in LangSmith: [LangSmith trace with Pipecat](https://smith.langchain.com/public/07721f41-cd27-413e-bc79-90bd23b6807d/r).

View the complete [agent.py code](https://github.com/langchain-ai/voice-agents-tracing/blob/main/pipecat/agent.py).

## Advanced usage

### Custom metadata and tags

You can add custom metadata to your traces using span attributes:
```

---

## Improve LLM-as-judge evaluators using human feedback

**URL:** llms-txt#improve-llm-as-judge-evaluators-using-human-feedback

**Contents:**
- How it works
- Prerequisites
  - Offline evaluations
  - Online evaluations
- Getting started
- 1. Select experiments or runs
- 2. Label examples
- 3. Test your evaluator prompt against the labeled examples
- 4. Repeat to improve evaluator alignment
  - Tips for improving evaluator alignment

Source: https://docs.langchain.com/langsmith/improve-judge-evaluator-feedback

<Check>
  Before working through this page, it might be helpful to read the following:

* [Evaluation concepts](/langsmith/evaluation-concepts#evaluators)
  * [Creating LLM-as-a-judge evaluators](/langsmith/llm-as-judge)
</Check>

Reliable [*LLM-as-a-judge evaluators*](/langsmith/evaluation-concepts#llm-as-judge) are critical for making informed decisions about your AI applications (e.g., prompt, model, architecture changes). Defining the evaluator prompt correctly can be difficult, but it directly affects the trustworthiness of your evaluations.

This guide describes how to align your LLM-as-a-judge evaluator using human feedback to improve your evaluator's quality and help you build reliable AI applications.

LangSmith's **Align Evaluator** feature has a series of steps that help you align your LLM-as-a-judge evaluator with human expert feedback. You can use this feature to align evaluators that run on a dataset for [offline evaluations](/langsmith/evaluation-concepts#offline-evaluation) or for [online evaluations](/langsmith/evaluation-concepts#online-evaluation). In either case, the steps are similar:

1. **Select experiments or runs** that contain outputs from your application.
2. Add the selected experiments or runs to an **annotation queue** where a human expert can label the data.
3. **Test your LLM-as-a-judge evaluator prompt** against the labeled examples. Check the cases where your evaluator result is not aligned with the labeled data. This indicates areas where your evaluator prompt needs improvement.
4. **Refine and repeat** to improve evaluator alignment. Update your LLM-as-a-judge evaluator prompt and test again.

You'll need the following before starting this guide for [offline evaluations](#offline-evaluations) or [online evaluations](#online-evaluations):

### Offline evaluations

* A [dataset](/langsmith/evaluation-concepts#datasets) with at least one [experiment](/langsmith/evaluation-concepts#experiment).
* You'll need to upload or create datasets via the [SDK](/langsmith/manage-datasets-programmatically#create-a-dataset) or the [UI](/langsmith/manage-datasets-in-application#set-up-your-dataset) and run an experiment via the [SDK](/langsmith/evaluate-llm-application#run-the-evaluation) or the [Playground](/langsmith/run-evaluation-from-prompt-playground#5-run-your-evaluation).

### Online evaluations

* An application that’s already sending traces to LangSmith.
* Configure this with one of the [tracing integrations](/langsmith/observability-concepts#integrations) to start.

You can enter the alignment flow for both new and existing evaluators in datasets and tracing projects.

|                                              | Dataset Evaluators                                                                                                                                                                                     | Tracing Project Evaluators                                                                                                                                                                         |
| -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Create an aligned evaluator from scratch** | 1. **Datasets & Experiments** and select your dataset<br />2. Click **+ Evaluator** > **Create from labeled data**<br />3. Enter a descriptive feedback key name (e.g. `correctness`, `hallucination`) | 1. **Projects** and select your project<br />2. Click **+ New** > **Evaluator** > **Create from labeled data**<br />3. Enter a descriptive feedback‑key name (e.g. `correctness`, `hallucination`) |
| **Align an existing evaluator**              | 1. **Datasets & Experiments** > select your dataset > **Evaluators** tab<br />2. In the **Align Evaluator with experiment data** box, click **Select Experiments**                                     | 1. **Projects** > select your project > **Evaluators** tab<br />2. In the **Align Evaluator with experiment data** box, click **Select Experiments**                                               |

## 1. Select experiments or runs

Select one or more experiments (or runs) to send for human labeling. This will add runs to an [annotation queue](/langsmith/evaluation-concepts#annotation-queues).

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-evaluator-queue.gif?s=3c457faaaf2c7f31425ba510a518260e" alt="Add to evaluator queue" data-og-width="1976" width="1976" data-og-height="1080" height="1080" data-path="langsmith/images/add-to-evaluator-queue.gif" data-optimize="true" data-opv="3" />

To add any new experiments/runs to an existing annotation queue, head to the **Evaluators** tab, select the evaluator you are aligning and click **Add to Queue.**

<Check>
  Datasets should be representative of inputs and outputs you expect to see in production.

While you don’t need to cover every possible scenario, it’s important to include examples across the full range of expected use cases. For example, if you're building a sports bot that answers questions about baseball, basketball, and football, your dataset should include at least one labeled example from each sport.
</Check>

Label examples in the annotation queue by adding a feedback score. Once you've labeled an example, click **Add to Reference Dataset**.

<Check>
  If you have a large number of examples in your experiments, you don't need to label every example to get started. We recommend starting with at least 20 examples, you can always add more later. We recommend that the examples that you label are diverse (balanced in both 0 and 1 labels) to ensure that you're building a well rounded evaluator prompt.
</Check>

## 3. Test your evaluator prompt against the labeled examples

Once you have labeled examples, the next step is iterating on your evaluator prompt to mimic the labeled data as well as possible. This iteration is done in the **Evaluator Playground**.

To go to the evaluator playground: Click the **View evaluator** button on the top right of the evaluator queue. This will take you to the detail page of the evaluator you are aligning. Click the **Evaluator Playground** button to access the playground.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/evaluator-pg.gif?s=c8108ce3abc95ce756fcbd1b3726916d" alt="Evaluator Playground" data-og-width="1916" width="1916" data-og-height="1080" height="1080" data-path="langsmith/images/evaluator-pg.gif" data-optimize="true" data-opv="3" />

In the evaluator playground you can create or edit your evaluator prompt and click **Start Alignment** to run it over the set of labeled examples that you created in Step 2. After running your evaluator, you'll see how its generated scores compare to your human labels. The alignment score is the percentage of examples where the evaluator's judgment matches that of the human expert.

## 4. Repeat to improve evaluator alignment

Iterate by updating your prompt and testing again to improve evaluator alignment.

<Check>
  Updates to your evaluator prompt are **not saved by default**. We reccomend saving your evaluator prompt regularly, and especially after you see your alignment score improve.

The evaluator playground will show the alignment score for the most recently saved version of your evaluator prompt for comparison when you're iterating on your prompt.
</Check>

Improving the alignment score of your evaluator isn't an exact science but there are a few strategies that are helpful in increasing the alignment score.

### Tips for improving evaluator alignment

**1. Investigate misaligned examples**

Digging into misaligned examples and trying to group them into common failure modes is a great first step for improving your evaluator alignment.

Once you have identified the common failure modes, add instructions to your evaluator prompt so the LLM knows about them. For example, you could explain that "MFA stands for "multi-factor authentication" if you notice it not understanding that specific acronym. Or you could tell it that "a good response will always contain at least 3 potential hotels to book" if it is confused on what good/bad means in your evaluator's context.

**2. Inspect the reasoning behind the LLM score**

To understand why the LLM scored an example the way it did, you can enable reasoning for your LLM-as-a-judge evaluator. Reasoning is helpful to understand the LLM's thought process and can help you identify common failure modes to incorporate into your evaluator prompt as well..

In order to see the reasoning in the evaluator playground, hover over the LLM score.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/enable-reasoning.gif?s=d6ed09e4149a63723ef9305544b8b652" alt="Enable reasoning" data-og-width="1520" width="1520" data-og-height="1080" height="1080" data-path="langsmith/images/enable-reasoning.gif" data-optimize="true" data-opv="3" />

This will show the reasoning behind the LLM's score in the evaluator playground.

**3. Add more labeled examples and validate performance**

To avoid overfitting to the labeled examples, it's important to add more labeled examples and test performance, especially if you started off with a small number of examples.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/-9o94oj4x0A?si=wfv9cN3L4DalMD2e" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/improve-judge-evaluator-feedback.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Include HTTP headers in server logs

**URL:** llms-txt#include-http-headers-in-server-logs

Source: https://docs.langchain.com/langsmith/configurable-logs

By default, the [Agent Server](/langsmith/agent-server) omits HTTP headers from server logs for privacy reasons. However, logging request and correlation IDs can help you debug issues and trace requests across distributed systems. You can opt-in to logging headers for all API calls by modifying the `logging_headers` section in your [`langgraph.json`](/langsmith/application-structure#configuration-file) file.

The `includes` and `excludes` lists accept exact header names or glob patterns using `*` as a wildcard to match any number of characters (case-insensitive). For your security, no other pattern types are supported.

Note that exclusions take precedence over inclusions. For example, if you include `*-id` but exclude `x-user-id`, the `x-user-id` header will not be logged.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configurable-logs.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Include multimodal content in a prompt

**URL:** llms-txt#include-multimodal-content-in-a-prompt

**Contents:**
- Inline content
- Template variables
- Populate the template variable
- Run an evaluation

Source: https://docs.langchain.com/langsmith/multimodal-content

Some applications are based around multimodal content, like a chatbot that can answer questions about a PDF or image. In these cases, you'll want to include multimodal content in your prompt and test the model's ability to answer questions about the content.

The LangSmith Playground supports two methods for incorporating multimodal content in your prompts:

1. Inline content: Embed static files (images, PDFs, audio) directly in your prompt. This is ideal when you want to consistently include the same multimodal content across all uses of the prompt. For example, you might include a reference image that helps ground the model's responses.

2. Template variables: Create dynamic placeholders for attachments that can be populated with different content each time. This approach offers more flexibility, allowing you to:

* Test how the model handles different inputs
   * Create reusable prompts that work with varying content

<Note>
  Not all models support multimodal content. Before using multimodal features in the playground, make sure your selected model supports the file types you want to use.
</Note>

Click the file icon in the message where you want to add multimodal content. Under the `Upload content` tab, you can upload a file and include it inline in the prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b93d2a6731d26d3ff58a3d0d6d909159" alt="Upload inline multimodal content" data-og-width="410" width="410" data-og-height="339" height="339" data-path="langsmith/images/upload-inline-multimodal-content.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ae329136b8de8d5f4b12ecab49651063 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f1644a04d5a33e803a15bc9794ecb528 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=275ffe46010c6b24012ed049a17a59fb 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9591d26a3b27c0462f7674f84c2a525c 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f4439dc91e005750232b40efbb19954d 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/upload-inline-multimodal-content.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1c429f8f78846a14e0c9a379b9a24355 2500w" />

## Template variables

Click the file icon in the message where you want to add multimodal content. Under the `Template variables` tab, you can create a template variable for a specific attachment type. Currently, only images, PDFs, and audio files (.wav, .mp3) are supported.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e69596dd9fe7d16252c7054bf9efcdf0" alt="Template variable multimodal content" data-og-width="391" width="391" data-og-height="303" height="303" data-path="langsmith/images/template-variable-multimodal-content.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1efb9ed1c9a4a64be7174b90ffbfe664 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1375989ffa599f506d308441e46907f9 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f50c62b520314c18e433f77952ff79a2 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e38e8283352a13b54e96a381a2449396 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=331ff1a4664debd8d7b08a6ad645d9c6 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/template-variable-multimodal-content.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ffa740fa2a3f4f52173cd073843d697f 2500w" />

## Populate the template variable

Once you've added a template variable, you can provide content for it using the panel on the right side of the screen. Simply click the `+` button to upload or select content that will be used to populate the template variable.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5983e91ca9f596918c9068f8d7450d8d" alt="Manual prompt multimodal" data-og-width="1466" width="1466" data-og-height="482" height="482" data-path="langsmith/images/manual-prompt-multimodal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ccb36221eb97f54196de13df4f9749e7 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6b4554ac3156e805655eba6afd2ce771 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3e85646401f9aa31e1d79257344c158 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9727ce9dec69152f6e49e3d8ff575300 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f73957416f51c78a0e29c449f64ff1e2 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/manual-prompt-multimodal.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ab8421f512b312b56ac515b970d337f4 2500w" />

After testing out your prompt manually, you can [run an evaluation](/langsmith/evaluate-with-attachments?mode=ui) to see how the prompt performs over a golden dataset of examples.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multimodal-content.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Indicate that a chat model supports image inputs

**URL:** llms-txt#indicate-that-a-chat-model-supports-image-inputs

**Contents:**
- Running tests

class TestChatParrotLinkStandard(ChatModelIntegrationTests):
    # ... other required properties

@property
    def supports_image_inputs(self) -> bool:
        return True  # (The default is False)
bash  theme={null}
make test
make integration_test
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You should organize tests in these subdirectories relative to the root of your package:

  * `tests/unit_tests` for unit tests
  * `tests/integration_tests` for integration tests
</Note>

To see the complete list of configurable capabilities and their defaults, visit the [API reference](https://reference.langchain.com/python/langchain_tests) for standard tests.

Here are some example implementations of standard tests from popular integrations:

<Tabs>
  <Tab title="Unit tests">
    <Columns cols={3}>
      <Card title="ChatOpenAI" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/tests/unit_tests/chat_models/test_base_standard.py" arrow>Unit tests</Card>
      <Card title="ChatAnthropic" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/tests/unit_tests/test_standard.py" arrow>Unit tests</Card>
      <Card title="ChatGenAI" href="https://github.com/langchain-ai/langchain-google/blob/main/libs/genai/tests/unit_tests/test_standard.py" arrow>Unit tests</Card>
    </Columns>
  </Tab>

  <Tab title="Integration tests">
    <Columns cols={3}>
      <Card title="ChatOpenAI" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/openai/tests/integration_tests/chat_models/test_base_standard.py" arrow>Integration tests</Card>
      <Card title="ChatAnthropic" href="https://github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/tests/integration_tests/test_standard.py" arrow>Integration tests</Card>
      <Card title="ChatGenAI" href="https://github.com/langchain-ai/langchain-google/blob/main/libs/genai/tests/integration_tests/test_standard.py" arrow>Integration tests</Card>
    </Columns>
  </Tab>
</Tabs>

***

## Running tests

If bootstrapping an integration from a template, a `Makefile` is provided that includes targets for running unit and integration tests:
```

Example 2 (unknown):
```unknown
Otherwise, if you follow the recommended directory structure, you can run tests with:
```

---

## Initialize an in-memory checkpointer for persistence

**URL:** llms-txt#initialize-an-in-memory-checkpointer-for-persistence

checkpointer = InMemorySaver()

@task
def slow_task():
    """
    Simulates a slow-running task by introducing a 1-second delay.
    """
    time.sleep(1)
    return "Ran slow task."

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer: StreamWriter):
    """
    Main workflow function that runs the slow_task and get_info tasks sequentially.

Parameters:
    - inputs: Dictionary containing workflow input values.
    - writer: StreamWriter for streaming custom data.

The workflow first executes `slow_task` and then attempts to execute `get_info`,
    which will fail on the first invocation.
    """
    slow_task_result = slow_task().result()  # Blocking call to slow_task
    get_info().result()  # Exception will be raised here on the first attempt
    return slow_task_result

---

## Initialize multiple instrumentors

**URL:** llms-txt#initialize-multiple-instrumentors

OpenAIInstrumentor().instrument()
DSPyInstrumentor().instrument()

---

## Initialize the LangSmith Client so we can use to get the dataset

**URL:** llms-txt#initialize-the-langsmith-client-so-we-can-use-to-get-the-dataset

---

## initialize the langsmith client with the anonymization functions

**URL:** llms-txt#initialize-the-langsmith-client-with-the-anonymization-functions

langsmith_client = Client(
  hide_inputs=comprehend_anonymize, hide_outputs=comprehend_anonymize
)

---

## Initial run - hits the interrupt and pauses

**URL:** llms-txt#initial-run---hits-the-interrupt-and-pauses

---

## InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.

**URL:** llms-txt#inmemorystore-saves-data-to-an-in-memory-dictionary.-use-a-db-backed-store-in-production-use.

store = InMemoryStore(index={"embed": embed, "dims": 2}) # [!code highlight]
user_id = "my-user"
application_context = "chitchat"
namespace = (user_id, application_context) # [!code highlight]
store.put( # [!code highlight]
    namespace,
    "a-memory",
    {
        "rules": [
            "User likes short, direct language",
            "User only speaks English & python",
        ],
        "my-key": "my-value",
    },
)

---

## InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.

**URL:** llms-txt#inmemorystore-saves-data-to-an-in-memory-dictionary.-use-a-db-backed-store-in-production.

store = InMemoryStore() # [!code highlight]

@dataclass
class Context:
    user_id: str

---

## 'inputs' will come from your dataset.

**URL:** llms-txt#'inputs'-will-come-from-your-dataset.

---

## Instructions for extracting the user/purchase info from the conversation.

**URL:** llms-txt#instructions-for-extracting-the-user/purchase-info-from-the-conversation.

gather_info_instructions = """You are managing an online music store that sells song tracks. \
Customers can buy multiple tracks at a time and these purchases are recorded in a database as \
an Invoice per purchase and an associated set of Invoice Lines for each purchased track.

Your task is to help customers who would like a refund for one or more of the tracks they've \
purchased. In order for you to be able refund them, the customer must specify the Invoice ID \
to get a refund on all the tracks they bought in a single transaction, or one or more Invoice \
Line IDs if they would like refunds on individual tracks.

Often a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \
would like a refund. In this case you can help them look up their invoices by asking them to \
specify:
- Required: Their first name, last name, and phone number.
- Optionally: The track name, artist name, album name, or purchase date.

If the customer has not specified the required information (either Invoice/Invoice Line IDs \
or first name, last name, phone) then please ask them to specify it."""

---

## Instructions for routing.

**URL:** llms-txt#instructions-for-routing.

route_instructions = """You are managing an online music store that sells song tracks. \
You can help customers in two types of ways: (1) answering general questions about \
tracks sold at your store, (2) helping them get a refund on a purhcase they made at your store.

Based on the following conversation, determine if the user is currently seeking general \
information about song tracks or if they are trying to refund a specific purchase.

Return 'refund' if they are trying to get a refund and 'question_answering' if they are \
asking a general music question. Do NOT return anything else. Do NOT try to respond to \
the user.
"""

---

## Instrument AutoGen and OpenAI

**URL:** llms-txt#instrument-autogen-and-openai

AutogenInstrumentor().instrument()
OpenAIInstrumentor().instrument()

---

## Instrument AutoGen and OpenAI calls

**URL:** llms-txt#instrument-autogen-and-openai-calls

**Contents:**
  - 3. Create and run your AutoGen application

AutogenInstrumentor().instrument()
OpenAIInstrumentor().instrument()
python  theme={null}
import autogen
from openinference.instrumentation.autogen import AutogenInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor
from langsmith.integrations.otel import configure
import os
import dotenv

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  You do not need to set any OpenTelemetry environment variables or configure exporters manually—`configure()` handles everything automatically.
</Note>

### 3. Create and run your AutoGen application

Once configured, your AutoGen application will automatically send traces to LangSmith:
```

---

## Instrument CrewAI and OpenAI

**URL:** llms-txt#instrument-crewai-and-openai

CrewAIInstrumentor().instrument()
OpenAIInstrumentor().instrument()

---

## Instrument OpenAI calls

**URL:** llms-txt#instrument-openai-calls

OpenAIInstrumentor().instrument()

---

## Integrations

**URL:** llms-txt#integrations

Source: https://docs.langchain.com/oss/python/reference/integrations-python

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/integrations-python.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Integration Packages

**URL:** llms-txt#integration-packages

**Contents:**
- Popular providers
- All providers

Source: https://docs.langchain.com/oss/javascript/integrations/providers/overview

LangChain integrates with a wide variety of chat & embedding models, tools & toolkits, document loaders, vector stores, and more.

These providers have standalone `langchain-provider` packages for improved versioning, dependency management, and testing.

| Provider                                                                                 | Package                                                                                          | Downloads                                                                  | Latest                                                              |
| :--------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------- | :------------------------------------------------------------------ |
| [Anthropic](/oss/javascript/integrations/providers/anthropic)                            | [`@langchain/anthropic`](https://www.npmjs.com/package/@langchain/anthropic)                     | ![Downloads](https://img.shields.io/npm/dm/@langchain/anthropic)           | ![NPM](https://img.shields.io/npm/v/@langchain/anthropic)           |
| [Azure CosmosDB](/oss/javascript/integrations/vectorstores/azure_cosmosdb_nosql)         | [`@langchain/azure-cosmosdb`](https://www.npmjs.com/package/@langchain/azure-cosmosdb)           | ![Downloads](https://img.shields.io/npm/dm/@langchain/azure-cosmosdb)      | ![NPM](https://img.shields.io/npm/v/@langchain/azure-cosmosdb)      |
| [Cerebras](/oss/javascript/integrations/chat/cerebras)                                   | [`@langchain/cerebras`](https://www.npmjs.com/package/@langchain/cerebras)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/cerebras)            | ![NPM](https://img.shields.io/npm/v/@langchain/cerebras)            |
| Cloudflare                                                                               | [`@langchain/cloudflare`](https://www.npmjs.com/package/@langchain/cloudflare)                   | ![Downloads](https://img.shields.io/npm/dm/@langchain/cloudflare)          | ![NPM](https://img.shields.io/npm/v/@langchain/cloudflare)          |
| [Cohere](/oss/javascript/integrations/chat/cohere)                                       | [`@langchain/cohere`](https://www.npmjs.com/package/@langchain/cohere)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/cohere)              | ![NPM](https://img.shields.io/npm/v/@langchain/cohere)              |
| [Exa](/oss/javascript/integrations/retrievers/exa)                                       | [`langchain-exa`](https://www.npmjs.com/package/@langchain/exa)                                  | ![Downloads](https://img.shields.io/npm/dm/@langchain/exa)                 | ![NPM](https://img.shields.io/npm/v/@langchain/exa)                 |
| [Google GenAI](/oss/javascript/integrations/chat/google_generative_ai)                   | [`@langchain/google-genai`](https://www.npmjs.com/package/@langchain/google-genai)               | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-genai)        | ![NPM](https://img.shields.io/npm/v/@langchain/google-genai)        |
| [Google VertexAI](/oss/javascript/integrations/chat/google_vertex_ai)                    | [`@langchain/google-vertexai`](https://www.npmjs.com/package/@langchain/google-vertexai)         | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai)     | ![NPM](https://img.shields.io/npm/v/@langchain/google-vertexai)     |
| [Google VertexAI (Web Environments)](/oss/javascript/integrations/chat/google_vertex_ai) | [`@langchain/google-vertexai-web`](https://www.npmjs.com/package/@langchain/google-vertexai-web) | ![Downloads](https://img.shields.io/npm/dm/@langchain/google-vertexai-web) | ![NPM](https://img.shields.io/npm/v/@langchain/google-vertexai-web) |
| [Groq](/oss/javascript/integrations/chat/groq)                                           | [`@langchain/groq`](https://www.npmjs.com/package/@langchain/groq)                               | ![Downloads](https://img.shields.io/npm/dm/@langchain/groq)                | ![NPM](https://img.shields.io/npm/v/@langchain/groq)                |
| [MistralAI](/oss/javascript/integrations/chat/mistral)                                   | [`@langchain/mistralai`](https://www.npmjs.com/package/@langchain/mistralai)                     | ![Downloads](https://img.shields.io/npm/dm/@langchain/mistralai)           | ![NPM](https://img.shields.io/npm/v/@langchain/mistralai)           |
| [MongoDB](/oss/javascript/integrations/vectorstores/mongodb_atlas)                       | [`@langchain/mongodb`](https://www.npmjs.com/package/@langchain/mongodb)                         | ![Downloads](https://img.shields.io/npm/dm/@langchain/mongodb)             | ![NPM](https://img.shields.io/npm/v/@langchain/mongodb)             |
| [Nomic](/oss/javascript/integrations/text_embedding/nomic)                               | [`@langchain/nomic`](https://www.npmjs.com/package/@langchain/nomic)                             | ![Downloads](https://img.shields.io/npm/dm/@langchain/nomic)               | ![NPM](https://img.shields.io/npm/v/@langchain/nomic)               |
| [OpenAI](/oss/javascript/integrations/providers/openai)                                  | [`@langchain/openai`](https://www.npmjs.com/package/@langchain/openai)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/openai)              | ![NPM](https://img.shields.io/npm/v/@langchain/openai)              |
| [Pinecone](/oss/javascript/integrations/vectorstores/pinecone)                           | [`@langchain/pinecone`](https://www.npmjs.com/package/@langchain/pinecone)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/pinecone)            | ![NPM](https://img.shields.io/npm/v/@langchain/pinecone)            |
| [Qdrant](/oss/javascript/integrations/vectorstores/qdrant)                               | [`@langchain/qdrant`](https://www.npmjs.com/package/@langchain/qdrant)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/qdrant)              | ![NPM](https://img.shields.io/npm/v/@langchain/qdrant)              |
| [Tavily](/oss/javascript/integrations/retrievers/tavily)                                 | [`@langchain/tavily`](https://www.npmjs.com/package/@langchain/tavily)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/tavily)              | ![NPM](https://img.shields.io/npm/v/@langchain/tavily)              |
| [Weaviate](/oss/javascript/integrations/vectorstores/weaviate)                           | [`@langchain/weaviate`](https://www.npmjs.com/package/@langchain/weaviate)                       | ![Downloads](https://img.shields.io/npm/dm/@langchain/weaviate)            | ![NPM](https://img.shields.io/npm/v/@langchain/weaviate)            |
| [xAI](/oss/javascript/integrations/chat/xai)                                             | [`@langchain/xai`](https://www.npmjs.com/package/@langchain/xai)                                 | ![Downloads](https://img.shields.io/npm/dm/@langchain/xai)                 | ![NPM](https://img.shields.io/npm/v/@langchain/xai)                 |
| [Yandex](/oss/javascript/integrations/chat/yandex)                                       | [`@langchain/yandex`](https://www.npmjs.com/package/@langchain/yandex)                           | ![Downloads](https://img.shields.io/npm/dm/@langchain/yandex)              | ![NPM](https://img.shields.io/npm/v/@langchain/yandex)              |

[See all providers](/oss/javascript/integrations/providers/all_providers) or search for a provider using the search field.

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/javascript/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/providers/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Interact with your self-hosted instance of LangSmith

**URL:** llms-txt#interact-with-your-self-hosted-instance-of-langsmith

**Contents:**
  - Configuring the application you want to use with LangSmith
  - Self-Signed Certificates

Source: https://docs.langchain.com/langsmith/self-host-usage

This guide will walk you through the process of using your self-hosted instance of LangSmith.

<Info>
  This guide assumes you have already deployed a self-hosted LangSmith instance. If you have not, please refer to the [kubernetes deployment guide](/langsmith/kubernetes) or the [docker deployment guide](/langsmith/docker).
</Info>

### Configuring the application you want to use with LangSmith

LangSmith has a single API for interacting with both the hub and the LangSmith backend.

1. Once you have deployed your instance, you can access the LangSmith UI at `http(s)://<host>`.
2. The LangSmith API will be available at `http(s)://<host>/api/v1`
3. The LangSmith Control Plane will be available at `http(s)://<host>/api-host`

To use the API of your instance, you will need to set the following environment variables in your application:

You can also configure these variables directly in the LangSmith SDK client:

After setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the [*quickstart guide*](https://docs.smith.langchain.com/#quick-start) to get a feel for how to use LangSmith.

### Self-Signed Certificates

If you are using self-signed certificates for your self-hosted LangSmith instance, this can be problematic as Python comes with its own set of trusted certificates, which may not include your self-signed certificate. To resolve this, you may need to use something like `truststore` to load system certificates into your Python environment.

You can do this like so:

1. pip install truststore (or similar depending on the package manager you are using)

Then use the following code to load the system certificates:

```python  theme={null}
import truststore
truststore.inject_into_ssl()

**Examples:**

Example 1 (unknown):
```unknown
You can also configure these variables directly in the LangSmith SDK client:
```

Example 2 (unknown):
```unknown
After setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the [*quickstart guide*](https://docs.smith.langchain.com/#quick-start) to get a feel for how to use LangSmith.

### Self-Signed Certificates

If you are using self-signed certificates for your self-hosted LangSmith instance, this can be problematic as Python comes with its own set of trusted certificates, which may not include your self-signed certificate. To resolve this, you may need to use something like `truststore` to load system certificates into your Python environment.

You can do this like so:

1. pip install truststore (or similar depending on the package manager you are using)

Then use the following code to load the system certificates:
```

---

## Interrupts

**URL:** llms-txt#interrupts

**Contents:**
- Pause using `interrupt`
- Resuming interrupts

Source: https://docs.langchain.com/oss/python/langgraph/interrupts

Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its [persistence](/oss/python/langgraph/persistence) layer and waits indefinitely until you resume execution.

Interrupts work by calling the `interrupt()` function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you're ready to continue, you resume execution by re-invoking the graph using `Command`, which then becomes the return value of the `interrupt()` call from inside the node.

Unlike static breakpoints (which pause before or after specific nodes), interrupts are **dynamic**—they can be placed anywhere in your code and can be conditional based on your application logic.

* **Checkpointing keeps your place:** the checkpointer writes the exact graph state so you can resume later, even when in an error state.
* **`thread_id` is your pointer:** set `config={"configurable": {"thread_id": ...}}` to tell the checkpointer which state to load.
* **Interrupt payloads surface as `__interrupt__`:** the values you pass to `interrupt()` return to the caller in the `__interrupt__` field so you know what the graph is waiting on.

The `thread_id` you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state.

## Pause using `interrupt`

The [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function pauses graph execution and returns a value to the caller. When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) within a node, LangGraph saves the current graph state and waits for you to resume execution with input.

To use [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt), you need:

1. A **checkpointer** to persist the graph state (use a durable checkpointer in production)
2. A **thread ID** in your config so the runtime knows which state to resume from
3. To call `interrupt()` where you want to pause (payload must be JSON-serializable)

When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt), here's what happens:

1. **Graph execution gets suspended** at the exact point where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) is called
2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)
3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)
4. **Graph waits indefinitely** until you resume execution with a response
5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call

## Resuming interrupts

After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed back to the `interrupt` call, allowing the node to continue execution with the external input.

```python  theme={null}
from langgraph.types import Command

**Examples:**

Example 1 (unknown):
```unknown
When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt), here's what happens:

1. **Graph execution gets suspended** at the exact point where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) is called
2. **State is saved** using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database)
3. **Value is returned** to the caller under `__interrupt__`; it can be any JSON-serializable value (string, object, array, etc.)
4. **Graph waits indefinitely** until you resume execution with a response
5. **Response is passed back** into the node when you resume, becoming the return value of the `interrupt()` call

## Resuming interrupts

After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed back to the `interrupt` call, allowing the node to continue execution with the external input.
```

---

## Interrupt concurrent

**URL:** llms-txt#interrupt-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/interrupt-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `interrupt` option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option does not delete the first run, but rather keeps it in the database but sets its status to `interrupted`. Below is a quick example of using the `interrupt` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can start our two runs and join the second one until it has completed:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can see that the thread has partial data from the first run + data from the second run

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the original, interrupted run was interrupted

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/interrupt-concurrent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Invoke

**URL:** llms-txt#invoke

**Contents:**
- Evaluator-optimizer
- Agents

state = orchestrator_worker.invoke({"topic": "Create a report on LLM scaling laws"})

from IPython.display import Markdown
Markdown(state["final_report"])
python Graph API theme={null}
  # Graph state
  class State(TypedDict):
      joke: str
      topic: str
      feedback: str
      funny_or_not: str

# Schema for structured output to use in evaluation
  class Feedback(BaseModel):
      grade: Literal["funny", "not funny"] = Field(
          description="Decide if the joke is funny or not.",
      )
      feedback: str = Field(
          description="If the joke is not funny, provide feedback on how to improve it.",
      )

# Augment the LLM with schema for structured output
  evaluator = llm.with_structured_output(Feedback)

# Nodes
  def llm_call_generator(state: State):
      """LLM generates a joke"""

if state.get("feedback"):
          msg = llm.invoke(
              f"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}"
          )
      else:
          msg = llm.invoke(f"Write a joke about {state['topic']}")
      return {"joke": msg.content}

def llm_call_evaluator(state: State):
      """LLM evaluates the joke"""

grade = evaluator.invoke(f"Grade the joke {state['joke']}")
      return {"funny_or_not": grade.grade, "feedback": grade.feedback}

# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator
  def route_joke(state: State):
      """Route back to joke generator or end based upon feedback from the evaluator"""

if state["funny_or_not"] == "funny":
          return "Accepted"
      elif state["funny_or_not"] == "not funny":
          return "Rejected + Feedback"

# Build workflow
  optimizer_builder = StateGraph(State)

# Add the nodes
  optimizer_builder.add_node("llm_call_generator", llm_call_generator)
  optimizer_builder.add_node("llm_call_evaluator", llm_call_evaluator)

# Add edges to connect nodes
  optimizer_builder.add_edge(START, "llm_call_generator")
  optimizer_builder.add_edge("llm_call_generator", "llm_call_evaluator")
  optimizer_builder.add_conditional_edges(
      "llm_call_evaluator",
      route_joke,
      {  # Name returned by route_joke : Name of next node to visit
          "Accepted": END,
          "Rejected + Feedback": "llm_call_generator",
      },
  )

# Compile the workflow
  optimizer_workflow = optimizer_builder.compile()

# Show the workflow
  display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))

# Invoke
  state = optimizer_workflow.invoke({"topic": "Cats"})
  print(state["joke"])
  python Functional API theme={null}
  # Schema for structured output to use in evaluation
  class Feedback(BaseModel):
      grade: Literal["funny", "not funny"] = Field(
          description="Decide if the joke is funny or not.",
      )
      feedback: str = Field(
          description="If the joke is not funny, provide feedback on how to improve it.",
      )

# Augment the LLM with schema for structured output
  evaluator = llm.with_structured_output(Feedback)

# Nodes
  @task
  def llm_call_generator(topic: str, feedback: Feedback):
      """LLM generates a joke"""
      if feedback:
          msg = llm.invoke(
              f"Write a joke about {topic} but take into account the feedback: {feedback}"
          )
      else:
          msg = llm.invoke(f"Write a joke about {topic}")
      return msg.content

@task
  def llm_call_evaluator(joke: str):
      """LLM evaluates the joke"""
      feedback = evaluator.invoke(f"Grade the joke {joke}")
      return feedback

@entrypoint()
  def optimizer_workflow(topic: str):
      feedback = None
      while True:
          joke = llm_call_generator(topic, feedback).result()
          feedback = llm_call_evaluator(joke).result()
          if feedback.grade == "funny":
              break

# Invoke
  for step in optimizer_workflow.stream("Cats", stream_mode="updates"):
      print(step)
      print("\n")
  python Using tools theme={null}
from langchain.tools import tool

**Examples:**

Example 1 (unknown):
```unknown
## Evaluator-optimizer

In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a [human-in-the-loop](/oss/python/langgraph/interrupts) determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.

Evaluator-optimizer workflows are commonly used when there's particular success criteria for a task, but iteration is required to meet that criteria. For example, there's not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=9bd0474f42b6040b14ed6968a9ab4e3c" alt="evaluator_optimizer.png" data-og-width="1004" width="1004" data-og-height="340" height="340" data-path="oss/images/evaluator_optimizer.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ab36856e5f9a518b22e71278aa8b1711 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ec597c92270278c2bac203d36b611c2 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=3ad3bfb734a0e509d9b87fdb4e808bfd 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e82bd25a463d3cdf76036649c03358a9 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d31717ae3e76243dd975a53f46e8c1f6 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/evaluator_optimizer.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=a9bb4fb1583f6ad06c0b13602cd14811 2500w" />

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Agents

Agents are typically implemented as an LLM performing actions using [tools](/oss/python/langchain/tools). They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd8da41dbf8b5e6fc9ea6bb10cb63e38" alt="agent.png" data-og-width="1732" width="1732" data-og-height="712" height="712" data-path="oss/images/agent.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f7a590604edc49cfa273b5856f3a3ee3 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=dff9b17d345fe0fea25616b3b0dc6ebf 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d53318b0c9c898a6146991691cbac058 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ea66fb96bc07c595d321b8b71e651ddb 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=b02599a3c9ba2a5c830b9a346f9d26c9 2500w" />

<Note>
  To get started with agents, see the [quickstart](/oss/python/langchain/quickstart) or read more about [how they work](/oss/python/langchain/agents) in LangChain.
</Note>
```

---

## Invoke the augmented LLM

**URL:** llms-txt#invoke-the-augmented-llm

output = structured_llm.invoke("How does Calcium CT score relate to high cholesterol?")

---

## Invoke the graph

**URL:** llms-txt#invoke-the-graph

config = {"configurable": {"thread_id": "2", "user_id": "1"}}

---

## Invoke the graph with an input and print the result

**URL:** llms-txt#invoke-the-graph-with-an-input-and-print-the-result

**Contents:**
  - Pass private state between nodes

print(graph.invoke({"question": "hi"}))

{'answer': 'bye'}
python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
Notice that the output of invoke only includes the output schema.

### Pass private state between nodes

In some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn't need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.

Below, we'll create an example sequential graph consisting of three nodes (node\_1, node\_2 and node\_3), where private data is passed between the first two steps (node\_1 and node\_2), while the third step (node\_3) only has access to the public overall state.
```

---

## Invoke the graph with the initial state

**URL:** llms-txt#invoke-the-graph-with-the-initial-state

**Contents:**
  - Use Pydantic models for graph state

response = graph.invoke(
    {
        "a": "set at start",
    }
)

print()
print(f"Output of graph invocation: {response}")

Entered node `node_1`:
    Input: {'a': 'set at start'}.
    Returned: {'private_data': 'set by node_1'}
Entered node `node_2`:
    Input: {'private_data': 'set by node_1'}.
    Returned: {'a': 'set by node_2'}
Entered node `node_3`:
    Input: {'a': 'set by node_2'}.
    Returned: {'a': 'set by node_3'}

Output of graph invocation: {'a': 'set by node_3'}
python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
### Use Pydantic models for graph state

A [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.StateGraph) accepts a [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) argument on initialization that specifies the "shape" of the state that the nodes in the graph can access and update.

In our examples, we typically use a python-native `TypedDict` or [`dataclass`](https://docs.python.org/3/library/dataclasses.html) for `state_schema`, but [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) can be any [type](https://docs.python.org/3/library/stdtypes.html#type-objects).

Here, we'll see how a [Pydantic BaseModel](https://docs.pydantic.dev/latest/api/base_model/) can be used for [`state_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state_schema) to add run-time validation on **inputs**.

<Note>
  **Known Limitations**

  * Currently, the output of the graph will **NOT** be an instance of a pydantic model.
  * Run-time validation only occurs on inputs to the first node in the graph, not on subsequent nodes or outputs.
  * The validation error trace from pydantic does not show which node the error arises in.
  * Pydantic's recursive validation can be slow. For performance-sensitive applications, you may want to consider using a `dataclass` instead.
</Note>
```

---

## Invoke the LLM with input that triggers the tool call

**URL:** llms-txt#invoke-the-llm-with-input-that-triggers-the-tool-call

msg = llm_with_tools.invoke("What is 2 times 3?")

---

## In pyproject.toml

**URL:** llms-txt#in-pyproject.toml

**Contents:**
- Usage
- Custom Embeddings

[project]
dependencies = [
    "langchain>=0.3.8"
]

langchain>=0.3.8
python  theme={null}
def search_memory(state: State, *, store: BaseStore):
    # Search the store using semantic similarity
    # The namespace tuple helps organize different types of memories
    # e.g., ("user_facts", "preferences") or ("conversation", "summaries")
    results = store.search(
        namespace=("memory", "facts"),  # Organize memories by type
        query="your search query",
        limit=3  # number of results to return
    )
    return results
json  theme={null}
{
    ...
    "store": {
        "index": {
            "embed": "path/to/embedding_function.py:embed",
            "dims": 1536,
            "fields": ["$"]
        }
    }
}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Or if using [requirements.txt](/langsmith/setup-app-requirements-txt):
```

Example 2 (unknown):
```unknown
## Usage

Once configured, you can use semantic search in your [nodes](/oss/python/langgraph/graph-api#nodes). The store requires a namespace tuple to organize memories:
```

Example 3 (unknown):
```unknown
## Custom Embeddings

If you want to use custom embeddings, you can pass a path to a custom embedding function:
```

Example 4 (unknown):
```unknown
The deployment will look for the function in the specified path. The function must be async and accept a list of strings:
```

---

## In your auth handler:

**URL:** llms-txt#in-your-auth-handler:

**Contents:**
- Supported resources
- Next steps

@auth.authenticate
async def authenticate(headers: dict) -> Auth.types.MinimalUserDict:
    ...
    return {
        "identity": "user-123",
        "is_authenticated": True,
        "permissions": ["threads:write", "threads:read"]  # Define permissions in auth
    }

def _default(ctx: Auth.types.AuthContext, value: dict):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

@auth.on.threads.create
async def create_thread(ctx: Auth.types.AuthContext, value: dict):
    if "threads:write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="Unauthorized"
        )
    return _default(ctx, value)

@auth.on.threads.read
async def rbac_create(ctx: Auth.types.AuthContext, value: dict):
    if "threads:read" not in ctx.permissions and "threads:write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="Unauthorized"
        )
    return _default(ctx, value)
python  theme={null}
  @auth.on.threads.create
  async def on_thread_create(
  ctx: Auth.types.AuthContext,
  value: Auth.types.on.threads.create.value  # Specific type for thread creation
  ):
  ...

@auth.on.threads
  async def on_threads(
  ctx: Auth.types.AuthContext,
  value: Auth.types.on.threads.value  # Union type of all thread actions
  ):
  ...

@auth.on
  async def on_all(
  ctx: Auth.types.AuthContext,
  value: dict  # Union type of all possible actions
  ):
  ...
  ```

More specific handlers provide better type hints since they handle fewer action types.
</Tip>

<a id="supported-actions" />

#### Supported actions and types

Here are all the supported action handlers:

| Resource       | Handler                       | Description                | Value Type                                                                                                                       |
| -------------- | ----------------------------- | -------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Threads**    | `@auth.on.threads.create`     | Thread creation            | [`ThreadsCreate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsCreate)       |
|                | `@auth.on.threads.read`       | Thread retrieval           | [`ThreadsRead`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsRead)           |
|                | `@auth.on.threads.update`     | Thread updates             | [`ThreadsUpdate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsUpdate)       |
|                | `@auth.on.threads.delete`     | Thread deletion            | [`ThreadsDelete`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsDelete)       |
|                | `@auth.on.threads.search`     | Listing threads            | [`ThreadsSearch`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.ThreadsSearch)       |
|                | `@auth.on.threads.create_run` | Creating or updating a run | [`RunsCreate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.RunsCreate)             |
| **Assistants** | `@auth.on.assistants.create`  | Assistant creation         | [`AssistantsCreate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsCreate) |
|                | `@auth.on.assistants.read`    | Assistant retrieval        | [`AssistantsRead`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsRead)     |
|                | `@auth.on.assistants.update`  | Assistant updates          | [`AssistantsUpdate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsUpdate) |
|                | `@auth.on.assistants.delete`  | Assistant deletion         | [`AssistantsDelete`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsDelete) |
|                | `@auth.on.assistants.search`  | Listing assistants         | [`AssistantsSearch`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AssistantsSearch) |
| **Crons**      | `@auth.on.crons.create`       | Cron job creation          | [`CronsCreate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsCreate)           |
|                | `@auth.on.crons.read`         | Cron job retrieval         | [`CronsRead`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsRead)               |
|                | `@auth.on.crons.update`       | Cron job updates           | [`CronsUpdate`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsUpdate)           |
|                | `@auth.on.crons.delete`       | Cron job deletion          | [`CronsDelete`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsDelete)           |
|                | `@auth.on.crons.search`       | Listing cron jobs          | [`CronsSearch`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.CronsSearch)           |

<Note>
  "About Runs"

Runs are scoped to their parent thread for access control. This means permissions are typically inherited from the thread, reflecting the conversational nature of the data model. All run operations (reading, listing) except creation are controlled by the thread's handlers.
  There is a specific `create_run` handler for creating new runs because it had more arguments that you can view in the handler.
</Note>

For implementation details:

* Check out the introductory tutorial on [setting up authentication](/langsmith/set-up-custom-auth)
* See the how-to guide on implementing a [custom auth handlers](/langsmith/custom-auth)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/auth.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Supported resources

LangGraph provides three levels of authorization handlers, from most general to most specific:

1. **Global Handler** (`@auth.on`): Matches all resources and actions
2. **Resource Handler** (e.g., `@auth.on.threads`, `@auth.on.assistants`, `@auth.on.crons`): Matches all actions for a specific resource
3. **Action Handler** (e.g., `@auth.on.threads.create`, `@auth.on.threads.read`): Matches a specific action on a specific resource

The most specific matching handler will be used. For example, `@auth.on.threads.create` takes precedence over `@auth.on.threads` for thread creation.
If a more specific handler is registered, the more general handler will not be called for that resource and action.

<Tip>
  "Type Safety"
  Each handler has type hints available for its `value` parameter at `Auth.types.on.<resource>.<action>.value`. For example:
```

---

## is a distance metric that varies inversely with similarity.

**URL:** llms-txt#is-a-distance-metric-that-varies-inversely-with-similarity.

**Contents:**
- 4. Retrievers
- Next steps

results = vector_store.similarity_search_with_score("What was Nike's revenue in 2023?")
doc, score = results[0]
print(f"Score: {score}\n")
print(doc)
python  theme={null}
Score: 0.23699893057346344

page_content='Table of Contents
FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS
The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line:
FISCAL 2023 COMPARED TO FISCAL 2022
•NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively.
The increase was due to higher revenues in North America, Europe, Middle East & Africa ("EMEA"), APLA and Greater China, which contributed approximately 7, 6,
2 and 1 percentage points to NIKE, Inc. Revenues, respectively.
•NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This
increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale
equivalent basis.' metadata={'page': 35, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}
python  theme={null}
embedding = embeddings.embed_query("How were Nike's margins impacted in 2023?")

results = vector_store.similarity_search_by_vector(embedding)
print(results[0])
python  theme={null}
page_content='Table of Contents
GROSS MARGIN
FISCAL 2023 COMPARED TO FISCAL 2022
For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to
43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following:
*Wholesale equivalent
The decrease in gross margin for fiscal 2023 was primarily due to:
•Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as
product mix;
•Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in
the prior period resulting from lower available inventory supply;
•Unfavorable changes in net foreign currency exchange rates, including hedges; and
•Lower off-price margin, on a wholesale equivalent basis.
This was partially offset by:' metadata={'page': 36, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}
python  theme={null}
from typing import List

from langchain_core.documents import Document
from langchain_core.runnables import chain

@chain
def retriever(query: str) -> List[Document]:
    return vector_store.similarity_search(query, k=1)

retriever.batch(
    [
        "How many distribution centers does Nike have in the US?",
        "When was Nike incorporated?",
    ],
)
text  theme={null}
[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],
 [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]
python  theme={null}
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 1},
)

retriever.batch(
    [
        "How many distribution centers does Nike have in the US?",
        "When was Nike incorporated?",
    ],
)
text  theme={null}
[[Document(metadata={'page': 4, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 3125}, page_content='direct to consumer operations sell products through the following number of retail stores in the United States:\nU.S. RETAIL STORES NUMBER\nNIKE Brand factory stores 213 \nNIKE Brand in-line stores (including employee-only stores) 74 \nConverse stores (including factory stores) 82 \nTOTAL 369 \nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\n2023 FORM 10-K 2')],
 [Document(metadata={'page': 3, 'source': '../example_data/nke-10k-2023.pdf', 'start_index': 0}, page_content='Table of Contents\nPART I\nITEM 1. BUSINESS\nGENERAL\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this "Annual Report"), the terms "we," "us," "our,"\n"NIKE" and the "Company" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\nand sales through our digital platforms (also referred to as "NIKE Brand Digital"), to retail accounts and to a mix of independent distributors, licensees and sales')]]
```

`VectorStoreRetriever` supports search types of `"similarity"` (default), `"mmr"` (maximum marginal relevance, described above), and `"similarity_score_threshold"`. We can use the latter to threshold documents output by the retriever by similarity score.

Retrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/oss/python/langchain/retrieval) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/oss/python/langchain/rag) tutorial.

You've now seen how to build a semantic search engine over a PDF document.

For more on document loaders:

* [Overview](/oss/python/langchain/retrieval#document_loaders)
* [Available integrations](/oss/python/integrations/document_loaders/)

For more on embeddings:

* [Overview](/oss/python/langchain/retrieval#embedding_models/)
* [Available integrations](/oss/python/integrations/text_embedding/)

For more on vector stores:

* [Overview](/oss/python/langchain/retrieval#vectorstores/)
* [Available integrations](/oss/python/integrations/vectorstores/)

For more on RAG, see:

* [Build a Retrieval Augmented Generation (RAG) App](/oss/python/langchain/rag/)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/knowledge-base.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
Return documents based on similarity to an embedded query:
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
Learn more:

* [API Reference](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore)
* [Integration-specific docs](/oss/python/integrations/vectorstores)

## 4. Retrievers

LangChain [`VectorStore`](https://reference.langchain.com/python/langchain_core/vectorstores/?h=#langchain_core.vectorstores.base.VectorStore) objects do not subclass [Runnable](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.Runnable). LangChain [Retrievers](https://reference.langchain.com/python/langchain_core/retrievers/#langchain_core.retrievers.BaseRetriever) are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).

We can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity_search` method:
```

---

## It's not something you will have in your actual code.

**URL:** llms-txt#it's-not-something-you-will-have-in-your-actual-code.

@task()
def get_info():
    """
    Simulates a task that fails once before succeeding.
    Raises an exception on the first attempt, then returns "OK" on subsequent tries.
    """
    global attempts
    attempts += 1

if attempts < 2:
        raise ValueError("Failure")  # Simulate a failure on the first attempt
    return "OK"

---

## it will take precedence for any "create" actions on the "threads" resources

**URL:** llms-txt#it-will-take-precedence-for-any-"create"-actions-on-the-"threads"-resources

@auth.on.threads.create
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create.value
):
    # Reject if the user does not have write access
    if "write" not in ctx.permissions:
        raise Auth.exceptions.HTTPException(
            status_code=403,
            detail="User lacks the required permissions."
        )
    # Setting metadata on the thread being created
    # will ensure that the resource contains an "owner" field
    # Then any time a user tries to access this thread or runs within the thread,
    # we can filter by owner
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

---

## it will take precedence for any "read" actions on the "threads" resource

**URL:** llms-txt#it-will-take-precedence-for-any-"read"-actions-on-the-"threads"-resource

@auth.on.threads.read
async def on_thread_read(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.read.value
):
    # Since we are reading (and not creating) a thread,
    # we don't need to set metadata. We just need to
    # return a filter to ensure users can only see their own threads
    return {"owner": ctx.user.identity}

---

## Join Run

**URL:** llms-txt#join-run

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/join-run

langsmith/agent-server-openapi.json get /threads/{thread_id}/runs/{run_id}/join
Wait for a run to finish.

---

## Join Run Stream

**URL:** llms-txt#join-run-stream

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/join-run-stream

langsmith/agent-server-openapi.json get /threads/{thread_id}/runs/{run_id}/stream
Join a run stream. This endpoint streams output in real-time from a run similar to the /threads/__THREAD_ID__/runs/stream endpoint. If the run has been created with `stream_resumable=true`, the stream can be resumed from the last seen event ID.

---

## Join Thread Stream

**URL:** llms-txt#join-thread-stream

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/join-thread-stream

langsmith/agent-server-openapi.json get /threads/{thread_id}/stream
This endpoint streams output in real-time from a thread. The stream will include the output of each run executed sequentially on the thread and will remain open indefinitely. It is the responsibility of the calling client to close the connection.

---

## Judge LLM

**URL:** llms-txt#judge-llm

grader_llm = init_chat_model("gpt-4o-mini", temperature=0).with_structured_output(Grade, method="json_schema", strict=True)

---

## Keep our previous handlers...

**URL:** llms-txt#keep-our-previous-handlers...

from langgraph_sdk import Auth

@auth.on.threads.create
async def on_thread_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.threads.create.value,
):
    """Add owner when creating threads.

This handler runs when creating new threads and does two things:
    1. Sets metadata on the thread being created to track ownership
    2. Returns a filter that ensures only the creator can access it
    """
    # Example value:
    #  {'thread_id': UUID('99b045bc-b90b-41a8-b882-dabc541cf740'), 'metadata': {}, 'if_exists': 'raise'}

# Add owner metadata to the thread being created
    # This metadata is stored with the thread and persists
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity

# Return filter to restrict access to just the creator
    return {"owner": ctx.user.identity}

@auth.on.threads.read
async def on_thread_read(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.threads.read.value,
):
    """Only let users read their own threads.

This handler runs on read operations. We don't need to set
    metadata since the thread already exists - we just need to
    return a filter to ensure users can only see their own threads.
    """
    return {"owner": ctx.user.identity}

@auth.on.assistants
async def on_assistants(
    ctx: Auth.types.AuthContext,
    value: Auth.types.on.assistants.value,
):
    # For illustration purposes, we will deny all requests
    # that touch the assistants resource
    # Example value:
    # {
    #     'assistant_id': UUID('63ba56c3-b074-4212-96e2-cc333bbc4eb4'),
    #     'graph_id': 'agent',
    #     'config': {},
    #     'metadata': {},
    #     'name': 'Untitled'
    # }
    raise Auth.exceptions.HTTPException(
        status_code=403,
        detail="User lacks the required permissions.",
    )

---

## Keep our resource authorization from the previous tutorial

**URL:** llms-txt#keep-our-resource-authorization-from-the-previous-tutorial

**Contents:**
- 4. Test authentication flow

@auth.on
async def add_owner(ctx, value):
    """Make resources private to their creator using resource metadata."""
    filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)
    return filters
python  theme={null}
import os
import httpx
from getpass import getpass
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
The most important change is that we're now validating tokens with a real authentication server. Our authentication handler has the private key for our Supabase project, which we can use to validate the user's token and extract their information.

## 4. Test authentication flow

Let's test out the new authentication flow. You can run the following code in a file or notebook. You will need to provide:

* A valid email address
* A Supabase project URL (from [above](#setup-auth-provider))
* A Supabase anon **public key** (also from [above](#setup-auth-provider))
```

---

## Keep our test users from the previous tutorial

**URL:** llms-txt#keep-our-test-users-from-the-previous-tutorial

**Contents:**
- 2. Test private conversations

VALID_TOKENS = {
    "user1-token": {"id": "user1", "name": "Alice"},
    "user2-token": {"id": "user2", "name": "Bob"},
}

@auth.authenticate
async def get_current_user(authorization: str | None) -> Auth.types.MinimalUserDict:
    """Our authentication handler from the previous tutorial."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"

if token not in VALID_TOKENS:
        raise Auth.exceptions.HTTPException(status_code=401, detail="Invalid token")

user_data = VALID_TOKENS[token]
    return {
        "identity": user_data["id"],
    }

@auth.on
async def add_owner(
    ctx: Auth.types.AuthContext,  # Contains info about the current user
    value: dict,  # The resource being created/accessed
):
    """Make resources private to their creator."""
    # Examples:
    # ctx: AuthContext(
    #     permissions=[],
    #     user=ProxyUser(
    #         identity='user1',
    #         is_authenticated=True,
    #         display_name='user1'
    #     ),
    #     resource='threads',
    #     action='create_run'
    # )
    # value:
    # {
    #     'thread_id': UUID('1e1b2733-303f-4dcd-9620-02d370287d72'),
    #     'assistant_id': UUID('fe096781-5601-53d2-b2f6-0d3403f7e9ca'),
    #     'run_id': UUID('1efbe268-1627-66d4-aa8d-b956b0f02a41'),
    #     'status': 'pending',
    #     'metadata': {},
    #     'prevent_insert_if_inflight': True,
    #     'multitask_strategy': 'reject',
    #     'if_not_exists': 'reject',
    #     'after_seconds': 0,
    #     'kwargs': {
    #         'input': {'messages': [{'role': 'user', 'content': 'Hello!'}]},
    #         'command': None,
    #         'config': {
    #             'configurable': {
    #                 'langgraph_auth_user': ... Your user object...
    #                 'langgraph_auth_user_id': 'user1'
    #             }
    #         },
    #         'stream_mode': ['values'],
    #         'interrupt_before': None,
    #         'interrupt_after': None,
    #         'webhook': None,
    #         'feedback_keys': None,
    #         'temporary': False,
    #         'subgraphs': False
    #     }
    # }

# Does 2 things:
    # 1. Add the user's ID to the resource's metadata. Each LangGraph resource has a `metadata` dict that persists with the resource.
    # this metadata is useful for filtering in read and update operations
    # 2. Return a filter that lets users only see their own resources
    filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)

# Only let users see their own resources
    return filters
python  theme={null}
from langgraph_sdk import get_client

**Examples:**

Example 1 (unknown):
```unknown
The handler receives two parameters:

1. `ctx` ([AuthContext](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.types.AuthContext)): contains info about the current `user`, the user's `permissions`, the `resource` ("threads", "crons", "assistants"), and the `action` being taken ("create", "read", "update", "delete", "search", "create\_run")
2. `value` (`dict`): data that is being created or accessed. The contents of this dict depend on the resource and action being accessed. See [adding scoped authorization handlers](#scoped-authorization) below for information on how to get more tightly scoped access control.

Notice that the simple handler does two things:

1. Adds the user's ID to the resource's metadata.
2. Returns a metadata filter so users only see resources they own.

## 2. Test private conversations

Test your authorization. If you have set things up correctly, you will see all ✅ messages. Be sure to have your development server running (run `langgraph dev`):
```

---

## Key-value stores

**URL:** llms-txt#key-value-stores

**Contents:**
- Overview
- Interface
- Built-in stores for local development
- Custom stores
- All key-value stores

Source: https://docs.langchain.com/oss/python/integrations/stores/index

LangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching [embeddings](/oss/python/integrations/text_embedding).

All [`BaseStores`](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) support the following interface:

* `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist
* `mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys
* `mdelete(key: Sequence[str]) -> None`: delete multiple keys
* `yield_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix

<Note>
  Base stores are designed to work **multiple** key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.
</Note>

## Built-in stores for local development

<Columns cols={2}>
  <Card title="InMemoryByteStore" icon="link" href="/oss/python/integrations/stores/in_memory" arrow="true" cta="View guide" />

<Card title="LocalFileStore" icon="link" href="/oss/python/integrations/stores/file_system" arrow="true" cta="View guide" />
</Columns>

You can also implement your own custom store by extending the [`BaseStore`](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) class. See the [store interface documentation](https://python.langchain.com/api_reference/core/stores/langchain_core.stores.BaseStore.html) for more details.

## All key-value stores

<Columns cols={3}>
  <Card title="AstraDBByteStore" icon="link" href="/oss/python/integrations/stores/astradb" arrow="true" cta="View guide" />

<Card title="CassandraByteStore" icon="link" href="/oss/python/integrations/stores/cassandra" arrow="true" cta="View guide" />

<Card title="ElasticsearchEmbeddingsCache" icon="link" href="/oss/python/integrations/stores/elasticsearch" arrow="true" cta="View guide" />

<Card title="RedisStore" icon="link" href="/oss/python/integrations/stores/redis" arrow="true" cta="View guide" />

<Card title="UpstashRedisByteStore" icon="link" href="/oss/python/integrations/stores/upstash_redis" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/stores/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangChain Academy

**URL:** llms-txt#langchain-academy

Source: https://docs.langchain.com/oss/python/langchain/academy

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/academy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangChain integrations packages

**URL:** llms-txt#langchain-integrations-packages

**Contents:**
- Popular providers
- All providers

Source: https://docs.langchain.com/oss/python/integrations/providers/overview

{/* Do not manually edit */}

LangChain offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.

<Columns cols={3}>
  <Card title="Chat models" icon="message" href="/oss/python/integrations/chat" arrow />

<Card title="Embedding models" icon="layer-group" href="/oss/python/integrations/text_embedding" arrow />

<Card title="Tools and toolkits" icon="screwdriver-wrench" href="/oss/python/integrations/tools" arrow />
</Columns>

To see a full list of integrations by component type, refer to the categories in the sidebar.

| Provider                                                            | Package                                                                                                               | Downloads                                                                                                                                                                                                                         | Latest version                                                                                                                                                                                                                                           | <Tooltip tip="Whether an equivalent version exists in the TypeScript version of LangChain. Click the checkmark to visit the respective package.">JS/TS support</Tooltip> |
| :------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [OpenAI](/oss/python/integrations/providers/openai/)                | [`langchain-openai`](https://reference.langchain.com/python/integrations/langchain_openai/)                           | <a href="https://pypi.org/project/langchain-openai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-openai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-openai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-openai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/openai)                                                                                                                     |
| [Google (Vertex AI)](/oss/python/integrations/providers/google)     | [`langchain-google-vertexai`](https://reference.langchain.com/python/integrations/langchain_google_vertexai/)         | <a href="https://pypi.org/project/langchain-google-vertexai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-google-vertexai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>         | <a href="https://pypi.org/project/langchain-google-vertexai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-google-vertexai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>         | [✅](https://www.npmjs.com/package/@langchain/google-vertexai)                                                                                                            |
| [Anthropic (Claude)](/oss/python/integrations/providers/anthropic/) | [`langchain-anthropic`](https://reference.langchain.com/python/integrations/langchain_anthropic/)                     | <a href="https://pypi.org/project/langchain-anthropic/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-anthropic/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                     | <a href="https://pypi.org/project/langchain-anthropic/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-anthropic?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/anthropic)                                                                                                                  |
| [AWS](/oss/python/integrations/providers/aws/)                      | [`langchain-aws`](https://reference.langchain.com/python/integrations/langchain_aws/)                                 | <a href="https://pypi.org/project/langchain-aws/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-aws/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                                 | <a href="https://pypi.org/project/langchain-aws/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-aws?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/aws)                                                                                                                        |
| [Google (GenAI)](/oss/python/integrations/providers/google)         | [`langchain-google-genai`](https://reference.langchain.com/python/integrations/langchain_google_genai/)               | <a href="https://pypi.org/project/langchain-google-genai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-google-genai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>               | <a href="https://pypi.org/project/langchain-google-genai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-google-genai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>               | [✅](https://www.npmjs.com/package/@langchain/google-genai)                                                                                                               |
| [Ollama](/oss/python/integrations/providers/ollama/)                | [`langchain-ollama`](https://reference.langchain.com/python/integrations/langchain_ollama/)                           | <a href="https://pypi.org/project/langchain-ollama/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-ollama/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-ollama/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-ollama?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/ollama)                                                                                                                     |
| [Chroma](/oss/python/integrations/providers/chroma/)                | [`langchain-chroma`](https://reference.langchain.com/python/integrations/langchain_chroma/)                           | <a href="https://pypi.org/project/langchain-chroma/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-chroma/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-chroma/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-chroma?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Groq](/oss/python/integrations/providers/groq/)                    | [`langchain-groq`](https://reference.langchain.com/python/integrations/langchain_groq/)                               | <a href="https://pypi.org/project/langchain-groq/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-groq/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                               | <a href="https://pypi.org/project/langchain-groq/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-groq?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                               | [✅](https://www.npmjs.com/package/@langchain/groq)                                                                                                                       |
| [Huggingface](/oss/python/integrations/providers/huggingface/)      | [`langchain-huggingface`](https://reference.langchain.com/python/integrations/langchain_huggingface/)                 | <a href="https://pypi.org/project/langchain-huggingface/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-huggingface/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                 | <a href="https://pypi.org/project/langchain-huggingface/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-huggingface?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                 | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Pinecone](/oss/python/integrations/providers/pinecone/)            | [`langchain-pinecone`](https://reference.langchain.com/python/integrations/langchain_pinecone/)                       | <a href="https://pypi.org/project/langchain-pinecone/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-pinecone/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-pinecone/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-pinecone?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/pinecone)                                                                                                                   |
| [Cohere](/oss/python/integrations/providers/cohere/)                | [`langchain-cohere`](https://reference.langchain.com/python/integrations/langchain_cohere/)                           | <a href="https://pypi.org/project/langchain-cohere/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-cohere/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-cohere/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-cohere?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/cohere)                                                                                                                     |
| [Postgres](/oss/python/integrations/providers/pgvector)             | [`langchain-postgres`](https://reference.langchain.com/python/integrations/langchain_postgres/)                       | <a href="https://pypi.org/project/langchain-postgres/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-postgres/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-postgres/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-postgres?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Databricks](/oss/python/integrations/providers/databricks/)        | [`databricks-langchain`](https://pypi.org/project/databricks-langchain/)                                              | <a href="https://pypi.org/project/databricks-langchain/" target="_blank"><img src="https://static.pepy.tech/badge/databricks-langchain/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                   | <a href="https://pypi.org/project/databricks-langchain/" target="_blank"><img src="https://img.shields.io/pypi/v/databricks-langchain?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                   | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [MistralAI](/oss/python/integrations/providers/mistralai/)          | [`langchain-mistralai`](https://reference.langchain.com/python/integrations/langchain_mistralai/)                     | <a href="https://pypi.org/project/langchain-mistralai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-mistralai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                     | <a href="https://pypi.org/project/langchain-mistralai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-mistralai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/mistralai)                                                                                                                  |
| [Perplexity](/oss/python/integrations/providers/perplexity/)        | [`langchain-perplexity`](https://reference.langchain.com/python/integrations/langchain_perplexity/)                   | <a href="https://pypi.org/project/langchain-perplexity/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-perplexity/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                   | <a href="https://pypi.org/project/langchain-perplexity/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-perplexity?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                   | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [MongoDB](/oss/python/integrations/providers/mongodb_atlas)         | [`langchain-mongodb`](https://reference.langchain.com/python/integrations/langchain_mongodb/)                         | <a href="https://pypi.org/project/langchain-mongodb/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-mongodb/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                         | <a href="https://pypi.org/project/langchain-mongodb/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-mongodb?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                         | [✅](https://www.npmjs.com/package/@langchain/mongodb)                                                                                                                    |
| [Deepseek](/oss/python/integrations/providers/deepseek/)            | [`langchain-deepseek`](https://reference.langchain.com/python/integrations/langchain_deepseek/)                       | <a href="https://pypi.org/project/langchain-deepseek/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-deepseek/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-deepseek/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-deepseek?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/deepseek)                                                                                                                   |
| [Fireworks](/oss/python/integrations/providers/fireworks/)          | [`langchain-fireworks`](https://reference.langchain.com/python/integrations/langchain_fireworks/)                     | <a href="https://pypi.org/project/langchain-fireworks/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-fireworks/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                     | <a href="https://pypi.org/project/langchain-fireworks/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-fireworks?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                     | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Nvidia AI Endpoints](/oss/python/integrations/providers/nvidia)    | [`langchain-nvidia-ai-endpoints`](https://reference.langchain.com/python/integrations/langchain_nvidia_ai_endpoints/) | <a href="https://pypi.org/project/langchain-nvidia-ai-endpoints/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-nvidia-ai-endpoints/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a> | <a href="https://pypi.org/project/langchain-nvidia-ai-endpoints/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-nvidia-ai-endpoints?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a> | ❌                                                                                                                                                                        |
| [IBM](/oss/python/integrations/providers/ibm/)                      | [`langchain-ibm`](https://reference.langchain.com/python/integrations/langchain_ibm/)                                 | <a href="https://pypi.org/project/langchain-ibm/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-ibm/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                                 | <a href="https://pypi.org/project/langchain-ibm/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-ibm?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/ibm)                                                                                                                        |
| [Qdrant](/oss/python/integrations/providers/qdrant/)                | [`langchain-qdrant`](https://reference.langchain.com/python/integrations/langchain_qdrant/)                           | <a href="https://pypi.org/project/langchain-qdrant/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-qdrant/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-qdrant/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-qdrant?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/qdrant)                                                                                                                     |
| [Milvus](/oss/python/integrations/providers/milvus/)                | [`langchain-milvus`](https://reference.langchain.com/python/integrations/langchain_milvus/)                           | <a href="https://pypi.org/project/langchain-milvus/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-milvus/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-milvus/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-milvus?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Tavily](/oss/python/integrations/providers/tavily/)                | [`langchain-tavily`](https://pypi.org/project/langchain-tavily/)                                                      | <a href="https://pypi.org/project/langchain-tavily/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-tavily/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                           | <a href="https://pypi.org/project/langchain-tavily/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-tavily?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                           | [✅](https://www.npmjs.com/package/@langchain/tavily)                                                                                                                     |
| [xAI (Grok)](/oss/python/integrations/providers/xai/)               | [`langchain-xai`](https://reference.langchain.com/python/integrations/langchain_xai/)                                 | <a href="https://pypi.org/project/langchain-xai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-xai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                                 | <a href="https://pypi.org/project/langchain-xai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-xai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                                 | [✅](https://www.npmjs.com/package/@langchain/xai)                                                                                                                        |
| [Elasticsearch](/oss/python/integrations/providers/elasticsearch/)  | [`langchain-elasticsearch`](https://reference.langchain.com/python/integrations/langchain_elasticsearch/)             | <a href="https://pypi.org/project/langchain-elasticsearch/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-elasticsearch/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>             | <a href="https://pypi.org/project/langchain-elasticsearch/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-elasticsearch?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>             | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Azure AI](/oss/python/integrations/providers/azure_ai)             | [`langchain-azure-ai`](https://reference.langchain.com/python/integrations/langchain_azure_ai/)                       | <a href="https://pypi.org/project/langchain-azure-ai/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-azure-ai/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-azure-ai/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-azure-ai?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/openai)                                                                                                                     |
| [DataStax Astra DB](/oss/python/integrations/providers/astradb/)    | [`langchain-astradb`](https://reference.langchain.com/python/integrations/langchain_astradb/)                         | <a href="https://pypi.org/project/langchain-astradb/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-astradb/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                         | <a href="https://pypi.org/project/langchain-astradb/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-astradb?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                         | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [LiteLLM](/oss/python/integrations/providers/litellm/)              | [`langchain-litellm`](https://pypi.org/project/langchain-litellm/)                                                    | <a href="https://pypi.org/project/langchain-litellm/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-litellm/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                         | <a href="https://pypi.org/project/langchain-litellm/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-litellm?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                         | N/A                                                                                                                                                                      |
| [Redis](/oss/python/integrations/providers/redis/)                  | [`langchain-redis`](https://reference.langchain.com/python/integrations/langchain_redis/)                             | <a href="https://pypi.org/project/langchain-redis/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-redis/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                             | <a href="https://pypi.org/project/langchain-redis/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-redis?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                             | [✅](https://www.npmjs.com/package/@langchain/redis)                                                                                                                      |
| [Together](/oss/python/integrations/providers/together/)            | [`langchain-together`](https://reference.langchain.com/python/integrations/langchain_together/)                       | <a href="https://pypi.org/project/langchain-together/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-together/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                       | <a href="https://pypi.org/project/langchain-together/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-together?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                       | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [MCP Toolbox (Google)](/oss/python/integrations/providers/toolbox/) | [`toolbox-langchain`](https://pypi.org/project/toolbox-langchain/)                                                    | <a href="https://pypi.org/project/toolbox-langchain/" target="_blank"><img src="https://static.pepy.tech/badge/toolbox-langchain/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                         | <a href="https://pypi.org/project/toolbox-langchain/" target="_blank"><img src="https://img.shields.io/pypi/v/toolbox-langchain?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                         | ❌                                                                                                                                                                        |
| [Google (Community)](/oss/python/integrations/providers/google)     | [`langchain-google-community`](https://reference.langchain.com/python/integrations/langchain_google_community/)       | <a href="https://pypi.org/project/langchain-google-community/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-google-community/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>       | <a href="https://pypi.org/project/langchain-google-community/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-google-community?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>       | ❌                                                                                                                                                                        |
| [Unstructured](/oss/python/integrations/providers/unstructured/)    | [`langchain-unstructured`](https://reference.langchain.com/python/integrations/langchain_unstructured/)               | <a href="https://pypi.org/project/langchain-unstructured/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-unstructured/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>               | <a href="https://pypi.org/project/langchain-unstructured/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-unstructured?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>               | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Neo4J](/oss/python/integrations/providers/neo4j/)                  | [`langchain-neo4j`](https://reference.langchain.com/python/integrations/langchain_neo4j/)                             | <a href="https://pypi.org/project/langchain-neo4j/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-neo4j/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                             | <a href="https://pypi.org/project/langchain-neo4j/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-neo4j?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                             | [✅](https://www.npmjs.com/package/@langchain/community)                                                                                                                  |
| [Graph RAG](/oss/python/integrations/providers/graph_rag)           | [`langchain-graph-retriever`](https://pypi.org/project/langchain-graph-retriever/)                                    | <a href="https://pypi.org/project/langchain-graph-retriever/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-graph-retriever/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>         | <a href="https://pypi.org/project/langchain-graph-retriever/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-graph-retriever?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>         | ❌                                                                                                                                                                        |
| [Sambanova](/oss/python/integrations/providers/sambanova/)          | [`langchain-sambanova`](https://pypi.org/project/langchain-sambanova/)                                                | <a href="https://pypi.org/project/langchain-sambanova/" target="_blank"><img src="https://static.pepy.tech/badge/langchain-sambanova/month" alt="Downloads per month" noZoom class="rounded not-prose" /></a>                     | <a href="https://pypi.org/project/langchain-sambanova/" target="_blank"><img src="https://img.shields.io/pypi/v/langchain-sambanova?style=flat-square&label=%20" alt="PyPI - Latest version" noZoom class="rounded not-prose" /></a>                     | ❌                                                                                                                                                                        |

[See all providers](/oss/python/integrations/providers/all_providers) or search for a provider using the search field.

Community integrations can be found in [`langchain-community`](https://github.com/langchain-ai/langchain-community).

<Info>
  If you'd like to contribute an integration, see the [contributing guide](/oss/python/contributing).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/overview.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangChain SDK

**URL:** llms-txt#langchain-sdk

Source: https://docs.langchain.com/oss/python/reference/langchain-python

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/langchain-python.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangGraph CLI

**URL:** llms-txt#langgraph-cli

**Contents:**
- Installation
  - Quick commands
- Configuration file
  - Examples
- Commands
  - `dev`
  - `build`
  - `up`
  - `dockerfile`

Source: https://docs.langchain.com/langsmith/cli

**LangGraph CLI** is a command-line tool for building and running the [Agent Server](/langsmith/agent-server) locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage.

1. Ensure Docker is installed (e.g., `docker --version`).

3. Verify the install

| Command                               | What it does                                                                                                                         |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| [`langgraph dev`](#dev)               | Starts a lightweight local dev server (no Docker required), ideal for rapid testing.                                                 |
| [`langgraph build`](#build)           | Builds a Docker image of your LangGraph API server for deployment.                                                                   |
| [`langgraph dockerfile`](#dockerfile) | Emits a Dockerfile derived from your config for custom builds.                                                                       |
| [`langgraph up`](#up)                 | Starts the LangGraph API server locally in Docker. Requires Docker running; LangSmith API key for local dev; license for production. |

For JS, use `npx @langchain/langgraph-cli <command>` (or `langgraphjs` if installed globally).

## Configuration file

To build and run a valid application, the LangGraph CLI requires a JSON configuration file that follows this [schema](https://raw.githubusercontent.com/langchain-ai/langgraph/refs/heads/main/libs/cli/schemas/schema.json). It contains the following properties:

<Note>The LangGraph CLI defaults to using the configuration file named <strong>langgraph.json</strong> in the current directory.</Note>

<Tabs>
  <Tab title="Python">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    | ---------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
    | <span style={{ whiteSpace: "nowrap" }}>`dependencies`</span>     | **Required**. Array of dependencies for LangSmith API server. Dependencies can be one of the following: <ul><li>A single period (`"."`), which will look for local Python packages.</li><li>The directory path where `pyproject.toml`, `setup.py` or `requirements.txt` is located.<br />For example, if `requirements.txt` is located in the root of the project directory, specify `"./"`. If it's located in a subdirectory called `local_package`, specify `"./local_package"`. Do not specify the string `"requirements.txt"` itself.</li><li>A Python package name.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./your_package/your_file.py:variable`, where `variable` is an instance of `langgraph.graph.state.CompiledStateGraph`</li><li>`./your_package/your_file.py:make_graph`, where `make_graph` is a function that takes a config dictionary (`langchain_core.runnables.RunnableConfig`) and returns an instance of `langgraph.graph.state.StateGraph` or `langgraph.graph.state.CompiledStateGraph`. See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | <span style={{ whiteSpace: "nowrap" }}>`auth`</span>             | *(Added in v0.0.11)* Auth configuration containing the path to your authentication handler. Example: `./your_package/auth.py:auth`, where `auth` is an instance of `langgraph_sdk.Auth`. See [authentication guide](/langsmith/auth) for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | <span style={{ whiteSpace: "nowrap" }}>`base_image`</span>       | Optional. Base image to use for the LangGraph API server. Defaults to `langchain/langgraph-api` or `langchain/langgraphjs-api`. Use this to pin your builds to a particular version of the langgraph API, such as `"langchain/langgraph-server:0.2"`. See [https://hub.docker.com/r/langchain/langgraph-server/tags](https://hub.docker.com/r/langchain/langgraph-server/tags) for more details. (added in `langgraph-cli==0.2.8`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`image_distro`</span>     | Optional. Linux distribution for the base image. Must be one of `"debian"`, `"wolfi"`, `"bookworm"`, or `"bullseye"`. If omitted, defaults to `"debian"`. Available in `langgraph-cli>=0.2.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    | <span style={{ whiteSpace: "nowrap" }}>`ui`</span>               | Optional. Named definitions of UI components emitted by the agent, each pointing to a JS/TS file. (added in `langgraph-cli==0.1.84`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
    | <span style={{ whiteSpace: "nowrap" }}>`python_version`</span>   | `3.11`, `3.12`, or `3.13`. Defaults to `3.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_config_file`</span>  | Path to `pip` config file.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_installer`</span>    | *(Added in v0.3)* Optional. Python package installer selector. It can be set to `"auto"`, `"pip"`, or `"uv"`. From version 0.3 onward the default strategy is to run `uv pip`, which typically delivers faster builds while remaining a drop-in replacement. In the uncommon situation where `uv` cannot handle your dependency graph or the structure of your `pyproject.toml`, specify `"pip"` here to revert to the earlier behaviour.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`keep_pkg_tools`</span>   | *(Added in v0.3.4)* Optional. Control whether to retain Python packaging tools (`pip`, `setuptools`, `wheel`) in the final image. Accepted values: <ul><li><code>true</code> : Keep all three tools (skip uninstall).</li><li><code>false</code> / omitted : Uninstall all three tools (default behaviour).</li><li><code>list\[str]</code> : Names of tools <strong>to retain</strong>. Each value must be one of "pip", "setuptools", "wheel".</li></ul>. By default, all three tools are uninstalled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Supports: <ul><li>`ttl` (optional): Object with `strategy`, `sweep_interval_minutes`, `default_ttl` controlling checkpoint expiry.</li><li>`serde` (optional, 0.5+): Object with `allowed_json_modules` and `pickle_fallback` to tune deserialization behavior.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration with the following fields: <ul><li>`app`: Path to custom Starlette/FastAPI app (e.g., `"./src/agent/webapp.py:app"`). See [custom routes guide](/langsmith/custom-routes).</li><li>`cors`: CORS configuration with fields such as `allow_origins`, `allow_methods`, `allow_headers`, `allow_credentials`, `allow_origin_regex`, `expose_headers`, and `max_age`.</li><li>`configurable_headers`: Define which request headers to expose as configurable values via `includes` / `excludes` patterns.</li><li>`logging_headers`: Mirror of `configurable_headers` for excluding sensitive headers from logs.</li><li>`middleware_order`: Choose how custom middleware and auth interact. `auth_first` runs authentication hooks before custom middleware, while `middleware_first` (default) runs your middleware first.</li><li>`enable_custom_route_auth`: Apply auth checks to routes added through `app`.</li><li>`disable_assistants`, `disable_mcp`, `disable_a2a`, `disable_meta`, `disable_runs`, `disable_store`, `disable_threads`, `disable_ui`, `disable_webhooks`: Disable built-in routes or hooks.</li><li>`mount_prefix`: Prefix for mounted routes (e.g., "/my-deployment/api").</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/agent-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
  </Tab>

<Tab title="JS">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
    | ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./src/graph.ts:variable`, where `variable` is an instance of [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph)</li><li>`./src/graph.ts:makeGraph`, where `makeGraph` is a function that takes a config dictionary (`LangGraphRunnableConfig`) and returns an instance of [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph). See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Supports: <ul><li>`ttl` (optional): Object with `strategy`, `sweep_interval_minutes`, `default_ttl` controlling checkpoint expiry.</li><li>`serde` (optional, 0.5+): Object with `allowed_json_modules` and `pickle_fallback` to tune deserialization behavior.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration mirroring the Python options: <ul><li>`cors` with `allow_origins`, `allow_methods`, `allow_headers`, `allow_credentials`, `allow_origin_regex`, `expose_headers`, `max_age`.</li><li>`configurable_headers` and `logging_headers` pattern lists.</li><li>`middleware_order` (`auth_first` or `middleware_first`).</li><li>`enable_custom_route_auth` plus the same boolean route toggles as above.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/agent-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    #### Basic configuration

#### Using Wolfi base images

You can specify the Linux distribution for your base image using the `image_distro` field. Valid options are `debian`, `wolfi`, `bookworm`, or `bullseye`. Wolfi is the recommended option as it provides smaller and more secure images. This is available in `langgraph-cli>=0.2.11`.

#### Adding semantic search to the store

All deployments come with a DB-backed BaseStore. Adding an "index" configuration to your `langgraph.json` will enable [semantic search](/langsmith/semantic-search) within the BaseStore of your deployment.

The `index.fields` configuration determines which parts of your documents to embed:

* If omitted or set to `["$"]`, the entire document will be embedded
    * To embed specific fields, use JSON path notation: `["metadata.title", "content.text"]`
    * Documents missing specified fields will still be stored but won't have embeddings for those fields
    * You can still override which fields to embed on a specific item at `put` time using the `index` parameter

<Note>
      **Common model dimensions**

* `openai:text-embedding-3-large`: 3072
      * `openai:text-embedding-3-small`: 1536
      * `openai:text-embedding-ada-002`: 1536
      * `cohere:embed-english-v3.0`: 1024
      * `cohere:embed-english-light-v3.0`: 384
      * `cohere:embed-multilingual-v3.0`: 1024
      * `cohere:embed-multilingual-light-v3.0`: 384
    </Note>

#### Semantic search with a custom embedding function

If you want to use semantic search with a custom embedding function, you can pass a path to a custom embedding function:

The `embed` field in store configuration can reference a custom function that takes a list of strings and returns a list of embeddings. Example implementation:

#### Adding custom authentication

See the [authentication conceptual guide](/langsmith/auth) for details, and the [setting up custom authentication](/langsmith/set-up-custom-auth) guide for a practical walk through of the process.

#### Configuring store item Time-to-Live

You can configure default data expiration for items/memories in the BaseStore using the `store.ttl` key. This determines how long items are retained after they are last accessed (with reads potentially refreshing the timer based on `refresh_on_read`). Note that these defaults can be overwritten on a per-call basis by modifying the corresponding arguments in `get`, `search`, etc.

The `ttl` configuration is an object containing optional fields:

* `refresh_on_read`: If `true` (the default), accessing an item via `get` or `search` resets its expiration timer. Set to `false` to only refresh TTL on writes (`put`).
    * `default_ttl`: The default lifespan of an item in **minutes**. Applies only to newly created items; existing items are not modified. If not set, items do not expire by default.
    * `sweep_interval_minutes`: How frequently (in minutes) the system should run a background process to delete expired items. If not set, sweeping does not occur automatically.

Here is an example enabling a 7-day TTL (10080 minutes), refreshing on reads, and sweeping every hour:

#### Configuring checkpoint Time-to-Live

You can configure the time-to-live (TTL) for checkpoints using the `checkpointer` key. This determines how long checkpoint data is retained before being automatically handled according to the specified strategy (e.g., deletion). Two optional sub-objects are supported:

* `ttl`: Includes `strategy`, `sweep_interval_minutes`, and `default_ttl`, which collectively set how checkpoints expire.
    * `serde` *(Agent server 0.5+)* : Lets you control deserialization behavior for checkpoint payloads.

Here's an example setting a default TTL of 30 days (43200 minutes):

In this example, checkpoints older than 30 days will be deleted, and the check runs every 10 minutes.

#### Configuring checkpointer serde

The `checkpointer.serde` object shapes deserialization:

* `allowed_json_modules` defines an allow list for custom Python objects you want the server to be able to deserialize from payloads saved in "json" mode. This is a list of `[path, to, module, file, symbol]` sequences. If omitted, only LangChain-safe defaults are allowed. You can unsafely set to `true` to allow any module to be deserialized.
    * `pickle_fallback`: Whether to fall back to pickle deserialization when JSON decoding fails.

#### Customizing HTTP middleware and headers

The `http` block lets you fine-tune request handling:

* `middleware_order`: Choose `"auth_first"` to run authentication before your middleware, or `"middleware_first"` (default) to invert that order.
    * `enable_custom_route_auth`: Extend authentication to routes you mount through `http.app`.
    * `configurable_headers` / `logging_headers`: Each accepts an object with optional `includes` and `excludes` arrays; wildcards are supported and exclusions run before inclusions.
    * `cors`: In addition to `allow_origins`, `allow_methods`, and `allow_headers`, you can set `allow_credentials`, `allow_origin_regex`, `expose_headers`, and `max_age` for detailed browser control.

<a id="api-version" />

#### Pinning API version

You can pin the API version of the Agent Server by using the `api_version` key. This is useful if you want to ensure that your server uses a specific version of the API.
    By default, builds in Cloud deployments use the latest stable version of the server. This can be pinned by setting the `api_version` key to a specific version.

<Tab title="JS">
    #### Basic configuration

<a id="api-version" />

#### Pinning API version

You can pin the API version of the Agent Server by using the `api_version` key. This is useful if you want to ensure that your server uses a specific version of the API.
    By default, builds in Cloud deployments use the latest stable version of the server. This can be pinned by setting the `api_version` key to a specific version.

<Tabs>
  <Tab title="Python">
    The base command for the LangGraph CLI is `langgraph`.

<Tab title="JS">
    The base command for the LangGraph.js CLI is `langgraphjs`.

We recommend using `npx` to always use the latest version of the CLI.
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Run LangGraph API server in development mode with hot reloading and debugging capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.

<Note>Currently, the CLI only supports Python >= 3.11.</Note>

This command requires the "inmem" extra to be installed:

| Option                        | Default          | Description                                                                                                                                                                  |
    | ----------------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE`           | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables                                                                                          |
    | `--host TEXT`                 | `127.0.0.1`      | Host to bind the server to                                                                                                                                                   |
    | `--port INTEGER`              | `2024`           | Port to bind the server to                                                                                                                                                   |
    | `--no-reload`                 |                  | Disable auto-reload                                                                                                                                                          |
    | `--n-jobs-per-worker INTEGER` |                  | Number of jobs per worker. Default is 10                                                                                                                                     |
    | `--debug-port INTEGER`        |                  | Port for debugger to listen on                                                                                                                                               |
    | `--wait-for-client`           | `False`          | Wait for a debugger client to connect to the debug port before starting the server                                                                                           |
    | `--no-browser`                |                  | Skip automatically opening the browser when the server starts                                                                                                                |
    | `--studio-url TEXT`           |                  | URL of the Studio instance to connect to. Defaults to [https://smith.langchain.com](https://smith.langchain.com)                                                             |
    | `--allow-blocking`            | `False`          | Do not raise errors for synchronous I/O blocking operations in your code (added in `0.2.6`)                                                                                  |
    | `--tunnel`                    | `False`          | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers like Safari or networks blocking localhost connections |
    | `--help`                      |                  | Display command documentation                                                                                                                                                |
  </Tab>

<Tab title="JS">
    Run LangGraph API server in development mode with hot reloading capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.

| Option                        | Default          | Description                                                                                                                                                      |
    | ----------------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE`           | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables                                                                              |
    | `--host TEXT`                 | `127.0.0.1`      | Host to bind the server to                                                                                                                                       |
    | `--port INTEGER`              | `2024`           | Port to bind the server to                                                                                                                                       |
    | `--no-reload`                 |                  | Disable auto-reload                                                                                                                                              |
    | `--n-jobs-per-worker INTEGER` |                  | Number of jobs per worker. Default is 10                                                                                                                         |
    | `--debug-port INTEGER`        |                  | Port for debugger to listen on                                                                                                                                   |
    | `--wait-for-client`           | `False`          | Wait for a debugger client to connect to the debug port before starting the server                                                                               |
    | `--no-browser`                |                  | Skip automatically opening the browser when the server starts                                                                                                    |
    | `--studio-url TEXT`           |                  | URL of the Studio instance to connect to. Defaults to [https://smith.langchain.com](https://smith.langchain.com)                                                 |
    | `--allow-blocking`            | `False`          | Do not raise errors for synchronous I/O blocking operations in your code                                                                                         |
    | `--tunnel`                    | `False`          | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers or networks blocking localhost connections |
    | `--help`                      |                  | Display command documentation                                                                                                                                    |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Build LangSmith API server Docker image.

| Option                                | Default          | Description                                                                                                                                             |
    | ------------------------------------- | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | `--platform TEXT`                     |                  | Target platform(s) to build the Docker image for. Example: `langgraph build --platform linux/amd64,linux/arm64`                                         |
    | `-t, --tag TEXT`                      |                  | **Required**. Tag for the Docker image. Example: `langgraph build -t my-image`                                                                          |
    | `--pull / --no-pull`                  | `--pull`         | Build with latest remote Docker image. Use `--no-pull` for running the LangSmith API server with locally built images.                                  |
    | `-c, --config FILE`                   | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables.                                                                    |
    | `--build-command TEXT`<sup>\*</sup>   |                  | Build command to run. Runs from the directory where your `langgraph.json` file lives. Example: `langgraph build --build-command "yarn run turbo build"` |
    | `--install-command TEXT`<sup>\*</sup> |                  | Install command to run. Runs from the directory where you call `langgraph build` from. Example: `langgraph build --install-command "yarn install"`      |
    | `--help`                              |                  | Display command documentation.                                                                                                                          |

<sup>\*</sup>Only supported for JS deployments, will have no impact on Python deployments.
  </Tab>

<Tab title="JS">
    Build LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `--platform TEXT`   |                  | Target platform(s) to build the Docker image for. Example: `langgraph build --platform linux/amd64,linux/arm64` |
    | `-t, --tag TEXT`    |                  | **Required**. Tag for the Docker image. Example: `langgraph build -t my-image`                                  |
    | `--no-pull`         |                  | Use locally built images. Defaults to `false` to build with latest remote Docker image.                         |
    | `-c, --config FILE` | `langgraph.json` | Path to configuration file declaring dependencies, graphs and environment variables.                            |
    | `--help`            |                  | Display command documentation.                                                                                  |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Start LangGraph API server. For local testing, requires a LangSmith API key with access to LangSmith. Requires a license key for production use.

| Option                       | Default                   | Description                                                                                                             |
    | ---------------------------- | ------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
    | `--wait`                     |                           | Wait for services to start before returning. Implies --detach                                                           |
    | `--base-image TEXT`          | `langchain/langgraph-api` | Base image to use for the LangGraph API server. Pin to specific versions using version tags.                            |
    | `--image TEXT`               |                           | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly.           |
    | `--postgres-uri TEXT`        | Local database            | Postgres URI to use for the database.                                                                                   |
    | `--watch`                    |                           | Restart on file changes                                                                                                 |
    | `--debugger-base-url TEXT`   | `http://127.0.0.1:[PORT]` | URL used by the debugger to access LangGraph API.                                                                       |
    | `--debugger-port INTEGER`    |                           | Pull the debugger image locally and serve the UI on specified port                                                      |
    | `--verbose`                  |                           | Show more output from the server logs.                                                                                  |
    | `-c, --config FILE`          | `langgraph.json`          | Path to configuration file declaring dependencies, graphs and environment variables.                                    |
    | `-d, --docker-compose FILE`  |                           | Path to docker-compose.yml file with additional services to launch.                                                     |
    | `-p, --port INTEGER`         | `8123`                    | Port to expose. Example: `langgraph up --port 8000`                                                                     |
    | `--pull / --no-pull`         | `pull`                    | Pull latest images. Use `--no-pull` for running the server with locally-built images. Example: `langgraph up --no-pull` |
    | `--recreate / --no-recreate` | `no-recreate`             | Recreate containers even if their configuration and image haven't changed                                               |
    | `--help`                     |                           | Display command documentation.                                                                                          |
  </Tab>

<Tab title="JS">
    Start LangGraph API server. For local testing, requires a LangSmith API key with access to LangSmith. Requires a license key for production use.

| Option                                                                    | Default                                                                 | Description                                                                                                   |
    | ------------------------------------------------------------------------- | ----------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`--wait`</span>                    |                                                                         | Wait for services to start before returning. Implies --detach                                                 |
    | <span style={{ whiteSpace: "nowrap" }}>`--base-image TEXT`</span>         | <span style={{ whiteSpace: "nowrap" }}>`langchain/langgraph-api`</span> | Base image to use for the LangGraph API server. Pin to specific versions using version tags.                  |
    | <span style={{ whiteSpace: "nowrap" }}>`--image TEXT`</span>              |                                                                         | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly. |
    | <span style={{ whiteSpace: "nowrap" }}>`--postgres-uri TEXT`</span>       | Local database                                                          | Postgres URI to use for the database.                                                                         |
    | <span style={{ whiteSpace: "nowrap" }}>`--watch`</span>                   |                                                                         | Restart on file changes                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`-c, --config FILE`</span>         | `langgraph.json`                                                        | Path to configuration file declaring dependencies, graphs and environment variables.                          |
    | <span style={{ whiteSpace: "nowrap" }}>`-d, --docker-compose FILE`</span> |                                                                         | Path to docker-compose.yml file with additional services to launch.                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`-p, --port INTEGER`</span>        | `8123`                                                                  | Port to expose. Example: `langgraph up --port 8000`                                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`--no-pull`</span>                 |                                                                         | Use locally built images. Defaults to `false` to build with latest remote Docker image.                       |
    | <span style={{ whiteSpace: "nowrap" }}>`--recreate`</span>                |                                                                         | Recreate containers even if their configuration and image haven't changed                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`--help`</span>                    |                                                                         | Display command documentation.                                                                                |
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    Generate a Dockerfile for building a LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE` | `langgraph.json` | Path to the [configuration file](#configuration-file) declaring dependencies, graphs and environment variables. |
    | `--help`            |                  | Show this message and exit.                                                                                     |

This generates a Dockerfile that looks similar to:

<Note>The `langgraph dockerfile` command translates all the configuration in your `langgraph.json` file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your `langgraph.json` file. Otherwise, your changes will not be reflected when you build or run the dockerfile.</Note>
  </Tab>

<Tab title="JS">
    Generate a Dockerfile for building a LangSmith API server Docker image.

| Option              | Default          | Description                                                                                                     |
    | ------------------- | ---------------- | --------------------------------------------------------------------------------------------------------------- |
    | `-c, --config FILE` | `langgraph.json` | Path to the [configuration file](#configuration-file) declaring dependencies, graphs and environment variables. |
    | `--help`            |                  | Show this message and exit.                                                                                     |

This generates a Dockerfile that looks similar to:

<Note>The `npx @langchain/langgraph-cli dockerfile` command translates all the configuration in your `langgraph.json` file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your `langgraph.json` file. Otherwise, your changes will not be reflected when you build or run the dockerfile.</Note>
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cli.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

3. Verify the install

   <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### Quick commands

| Command                               | What it does                                                                                                                         |
| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| [`langgraph dev`](#dev)               | Starts a lightweight local dev server (no Docker required), ideal for rapid testing.                                                 |
| [`langgraph build`](#build)           | Builds a Docker image of your LangGraph API server for deployment.                                                                   |
| [`langgraph dockerfile`](#dockerfile) | Emits a Dockerfile derived from your config for custom builds.                                                                       |
| [`langgraph up`](#up)                 | Starts the LangGraph API server locally in Docker. Requires Docker running; LangSmith API key for local dev; license for production. |

For JS, use `npx @langchain/langgraph-cli <command>` (or `langgraphjs` if installed globally).

## Configuration file

To build and run a valid application, the LangGraph CLI requires a JSON configuration file that follows this [schema](https://raw.githubusercontent.com/langchain-ai/langgraph/refs/heads/main/libs/cli/schemas/schema.json). It contains the following properties:

<Note>The LangGraph CLI defaults to using the configuration file named <strong>langgraph.json</strong> in the current directory.</Note>

<Tabs>
  <Tab title="Python">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    | ---------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
    | <span style={{ whiteSpace: "nowrap" }}>`dependencies`</span>     | **Required**. Array of dependencies for LangSmith API server. Dependencies can be one of the following: <ul><li>A single period (`"."`), which will look for local Python packages.</li><li>The directory path where `pyproject.toml`, `setup.py` or `requirements.txt` is located.<br />For example, if `requirements.txt` is located in the root of the project directory, specify `"./"`. If it's located in a subdirectory called `local_package`, specify `"./local_package"`. Do not specify the string `"requirements.txt"` itself.</li><li>A Python package name.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./your_package/your_file.py:variable`, where `variable` is an instance of `langgraph.graph.state.CompiledStateGraph`</li><li>`./your_package/your_file.py:make_graph`, where `make_graph` is a function that takes a config dictionary (`langchain_core.runnables.RunnableConfig`) and returns an instance of `langgraph.graph.state.StateGraph` or `langgraph.graph.state.CompiledStateGraph`. See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | <span style={{ whiteSpace: "nowrap" }}>`auth`</span>             | *(Added in v0.0.11)* Auth configuration containing the path to your authentication handler. Example: `./your_package/auth.py:auth`, where `auth` is an instance of `langgraph_sdk.Auth`. See [authentication guide](/langsmith/auth) for details.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
    | <span style={{ whiteSpace: "nowrap" }}>`base_image`</span>       | Optional. Base image to use for the LangGraph API server. Defaults to `langchain/langgraph-api` or `langchain/langgraphjs-api`. Use this to pin your builds to a particular version of the langgraph API, such as `"langchain/langgraph-server:0.2"`. See [https://hub.docker.com/r/langchain/langgraph-server/tags](https://hub.docker.com/r/langchain/langgraph-server/tags) for more details. (added in `langgraph-cli==0.2.8`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`image_distro`</span>     | Optional. Linux distribution for the base image. Must be one of `"debian"`, `"wolfi"`, `"bookworm"`, or `"bullseye"`. If omitted, defaults to `"debian"`. Available in `langgraph-cli>=0.2.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
    | <span style={{ whiteSpace: "nowrap" }}>`ui`</span>               | Optional. Named definitions of UI components emitted by the agent, each pointing to a JS/TS file. (added in `langgraph-cli==0.1.84`)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
    | <span style={{ whiteSpace: "nowrap" }}>`python_version`</span>   | `3.11`, `3.12`, or `3.13`. Defaults to `3.11`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_config_file`</span>  | Path to `pip` config file.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
    | <span style={{ whiteSpace: "nowrap" }}>`pip_installer`</span>    | *(Added in v0.3)* Optional. Python package installer selector. It can be set to `"auto"`, `"pip"`, or `"uv"`. From version 0.3 onward the default strategy is to run `uv pip`, which typically delivers faster builds while remaining a drop-in replacement. In the uncommon situation where `uv` cannot handle your dependency graph or the structure of your `pyproject.toml`, specify `"pip"` here to revert to the earlier behaviour.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`keep_pkg_tools`</span>   | *(Added in v0.3.4)* Optional. Control whether to retain Python packaging tools (`pip`, `setuptools`, `wheel`) in the final image. Accepted values: <ul><li><code>true</code> : Keep all three tools (skip uninstall).</li><li><code>false</code> / omitted : Uninstall all three tools (default behaviour).</li><li><code>list\[str]</code> : Names of tools <strong>to retain</strong>. Each value must be one of "pip", "setuptools", "wheel".</li></ul>. By default, all three tools are uninstalled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Supports: <ul><li>`ttl` (optional): Object with `strategy`, `sweep_interval_minutes`, `default_ttl` controlling checkpoint expiry.</li><li>`serde` (optional, 0.5+): Object with `allowed_json_modules` and `pickle_fallback` to tune deserialization behavior.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration with the following fields: <ul><li>`app`: Path to custom Starlette/FastAPI app (e.g., `"./src/agent/webapp.py:app"`). See [custom routes guide](/langsmith/custom-routes).</li><li>`cors`: CORS configuration with fields such as `allow_origins`, `allow_methods`, `allow_headers`, `allow_credentials`, `allow_origin_regex`, `expose_headers`, and `max_age`.</li><li>`configurable_headers`: Define which request headers to expose as configurable values via `includes` / `excludes` patterns.</li><li>`logging_headers`: Mirror of `configurable_headers` for excluding sensitive headers from logs.</li><li>`middleware_order`: Choose how custom middleware and auth interact. `auth_first` runs authentication hooks before custom middleware, while `middleware_first` (default) runs your middleware first.</li><li>`enable_custom_route_auth`: Apply auth checks to routes added through `app`.</li><li>`disable_assistants`, `disable_mcp`, `disable_a2a`, `disable_meta`, `disable_runs`, `disable_store`, `disable_threads`, `disable_ui`, `disable_webhooks`: Disable built-in routes or hooks.</li><li>`mount_prefix`: Prefix for mounted routes (e.g., "/my-deployment/api").</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/agent-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
  </Tab>

  <Tab title="JS">
    | Key                                                              | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
    | ---------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | <span style={{ whiteSpace: "nowrap" }}>`graphs`</span>           | **Required**. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: <ul><li>`./src/graph.ts:variable`, where `variable` is an instance of [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph)</li><li>`./src/graph.ts:makeGraph`, where `makeGraph` is a function that takes a config dictionary (`LangGraphRunnableConfig`) and returns an instance of [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph). See [how to rebuild a graph at runtime](/langsmith/graph-rebuild) for more details.</li></ul> |
    | <span style={{ whiteSpace: "nowrap" }}>`env`</span>              | Path to `.env` file or a mapping from environment variable to its value.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
    | <span style={{ whiteSpace: "nowrap" }}>`store`</span>            | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: <ul><li>`index` (optional): Configuration for semantic search indexing with fields `embed`, `dims`, and optional `fields`.</li><li>`ttl` (optional): Configuration for item expiration. An object with optional fields: `refresh_on_read` (boolean, defaults to `true`), `default_ttl` (float, lifespan in **minutes**; applied to newly created items only; existing items are unchanged; defaults to no expiration), and `sweep_interval_minutes` (integer, how often to check for expired items, defaults to no sweeping).</li></ul>                                                                                                                                                                |
    | <span style={{ whiteSpace: "nowrap" }}>`node_version`</span>     | Specify `node_version: 20` to use LangGraph.js.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
    | <span style={{ whiteSpace: "nowrap" }}>`dockerfile_lines`</span> | Array of additional lines to add to Dockerfile following the import from parent image.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`checkpointer`</span>     | Configuration for the checkpointer. Supports: <ul><li>`ttl` (optional): Object with `strategy`, `sweep_interval_minutes`, `default_ttl` controlling checkpoint expiry.</li><li>`serde` (optional, 0.5+): Object with `allowed_json_modules` and `pickle_fallback` to tune deserialization behavior.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
    | <span style={{ whiteSpace: "nowrap" }}>`http`</span>             | HTTP server configuration mirroring the Python options: <ul><li>`cors` with `allow_origins`, `allow_methods`, `allow_headers`, `allow_credentials`, `allow_origin_regex`, `expose_headers`, `max_age`.</li><li>`configurable_headers` and `logging_headers` pattern lists.</li><li>`middleware_order` (`auth_first` or `middleware_first`).</li><li>`enable_custom_route_auth` plus the same boolean route toggles as above.</li></ul>                                                                                                                                                                                                                                                                                                                                                                                     |
    | <span style={{ whiteSpace: "nowrap" }}>`api_version`</span>      | *(Added in v0.3.7)* Which semantic version of the LangGraph API server to use (e.g., `"0.3"`). Defaults to latest. Check the server [changelog](/langsmith/agent-server-changelog) for details on each release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
  </Tab>
</Tabs>

### Examples

<Tabs>
  <Tab title="Python">
    #### Basic configuration
```

---

## LangGraph JS/TS SDK

**URL:** llms-txt#langgraph-js/ts-sdk

Source: https://docs.langchain.com/langsmith/langgraph-js-ts-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langgraph-js-ts-sdk.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangGraph Python SDK

**URL:** llms-txt#langgraph-python-sdk

Source: https://docs.langchain.com/langsmith/langgraph-python-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langgraph-python-sdk.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangGraph runtime

**URL:** llms-txt#langgraph-runtime

**Contents:**
- Overview
- Actors
- Channels
- Examples
- High-level API

Source: https://docs.langchain.com/oss/python/langgraph/pregel

[`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) implements LangGraph's runtime, managing the execution of LangGraph applications.

Compiling a [StateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or creating an [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) produces a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) instance that can be invoked with input.

This guide explains the runtime at a high level and provides instructions for directly implementing applications with Pregel.

> **Note:** The [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) runtime is named after [Google's Pregel algorithm](https://research.google/pubs/pub37252/), which describes an efficient method for large-scale parallel computation using graphs.

In LangGraph, Pregel combines [**actors**](https://en.wikipedia.org/wiki/Actor_model) and **channels** into a single application. **Actors** read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the **Pregel Algorithm**/**Bulk Synchronous Parallel** model.

Each step consists of three phases:

* **Plan**: Determine which **actors** to execute in this step. For example, in the first step, select the **actors** that subscribe to the special **input** channels; in subsequent steps, select the **actors** that subscribe to channels updated in the previous step.
* **Execution**: Execute all selected **actors** in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.
* **Update**: Update the channels with the values written by the **actors** in this step.

Repeat until no **actors** are selected for execution, or a maximum number of steps is reached.

An **actor** is a `PregelNode`. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an **actor** in the Pregel algorithm. `PregelNodes` implement LangChain's Runnable interface.

Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:

* [`LastValue`](https://reference.langchain.com/python/langgraph/channels/#langgraph.channels.LastValue): The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.
* [`Topic`](https://reference.langchain.com/python/langgraph/channels/#langgraph.channels.Topic): A configurable PubSub Topic, useful for sending multiple values between **actors**, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.
* [`BinaryOperatorAggregate`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel--advanced-channels-context-and-binaryoperatoraggregate): stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,`total = BinaryOperatorAggregate(int, operator.add)`

While most users will interact with Pregel through the [StateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) API or the [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) decorator, it is possible to interact with Pregel directly.

Below are a few different examples to give you a sense of the Pregel API.

<Tabs>
  <Tab title="Single node">

<Tab title="Multiple nodes">

<Tab title="BinaryOperatorAggregate">
    This example demonstrates how to use the [`BinaryOperatorAggregate`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel--advanced-channels-context-and-binaryoperatoraggregate) channel to implement a reducer.

<Tab title="Cycle">
    This example demonstrates how to introduce a cycle in the graph, by having
    a chain write to a channel it subscribes to. Execution will continue
    until a `None` value is written to the channel.

LangGraph provides two high-level APIs for creating a Pregel application: the [StateGraph (Graph API)](/oss/python/langgraph/graph-api) and the [Functional API](/oss/python/langgraph/functional-api).

<Tabs>
  <Tab title="StateGraph (Graph API)">
    The [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.

The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.

You will see something like this:

You should see something like this

<Tab title="Functional API">
    In the [Functional API](/oss/python/langgraph/functional-api), you can use an [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) to create a Pregel application. The `entrypoint` decorator allows you to define a function that takes input and returns output.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/pregel.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Multiple nodes">
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Topic">
```

---

## LangGraph SDK

**URL:** llms-txt#langgraph-sdk

Source: https://docs.langchain.com/oss/python/reference/langgraph-python

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/langgraph-python.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith API reference

**URL:** llms-txt#langsmith-api-reference

Source: https://docs.langchain.com/langsmith/smith-api-ref

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-api-ref.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith control plane

**URL:** llms-txt#langsmith-control-plane

**Contents:**
- Control plane UI
- Control plane API
  - Integrations
  - Deployments
  - Revisions
  - Listeners
- Control plane features
  - Deployment types
  - Database provisioning
  - Asynchronous deployment

Source: https://docs.langchain.com/langsmith/control-plane

The *control plane* is the part of LangSmith that manages deployments. It includes the control plane UI, where users create and update [Agent Servers](/langsmith/agent-server), and the control plane APIs, which support the UI and provide programmatic access.

When you make an update through the control plane, the update is stored in control plane state. The [data plane](/langsmith/data-plane) “listener” polls for these updates by calling the control plane APIs.

From the control plane UI, you can:

* View a list of outstanding deployments.
* View details of an individual deployment.
* Create a new deployment.
* Update a deployment.
* Update environment variables for a deployment.
* View build and server logs of a deployment.
* View deployment metrics such as CPU and memory usage.
* Delete a deployment.

The Control plane UI is embedded in [LangSmith](https://docs.smith.langchain.com).

This section describes the data model of the control plane API. The API is used to create, update, and delete deployments. See the [control plane API reference](/langsmith/api-ref-control-plane) for more details.

An integration is an abstraction for a `git` repository provider (e.g. GitHub). It contains all of the required metadata needed to connect with and deploy from a `git` repository.

A deployment is an instance of an Agent Server. A single deployment can have many revisions.

A revision is an iteration of a deployment. When a new deployment is created, an initial revision is automatically created. To deploy code changes or update secrets for a deployment, a new revision must be created.

A listener is an instance of a ["listener" application](/langsmith/data-plane#”listener”-application). A listener contains metadata about the application (e.g. version) and metadata about the compute infrastructure where it can deploy to (e.g. Kubernetes namespaces).

The listener data model only applies for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.

## Control plane features

This section describes various features of the control plane.

For simplicity, the control plane offers two deployment types with different resource allocations: `Development` and `Production`.

| **Deployment Type** | **CPU/Memory**  | **Scaling**       | **Database**                                                                     |
| ------------------- | --------------- | ----------------- | -------------------------------------------------------------------------------- |
| Development         | 1 CPU, 1 GB RAM | Up to 1 replica   | 10 GB disk, no backups                                                           |
| Production          | 2 CPU, 2 GB RAM | Up to 10 replicas | Autoscaling disk, automatic backups, highly available (multi-zone configuration) |

CPU and memory resources are per replica.

<Warning>
  **Immutable Deployment Type**
  Once a deployment is created, the deployment type cannot be changed.
</Warning>

<Info>
  **Self-Hosted Deployment**
  Resources for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments can be fully customized. Deployment types are only applicable for [Cloud](/langsmith/cloud) deployments.
</Info>

`Production` type deployments are suitable for "production" workloads. For example, select `Production` for customer-facing applications in the critical path.

Resources for `Production` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. Contact support via [support.langchain.com](https://support.langchain.com) to request an increase in resources.

`Development` type deployments are suitable development and testing. For example, select `Development` for internal testing environments. `Development` type deployments are not suitable for "production" workloads.

<Danger>
  **Preemptible Compute Infrastructure**
  `Development` type deployments (API server, queue server, and database) are provisioned on preemptible compute infrastructure. This means the compute infrastructure **may be terminated at any time without notice**. This may result in intermittent...

* Redis connection timeouts/errors
  * Postgres connection timeouts/errors
  * Failed or retrying background runs

This behavior is expected. Preemptible compute infrastructure **significantly reduces the cost to provision a `Development` type deployment**. By design, Agent Server is fault-tolerant. The implementation will automatically attempt to recover from Redis/Postgres connection errors and retry failed background runs.

`Production` type deployments are provisioned on durable compute infrastructure, not preemptible compute infrastructure.
</Danger>

Database disk size for `Development` type deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. For most use cases, [TTLs](/langsmith/configure-ttl) should be configured to manage disk usage. Contact support via [support.langchain.com](https://support.langchain.com) to request an increase in resources.

### Database provisioning

The control plane and [data plane](/langsmith/data-plane) "listener" application coordinate to automatically create a Postgres database for each deployment. The database serves as the [persistence layer](/oss/python/langgraph/persistence#memory-store) for the deployment.

When implementing a LangGraph application, a [checkpointer](/oss/python/langgraph/persistence#checkpointer-libraries) does not need to be configured by the developer. Instead, a checkpointer is automatically configured for the graph. Any checkpointer configured for a graph will be replaced by the one that is automatically configured.

There is no direct access to the database. All access to the database occurs through the [Agent Server](/langsmith/agent-server).

The database is never deleted until the deployment itself is deleted.

<Info>
  A custom Postgres instance can be configured for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

### Asynchronous deployment

Infrastructure for deployments and revisions are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.

* When a new deployment is created, a new database is created for the deployment. Database creation is a one-time step. This step contributes to a longer deployment time for the initial revision of the deployment.
* When a subsequent revision is created for a deployment, there is no database creation step. The deployment time for a subsequent revision is significantly faster compared to the deployment time of the initial revision.
* The deployment process for each revision contains a build step, which can take up to a few minutes.

The control plane and [data plane](/langsmith/data-plane) "listener" application coordinate to achieve asynchronous deployments.

After a deployment is ready, the control plane monitors the deployment and records various metrics, such as:

* CPU and memory usage of the deployment.
* Number of container restarts.
* Number of replicas (this will increase with [autoscaling](/langsmith/data-plane#autoscaling)).
* [PostgreSQL](/langsmith/data-plane#postgres) CPU, memory usage, and disk usage.
* [Agent Server queue](/langsmith/agent-server#persistence-and-task-queue) pending/active run count.
* [Agent Server API](/langsmith/agent-server) success response count, error response count, and latency.

These metrics are displayed as charts in the Control Plane UI.

### LangSmith integration

A [LangSmith](/langsmith/home) tracing project is automatically created for each deployment. The tracing project has the same name as the deployment. When creating a deployment, the `LANGCHAIN_TRACING` and `LANGSMITH_API_KEY`/`LANGCHAIN_API_KEY` environment variables do not need to be specified; they are set automatically by the control plane.

When a deployment is deleted, the traces and the tracing project are not deleted.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/control-plane.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith data plane

**URL:** llms-txt#langsmith-data-plane

**Contents:**
- Server infrastructure
- "Listener" application
- PostgreSQL
- Redis
  - Communication
  - Ephemeral metadata
- Data plane features
  - Data region
  - Autoscaling
  - Static IP addresses

Source: https://docs.langchain.com/langsmith/data-plane

The *data plane* consists of your [Agent Servers](/langsmith/agent-server) (deployments), their supporting infrastructure, and the "listener" application that continuously polls for updates from the [LangSmith control plane](/langsmith/control-plane).

## Server infrastructure

In addition to the [Agent Server](/langsmith/agent-server) itself, the following infrastructure components for each server are also included in the broad definition of "data plane":

* **PostgreSQL**: persistence layer for user, run, and memory data.
* **Redis**: communication and ephemeral metadata for workers.
* **Secrets store**: secure management of environment secrets.
* **Autoscalers**: scale server containers based on load.

## "Listener" application

The data plane "listener" application periodically calls [control plane APIs](/langsmith/control-plane#control-plane-api) to:

* Determine if new deployments should be created.
* Determine if existing deployments should be updated (i.e. new revisions).
* Determine if existing deployments should be deleted.

In other words, the data plane "listener" reads the latest state of the control plane (desired state) and takes action to reconcile outstanding deployments (current state) to match the latest state.

PostgreSQL is the persistence layer for all user, run, and long-term memory data in a Agent Server. This stores both checkpoints (see more info [here](/oss/python/langgraph/persistence)), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info [here](/oss/python/langgraph/persistence#memory-store)).

Redis is used in each Agent Server as a way for server and queue workers to communicate, and to store ephemeral metadata. No user or run data is stored in Redis.

All runs in an Agent Server are executed by a pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.

1. A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run information. The run information is then retrieved from PostgreSQL by the worker.
2. A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.
3. A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open `/stream` request in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.

### Ephemeral metadata

Runs in an Agent Server may be retried for specific failures (currently only for transient PostgreSQL errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when it is picked up. This contains no run-specific info other than its ID, and expires after a short delay.

## Data plane features

This section describes various features of the data plane.

<Info>
  **Only for Cloud**
  Data regions are only applicable for [Cloud](/langsmith/cloud) deployments.
</Info>

Deployments can be created in 2 data regions: US and EU

The data region for a deployment is implied by the data region of the LangSmith organization where the deployment is created. Deployments and the underlying database for the deployments cannot be migrated between data regions.

[`Production` type](/langsmith/control-plane#deployment-types) deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:

1. CPU utilization
2. Memory utilization
3. Number of pending (in progress) [runs](/langsmith/assistants#execution)

For CPU utilization, the autoscaler targets 75% utilization. This means the autoscaler will scale the number of containers up or down to ensure that CPU utilization is at or near 75%. For memory utilization, the autoscaler targets 75% utilization as well.

For number of pending runs, the autoscaler targets 10 pending runs. For example, if the current number of containers is 1, but the number of pending runs is 20, the autoscaler will scale up the deployment to 2 containers (20 pending runs / 2 containers = 10 pending runs per container).

Each metric is computed independently and the autoscaler will determine the scaling action based on the metric that results in the largest number of containers.

Scale down actions are delayed for 30 minutes before any action is taken. In other words, if the autoscaler decides to scale down a deployment, it will first wait for 30 minutes before scaling down. After 30 minutes, the metrics are recomputed and the deployment will scale down if the recomputed metrics result in a lower number of containers than the current number. Otherwise, the deployment remains scaled up. This "cool down" period ensures that deployments do not scale up and down too frequently.

### Static IP addresses

<Info>
  **Only for Cloud**
  Static IP addresses are only available for [Cloud](/langsmith/cloud) deployments.
</Info>

All traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:

| US             | EU             |
| -------------- | -------------- |
| 35.197.29.146  | 34.13.192.67   |
| 34.145.102.123 | 34.147.105.64  |
| 34.169.45.153  | 34.90.22.166   |
| 34.82.222.17   | 34.147.36.213  |
| 35.227.171.135 | 34.32.137.113  |
| 34.169.88.30   | 34.91.238.184  |
| 34.19.93.202   | 35.204.101.241 |
| 34.19.34.50    | 35.204.48.32   |
| 34.59.244.194  |                |
| 34.9.99.224    |                |
| 34.68.27.146   |                |
| 34.41.178.137  |                |
| 34.123.151.210 |                |
| 34.135.61.140  |                |
| 34.121.166.52  |                |
| 34.31.121.70   |                |

<Info>
  **Only for Cloud**
  Payload size restrictions are only applicable to [Cloud](/langsmith/cloud) deployments.
</Info>

The maximum payload size for all requests sent to [Cloud](/langsmith/cloud) deployments is 25 MB. Attempting to send a request with a payload larger than 25 MB will result in a `413 Payload Too Large` error.

### Custom PostgreSQL

<Info>
  Custom PostgreSQL instances are only available for [hybrid](/langsmith/hybrid) and [self-hosted](/langsmith/self-hosted) deployments.
</Info>

A custom PostgreSQL instance can be used instead of the [one automatically created by the control plane](/langsmith/control-plane#database-provisioning). Specify the [`POSTGRES_URI_CUSTOM`](/langsmith/env-var#postgres-uri-custom) environment variable to use a custom PostgreSQL instance.

Multiple deployments can share the same PostgreSQL instance. For example, for `Deployment A`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>` and for `Deployment B`, `POSTGRES_URI_CUSTOM` can be set to `postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>`. `<database_name_1>` and `database_name_2` are different databases within the same instance, but `<hostname_1>` is shared. **The same database cannot be used for separate deployments**.

<Info>
  Custom Redis instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

A custom Redis instance can be used instead of the one automatically created by the control plane. Specify the [REDIS\_URI\_CUSTOM](/langsmith/env-var#redis-uri-custom) environment variable to use a custom Redis instance.

Multiple deployments can share the same Redis instance. For example, for `Deployment A`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/1` and for `Deployment B`, `REDIS_URI_CUSTOM` can be set to `redis://<hostname_1>:<port>/2`. `1` and `2` are different database numbers within the same instance, but `<hostname_1>` is shared. **The same database number cannot be used for separate deployments**.

### LangSmith tracing

Agent Server is automatically configured to send traces to LangSmith. See the table below for details with respect to each deployment option.

| Cloud                                  | Hybrid                                                    | Self-Hosted                                                                                |
| -------------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| Required<br />Trace to LangSmith SaaS. | Optional<br />Disable tracing or trace to LangSmith SaaS. | Optional<br />Disable tracing, trace to LangSmith SaaS, or trace to Self-Hosted LangSmith. |

Agent Server is automatically configured to report telemetry metadata for billing purposes. See the table below for details with respect to each deployment option.

| Cloud                             | Hybrid                            | Self-Hosted                                                                                                              |
| --------------------------------- | --------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| Telemetry sent to LangSmith SaaS. | Telemetry sent to LangSmith SaaS. | Self-reported usage (audit) for air-gapped license key.<br />Telemetry sent to LangSmith SaaS for LangSmith License Key. |

Agent Server is automatically configured to perform license key validation. See the table below for details with respect to each deployment option.

| Cloud                                               | Hybrid                                              | Self-Hosted                                                                      |
| --------------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------------- |
| LangSmith API Key validated against LangSmith SaaS. | LangSmith API Key validated against LangSmith SaaS. | Air-gapped license key or Platform License Key validated against LangSmith SaaS. |

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-plane.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith Deployment

**URL:** llms-txt#langsmith-deployment

**Contents:**
- Prerequisites
- Deploy your agent
  - 1. Create a repository on GitHub
  - 2. Deploy to LangSmith
  - 3. Test your application in Studio
  - 4. Get the API URL for your deployment
  - 5. Test the API

Source: https://docs.langchain.com/oss/python/langgraph/deploy

This guide shows you how to deploy your agent to **[LangSmith Cloud](/langsmith/deploy-to-cloud)**, a fully managed hosting platform designed for agent workloads. With Cloud deployment, you can deploy directly from your GitHub repository—LangSmith handles the infrastructure, scaling, and operational concerns.

Traditional hosting platforms are built for stateless, short-lived web applications. LangSmith Cloud is **purpose-built for stateful, long-running agents** that require persistent state and background execution.

<Tip>
  LangSmith offers multiple deployment options beyond Cloud, including deploying with a [control plane (hybrid/self-hosted)](/langsmith/deploy-with-control-plane) or as [standalone servers](/langsmith/deploy-standalone-server). For more information, refer to the [Deployment overview](/langsmith/deployments).
</Tip>

Before you begin, ensure you have the following:

* A [GitHub account](https://github.com/)
* A [LangSmith account](https://smith.langchain.com/) (free to sign up)

### 1. Create a repository on GitHub

Your application's code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the [local server setup guide](/oss/python/langgraph/studio#setup-local-agent-server). Then, push your code to the repository.

### 2. Deploy to LangSmith

<Steps>
  <Step title="Navigate to LangSmith Deployment">
    Log in to [LangSmith](https://smith.langchain.com/). In the left sidebar, select **Deployments**.
  </Step>

<Step title="Create new deployment">
    Click the **+ New Deployment** button. A pane will open where you can fill in the required fields.
  </Step>

<Step title="Link repository">
    If you are a first time user or adding a private repository that has not been previously connected, click the **Add new account** button and follow the instructions to connect your GitHub account.
  </Step>

<Step title="Deploy repository">
    Select your application's repository. Click **Submit** to deploy. This may take about 15 minutes to complete. You can check the status in the **Deployment details** view.
  </Step>
</Steps>

### 3. Test your application in Studio

Once your application is deployed:

1. Select the deployment you just created to view more details.
2. Click the **Studio** button in the top right corner. Studio will open to display your graph.

### 4. Get the API URL for your deployment

1. In the **Deployment details** view in LangGraph, click the **API URL** to copy it to your clipboard.
2. Click the `URL` to copy it to the clipboard.

You can now test the API:

<Tabs>
  <Tab title="Python">
    1. Install LangGraph Python:

2. Send a message to the agent:

<Tab title="Rest API">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/deploy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. Send a message to the agent:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Rest API">
```

---

## LangSmith docs

**URL:** llms-txt#langsmith-docs

**Contents:**
- Get started
  - More ways to build
- Workflow

Source: https://docs.langchain.com/langsmith/home

**LangSmith provides tools for developing, debugging, and deploying LLM applications.**
It helps you trace requests, evaluate outputs, test prompts, and manage deployments in one place.
LangSmith is framework agnostic, so you can use it with or without LangChain's open-source libraries
[`langchain`](/oss/python/langchain/overview) and [`langgraph`](/oss/python/langgraph/overview).
Prototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.

<Callout icon="bullhorn" color="#DFC5FE" iconType="regular">
  LangGraph Platform is now [LangSmith Deployment](/langsmith/deployments). For more information, check out the [Changelog](https://changelog.langchain.com/announcements/product-naming-changes-langsmith-deployment-and-langsmith-studio).
</Callout>

<Steps>
  <Step title="Create an account" icon="user-plus">
    Sign up at [smith.langchain.com](https://smith.langchain.com) (no credit card required).
    You can log in with **Google**, **GitHub**, or **email**.
  </Step>

<Step title="Create an API key" icon="key">
    Go to your [Settings page](https://smith.langchain.com/settings) → **API Keys** → **Create API Key**.
    Copy the key and save it securely.
  </Step>
</Steps>

Once your account and API key are ready, choose a quickstart to begin building with LangSmith:

<Columns cols={3}>
  <Card title="Observability" icon="magnifying-glass" href="/langsmith/observability-quickstart" arrow="true" cta="Start tracing">
    Gain visibility into every step your application takes to debug faster and improve reliability.
  </Card>

<Card title="Evaluation" icon="chart-line" href="/langsmith/evaluation-quickstart" arrow="true" cta="Evaluate your app">
    Measure and track quality over time to ensure your AI applications are consistent and trustworthy.
  </Card>

<Card title="Deployment" icon="cloud-arrow-up" iconType="solid" href="/langsmith/deployments" arrow="true" cta="Deploy your agents">
    Deploy your agents as Agent Servers, ready to scale in production.
  </Card>
</Columns>

### More ways to build

<Columns cols={2}>
  <Card title="Platform setup" icon="server" iconType="solid" href="/langsmith/platform-setup" arrow="true" cta="Choose how to set up LangSmith">
    Use LangSmith in managed cloud, in a self-hosted environment, or hybrid to match your infrastructure and compliance needs.
  </Card>

<Card title="Agent Builder (Beta)" icon="sparkles" href="/langsmith/agent-builder" arrow="true" cta="Build an agent">
    Design and deploy AI agents visually with a no-code interface—perfect for rapid prototyping and getting started without writing code.
  </Card>

<Card title="Studio" icon="window" href="/langsmith/quick-start-studio" arrow="true" cta="Develop with Studio">
    Use a visual interface to design, test, and refine applications end-to-end.
  </Card>

<Card title="Prompt testing" icon="flask" href="/langsmith/prompt-engineering-quickstart" arrow="true" cta="Test your prompts">
    Iterate on prompts with built-in versioning and collaboration to ship improvements faster.
  </Card>
</Columns>

<Callout icon="lock" color="#DFC5FE" iconType="regular">
  LangSmith meets the highest standards of data security and privacy with HIPAA, SOC 2 Type 2, and GDPR compliance. For more information, see the [Trust Center](https://trust.langchain.com/).
</Callout>

LangSmith combines observability, evaluation, deployment, and platform setup in one integrated workflow—from local development to production.

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=84312d9c72c44e44ac513eaa78abefc6" alt="Diagram showing how LangSmith integrates observability, evaluation, deployment, and platform setup in a single workflow from development to production." data-og-width="1138" width="1138" data-og-height="549" height="549" data-path="langsmith/images/overview-light.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=280&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=32fa495af106d1eb30c9d512b59c3926 280w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=560&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=f30ac54ecdc55dd22b5762800f33a5bb 560w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=840&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=aee1643d7b4ab4cf74ffb54b555e3788 840w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=1100&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=65946469f91ff9cfbd26d154d31c6caf 1100w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=1650&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=a310e56592be23e233075ebffa1b124d 1650w, https://mintcdn.com/langchain-5e9cc07a/gnE-C9CZc2IgnmJo/langsmith/images/overview-light.svg?w=2500&fit=max&auto=format&n=gnE-C9CZc2IgnmJo&q=85&s=f9e2edcac083797d0498806cb512d5de 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=f930d2f475ab68e1cdfe7105b5f1abe6" alt="Diagram showing how LangSmith integrates observability, evaluation, deployment, and platform setup in a single workflow from development to production." data-og-width="1157" width="1157" data-og-height="549" height="549" data-path="langsmith/images/overview-dark.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=280&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=cab90851ba7c49d855f89c86f6bfdd0b 280w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=560&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=d04a2b423106d1f64fa0a3a05948a19a 560w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=840&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=84c821c5464a20cf4a3d397e5baf7357 840w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=1100&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=202f8ed1e7c38c5a6184b9e1b2ea377f 1100w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=1650&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=f8d03af520cd2f0363f9b41f86428396 1650w, https://mintcdn.com/langchain-5e9cc07a/Dai6LUVulRwLjxET/langsmith/images/overview-dark.svg?w=2500&fit=max&auto=format&n=Dai6LUVulRwLjxET&q=85&s=576885f21fd2e6bc4ecb4e4176515a3d 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/home.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith Evaluation

**URL:** llms-txt#langsmith-evaluation

**Contents:**
- Evaluation workflow
- Get started

Source: https://docs.langchain.com/langsmith/evaluation

LangSmith supports two types of evaluations based on when and where they run:

<CardGroup cols={2}>
  <Card title="Offline Evaluation" icon="flask">
    **Test before you ship**

Run evaluations on curated datasets during development to compare versions, benchmark performance, and catch regressions.
  </Card>

<Card title="Online Evaluation" icon="radar">
    **Monitor in production**

Evaluate real user interactions in real-time to detect issues and measure quality on live traffic.
  </Card>
</CardGroup>

## Evaluation workflow

<Tabs>
  <Tab title="Offline evaluation flow">
    <Steps>
      <Step title="Create a dataset">
        Create a [dataset](/langsmith/manage-datasets) with <Tooltip tip="Individual test cases with inputs and reference outputs">[examples](/langsmith/evaluation-concepts#examples)</Tooltip> from manually curated test cases, historical production traces, or synthetic data generation.
      </Step>

<Step title="Define evaluators">
        Create <Tooltip tip="Functions that score how well your application performs">[evaluators](/langsmith/evaluation-concepts#evaluators)</Tooltip> to score performance:

* [Human](/langsmith/evaluation-concepts#human) review
        * [Code](/langsmith/evaluation-concepts#code) rules
        * [LLM-as-judge](/langsmith/llm-as-judge)
        * [Pairwise](/langsmith/evaluate-pairwise) comparison
      </Step>

<Step title="Run an experiment">
        Execute your application on the dataset to create an <Tooltip tip="Results of evaluating a specific application version on a dataset">[experiment](/langsmith/evaluation-concepts#experiment)</Tooltip>. Configure [repetitions, concurrency, and caching](/langsmith/experiment-configuration) to optimize runs.
      </Step>

<Step title="Analyze results">
        Compare experiments for [benchmarking](/langsmith/evaluation-types#benchmarking), [unit tests](/langsmith/evaluation-types#unit-tests), [regression tests](/langsmith/evaluation-types#regression-tests), or [backtesting](/langsmith/evaluation-types#backtesting).
      </Step>
    </Steps>
  </Tab>

<Tab title="Online evaluation flow">
    <Steps>
      <Step title="Deploy your application">
        Each interaction creates a <Tooltip tip="A single execution trace including inputs, outputs, and intermediate steps">[run](/langsmith/evaluation-concepts#runs)</Tooltip> without reference outputs.
      </Step>

<Step title="Configure online evaluators">
        Set up [evaluators](/langsmith/online-evaluations) to run automatically on production traces: safety checks, format validation, quality heuristics, and reference-free LLM-as-judge. Apply [filters and sampling rates](/langsmith/online-evaluations#4-optional-configure-a-sampling-rate) to control costs.
      </Step>

<Step title="Monitor in real-time">
        Evaluators run automatically on [runs](/langsmith/evaluation-concepts#runs) or <Tooltip tip="Collections of related runs forming multi-turn conversations">[threads](/langsmith/online-evaluations#configure-multi-turn-online-evaluators)</Tooltip>, providing real-time monitoring, anomaly detection, and alerting.
      </Step>

<Step title="Establish a feedback loop">
        Add failing production traces to your [dataset](/langsmith/manage-datasets), create targeted evaluators, validate fixes with offline experiments, and redeploy.
      </Step>
    </Steps>
  </Tab>
</Tabs>

<Tip>
  For more on the differences between offline and online evaluation, refer to the [Evaluation concepts](/langsmith/evaluation-concepts#quick-reference-offline-vs-online-evaluation) page.
</Tip>

<Columns cols={3}>
  <Card title="Evaluation quickstart" icon="rocket" href="/langsmith/evaluation-quickstart" arrow="true">
    Get started with offline evaluation.
  </Card>

<Card title="Manage datasets" icon="database" href="/langsmith/manage-datasets" arrow="true">
    Create and manage datasets for evaluation through the UI or SDK.
  </Card>

<Card title="Run offline evaluations" icon="microscope" href="/langsmith/evaluate-llm-application" arrow="true">
    Explore evaluation types, techniques, and frameworks for comprehensive testing.
  </Card>

<Card title="Analyze results" icon="chart-bar" href="/langsmith/analyze-an-experiment" arrow="true">
    View and analyze evaluation results, compare experiments, filter data, and export findings.
  </Card>

<Card title="Run online evaluations" icon="radar" href="/langsmith/online-evaluations" arrow="true">
    Monitor production quality in real-time from the Observability tab.
  </Card>

<Card title="Follow tutorials" icon="book" href="/langsmith/evaluate-chatbot-tutorial" arrow="true">
    Learn by following step-by-step tutorials, from simple chatbots to complex agent evaluations.
  </Card>
</Columns>

<Note>
  To set up a LangSmith instance, visit the [Platform setup section](/langsmith/platform-setup) to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluation.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith Fetch

**URL:** llms-txt#langsmith-fetch

**Contents:**
- Installation
- Setup
  - Use with a coding agent
- Find project and trace IDs
- Usage
  - Options
  - Output formats
  - Fetch a trace or thread
  - Fetch multiple
  - Include metadata and feedback

Source: https://docs.langchain.com/langsmith/langsmith-fetch

LangSmith Fetch is a command-line interface (CLI) tool for retrieving trace data ([runs](/langsmith/observability-concepts#runs), [traces](/langsmith/observability-concepts#traces), and [threads](/langsmith/observability-concepts#threads)) from your LangSmith projects. It allows you to use LangSmith’s tracing and debugging features directly in your terminal and development workflows.

You can use LangSmith Fetch for the following use cases:

* Immediate debugging: Fetch the most recent trace of a failed or unexpected agent run with a single command.
* Bulk export for analysis: Export large numbers of traces or entire conversation threads to JSON files for offline analysis, building [evaluation](/langsmith/evaluation-concepts) datasets, or [regression tests](/langsmith/evaluation-types#regression-tests).
* Terminal-based workflows: Integrate trace data into your existing tools; for example, piping output to Unix utilities like jq, or feeding traces into an AI coding assistant for automated analysis.

Set your [LangSmith API key](/langsmith/create-account-api-key) and project name:

The CLI will automatically fetch traces or threads in `LANGSMITH_PROJECT`. Replace `your-project-name` with the name of your LangSmith project (if it doesn't exist, it will be created automatically on first use).

<Note>
  `langsmith-fetch` only requires the `LANGSMITH_PROJECT` environment variable. It automatically looks up the project UUID and saves both to `~/.langsmith-cli/config.yaml`. You can also specify [a project by its UUID](#override-the-configured-tracing-project) via a CLI flag.
</Note>

### Use with a coding agent

After you've installed and set up `langsmith-fetch`, use your coding agent to ask questions like the following:

Many agents will use the `langsmith-fetch --help` command to understand how to use the CLI and complete your request.

## Find project and trace IDs

In most cases, you won’t need to find IDs manually (the CLI uses your project name and latest traces by default). However, if you want to fetch a specific item by ID, you can find them in the [LangSmith UI](https://smith.langchain.com):

* **Project UUID**: Each project has a unique ID (UUID). You can find it in the project's URL or by hovering over <Icon icon="link-simple" /> **ID** next to the project's name. This UUID can be used with the `--project-uuid` flag on CLI commands
* **Trace ID**: Every trace (single execution) has an ID. In the **Runs** view, click on a specific run to see its Trace ID (copyable from the trace details panel). You can use `langsmith-fetch trace <trace-id>` to retrieve that exact trace if you have the ID.

After installation and setup, you can use the `langsmith-fetch` command to retrieve traces or threads. The general usage is:

LangSmith Fetch provides the following commands to fetch either single items or in bulk:

| Command               | Fetches                                    | Output location                                                                                                                                                                         |
| --------------------- | ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `trace <id>`          | A specific trace by ID                     | Prints to stdout (or to a file with `--file`)                                                                                                                                           |
| `thread <id>`         | A specific thread by ID                    | Prints to stdout (or to a file with `--file`)                                                                                                                                           |
| `traces [directory]`  | Recent traces from the project (multiple)  | Saves each trace as a JSON file in the given directory, or prints to stdout if no directory is provided. **Tip:** Using a directory is recommended for [bulk exports](#fetch-multiple). |
| `threads [directory]` | Recent threads from the project (multiple) | Saves each thread as a JSON file in the given directory, or prints to stdout if no directory is provided.                                                                               |

<Note>
  Traces are fetched chronologically with most recent first.
</Note>

The commands support additional flags to filter and format the output:

| Option / Flag               | Applies to                       | Description                                                                                                                                                                   | Default                                 |
| --------------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- |
| `-n, --limit <int>`         | `traces`, `threads`              | Maximum number of traces/threads to fetch. Use this to limit how many items you retrieve (e.g., the last 5 traces).                                                           | **1** (if not specified)                |
| `--last-n-minutes <int>`    | `traces`, `threads`              | Only fetch items from the last N minutes. This is useful to get recent data (e.g.,`--last-n-minutes 30` for the past half hour).                                              | *(no time filter)*                      |
| `--since <timestamp>`       | `traces`, `threads`              | Only fetch items since a specific time. Provide an ISO 8601 timestamp (e.g.,`2025-12-01T00:00:00Z`) to get data after that time.                                              | *(no time filter)*                      |
| `--project-uuid <uuid>`     | `trace, thread, traces, threads` | Manually specify the project by UUID (overrides the `LANGSMITH_PROJECT` env setting). Use this if you want to fetch from a different project without changing your env var.   | From env/config                         |
| `--filename-pattern <text>` | `traces`, `threads`              | Pattern for output filenames when saving multiple files. You can use placeholders like `{trace_id}`, `{thread_id}`, `{index}`.                                                | `{trace_id}.json` or `{thread_id}.json` |
| `--format <type>`           | **All commands**                 | Output format: `pretty`, `json`, or `raw`. (Refer to [Output formats](#output-formats) for details.)                                                                          | `pretty`                                |
| `--file <path>`             | `trace`, `thread`                | Save the fetched trace/thread to a file instead of printing it.                                                                                                               | *(stdout)*                              |
| `--include-metadata`        | `traces` (bulk fetch)            | Include run metadata in the output (such as tokens used, execution time, status, costs). This will add a `"metadata"` section to each trace’s JSON.                           | *Off by default*                        |
| `--include-feedback`        | `traces` (bulk fetch)            | Include any feedback entries attached to the runs. Enabling this will make an extra API call for each trace to fetch feedback data.                                           | *Off by default*                        |
| `--max-concurrent <int>`    | `traces`, `threads`              | Maximum concurrent fetch requests. Tune this if you are fetching a large number of items; increasing it may speed up retrieval but 5–10 is recommended to avoid API overload. | **5**                                   |
| `--no-progress`             | `traces`, `threads`              | Disable the progress bar output. By default a progress indicator is shown when fetching multiple items; use this flag to hide it (useful for non-interactive scripts).        | Progress bar on                         |

The `--format` option controls how the fetched data is displayed:

* `pretty` (default): A human-readable view with rich text formatting for easy inspection in the terminal. This format is great for quick debugging of a single trace or thread.

Explicitly specify the format:

* `json`: Well-formatted JSON output with syntax highlighting. Use this if you want to examine the raw data structure or pipe it into JSON processing tools.

* `raw`: Compact JSON with no extra whitespace. This is useful for piping the output to other programs (e.g., using `jq` or saving directly) without extra formatting.

### Fetch a trace or thread

You can fetch a single thread or trace with the ID. The command will output to the terminal by default:

<img src="https://mintcdn.com/langchain-5e9cc07a/NS5dAlKHYqJeZH1b/langsmith/images/langsmith-fetch-output-single.png?fit=max&auto=format&n=NS5dAlKHYqJeZH1b&q=85&s=169393d4790449b3fd718e027c7f17d1" alt="Output from a single trace fetch in default pretty format" data-og-width="3090" width="3090" data-og-height="356" height="356" data-path="langsmith/images/langsmith-fetch-output-single.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/NS5dAlKHYqJeZH1b/langsmith/images/langsmith-fetch-output-single.png?w=280&fit=max&auto=format&n=NS5dAlKHYqJeZH1b&q=85&s=e217554b2c5fe84d91949cb5831424d5 280w, https://mintcdn.com/langchain-5e9cc07a/NS5dAlKHYqJeZH1b/langsmith/images/langsmith-fetch-output-single.png?w=560&fit=max&auto=format&n=NS5dAlKHYqJeZH1b&q=85&s=d5af45552d6ea96521707974fab82247 560w, https://mintcdn.com/langchain-5e9cc07a/NS5dAlKHYqJeZH1b/langsmith/images/langsmith-fetch-output-single.png?w=840&fit=max&auto=format&n=NS5dAlKHYqJeZH1b&q=85&s=cb4e434511324385baf83786ce9d03fa 840w, https://mintcdn.com/langchain-5e9cc07a/NS5dAlKHYqJeZH1b/langsmith/images/langsmith-fetch-output-single.png?w=1100&fit=max&auto=format&n=NS5dAlKHYqJeZH1b&q=85&s=52b86ae2c326d9bb7728b2df4c0eb2ae 1100w, https://mintcdn.com/langchain-5e9cc07a/NS5dAlKHYqJeZH1b/langsmith/images/langsmith-fetch-output-single.png?w=1650&fit=max&auto=format&n=NS5dAlKHYqJeZH1b&q=85&s=c154515ff66d11039531d65218ca1b51 1650w, https://mintcdn.com/langchain-5e9cc07a/NS5dAlKHYqJeZH1b/langsmith/images/langsmith-fetch-output-single.png?w=2500&fit=max&auto=format&n=NS5dAlKHYqJeZH1b&q=85&s=69b91328f82da1dbd8283716532fc2f2 2500w" />

You can optionally redirect the thread or trace data to a file using the `--file` option.

<Note>
  For bulk fetches of traces or threads, we recommend specifying a target directory path. Each fetched trace or thread will be saved as a separate JSON file in that folder, making it easy to browse or process them later.
</Note>

You can specify a destination directory for the bulk commands (`traces`/`threads`). For example, the following command will save the 10 most recent traces as JSON files in the `my-traces-data` directory:

If you omit the directory and `--limit`, the tool will output the results of the most recent, single trace to your terminal.

When sending to a directory, files will be named in the following way:

* Default: Files named by trace ID (e.g., `3b0b15fe-1e3a-4aef-afa8-48df15879cfe.json`).
* Custom pattern: Use `--filename-pattern` with placeholders:
  * `{trace_id}`: Trace ID (default: `{trace_id}.json`).
  * `{index}` or `{idx}`: Sequential number starting from 1.
  * Format specs supported: `{index:03d}` for zero-padded numbers.

### Include metadata and feedback

You can include [run metadata](/langsmith/observability-concepts#metadata) and any [feedback](/langsmith/observability-concepts#feedback) associated with the trace:

### Override the configured tracing project

To fetch traces from a different project than the one configured with `LANGSMITH_PROJECT`, use the `--project-uuid` option:

Running this command will just fetch traces from that project, it will not modify the LangSmith project already configured in `~/.langsmith-cli/config.yaml`.

You can fetch traces or full threads and export to a file:

This command retrieves all threads that have occurred since December 1, 2025, saving each conversation as a JSON file under `./my_threads`. This is useful for exporting chat transcripts or building regression tests on multi-turn conversations. You could also use `--limit` with threads to fetch a specific number of recent threads, and `--last-n-minutes` works here as well.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langsmith-fetch.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Setup

Set your [LangSmith API key](/langsmith/create-account-api-key) and project name:
```

Example 2 (unknown):
```unknown
The CLI will automatically fetch traces or threads in `LANGSMITH_PROJECT`. Replace `your-project-name` with the name of your LangSmith project (if it doesn't exist, it will be created automatically on first use).

<Note>
  `langsmith-fetch` only requires the `LANGSMITH_PROJECT` environment variable. It automatically looks up the project UUID and saves both to `~/.langsmith-cli/config.yaml`. You can also specify [a project by its UUID](#override-the-configured-tracing-project) via a CLI flag.
</Note>

### Use with a coding agent

After you've installed and set up `langsmith-fetch`, use your coding agent to ask questions like the following:
```

Example 3 (unknown):
```unknown
Many agents will use the `langsmith-fetch --help` command to understand how to use the CLI and complete your request.

## Find project and trace IDs

In most cases, you won’t need to find IDs manually (the CLI uses your project name and latest traces by default). However, if you want to fetch a specific item by ID, you can find them in the [LangSmith UI](https://smith.langchain.com):

* **Project UUID**: Each project has a unique ID (UUID). You can find it in the project's URL or by hovering over <Icon icon="link-simple" /> **ID** next to the project's name. This UUID can be used with the `--project-uuid` flag on CLI commands
* **Trace ID**: Every trace (single execution) has an ID. In the **Runs** view, click on a specific run to see its Trace ID (copyable from the trace details panel). You can use `langsmith-fetch trace <trace-id>` to retrieve that exact trace if you have the ID.

## Usage

After installation and setup, you can use the `langsmith-fetch` command to retrieve traces or threads. The general usage is:
```

Example 4 (unknown):
```unknown
LangSmith Fetch provides the following commands to fetch either single items or in bulk:

| Command               | Fetches                                    | Output location                                                                                                                                                                         |
| --------------------- | ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `trace <id>`          | A specific trace by ID                     | Prints to stdout (or to a file with `--file`)                                                                                                                                           |
| `thread <id>`         | A specific thread by ID                    | Prints to stdout (or to a file with `--file`)                                                                                                                                           |
| `traces [directory]`  | Recent traces from the project (multiple)  | Saves each trace as a JSON file in the given directory, or prints to stdout if no directory is provided. **Tip:** Using a directory is recommended for [bulk exports](#fetch-multiple). |
| `threads [directory]` | Recent threads from the project (multiple) | Saves each thread as a JSON file in the given directory, or prints to stdout if no directory is provided.                                                                               |

<Note>
  Traces are fetched chronologically with most recent first.
</Note>

### Options

The commands support additional flags to filter and format the output:

| Option / Flag               | Applies to                       | Description                                                                                                                                                                   | Default                                 |
| --------------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- |
| `-n, --limit <int>`         | `traces`, `threads`              | Maximum number of traces/threads to fetch. Use this to limit how many items you retrieve (e.g., the last 5 traces).                                                           | **1** (if not specified)                |
| `--last-n-minutes <int>`    | `traces`, `threads`              | Only fetch items from the last N minutes. This is useful to get recent data (e.g.,`--last-n-minutes 30` for the past half hour).                                              | *(no time filter)*                      |
| `--since <timestamp>`       | `traces`, `threads`              | Only fetch items since a specific time. Provide an ISO 8601 timestamp (e.g.,`2025-12-01T00:00:00Z`) to get data after that time.                                              | *(no time filter)*                      |
| `--project-uuid <uuid>`     | `trace, thread, traces, threads` | Manually specify the project by UUID (overrides the `LANGSMITH_PROJECT` env setting). Use this if you want to fetch from a different project without changing your env var.   | From env/config                         |
| `--filename-pattern <text>` | `traces`, `threads`              | Pattern for output filenames when saving multiple files. You can use placeholders like `{trace_id}`, `{thread_id}`, `{index}`.                                                | `{trace_id}.json` or `{thread_id}.json` |
| `--format <type>`           | **All commands**                 | Output format: `pretty`, `json`, or `raw`. (Refer to [Output formats](#output-formats) for details.)                                                                          | `pretty`                                |
| `--file <path>`             | `trace`, `thread`                | Save the fetched trace/thread to a file instead of printing it.                                                                                                               | *(stdout)*                              |
| `--include-metadata`        | `traces` (bulk fetch)            | Include run metadata in the output (such as tokens used, execution time, status, costs). This will add a `"metadata"` section to each trace’s JSON.                           | *Off by default*                        |
| `--include-feedback`        | `traces` (bulk fetch)            | Include any feedback entries attached to the runs. Enabling this will make an extra API call for each trace to fetch feedback data.                                           | *Off by default*                        |
| `--max-concurrent <int>`    | `traces`, `threads`              | Maximum concurrent fetch requests. Tune this if you are fetching a large number of items; increasing it may speed up retrieval but 5–10 is recommended to avoid API overload. | **5**                                   |
| `--no-progress`             | `traces`, `threads`              | Disable the progress bar output. By default a progress indicator is shown when fetching multiple items; use this flag to hide it (useful for non-interactive scripts).        | Progress bar on                         |

### Output formats

The `--format` option controls how the fetched data is displayed:

* `pretty` (default): A human-readable view with rich text formatting for easy inspection in the terminal. This format is great for quick debugging of a single trace or thread.

  By default:
```

---

## LangSmith JS/TS SDK

**URL:** llms-txt#langsmith-js/ts-sdk

Source: https://docs.langchain.com/langsmith/smith-js-ts-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-js-ts-sdk.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith-managed ClickHouse

**URL:** llms-txt#langsmith-managed-clickhouse

**Contents:**
- Architecture Overview
- Requirements
- Data storage
  - Stored feedback data fields
  - Stored run data fields

Source: https://docs.langchain.com/langsmith/langsmith-managed-clickhouse

<Check>
  Please read the [LangSmith architectural overview](/langsmith/self-hosted) and [guide on connecting to external ClickHouse](/langsmith/self-host-external-clickhouse) before proceeding with this guide.
</Check>

LangSmith uses ClickHouse as the primary storage engine for **traces** and **feedback**. For easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external ClickHouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team.

## Architecture Overview

The architecture of using LangSmith-managed ClickHouse with your self-hosted LangSmith instance is similar to using a fully self-hosted ClickHouse instance, with a few key differences:

* You will need to set up a private network connection between your LangSmith instance and the LangSmith-managed ClickHouse instance. This is to ensure that your data is secure and that you can connect to the ClickHouse instance from your self-hosted LangSmith instance.
* With this option, sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of ClickHouse to ensure that sensitive information doesn't leave your VPC. For more details on where particular data fields are stored, refer to [Data storage](#data-storage).
* The LangSmith team will monitor your ClickHouse instance and ensure that it is running smoothly. This allows us to track metrics like run-ingestion delay and query performance.

The overall architecture looks like this:

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=26fae5c3f413c15302ea0c00bebf8e93" alt="LangSmith managed ClickHouse architecture." data-og-width="2196" width="2196" data-og-height="1755" height="1755" data-path="langsmith/images/managed-clickhouse-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b9cb43d51325b0d9858123066c0f8812 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=f21865c150f4047d9cfd0eee690099af 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=3df50b05e66bc3092558e23e321d11fe 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=821f427c87e452c5d93aec971de3e5c3 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ffd26e217759a683feeacdacde65c047 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-light.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=75b39fb829a3ed7491f3550ad5323965 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a3062f45f9c01f05e6917bca3f34735e" alt="LangSmith managed ClickHouse architecture." data-og-width="2196" width="2196" data-og-height="1755" height="1755" data-path="langsmith/images/managed-clickhouse-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=c2a591803a03df470900d351a3ba59dc 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=80f1fd346d92de102121180ac7995e0f 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=6bb18f947823c721cba47bc275999cc2 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=0d7313f3682d35e743a82f44465b9af6 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=2f96d154dff0885379ce9d0b6a8e40e9 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/managed-clickhouse-dark.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=b750629ab6f6ce608695538ff3ad46f5 2500w" />

* **You must use a supported blob storage option.** Read the [blob storage guide](/langsmith/self-host-blob-storage) for more information.
* To use private endpoints, ensure that your VPC is in a ClickHouse Cloud supported [region](https://clickhouse.com/docs/en/cloud/reference/supported-regions). Otherwise, you will need to use a public endpoint we will secure with firewall rules. Your VPC will need to have a NAT gateway to allow us to allowlist your traffic.
* You must have a VPC that can connect to the LangSmith-managed ClickHouse service. You will need to work with our team to set up the necessary networking.
* You must have a LangSmith self-hosted instance running. You can use our managed ClickHouse service with both [Kubernetes](/langsmith/kubernetes) and [Docker](/langsmith/docker) installations.

ClickHouse stores **runs** and **feedback** data, specifically:

* All feedback data fields.
* Some run data fields.

For a list of fields, refer to [Stored run data fields](#stored-run-data-fields) and [Stored feedback data fields](#stored-feedback-data-fields).

LangChain defines sensitive application data as `inputs`, `outputs`, `errors`, `manifests`, `extras`, and `events` of a run, since these fields may contain LLM prompts and completions. With LangSmith-managed ClickHouse, these sensitive fields are stored in cloud object storage (S3 or GCS) within your cloud, while the rest of the run data is stored in ClickHouse, ensuring sensitive information never leaves your VPC.

### Stored feedback data fields

<Note>
  Because all feedback data is stored in ClickHouse, do not send sensitive information in feedback (scores and annotations/comments) or in any other run fields that are mentioned in [Stored run data fields](#stored-run-data-fields).
</Note>

Using a LangSmith-managed ClickHouse setup, **all feedback data fields are stored in ClickHouse**:

| Field Name                | Type     | Description                                                                                            |
| ------------------------- | -------- | ------------------------------------------------------------------------------------------------------ |
| id                        | UUID     | Unique identifier for the record itself                                                                |
| created\_at               | datetime | Timestamp when the record was created                                                                  |
| modified\_at              | datetime | Timestamp when the record was last modified                                                            |
| session\_id               | UUID     | Unique identifier for the experiment or tracing project the run was a part of                          |
| run\_id                   | UUID     | Unique identifier for a specific run within a session                                                  |
| key                       | string   | A key describing the criteria of the feedback, eg "correctness"                                        |
| score                     | number   | Numerical score associated with the feedback key                                                       |
| value                     | string   | Reserved for storing a value associated with the score. Useful for categorical feedback.               |
| comment                   | string   | Any comment or annotation associated with the record. This can be a justification for the score given. |
| correction                | object   | Reserved for storing correction details, if any                                                        |
| feedback\_source          | object   | Object containing information about the feedback source                                                |
| feedback\_source.type     | string   | The type of source where the feedback originated, eg "api", "app", "evaluator"                         |
| feedback\_source.metadata | object   | Reserved for additional metadata, currently                                                            |
| feedback\_source.user\_id | UUID     | Unique identifier for the user providing feedback                                                      |

This [reference doc](/langsmith/feedback-data-format) explains the stored feedback format, which is the LangSmith's way of representing evaluation scores and annotations on runs.

### Stored run data fields

Run data fields are split between the managed ClickHouse database and your cloud object storage (e.g., S3 or GCS).

<Note>
  For run fields stored in object storage, only a reference or pointer is kept in ClickHouse. For example, `inputs` and `outputs` content are offloaded to S3/GCS, with the ClickHouse record storing corresponding S3 URLs in the `inputs_s3_urls` and `outputs_s3_urls` fields.
</Note>

The table details each run field and where it is stored:

| Field                          | Storage Location   |
| ------------------------------ | ------------------ |
| `id`                           | ClickHouse         |
| `name`                         | ClickHouse         |
| `inputs`                       | **Object Storage** |
| `run_type`                     | ClickHouse         |
| `start_time`                   | ClickHouse         |
| `end_time`                     | ClickHouse         |
| `extra`                        | **Object Storage** |
| `error`                        | **Object Storage** |
| `outputs`                      | **Object Storage** |
| `events`                       | **Object Storage** |
| `tags`                         | ClickHouse         |
| `trace_id`                     | ClickHouse         |
| `dotted_order`                 | ClickHouse         |
| `status`                       | ClickHouse         |
| `child_run_ids`                | ClickHouse         |
| `direct_child_run_ids`         | ClickHouse         |
| `parent_run_ids`               | ClickHouse         |
| `feedback_stats`               | ClickHouse         |
| `reference_example_id`         | ClickHouse         |
| `total_tokens`                 | ClickHouse         |
| `prompt_tokens`                | ClickHouse         |
| `completion_tokens`            | ClickHouse         |
| `total_cost`                   | ClickHouse         |
| `prompt_cost`                  | ClickHouse         |
| `completion_cost`              | ClickHouse         |
| `first_token_time`             | ClickHouse         |
| `session_id`                   | ClickHouse         |
| `in_dataset`                   | ClickHouse         |
| `parent_run_id`                | ClickHouse         |
| `execution_order` (deprecated) | ClickHouse         |
| `serialized`                   | ClickHouse         |
| `manifest_id` (deprecated)     | ClickHouse         |
| `manifest_s3_id`               | ClickHouse         |
| `inputs_s3_urls`               | ClickHouse         |
| `outputs_s3_urls`              | ClickHouse         |
| `price_model_id`               | ClickHouse         |
| `app_path`                     | ClickHouse         |
| `last_queued_at`               | ClickHouse         |
| `share_token`                  | ClickHouse         |

This [reference doc](/langsmith/run-data-format) explains the format of stored runs (spans), which are the building blocks of traces.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/langsmith-managed-clickhouse.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith Observability

**URL:** llms-txt#langsmith-observability

**Contents:**
- Prerequisites
- Enable tracing
- Trace selectively

Source: https://docs.langchain.com/oss/python/langgraph/observability

Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](/langsmith/trace-with-langgraph). This enables you to do the following:

* [Debug a locally running application](/langsmith/observability-studio#debug-langsmith-traces).
* [Evaluate the application performance](/oss/python/langchain/evals).
* [Monitor the application](/langsmith/dashboards).

Before you begin, ensure you have the following:

* **A LangSmith account**: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com).
* **A LangSmith API key**: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide.

To enable tracing for your application, set the following environment variables:

By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).

For more information, see [Trace with LangGraph](/langsmith/trace-with-langgraph).

You may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:

```python  theme={null}
import langsmith as ls

**Examples:**

Example 1 (unknown):
```unknown
By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project).

For more information, see [Trace with LangGraph](/langsmith/trace-with-langgraph).

## Trace selectively

You may opt to trace specific invocations or parts of your application using LangSmith's `tracing_context` context manager:
```

---

## LangSmith Polly

**URL:** llms-txt#langsmith-polly

**Contents:**
  - Tracing page
  - Thread view
  - Prompt Playground
- What's next

Source: https://docs.langchain.com/langsmith/polly

<Callout color="#4F46E5">
  **Polly is in beta.** Your [feedback](https://forum.langchain.com) on Polly is invaluable as the team refines its capabilities.
</Callout>

**LangSmith Polly** is an AI assistant embedded directly in your LangSmith [workspace](/langsmith/administration-overview#workspaces) to help you analyze and understand your application data.

Polly helps you gain insight from your traces, conversation threads, and prompts without having to dig through data manually. By asking natural language questions, you can quickly understand agent performance, debug issues, and analyze user sentiment.

<img src="https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly.png?fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=fa4a72becb05f414f053f06af4ce6afb" alt="LangSmith Polly icon" style={{float: 'left', marginRight: '20px', marginTop: '-1px', marginBottom: '20px', maxWidth: '100px'}} data-og-width="120" width="120" data-og-height="54" height="54" data-path="langsmith/images/polly.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly.png?w=280&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=e0466a20faaeb2a5a89007aa07788cfb 280w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly.png?w=560&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=5ebe583f22eb955aade91c1d030a9c8f 560w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly.png?w=840&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=f8f45119dda87228e2d5d9f59de40025 840w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly.png?w=1100&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=d1c4e7b178aad853b49bcd74e1afac22 1100w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly.png?w=1650&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=865d3c5195858e7114b3ec2e5f9876ea 1650w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly.png?w=2500&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=725b2b5615c1248dd39959e16cac36d2 2500w" /> Polly appears in the right-hand bottom corner of the following locations within [LangSmith UI](https://smith.langchain.com), optimized for different use cases:

* [Trace pages](#tracing-page)
* [Thread views](#thread-views)
* [Prompt Playground](#prompt-playground)

On an individual [trace](/langsmith/observability-concepts#traces), Polly pulls in the context of the page and analyzes the [run](/langsmith/observability-concepts#runs). Polly reads the run data and trajectory to help you understand what happened and identify areas for improvement.

To ask Polly about your tracing:

1. In your **Tracing Projects**, click on a trace to view its details page.
2. Select a run in the trace.
3. Open Polly in the right-hand corner of the page to ask questions relating to this run.
4. Ask Polly a question about your data. You can use the sample questions or you might ask questions like:

* "Is there anything that the agent could have done better here?"
   * "Why did this run fail?"
   * "What took the most time in this trace?"
   * "What errors occurred during this run?"
   * "Summarize what happened in this trace"

When analyzing runs, Polly will examine the full trace context, including [run metadata](/langsmith/observability-concepts#metadata), inputs, outputs, intermediate steps, and configuration to provide actionable insights. This helps you diagnose issues without manually expanding each step in the trace tree or cross-referencing multiple runs.

Under the **Threads** tab, Polly analyzes conversation [threads](/langsmith/observability-concepts#threads) by pulling in relevant information about the user interaction. This helps you understand user sentiment and conversation outcomes.

To ask Polly about your threads:

1. Select a thread.
2. Open Polly in the right-hand corner of the page to ask questions relating to this thread.
3. Ask Polly a question about the conversation thread. You might ask questions like:

* "Did the user seem frustrated?"
   * "What issues is the user experiencing?"
   * "How did this conversation resolve?"
   * "Was the user's problem solved?"
   * "What was the main topic of this thread?"

Use Polly in thread view to gain insights into how users are interacting with your application. Understand conversation outcomes and whether issues were resolved, identify common user pain points, and track user sentiment through thread analysis. This helps you improve user experience by understanding what's working and what needs improvement in your application's responses.

### Prompt Playground

When you open a [prompt](/langsmith/prompt-engineering-concepts#prompt-in-langsmith) in the [Playground](/langsmith/prompt-engineering-concepts#prompt-playground), Polly can help you edit and improve your prompts based on your instructions. Polly reads the prompt and makes suggested edits.

To ask Polly about your prompt:

1. Enter the **Playground** from the left-hand navigation or trace view.
2. Select a prompt to experiment with.
3. Open Polly in the right-hand corner of the page to work on this prompt.
4. You can use one of the automated options that Polly suggests:

* <Icon icon="play" /> **Optimize prompt**: Polly will analyze the current prompt and make edits to the prompt with a summary of the changes.
   * <Icon icon="wrench" /> **Generate a tool**: Give details to Polly on the tool you would like to add. It will generate a tool for your prompt template. It can also help you modify existing tooling or system messages about tooling. Then, have Polly test tool configurations with reviews of sample output from the model using the tool.
   * <Icon icon="brackets-curly" /> **Generate an output schema**: Polly will create a JSON schema that defines the structure of the output you want the model to generate. This is useful when you need the model to return data in a specific format. Select this option, and then provide Polly with the type of data, fields/properties, and any other constraints you might need.

Or, you might ask your own questions, like:

* "Make it respond in Italian"
   * "Add more context about the user's role"
   * "Make the tone more professional"
   * "Simplify the instructions"
   * "Add examples to the prompt"

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool.png?fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=f99e532bc008d41a40808c8f2eb988d3" alt="Prompt Playground showing Polly chat in the sidebar with information on a generated tool." data-og-width="1520" width="1520" data-og-height="1151" height="1151" data-path="langsmith/images/polly-prompt-tool.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool.png?w=280&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=32813c69c4faf716649aafccb0fe8275 280w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool.png?w=560&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=25d105f79090bea64897cd7a439b3ddb 560w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool.png?w=840&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=259d26f4be950927a72acdc7b2070a68 840w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool.png?w=1100&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=2b52a59e445e14388c66cc585cb34864 1100w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool.png?w=1650&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=3fde3e574cab288f671cc0f2864e9a64 1650w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool.png?w=2500&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=98f8ea9f62eace790f560e0563d4cc7b 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool-dark.png?fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=1b0b035d9ac5471b7fc4d536e66149a6" alt="Prompt Playground showing Polly chat in the sidebar with information on a generated tool." data-og-width="1523" width="1523" data-og-height="1152" height="1152" data-path="langsmith/images/polly-prompt-tool-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool-dark.png?w=280&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=63b88b48b4c46acc691a8de870e29b56 280w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool-dark.png?w=560&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=837158f3440dcf6e9e031d4426c712a0 560w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool-dark.png?w=840&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=651982d8dc9d454ea7cc99dbbf23c454 840w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool-dark.png?w=1100&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=98ead88299efdcee34f54dd05084ab7b 1100w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool-dark.png?w=1650&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=5d79413aaf531d5cfe9881fec582140d 1650w, https://mintcdn.com/langchain-5e9cc07a/Ttks5oP9I9O2zjYP/langsmith/images/polly-prompt-tool-dark.png?w=2500&fit=max&auto=format&n=Ttks5oP9I9O2zjYP&q=85&s=fa2fb0268ae7784d6f733aa4020dd00f 2500w" />

Learn more about the features that Polly helps you explore:

<CardGroup cols={2}>
  <Card title="Observability" icon="magnifying-glass" href="/langsmith/observability">
    Learn more about tracing and monitoring your LLM applications
  </Card>

<Card title="Threads" icon="comments" href="/langsmith/threads">
    Understand how threads work in LangSmith
  </Card>

<Card title="Prompt Engineering" icon="wand-magic-sparkles" href="/langsmith/prompt-engineering">
    Create and iterate on prompts in the playground
  </Card>

<Card title="Evaluation" icon="clipboard-check" href="/langsmith/evaluation">
    Evaluate and test your applications systematically
  </Card>
</CardGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/polly.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith Python SDK

**URL:** llms-txt#langsmith-python-sdk

Source: https://docs.langchain.com/langsmith/smith-python-sdk

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/smith-python-sdk.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith reference

**URL:** llms-txt#langsmith-reference

Source: https://docs.langchain.com/langsmith/reference

The following sections provide API references and SDK documentation for LangSmith:

<Columns cols={3}>
  <Card title="Python SDK" icon="python" href="/langsmith/smith-python-sdk" arrow="true">
    Reference documentation for the LangSmith Python SDK.
  </Card>

<Card title="JavaScript/TypeScript SDK" icon="js" href="/langsmith/smith-js-ts-sdk" arrow="true">
    Reference documentation for the LangSmith JavaScript/TypeScript SDK.
  </Card>

<Card title="LangGraph Python SDK" icon="diagram-project" href="/langsmith/langgraph-python-sdk" arrow="true">
    Reference documentation for deploying LangGraph applications with Python.
  </Card>

<Card title="LangGraph JS/TS SDK" icon="diagram-project" href="/langsmith/langgraph-js-ts-sdk" arrow="true">
    Reference documentation for deploying LangGraph applications with JavaScript/TypeScript.
  </Card>

<Card title="LangSmith API" icon="code" href="/langsmith/smith-api-ref" arrow="true">
    Complete REST API reference for LangSmith platform features.
  </Card>

<Card title="Deployment APIs" icon="server" href="/langsmith/server-api-ref" arrow="true">
    API references for self-hosted and hybrid LangSmith deployments.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/reference.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LangSmith status

**URL:** llms-txt#langsmith-status

Source: https://docs.langchain.com/langsmith/status

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/status.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Let's configure the RetryPolicy to retry on ValueError.

**URL:** llms-txt#let's-configure-the-retrypolicy-to-retry-on-valueerror.

---

## Let's say hi again

**URL:** llms-txt#let's-say-hi-again

**Contents:**
- Checkpointer libraries
  - Checkpointer interface
  - Serializer

for update in graph.stream(
    {"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates"
):
    print(update)
json  theme={null}
{
    ...
    "store": {
        "index": {
            "embed": "openai:text-embeddings-3-small",
            "dims": 1536,
            "fields": ["$"]
        }
    }
}
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer

**Examples:**

Example 1 (unknown):
```unknown
When we use the LangSmith, either locally (e.g., in [Studio](/langsmith/studio)) or [hosted with LangSmith](/langsmith/platform-setup), the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you **do** need to configure the indexing settings in your `langgraph.json` file. For example:
```

Example 2 (unknown):
```unknown
See the [deployment guide](/langsmith/semantic-search) for more details and configuration options.

## Checkpointer libraries

Under the hood, checkpointing is powered by checkpointer objects that conform to [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:

* `langgraph-checkpoint`: The base interface for checkpointer savers ([`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)) and serialization/deserialization interface ([`SerializerProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)). Includes in-memory checkpointer implementation ([`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)) for experimentation. LangGraph comes with `langgraph-checkpoint` included.
* `langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ([`SqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [`AsyncSqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)). Ideal for experimentation and local workflows. Needs to be installed separately.
* `langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ([`PostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)), used in LangSmith. Ideal for using in production. Needs to be installed separately.

### Checkpointer interface

Each checkpointer conforms to [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface and implements the following methods:

* `.put` - Store a checkpoint with its configuration and metadata.
* `.put_writes` - Store intermediate writes linked to a checkpoint (i.e. [pending writes](#pending-writes)).
* `.get_tuple` - Fetch a checkpoint tuple using for a given configuration (`thread_id` and `checkpoint_id`). This is used to populate `StateSnapshot` in `graph.get_state()`.
* `.list` - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.get_state_history()`

If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via `.ainvoke`, `.astream`, `.abatch`), asynchronous versions of the above methods will be used (`.aput`, `.aput_writes`, `.aget_tuple`, `.alist`).

<Note>
  For running your graph asynchronously, you can use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver), or async versions of Sqlite/Postgres checkpointers -- [`AsyncSqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver) / [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver) checkpointers.
</Note>

### Serializer

When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.

`langgraph_checkpoint` defines [protocol](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol) for implementing serializers provides a default implementation ([`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.

#### Serialization with `pickle`

The default serializer, [`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer), uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.

If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes),
you can use the `pickle_fallback` argument of the [`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer):
```

---

## List buckets

**URL:** llms-txt#list-buckets

aws s3 --endpoint-url=<endpoint_url> ls /

---

## List Deployments

**URL:** llms-txt#list-deployments

Source: https://docs.langchain.com/api-reference/deployments-v2/list-deployments

https://api.host.langchain.com/openapi.json get /v2/deployments
List all deployments.

---

## List GitHub Integrations

**URL:** llms-txt#list-github-integrations

Source: https://docs.langchain.com/api-reference/integrations-v1/list-github-integrations

https://api.host.langchain.com/openapi.json get /v1/integrations/github/install
List available GitHub integrations for LangGraph Platfom Cloud SaaS.

---

## List GitHub Repositories

**URL:** llms-txt#list-github-repositories

Source: https://docs.langchain.com/api-reference/integrations-v1/list-github-repositories

https://api.host.langchain.com/openapi.json get /v1/integrations/github/{integration_id}/repos
List available GitHub repositories for an integration that are available to deploy to LangSmith Deployment.

---

## List Listeners

**URL:** llms-txt#list-listeners

Source: https://docs.langchain.com/api-reference/listeners-v2/list-listeners

https://api.host.langchain.com/openapi.json get /v2/listeners
List all listeners.

---

## List namespaces with optional match conditions.

**URL:** llms-txt#list-namespaces-with-optional-match-conditions.

Source: https://docs.langchain.com/langsmith/agent-server-api/store/list-namespaces-with-optional-match-conditions

langsmith/agent-server-openapi.json post /store/namespaces

---

## List Oauth Providers

**URL:** llms-txt#list-oauth-providers

Source: https://docs.langchain.com/api-reference/auth-service-v2/list-oauth-providers

https://api.host.langchain.com/openapi.json get /v2/auth/providers
List OAuth providers.

---

## List of standard content blocks

**URL:** llms-txt#list-of-standard-content-blocks

**Contents:**
  - Standard content blocks
  - Multimodal
  - Content block reference
- Use with chat models

human_message = HumanMessage(content_blocks=[
    {"type": "text", "text": "Hello, how are you?"},
    {"type": "image", "url": "https://example.com/image.jpg"},
])
python  theme={null}
    from langchain.messages import AIMessage

message = AIMessage(
        content=[
            {"type": "thinking", "thinking": "...", "signature": "WaUjzkyp..."},
            {"type": "text", "text": "..."},
        ],
        response_metadata={"model_provider": "anthropic"}
    )
    message.content_blocks
    
    [{'type': 'reasoning',
      'reasoning': '...',
      'extras': {'signature': 'WaUjzkyp...'}},
     {'type': 'text', 'text': '...'}]
    python  theme={null}
    from langchain.messages import AIMessage

message = AIMessage(
        content=[
            {
                "type": "reasoning",
                "id": "rs_abc123",
                "summary": [
                    {"type": "summary_text", "text": "summary 1"},
                    {"type": "summary_text", "text": "summary 2"},
                ],
            },
            {"type": "text", "text": "...", "id": "msg_abc123"},
        ],
        response_metadata={"model_provider": "openai"}
    )
    message.content_blocks
    
    [{'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 1'},
     {'type': 'reasoning', 'id': 'rs_abc123', 'reasoning': 'summary 2'},
     {'type': 'text', 'text': '...', 'id': 'msg_abc123'}]
    python  theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-5-nano", output_version="v1")
  python Image input theme={null}
  # From URL
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {"type": "image", "url": "https://example.com/path/to/image.jpg"},
      ]
  }

# From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {
              "type": "image",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "image/jpeg",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this image."},
          {"type": "image", "file_id": "file-abc123"},
      ]
  }
  python PDF document input theme={null}
  # From URL
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {"type": "file", "url": "https://example.com/path/to/document.pdf"},
      ]
  }

# From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {
              "type": "file",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "application/pdf",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this document."},
          {"type": "file", "file_id": "file-abc123"},
      ]
  }
  python Audio input theme={null}
  # From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this audio."},
          {
              "type": "audio",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "audio/wav",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this audio."},
          {"type": "audio", "file_id": "file-abc123"},
      ]
  }
  python Video input theme={null}
  # From base64 data
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this video."},
          {
              "type": "video",
              "base64": "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
              "mime_type": "video/mp4",
          },
      ]
  }

# From provider-managed File ID
  message = {
      "role": "user",
      "content": [
          {"type": "text", "text": "Describe the content of this video."},
          {"type": "video", "file_id": "file-abc123"},
      ]
  }
  python  theme={null}
        {
            "type": "text",
            "text": "Hello world",
            "annotations": []
        }
        python  theme={null}
        {
            "type": "reasoning",
            "reasoning": "The user is asking about...",
            "extras": {"signature": "abc123"},
        }
        python  theme={null}
        {
            "type": "tool_call",
            "name": "search",
            "args": {"query": "weather"},
            "id": "call_123"
        }
        ```
      </Accordion>

<Accordion title="ToolCallChunk" icon="puzzle-piece">
        **Purpose:** Streaming tool call fragments

<ParamField body="type" type="string" required>
          Always `"tool_call_chunk"`
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool being called
        </ParamField>

<ParamField body="args" type="string">
          Partial tool arguments (may be incomplete JSON)
        </ParamField>

<ParamField body="id" type="string">
          Tool call identifier
        </ParamField>

<ParamField body="index" type="number | string">
          Position of this chunk in the stream
        </ParamField>
      </Accordion>

<Accordion title="InvalidToolCall" icon="triangle-exclamation">
        **Purpose:** Malformed calls, intended to catch JSON parsing errors.

<ParamField body="type" type="string" required>
          Always `"invalid_tool_call"`
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool that failed to be called
        </ParamField>

<ParamField body="args" type="object">
          Arguments to pass to the tool
        </ParamField>

<ParamField body="error" type="string">
          Description of what went wrong
        </ParamField>
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Server-Side Tool Execution" icon="server">
    <AccordionGroup>
      <Accordion title="ServerToolCall" icon="wrench">
        **Purpose:** Tool call that is executed server-side.

<ParamField body="type" type="string" required>
          Always `"server_tool_call"`
        </ParamField>

<ParamField body="id" type="string" required>
          An identifier associated with the tool call.
        </ParamField>

<ParamField body="name" type="string" required>
          The name of the tool to be called.
        </ParamField>

<ParamField body="args" type="string" required>
          Partial tool arguments (may be incomplete JSON)
        </ParamField>
      </Accordion>

<Accordion title="ServerToolCallChunk" icon="puzzle-piece">
        **Purpose:** Streaming server-side tool call fragments

<ParamField body="type" type="string" required>
          Always `"server_tool_call_chunk"`
        </ParamField>

<ParamField body="id" type="string">
          An identifier associated with the tool call.
        </ParamField>

<ParamField body="name" type="string">
          Name of the tool being called
        </ParamField>

<ParamField body="args" type="string">
          Partial tool arguments (may be incomplete JSON)
        </ParamField>

<ParamField body="index" type="number | string">
          Position of this chunk in the stream
        </ParamField>
      </Accordion>

<Accordion title="ServerToolResult" icon="box-open">
        **Purpose:** Search results

<ParamField body="type" type="string" required>
          Always `"server_tool_result"`
        </ParamField>

<ParamField body="tool_call_id" type="string" required>
          Identifier of the corresponding server tool call.
        </ParamField>

<ParamField body="id" type="string">
          Identifier associated with the server tool result.
        </ParamField>

<ParamField body="status" type="string" required>
          Execution status of the server-side tool. `"success"` or `"error"`.
        </ParamField>

<ParamField body="output">
          Output of the executed tool.
        </ParamField>
      </Accordion>
    </AccordionGroup>
  </Accordion>

<Accordion title="Provider-Specific Blocks" icon="plug">
    <Accordion title="NonStandardContentBlock" icon="asterisk">
      **Purpose:** Provider-specific escape hatch

<ParamField body="type" type="string" required>
        Always `"non_standard"`
      </ParamField>

<ParamField body="value" type="object" required>
        Provider-specific data structure
      </ParamField>

**Usage:** For experimental or provider-unique features
    </Accordion>

Additional provider-specific content types may be found within the [reference documentation](/oss/python/integrations/providers/overview) of each model provider.
  </Accordion>
</AccordionGroup>

<Tip>
  View the canonical type definitions in the [API reference](https://reference.langchain.com/python/langchain/messages).
</Tip>

<Info>
  Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code.

Content blocks are not a replacement for the [`content`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content) property, but rather a new property that can be used to access the content of a message in a standardized format.
</Info>

## Use with chat models

[Chat models](/oss/python/langchain/models) accept a sequence of message objects as input and return an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.

Refer to the below guides to learn more:

* Built-in features for [persisting and managing conversation histories](/oss/python/langchain/short-term-memory)
* Strategies for managing context windows, including [trimming and summarizing messages](/oss/python/langchain/short-term-memory#common-patterns)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/messages.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  Specifying `content_blocks` when initializing a message will still populate message
  `content`, but provides a type-safe interface for doing so.
</Tip>

### Standard content blocks

LangChain provides a standard representation for message content that works across providers.

Message objects implement a `content_blocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [`ChatAnthropic`](/oss/python/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/python/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:

<Tabs>
  <Tab title="Anthropic">
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="OpenAI">
```

Example 4 (unknown):
```unknown

```

---

## List Revisions

**URL:** llms-txt#list-revisions

Source: https://docs.langchain.com/api-reference/deployments-v2/list-revisions

https://api.host.langchain.com/openapi.json get /v2/deployments/{deployment_id}/revisions
List all revisions for a deployment.

---

## List Runs

**URL:** llms-txt#list-runs

Source: https://docs.langchain.com/langsmith/agent-server-api/thread-runs/list-runs

langsmith/agent-server-openapi.json get /threads/{thread_id}/runs
List runs for a thread.

---

## List threads - each user only sees their own

**URL:** llms-txt#list-threads---each-user-only-sees-their-own

**Contents:**
- 3. Add scoped authorization handlers

alice_threads = await alice.threads.search()
bob_threads = await bob.threads.search()
print(f"✅ Alice sees {len(alice_threads)} thread")
print(f"✅ Bob sees {len(bob_threads)} thread")
bash  theme={null}
✅ Alice created assistant: fc50fb08-78da-45a9-93cc-1d3928a3fc37
✅ Alice created thread: 533179b7-05bc-4d48-b47a-a83cbdb5781d
✅ Bob correctly denied access: Client error '404 Not Found' for url 'http://localhost:2024/threads/533179b7-05bc-4d48-b47a-a83cbdb5781d'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
✅ Bob created his own thread: 437c36ed-dd45-4a1e-b484-28ba6eca8819
✅ Alice sees 1 thread
✅ Bob sees 1 thread
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Output:
```

Example 2 (unknown):
```unknown
This means:

1. Each user can create and chat in their own threads
2. Users can't see each other's threads
3. Listing threads only shows your own

<a id="scoped-authorization" />

## 3. Add scoped authorization handlers

The broad `@auth.on` handler matches on all [authorization events](/langsmith/auth#supported-resources). This is concise, but it means the contents of the `value` dict are not well-scoped, and the same user-level access control is applied to every resource. If you want to be more fine-grained, you can also control specific actions on resources.

Update `src/security/auth.py` to add handlers for specific resource types:
```

---

## LLMs

**URL:** llms-txt#llms

**Contents:**
- All LLMs

Source: https://docs.langchain.com/oss/javascript/integrations/llms/index

<Warning>
  **You are currently on a page documenting the use of text completion models. Many of the latest and most popular models are [chat completion models](/oss/javascript/langchain/models).**

Unless you are specifically using more advanced prompting techniques, you are probably looking for [this page instead](/oss/javascript/integrations/chat/).
</Warning>

[LLMs](/oss/javascript/langchain/models) are language models that takes a string as input and return a string as output.

<Columns cols={3}>
  <Card title="AI21" icon="link" href="/oss/javascript/integrations/llms/ai21" arrow="true" cta="View guide" />

<Card title="AlephAlpha" icon="link" href="/oss/javascript/integrations/llms/aleph_alpha" arrow="true" cta="View guide" />

<Card title="Arcjet Redact" icon="link" href="/oss/javascript/integrations/llms/arcjet" arrow="true" cta="View guide" />

<Card title="AWS SageMakerEndpoint" icon="link" href="/oss/javascript/integrations/llms/aws_sagemaker" arrow="true" cta="View guide" />

<Card title="Azure OpenAI" icon="link" href="/oss/javascript/integrations/llms/azure" arrow="true" cta="View guide" />

<Card title="Bedrock" icon="link" href="/oss/javascript/integrations/llms/bedrock" arrow="true" cta="View guide" />

<Card title="ChromeAI" icon="link" href="/oss/javascript/integrations/llms/chrome_ai" arrow="true" cta="View guide" />

<Card title="Cloudflare Workers AI" icon="link" href="/oss/javascript/integrations/llms/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/javascript/integrations/llms/cohere" arrow="true" cta="View guide" />

<Card title="Deep Infra" icon="link" href="/oss/javascript/integrations/llms/deep_infra" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/javascript/integrations/llms/fireworks" arrow="true" cta="View guide" />

<Card title="Friendli" icon="link" href="/oss/javascript/integrations/llms/friendli" arrow="true" cta="View guide" />

<Card title="Google Vertex AI" icon="link" href="/oss/javascript/integrations/llms/google_vertex_ai" arrow="true" cta="View guide" />

<Card title="Gradient AI" icon="link" href="/oss/javascript/integrations/llms/gradient_ai" arrow="true" cta="View guide" />

<Card title="HuggingFaceInference" icon="link" href="/oss/javascript/integrations/llms/huggingface_inference" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/javascript/integrations/llms/ibm" arrow="true" cta="View guide" />

<Card title="JigsawStack Prompt Engine" icon="link" href="/oss/javascript/integrations/llms/jigsawstack" arrow="true" cta="View guide" />

<Card title="Layerup Security" icon="link" href="/oss/javascript/integrations/llms/layerup_security" arrow="true" cta="View guide" />

<Card title="Llama CPP" icon="link" href="/oss/javascript/integrations/llms/llama_cpp" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/javascript/integrations/llms/mistral" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/javascript/integrations/llms/ollama" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/javascript/integrations/llms/openai" arrow="true" cta="View guide" />

<Card title="RaycastAI" icon="link" href="/oss/javascript/integrations/llms/raycast" arrow="true" cta="View guide" />

<Card title="Replicate" icon="link" href="/oss/javascript/integrations/llms/replicate" arrow="true" cta="View guide" />

<Card title="Together AI" icon="link" href="/oss/javascript/integrations/llms/together" arrow="true" cta="View guide" />

<Card title="WRITER" icon="link" href="/oss/javascript/integrations/llms/writer" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/javascript/integrations/llms/yandex" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llms/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## LLM-as-judge instructions

**URL:** llms-txt#llm-as-judge-instructions

grader_instructions = """You are a teacher grading a quiz.

You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE.

Here is the grade criteria to follow:
(1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer.
(2) Ensure that the student response does not contain any conflicting statements.
(3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the  ground truth response.

Correctness:
True means that the student's response meets all of the criteria.
False means that the student's response does not meet all of the criteria.

Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct."""

---

## LLM-as-judge output schema

**URL:** llms-txt#llm-as-judge-output-schema

class Grade(TypedDict):
    """Compare the expected and actual answers and grade the actual answer."""
    reasoning: Annotated[str, ..., "Explain your reasoning for whether the actual response is correct or not."]
    is_correct: Annotated[bool, ..., "True if the student response is mostly or exactly correct, otherwise False."]

---

## Load all documents

**URL:** llms-txt#load-all-documents

documents = loader.load()

---

## Load all resources from a server

**URL:** llms-txt#load-all-resources-from-a-server

blobs = await client.get_resources("server_name")  # [!code highlight]

---

## Load a prompt by name

**URL:** llms-txt#load-a-prompt-by-name

messages = await client.get_prompt("server_name", "summarize")  # [!code highlight]

---

## Load a prompt with arguments

**URL:** llms-txt#load-a-prompt-with-arguments

messages = await client.get_prompt(  # [!code highlight]
    "server_name",  # [!code highlight]
    "code_review",  # [!code highlight]
    arguments={"language": "python", "focus": "security"}  # [!code highlight]
)  # [!code highlight]

---

## Load environment variables

**URL:** llms-txt#load-environment-variables

dotenv.load_dotenv(".env.local")

---

## Load files and create attachments

**URL:** llms-txt#load-files-and-create-attachments

image_data = load_file("my_image.png")
audio_data = load_file("my_mp3.mp3")
video_data = load_file("my_video.mp4")
pdf_data = load_file("my_document.pdf")

image_attachment = Attachment(mime_type="image/png", data=image_data)
audio_attachment = Attachment(mime_type="audio/mpeg", data=audio_data)
video_attachment = Attachment(mime_type="video/mp4", data=video_data)
pdf_attachment = ("application/pdf", pdf_data) # Can just define as tuple of (mime_type, data)
csv_attachment = Attachment(mime_type="text/csv", data=Path(os.getcwd()) / "my_csv.csv")

---

## Logs: [OTel Example](/langsmith/langsmith-collector#logs)

**URL:** llms-txt#logs:-[otel-example](/langsmith/langsmith-collector#logs)

All services that are part of the LangSmith self-hosted deployment write logs to their node's filesystem and to stdout. In order to access these logs, you need to set up your collector to read from either the filesystem or stdout. Most popular collectors support reading logs from filesystems.

* **OpenTelemetry**: [File Log Receiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/filelogreceiver)
* **FluentBit**: [Tail Input](https://docs.fluentbit.io/manual/pipeline/inputs/tail)
* **Datadog**: [Kubernetes Log Collection](https://docs.datadoghq.com/containers/kubernetes/log/?tab=datadogoperator)

---

## Log in as user 1

**URL:** llms-txt#log-in-as-user-1

user1_token = await login(email1, password)
user1_client = get_client(
    url="http://localhost:2024", headers={"Authorization": f"Bearer {user1_token}"}
)

---

## Log LLM calls

**URL:** llms-txt#log-llm-calls

**Contents:**
- Messages Format
  - Examples
- Converting custom I/O formats into LangSmith compatible formats
- Identifying a custom model in traces
- Provide token and cost information
- Time-to-first-token

Source: https://docs.langchain.com/langsmith/log-llm-trace

This guide will cover how to log LLM calls to LangSmith when you are using a custom model or a custom input/output format. To make the most of LangSmith's LLM trace processing, you should log your LLM traces in one of the specified formats.

LangSmith offers the following benefits for LLM traces:

* Rich, structured rendering of message lists
* Token and cost tracking per LLM call, per trace and across traces over time

If you don't log your LLM traces in the suggested formats, you will still be able to log the data to LangSmith, but it may not be processed or rendered in expected ways.

If you are using [LangChain OSS](https://python.langchain.com/docs/tutorials/llm_chain/) to call language models or LangSmith wrappers ([OpenAI](/langsmith/trace-openai), [Anthropic](/langsmith/trace-anthropic)), these approaches will automatically log traces in the correct format.

<Note>
  The examples on this page use the `traceable` decorator/wrapper to log the model run (which is the recommended approach for Python and JS/TS). However, the same idea applies if you are using the [RunTree](/langsmith/annotate-code#use-the-runtree-api) or [API](https://api.smith.langchain.com/redoc) directly.
</Note>

When tracing a custom model or a custom input/output format, it must either follow the LangChain format, OpenAI completions format or Anthropic messages format. For more details,  refer to the [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat/create) or [Anthropic Messages](https://platform.claude.com/docs/en/api/messages) documentation. The LangChain format is:

<Expandable title="LangChain format">
  <ParamField path="messages" type="array" required>
    A list of messages containing the content of the conversation.

<ParamField path="role" type="string" required>
      Identifies the message type. One of: <code>system</code> | <code>reasoning</code> | <code>user</code> | <code>assistant</code> | <code>tool</code>
    </ParamField>

<ParamField path="content" type="array" required>
      Content of the message. List of typed dictionaries.

<Expandable title="Content options">
        <ParamField path="type" type="string" required>
          One of: <code>text</code> | <code>image</code> | <code>file</code> | <code>audio</code> | <code>video</code> | <code>tool\_call</code> | <code>server\_tool\_call</code> | <code>server\_tool\_result</code>.
        </ParamField>

<Expandable title="text">
          <ParamField path="type" type="literal('text')" required />

<ParamField path="text" type="string" required>
            Text content.
          </ParamField>

<ParamField path="annotations" type="object[]">
            List of annotations for the text
          </ParamField>

<ParamField path="extras" type="object">
            Additional provider-specific data.
          </ParamField>
        </Expandable>

<Expandable title="reasoning">
          <ParamField path="type" type="literal('reasoning')" required />

<ParamField path="text" type="string" required>
            Text content.
          </ParamField>

<ParamField path="extras" type="object">
            Additional provider-specific data.
          </ParamField>
        </Expandable>

<Expandable title="image">
          <ParamField path="type" type="literal('image')" required />

<ParamField path="url" type="string">
            URL pointing to the image location.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded image data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored image (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`).
          </ParamField>
        </Expandable>

<Expandable title="file (e.g., PDFs)">
          <ParamField path="type" type="literal('file')" required />

<ParamField path="url" type="string">
            URL pointing to the file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded file data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `application/pdf`).
          </ParamField>
        </Expandable>

<Expandable title="audio">
          <ParamField path="type" type="literal('audio')" required />

<ParamField path="url" type="string">
            URL pointing to the audio file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded audio data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored audio file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `audio/mpeg`, `audio/wav`).
          </ParamField>
        </Expandable>

<Expandable title="video">
          <ParamField path="type" type="literal('video')" required />

<ParamField path="url" type="string">
            URL pointing to the video file.
          </ParamField>

<ParamField path="base64" type="string" required>
            Base64-encoded video data.
          </ParamField>

<ParamField path="id" type="string">
            Reference ID to an externally stored video file (e.g., in a provider’s file system or in a bucket).
          </ParamField>

<ParamField path="mime_type" type="string">
            Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `video/mp4`, `video/webm`).
          </ParamField>
        </Expandable>

<Expandable title="tool_call">
          <ParamField path="type" type="literal('tool_call')" required />

<ParamField path="name" type="string" />

<ParamField path="args" type="object" required>
            Arguments to pass to the tool.
          </ParamField>

<ParamField path="id" type="string">
            Unique identifier for this tool call.
          </ParamField>
        </Expandable>

<Expandable title="server_tool_call">
          <ParamField path="type" type="literal('server_tool_call')" required />

<ParamField path="id" type="string" required>
            Unique identifier for this tool call.
          </ParamField>

<ParamField path="name" type="string" required>
            The name of the tool to be called.
          </ParamField>

<ParamField path="args" type="object" required>
            Arguments to pass to the tool.
          </ParamField>
        </Expandable>

<Expandable title="server_tool_result">
          <ParamField path="type" type="literal('server_tool_result')" required />

<ParamField path="tool_call_id" type="string" required>
            Identifier of the corresponding server tool call.
          </ParamField>

<ParamField path="id" type="string">
            Unique identifier for this tool call.
          </ParamField>

<ParamField path="status" type="string" required>
            Execution status of the server-side tool. One of: <code>success</code> | <code>error</code>.
          </ParamField>

<ParamField path="output">
            Output of the executed tool.
          </ParamField>
        </Expandable>
      </Expandable>
    </ParamField>

<ParamField path="tool_call_id" type="string">
      Must match the <code>id</code> of a prior <code>assistant</code> message’s <code>tool\_calls\[i]</code> entry. Only valid when <code>role</code> is <code>tool</code>.
    </ParamField>

<ParamField path="usage_metadata" type="object">
      Use this field to send token counts and/or costs with your model's output. See [this guide](/langsmith/log-llm-trace#provide-token-and-cost-information) for more details.
    </ParamField>
  </ParamField>
</Expandable>

## Converting custom I/O formats into LangSmith compatible formats

If you're using a custom input or output format, you can convert it to a LangSmith compatible format using `process_inputs`/`processInputs` and `process_outputs`/`processOutputs` functions on the [`@traceable` decorator](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) (Python) or [`traceable` function](https://docs.smith.langchain.com/reference/js/functions/traceable.traceable) (TS).

`process_inputs`/`processInputs` and `process_outputs`/`processOutputs` accept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace's inputs and outputs, and can return a new dictionary with the processed data.

Here's a boilerplate example of how to use `process_inputs` and `process_outputs` to convert a custom I/O format into a LangSmith compatible format:

<Expandable title="the code">
  <CodeGroup>
    
  </CodeGroup>
</Expandable>

## Identifying a custom model in traces

When using a custom model, it is recommended to also provide the following `metadata` fields to identify the model when viewing traces and when filtering.

* `ls_provider`: The provider of the model, eg "openai", "anthropic", etc.
* `ls_model_name`: The name of the model, eg "gpt-4o-mini", "claude-3-opus-20240229", etc.

This code will log the following trace:

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=f152f49a6313d98e29d3a7b42b76c11f" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="1169" width="1169" data-og-height="548" height="548" data-path="langsmith/images/chat-model-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=280&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=083affd641c8eb41b0fcce26c8485076 280w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=560&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=8ff2ced008bb1be8db587c40cc4a6cd8 560w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=840&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=8891eae04f01247ec86c6e6b3de7a9cb 840w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=1100&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=3287ed0315422c879ff151bf2561e199 1100w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=1650&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=f9995381307324951553d7cfe8d00cdd 1650w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-light.png?w=2500&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=4fd10d74ce1253b1b3da84e49a439e33 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=1da2f0a1adc972aa6de6df94cbfc1407" alt="LangSmith UI showing an LLM call trace called ChatOpenAI with a system and human input followed by an AI Output." data-og-width="1168" width="1168" data-og-height="563" height="563" data-path="langsmith/images/chat-model-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=280&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=6662af6b4871f8250ab39659fd594df8 280w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=560&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=c3986c36300cef013831eb0ba951b0fc 560w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=840&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=05d5f21e2509e36e8176fb8ace2c1e79 840w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=1100&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=464d2babbc58e4c42b3e720520af680c 1100w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=1650&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=db6c45b71fa4ff605edc8c7f88404ec2 1650w, https://mintcdn.com/langchain-5e9cc07a/9cRCWDFnPjFk6hYc/langsmith/images/chat-model-dark.png?w=2500&fit=max&auto=format&n=9cRCWDFnPjFk6hYc&q=85&s=cffed55abf3973cb5b908725433e001e 2500w" />
</div>

If you implement a custom streaming chat\_model, you can "reduce" the outputs into the same format as the non-streaming version. This is currently only supported in Python.

<Check>
  If `ls_model_name` is not present in `extra.metadata`, other fields might be used from the `extra.metadata` for estimating token counts. The following fields are used in the order of precedence:

1. `metadata.ls_model_name`
  2. `inputs.model`
  3. `inputs.model_name`
</Check>

To learn more about how to use the `metadata` fields, refer to the [Add metadata and tags](/langsmith/add-metadata-tags) guide.

## Provide token and cost information

LangSmith calculates costs derived from token counts and model prices automatically. Learn about [how to provide tokens and/or costs in a run](/langsmith/cost-tracking#cost-tracking) and [viewing costs in the LangSmith UI](/langsmith/cost-tracking#viewing-costs-in-the-langsmith-ui).

## Time-to-first-token

If you are using `traceable` or one of our SDK wrappers, LangSmith will automatically populate time-to-first-token for streaming LLM runs.
However, if you are using the `RunTree` API directly, you will need to add a `new_token` event to the run tree in order to properly populate time-to-first-token.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-llm-trace.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Converting custom I/O formats into LangSmith compatible formats

If you're using a custom input or output format, you can convert it to a LangSmith compatible format using `process_inputs`/`processInputs` and `process_outputs`/`processOutputs` functions on the [`@traceable` decorator](https://docs.smith.langchain.com/reference/python/run_helpers/langsmith.run_helpers.traceable) (Python) or [`traceable` function](https://docs.smith.langchain.com/reference/js/functions/traceable.traceable) (TS).

`process_inputs`/`processInputs` and `process_outputs`/`processOutputs` accept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace's inputs and outputs, and can return a new dictionary with the processed data.

Here's a boilerplate example of how to use `process_inputs` and `process_outputs` to convert a custom I/O format into a LangSmith compatible format:

<Expandable title="the code">
  <CodeGroup>
```

---

## Log multimodal traces

**URL:** llms-txt#log-multimodal-traces

Source: https://docs.langchain.com/langsmith/log-multimodal-traces

LangSmith supports logging and rendering images as part of traces. This is currently supported for multimodal LLM runs.

In order to log images, use `wrap_openai`/ `wrapOpenAI` in Python or TypeScript respectively and pass an image URL or base64 encoded image as part of the input.

The image will be rendered as part of the trace in the LangSmith UI.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ff41711a0992c77f86cbc9f523e2ae93" alt="Multimodal" data-og-width="1600" width="1600" data-og-height="1216" height="1216" data-path="langsmith/images/multimodal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d1119193b53a405869cbda1d17c88544 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=147037549a28d87622e4c7d4d15ccc2b 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=145003b20883ea39001adaf0eaf45529 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=01cc9ca22bca3312a5d4c1b356e5b8fc 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=047b36000ced498139fa793c1815440a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multimodal.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=f7868994ae8fd23e9239b4b8d77ee4a0 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-multimodal-traces.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Log retriever traces

**URL:** llms-txt#log-retriever-traces

Source: https://docs.langchain.com/langsmith/log-retriever-trace

<Note>
  Nothing will break if you don't log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps.
</Note>

Many LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.

1. Annotate the retriever step with `run_type="retriever"`.

2. Return a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:

* `page_content`: The text of the document.
   * `type`: This should always be "Document".
   * `metadata`: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace.

The following code snippets show how to log a retrieval steps in Python and TypeScript.

The following image shows how a retriever step is rendered in a trace. The contents along with the metadata are displayed with each document.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=786c74f63e4c94d35535aa46ac9f38f4" alt="Retriever trace" data-og-width="1614" width="1614" data-og-height="736" height="736" data-path="langsmith/images/retriever-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c2e2720a208cc2402e869e214609ec21 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=8949f5484519221d159191e9437a862f 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7f376f3ca4575ffc08eb7fc6bdc0db4c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=06ac1c1dfa24dab91376f2d641fe9aa2 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5046d388f4583d270740adfbd1e58539 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/retriever-trace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aa3aae9c839dd00ca450cfeb7b285371 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-retriever-trace.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Log traces to a specific project

**URL:** llms-txt#log-traces-to-a-specific-project

**Contents:**
- Set the destination project statically
- Set the destination project dynamically

Source: https://docs.langchain.com/langsmith/log-traces-to-project

You can change the destination project of your traces both statically through environment variables and dynamically at runtime.

## Set the destination project statically

As mentioned in the [Tracing Concepts](/langsmith/observability-concepts#projects) section, LangSmith uses the concept of a `Project` to group traces. If left unspecified, the project is set to `default`. You can set the `LANGSMITH_PROJECT` environment variable to configure a custom project name for an entire application run. This should be done before executing your application.

<Warning>
  The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
</Warning>

If the project specified does not exist, it will be created automatically when the first trace is ingested.

## Set the destination project dynamically

You can also set the project name at program runtime in various ways, depending on how you are [annotating your code for tracing](/langsmith/annotate-code). This is useful when you want to log traces to different projects within the same application.

<Note>
  Setting the project name dynamically using one of the below methods overrides the project name set by the `LANGSMITH_PROJECT` environment variable.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/log-traces-to-project.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
export LANGSMITH_PROJECT=my-custom-project
```

Example 2 (unknown):
```unknown

```

---

## Log user feedback using the SDK

**URL:** llms-txt#log-user-feedback-using-the-sdk

**Contents:**
- Use [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)

Source: https://docs.langchain.com/langsmith/attach-user-feedback

<Tip>
  **Key concepts**

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
  * [Reference guide on feedback data format](/langsmith/feedback-data-format)
</Tip>

LangSmith makes it easy to attach feedback to traces.
This feedback can come from users, annotators, automated evaluators, etc., and is crucial for monitoring and evaluating applications.

## Use [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback) / [createFeedback()](https://docs.smith.langchain.com/reference/js/classes/client.Client#createfeedback)

Here we'll walk through how to log feedback using the SDK.

<Info>
  **Child runs**
  You can attach user feedback to ANY child run of a trace, not just the trace (root run) itself.
  This is useful for critiquing specific steps of the LLM application, such as the retrieval step or generation step of a RAG pipeline.
</Info>

<Tip>
  **Non-blocking creation (Python only)**
  The Python client will automatically background feedback creation if you pass `trace_id=` to [create\_feedback()](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_feedback).
  This is essential for low-latency environments, where you want to make sure your application isn't blocked on feedback creation.
</Tip>

You can even log feedback for in-progress runs using `create_feedback() / createFeedback()`. See [this guide](/langsmith/access-current-span) for how to get the run ID of an in-progress run.

To learn more about how to filter traces based on various attributes, including user feedback, see [this guide](/langsmith/filter-traces-in-application).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/attach-user-feedback.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## longlived: "7776000"  # 90 days (default is 400 days)

**URL:** llms-txt#longlived:-"7776000"--#-90-days-(default-is-400-days)

---

## Make conversations private

**URL:** llms-txt#make-conversations-private

**Contents:**
- Prerequisites
- 1. Add resource authorization

Source: https://docs.langchain.com/langsmith/resource-auth

In this tutorial, you will extend [the chatbot created in the last tutorial](/langsmith/set-up-custom-auth) to give each user their own private conversations. You'll add [resource-level access control](/langsmith/auth#single-owner-resources) so users can only see their own threads.

<img src="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=8daa07dd8efb13d7f9d7aa35117b2138" alt="Authorization flow: after authentication, an authorization handler tags each resource with owner=user id and returns a filter so users only see their own threads." data-og-width="2617" width="2617" data-og-height="1673" height="1673" data-path="langsmith/images/authorization.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=280&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=a617e2e62772c307a7b69a78e627ac40 280w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=560&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=c978257553e23b1cb19348959ed72ffc 560w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=840&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=62370abd0f9ca0093d252fd9c1f7cda8 840w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=1100&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=6388bdfb9d7a61105c32683b8b750db1 1100w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=1650&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=95ff1d87241c5b94fd38483350159abb 1650w, https://mintcdn.com/langchain-5e9cc07a/IMK8wJkjSpMCGODD/langsmith/images/authorization.png?w=2500&fit=max&auto=format&n=IMK8wJkjSpMCGODD&q=85&s=bd94714727ddb6d79a49109d4ca7014f 2500w" />

Before you start this tutorial, ensure you have the [bot from the first tutorial](/langsmith/set-up-custom-auth) running without errors.

## 1. Add resource authorization

Recall that in the last tutorial, the [`Auth`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) object lets you register an [authentication function](/langsmith/auth#authentication), which LangSmith uses to validate the bearer tokens in incoming requests. Now you'll use it to register an **authorization** handler.

Authorization handlers are functions that run **after** authentication succeeds. These handlers can add [metadata](/langsmith/auth#filter-operations) to resources (like who owns them) and filter what each user can see.

Update your `src/security/auth.py` and add one authorization handler to run on every request:

```python {highlight={29-39}} title="src/security/auth.py" theme={null}
from langgraph_sdk import Auth

---

## Manage assistants

**URL:** llms-txt#manage-assistants

**Contents:**
- Understand assistant configuration

Source: https://docs.langchain.com/langsmith/configuration-cloud

This page describes how to create, configure, and manage [assistants](/langsmith/assistants). Assistants allow you to customize your [deployed](/langsmith/deployments) graph's behavior through configuration—such as model selection, prompts, and tool availability—without changing the underlying graph code.

You can work with the [SDK](https://reference.langchain.com/python/langsmith/deployment/sdk/) or in the [LangSmith UI](https://smith.langchain.com).

## Understand assistant configuration

Assistants store *context* values that customize graph behavior at runtime. You define a context schema in your graph code, then provide specific context values when creating an assistant via the [`context` parameter](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.AssistantsClient.create).

Consider this example of a `call_model` node that reads the `model_name` from the context:

When you create an assistant, you provide specific values for these configuration fields. The assistant stores this configuration and applies it whenever the graph runs.

For more information on configuration in [LangGraph](/oss/python/langgraph/overview), refer to the [runtime context documentation](/oss/python/langgraph/graph-api#runtime-context).

**Select SDK or UI for your workflow:**

<Tabs>
  <Tab title="SDK">
    ## Create an assistant

Use the [AssistantsClient.create](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.AssistantsClient.create) method to create a new assistant. This method requires:

* **Graph ID**: The name of the deployed graph this assistant will use (e.g., `"agent"`).
    * **Context**: Configuration values matching your graph's context schema.
    * **Name**: A descriptive name for the assistant.

The following example creates an assistant with `model_name` set to `openai`:

The API returns an assistant object containing:

* `assistant_id`: A UUID that uniquely identifies this assistant
    * `graph_id`: The graph this assistant is configured for
    * `context`: The configuration values you provided
    * `name`, `metadata`, timestamps, and other fields

The `assistant_id` (a UUID like `"62e209ca-9154-432a-b9e9-2d75c7a9219b"`) uniquely identifies this assistant configuration. You'll use this ID when running your graph to specify which configuration to apply.

<Note>
      **Graph ID vs Assistant ID**

When creating an assistant, you specify a **graph ID** (graph name like `"agent"`). This returns an **assistant ID** (UUID like `"62e209ca..."`). You can use either when running your graph:

* **Graph ID** (e.g., `"agent"`): Uses the default assistant for that graph
      * **Assistant ID** (UUID): Uses the specific assistant configuration

See [Use an assistant](#use-an-assistant) for examples.
    </Note>

To use an assistant, pass its `assistant_id` when creating a run. The example below uses the assistant we created above:

The stream returns events as the graph executes with your assistant's configuration:

<Note>
      **Using graph ID vs assistant ID**

You can pass either a **graph ID** or **assistant ID** when running your graph:

## Create a new version for your assistant

Use the [AssistantsClient.update](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.AssistantsClient.update) method to create a new version of an assistant.

<Warning>
      **Updates require full configuration**

You must provide the **entire** configuration when updating. The update endpoint creates new versions from scratch and does not merge with previous versions. Include all configuration fields you want to retain.
    </Warning>

For example, to add a system prompt to the assistant:

The update creates a new version and automatically sets it as active. All future runs using this assistant ID will use the new configuration.

## Use a previous assistant version

Use the `setLatest` method to change which version is active:

After changing the active version, all runs using this assistant ID will use the specified version's configuration.
  </Tab>

<Tab title="UI">
    ## Create an assistant

You can create assistants from the [LangSmith UI](https://smith.langchain.com):

1. Navigate to your deployment and select the **Assistants** tab.
    2. Click **+ New assistant**.
    3. In the form that opens:
       * Select the graph this assistant is for.
       * Provide a name and description.
       * Configure the assistant using the configuration schema for that graph.
    4. Click **Create assistant**.

This will take you to [Studio](/langsmith/studio) where you can test the assistant. Return to the **Assistants** tab to see your newly created assistant in the table.

To use an assistant in the LangSmith UI:

1. Navigate to your deployment and select the **Assistants** tab.
    2. Find the assistant you want to use.
    3. Click **Studio** for that assistant.

This opens [Studio](/langsmith/studio) with the selected assistant. When you submit an input (in **Graph** or **Chat** mode), the assistant's configuration will be applied to the run.

## Create a new version for your assistant

To update an assistant and create a new version from the UI, you can use either the Assistants tab or Studio. Either method creates a new version and sets it as the active version:

<Tabs>
      <Tab title="Assistants tab">
        1. Navigate to your deployment and select the **Assistants** tab.
        2. Find the assistant you want to edit.
        3. Click **Edit**.
        4. Modify the assistant's name, description, or configuration.
        5. Save your changes.
      </Tab>

<Tab title="Studio">
        1. Open Studio for the assistant.
        2. Click **Manage Assistants**.
        3. Edit the assistant's configuration.
        4. Save your changes.
      </Tab>
    </Tabs>

## Use a previous assistant version

To set a previous version as active from Studio:

1. Open Studio for the assistant.
    2. Click **Manage Assistants**.
    3. Locate the assistant and select the version you want to use.
    4. Toggle the **Active** switch for that version.

This updates the assistant to use the selected version for all future runs.

<Warning>
      Deleting an assistant will delete **all** of its versions. There is currently no way to delete a single version. To skip a version, simply set a different version as active.
    </Warning>
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configuration-cloud.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

When you create an assistant, you provide specific values for these configuration fields. The assistant stores this configuration and applies it whenever the graph runs.

For more information on configuration in [LangGraph](/oss/python/langgraph/overview), refer to the [runtime context documentation](/oss/python/langgraph/graph-api#runtime-context).

**Select SDK or UI for your workflow:**

<Tabs>
  <Tab title="SDK">
    ## Create an assistant

    Use the [AssistantsClient.create](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.AssistantsClient.create) method to create a new assistant. This method requires:

    * **Graph ID**: The name of the deployed graph this assistant will use (e.g., `"agent"`).
    * **Context**: Configuration values matching your graph's context schema.
    * **Name**: A descriptive name for the assistant.

    The following example creates an assistant with `model_name` set to `openai`:

    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Manage billing in your account

**URL:** llms-txt#manage-billing-in-your-account

**Contents:**
- Set up billing for your account
  - Developer Plan: set up billing on your personal organization
  - Plus Plan: set up billing on a shared organization
- Update your information (Paid plans only)
  - Invoice email
  - Business information and tax ID
- Enforce spend limits
  - Understand your current usage
  - Set limits on usage
  - Other methods of managing traces

Source: https://docs.langchain.com/langsmith/billing

This page describes how to manage billing for your LangSmith organization:

* [Set up billing for your account](#set-up-billing-for-your-account): Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts.
* [Update your information](#update-your-information): Modify invoice email addresses, business information, and tax IDs for your organization.
* [Enforce spend limits](#enforce-spend-limits): Learn how to manage your spend through usage limits and data retention.

## Set up billing for your account

<Note>
  Before using this guide, note the following:

* If you are interested in the [Enterprise](https://www.langchain.com/pricing) plan, please [contact sales](https://www.langchain.com/contact-sales). This guide is only for our self-serve billing plans.
</Note>

To set up billing for your LangSmith organization, navigate to the [Billing and Usage](https://smith.langchain.com/settings/payments) page under **Settings**. Depending on your organization's settings, there are different setup guides:

* [Developer plan](#developer-plan%3A-set-up-billing-on-your-personal-organization)
* [Plus plan](#plus-plan%3A-set-up-billing-on-a-shared-organization)

### Developer Plan: set up billing on your personal organization

Personal organizations are limited to 5,000 traces per month until a credit card is added. To add a card:

1. Click **Add card to remove trace limit**.
2. Add your credit card information.
3. Once complete, you will no longer be rate limited to 5,000 traces, and you will be charged for any excess traces at rates specified on the [pricing](https://www.langchain.com/pricing-langsmith) page.

### Plus Plan: set up billing on a shared organization

Team organizations are given an initial 10,000 traces per month. Any excess traces will be charged at rates specified on the [pricing](https://www.langchain.com/pricing-langsmith) page.

<Note>
  New organizations that you manually create are required to be on the Plus Plan. If you see a message about needing to upgrade to Plus to use this organization, follow these steps.
</Note>

1. Click **Upgrade to Plus**.
2. Invite members to your organization, as desired.
3. Enter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the **This is a business** checkbox and enter the information accordingly. For more information, refer to the [Update your information section](#update-your-information).

## Update your information (Paid plans only)

To update business information for your LangSmith organization, head to the [Billing and Usage](https://smith.langchain.com/settings/payments) page under **Settings**.

To update the email address for invoices, follow these steps:

1. Navigate to the **Plans and Billing** tab.
2. Locate the section beneath the payment method, where the current invoice email is displayed.
3. Enter the new email address for invoices in the provided field.
4. The new email address will be automatically saved.

You will receive all future invoices to the updated email address.

### Business information and tax ID

<Note>
  In certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption.
</Note>

To update your organization's business information, follow these steps:

1. Navigate to the **Plans and Billing** tab.
2. Below the invoice email section, you will find a checkbox labeled **Business**.
3. Check the **Business** checkbox if your organization belongs to a business.
4. A business information section will appear, allowing you to enter or update the following details:
   * Business Name
   * Address
   * Tax ID for applicable jurisdictions
5. A Tax ID field will appear for applicable jurisdictions after you select a country.
6. After entering the necessary information, click the **Save** button to save your changes.

This ensures that your business information is up-to-date and accurate for billing and tax purposes.

## Enforce spend limits

<Check>
  You may find it helpful to read the following pages, before continuing with this section on optimizing your tracing spend:

* [Data Retention Conceptual Docs](/langsmith/administration-overview#data-retention)
  * [Usage Limiting Conceptual Docs](/langsmith/administration-overview#usage-limits)
</Check>

<Note>
  Some of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, contact your sales rep or support via [support.langchain.com](https://support.langchain.com).
</Note>

### Understand your current usage

The first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: [Usage graph](#usage-graph) and [Invoices](#invoices).

LangSmith Usage is measured per workspace, because workspaces often represent development environments (as in the example), or teams within an organization.

The usage graph lets you examine how much of each usage-based pricing metric you have consumed. It does not directly show spend (which you will review later in the draft invoice).

Navigate to the usage graph under **Settings** -> **Billing and Usage** -> **Usage Graph**.

There are several usage metrics that LangSmith charges for:

* LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith.
* LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention.
* LangSmith Deployment Agent Runs: tracks end-to-end invocations of deployed LangGraph agents.

For more details on traces, refer to the [data retention conceptual docs](/langsmith/administration-overview#data-retention). For more details on Agent Runs, refer to [LangSmith Deployment billing](#langsmith-deployment-billing).

To understand how your usage translates to spend, navigate to the **Invoices** tab. The first invoice that will appear on screen is a draft of your current month's invoice, which shows your running spend thus far this month.

<Note>
  LangSmith's Usage Graph and Invoice use the term `tenant_id` to refer to a workspace ID. They are interchangeable.
</Note>

### Set limits on usage

<img src="https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=27addecc92b87dd4131683fb8500f96c" alt="P2usagelimitsempty v2" data-og-width="2598" width="2598" data-og-height="1582" height="1582" data-path="langsmith/images/p2usagelimitsempty-v2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=280&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=1b544ddf00fd88a470ec19db0283a9ec 280w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=560&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=91a0e4559da14d35d8644e20ff2210ec 560w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=840&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=0f77081378606e6bb4061e4c7b694381 840w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=1100&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=e5ea7731ac5e71276ed58f8f8df03d18 1100w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=1650&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=f36ec6fc852a08f1d83bb8e0a0cc0fa8 1650w, https://mintcdn.com/langchain-5e9cc07a/-XAfdD9knKGGfZBx/langsmith/images/p2usagelimitsempty-v2.png?w=2500&fit=max&auto=format&n=-XAfdD9knKGGfZBx&q=85&s=88f9537239e3a5250b056070d3b57a93 2500w" />

#### Set spend limit for workspace

1. To set limits, navigate to **Settings** -> **Billing and Usage** -> **Usage limits**.
2. Input a spend limit for your selected workspace. LangSmith will determine an appropriate number of base and extended trace limits to match that spend. The trace limits include the free trace allocation that comes with your plan (see details on [pricing page](https://smith.langchain.com/settings/payments)).

<Note>
  For organizations with **multiple workspaces only**: For simplicity, LangSmith incorporates the free traces into the cost calculation of the **first workspace only**. In actuality, the free traces can be "consumed" by any workspace. Therefore, although workspace-level spend limits are approximate for multi-workspace organizations, the organization-level spend limit is absolute.
</Note>

#### Configure trace tier distrubution

LangSmith has two trace tiers: base traces and extended traces. Base traces have the base retention and are short-lived (14 days), while extended traces have extended retention and are long-lived (400 days). For more information, refer to the [data retention conceptual docs](/langsmith/administration-overview#data-retention).

Set the desired default trace tier by selecting an option below the **Default data retention** label. All traces will have this tier by default when they are registered. Note that because extended traces cost more than base traces, selecting **Extended** as your default data retention option will result in less overall traces allowed in the billing period. By default, updating this setting will only apply to future incoming traces. To apply to all existing traces in the workspace, select the checkbox.

If the default data retention is set to **Base** you can optionally use the slider to distribute trace limits across base and extended tracess. LangSmith automatically provides a suggestion for this distribution but you can tailor this to your needs. For example, if you are running lots of automations or other features that may upgrade a trace to extended, you may want to increase your extended trace limits. To see the complete list of features that may upgrade a trace, [see here](https://docs.langchain.com/langsmith/administration-overview#how-it-works:~:text=Data%20retention%20auto%2Dupgrades).

<Note>
  The extended data retention limit can cause features other than tracing to stop working once reached. If you plan to use this feature, read more about its [functionality and side effects](/langsmith/administration-overview#side-effects-of-extended-data-retention-traces-limit).
</Note>

### Other methods of managing traces

#### Change project-level default retention

Data retention settings are adjustable per tracing project.

Navigate to **Projects** > ***Your project name*** > Select **Retention** and select the desired default retention. This will only affect retention (and pricing) for **traces going forward**.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=394b513df5ef31d0309f5f3c78bd315a" alt="P1projectretention" data-og-width="1358" width="1358" data-og-height="452" height="452" data-path="langsmith/images/p1projectretention.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0ebc83ac05d14858da153e707bd02f6b 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=0baaf55ed3a4719c2b3c3545778e1ded 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=30989a6636683bca7e456ea1897c2986 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=ee7c87a83e1a15bc4d843bae3ad32811 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7947cbde975873a53d197019453485e5 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/p1projectretention.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=53e1f6edb92789ee5034d5f7f1153af6 2500w" />

#### Apply extended data retention to a percentage of traces

You may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an [automation rule](/langsmith/rules). You might want to apply extended data retention to specific types of traces, such as:

* 10% of all traces: For general analysis or analyzing trends long term.
* Errored traces: To investigate and debug issues thoroughly.
* Traces with specific metadata: For long-term examination of particular features or user flows.

1. Navigate to **Projects** > ***Your project name*** > Select **+ New** > Select **New Automation**.
2. Name your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to [filtering techniques](/langsmith/filter-traces-in-application#filter-operators).

<Note>
  When an automation rule matches any [run](/langsmith/observability-concepts#runs) within a [trace](/langsmith/observability-concepts#traces), then all runs within the trace are upgraded to be retained for 400 days.
</Note>

For example, this is the expected configuration to keep 10% of all traces for extended data retention:

<img src="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=09bbdf5ef7cf3a5a99d6bf0a704e2143" alt="P2sampletraces" data-og-width="640" width="640" data-og-height="610" height="610" data-path="langsmith/images/P2SampleTraces.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=b40ff90a0e3ef5f52ecda25965ec1daa 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4381f3239c4d5c71c8ccd9dcdb4fe859 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=81da02c26dcc067fbbc4a4d7de318dee 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=5f703039c242ba31d192fdb8f4742362 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=e293a6cca1f7da6c8ca62b64e249f6b2 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/P2SampleTraces.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=558b6d18d598241a2f81e8327b13ce3d 2500w" />

If you want to keep a subset of traces for **longer than 400 days** for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.

### LangSmith Deployment billing

In addition to traces, LangSmith charges for deployed agents via LangSmith Deployment (formerly LangGraph Platform).

* **Agent Runs**: An Agent Run is one end-to-end invocation of a deployed LangGraph agent and is billed at \$0.005 each. Nodes and subgraphs within a single agent execution are not charged separately. Calls to other LangGraph agents are charged separately to the deployment hosting the called agent. When using human-in-the-loop with interrupts, resuming after an interrupt creates a separate Agent Run.
* **Deployment Uptime**: You are also charged for the time your deployment's database is live and persisting state. See the [pricing page](https://www.langchain.com/pricing) for uptime costs by deployment type (Development vs Production).

For high-volume deployment usage, please [contact our sales team](https://www.langchain.com/contact-sales) to discuss custom pricing options.

If you have questions about further managing your spend, please contact support via [support.langchain.com](https://support.langchain.com).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/billing.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Manage datasets

**URL:** llms-txt#manage-datasets

**Contents:**
- Version a dataset
  - Create a new version of a dataset
  - Tag a version

Source: https://docs.langchain.com/langsmith/manage-datasets

LangSmith provides tools for managing and working with your [*datasets*](/langsmith/evaluation-concepts#datasets). This page describes dataset operations including:

* [Versioning datasets](#version-a-dataset) to track changes over time.
* [Filtering](#evaluate-on-a-filtered-view-of-a-dataset) and [splitting](#evaluate-on-a-dataset-split) datasets for evaluation.
* [Sharing datasets](#share-a-dataset) publicly.
* [Exporting datasets](#export-a-dataset) in various formats.

You'll also learn how to [export filtered traces](#export-filtered-traces-from-experiment-to-dataset) from [experiments](/langsmith/evaluation-concepts#experiment) back to datasets for further analysis and iteration.

In LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.

### Create a new version of a dataset

Any time you add, update, or delete examples in your dataset, a new [version](/langsmith/evaluation-concepts#versions) of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved.

By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the **Examples** tab, you will find the state of the dataset at that point in time.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=da312a60576f449797be71e24229ea31" alt="Version Datasets" data-og-width="2544" width="2544" data-og-height="1241" height="1241" data-path="langsmith/images/version-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4ab823875dfefd0578cf98b5e7722c59 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bbf9ef95b76653348454895c30475c88 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4f169ff2abd8401334aea31668389cff 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=71a528f7c942c60acf5aeff46a0abc56 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b36ec02adba0f4df157298892b4eddb0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=44b47e051fd375b396203b5912f5be13 2500w" />

Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.

<Note>
  By default, the latest version of the dataset is shown in the **Examples** tab and experiments from all versions are shown in the **Tests** tab.
</Note>

In the **Tests** tab, you will find the results of tests run on the dataset at different versions.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=42c5ac800ef7282fa65013f6de02e45a" alt="Version Datasets" data-og-width="2483" width="2483" data-og-height="963" height="963" data-path="langsmith/images/version-dataset-tests.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d4a0e67f595dce4ae3769d3ecf01705c 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a7743a10620ad86b18cf84dcba7943c8 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=146ca947da81dbbdd8f88067d1630874 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=066ce46f2b51a55c28f9c43430b7db8b 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3c970aa22e36c5d8a5cc17ee1398b95a 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/version-dataset-tests.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2eab0c66d507ab2f4f35f07b774aec19 2500w" />

You can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your dataset's history.

For example, you might tag a version of your dataset as "prod" and use it to run tests against your LLM pipeline.

You can tag a version of your dataset in the UI by clicking on **+ Tag this version** in the **Examples** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c3f7a97c92eb645f7b0888f4e35ffd48" alt="Tagging Datasets" data-og-width="662" width="662" data-og-height="124" height="124" data-path="langsmith/images/tag-this-version.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=93c44c3c1243429e10fea238ca078d37 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=99d4bd65c4b30f29c75443846c785b0a 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=34c4a8a96a8024cdebcad7815825d0a0 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=af3c62be520a88604634349c8a5884db 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c9a00d92647ebd171976304ad4def786 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tag-this-version.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=21c96cdd5609bb4bfb5148ac7dd8e098 2500w" />

You can also tag versions of your dataset using the SDK. Here's an example of how to tag a version of a dataset using the [Python SDK](https://docs.smith.langchain.com/reference/python/reference):

```python  theme={null}
from langsmith import Client
from datetime import datetime

client = Client()
initial_time = datetime(2024, 1, 1, 0, 0, 0) # The timestamp of the version you want to tag

---

## Manage prompts

**URL:** llms-txt#manage-prompts

**Contents:**
- Commit tags
  - Create a tag
  - Move a tag
  - Delete a tag
  - Use tags in code

Source: https://docs.langchain.com/langsmith/manage-prompts

LangSmith provides several tools to help you manage your [*prompts*](/langsmith/prompt-engineering-concepts) effectively. This page describes the following features:

* [Commit tags](#commit-tags) for version control and environment management.
* [Webhook triggers](#trigger-a-webhook-on-prompt-commit) for automating workflows when prompts are updated.
* [Public prompt hub](#public-prompt-hub) for discovering and using community-created prompts.

[*Commit tags*](/langsmith/prompt-engineering-concepts#tags) are labels that reference a specific [*commit*](/langsmith/prompt-engineering-concepts#commits) in your prompt's version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can update which version is being used without modifying the code itself.

Each tag references exactly one commit, though you can reassign a tag to point to a different commit.

<Note>
  **Not to be confused with resource tags**: Commit tags are specific to prompt versioning and reference individual commits in a prompt's history. [Resource tags](/langsmith/set-up-resource-tags) are key-value pairs used to organize workspace resources like projects, datasets, and prompts. While both can use similar naming conventions (like `prod` or `staging`), commit tags control **which version** of a prompt runs, while resource tags help you **organize and filter** resources across your workspace.
</Note>

To create a tag, navigate to the **Commits** tab for a prompt. Click on the tag icon next to the commit you want to tag. Click **New Tag** and enter a name for the tag.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b0cb7961f70d5c0bab9af960041bc54f" alt="Commits tab" data-og-width="2868" width="2868" data-og-height="992" height="992" data-path="langsmith/images/commits-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5786acfc0582c73c73efb5535a954ab4 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=410aa69f0ef5cc13ff7651df331495b7 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=b9845d256b71636a9bb6ee60c774a29e 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e16686b15bf2a95709a5d963ffc339d0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cb9560e6aa774505cb9ff1857f95ae61 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commits-tab.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=bef88e11fa0de6173fc4dfd9c9339f39 2500w" /> <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cc27df9c5392a9b71319969c14924c61" alt="Create new prompt tag" data-og-width="1410" width="1410" data-og-height="872" height="872" data-path="langsmith/images/create-new-prompt-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ed473769995e6148b60aec9cdd652ab6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1f817be111dfc54af9717f9cdb664752 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fb0b979bf3362bd8faf796e4d617ac1c 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=22c63285bd22dd0c82d53b8d46c6638b 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1b4d05163bd948c4bce8ea08b5ceb70f 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-new-prompt-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c34b5ce29b2f1914399b6a084885e09a 2500w" />

To point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3cb3c6218961cbdd8f6f1fb6d06b50e3" alt="Move prompt tag" data-og-width="874" width="874" data-og-height="694" height="694" data-path="langsmith/images/move-prompt-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c2094e7486cfd3a7f4d7979e7883d1bc 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=163481cc4580e4fe7934a5e02497fb9f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9d53ef06dc6d69cc4024821fe8cfc32d 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=00ce6c08638961bfbbe5713c0066929b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fa898ee3d7089b443e741d4f10b7845a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/move-prompt-tag.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cd97d2004b3b0a2ac0d3c7e4d3e7fff7 2500w" />

To delete a tag, click on the delete icon next to the tag you want to delete. This will delete the tag altogether and it will no longer be associated with any commit.

Tags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags that can be updated without changing your code.

Here is an example of pulling a prompt by tag in Python:

```python  theme={null}
prompt = client.pull_prompt("joke-generator:prod")

---

## Manage prompts programmatically

**URL:** llms-txt#manage-prompts-programmatically

**Contents:**
- Install packages
- Configure environment variables
- Push a prompt
- Pull a prompt
- Use a prompt without LangChain
  - OpenAI
  - Anthropic
- List, delete, and like prompts

Source: https://docs.langchain.com/langsmith/manage-prompts-programmatically

You can use the LangSmith Python and TypeScript SDK to manage prompts programmatically.

<Note>
  Previously this functionality lived in the `langchainhub` package which is now deprecated. All functionality going forward will live in the `langsmith` package.
</Note>

In Python, you can directly use the LangSmith SDK (*recommended, full functionality*) or you can use through the LangChain package (limited to pushing and pulling prompts).

In TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.

## Configure environment variables

If you already have `LANGSMITH_API_KEY` set to your current workspace's api key from LangSmith, you can skip this step.

Otherwise, get an API key for your workspace by navigating to `Settings > API Keys > Create API Key` in LangSmith.

Set your environment variable.

<Note>
  What we refer to as "prompts" used to be called "repos", so any references to "repo" in the code are referring to a prompt.
</Note>

To create a new prompt or update an existing prompt, you can use the `push prompt` method.

You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground. (see settings here: [Supported Providers](https://langsmith.com/playground))

To pull a prompt, you can use the `pull prompt` method, which returns a the prompt as a langchain `PromptTemplate`.

To pull a **private prompt** you do not need to specify the owner handle (though you can, if you have one set).

To pull a **public prompt** from the LangChain Hub, you need to specify the handle of the prompt's author.

Similar to pushing a prompt, you can also pull a prompt as a RunnableSequence of a prompt and a model. Just specify include\_model when pulling the prompt. If the stored prompt includes a model, it will be returned as a RunnableSequence. Make sure you have the proper environment variables set for the model you are using.

When pulling a prompt, you can also specify a specific commit hash or [commit tag](/langsmith/manage-prompts#commit-tags) to pull a specific version of the prompt.

To pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt's author.

<Note>
  For pulling prompts, if you are using Node.js or an environment that supports dynamic imports, we recommend using the `langchain/hub/node` entrypoint, as it handles deserialization of models associated with your prompt configuration automatically.

If you are in a non-Node environment, "includeModel" is not supported for non-OpenAI models and you should use the base `langchain/hub` entrypoint.
</Note>

## Use a prompt without LangChain

If you want to store your prompts in LangSmith but use them directly with a model provider's API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API.

These conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:

## List, delete, and like prompts

You can also list, delete, and like/unlike prompts using the `list prompts`, `delete prompt`, `like prompt` and `unlike prompt` methods. See the [LangSmith SDK client](https://github.com/langchain-ai/langsmith-sdk) for extensive documentation on these methods.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-prompts-programmatically.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Configure environment variables

If you already have `LANGSMITH_API_KEY` set to your current workspace's api key from LangSmith, you can skip this step.

Otherwise, get an API key for your workspace by navigating to `Settings > API Keys > Create API Key` in LangSmith.

Set your environment variable.
```

Example 4 (unknown):
```unknown
<Note>
  What we refer to as "prompts" used to be called "repos", so any references to "repo" in the code are referring to a prompt.
</Note>

## Push a prompt

To create a new prompt or update an existing prompt, you can use the `push prompt` method.

<CodeGroup>
```

---

## Manage your organization using the API

**URL:** llms-txt#manage-your-organization-using-the-api

**Contents:**
- Workspaces
- User management
  - RBAC
  - Membership management
- API keys
- Security settings
- User-only endpoints
- Sample code

Source: https://docs.langchain.com/langsmith/manage-organization-by-api

LangSmith's API supports programmatic access via API key to all of the actions available in the UI, with only a few exceptions that are noted in [User-only endpoints](#user-only-endpoints).

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on organizations and workspaces](/langsmith/administration-overview)
  * [Organization setup how-to guild](/langsmith/set-up-a-workspace#set-up-an-organization)
</Check>

<Note>
  There are a few limitations that will be lifted soon:

* The LangSmith SDKs do not support these organization management actions yet.
  * Organization-scoped [service keys](/langsmith/administration-overview#service-keys) with Organization Admin permission may be used for these actions.
</Note>

<Warning>
  Use the `X-Tenant-Id` header to specify which workspace to target. If the header is not present, operations will default to the workspace the API key was initially created in if it is not organization-scoped.

**If `X-Tenant-Id` is not specified when accessing workspace-scoped resources with an organization-scoped API key, the request will fail with `403 Forbidden`.**
</Warning>

Some commonly-used endpoints and use cases are listed below. For a complete list of available endpoints, see the [API docs](https://api.smith.langchain.com/redoc). **The `X-Organization-Id` header should be present on all requests, and `X-Tenant-Id` header should be present on requests that are scoped to a particular workspace.**

* [List workspaces](https://api.smith.langchain.com/redoc#tag/workspaces/operation/list_workspaces_api_v1_workspaces_get)
* [Create workspace](https://api.smith.langchain.com/redoc#tag/workspaces/operation/create_workspace_api_v1_workspaces_post)
* [Update workspace name](https://api.smith.langchain.com/redoc#tag/workspaces/operation/patch_workspace_api_v1_workspaces__workspace_id__patch)

* [List roles](https://api.smith.langchain.com/redoc#tag/orgs/operation/list_organization_roles_api_v1_orgs_current_roles_get)
* [List permissions](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch)
* [Create role](https://api.smith.langchain.com/redoc#tag/orgs/operation/create_organization_roles_api_v1_orgs_current_roles_post)
* [Update role](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_organization_roles_api_v1_orgs_current_roles__role_id__patch)

### Membership management

`List roles` under [RBAC](#rbac) should be used for retrieving role IDs of these operations. `List [organization|workspace] members` endpoints (below) response `"id"`s should be used as `identity_id` in these operations.

* [List active organization members](https://api.smith.langchain.com/redoc#tag/orgs/operation/get_current_active_org_members_api_v1_orgs_current_members_active_get)
* [List pending organization members](https://api.smith.langchain.com/redoc#tag/orgs/operation/get_current_pending_org_members_api_v1_orgs_current_members_pending_get)
* [Invite a user to the organization and one or more workspaces](https://api.smith.langchain.com/redoc#tag/orgs/operation/add_members_to_current_org_batch_api_v1_orgs_current_members_batch_post). This should be used when the user is not already a member in the organization.
* [Update a user's organization role](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Remove someone from the organization](https://api.smith.langchain.com/redoc#tag/orgs/operation/remove_member_from_current_org_api_v1_orgs_current_members__identity_id__delete)

* [List workspace members](https://api.smith.langchain.com/redoc#tag/workspaces/operation/get_current_workspace_members_api_v1_workspaces_current_members_get)
* [Add a member to a workspace that is already part of the organization](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Update a user's workspace role](https://api.smith.langchain.com/redoc#tag/workspaces/operation/add_member_to_current_workspace_api_v1_workspaces_current_members_post)
* [Remove someone from a workspace](https://api.smith.langchain.com/redoc#tag/workspaces/operation/delete_current_workspace_member_api_v1_workspaces_current_members__identity_id__delete)

<Note>
  These params should be omitted: `read_only` (deprecated), `password` and `full_name` ([basic auth](/langsmith/authentication-methods) only)
</Note>

* [Create a service key](https://api.smith.langchain.com/redoc#tag/api-key/operation/generate_api_key_api_v1_api_key_post)
* [Delete a service key](https://api.smith.langchain.com/redoc#tag/api-key/operation/delete_api_key_api_v1_api_key__api_key_id__delete)

<Note>
  Organization Admin permissions are required to make these changes.
</Note>

<Note>
  "Shared resources" in this context refer to [public prompts](/langsmith/create-a-prompt#save-your-prompt), [shared runs](/langsmith/share-trace), and [shared datasets](/langsmith/manage-datasets#share-a-dataset).
</Note>

<Warning>
  Updating these settings affects **all resources in the organization**.
</Warning>

You can update these settings under the **Settings > Shared** tab for a workspace, or via API:

* [Update organization sharing settings](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch)
  * use `unshare_all` to unshare **ALL** shared resources in the organization - use `disable_public_sharing` to prevent future sharing of resources

These settings are only editable via API:

* [Disable/enable PAT creation](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch) (for self-hosted, available in Helm chart version 0.11.25+)
  * Use `pat_creation_disabled` to disable PAT creation for the entire organization.
  * See the [admin guide](/langsmith/administration-overview#organization-roles) for information about the Organization Viewer role, which cannot create PATs.

## User-only endpoints

These endpoints are user-scoped and require a logged-in user's JWT, so they should only be executed through the UI.

* `/api-key/current` endpoints: these are related a user's PATs
* `/sso/email-verification/send` (Cloud-only): this endpoint is related to [SAML SSO](/langsmith/user-management)

The sample code below goes through a few common workflows related to organization management. Make sure to make necessary replacements wherever `<replace_me>` is in the code.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-organization-by-api.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Matches the "thread" resource and all actions - create, read, update, delete, search

**URL:** llms-txt#matches-the-"thread"-resource-and-all-actions---create,-read,-update,-delete,-search

---

## maxReplicas: 10

**URL:** llms-txt#maxreplicas:-10

---

## maxReplicas: 160

**URL:** llms-txt#maxreplicas:-160

---

## maxReplicas: 16

**URL:** llms-txt#maxreplicas:-16

---

## maxReplicas: 20

**URL:** llms-txt#maxreplicas:-20

---

## maxReplicas: 40

**URL:** llms-txt#maxreplicas:-40

---

## maxReplicas: 4

**URL:** llms-txt#maxreplicas:-4

---

## maxReplicas: 50

**URL:** llms-txt#maxreplicas:-50

---

## maxReplicas: 5

**URL:** llms-txt#maxreplicas:-5

---

## maxReplicas: 6

**URL:** llms-txt#maxreplicas:-6

---

## "max_input_tokens": 400000,

**URL:** llms-txt#"max_input_tokens":-400000,

---

## MCP Get

**URL:** llms-txt#mcp-get

Source: https://docs.langchain.com/langsmith/agent-server-api/mcp/mcp-get

langsmith/agent-server-openapi.json get /mcp/
Implemented according to the Streamable HTTP Transport specification.

---

## MCP Post

**URL:** llms-txt#mcp-post

Source: https://docs.langchain.com/langsmith/agent-server-api/mcp/mcp-post

langsmith/agent-server-openapi.json post /mcp/
Implemented according to the Streamable HTTP Transport specification.
Sends a JSON-RPC 2.0 message to the server.

- **Request**: Provide an object with `jsonrpc`, `id`, `method`, and optional `params`.
- **Response**: Returns a JSON-RPC response or acknowledgment.

**Notes:**
- Stateless: Sessions are not persisted across requests.

---

## meaning you can use it as you would any other runnable.

**URL:** llms-txt#meaning-you-can-use-it-as-you-would-any-other-runnable.

---

## Memory

**URL:** llms-txt#memory

**Contents:**
- Add short-term memory
  - Use in production
  - Use in subgraphs

Source: https://docs.langchain.com/oss/python/langgraph/add-memory

AI applications need [memory](/oss/python/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory:

* [Add short-term memory](#add-short-term-memory) as a part of your agent's [state](/oss/python/langgraph/graph-api#state) to enable multi-turn conversations.
* [Add long-term memory](#add-long-term-memory) to store user-specific or application-level data across sessions.

## Add short-term memory

**Short-term** memory (thread-level [persistence](/oss/python/langgraph/persistence)) enables agents to track multi-turn conversations. To add short-term memory:

### Use in production

In production, use a checkpointer backed by a database:

<Accordion title="Example: using Postgres checkpointer">

<Tip>
    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer
  </Tip>

<Tabs>
    <Tab title="Sync">
      
    </Tab>

<Tab title="Async">
      
    </Tab>
  </Tabs>
</Accordion>

<Accordion title="Example: using [MongoDB](https://pypi.org/project/langgraph-checkpoint-mongodb/) checkpointer">

<Note>
    **Setup**
    To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow [this guide](https://www.mongodb.com/docs/guides/atlas/cluster/) to create a cluster if you don't already have one.
  </Note>

<Tabs>
    <Tab title="Sync">
      
    </Tab>

<Tab title="Async">
      
    </Tab>
  </Tabs>
</Accordion>

<Accordion title="Example: using Redis checkpointer">

<Tip>
    You need to call `checkpointer.setup()` the first time you're using Redis checkpointer.
  </Tip>

<Tabs>
    <Tab title="Sync">
      
    </Tab>

<Tab title="Async">
      
    </Tab>
  </Tabs>
</Accordion>

If your graph contains [subgraphs](/oss/python/langgraph/use-subgraphs), you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.

```python  theme={null}
from langgraph.graph import START, StateGraph
from langgraph.checkpoint.memory import InMemorySaver
from typing import TypedDict

class State(TypedDict):
    foo: str

**Examples:**

Example 1 (unknown):
```unknown
### Use in production

In production, use a checkpointer backed by a database:
```

Example 2 (unknown):
```unknown
<Accordion title="Example: using Postgres checkpointer">
```

Example 3 (unknown):
```unknown
<Tip>
    You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer
  </Tip>

  <Tabs>
    <Tab title="Sync">
```

Example 4 (unknown):
```unknown
</Tab>

    <Tab title="Async">
```

---

## Messages

**URL:** llms-txt#messages

**Contents:**
- Basic usage

Source: https://docs.langchain.com/oss/python/langchain/messages

Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM.

Messages are objects that contain:

* <Icon icon="user" size={16} /> [**Role**](#message-types) - Identifies the message type (e.g. `system`, `user`)
* <Icon icon="folder-closed" size={16} /> [**Content**](#message-content) - Represents the actual content of the message (like text, images, audio, documents, etc.)
* <Icon icon="tag" size={16} /> [**Metadata**](#message-metadata) - Optional fields such as response information, message IDs, and token usage

LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called.

The simplest way to use messages is to create message objects and pass them to a model when [invoking](/oss/python/langchain/models#invocation).

```python  theme={null}
from langchain.chat_models import init_chat_model
from langchain.messages import HumanMessage, AIMessage, SystemMessage

model = init_chat_model("gpt-5-nano")

system_msg = SystemMessage("You are a helpful assistant.")
human_msg = HumanMessage("Hello, how are you?")

---

## Messages and content

**URL:** llms-txt#messages-and-content

from langchain.messages import AIMessage, HumanMessage

---

## Metadata parameters reference

**URL:** llms-txt#metadata-parameters-reference

**Contents:**
- Basic usage example
- All parameters
  - User-configurable parameters
  - System-generated parameters
  - Experiment parameters
- Parameter details
  - `ls_provider`
  - `ls_model_name`
  - `ls_temperature`
  - `ls_max_tokens`

Source: https://docs.langchain.com/langsmith/ls-metadata-parameters

When you trace LLM calls with LangSmith, you often want to [track costs](/langsmith/cost-tracking), compare model configurations, and analyze performance across different providers. LangSmith's native integrations (like [LangChain](/langsmith/trace-with-langchain) or the [OpenAI](/langsmith/trace-openai)/[Anthropic](/langsmith/trace-anthropic) wrappers) handle this automatically, but custom model wrappers and self-hosted models require a standardized way to provide this information. LangSmith uses `ls_` metadata parameters for this purpose.

These metadata parameters (all prefixed with `ls_`) let you pass model configuration and identification information through the standard `metadata` field. Once set, LangSmith can automatically calculate costs, display model information in the UI, and enable filtering and analytics across your traces.

Use `ls_` metadata parameters to:

* **Enable automatic cost tracking** for custom or self-hosted models by identifying the provider and model name.
* **Track model configuration** like temperature, max tokens, and other parameters for experiment comparison.
* **Filter and analyze traces** by provider or configuration settings
* **Improve debugging** by recording exactly which model settings were used for each run.

## Basic usage example

The most common use case is enabling cost tracking for custom model wrappers. To do this, you need to provide two key pieces of information: the provider name (`ls_provider`) and the model name (`ls_model_name`). These work together to match against LangSmith's pricing database.

This minimal setup tells LangSmith what model you're using, enabling automatic cost calculation if the model exists in the pricing database or if you've [configured custom pricing](/langsmith/cost-tracking#set-up-model-pricing).

For more comprehensive tracking, you can include additional configuration parameters. This is especially useful when [running experiments](/langsmith/evaluation-quickstart) or comparing different model settings:

With this setup, you can later filter traces by temperature, compare runs with different max token settings, or analyze which configuration parameters produce the best results. All these parameters are optional except for the `ls_provider` and `ls_model_name` pair needed for cost tracking.

### User-configurable parameters

| Parameter                                       | Type       | Required | Description                         |
| ----------------------------------------------- | ---------- | -------- | ----------------------------------- |
| [`ls_provider`](#ls-provider)                   | `string`   | Yes\*    | LLM provider name for cost tracking |
| [`ls_model_name`](#ls-model-name)               | `string`   | Yes\*    | Model identifier for cost tracking  |
| [`ls_temperature`](#ls-temperature)             | `number`   | No       | Temperature parameter used          |
| [`ls_max_tokens`](#ls-max-tokens)               | `number`   | No       | Maximum tokens parameter used       |
| [`ls_stop`](#ls-stop)                           | `string[]` | No       | Stop sequences used                 |
| [`ls_invocation_params`](#ls-invocation-params) | `object`   | No       | Additional invocation parameters    |

\* `ls_provider` and `ls_model_name` must be provided together for cost tracking

### System-generated parameters

| Parameter                       | Type      | Description                                                            |
| ------------------------------- | --------- | ---------------------------------------------------------------------- |
| [`ls_run_depth`](#ls-run-depth) | `integer` | Depth in trace tree (0=root, 1=child, etc.) - automatically calculated |
| [`ls_method`](#ls-method)       | `string`  | Tracing method used (e.g., "traceable") - set by SDK                   |

### Experiment parameters

| Parameter                               | Type            | Description                                                             |
| --------------------------------------- | --------------- | ----------------------------------------------------------------------- |
| [`ls_example_*`](#ls-example-)          | `any`           | Example metadata prefixed with `ls_example_` - added during experiments |
| [`ls_experiment_id`](#ls-experiment-id) | `string` (UUID) | Unique experiment identifier - added during experiments                 |

* **Type:** `string`
* **Required:** Yes (with [`ls_model_name`](#ls-model-name))

**What it does:**
Identifies the LLM provider. Combined with `ls_model_name`, enables automatic cost calculation by matching against [LangSmith's model pricing database](https://smith.langchain.com/settings/workspaces/models).

* `"openai"`
* `"anthropic"`
* `"azure"`
* `"bedrock"`
* `"google_vertexai"`
* `"google_genai"`
* `"fireworks"`
* `"mistral"`
* `"groq"`
* Or, any custom string

**When to use:**
When you want [automatic cost tracking](/langsmith/cost-tracking) for custom model wrappers or self-hosted models.

* **Requires** [`ls_model_name`](#ls-model-name) for cost tracking to work.
* Works with token usage data to calculate costs.

* **Type:** `string`
* **Required:** Yes (with `ls_provider`)

**What it does:**
Identifies the specific model. Combined with `ls_provider`, matches against pricing database for automatic cost calculation.

* OpenAI: `"gpt-4o"`, `"gpt-4o-mini"`, `"gpt-3.5-turbo"`
* Anthropic: `"claude-3-5-sonnet-20241022"`, `"claude-3-opus-20240229"`
* Custom: Any model identifier

**When to use:**
When you want automatic [cost tracking](/langsmith/cost-tracking) and model identification in the [UI](https://smith.langchain.com).

* **Requires** [`ls_provider`](#ls-provider) for cost tracking to work.
* Works with token usage data to calculate costs.

* **Type:** `number` (nullable)
* **Required:** No

**What it does:**
Records the temperature setting used. This is for tracking only—does not affect LangSmith behavior.

**When to use:**
When you want to track model configuration for experiments or debugging.

* Independent; just for tracking.
* Useful alongside other config parameters for experiment comparison.

* **Type:** `number` (nullable)
* **Required:** No

**What it does:**
Records the maximum tokens setting used. This is for tracking only—does not affect LangSmith behavior.

**When to use:**
When you want to track model configuration for experiments or debugging.

* Independent; just for tracking.
* Useful for cost analysis when combined with actual token usage.

* **Type:** `string[]` (nullable)
* **Required:** No

**What it does:**
Records stop sequences used. This is for tracking only—does not affect LangSmith behavior.

**When to use:**
When you want to track model configuration for experiments or debugging.

* Independent; just for tracking.

### `ls_invocation_params`

* **Type:** `object` (any key-value pairs)
* **Required:** No

**What it does:**
Stores additional model parameters that don't fit the specific `ls_` parameters. Can include provider-specific settings.

**Common parameters:**
`top_p`, `frequency_penalty`, `presence_penalty`, `top_k`, `seed`, or any custom parameters

**When to use:**
When you need to track additional configuration beyond the standard parameters.

* Independent; stores arbitrary configuration.

* **Type:** `integer`
* **Set by:** LangSmith backend (automatic)
* **Cannot be overridden**

**What it does:**
Indicates depth in the trace tree:

* `0` = Root run (top-level)
* `1` = Direct child
* `2` = Grandchild
* etc.

**When it's used:**
Automatically calculated during trace ingestion. Used for filtering (e.g., "show only root runs") and UI visualization.

* Determined by trace parent-child structure.
* Cannot be set manually.

* **Type:** `string`
* **Set by:** SDK (automatic)

**What it does:**
Indicates which SDK method created the trace (commonly `"traceable"` for `@traceable` decorator).

**When it's used:**
Automatically set by the tracing SDK. Used for debugging and analytics.

* Set by SDK based on how trace was created.
* Cannot be set manually.

* **Type:** Any (depends on example metadata)
* **Pattern:** `ls_example_{original_key}`
* **Set by:** LangSmith experiments system (automatic)

**What it does:**
When running [experiments on datasets](/langsmith/evaluation-quickstart), metadata from the example is automatically prefixed with `ls_example_` and added to the trace.

**Special parameter:**

* `ls_example_dataset_split`: Dataset split (e.g., "train", "test", "validation")

**When it's used:**
During dataset experiments. Allows filtering/grouping by example characteristics.

**Example:**
If example has metadata `{"category": "technical", "difficulty": "hard"}`, trace gets:

* Automatically derived from example metadata.
* Cannot be set manually on traces.

### `ls_experiment_id`

* **Type:** `string` (UUID)
* **Set by:** LangSmith experiments system (automatic)

**What it does:**
Unique identifier for an experiment run.

**When it's used:**
Automatically added when running [experiments/evaluations on datasets](/langsmith/evaluation-quickstart). Used to group all runs from the same experiment.

* Links runs to specific experiments.
* Cannot be set manually.

## Parameter relationships

### Cost tracking dependencies

For LangSmith to automatically calculate costs, several parameters must work together. Here's what's required:

**Primary requirement:** [`ls_provider`](#ls-provider) + [`ls_model_name`](#ls-model-name)

* Both should be present for automatic cost calculation.
* If [`ls_model_name`](#ls-model-name) is missing, system will fall back to checking [`ls_invocation_params`](#ls-invocation-params) for model name.
* [`ls_provider`](#ls-provider) must match a provider in the [pricing database](https://smith.langchain.com/settings/workspaces/models) (or use custom pricing).

**Additional requirements:**

* Run must have `run_type="llm"` (or [arbitrary cost tracking](/langsmith/cost-tracking#tracking-costs-for-arbitrary-runs) must be enabled).
* [Token usage data](/langsmith/log-llm-trace#provide-token-and-cost-information) must be present in the trace (prompt\_tokens, completion\_tokens).
* Model must exist in pricing database or have [custom pricing configured](/langsmith/cost-tracking#set-up-model-pricing).

**Fallback behavior:**
If [`ls_model_name`](#ls-model-name) is not in metadata, the system checks [`ls_invocation_params`](#ls-invocation-params) for model identifiers like `"model"` before giving up on cost tracking.

### Configuration tracking group

These parameters help you track model settings but don't affect LangSmith's core functionality:

**Optional, work independently:** [`ls_temperature`](#ls-temperature), [`ls_max_tokens`](#ls-max-tokens), [`ls_stop`](#ls-stop)

* These are for tracking/display.
* Do not affect LangSmith behavior or cost calculation.
* Useful for experiment comparison and debugging.

### Invocation params special case

The `ls_invocation_params` parameter has a dual role as both a tracking field and a fallback mechanism:

**[`ls_invocation_params`](#ls-invocation-params)**; partially independent with fallback role:

* Primarily stores arbitrary configuration for tracking.
* **Can serve as fallback** for cost tracking if [`ls_model_name`](#ls-model-name) is missing.
* Does not directly affect cost calculation when [`ls_model_name`](#ls-model-name) is present.

### System parameters

These parameters are automatically generated by LangSmith and cannot be manually set:

**Cannot be user-set:** [`ls_run_depth`](#ls-run-depth), [`ls_method`](#ls-method), [`ls_example_*`](#ls-example-), [`ls_experiment_id`](#ls-experiment-id)

* Automatically set by system.
* Used for filtering, analytics, and system tracking.

## Filter traces by metadata parameters

Once you've added `ls_` metadata parameters to your traces, you can use them to filter and search traces programmatically via the [API](https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post) or interactively in the [LangSmith UI](https://smith.langchain.com). This lets you narrow down traces by model, provider, configuration settings, or trace depth.

Use the [`Client`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client) class with the [`list_runs()`](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.list_runs) method (Python) or [`listRuns()`](https://docs.smith.langchain.com/reference/js/classes/client.Client#listruns) method (TypeScript) to query traces based on metadata values. The [filter syntax](/langsmith/trace-query-syntax) supports equality checks, comparisons, and logical operators.

These examples show common filtering patterns:

* **Filter by provider or model** to analyze usage patterns or costs for specific models
* **Filter by run depth** to get only root traces (depth 0) or child runs at specific nesting levels
* **Filter by configuration** to compare experiments with different temperature, max tokens, or other settings

In the [LangSmith UI](https://smith.langchain.com), use the filter/search bar with the [filter syntax](/langsmith/trace-query-syntax):

* [Cost tracking guide](/langsmith/cost-tracking): Learn how to track and analyze LLM costs in LangSmith.
* [Log LLM traces](/langsmith/log-llm-trace): Format requirements for logging LLM calls with proper token tracking.
* [Trace query syntax](/langsmith/trace-query-syntax): Complete reference for filtering and searching traces.
* [Evaluation quickstart](/langsmith/evaluation-quickstart): Run experiments on datasets to compare model configurations.
* [Add metadata and tags](/langsmith/add-metadata-tags): General guide to adding metadata to traces.
* [Filter traces in application](/langsmith/filter-traces-in-application): Programmatically filter traces in your code.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/ls-metadata-parameters.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

This minimal setup tells LangSmith what model you're using, enabling automatic cost calculation if the model exists in the pricing database or if you've [configured custom pricing](/langsmith/cost-tracking#set-up-model-pricing).

For more comprehensive tracking, you can include additional configuration parameters. This is especially useful when [running experiments](/langsmith/evaluation-quickstart) or comparing different model settings:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

With this setup, you can later filter traces by temperature, compare runs with different max token settings, or analyze which configuration parameters produce the best results. All these parameters are optional except for the `ls_provider` and `ls_model_name` pair needed for cost tracking.

## All parameters

### User-configurable parameters

| Parameter                                       | Type       | Required | Description                         |
| ----------------------------------------------- | ---------- | -------- | ----------------------------------- |
| [`ls_provider`](#ls-provider)                   | `string`   | Yes\*    | LLM provider name for cost tracking |
| [`ls_model_name`](#ls-model-name)               | `string`   | Yes\*    | Model identifier for cost tracking  |
| [`ls_temperature`](#ls-temperature)             | `number`   | No       | Temperature parameter used          |
| [`ls_max_tokens`](#ls-max-tokens)               | `number`   | No       | Maximum tokens parameter used       |
| [`ls_stop`](#ls-stop)                           | `string[]` | No       | Stop sequences used                 |
| [`ls_invocation_params`](#ls-invocation-params) | `object`   | No       | Additional invocation parameters    |

\* `ls_provider` and `ls_model_name` must be provided together for cost tracking

### System-generated parameters

| Parameter                       | Type      | Description                                                            |
| ------------------------------- | --------- | ---------------------------------------------------------------------- |
| [`ls_run_depth`](#ls-run-depth) | `integer` | Depth in trace tree (0=root, 1=child, etc.) - automatically calculated |
| [`ls_method`](#ls-method)       | `string`  | Tracing method used (e.g., "traceable") - set by SDK                   |

### Experiment parameters

| Parameter                               | Type            | Description                                                             |
| --------------------------------------- | --------------- | ----------------------------------------------------------------------- |
| [`ls_example_*`](#ls-example-)          | `any`           | Example metadata prefixed with `ls_example_` - added during experiments |
| [`ls_experiment_id`](#ls-experiment-id) | `string` (UUID) | Unique experiment identifier - added during experiments                 |

## Parameter details

### `ls_provider`

* **Type:** `string`
* **Required:** Yes (with [`ls_model_name`](#ls-model-name))

**What it does:**
Identifies the LLM provider. Combined with `ls_model_name`, enables automatic cost calculation by matching against [LangSmith's model pricing database](https://smith.langchain.com/settings/workspaces/models).

**Common values:**

* `"openai"`
* `"anthropic"`
* `"azure"`
* `"bedrock"`
* `"google_vertexai"`
* `"google_genai"`
* `"fireworks"`
* `"mistral"`
* `"groq"`
* Or, any custom string

**When to use:**
When you want [automatic cost tracking](/langsmith/cost-tracking) for custom model wrappers or self-hosted models.

**Example:**
```

---

## Method 1: Regex pattern string

**URL:** llms-txt#method-1:-regex-pattern-string

agent1 = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        PIIMiddleware(
            "api_key",
            detector=r"sk-[a-zA-Z0-9]{32}",
            strategy="block",
        ),
    ],
)

---

## Method 2: Compiled regex pattern

**URL:** llms-txt#method-2:-compiled-regex-pattern

agent2 = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        PIIMiddleware(
            "phone_number",
            detector=re.compile(r"\+?\d{1,3}[\s.-]?\d{3,4}[\s.-]?\d{4}"),
            strategy="mask",
        ),
    ],
)

---

## Method 3: Custom detector function

**URL:** llms-txt#method-3:-custom-detector-function

**Contents:**
  - To-do list
  - LLM tool selector
  - Tool retry
  - Model retry
  - LLM tool emulator
  - Context editing
  - Shell tool
  - File search
- Provider-specific middleware

def detect_ssn(content: str) -> list[dict[str, str | int]]:
    """Detect SSN with validation.

Returns a list of dictionaries with 'text', 'start', and 'end' keys.
    """
    import re
    matches = []
    pattern = r"\d{3}-\d{2}-\d{4}"
    for match in re.finditer(pattern, content):
        ssn = match.group(0)
        # Validate: first 3 digits shouldn't be 000, 666, or 900-999
        first_three = int(ssn[:3])
        if first_three not in [0, 666] and not (900 <= first_three <= 999):
            matches.append({
                "text": ssn,
                "start": match.start(),
                "end": match.end(),
            })
    return matches

agent3 = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        PIIMiddleware(
            "ssn",
            detector=detect_ssn,
            strategy="hash",
        ),
    ],
)
python  theme={null}
def detector(content: str) -> list[dict[str, str | int]]:
    return [
        {"text": "matched_text", "start": 0, "end": 12},
        # ... more matches
    ]
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import TodoListMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[read_file, write_file, run_tests],
    middleware=[TodoListMiddleware()],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolSelectorMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[tool1, tool2, tool3, tool4, tool5, ...],
    middleware=[
        LLMToolSelectorMiddleware(
            model="gpt-4o-mini",
            max_tools=3,
            always_include=["search"],
        ),
    ],
)
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import ToolRetryMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, database_tool],
    middleware=[
        ToolRetryMiddleware(
            max_retries=3,
            backoff_factor=2.0,
            initial_delay=1.0,
        ),
    ],
)
python  theme={null}
  from langchain.agents import create_agent
  from langchain.agents.middleware import ToolRetryMiddleware

agent = create_agent(
      model="gpt-4o",
      tools=[search_tool, database_tool, api_tool],
      middleware=[
          ToolRetryMiddleware(
              max_retries=3,
              backoff_factor=2.0,
              initial_delay=1.0,
              max_delay=60.0,
              jitter=True,
              tools=["api_tool"],
              retry_on=(ConnectionError, TimeoutError),
              on_failure="continue",
          ),
      ],
  )
  python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import ModelRetryMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, database_tool],
    middleware=[
        ModelRetryMiddleware(
            max_retries=3,
            backoff_factor=2.0,
            initial_delay=1.0,
        ),
    ],
)
python  theme={null}
  from langchain.agents import create_agent
  from langchain.agents.middleware import ModelRetryMiddleware

# Basic usage with default settings (2 retries, exponential backoff)
  agent = create_agent(
      model="gpt-4o",
      tools=[search_tool],
      middleware=[ModelRetryMiddleware()],
  )

# Custom exception filtering
  class TimeoutError(Exception):
      """Custom exception for timeout errors."""
      pass

class ConnectionError(Exception):
      """Custom exception for connection errors."""
      pass

# Retry specific exceptions only
  retry = ModelRetryMiddleware(
      max_retries=4,
      retry_on=(TimeoutError, ConnectionError),
      backoff_factor=1.5,
  )

def should_retry(error: Exception) -> bool:
      # Only retry on rate limit errors
      if isinstance(error, TimeoutError):
          return True
      # Or check for specific HTTP status codes
      if hasattr(error, "status_code"):
          return error.status_code in (429, 503)
      return False

retry_with_filter = ModelRetryMiddleware(
      max_retries=3,
      retry_on=should_retry,
  )

# Return error message instead of raising
  retry_continue = ModelRetryMiddleware(
      max_retries=4,
      on_failure="continue",  # Return AIMessage with error instead of raising
  )

# Custom error message formatting
  def format_error(error: Exception) -> str:
      return f"Model call failed: {error}. Please try again later."

retry_with_formatter = ModelRetryMiddleware(
      max_retries=4,
      on_failure=format_error,
  )

# Constant backoff (no exponential growth)
  constant_backoff = ModelRetryMiddleware(
      max_retries=5,
      backoff_factor=0.0,  # No exponential growth
      initial_delay=2.0,  # Always wait 2 seconds
  )

# Raise exception on failure
  strict_retry = ModelRetryMiddleware(
      max_retries=2,
      on_failure="error",  # Re-raise exception instead of returning message
  )
  python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import LLMToolEmulator

agent = create_agent(
    model="gpt-4o",
    tools=[get_weather, search_database, send_email],
    middleware=[
        LLMToolEmulator(),  # Emulate all tools
    ],
)
python  theme={null}
  from langchain.agents import create_agent
  from langchain.agents.middleware import LLMToolEmulator
  from langchain.tools import tool

@tool
  def get_weather(location: str) -> str:
      """Get the current weather for a location."""
      return f"Weather in {location}"

@tool
  def send_email(to: str, subject: str, body: str) -> str:
      """Send an email."""
      return "Email sent"

# Emulate all tools (default behavior)
  agent = create_agent(
      model="gpt-4o",
      tools=[get_weather, send_email],
      middleware=[LLMToolEmulator()],
  )

# Emulate specific tools only
  agent2 = create_agent(
      model="gpt-4o",
      tools=[get_weather, send_email],
      middleware=[LLMToolEmulator(tools=["get_weather"])],
  )

# Use custom model for emulation
  agent4 = create_agent(
      model="gpt-4o",
      tools=[get_weather, send_email],
      middleware=[LLMToolEmulator(model="claude-sonnet-4-5-20250929")],
  )
  python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit

agent = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        ContextEditingMiddleware(
            edits=[
                ClearToolUsesEdit(
                    trigger=100000,
                    keep=3,
                ),
            ],
        ),
    ],
)
python  theme={null}
  from langchain.agents import create_agent
  from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit

agent = create_agent(
      model="gpt-4o",
      tools=[search_tool, your_calculator_tool, database_tool],
      middleware=[
          ContextEditingMiddleware(
              edits=[
                  ClearToolUsesEdit(
                      trigger=2000,
                      keep=3,
                      clear_tool_inputs=False,
                      exclude_tools=[],
                      placeholder="[cleared]",
                  ),
              ],
          ),
      ],
  )
  python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import (
    ShellToolMiddleware,
    HostExecutionPolicy,
)

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool],
    middleware=[
        ShellToolMiddleware(
            workspace_root="/workspace",
            execution_policy=HostExecutionPolicy(),
        ),
    ],
)
python  theme={null}
  from langchain.agents import create_agent
  from langchain.agents.middleware import (
      ShellToolMiddleware,
      HostExecutionPolicy,
      DockerExecutionPolicy,
      RedactionRule,
  )

# Basic shell tool with host execution
  agent = create_agent(
      model="gpt-4o",
      tools=[search_tool],
      middleware=[
          ShellToolMiddleware(
              workspace_root="/workspace",
              execution_policy=HostExecutionPolicy(),
          ),
      ],
  )

# Docker isolation with startup commands
  agent_docker = create_agent(
      model="gpt-4o",
      tools=[],
      middleware=[
          ShellToolMiddleware(
              workspace_root="/workspace",
              startup_commands=["pip install requests", "export PYTHONPATH=/workspace"],
              execution_policy=DockerExecutionPolicy(
                  image="python:3.11-slim",
                  command_timeout=60.0,
              ),
          ),
      ],
  )

# With output redaction
  agent_redacted = create_agent(
      model="gpt-4o",
      tools=[],
      middleware=[
          ShellToolMiddleware(
              workspace_root="/workspace",
              redaction_rules=[
                  RedactionRule(pii_type="api_key", detector=r"sk-[a-zA-Z0-9]{32}"),
              ],
          ),
      ],
  )
  python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import FilesystemFileSearchMiddleware

agent = create_agent(
    model="gpt-4o",
    tools=[],
    middleware=[
        FilesystemFileSearchMiddleware(
            root_path="/workspace",
            use_ripgrep=True,
        ),
    ],
)
python  theme={null}
  from langchain.agents import create_agent
  from langchain.agents.middleware import FilesystemFileSearchMiddleware
  from langchain.messages import HumanMessage

agent = create_agent(
      model="gpt-4o",
      tools=[],
      middleware=[
          FilesystemFileSearchMiddleware(
              root_path="/workspace",
              use_ripgrep=True,
              max_file_size_mb=10,
          ),
      ],
  )

# Agent can now use glob_search and grep_search tools
  result = agent.invoke({
      "messages": [HumanMessage("Find all Python files containing 'async def'")]
  })

# The agent will use:
  # 1. glob_search(pattern="**/*.py") to find Python files
  # 2. grep_search(pattern="async def", include="*.py") to find async functions
  ```
</Accordion>

## Provider-specific middleware

These middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples.

<Columns cols={2}>
  <Card title="Anthropic" href="/oss/python/integrations/middleware/anthropic" icon="anthropic" arrow>
    Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.
  </Card>

<Card title="OpenAI" href="/oss/python/integrations/middleware/openai" icon="openai" arrow>
    Content moderation middleware for OpenAI models.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/built-in.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Custom detector function signature:**

The detector function must accept a string (content) and return matches:

Returns a list of dictionaries with `text`, `start`, and `end` keys:
```

Example 2 (unknown):
```unknown
<Tip>
  For custom detectors:

  * Use regex strings for simple patterns
  * Use RegExp objects when you need flags (e.g., case-insensitive matching)
  * Use custom functions when you need validation logic beyond pattern matching
  * Custom functions give you full control over detection logic and can implement complex validation rules
</Tip>

<Accordion title="Configuration options">
  <ParamField body="pii_type" type="string" required>
    Type of PII to detect. Can be a built-in type (`email`, `credit_card`, `ip`, `mac_address`, `url`) or a custom type name.
  </ParamField>

  <ParamField body="strategy" type="string" default="redact">
    How to handle detected PII. Options:

    * `'block'` - Raise exception when detected
    * `'redact'` - Replace with `[REDACTED_{PII_TYPE}]`
    * `'mask'` - Partially mask (e.g., `****-****-****-1234`)
    * `'hash'` - Replace with deterministic hash
  </ParamField>

  <ParamField body="detector" type="function | regex">
    Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.
  </ParamField>

  <ParamField body="apply_to_input" type="boolean" default="True">
    Check user messages before model call
  </ParamField>

  <ParamField body="apply_to_output" type="boolean" default="False">
    Check AI messages after model call
  </ParamField>

  <ParamField body="apply_to_tool_results" type="boolean" default="False">
    Check tool result messages after execution
  </ParamField>
</Accordion>

### To-do list

Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following:

* Complex multi-step tasks requiring coordination across multiple tools.
* Long-running operations where progress visibility is important.

<Note>
  This middleware automatically provides agents with a `write_todos` tool and system prompts to guide effective task planning.
</Note>

**API reference:** [`TodoListMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.TodoListMiddleware)
```

Example 3 (unknown):
```unknown
<Callout icon="circle-play" iconType="solid">
  Watch this [video guide](https://www.youtube.com/watch?v=yTWocbVKQxw) demonstrating To-do List middleware behavior.
</Callout>

<Accordion title="Configuration options">
  <ParamField body="system_prompt" type="string">
    Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.
  </ParamField>

  <ParamField body="tool_description" type="string">
    Custom description for the `write_todos` tool. Uses built-in description if not specified.
  </ParamField>
</Accordion>

### LLM tool selector

Use an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following:

* Agents with many tools (10+) where most aren't relevant per query.
* Reducing token usage by filtering irrelevant tools.
* Improving model focus and accuracy.

This middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes.

**API reference:** [`LLMToolSelectorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.LLMToolSelectorMiddleware)
```

Example 4 (unknown):
```unknown
<Accordion title="Configuration options">
  <ParamField body="model" type="string | BaseChatModel">
    Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model\(model\)) for more information.

    Defaults to the agent's main model.
  </ParamField>

  <ParamField body="system_prompt" type="string">
    Instructions for the selection model. Uses built-in prompt if not specified.
  </ParamField>

  <ParamField body="max_tools" type="number">
    Maximum number of tools to select. If the model selects more, only the first max\_tools will be used. No limit if not specified.
  </ParamField>

  <ParamField body="always_include" type="list[string]">
    Tool names to always include regardless of selection. These do not count against the max\_tools limit.
  </ParamField>
</Accordion>

### Tool retry

Automatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following:

* Handling transient failures in external API calls.
* Improving reliability of network-dependent tools.
* Building resilient agents that gracefully handle temporary errors.

**API reference:** [`ToolRetryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ToolRetryMiddleware)
```

---

## Metrics: [OTel Example](/langsmith/langsmith-collector#metrics)

**URL:** llms-txt#metrics:-[otel-example](/langsmith/langsmith-collector#metrics)

**Contents:**
- LangSmith Services
- Frontend Nginx
- Postgres + Redis
- Clickhouse

## LangSmith Services

The following LangSmith services expose metrics at an endpoint, in the Prometheus metrics format. The frontend does not currently expose metrics.

* **Backend**: `http://<langsmith_release_name>-backend.<namespace>.svc.cluster.local:1984/metrics`
* **Platform Backend**: `http://<langsmith_release_name>-platform-backend.<namespace>.svc.cluster.local:1986/metrics`
* **Playground**: `http://<langsmith_release_name>-playground.<namespace>.svc.cluster.local:1988/metrics`
* **(LangSmith Control Plane only) Host Backend**: `http://<langsmith_release_name>-host-backend.<namespace>.svc.cluster.local:1985/metrics`

You can use a [Prometheus](https://prometheus.io/docs/prometheus/latest/getting_started/#configure-prometheus-to-monitor-the-sample-targets) or [OpenTelemetry](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver) collector to scrape the endpoints, and export metrics to the backend of your choice.

The frontend service exposes its Nginx metrics at the following endpoint: `langsmith-frontend.langsmith.svc.cluster.local:80/nginx_status`. You can either scrape them yourself, or bring up a Prometheus Nginx exporter using the [LangSmith Observability Helm Chart](/langsmith/observability-stack)

<Warning>
  **The following sections apply for in-cluster databases only. If you are using external databases, you will need to configure exposing and fetching metrics.**
</Warning>

If you are using in-cluster Postgres/Redis instances, you can use a Prometheus exporter to expose metrics from your instance. You can deploy your own, or if you would like, you can use the [LangSmith Observability Helm Chart](/langsmith/observability-stack) to deploy an exporter for you.

The in-cluster Clickhouse is configured to expose metrics without the need for an exporter. You can use your collector to scrape metrics at `http://<langsmith_release_name>-clickhouse.<namespace>.svc.cluster.local:9363/metrics`

---

## Microsoft

**URL:** llms-txt#microsoft

**Contents:**
- Chat models
  - Azure OpenAI
  - Azure AI
  - Azure ML Chat Online Endpoint
- LLMs
  - Azure ML
  - Azure OpenAI
- Embedding Models
  - Azure OpenAI
  - Azure AI

Source: https://docs.langchain.com/oss/python/integrations/providers/microsoft

This page covers all LangChain integrations with [Microsoft Azure](https://portal.azure.com) and other [Microsoft](https://www.microsoft.com) products.

Microsoft offers three main options for accessing chat models through Azure:

1. [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) - Provides access to OpenAI's powerful models like o3, 4.1, and other models through Microsoft Azure's secure enterprise platform.
2. [Azure AI](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models) - Offers access to a variety of models from different providers including Anthropic, DeepSeek, Cohere, Phi and Mistral through a unified API.
3. [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/) - Allows deployment and management of your own custom models or fine-tuned open-source models with Azure Machine Learning.

> [Microsoft Azure](https://en.wikipedia.org/wiki/Microsoft_Azure), often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.

> [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/) is an `Azure` service with powerful language models from OpenAI including the `GPT-3`, `Codex` and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.

Set the environment variables to get access to the `Azure OpenAI` service.

See a [usage example](/oss/python/integrations/chat/azure_chat_openai)

> [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the `AzureAIChatCompletionsModel` class.

Configure your API key and Endpoint.

See a [usage example](/oss/python/integrations/chat/azure_ai)

### Azure ML Chat Online Endpoint

See the documentation [here](/oss/python/integrations/chat/azureml_chat_endpoint) for accessing chat
models hosted with [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/).

See a [usage example](/oss/python/integrations/llms/azure_ml).

See a [usage example](/oss/python/integrations/llms/azure_openai).

Microsoft offers two main options for accessing embedding models through Azure:

See a [usage example](/oss/python/integrations/text_embedding/azure_openai)

Configure your API key and Endpoint.

> [Azure AI Foundry (formerly Azure AI Studio](https://ai.azure.com/) provides the capability to upload data assets
> to cloud storage and register existing data assets from the following sources:
>
> * `Microsoft OneLake`
> * `Azure Blob Storage`
> * `Azure Data Lake gen 2`

First, you need to install several python packages.

See a [usage example](/oss/python/integrations/document_loaders/azure_ai_data).

### Azure AI Document Intelligence

> [Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known
> as `Azure Form Recognizer`) is machine-learning
> based service that extracts texts (including handwriting), tables, document structures,
> and key-value-pairs
> from digital or scanned PDFs, images, Office and HTML files.
>
> Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.

First, you need to install a python package.

See a [usage example](/oss/python/integrations/document_loaders/azure_document_intelligence).

### Azure Blob Storage

> [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.

`Azure Blob Storage` is designed for:

* Serving images or documents directly to a browser.
* Storing files for distributed access.
* Streaming video and audio.
* Writing to log files.
* Storing data for backup and restore, disaster recovery, and archiving.
* Storing data for analysis by an on-premises or Azure-hosted service.

See [usage examples for the Azure Blob Storage Loader](/oss/python/integrations/document_loaders/azure_blob_storage).

### Microsoft OneDrive

> [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.

First, you need to install a python package.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_onedrive).

### Microsoft OneDrive File

> [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft.

First, you need to install a python package.

> [Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) is a word processor developed by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_word).

> [Microsoft Excel](https://en.wikipedia.org/wiki/Microsoft_Excel) is a spreadsheet editor developed by
> Microsoft for Windows, macOS, Android, iOS and iPadOS.
> It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming
> language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.

The `UnstructuredExcelLoader` is used to load `Microsoft Excel` files. The loader works with both `.xlsx` and `.xls` files.
The page content will be the raw text of the Excel file. If you use the loader in `"elements"` mode, an HTML
representation of the Excel file will be available in the document metadata under the `text_as_html` key.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_excel).

### Microsoft SharePoint

> [Microsoft SharePoint](https://en.wikipedia.org/wiki/SharePoint) is a website-based collaboration system
> that uses workflow applications, “list” databases, and other web parts and security features to
> empower business teams to work together developed by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_sharepoint).

### Microsoft PowerPoint

> [Microsoft PowerPoint](https://en.wikipedia.org/wiki/Microsoft_PowerPoint) is a presentation program by Microsoft.

See a [usage example](/oss/python/integrations/document_loaders/microsoft_powerpoint).

### Microsoft OneNote

First, let's install dependencies:

See a [usage example](/oss/python/integrations/document_loaders/microsoft_onenote).

### Playwright URL Loader

> [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool
> developed by `Microsoft` that allows you to programmatically control and automate
> web browsers. It is designed for end-to-end testing, scraping, and automating
> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.

First, let's install dependencies:

See a [usage example](/oss/python/integrations/document_loaders/url/#playwright-url-loader).

### Azure Cosmos DB Chat Message History

> [Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/) provides chat message history storage for conversational AI applications, enabling you to persist and retrieve conversation history with low latency and high availability.

Configure your Azure Cosmos DB connection:

AI agents can rely on Azure Cosmos DB as a unified [memory system](https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents#memory-can-make-or-break-agents) solution, enjoying speed, scale, and simplicity. This service successfully [enabled OpenAI's ChatGPT service](https://www.youtube.com/watch?v=6IIUtEFKJec\&t) to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world's first globally distributed [NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql), [relational](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-relational), and [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) service that offers a serverless mode.

Below are two available Azure Cosmos DB APIs that can provide vector store functionalities.

#### Azure Cosmos DB for MongoDB (vCore)

> [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support.
> You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string.
> Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB.

##### Installation and Setup

See [detailed configuration instructions](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore).

We need to install `langchain-azure-ai` and `pymongo` python packages.

##### Deploy Azure Cosmos DB on Microsoft Azure

Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.

With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.

[Sign Up](https://azure.microsoft.com/en-us/free/) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore).

#### Azure Cosmos DB NoSQL

> [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search) now offers vector indexing and search in preview.
> This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors
> directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data,
> but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching,
> as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the
> efficiency of vector-based operations.

##### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql).

We need to install `langchain-azure-ai` and `azure-cosmos` python packages.

##### Deploy Azure Cosmos DB on Microsoft Azure

Azure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available
in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.

[Sign Up](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-python?pivots=devcontainer-codespace) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql).

### Azure Database for PostgreSQL

> [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.

See [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL.

Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.

Since Azure Database for PostgreSQL is open-source Postgres, you can use the [LangChain's Postgres support](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.

### Azure SQL Database

> [Azure SQL Database](https://learn.microsoft.com/azure/azure-sql/database/sql-database-paas-overview?view=azuresql) is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution.  It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity.

By leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems.

##### Installation and Setup

See [detail configuration instructions](/oss/python/integrations/vectorstores/sqlserver).

We need to install the `langchain-sqlserver` python package.

##### Deploy Azure SQL DB on Microsoft Azure

[Sign Up](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql) for free to get started today.

See a [usage example](/oss/python/integrations/vectorstores/sqlserver).

[Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) is a cloud search service
that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid
queries at scale. See [here](/oss/python/integrations/vectorstores/azuresearch) for usage examples.

> [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as `Azure Search` or `Azure Cognitive Search` ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.

> Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities:
>
> * A search engine for full text search over a search index containing user-owned content
> * Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation
> * Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more
> * Programmability through REST APIs and client libraries in Azure SDKs
> * Azure integration at the data layer, machine learning layer, and AI (AI Services)

See [set up instructions](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal).

See a [usage example](/oss/python/integrations/retrievers/azure_ai_search).

### Azure Database for PostgreSQL

> [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability.

See [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL.

You need to [enable pgvector extension](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-use-pgvector) in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the [PGVector in LangChain](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL.

See a [usage example](/oss/python/integrations/vectorstores/pgvector/). Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal.

### Azure Container Apps dynamic sessions

We need to get the `POOL_MANAGEMENT_ENDPOINT` environment variable from the Azure Container Apps service.
See the instructions [here](/oss/python/integrations/tools/azure_dynamic_sessions/#setup).

We need to install a python package.

See a [usage example](/oss/python/integrations/tools/azure_dynamic_sessions).

Follow the documentation [here](/oss/python/integrations/tools/bing_search) to get a detail explanations and instructions of this tool.

The environment variable `BING_SUBSCRIPTION_KEY` and `BING_SEARCH_URL` are required from Bing Search resource.

### Azure AI Services

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/azure_ai_services).

#### Azure AI Services individual tools

The `azure_ai_services` toolkit includes the following tools:

* Image Analysis: [AzureAiServicesImageAnalysisTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.image_analysis.AzureAiServicesImageAnalysisTool.html)
* Document Intelligence: [AzureAiServicesDocumentIntelligenceTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.document_intelligence.AzureAiServicesDocumentIntelligenceTool.html)
* Speech to Text: [AzureAiServicesSpeechToTextTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.speech_to_text.AzureAiServicesSpeechToTextTool.html)
* Text to Speech: [AzureAiServicesTextToSpeechTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_to_speech.AzureAiServicesTextToSpeechTool.html)
* Text Analytics for Health: [AzureAiServicesTextAnalyticsForHealthTool](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.azure_ai_services.text_analytics_for_health.AzureAiServicesTextAnalyticsForHealthTool.html)

### Azure Cognitive Services

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/azure_cognitive_services).

#### Azure AI Services individual tools

The `azure_ai_services` toolkit includes the tools that queries the `Azure Cognitive Services`:

* `AzureCogsFormRecognizerTool`: Form Recognizer API
* `AzureCogsImageAnalysisTool`: Image Analysis API
* `AzureCogsSpeech2TextTool`: Speech2Text API
* `AzureCogsText2SpeechTool`: Text2Speech API
* `AzureCogsTextAnalyticsHealthTool`: Text Analytics for Health API

### Microsoft Office 365 email and calendar

We need to install `O365` python package.

See a [usage example](/oss/python/integrations/tools/office365).

#### Office 365 individual tools

You can use individual tools from the Office 365 Toolkit:

* `O365CreateDraftMessage`: creating a draft email in Office 365
* `O365SearchEmails`: searching email messages in Office 365
* `O365SearchEvents`: searching calendar events in Office 365
* `O365SendEvent`: sending calendar events in Office 365
* `O365SendMessage`: sending an email in Office 365

### Microsoft Azure PowerBI

We need to install `azure-identity` python package.

See a [usage example](/oss/python/integrations/tools/powerbi).

#### PowerBI individual tools

You can use individual tools from the Azure PowerBI Toolkit:

* `InfoPowerBITool`: getting metadata about a PowerBI Dataset
* `ListPowerBITool`: getting tables names
* `QueryPowerBITool`: querying a PowerBI Dataset

### PlayWright Browser Toolkit

> [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool
> developed by `Microsoft` that allows you to programmatically control and automate
> web browsers. It is designed for end-to-end testing, scraping, and automating
> tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`.

We need to install several python packages.

See a [usage example](/oss/python/integrations/tools/playwright).

#### PlayWright Browser individual tools

You can use individual tools from the PlayWright Browser Toolkit.

### Azure Cosmos DB for Apache Gremlin

We need to install a python package.

See a [usage example](/oss/python/integrations/graphs/azure_cosmosdb_gremlin).

> [Microsoft Bing](https://www.bing.com/), commonly referred to as `Bing` or `Bing Search`,
> is a web search engine owned and operated by `Microsoft`.

See a [usage example](/oss/python/integrations/tools/bing_search).

### Microsoft Presidio

> [Presidio](https://microsoft.github.io/presidio/) (Origin from Latin praesidium ‘protection, garrison’)
> helps to ensure sensitive data is properly managed and governed. It provides fast identification and
> anonymization modules for private entities in text and images such as credit card numbers, names,
> locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.

First, you need to install several python packages and download a `SpaCy` model.

See [usage examples](https://python.langchain.com/v0.1/docs/guides/productionization/safety/presidio_data_anonymization).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/microsoft.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Set the environment variables to get access to the `Azure OpenAI` service.
```

Example 3 (unknown):
```unknown
See a [usage example](/oss/python/integrations/chat/azure_chat_openai)
```

Example 4 (unknown):
```unknown
### Azure AI

> [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the `AzureAIChatCompletionsModel` class.

<CodeGroup>
```

---

## minReplicas: 16

**URL:** llms-txt#minreplicas:-16

---

## minReplicas: 20

**URL:** llms-txt#minreplicas:-20

**Contents:**
- Ensure your Redis cache is at least 200 GB

## Ensure your Redis cache is at least 200 GB
redis:
  external:
    enabled: true
    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)

---

## minReplicas: 2

**URL:** llms-txt#minreplicas:-2

platformBackend:
  deployment:
    replicas: 20 # OR enable autoscaling to this level (example below)

---

## minReplicas: 3

**URL:** llms-txt#minreplicas:-3

**Contents:**
- Ensure your Redis cache is at least 200 GB
  - High reads, low writes <a name="high-reads-low-writes" />

## Ensure your Redis cache is at least 200 GB
redis:
  external:
    enabled: true
    existingSecretName: langsmith-redis-secret # Set the connection url for your external Redis instance (200+ GB)

clickhouse:
  statefulSet:
    persistence:
      # This may depend on your configured TTL (see config section).
      # We recommend 600Gi for every shortlived TTL day if operating at this scale constantly.
      size: 4200Gi # This assumes 7 days TTL and operating a this scale constantly.
    resources:
      requests:
        cpu: "10"
        memory: "32Gi"
      limits:
        cpu: "16"
        memory: "48Gi"

commonEnv:
  - name: "CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT"
    value: "0"
yaml  theme={null}
config:
  blobStorage:
    # Please also set the other keys to connect to your blob storage. See configuration section.
    enabled: true

frontend:
  deployment:
    replicas: 2

queue:
  deployment:
    replicas: 6 # OR enable autoscaling to this level (example below)

**Examples:**

Example 1 (unknown):
```unknown
### High reads, low writes <a name="high-reads-low-writes" />

You have a relatively low scale of trace ingestions, but many frontend users querying traces and/or have scripts that hit the `/runs/query` or `/runs/<run-id>` endpoints frequently.

**For this, we strongly recommend setting up a replicated ClickHouse cluster to enable high read scale at low latency.** See our [external ClickHouse doc](/langsmith/self-host-external-clickhouse#ha-replicated-clickhouse-cluster) for more guidance on how to setup a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 8+ cores and 16+ GB memory, and resource limit of 12 cores and 32 GB memory.

For this, we recommend a configuration like this:
```

---

## minReplicas: 40

**URL:** llms-txt#minreplicas:-40

backend:
  deployment:
    replicas: 50 # OR enable autoscaling to this level (example below)

---

## minReplicas: 4

**URL:** llms-txt#minreplicas:-4

backend:
  deployment:
    replicas: 40 # OR enable autoscaling to this level (example below)

---

## minReplicas: 5

**URL:** llms-txt#minreplicas:-5

backend:
  deployment:
    replicas: 16 # OR enable autoscaling to this level (example below)

---

## minReplicas: 8

**URL:** llms-txt#minreplicas:-8

**Contents:**
- Note that we are actively working on improving performance of this service to reduce the number of replicas.

## Note that we are actively working on improving performance of this service to reduce the number of replicas.
queue:
  deployment:
    replicas: 160 # OR enable autoscaling to this level (example below)

---

## Models

**URL:** llms-txt#models

**Contents:**
- Basic usage
  - Initialize a model
  - Key methods
- Parameters
- Invocation
  - Invoke
  - Stream

Source: https://docs.langchain.com/oss/python/langchain/models

[LLMs](https://en.wikipedia.org/wiki/Large_language_model) are powerful AI tools that can interpret and generate text like humans. They're versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task.

In addition to text generation, many models support:

* <Icon icon="hammer" size={16} /> [Tool calling](#tool-calling) - calling external tools (like databases queries or API calls) and use results in their responses.
* <Icon icon="shapes" size={16} /> [Structured output](#structured-output) - where the model's response is constrained to follow a defined format.
* <Icon icon="image" size={16} /> [Multimodality](#multimodal) - process and return data other than text, such as images, audio, and video.
* <Icon icon="brain" size={16} /> [Reasoning](#reasoning) - models perform multi-step reasoning to arrive at a conclusion.

Models are the reasoning engine of [agents](/oss/python/langchain/agents). They drive the agent's decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.

The quality and capabilities of the model you choose directly impact your agent's baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information.

LangChain's standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.

<Info>
  For provider-specific integration information and capabilities, see the provider's [chat model page](/oss/python/integrations/chat).
</Info>

Models can be utilized in two ways:

1. **With agents** - Models can be dynamically specified when creating an [agent](/oss/python/langchain/agents#model).
2. **Standalone** - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework.

The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.

### Initialize a model

The easiest way to get started with a standalone model in LangChain is to use [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) to initialize one from a [chat model provider](/oss/python/integrations/chat) of your choice (examples below):

<Tabs>
  <Tab title="OpenAI">
    👉 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)

</CodeGroup>
  </Tab>

<Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)

</CodeGroup>
  </Tab>

<Tab title="Azure">
    👉 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)

</CodeGroup>
  </Tab>

<Tab title="Google Gemini">
    👉 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)

</CodeGroup>
  </Tab>

<Tab title="AWS Bedrock">
    👉 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)

</CodeGroup>
  </Tab>

<Tab title="HuggingFace">
    👉 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)

</CodeGroup>
  </Tab>
</Tabs>

See [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model) for more detail, including information on how to pass model [parameters](#parameters).

<Card title="Invoke" href="#invoke" icon="paper-plane" arrow="true" horizontal>
  The model takes messages as input and outputs messages after generating a complete response.
</Card>

<Card title="Stream" href="#stream" icon="tower-broadcast" arrow="true" horizontal>
  Invoke the model, but stream the output as it is generated in real-time.
</Card>

<Card title="Batch" href="#batch" icon="grip" arrow="true" horizontal>
  Send multiple requests to a model in a batch for more efficient processing.
</Card>

<Info>
  In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the [integrations page](/oss/python/integrations/providers/overview) for details.
</Info>

A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:

<ParamField body="model" type="string" required>
  The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the '{model_provider}:{model}' format, for example, 'openai:o1'.
</ParamField>

<ParamField body="api_key" type="string">
  The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an <Tooltip tip="A variable whose value is set outside the program, typically through functionality built into the operating system or microservice.">environment variable</Tooltip>.
</ParamField>

<ParamField body="temperature" type="number">
  Controls the randomness of the model's output. A higher number makes responses more creative; lower ones make them more deterministic.
</ParamField>

<ParamField body="max_tokens" type="number">
  Limits the total number of <Tooltip tip="The basic unit that a model reads and generates. Providers may define them differently, but in general, they can represent a whole or part of word.">tokens</Tooltip> in the response, effectively controlling how long the output can be.
</ParamField>

<ParamField body="timeout" type="number">
  The maximum time (in seconds) to wait for a response from the model before canceling the request.
</ParamField>

<ParamField body="max_retries" type="number">
  The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.
</ParamField>

Using [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), pass these parameters as inline <Tooltip tip="Arbitrary keyword arguments" cta="Learn more" href="https://www.w3schools.com/python/python_args_kwargs.asp">`**kwargs`</Tooltip>:

<Info>
  Each chat model integration may have additional params used to control provider-specific functionality.

For example, [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI) has `use_responses_api` to dictate whether to use the OpenAI Responses or Completions API.

To find all the parameters supported by a given chat model, head to the [chat model integrations](/oss/python/integrations/chat) page.
</Info>

A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.

The most straightforward way to call a model is to use [`invoke()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.invoke) with a single message or a list of messages.

A list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation.

See the [messages](/oss/python/langchain/messages) guide for more detail on roles, types, and content.

<Info>
  If the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with "Chat", e.g., [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI)(/oss/integrations/chat/openai).
</Info>

Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses.

Calling [`stream()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.stream) returns an <Tooltip tip="An object that progressively provides access to each item of a collection, in order.">iterator</Tooltip> that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:

As opposed to [`invoke()`](#invoke), which returns a single [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) after the model has finished generating its full response, `stream()` returns multiple [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation:

```python Construct an AIMessage theme={null}
full = None  # None | AIMessageChunk
for chunk in model.stream("What color is the sky?"):
    full = chunk if full is None else full + chunk
    print(full.text)

**Examples:**

Example 1 (unknown):
```unknown
<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>
  </Tab>

  <Tab title="Anthropic">
    👉 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)
```

Example 4 (unknown):
```unknown
<CodeGroup>
```

---

## model_1 is tagged with "joke"

**URL:** llms-txt#model_1-is-tagged-with-"joke"

model_1 = init_chat_model(model="gpt-4o-mini", tags=['joke'])

---

## model_2 is tagged with "poem"

**URL:** llms-txt#model_2-is-tagged-with-"poem"

model_2 = init_chat_model(model="gpt-4o-mini", tags=['poem'])

graph = ... # define a graph that uses these LLMs

---

## Model caches

**URL:** llms-txt#model-caches

Source: https://docs.langchain.com/oss/javascript/integrations/llm_caching/index

[Caching LLM calls](/oss/javascript/langchain/models#caching) can be useful for testing, cost savings, and speed.

Below are some integrations that allow you to cache results of individual LLM calls using different caches with different strategies.

<Columns cols={3}>
  <Card title="Azure Cosmos DB NoSQL Semantic Cache" icon="link" href="/oss/javascript/integrations/llm_caching/azure_cosmosdb_nosql" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/javascript/integrations/llm_caching/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Model for performing extraction.

**URL:** llms-txt#model-for-performing-extraction.

info_llm = init_chat_model("gpt-4o-mini").with_structured_output(
    PurchaseInformation, method="json_schema", include_raw=True
)

---

## Model initialization

**URL:** llms-txt#model-initialization

**Contents:**
  - `langchain-classic`
- Migration guide
- Reporting issues
- Additional resources
- See also

from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings
bash pip theme={null}
  pip install langchain-classic
  bash uv theme={null}
  uv add langchain-classic
  python  theme={null}
from langchain import ...  # [!code --]
from langchain_classic import ...  # [!code ++]

from langchain.chains import ...  # [!code --]
from langchain_classic.chains import ...  # [!code ++]

from langchain.retrievers import ...  # [!code --]
from langchain_classic.retrievers import ...  # [!code ++]

from langchain import hub  # [!code --]
from langchain_classic import hub  # [!code ++]
```

See our [migration guide](/oss/python/migrate/langchain-v1) for help updating your code to LangChain v1.

Please report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchain/issues) using the `'v1'` [label](https://github.com/langchain-ai/langchain/issues?q=state%3Aopen%20label%3Av1).

## Additional resources

<CardGroup cols={3}>
  <Card title="LangChain 1.0" icon="rocket" href="https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/">
    Read the announcement
  </Card>

<Card title="Middleware guide" icon="puzzle-piece" href="https://blog.langchain.com/agent-middleware/">
    Deep dive into middleware
  </Card>

<Card title="Agents Documentation" icon="book" href="/oss/python/langchain/agents" arrow>
    Full agent documentation
  </Card>

<Card title="Message Content" icon="message" href="/oss/python/langchain/messages#message-content" arrow>
    New content blocks API
  </Card>

<Card title="Migration guide" icon="arrow-right-arrow-left" href="/oss/python/migrate/langchain-v1" arrow>
    How to migrate to LangChain v1
  </Card>

<Card title="GitHub" icon="github" href="https://github.com/langchain-ai/langchain">
    Report issues or contribute
  </Card>
</CardGroup>

* [Versioning](/oss/python/versioning) – Understanding version numbers
* [Release policy](/oss/python/release-policy) – Detailed release policies

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langchain-v1.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### `langchain-classic`

Legacy functionality has moved to [`langchain-classic`](https://pypi.org/project/langchain-classic) to keep the core packages lean and focused.

**What's in `langchain-classic`:**

* Legacy chains and chain implementations
* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module)
* The indexing API
* The hub module (for managing prompts programmatically)
* [`langchain-community`](https://pypi.org/project/langchain-community) exports
* Other deprecated functionality

If you use any of this functionality, install [`langchain-classic`](https://pypi.org/project/langchain-classic):

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Then update your imports:
```

---

## Monitor projects with dashboards

**URL:** llms-txt#monitor-projects-with-dashboards

**Contents:**
- Prebuilt dashboards
  - Dashboard sections
  - Group by
- Custom Dashboards
  - Creating a new dashboard
  - Adding charts to your dashboard
  - Chart configuration
  - Save and manage charts
- Linking to a dashboard from a tracing project
- Example: user-journey monitoring

Source: https://docs.langchain.com/langsmith/dashboards

Dashboards give you high-level insights into your trace data, helping you spot trends and monitor the health of your applications. Dashboards are available in the **Monitoring** tab in the left sidebar.

LangSmith offers two dashboard types:

* **Prebuilt dashboards**: Automatically generated for every tracing project.
* **Custom dashboards**: Fully configurable collections of charts tailored to your needs.

## Prebuilt dashboards

Prebuilt dashboards are created automatically for each project and cover essential metrics, such as trace count, error rates, token usage, and more. By default, the prebuilt dashboard for your tracing project can be accessed using the **Dashboard** button on the top right of the tracing project page.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prebuilt.gif?s=cbb7410db7c9036ed6c03af251f13a99" alt="prebuilt" data-og-width="1392" width="1392" data-og-height="1080" height="1080" data-path="langsmith/images/prebuilt.gif" data-optimize="true" data-opv="3" />

<Note>**You cannot modify a prebuilt dashboard. In the future, we plan to allow you to clone a default dashboard in order to have a starting point to customize it.**</Note>

### Dashboard sections

Prebuilt dashboards are broken down into the following sections:

| Section         | What it shows                                                                                                                                                                                                                                                                                                    |
| :-------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Traces          | Trace count, latency and error rates. A [trace](/langsmith/observability-concepts#traces) is a collection of [runs](/langsmith/observability-concepts#runs) related to a single operation. For example, if a user request triggers an agent, all runs for that agent invocation would be part of the same trace. |
| LLM Calls       | LLM call count and latency. Includes all runs where run type is "llm".                                                                                                                                                                                                                                           |
| Cost & Tokens   | Total and per-trace token counts and costs, broken down by token type. Costs are measured using [LangSmith's cost tracking](/langsmith/log-llm-trace#manually-provide-token-counts).                                                                                                                             |
| Tools           | Run counts, error rates, and latency stats for tool runs broken down by tool name. Includes runs where run type is "tool". Limits to top 5 most frequently occurring tools.                                                                                                                                      |
| Run Types       | Run counts, error rates, and latency stats for runs that are immediate children of the root run. This helps in understanding the high-level execution path of agents. Limits to top 5 most frequently occurring run names. Refer to the image following this table.                                              |
| Feedback Scores | Aggregate stats for the top 5 most frequently occurring types of feedback. Charts show average score for numerical feedback and category counts for categorical feedback.                                                                                                                                        |

For example, for the following trace, the following runs have a depth of 1:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=382b7d6064e09c23efc6770fcd983a69" alt="Run depth explained" data-og-width="524" width="524" data-og-height="810" height="810" data-path="langsmith/images/run-depth-explained.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=03503ce7bfd170a22ff5152a7564e130 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a84d5785aaa93bb3e8ab7d2a4143f063 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=5d6df2d7b6dbe6e358ad2f03df341661 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=74abfc6a192c9a433dcf62b01a571594 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9e3089b4311d828f80594649647f85bd 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run-depth-explained.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=c6e3a31aded7c2ad7ae96f948613c983 2500w" />

Group by [run tag or metadata](/langsmith/add-metadata-tags) can be used to split data over attributes that are important to your application. The global group by setting appears on the top right hand side of the dashboard. Note that the Tool and Run Type charts already have a group by applied, so the global group by won't take effect; the global group by will apply to all other charts.

<Note>When adding metadata to runs, we recommend having the same metadata on the trace, as well as the specific run (e.g. LLM call). Metadata and tags are not propagated from parent to child runs, or vice versa. So, if you want to see e.g. both your trace charts and your LLM call charts grouped on some metadata key then both your traces (root runs) and your LLM runs need to have that metadata attached.</Note>

Create tailored collections of charts for tracking metrics that matter most for your application.

### Creating a new dashboard

1. Navigate to the **Monitor** tab in the left sidebar.
2. Click on the **+ New Dashboard** button.
3. Give your dashboard a name and a description.
4. Click on **Create**.

### Adding charts to your dashboard

1. Within a dashboard, click on the **+ New Chart** button to open up the chart creation pane.
2. Give your chart a name and a description.
3. Configure the chart.

### Chart configuration

#### Select tracing projects and filter runs

* Select one or more tracing projects to track metrics for.
* Use the **Chart filters** section to refine the matching runs. This filter applies to all data series in the chart. For more information on filtering traces, view our guide on [filtering traces in application](./filter-traces-in-application).

* Choose a metric from the dropdown menu to set the y-axis of your chart. With a project and a metric selected, you'll see a preview of your chart and the matching runs.
* For certain metrics (such as latency, token usage, cost), we support comparing multiple metrics with the same unit. For example, you may want one chart where you can see prompt tokens and completion tokens. Each metric appears as a separate line.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3543d454f25f5d11046fbd5bcab7aeff" alt="Multiple metrics" data-og-width="1475" width="1475" data-og-height="741" height="741" data-path="langsmith/images/compare-metrics.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=75f80a706d3e3ddd09117bc9d317ecdc 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=2606ae254c47349e2c5bd14ae4fc49b8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4239ab64861d8e3768f90f837dbefe67 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0e8d5f3e7fcf40fe6f33b6066afa346f 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d45519451d443538ad76fdcbb0d21f62 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/compare-metrics.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33024dfb462b8229b16c4fbd9cc650ba 2500w" />

There are two ways to create multiple series in a chart (i.e. create multiple lines in a chart):

1. **Group by**: Group runs by [run tag or metadata](/langsmith/add-metadata-tags), run name, or run type. Group by automatically splits the data into multiple series based on the field selected. Note that group by is limited to the top 5 elements by frequency.

2. **Data series**: Manually define multiple series with individual filters. This is useful for comparing granular data within a single metric.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e5ed30b317abdb90aea612702d94cf04" alt="Multiple data series" data-og-width="2796" width="2796" data-og-height="1396" height="1396" data-path="langsmith/images/multiple-data-series.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=75134cada0ab88b532d6073c3317dc36 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=05a0b345dd25435cae0c6e25fee62942 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=aede683fd53d369cbfe7c1649d128953 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e96e54ecf149d6a5881bc44d81941220 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fee4a18237d105668c227eafb014d0d5 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiple-data-series.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6a93006b73bc0451c68acf10189155a4 2500w" />

#### Pick a chart type

* Choose between a line chart and a bar chart for visualizing

### Save and manage charts

* Click `Save` to save your chart to the dashboard.
* Edit or delete a chart by clicking the triple dot button in the top right of the chart.
* Clone a chart by clicking the triple line button in the top right of the chart and selecting **+ Clone**. This will open a new chart creation pane with the same configurations as the original.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b40f5f132a2c0770cfe04a125dcbc7f2" alt="More actions bar" data-og-width="2102" width="2102" data-og-height="758" height="758" data-path="langsmith/images/more-actions-bar.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b4033fc996ce15ac3eee9e42430f3089 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0176aa9470908b2ee287c6588f8898ae 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=38af0f07ca3394b40dbdd4f6d83af5ba 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b41c74e251946e2f6f31739c71a42096 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c5a6f79532146e2596857be1f85f342a 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/more-actions-bar.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=623e4b00a5a04e950e8312cf2a2233b6 2500w" />

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d0c5597d6adfa7d02888ef4b89ac616d" alt="Expanded chart" data-og-width="2238" width="2238" data-og-height="1662" height="1662" data-path="langsmith/images/expanded-chart.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=73fa53c7cdb5a89eaac5e685035bfc82 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=df3f46036e1692777f6c0d460b78ca0c 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9c4abdd5d8cf517a9e537d09f6d6e761 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=91a2ff0f83ad36710058479fc5f30698 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7f16a10cf75c87fb38f3cf6db39e1fe4 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/expanded-chart.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=97d6ff4e7841cdf169c4e069605c7089 2500w" />

## Linking to a dashboard from a tracing project

You can link to any dashboard directly from a tracing project. By default, the prebuilt dashboard for your tracing project is selected. If you have a custom dashboard that you would like to link instead:

1. In your tracing project, click the three dots next to the **Dashboard** button.
2. Choose a dashboard to set as the new default.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=06b63ab9c4f8d7185c72a77e84862f3a" alt="Tracing project to dashboard" data-og-width="2080" width="2080" data-og-height="770" height="770" data-path="langsmith/images/tracing-project-to-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b06a5e1a4fe6b82260032624bdc1ce68 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ea5dd2397337b290c996ef26948836ea 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=26832107ff0613d0575432fc4f2424d6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2ace3a16bd11cd8a8906bbdab77c3f10 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c5b29e7497d837a0d84b97f5c3c522a6 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-project-to-dashboard.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=6055757e9ac1f5fd9381d14bd92cf088 2500w" />

## Example: user-journey monitoring

Use monitoring charts for mapping the decisions made by an agent at a particular node.

Consider an email assistant agent. At a particular node it makes a decision about an email to:

* send an email back
* notify the user
* no response needed

We can create a chart to track and visualize the breakdown of these decisions.

**Creating the chart**

1. **Metric Selection**: Select the metric `Run count`.

2. **Chart Filters**: Add a tree filter to include all of the traces with name `triage_input`. This means we only include traces that hit the `triage_input` node. Also add a chart filter for `Is Root` is `true`, so our count is not inflated by the number of nodes in the trace.
   <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f769fb23624b3b1b6c042ff5cfd910e6" alt="Decision at node" data-og-width="2620" width="2620" data-og-height="1698" height="1698" data-path="langsmith/images/chart-filters-for-node-decision.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e09f22192fa9cb75b32f6812071e1e07 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e3108ae084660ecc151a68a47fab6912 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fb75ff0f6c0e9140bc4554cceb93134b 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=704fb96e453fdd92dcce1e34a35a48f2 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=902d095d27fb3f20b8e403f4ddc18e24 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chart-filters-for-node-decision.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9afff6512a968017060e6fde0a5fb992 2500w" />

3. **Data Series**: Create a data series for each decision made at the `triage_input` node. The output of the decision is stored in the `triage.response` field of the output object, and the value of the decision is either `no`, `email`, or `notify`. Each of these decisions generates a separate data series in the chart.
   <img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=eb98a2c2c7988b5b6c5c3db9740ed172" alt="Decision at node" data-og-width="2578" width="2578" data-og-height="1692" height="1692" data-path="langsmith/images/decision-at-node.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=665c6cf0c3368c8f180db15cc7bf4cba 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e68f21187ec95e74e033850200893c66 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33f729b74f697b7b7554badb6c76cec0 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1ebed1b633a724ed5c9e2b93d62b1c2e 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8d3287007e200f37151955edd6cc2632 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/decision-at-node.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ee0d68172a80dce21c1f1afad533298d 2500w" />

Now we can visualize the decisions made at the `triage_input` node over time.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/VxsIvf9NdxI?si=7ksp9qyw-i0lcwxg" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/dashboards.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Multiple nodes can access and modify shared state

**URL:** llms-txt#multiple-nodes-can-access-and-modify-shared-state

class WorkflowState(TypedDict):
    user_input: str
    search_results: list
    generated_response: str
    validation_status: str

def search_node(state):
    # Access shared state
    results = search(state["user_input"])
    return {"search_results": results}

def validation_node(state):
    # Access results from previous node
    is_valid = validate(state["generated_response"])
    return {"validation_status": "valid" if is_valid else "invalid"}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**3. Parallel processing with synchronization**

When you need to run multiple operations in parallel and then combine their results, the Graph API handles this naturally.
```

---

## Must set dangerously_allow_filesystem to True if you want to use file paths

**URL:** llms-txt#must-set-dangerously_allow_filesystem-to-true-if-you-want-to-use-file-paths

@traceable(dangerously_allow_filesystem=True)
def trace_with_attachments(
    val: int,
    text: str,
    image: Attachment,
    audio: Attachment,
    video: Attachment,
    pdf: Attachment,
    csv: Attachment,
):
    return f"Processed: {val}, {text}, {len(image.data)}, {len(audio.data)}, {len(video.data)}, {len(pdf.data), {len(csv.data)}}"

---

## my_graph.py.

**URL:** llms-txt#my_graph.py.

**Contents:**
  - Opt-out of configurable headers

@contextlib.asynccontextmanager
async def generate_agent(config):
  organization_id = config["configurable"].get("x-organization-id")
  if organization_id == "org1":
    graph = ...
    yield graph
  else:
    graph = ...
    yield graph

json  theme={null}
{
  "graphs": {"agent": "my_grph.py:generate_agent"}
}
json  theme={null}
{
  "http": {
    "configurable_headers": {
      "excludes": ["*"]
    }
  }
}
```

This will exclude all headers from being added to your run's configuration.

Note that exclusions take precedence over inclusions.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/configurable-headers.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
### Opt-out of configurable headers

If you'd like to opt-out of configurable headers, you can simply set a wildcard pattern in the `s` list:
```

---

## Name of the dataset we want to create

**URL:** llms-txt#name-of-the-dataset-we-want-to-create

dataset_name = f'{project_name}-backtesting {start_time.strftime("%Y-%m-%d")}-{end_time.strftime("%Y-%m-%d")}'

---

## Name of the experiment we want to create from the historical runs

**URL:** llms-txt#name-of-the-experiment-we-want-to-create-from-the-historical-runs

baseline_experiment_name = f"prod-baseline-gpt-3.5-turbo-{str(uuid4())[:4]}"

---

## Next steps

**URL:** llms-txt#next-steps

Now that you've created a prompt, you can use it in your application code. See [how to pull a prompt programmatically](/langsmith/manage-prompts-programmatically#pull-a-prompt).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/create-a-prompt.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Nodes

**URL:** llms-txt#nodes

def orchestrator(state: State):
    """Orchestrator that generates a plan for the report"""

# Generate queries
    report_sections = planner.invoke(
        [
            SystemMessage(content="Generate a plan for the report."),
            HumanMessage(content=f"Here is the report topic: {state['topic']}"),
        ]
    )

return {"sections": report_sections.sections}

def llm_call(state: WorkerState):
    """Worker writes a section of the report"""

# Generate section
    section = llm.invoke(
        [
            SystemMessage(
                content="Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting."
            ),
            HumanMessage(
                content=f"Here is the section name: {state['section'].name} and description: {state['section'].description}"
            ),
        ]
    )

# Write the updated section to completed sections
    return {"completed_sections": [section.content]}

def synthesizer(state: State):
    """Synthesize full report from sections"""

# List of completed sections
    completed_sections = state["completed_sections"]

# Format completed section to str to use as context for final sections
    completed_report_sections = "\n\n---\n\n".join(completed_sections)

return {"final_report": completed_report_sections}

---

## node_2 accepts private data from node_1, whereas

**URL:** llms-txt#node_2-accepts-private-data-from-node_1,-whereas

---

## Node 2 input only requests the private data available after node_1

**URL:** llms-txt#node-2-input-only-requests-the-private-data-available-after-node_1

class Node2Input(TypedDict):
    private_data: str

def node_2(state: Node2Input) -> OverallState:
    output = {"a": "set by node_2"}
    print(f"Entered node `node_2`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## node_3 does not see the private data.

**URL:** llms-txt#node_3-does-not-see-the-private-data.

builder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])
builder.add_edge(START, "node_1")
graph = builder.compile()

---

## Node 3 only has access to the overall state (no access to private data from node_1)

**URL:** llms-txt#node-3-only-has-access-to-the-overall-state-(no-access-to-private-data-from-node_1)

def node_3(state: OverallState) -> OverallState:
    output = {"a": "set by node_3"}
    print(f"Entered node `node_3`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## Node for routing.

**URL:** llms-txt#node-for-routing.

async def intent_classifier(
    state: State,
) -> Command[Literal["refund_agent", "question_answering_agent"]]:
    response = router_llm.invoke(
        [{"role": "system", "content": route_instructions}, *state["messages"]]
    )
    return Command(goto=response["intent"] + "_agent")

---

## Node that updates instructions

**URL:** llms-txt#node-that-updates-instructions

**Contents:**
  - Writing memories
  - Memory storage

def update_instructions(state: State, store: BaseStore):
    namespace = ("instructions",)
    instructions = store.search(namespace)[0]
    # Memory logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"], conversation=state["messages"])
    output = llm.invoke(prompt)
    new_instructions = output['new_instructions']
    store.put(("agent_instructions",), "agent_a", {"instructions": new_instructions})
    ...
python  theme={null}
from langgraph.store.memory import InMemoryStore

def embed(texts: list[str]) -> list[list[float]]:
    # Replace with an actual embedding function or LangChain embeddings object
    return [[1.0, 2.0] * len(texts)]

**Examples:**

Example 1 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=13644c954ed79a45b8a1a762b3e39da1" alt="Update instructions" data-og-width="493" width="493" data-og-height="515" height="515" data-path="oss/images/update-instructions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=90632c71febee5777be6ae2c338f0880 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=aefcc771a030a2d6a89f815b87e60fd4 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=9115490b76daffe987e3867bc9176386 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=0df26f2e6f669f2fbea59a9a49482fb4 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=132e7b1f377e0b57c03ab31c6d788df4 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/update-instructions.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=651cd1bb14e445a972a671a196b6a893 2500w" />

### Writing memories

There are two primary methods for agents to write memories: ["in the hot path"](#in-the-hot-path) and ["in the background"](#in-the-background).

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=edd006d6189dc29a2edcba57c41fd744" alt="Hot path vs background" data-og-width="842" width="842" data-og-height="418" height="418" data-path="oss/images/hot_path_vs_background.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=3efd9962012347a64b596d1d36925b33 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=add54b5469d7b4a8f22d7da250c19ddf 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1e763d0f2ee8aa4f5b302ad44bc19d2f 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ce84a68af250e53a4693332d39179136 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5f807accb9c63dae57c27c9a1d17f29a 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/hot_path_vs_background.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=ecef974859d58f691dbb22a4a1cc1572 2500w" />

#### In the hot path

Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.

However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.

As an example, ChatGPT uses a [save\_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/) tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our [memory-agent](https://github.com/langchain-ai/memory-agent) template as an reference implementation.

#### In the background

Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.

However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.

See our [memory-service](https://github.com/langchain-ai/memory-template) template as an reference implementation.

### Memory storage

LangGraph stores long-term memories as JSON documents in a [store](/oss/python/langgraph/persistence#memory-store). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.
```

---

## Node that *uses* the instructions

**URL:** llms-txt#node-that-*uses*-the-instructions

def call_model(state: State, store: BaseStore):
    namespace = ("agent_instructions", )
    instructions = store.get(namespace, key="agent_a")[0]
    # Application logic
    prompt = prompt_template.format(instructions=instructions.value["instructions"])
    ...

---

## Note that here we inspect the runtime config for an "env" variable.

**URL:** llms-txt#note-that-here-we-inspect-the-runtime-config-for-an-"env"-variable.

---

## Note that providers implement different scores; the score here

**URL:** llms-txt#note-that-providers-implement-different-scores;-the-score-here

---

## Note that we're (optionally) passing the memory when compiling the graph

**URL:** llms-txt#note-that-we're-(optionally)-passing-the-memory-when-compiling-the-graph

**Contents:**
  - Create a dataset
  - Create an evaluator
  - Run evaluations

app = workflow.compile()
python  theme={null}
from langsmith import Client

questions = [
    "what's the weather in sf",
    "whats the weather in san fran",
    "whats the weather in tangier"
]

answers = [
    "It's 60 degrees and foggy.",
    "It's 60 degrees and foggy.",
    "It's 90 degrees and sunny.",
]

ls_client = Client()
dataset = ls_client.create_dataset(
    "weather agent",
    inputs=[{"question": q} for q in questions],
    outputs=[{"answers": a} for a in answers],
)
python  theme={null}
judge_llm = init_chat_model("gpt-4o")

async def correct(outputs: dict, reference_outputs: dict) -> bool:
    instructions = (
        "Given an actual answer and an expected answer, determine whether"
        " the actual answer contains all of the information in the"
        " expected answer. Respond with 'CORRECT' if the actual answer"
        " does contain all of the expected information and 'INCORRECT'"
        " otherwise. Do not include anything else in your response."
    )
    # Our graph outputs a State dictionary, which in this case means
    # we'll have a 'messages' key and the final message should
    # be our actual answer.
    actual_answer = outputs["messages"][-1].content
    expected_answer = reference_outputs["answer"]
    user_msg = (
        f"ACTUAL ANSWER: {actual_answer}"
        f"\n\nEXPECTED ANSWER: {expected_answer}"
    )
    response = await judge_llm.ainvoke(
        [
            {"role": "system", "content": instructions},
            {"role": "user", "content": user_msg}
        ]
    )
    return response.content.upper() == "CORRECT"
python  theme={null}
from langsmith import aevaluate

def example_to_state(inputs: dict) -> dict:
  return {"messages": [{"role": "user", "content": inputs['question']}]}

**Examples:**

Example 1 (unknown):
```unknown
### Create a dataset

Let's create a simple dataset of questions and expected responses:
```

Example 2 (unknown):
```unknown
### Create an evaluator

And a simple evaluator:

Requires `langsmith>=0.2.0`
```

Example 3 (unknown):
```unknown
### Run evaluations

Now we can run our evaluations and explore the results. We'll just need to wrap our graph function so that it can take inputs in the format they're stored on our example:

<Note>
  If all of your graph nodes are defined as sync functions then you can use `evaluate` or `aevaluate`. If any of you nodes are defined as async, you'll need to use `aevaluate`
</Note>

Requires `langsmith>=0.2.0`
```

---

## NOTE: there are no edges between nodes A, B and C!

**URL:** llms-txt#note:-there-are-no-edges-between-nodes-a,-b-and-c!

**Contents:**
  - Navigate to a node in a parent graph
  - Use inside tools
- Visualize your graph
  - Mermaid
  - PNG

graph = builder.compile()
python  theme={null}
from IPython.display import display, Image

display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
graph.invoke({"foo": ""})

Called A
Called C
python  theme={null}
def my_node(state: State) -> Command[Literal["my_other_node"]]:
    return Command(
        update={"foo": "bar"},
        goto="other_subgraph",  # where `other_subgraph` is a node in the parent graph
        graph=Command.PARENT
    )
python  theme={null}
import operator
from typing_extensions import Annotated

class State(TypedDict):
    # NOTE: we define a reducer here
    foo: Annotated[str, operator.add]  # [!code highlight]

def node_a(state: State):
    print("Called A")
    value = random.choice(["a", "b"])
    # this is a replacement for a conditional edge function
    if value == "a":
        goto = "node_b"
    else:
        goto = "node_c"

# note how Command allows you to BOTH update the graph state AND route to the next node
    return Command(
        update={"foo": value},
        goto=goto,
        # this tells LangGraph to navigate to node_b or node_c in the parent graph
        # NOTE: this will navigate to the closest parent graph relative to the subgraph
        graph=Command.PARENT,  # [!code highlight]
    )

subgraph = StateGraph(State).add_node(node_a).add_edge(START, "node_a").compile()

def node_b(state: State):
    print("Called B")
    # NOTE: since we've defined a reducer, we don't need to manually append
    # new characters to existing 'foo' value. instead, reducer will append these
    # automatically (via operator.add)
    return {"foo": "b"}  # [!code highlight]

def node_c(state: State):
    print("Called C")
    return {"foo": "c"}  # [!code highlight]

builder = StateGraph(State)
builder.add_edge(START, "subgraph")
builder.add_node("subgraph", subgraph)
builder.add_node(node_b)
builder.add_node(node_c)

graph = builder.compile()
python  theme={null}
graph.invoke({"foo": ""})

Called A
Called C
python  theme={null}
@tool
def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):
    """Use this to look up user information to better assist them with their questions."""
    user_info = get_user_info(config.get("configurable", {}).get("user_id"))
    return Command(
        update={
            # update the state keys
            "user_info": user_info,
            # update the message history
            "messages": [ToolMessage("Successfully looked up user information", tool_call_id=tool_call_id)]
        }
    )
python  theme={null}
import random
from typing import Annotated, Literal
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]

class MyNode:
    def __init__(self, name: str):
        self.name = name
    def __call__(self, state: State):
        return {"messages": [("assistant", f"Called node {self.name}")]}

def route(state) -> Literal["entry_node", END]:
    if len(state["messages"]) > 10:
        return END
    return "entry_node"

def add_fractal_nodes(builder, current_node, level, max_level):
    if level > max_level:
        return
    # Number of nodes to create at this level
    num_nodes = random.randint(1, 3)  # Adjust randomness as needed
    for i in range(num_nodes):
        nm = ["A", "B", "C"][i]
        node_name = f"node_{current_node}_{nm}"
        builder.add_node(node_name, MyNode(node_name))
        builder.add_edge(current_node, node_name)
        # Recursively add more nodes
        r = random.random()
        if r > 0.2 and level + 1 < max_level:
            add_fractal_nodes(builder, node_name, level + 1, max_level)
        elif r > 0.05:
            builder.add_conditional_edges(node_name, route, node_name)
        else:
            # End
            builder.add_edge(node_name, END)

def build_fractal_graph(max_level: int):
    builder = StateGraph(State)
    entry_point = "entry_node"
    builder.add_node(entry_point, MyNode(entry_point))
    builder.add_edge(START, entry_point)
    add_fractal_nodes(builder, entry_point, 1, max_level)
    # Optional: set a finish point if required
    builder.add_edge(entry_point, END)  # or any specific node
    return builder.compile()

app = build_fractal_graph(3)
python  theme={null}
print(app.get_graph().draw_mermaid())

%%{init: {'flowchart': {'curve': 'linear'}}}%%
graph TD;
    tart__([<p>__start__</p>]):::first
    ry_node(entry_node)
    e_entry_node_A(node_entry_node_A)
    e_entry_node_B(node_entry_node_B)
    e_node_entry_node_B_A(node_node_entry_node_B_A)
    e_node_entry_node_B_B(node_node_entry_node_B_B)
    e_node_entry_node_B_C(node_node_entry_node_B_C)
    nd__([<p>__end__</p>]):::last
    tart__ --> entry_node;
    ry_node --> __end__;
    ry_node --> node_entry_node_A;
    ry_node --> node_entry_node_B;
    e_entry_node_B --> node_node_entry_node_B_A;
    e_entry_node_B --> node_node_entry_node_B_B;
    e_entry_node_B --> node_node_entry_node_B_C;
    e_entry_node_A -.-> entry_node;
    e_entry_node_A -.-> __end__;
    e_node_entry_node_B_A -.-> entry_node;
    e_node_entry_node_B_A -.-> __end__;
    e_node_entry_node_B_B -.-> entry_node;
    e_node_entry_node_B_B -.-> __end__;
    e_node_entry_node_B_C -.-> entry_node;
    e_node_entry_node_B_C -.-> __end__;
    ssDef default fill:#f2f0ff,line-height:1.2
    ssDef first fill-opacity:0
    ssDef last fill:#bfb6fc
python  theme={null}
from IPython.display import Image, display
from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles

display(Image(app.get_graph().draw_mermaid_png()))
python  theme={null}
import nest_asyncio

nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions

display(
    Image(
        app.get_graph().draw_mermaid_png(
            curve_style=CurveStyle.LINEAR,
            node_colors=NodeStyles(first="#ffdfba", last="#baffc9", default="#fad7de"),
            wrap_label_n_words=9,
            output_file_path=None,
            draw_method=MermaidDrawMethod.PYPPETEER,
            background_color="white",
            padding=10,
        )
    )
)
python  theme={null}
try:
    display(Image(app.get_graph().draw_png()))
except ImportError:
    print(
        "You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt"
    )
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-graph-api.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Warning>
  You might have noticed that we used [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) as a return type annotation, e.g. `Command[Literal["node_b", "node_c"]]`. This is necessary for the graph rendering and tells LangGraph that `node_a` can navigate to `node_b` and `node_c`.
</Warning>
```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f11e5cddedbf2760d40533f294c44aea" alt="Command-based graph navigation" data-og-width="232" width="232" data-og-height="333" height="333" data-path="oss/images/graph_api_image_11.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c1b27d92b257a6c4ac57f34f007d0ee1 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=695d0062e5fb8ebea5525379edbba476 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7bd3f779df628beba60a397674f85b59 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=85a9194e8b4d9df2d01d10784dcf75d0 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=efd9118d4bcd6d1eb92760c573645fbd 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_11.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=1eb2a132386a64d18582af6978e4ac24 2500w" />

If we run the graph multiple times, we'd see it take different paths (A -> B or A -> C) based on the random choice in node A.
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
### Navigate to a node in a parent graph

If you are using [subgraphs](/oss/python/langgraph/use-subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:
```

---

## Note: This example requires the `requests` and `requests_toolbelt` libraries.

**URL:** llms-txt#note:-this-example-requires-the-`requests`-and-`requests_toolbelt`-libraries.

---

## No longer supported

**URL:** llms-txt#no-longer-supported

model_with_tools = ChatOpenAI().bind_tools([some_tool])
agent = create_agent(model_with_tools, tools=[])

---

## null

**URL:** llms-txt#null

**Contents:**
- Interface

Source: https://docs.langchain.com/oss/python/integrations/document_loaders/index

Document loaders provide a **standard interface** for reading data from different sources (such as Slack, Notion, or Google Drive) into LangChain’s [Document](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) format.
This ensures that data can be handled consistently regardless of the source.

All document loaders implement the [`BaseLoader`](https://reference.langchain.com/python/langchain_core/document_loaders/#langchain_core.document_loaders.BaseLoader) interface.

Each document loader may define its own parameters, but they share a common API:

* `load()` – Loads all documents at once.
* `lazy_load()` – Streams documents lazily, useful for large datasets.

```python  theme={null}
from langchain_community.document_loaders.csv_loader import CSVLoader

loader = CSVLoader(
    ...  # Integration-specific parameters here
)

---

## Oauth Callback

**URL:** llms-txt#oauth-callback

Source: https://docs.langchain.com/api-reference/auth-service-v2/oauth-callback

https://api.host.langchain.com/openapi.json post /v2/auth/callback/{provider_id}

---

## Observability concepts

**URL:** llms-txt#observability-concepts

**Contents:**
- Runs
- Traces
- Threads
- Projects
- Feedback
- Tags
- Metadata
- Data storage and retention
- Deleting traces from LangSmith

Source: https://docs.langchain.com/langsmith/observability-concepts

This page covers key concepts that are important to understand when logging traces to LangSmith.

A [*trace*](#traces) records the sequence of steps your application takes—from receiving an input, through intermediate processing, to producing a final output. Each step within a trace is represented by a [*run*](#runs). Multiple traces are grouped together within a [*project*](#projects), and traces from multi-turn conversations can be linked together as a [*thread*](#threads).

The following diagram displays these concepts in the context of a simple RAG app, which retrieves documents from an index and generates an answer.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=50c5f4d966f8fe4f8ae0be0beaf11bc4" alt="Primitives of LangSmith Project, Trace, Run in the context of a question and answer RAG app." data-og-width="2701" width="2701" data-og-height="1739" height="1739" data-path="langsmith/images/primitives.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=280&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=e0b6083af11ec78c1650c907a6b8649a 280w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=560&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=9054d8620c453c520e161c1ba8fb1fdc 560w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=840&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=8ece2e0b84019b722446e9bfdbf067f9 840w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=1100&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=c4e3700fe862b539954b8c6c0124bfac 1100w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=1650&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=f9dfe46dbb576c1faeb1f4ad52324527 1650w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives.png?w=2500&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=9137d815270a89f9d4df799381a88aac 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=3ca35a8a6cec65b9a5139e7ac8cac470" alt="Primitives of LangSmith Project, Trace, Run in the context of a question and answer RAG app." data-og-width="2919" width="2919" data-og-height="1752" height="1752" data-path="langsmith/images/primitives-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=280&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=41bff3be3eabb386bd11347b77908625 280w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=560&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=1e7d10f532b2a099ca74cd80f1f0f90d 560w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=840&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=10e2a103221b086ed8985a8478c9992c 840w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=1100&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=a82db1fc02ffd0db0222ed0a14e48a86 1100w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=1650&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=70a1cd6dd3f30b5863bba85e92a30733 1650w, https://mintcdn.com/langchain-5e9cc07a/Tf5b6pnNY9Uj6Vtl/langsmith/images/primitives-dark.png?w=2500&fit=max&auto=format&n=Tf5b6pnNY9Uj6Vtl&q=85&s=33a6bfeb3258ced236732b0be95b6ab3 2500w" />
</div>

A *run* is a span representing a single unit of work or operation within your LLM application. This could be anything from a single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a run as a span.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=a692d614eb441aef6e3a1f02f4a37e8a" alt="Run" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=911a09dc4da0d94015cc2a22f95efce6 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=6ae06bcbba3e3b240cfa41950f4e451d 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=2e886c505427057c726f30b495fc7c4c 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=48dfe76850483f46da1bbe4bcccb5f8b 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=eff65c5910eaa68456e26712dfa64d1d 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/run.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=d99e6ba31cd3f4039ad86b0ade0b78b4 2500w" />

A *trace* is a collection of runs for a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with [OpenTelemetry](https://opentelemetry.io/), you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dd692f6433fbcf0413a4516c170062f2" alt="Trace" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fea7f898d3f25c76bed212af157b1a1d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5a75129cb5ab5fab0a6618d0c3160ec2 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3102493a51a280b38004ff8628231ad6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f0e4d194f6a6b18b4505a2b4a4f40fc3 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d1f0bb5ae6e5dec024c5e3726b848394 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f5d14619201af89c134fd1fb99b7cdf0 2500w" />

A *thread* is a sequence of traces representing a single conversation. Many LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. Each turn in the conversation is represented as its own trace, but these traces are linked together by being part of the same thread. The most recent trace in a thread is the latest message exchange.

To group traces into threads, you pass a special metadata key (`session_id`, `thread_id`, or `conversation_id`) with a unique identifier value that links the traces together.

[Learn how to configure threads](/langsmith/threads).

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f7af4c3904073d5f58f28c656603ca19" alt="Thread representing a sequence of traces in a multi-turn conversation." data-og-width="1273" width="1273" data-og-height="757" height="757" data-path="langsmith/images/thread-overview-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cd769088ab3ab2dae09982915f23772d 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=70ae6b5a6b8edb83ba3604d4c6e0262e 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=61d89d8077072221373490edac65363c 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=ad8159fe12f056dbc561c612e3797b97 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=3611f7bcc95c45bcb91c093ca36ef348 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-light.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=1c0426d7e83562e1d76e079959bda186 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=f738de4cac932ed2b8657e8f3b706b77" alt="Thread representing a sequence of traces in a multi-turn conversation." data-og-width="1273" width="1273" data-og-height="753" height="753" data-path="langsmith/images/thread-overview-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=280&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=9bc9dd49c63661dceb981899c5f0332b 280w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=560&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=01713d47cf762f99be1a1143b01582e8 560w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=840&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=cfc77e449d0ce27cdfa51b2f7c6ed655 840w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1100&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d84f671a8f2c1207dbb98c72a37d1832 1100w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=1650&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=d592bd4c8671b3d7f19a49471888a901 1650w, https://mintcdn.com/langchain-5e9cc07a/zLS2qlRr5r04zU3G/langsmith/images/thread-overview-dark.png?w=2500&fit=max&auto=format&n=zLS2qlRr5r04zU3G&q=85&s=2405eaef2af227dd5e5efac85fc9e623 2500w" />

<Callout type="info" icon="bird">
  Use **[Polly](/langsmith/polly)** to analyze traces, runs, and threads. Polly helps you understand agent performance, debug issues, and gain insights from conversation threads without manually digging through data.
</Callout>

A *project* is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2426200ab2e619674636e41f11246c0d" alt="Project" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/project.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=97510cd2501c9d2dc529680172664219 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8624fbec9c58a1999d702aed79a606f6 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5d918966500e84ce29550591755d98f6 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d9a6a4b2365de4e4cf80506d3fc0a467 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=11761179be51c8cfc58193d6d0215269 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=779669a7f2f2106946d1f7eb3f427ed5 2500w" />

*Feedback* allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization.

You can collect feedback on runs in a number of ways:

1. [Sent up along with a trace](/langsmith/attach-user-feedback) from the LLM application.
2. Generated by a user in the app [inline](/langsmith/annotate-traces-inline) or in an [annotation queue](/langsmith/annotation-queues).
3. Generated by an automatic evaluator during [offline evaluation](/langsmith/evaluate-llm-application).
4. Generated by an [online evaluator](/langsmith/online-evaluations).

To learn more about how feedback is stored in the application, refer to the [Feedback data format guide](/langsmith/feedback-data-format).

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=13fa69590caa5ba050e3cda9dbb6a336" alt="Feedback" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ef7168bb4e77c0b70f5cc6b7062be002 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=75059818a510d95abea177fb5e3a1b4e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a29438f9e3126eb0aa01023e02c0b621 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=904b4e5e7aa4dcfd49121b82fb269c5f 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=f5d68cfe719b0cc0a9d5e2b0fefb12ae 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/feedback.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3be4d77ff49c91b49effcceeb96b38d3 2500w" />

*Tags* are collections of strings that can be attached to runs. You can use tags to do the following in the LangSmith UI:

* Categorize runs for easier search.
* Filter runs.
* Group runs together for analysis.

[Learn how to attach tags to your traces](/langsmith/add-metadata-tags).

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=44e6b6242fd76e811dcc28d88c5c6db5" alt="Tags" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/tags.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=35d294b4a295130dfb7f23a8237fe53d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ab7e4b06eced56da2b51eb9f88738e6f 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ecd67b36985d21a275b3b6aa34922a84 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dfc774fcd984d08202f178eb75cc14e6 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9e5c4c571cbaf45fdab26f21e854bc29 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tags.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b792075475d671a84e3efb1ca47d527b 2500w" />

*Metadata* is a collection of key-value pairs that you can attach to runs. You can use metadata to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similarly to tags, you can use metadata to filter runs in the LangSmith UI or group runs together for analysis.

[Learn how to add metadata to your traces](/langsmith/add-metadata-tags).

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=e48730c02b7974035bbb312734e86a92" alt="Metadata" data-og-width="1830" width="1830" data-og-height="1527" height="1527" data-path="langsmith/images/metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6fbe88a76fd74d9fb36e89465ef068cc 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7e3c71b02884707ca26863a88e4d04f6 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8799298d81f72725c01a937c549fdb81 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6dbb9c568c187cbd8bd14a471f6977b3 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=be48eaaa028a65c6ca6698c27cf9da6b 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/metadata.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=810f9462214e87aba38b724a9b5083af 2500w" />

## Data storage and retention

For traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database.

After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata retained for the purpose of showing accurate statistics, such as historic usage and cost.

<Note>
  If you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A [dataset](/langsmith/manage-datasets) allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.
</Note>

## Deleting traces from LangSmith

If you need to remove a trace from LangSmith before its expiration date, you can do so by deleting the project that contains it.

You can delete a project with one of the following ways:

* In the [LangSmith UI](https://smith.langchain.com), select the **Delete** option on the project's overflow menu.
* With the [`delete_tracer_sessions`](https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/delete_tracer_session_api_v1_sessions__session_id__delete) API endpoint
* With the `delete_project()` ([Python](https://reference.langchain.com/python/langsmith/observability/sdk/)) or `deleteProject()` ([JS/TS](https://reference.langchain.com/javascript/modules/langsmith.html)) in the LangSmith SDK.

LangSmith does not support self-service deletion of individual traces.

If you have a need to delete a single trace (or set of traces) from LangSmith project before its expiration date, the account owner should contact support via [LangSmith Support](https://support.langchain.com) with the organization ID and trace IDs.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-concepts.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Observability in Studio

**URL:** llms-txt#observability-in-studio

**Contents:**
- Iterate on prompts
  - Direct node editing
  - Graph configuration
  - Playground
- Run experiments over a dataset
  - Prerequisites
  - Experiment setup
- Debug LangSmith traces
  - Open deployed threads
  - Testing local agents with remote traces

Source: https://docs.langchain.com/langsmith/observability-studio

LangSmith [Studio](/langsmith/studio) provides tools to inspect, debug, and improve your app beyond execution. By working with traces, datasets, and prompts, you can see how your application behaves in detail, measure its performance, and refine its outputs:

* [Iterate on prompts](#iterate-on-prompts): Modify prompts inside graph nodes directly or with the LangSmith playground.
* [Run experiments over a dataset](#run-experiments-over-a-dataset): Execute your assistant over a LangSmith dataset to score and compare results.
* [Debug LangSmith traces](#debug-langsmith-traces): Import traced runs into Studio and optionally clone them into your local agent.
* [Add a node to a dataset](#add-node-to-dataset): Turn parts of thread history into dataset examples for evaluation or further analysis.

## Iterate on prompts

Studio supports the following methods for modifying prompts in your graph:

* [Direct node editing](#direct-node-editing)
* [Playground interface](#playground)

### Direct node editing

Studio allows you to edit prompts used inside individual nodes, directly from the graph interface.

### Graph configuration

Define your [configuration](/oss/python/langgraph/use-graph-api#add-runtime-configuration) to specify prompt fields and their associated nodes using `langgraph_nodes` and `langgraph_type` keys.

#### `langgraph_nodes`

* **Description**: Specifies which nodes of the graph a configuration field is associated with.
* **Value Type**: Array of strings, where each string is the name of a node in your graph.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:

#### `langgraph_type`

* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.
* **Value Type**: String
* **Supported Values**:
  * `"prompt"`: Indicates the field contains prompt text that should be treated specially in the UI.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:

<Accordion title="Full example configuration">
  
</Accordion>

#### Editing prompts in the UI

1. Locate the gear icon on nodes with associated configuration fields.
2. Click to open the configuration modal.
3. Edit the values.
4. Save to update the current assistant version or create a new one.

The [playground](/langsmith/create-a-prompt) interface allows testing individual LLM calls without running the full graph:

1. Select a thread.
2. Click **View LLM Runs** on a node. This lists all the LLM calls (if any) made inside the node.
3. Select an LLM run to open in the playground.
4. Modify prompts and test different model and tool settings.
5. Copy updated prompts back to your graph.

## Run experiments over a dataset

Studio lets you run [evaluations](/langsmith/evaluation-concepts) by executing your assistant against a predefined LangSmith [dataset](/langsmith/evaluation-concepts#datasets). This allows you to test performance across a variety of inputs, compare outputs to reference answers, and score results with configured [evaluators](/langsmith/evaluation-concepts#evaluators).

This guide shows you how to run a full end-to-end experiment directly from Studio.

Before running an experiment, ensure you have the following:

* **A LangSmith dataset**: Your dataset should contain the inputs you want to test and optionally, reference outputs for comparison. The schema for the inputs must match the required input schema for the assistant. For more information on schemas, see [here](/oss/python/langgraph/use-graph-api#schema). For more on creating datasets, refer to [How to Manage Datasets](/langsmith/manage-datasets-in-application#set-up-your-dataset).
* **(Optional) Evaluators**: You can attach evaluators (e.g., LLM-as-a-Judge, heuristics, or custom functions) to your dataset in LangSmith. These will run automatically after the graph has processed all inputs.
* **A running application**: The experiment can be run against:
  * An application deployed on [LangSmith](/langsmith/deployments).
  * A locally running application started via the [langgraph-cli](/langsmith/local-server).

<Note>
  Studio experiments follow the same [data retention](/langsmith/administration-overview#data-retention) rules as other experiments. By default, traces have base tier retention (14 days). However, traces will automatically upgrade to extended tier retention (400 days) if feedback is added to them. Feedback can be added in one of two ways:

* The [dataset has evaluators configured](/langsmith/bind-evaluator-to-dataset).
  * [Feedback](/langsmith/observability-concepts#feedback) is manually added to a trace.

This auto-upgrade increases both the retention period and the cost of the trace. For more details, refer to [Data retention auto-upgrades](/langsmith/administration-overview#how-it-works).
</Note>

1. Launch the experiment. Click the **Run experiment** button in the top right corner of the Studio page.
2. Select your dataset. In the modal that appears, select the dataset (or a specific dataset split) to use for the experiment and click **Start**.
3. Monitor the progress. All of the inputs in the dataset will now be run against the active assistant. Monitor the experiment's progress via the badge in the top right corner.
4. You can continue to work in Studio while the experiment runs in the background. Click the arrow icon button at any time to navigate to LangSmith and view the detailed experiment results.

## Debug LangSmith traces

This guide explains how to open LangSmith traces in Studio for interactive investigation and debugging.

### Open deployed threads

1. Open the LangSmith trace, selecting the root run.
2. Click **Run in Studio**.

This will open Studio connected to the associated deployment with the trace's parent thread selected.

### Testing local agents with remote traces

This section explains how to test a local agent against remote traces from LangSmith. This enables you to use production traces as input for local testing, allowing you to debug and verify agent modifications in your development environment.

* A LangSmith traced thread
* A [locally running agent](/langsmith/local-server#local-development-server).

<Info>
  **Local agent requirements**

* langgraph>=0.3.18
  * langgraph-api>=0.0.32
  * Contains the same set of nodes present in the remote trace
</Info>

1. Open the LangSmith trace, selecting the root run.
2. Click the dropdown next to **Run in Studio**.
3. Enter your local agent's URL.
4. Select **Clone thread locally**.
5. If multiple graphs exist, select the target graph.

A new thread will be created in your local agent with the thread history inferred and copied from the remote thread, and you will be navigated to Studio for your locally running application.

## Add node to dataset

Add [examples](/langsmith/evaluation-concepts#examples) to [LangSmith datasets](/langsmith/manage-datasets) from nodes in the thread log. This is useful to evaluate individual steps of the agent.

1. Select a thread.
2. Click **Add to Dataset**.
3. Select nodes whose input/output you want to add to a dataset.
4. For each selected node, select the target dataset to create the example in. By default a dataset for the specific assistant and node will be selected. If this dataset does not yet exist, it will be created.
5. Edit the example's input/output as needed before adding it to the dataset.
6. Select **Add to dataset** at the bottom of the page to add all selected nodes to their respective datasets.

For more details, refer to [How to evaluate an application's intermediate steps](/langsmith/evaluate-on-intermediate-steps).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-studio.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### `langgraph_type`

* **Description**: Specifies the type of configuration field, which determines how it's handled in the UI.
* **Value Type**: String
* **Supported Values**:
  * `"prompt"`: Indicates the field contains prompt text that should be treated specially in the UI.
* **Usage Context**: Include in the `json_schema_extra` dictionary for Pydantic models or the `metadata["json_schema_extra"]` dictionary for dataclasses.
* **Example**:
```

Example 2 (unknown):
```unknown
<Accordion title="Full example configuration">
```

---

## Ollama

**URL:** llms-txt#ollama

**Contents:**
- Model interfaces
- Other

Source: https://docs.langchain.com/oss/python/integrations/providers/ollama

This page covers all LangChain integrations with [Ollama](https://ollama.com/).

Ollama allows you to run open-source models (like [`gpt-oss`](https://ollama.com/library/gpt-oss)) locally.

For a complete list of supported models and variants, see the [Ollama model library](https://ollama.ai/library).

<Columns cols={2}>
  <Card title="ChatOllama" href="/oss/python/integrations/chat/ollama" cta="Get started" icon="message" arrow>
    Ollama chat models.
  </Card>

<Card title="OllamaEmbeddings" href="/oss/python/integrations/text_embedding/ollama" cta="Get started" icon="microsoft" arrow>
    Ollama embedding models.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="OllamaLLM" href="/oss/python/integrations/llms/ollama" cta="Get started" icon="i-cursor" arrow>
    (Legacy) Ollama text completion models.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/ollama.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## OpenAIEmbeddings

**URL:** llms-txt#openaiembeddings

**Contents:**
- Overview
  - Integration details
- Setup
  - Credentials

Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/openai

This will help you get started with OpenAIEmbeddings [embedding models](/oss/javascript/integrations/text_embedding) using LangChain. For detailed documentation on `OpenAIEmbeddings` features and configuration options, please refer to the [API reference](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html).

### Integration details

| Class                                                                                           | Package                                                                         | Local | [Py support](https://python.langchain.com/docs/integrations/text_embedding/openai/) |                                             Downloads                                             |                                             Version                                            |
| :---------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------ | :---: | :---------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |
| [OpenAIEmbeddings](https://api.js.langchain.com/classes/langchain_openai.OpenAIEmbeddings.html) | [@langchain/openai](https://api.js.langchain.com/modules/langchain_openai.html) |   ❌   |                                          ✅                                          | ![NPM - Downloads](https://img.shields.io/npm/dm/@langchain/openai?style=flat-square\&label=%20&) | ![NPM - Version](https://img.shields.io/npm/v/@langchain/openai?style=flat-square\&label=%20&) |

To access OpenAIEmbeddings embedding models you'll need to create an OpenAI account, get an API key, and install the `@langchain/openai` integration package.

Head to [platform.openai.com](https://platform.openai.com) to sign up to OpenAI and generate an API key. Once you've done this set the `OPENAI_API_KEY` environment variable:

If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
If you want to get automated tracing of your model calls you can also set your [LangSmith](https://docs.langchain.com/langsmith/home) API key by uncommenting below:
```

---

## OpenAI

**URL:** llms-txt#openai

**Contents:**
- Model interfaces
- Tools and toolkits
- Retrievers
- Document loaders
- Other

Source: https://docs.langchain.com/oss/python/integrations/providers/openai

This page covers all LangChain integrations with [OpenAI](https://en.wikipedia.org/wiki/OpenAI)

<Columns cols={2}>
  <Card title="ChatOpenAI" href="/oss/python/integrations/chat/openai" cta="Get started" icon="message" arrow>
    OpenAI chat models.
  </Card>

<Card title="AzureChatOpenAI" href="/oss/python/integrations/chat/azure_chat_openai" cta="Get started" icon="microsoft" arrow>
    Wrapper for OpenAI chat models hosted on Azure.
  </Card>

<Card title="OpenAIEmbeddings" href="/oss/python/integrations/text_embedding/openai" cta="Get started" icon="layer-group" arrow>
    OpenAI embedding models.
  </Card>

<Card title="AzureOpenAIEmbeddings" href="/oss/python/integrations/text_embedding/azure_openai" cta="Get started" icon="microsoft" arrow>
    Wrapper for OpenAI embedding models hosted on Azure.
  </Card>
</Columns>

## Tools and toolkits

<Columns cols={2}>
  <Card title="Dall-E Image Generator" href="/oss/python/integrations/tools/dalle_image_generator" cta="Get started" icon="image" arrow>
    Text-to-image generation using OpenAI's Dall-E models.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="ChatGPTPluginRetriever" href="/oss/python/integrations/retrievers/chatgpt-plugin" cta="Get started" icon="download" arrow>
    Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="ChatGPTLoader" href="/oss/python/integrations/document_loaders/chatgpt_loader" cta="Get started" icon="file" arrow>
    Load `conversations.json` from your ChatGPT data export folder.
  </Card>
</Columns>

<Columns cols={2}>
  <Card title="OpenAI" href="/oss/python/integrations/llms/openai" cta="Get started" icon="i-cursor" arrow>
    (Legacy) OpenAI text completion models.
  </Card>

<Card title="AzureOpenAI" href="/oss/python/integrations/llms/azure_openai" cta="Get started" icon="microsoft" arrow>
    Wrapper for (legacy) OpenAI text completion models hosted on Azure.
  </Card>

<Card title="Adapter" href="/oss/python/integrations/adapters/openai" cta="Get started" icon="arrows-left-right" arrow>
    Adapt LangChain models to OpenAI APIs.
  </Card>

<Card title="OpenAIModerationChain" href="https://python.langchain.com/v0.1/docs/guides/productionization/safety/moderation" cta="Get started" icon="link" arrow>
    Detect text that could be hateful, violent, etc.
  </Card>
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/openai.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Optimize a classifier

**URL:** llms-txt#optimize-a-classifier

**Contents:**
- The objective
- Getting started
- Set up automations
- Update the application
  - NEW CODE ###

Source: https://docs.langchain.com/langsmith/optimize-classifier

This tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.

In this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.

To get started, we will first set it up so that we send all traces to a specific project. We can do this by setting an environment variable:

We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.

We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.

Here's how we can invoke the application:

Here's how we can attach feedback after. We can collect feedback in two forms.

First, we can collect "positive" feedback - this is for examples that the model got right.

Next, we can focus on collecting feedback that corresponds to a "correction" to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.

## Set up automations

We can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.

The first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let's create a dataset called `classifier-github-issues` to add this data to.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e36c57f7e0e224ff1ea29bcfbe9891fc" alt="Optimization Negative" data-og-width="1033" width="1033" data-og-height="558" height="558" data-path="langsmith/images/class-optimization-neg.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4091f7ae7d447eab035b32b66788eaee 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9971db3fd8992d31d94b58095381e575 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=96f35883e764b0f207692ce1fba46d08 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f1aa5d6d49137a2f8d254ad2d327dabf 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c9d20165b113f53fe5638c66ff47eddb 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-neg.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0109460c47883c4225fd54115acadb2b 2500w" />

The second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to "Use Corrections". This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6485ca961ed1c29d33f25f75f90ba939" alt="Optimization Positive" data-og-width="1038" width="1038" data-og-height="506" height="506" data-path="langsmith/images/class-optimization-pos.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=07ee0c534c8d8ce3e34e7c17058af5c0 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d14fc77148d349e02b363af96f0752ad 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=56592428dbb8d34492bbbc2bee911500 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=81a4d0d24ca94ad2adb5b4463bb3f776 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=1c3086f1fa332b846e7b6f668084fada 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/class-optimization-pos.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=41ba1a118c1f220018e619edf2fecee2 2500w" />

## Update the application

We can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!

```python  theme={null}
### NEW CODE ###

**Examples:**

Example 1 (unknown):
```unknown
We can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.
```

Example 2 (unknown):
```unknown
We can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.

Here's how we can invoke the application:
```

Example 3 (unknown):
```unknown
Here's how we can attach feedback after. We can collect feedback in two forms.

First, we can collect "positive" feedback - this is for examples that the model got right.
```

Example 4 (unknown):
```unknown
Next, we can focus on collecting feedback that corresponds to a "correction" to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.
```

---

## Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.

**URL:** llms-txt#optionally-add-the-'traceable'-decorator-to-trace-the-inputs/outputs-of-this-function.

**Contents:**
- UI
  - Pre-built evaluators
- Customize your LLM-as-a-judge evaluator
  - Select/create the evaluator
  - Configure the evaluator
  - Save the evaluator

@traceable
def dummy_app(inputs: dict) -> dict:
    return {"answer": "hmm i'm not sure", "reasoning": "i didn't understand the question"}

ls_client = Client()
dataset = ls_client.create_dataset("big questions")
examples = [
    {"inputs": {"question": "how will the universe end"}},
    {"inputs": {"question": "are we alone"}},
]
ls_client.create_examples(dataset_id=dataset.id, examples=examples)

results = evaluate(
    dummy_app,
    data=dataset,
    evaluators=[valid_reasoning]
)
```

See [here](/langsmith/code-evaluator) for more on how to write a custom evaluator.

### Pre-built evaluators

Pre-built evaluators are a useful starting point when setting up evaluations. The LangSmith UI supports the following pre-built evaluators:

* **Hallucination**: Detect factually incorrect outputs. Requires a reference output.
* **Correctness**: Check semantic similarity to a reference.
* **Conciseness**: Evaluate whether an answer is a concise response to a question.
* **Code checker**: Verify correctness of code answers.

You can configure these evaluators::

* When running an evaluation using the [playground](/langsmith/observability-concepts#prompt-playground)
* As part of a dataset to [automatically run evaluations on experiments](/langsmith/bind-evaluator-to-dataset)
* When running an [online evaluation](/langsmith/online-evaluations#configure-llm-as-judge-evaluators)

## Customize your LLM-as-a-judge evaluator

Add specific instructions for your LLM-as-a-judge evalutor prompt and configure which parts of the input/output/reference output should be passed to the evaluator.

### Select/create the evaluator

* In the playground or from a dataset: Select the **+Evaluator** button
* From a tracing project: Select **Add rules**, configure your rule and select **Apply evaluator**

Select the **Create your own evaluator option**. Alternatively, you may start by selecting a pre-built evaluator and editing it.

### Configure the evaluator

Create a new prompt, or choose an existing prompt from the [prompt hub](/langsmith/prompt-engineering-quickstart).

* **Create your own prompt**: Create a custom prompt inline.

* **Pull a prompt from the prompt hub**: Use the **Select a prompt** dropdown to select from an existing prompt. You can't edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.

Select the desired model from the provided options.

#### Mapping variables

Use variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, an example (or run) is provided for reference. Click on the the variables in your prompt and use the dropdown to map them to the relevant parts of the input, output, or reference output.

To add prompt variables type the variable with double curly brackets `{{prompt_var}}` if using mustache formatting (the default) or single curly brackets `{prompt_var}` if using f-string formatting.

You may remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically don't need a reference output so you may remove that variable.

Previewing the prompt will show you of what the formatted prompt will look like using the reference run and dataset example shown on the right.

#### Improve your evaluator with few-shot examples

To better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect [human corrections](/langsmith/create-few-shot-evaluators#make-corrections) on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.

Learn [how to set up few-shot examples and make corrections](/langsmith/create-few-shot-evaluators).

#### Feedback configuration

Feedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as [feedback](/langsmith/observability-concepts#feedback) to a run or example. Defining feedback for your evaluator:

1. **Name the feedback key**: This is the name that will appear when viewing evaluation results. Names should be unique across experiments.

2. **Add a description**: Describe what the feedback represents.

3. **Choose a feedback type**:

* **Boolean**: True/false feedback.
* **Categorical**: Select from predefined categories.
* **Continuous**: Numerical scoring within a specified range.

Behind the scenes, feedback configuration is added as [structured output](https://python.langchain.com/docs/concepts/structured_outputs/) to the LLM-as-a-judge prompt. If you're using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.

### Save the evaluator

Once your are finished configuring, save your changes.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/llm-as-judge.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Optionally wrap the OpenAI client to trace all model calls.

**URL:** llms-txt#optionally-wrap-the-openai-client-to-trace-all-model-calls.

oai_client = wrappers.wrap_openai(OpenAI())

def valid_reasoning(inputs: dict, outputs: dict) -> bool:
    """Use an LLM to judge if the reasoning and the answer are consistent."""
    instructions = """
Given the following question, answer, and reasoning, determine if the reasoning
for the answer is logically valid and consistent with the question and the answer."""

class Response(BaseModel):
        reasoning_is_valid: bool

msg = f"Question: {inputs['question']}\nAnswer: {outputs['answer']}\nReasoning: {outputs['reasoning']}"
    response = oai_client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[{"role": "system", "content": instructions,}, {"role": "user", "content": msg}],
        response_format=Response
    )
    return response.choices[0].message.parsed.reasoning_is_valid

---

## Optional: OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true

**URL:** llms-txt#optional:-otel_python_logging_auto_instrumentation_enabled=true

**Contents:**
- `DD_API_KEY`
- `LANGCHAIN_TRACING_SAMPLING_RATE`
- `LANGGRAPH_AUTH_TYPE`
- `LANGGRAPH_POSTGRES_POOL_MAX_SIZE`
- `LANGSMITH_API_KEY`
- `LANGSMITH_ENDPOINT`
- `LANGSMITH_TRACING`
- `LOG_COLOR`
- `LOG_LEVEL`
- `LOG_JSON`

shell  theme={null}
OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://otlp.nr-data.net/v1/traces
OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp.nr-data.net
OTEL_EXPORTER_OTLP_HEADERS=api-key=<YOUR_INGEST_LICENSE_KEY>
```

<Note>
  OTel tracing was added in Agent Server version `0.5.32` and is currently in Alpha.
</Note>

Specify `DD_API_KEY` (your [Datadog API Key](https://docs.datadoghq.com/account_management/api-app-keys/)) to automatically enable Datadog tracing for the deployment. Specify other [`DD_*` environment variables](https://ddtrace.readthedocs.io/en/stable/configuration.html) to configure the tracing instrumentation.

If `DD_API_KEY` is specified, the application process is wrapped in the [`ddtrace-run` command](https://ddtrace.readthedocs.io/en/stable/installation_quickstart.html). Other `DD_*` environment variables (e.g. `DD_SITE`, `DD_ENV`, `DD_SERVICE`, `DD_TRACE_ENABLED`) are typically needed to properly configure the tracing instrumentation. See [`DD_*` environment variables](https://ddtrace.readthedocs.io/en/stable/configuration.html) for more details. You can enable `DD_TRACE_DEBUG=true` and set `DD_LOG_LEVEL=debug` to troubleshoot.

<Note>
  Enabling `DD_API_KEY` (and thus `ddtrace-run`) can override or interfere with other auto-instrumentation solutions (such as OpenTelemetry) that you may have instrumented into your application code.
</Note>

## `LANGCHAIN_TRACING_SAMPLING_RATE`

Sampling rate for traces sent to LangSmith. Valid values: Any float between `0` and `1`.

For more details, refer to [Set a sampling rate for traces](/langsmith/sample-traces).

## `LANGGRAPH_AUTH_TYPE`

Type of authentication for the Agent Server deployment. Valid values: `langsmith`, `noop`.

For deployments to LangSmith, this environment variable is set automatically. For local development or deployments where authentication is handled externally (e.g. self-hosted), set this environment variable to `noop`.

## `LANGGRAPH_POSTGRES_POOL_MAX_SIZE`

Beginning with langgraph-api version `0.2.12`, the maximum size of the Postgres connection pool (per replica) can be controlled using the `LANGGRAPH_POSTGRES_POOL_MAX_SIZE` environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Postgres database.

For example, if a deployment is scaled up to 10 replicas and `LANGGRAPH_POSTGRES_POOL_MAX_SIZE` is configured to `150`, then up to `1500` connections to Postgres can be established. This is particularly useful for deployments where database resources are limited (or more available) or where you need to tune connection behavior for performance or scaling reasons.

Defaults to `150` connections.

## `LANGSMITH_API_KEY`

For deployments with [self-hosted LangSmith](/langsmith/self-hosted) only.

To send traces to a self-hosted LangSmith instance, set `LANGSMITH_API_KEY` to an API key created from the self-hosted instance.

## `LANGSMITH_ENDPOINT`

For deployments with [self-hosted LangSmith](/langsmith/self-hosted) only.

To send traces to a self-hosted LangSmith instance, set `LANGSMITH_ENDPOINT` to the hostname of the self-hosted instance.

## `LANGSMITH_TRACING`

Set `LANGSMITH_TRACING` to `false` to disable tracing to LangSmith.

This is mainly relevant in the context of using the dev server via the `langgraph dev` command. Set `LOG_COLOR` to `true` to enable ANSI-colored console output when using the default console renderer. Disabling color output by setting this variable to `false` produces monochrome logs. Defaults to `true`.

Configure [log level](https://docs.python.org/3/library/logging.html#logging-levels). Defaults to `INFO`.

Set `LOG_JSON` to `true` to render all log messages as JSON objects using the configured `JSONRenderer`. This produces structured logs that can be easily parsed or ingested by log management systems. Defaults to `false`.

<Info>
  **Only Allowed in Self-Hosted Deployments**
  The `MOUNT_PREFIX` environment variable is only allowed in Self-Hosted Deployment models, LangSmith SaaS will not allow this environment variable.
</Info>

Set `MOUNT_PREFIX` to serve the Agent Server under a specific path prefix. This is useful for deployments where the server is behind a reverse proxy or load balancer that requires a specific path prefix.

For example, if the server is to be served under `https://example.com/langgraph`, set `MOUNT_PREFIX` to `/langgraph`.

## `N_JOBS_PER_WORKER`

Number of jobs per worker for the Agent Server task queue. Defaults to `10`.

## `POSTGRES_URI_CUSTOM`

<Info>
  **Only for Hybrid and Self-Hosted**
  Custom Postgres instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

Specify `POSTGRES_URI_CUSTOM` to use a custom Postgres instance. The value of `POSTGRES_URI_CUSTOM` must be a valid [Postgres connection URI](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING-URIS).

* Version 15.8 or higher.
* An initial database must be present and the connection URI must reference the database.

Control Plane Functionality:

* If `POSTGRES_URI_CUSTOM` is specified, the control plane will not provision a database for the server.
* If `POSTGRES_URI_CUSTOM` is removed, the control plane will not provision a database for the server and will not delete the externally managed Postgres instance.
* If `POSTGRES_URI_CUSTOM` is removed, deployment of the revision will not succeed. Once `POSTGRES_URI_CUSTOM` is specified, it must always be set for the lifecycle of the deployment.
* If the deployment is deleted, the control plane will not delete the externally managed Postgres instance.
* The value of `POSTGRES_URI_CUSTOM` can be updated. For example, a password in the URI can be updated.

Database Connectivity:

* The custom Postgres instance must be accessible by the Agent Server. The user is responsible for ensuring connectivity.

<Warning>
  This feature is in Alpha.
</Warning>

<Info>
  **Only Allowed in Self-Hosted Deployments**
  Redis Cluster mode is only available in Self-Hosted Deployment models, LangSmith SaaS will provision a redis instance for you by default.
</Info>

Set `REDIS_CLUSTER` to `True` to enable Redis Cluster mode. When enabled, the system will connect to Redis using cluster mode. This is useful when connecting to a Redis Cluster deployment.

## `REDIS_KEY_PREFIX`

<Info>
  **Available in API Server version 0.1.9+**
  This environment variable is supported in API Server version 0.1.9 and above.
</Info>

Specify a prefix for Redis keys. This allows multiple Agent Server instances to share the same Redis instance by using different key prefixes.

## `REDIS_URI_CUSTOM`

<Info>
  **Only for Hybrid and Self-Hosted**
  Custom Redis instances are only available for [Hybrid](/langsmith/hybrid) and [Self-Hosted](/langsmith/self-hosted) deployments.
</Info>

Specify `REDIS_URI_CUSTOM` to use a custom Redis instance. The value of `REDIS_URI_CUSTOM` must be a valid [Redis connection URI](https://redis-py.readthedocs.io/en/stable/connections.html#redis.Redis.from_url).

## `REDIS_MAX_CONNECTIONS`

The maximum size of the Redis connection pool (per replica) can be controlled using the `REDIS_MAX_CONNECTIONS` environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Redis instance.

For example, if a deployment is scaled up to 10 replicas and `REDIS_MAX_CONNECTIONS` is configured to `150`, then up to `1500` connections to Redis can be established.

## `RESUMABLE_STREAM_TTL_SECONDS`

Time-to-live in seconds for resumable stream data in Redis.

When a run is created and the output is streamed, the stream can be configured to be resumable (e.g. `stream_resumable=True`). If a stream is resumable, output from the stream is temporarily stored in Redis. The TTL for this data can be configured by setting `RESUMABLE_STREAM_TTL_SECONDS`.

See the [Python](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.stream) and [JS/TS](https://langchain-ai.github.io/langgraphjs/reference/classes/sdk_client.RunsClient.html#stream) SDKs for more details on how to implement resumable streams.

Defaults to `120` seconds.

<Note>
  Setting a very high value for `RESUMABLE_STREAM_TTL_SECONDS` can result in substantial Redis memory usage when there are many concurrent runs with large or frequent streaming output. Set this value to the minimum value to enable recovery during network interruptions and prefer checkpointing for long term durability and execution snapshotting.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/env-var.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, to submit OpenTelemetry traces to [New Relic's US region](https://docs.newrelic.com/docs/opentelemetry/best-practices/opentelemetry-otlp/), set the following:
```

---

## Optional. You can swap OpenAI for any other tool-calling chat model.

**URL:** llms-txt#optional.-you-can-swap-openai-for-any-other-tool-calling-chat-model.

os.environ["OPENAI_API_KEY"] = "YOUR OPENAI API KEY"

---

## Optional. You can swap Tavily for the free DuckDuckGo search tool if preferred.

**URL:** llms-txt#optional.-you-can-swap-tavily-for-the-free-duckduckgo-search-tool-if-preferred.

---

## Organization and workspace operations reference

**URL:** llms-txt#organization-and-workspace-operations-reference

**Contents:**
- Contents
- Legend
- Organization-level operations
  - Organization settings
  - Workspaces
  - Organization members
  - Roles and permissions
  - SSO and authentication
  - SCIM
  - Access policies

Source: https://docs.langchain.com/langsmith/organization-workspace-operations

This page provides a comprehensive reference table of [workspace](/langsmith/administration-overview#workspaces) and [organization](/langsmith/administration-overview#organizations) operations and which roles can perform them.

The list includes API operations in LangSmith along with:

* Which system roles can perform each operation.
* The specific permission string required.
* Notes about partial access or special cases.

<Info>
  For an overview of LangSmith's RBAC system, role definitions, and permission concepts, refer to [Role-based access control](/langsmith/rbac).
</Info>

| Organization-level operations                                                                                                                                                                                                                                                                               | Workspace-level operations                                                                                                                                                                                                                                                                                    |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Core management:**<br />• [Organization settings](#organization-settings): Org info and configuration<br />• [Workspaces](#workspaces): Workspace management<br />• [Organization members](#organization-members): Member management<br />• [Roles and permissions](#roles-and-permissions): Custom roles | **Core resources:**<br />• [Projects](#projects): Organize traces and runs<br />• [Runs](#runs): Individual execution traces<br />• [Datasets](#datasets): Test datasets for evaluation<br />• [Examples](#examples): Individual dataset examples<br />• [Experiments](#experiments): Comparative experiments |
| **Security and authentication:**<br />• [SSO and authentication](#sso-and-authentication): Single sign-on setup<br />• [SCIM](#scim): Identity provisioning<br />• [Access policies](#access-policies): Attribute-based access control                                                                      | **Monitoring and analysis:**<br />• [Rules](#rules): Automated run rules<br />• [Alerts](#alerts): Alert rules for monitoring<br />• [Feedback](#feedback): Scores and labels on outputs<br />• [Annotation Queues](#annotation-queues): Human review queues<br />• [Charts](#charts): Custom visualizations  |
| **Billing and accounts:**<br />• [Billing and payments](#billing-and-payments): Subscription management<br />• [API keys](#api-keys): Org-level keys                                                                                                                                                        | **Development and configuration:**<br />• [Prompts](#prompts): Prompt templates (LangChain Hub)<br />• [Deployments](#deployments): Deployment configurations<br />• [MCP Servers](#mcp-servers): Model Context Protocol servers                                                                              |
| **Analytics:**<br />• [Charts and dashboards](#organization-charts-and-dashboards): Org-level visualizations<br />• [Usage and analytics](#usage-and-analytics): Usage tracking and TTL settings                                                                                                            | **Workspace management:**<br />• [Workspace settings](#workspace-settings-and-management): Members, settings<br />• [Tags](#tags): Metadata tagging system<br />• [Bulk Exports](#bulk-exports): Data export operations                                                                                       |

**Additional information:**

* [User-level operations](#user-level-operations): Operations for all authenticated users
* [Permission inheritance](#permission-inheritance): How roles inherit across org/workspaces

* ✓ **Allowed**: User with this role can perform this action
* ✗ **Not Allowed**: User with this role cannot perform this action
* ⚠ **Partial**: User has limited access (see notes)

## Organization-level operations

<Info>
  Organization-level operations are controlled by organization roles, which are separate from the RBAC feature. Learn more in the [Role-based access control](/langsmith/rbac#organization-roles) guide.
</Info>

### Organization settings

| Operation                   | Org Admin | Org User | Org Viewer | Required Permission   |
| --------------------------- | :-------: | :------: | :--------: | --------------------- |
| View organization info      |     ✓     |     ✓    |      ✓     | `organization:read`   |
| View organization dashboard |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Update organization info    |     ✓     |     ✗    |      ✗     | `organization:manage` |
| View billing info           |     ✓     |     ✓    |      ✓     | `organization:read`   |
| View company info           |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Set company info            |     ✓     |     ✗    |      ✗     | `organization:manage` |

Organization-level workspace management operations.

| Operation           | Org Admin | Org User | Org Viewer | Required Permission   |
| ------------------- | :-------: | :------: | :--------: | --------------------- |
| List all workspaces |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Create workspace    |     ✓     |     ✗    |      ✗     | `organization:manage` |

### Organization members

| Operation                       | Org Admin | Org User | Org Viewer | Required Permission   |
| ------------------------------- | :-------: | :------: | :--------: | --------------------- |
| View organization members       |     ✓     |     ✓    |      ✓     | `organization:read`   |
| View active org members         |     ✓     |     ✓    |      ✓     | `organization:read`   |
| View pending org members        |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Invite member to organization   |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Invite members (batch)          |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Add basic auth members          |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Remove organization member      |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Update organization member role |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Delete pending org member       |     ✓     |     ✗    |      ✗     | `organization:manage` |

### Roles and permissions

| Operation                  | Org Admin | Org User | Org Viewer | Required Permission   |
| -------------------------- | :-------: | :------: | :--------: | --------------------- |
| List organization roles    |     ✓     |     ✓    |      ✓     | `organization:read`   |
| List available permissions |     ✓     |     ✓    |      ✓     | N/A (user-level)      |
| Create custom role         |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Update custom role         |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Delete custom role         |     ✓     |     ✗    |      ✗     | `organization:manage` |

### SSO and authentication

| Operation                    | Org Admin | Org User | Org Viewer | Required Permission   |
| ---------------------------- | :-------: | :------: | :--------: | --------------------- |
| View SSO settings            |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Create SSO settings          |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Update SSO settings          |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Delete SSO settings          |     ✓     |     ✗    |      ✗     | `organization:manage` |
| View login methods           |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Update allowed login methods |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Set default SSO provision    |     ✓     |     ✗    |      ✗     | `organization:manage` |

System for Cross-domain Identity Management for user provisioning.

| Operation         | Org Admin | Org User | Org Viewer | Required Permission   |
| ----------------- | :-------: | :------: | :--------: | --------------------- |
| List SCIM tokens  |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Get SCIM token    |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Create SCIM token |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Update SCIM token |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Delete SCIM token |     ✓     |     ✗    |      ✗     | `organization:manage` |

Attribute-based access control (ABAC) policies for fine-grained permissions.

<Note>
  ABAC is in private preview.
</Note>

| Operation                    | Org Admin | Org User | Org Viewer | Required Permission   |
| ---------------------------- | :-------: | :------: | :--------: | --------------------- |
| List access policies         |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Get access policy            |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Create access policy         |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Delete access policy         |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Attach access policy to role |     ✓     |     ✗    |      ✗     | `organization:manage` |

### Billing and payments

| Operation                      | Org Admin | Org User | Org Viewer | Required Permission   |
| ------------------------------ | :-------: | :------: | :--------: | --------------------- |
| Create Stripe setup intent     |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Handle payment method creation |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Change payment plan            |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Create Stripe checkout session |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Confirm checkout completion    |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Create Stripe account links    |     ✓     |     ✗    |      ✗     | `organization:manage` |

| Operation                                      | Org Admin | Org User | Org Viewer | Required Permission                                |
| ---------------------------------------------- | :-------: | :------: | :--------: | -------------------------------------------------- |
| List org-scoped API keys                       |     ✓     |     ✓    |      ✓     | `organization:read`                                |
| Create org-scoped API key (workspace-scoped)\* |     ✓     |     ⚠    |      ✗     | `organization:pats:create`                         |
| Create org-scoped API key (org-wide)\*         |     ✓     |     ✗    |      ✗     | `organization:pats:create` + `organization:manage` |
| List personal access tokens                    |     ✓     |     ✓    |      ✗     | `organization:read`                                |
| Create personal access token                   |     ✓     |     ✓    |      ✗     | `organization:pats:create`                         |
| Delete personal access token                   |     ✓     |     ✓    |      ✗     | `organization:read`                                |

<Note>
  \* Organization Users can create workspace-scoped API keys only for workspaces where they are a Workspace Admin. Org-wide API keys require the Organization Admin role.
</Note>

### Organization charts and dashboards

| Operation                | Org Admin | Org User | Org Viewer | Required Permission   |
| ------------------------ | :-------: | :------: | :--------: | --------------------- |
| List org charts          |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Get org chart by ID      |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Create org chart         |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Update org chart         |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Delete org chart         |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Render org chart         |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Get org chart section    |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Create org chart section |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Update org chart section |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Delete org chart section |     ✓     |     ✗    |      ✗     | `organization:manage` |
| Render org chart section |     ✓     |     ✓    |      ✓     | `organization:read`   |

### Usage and analytics

| Operation               | Org Admin | Org User | Org Viewer | Required Permission   |
| ----------------------- | :-------: | :------: | :--------: | --------------------- |
| View organization usage |     ✓     |     ✓    |      ✓     | `organization:read`   |
| View TTL settings       |     ✓     |     ✓    |      ✓     | `organization:read`   |
| Upsert TTL settings     |     ✓     |     ✗    |      ✗     | `organization:manage` |

## Workspace-level operations

These operations are controlled by [workspace-level roles and permissions](/langsmith/rbac#workspace-roles).

<Tip>
  To understand what each role means and their overall capabilities, refer to the [Role-based access control](/langsmith/rbac) guide.
</Tip>

Projects organize traces and runs from your LLM applications.

| Operation                                         | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission              |
| ------------------------------------------------- | :-------------: | :--------------: | :--------------: | -------------------------------- |
| Create a new project                              |        ✓        |         ✗        |         ✗        | `projects:create`                |
| View project list                                 |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| View project details                              |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| View prebuilt dashboard                           |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| View project metadata (top K values)              |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| Update project metadata (name, description, tags) |        ✓        |         ✓        |         ✗        | `projects:update`                |
| Create filter view                                |        ✓        |         ✗        |         ✗        | `projects:create`                |
| View filter views                                 |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| View specific filter view                         |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| Update filter view                                |        ✓        |         ✓        |         ✗        | `projects:update`                |
| Delete filter view                                |        ✓        |         ✗        |         ✗        | `projects:delete`                |
| Delete a project                                  |        ✓        |         ✗        |         ✗        | `projects:delete`                |
| Delete multiple projects                          |        ✓        |         ✗        |         ✗        | `projects:delete`                |
| Get insights jobs (Beta)                          |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| Get specific insights job (Beta)                  |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| Create insights job (Beta)                        |        ✓        |         ✓        |         ✓        | `projects:read` + `rules:create` |
| Update insights job (Beta)                        |        ✓        |         ✓        |         ✗        | `projects:update`                |
| Delete insights job (Beta)                        |        ✓        |         ✗        |         ✗        | `projects:delete`                |
| Get insights job configs (Beta)                   |        ✓        |         ✓        |         ✓        | `rules:read`                     |
| Create insights job config (Beta)                 |        ✓        |         ✓        |         ✗        | `rules:create`                   |
| Auto-generate insights job config (Beta)          |        ✓        |         ✓        |         ✗        | `rules:create`                   |
| Update insights job config (Beta)                 |        ✓        |         ✓        |         ✗        | `rules:update`                   |
| Delete insights job config (Beta)                 |        ✓        |         ✓        |         ✗        | `rules:delete`                   |
| Get run cluster from insights job (Beta)          |        ✓        |         ✓        |         ✓        | `projects:read`                  |
| Get runs from insights job (Beta)                 |        ✓        |         ✓        |         ✓        | `projects:read`                  |

Individual execution traces and spans from your LLM applications.

| Operation                                                              | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ---------------------------------------------------------------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| Send traces from SDK (includes single run, batch, multipart, and OTEL) |        ✓        |         ✓        |         ✗        | `runs:create`       |
| View a specific run                                                    |        ✓        |         ✓        |         ✓        | `runs:read`         |
| View thread preview                                                    |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Query/list runs                                                        |        ✓        |         ✓        |         ✓        | `runs:read`         |
| View run statistics                                                    |        ✓        |         ✓        |         ✓        | `runs:read`         |
| View grouped run statistics                                            |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Group runs by expression                                               |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Generate filter query from natural language                            |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Prefetch runs                                                          |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Update a run (PATCH)                                                   |        ✓        |         ✓        |         ✗        | `runs:create`       |
| View run sharing state                                                 |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Share a run publicly                                                   |        ✓        |         ✓        |         ✗        | `runs:share`        |
| Unshare a run                                                          |        ✓        |         ✓        |         ✗        | `runs:share`        |
| Delete runs by trace ID or metadata                                    |        ✓        |         ✗        |         ✗        | `runs:delete`       |

Automated run rules that trigger actions based on run conditions.

| Operation               | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ----------------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| List all run rules      |        ✓        |         ✓        |         ✓        | `rules:read`        |
| Create a run rule       |        ✓        |         ✓        |         ✗        | `rules:create`      |
| Update a run rule       |        ✓        |         ✓        |         ✗        | `rules:update`      |
| Delete a run rule       |        ✓        |         ✓        |         ✗        | `rules:delete`      |
| View rule logs          |        ✓        |         ✓        |         ✓        | `rules:read`        |
| Get last applied rule   |        ✓        |         ✓        |         ✓        | `rules:read`        |
| Manually trigger a rule |        ✓        |         ✓        |         ✗        | `rules:update`      |
| Trigger multiple rules  |        ✓        |         ✓        |         ✗        | `rules:update`      |

Alert rules for monitoring run conditions.

| Operation         | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ----------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| Create alert rule |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Update alert rule |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Delete alert rule |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Get alert rule    |        ✓        |         ✓        |         ✓        | `runs:read`         |
| List alert rules  |        ✓        |         ✓        |         ✓        | `runs:read`         |
| Test alert action |        ✓        |         ✓        |         ✓        | `runs:read`         |

Test datasets with examples for evaluation.

| Operation                                    | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission                                  |
| -------------------------------------------- | :-------------: | :--------------: | :--------------: | ---------------------------------------------------- |
| Create a dataset                             |        ✓        |         ✓        |         ✗        | `datasets:create`                                    |
| List datasets                                |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| View dataset details                         |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Update dataset metadata                      |        ✓        |         ✓        |         ✗        | `datasets:update`                                    |
| Delete a dataset                             |        ✓        |         ✗        |         ✗        | `datasets:delete`                                    |
| Upload CSV dataset                           |        ✓        |         ✓        |         ✗        | `datasets:create`                                    |
| Clone dataset                                |        ✓        |         ✓        |         ✗        | `datasets:update`                                    |
| Get dataset version                          |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Get dataset versions                         |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Diff dataset versions                        |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Update dataset version (tags)                |        ✓        |         ✓        |         ✗        | `datasets:update`                                    |
| Download dataset (OpenAI format)             |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Download dataset (OpenAI fine-tuning format) |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Download dataset (CSV)                       |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Download dataset (JSONL)                     |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| View dataset sharing state                   |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Share dataset publicly                       |        ✓        |         ✗        |         ✗        | `datasets:share`                                     |
| Unshare dataset                              |        ✓        |         ✗        |         ✗        | `datasets:share`                                     |
| Get index info                               |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Index dataset                                |        ✓        |         ✓        |         ✗        | `datasets:update`                                    |
| Sync dataset index                           |        ✓        |         ✓        |         ✗        | `datasets:update`                                    |
| Remove dataset index                         |        ✓        |         ✓        |         ✗        | `datasets:update`                                    |
| Search dataset                               |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Generate synthetic examples                  |        ✓        |         ✓        |         ✗        | `datasets:update`                                    |
| Get dataset splits                           |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Update dataset splits                        |        ✓        |         ✓        |         ✓        | `datasets:read`                                      |
| Run playground experiment (batch)            |        ✓        |         ⚠        |         ✗        | `prompts:read` + `datasets:read` + `projects:create` |
| Run playground experiment (stream)           |        ✓        |         ⚠        |         ✗        | `prompts:read` + `datasets:read` + `projects:create` |
| Run studio experiment                        |        ✓        |         ⚠        |         ✗        | `datasets:read` + `projects:create`                  |

<Note>
  Workspace Editors have partial access because they cannot create projects, which limits their ability to create new experiments.
</Note>

Individual examples within datasets.

| Operation                       | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ------------------------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| Count examples                  |        ✓        |         ✓        |         ✓        | `datasets:read`     |
| View a specific example         |        ✓        |         ✓        |         ✓        | `datasets:read`     |
| List examples                   |        ✓        |         ✓        |         ✓        | `datasets:read`     |
| Create a new example            |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| Create examples (bulk)          |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| Update a single example         |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| Update examples (bulk)          |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| Update examples (multipart)     |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| Upload examples from CSV        |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| Upload examples from JSONL      |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| Delete a single example         |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| Delete examples (bulk)          |        ✓        |         ✓        |         ✗        | `datasets:update`   |
| View examples with runs         |        ✓        |         ✓        |         ✓        | `datasets:read`     |
| View grouped examples with runs |        ✓        |         ✓        |         ✓        | `datasets:read`     |
| Validate a single example       |        ✓        |         ✓        |         ✓        | `datasets:read`     |
| Validate examples (bulk)        |        ✓        |         ✓        |         ✓        | `datasets:read`     |

Comparative experiments for evaluating LLM outputs.

| Operation                       | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission                                                       |
| ------------------------------- | :-------------: | :--------------: | :--------------: | ------------------------------------------------------------------------- |
| View comparative experiments    |        ✓        |         ✓        |         ✓        | `projects:read`                                                           |
| Create comparative experiment   |        ✓        |         ⚠        |         ✗        | `projects:create`                                                         |
| Delete comparative experiment   |        ✓        |         ✗        |         ✗        | `projects:delete`                                                         |
| View examples with runs         |        ✓        |         ✓        |         ✓        | `datasets:read`                                                           |
| View grouped examples with runs |        ✓        |         ✓        |         ✓        | `datasets:read`                                                           |
| View grouped experiments        |        ✓        |         ✓        |         ✓        | `datasets:read`                                                           |
| View feedback delta             |        ✓        |         ✓        |         ✓        | `datasets:read`                                                           |
| Upload experiment results       |        ✓        |         ⚠        |         ✗        | `datasets:create` + `datasets:update` + `projects:create` + `runs:create` |
| Get experiment view overrides   |        ✓        |         ✓        |         ✗        | `datasets:update`                                                         |
| Create experiment view override |        ✓        |         ✓        |         ✗        | `datasets:update`                                                         |
| Update experiment view override |        ✓        |         ✓        |         ✗        | `datasets:update`                                                         |
| Delete experiment view override |        ✓        |         ✓        |         ✗        | `datasets:update`                                                         |

<Note>
  Workspace Editors have partial access because they cannot create projects, which limits their ability to create new experiments.
</Note>

Scores, labels, and corrections on LLM outputs.

| Operation                                     | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| --------------------------------------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| List feedback formulas                        |        ✓        |         ✓        |         ✓        | `feedback:read`     |
| Get feedback formula                          |        ✓        |         ✓        |         ✓        | `feedback:read`     |
| Create feedback formula                       |        ✓        |         ✓        |         ✗        | `feedback:create`   |
| Update feedback formula                       |        ✓        |         ✓        |         ✗        | `feedback:update`   |
| Delete feedback formula                       |        ✓        |         ✓        |         ✗        | `feedback:delete`   |
| View specific feedback                        |        ✓        |         ✓        |         ✓        | `feedback:read`     |
| List feedbacks                                |        ✓        |         ✓        |         ✓        | `feedback:read`     |
| Create feedback                               |        ✓        |         ✓        |         ✗        | `feedback:create`   |
| Eagerly create feedback                       |        ✓        |         ✓        |         ✗        | `feedback:create`   |
| Update feedback                               |        ✓        |         ✓        |         ✗        | `feedback:update`   |
| Delete feedback                               |        ✓        |         ✓        |         ✗        | `feedback:delete`   |
| Batch ingest feedback                         |        ✓        |         ✓        |         ✗        | `feedback:create`   |
| Create feedback ingest token                  |        ✓        |         ✓        |         ✗        | `feedback:create`   |
| List feedback ingest tokens                   |        ✓        |         ✓        |         ✗        | `feedback:create`   |
| Create feedback with token (no auth required) |        ✓        |         ✓        |         ✓        | N/A (token-based)   |
| List feedback configs                         |        ✓        |         ✓        |         ✓        | `feedback:read`     |
| Create feedback config                        |        ✓        |         ✓        |         ✗        | `feedback:create`   |
| Update feedback config                        |        ✓        |         ✓        |         ✗        | `feedback:update`   |

### Annotation Queues

Human review queues for LLM outputs.

| Operation                                   | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission        |
| ------------------------------------------- | :-------------: | :--------------: | :--------------: | -------------------------- |
| List annotation queues                      |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |
| Get annotation queue                        |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |
| Create annotation queue                     |        ✓        |         ✓        |         ✗        | `annotation-queues:create` |
| Update annotation queue                     |        ✓        |         ✓        |         ✗        | `annotation-queues:update` |
| Delete annotation queue                     |        ✓        |         ✗        |         ✗        | `annotation-queues:delete` |
| Populate annotation queue                   |        ✓        |         ✓        |         ✗        | `annotation-queues:update` |
| Get runs from queue                         |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |
| Get run from queue (by index)               |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |
| Get queues for run                          |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |
| Get queue total size                        |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |
| Get queue total archived                    |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |
| Get queue size                              |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |
| Add runs to queue                           |        ✓        |         ✓        |         ✗        | `annotation-queues:update` |
| Update run in queue                         |        ✓        |         ✓        |         ✗        | `annotation-queues:update` |
| Delete run from queue                       |        ✓        |         ✓        |         ✗        | `annotation-queues:update` |
| Delete runs from queue (bulk)               |        ✓        |         ✓        |         ✗        | `annotation-queues:update` |
| Create identity annotation queue run status |        ✓        |         ✓        |         ✗        | `annotation-queues:update` |
| Export archived runs                        |        ✓        |         ✓        |         ✓        | `annotation-queues:read`   |

Prompt templates and chains in the LangChain Hub.

| Operation               | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ----------------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| List prompt repos       |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| View prompt repo        |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Create prompt repo      |        ✓        |         ✓        |         ✗        | `prompts:create`    |
| Fork prompt repo        |        ✓        |         ✓        |         ✗        | `prompts:create`    |
| Update prompt repo      |        ✓        |         ✓        |         ✗        | `prompts:update`    |
| Delete prompt repo      |        ✓        |         ✓        |         ✗        | `prompts:delete`    |
| List commits            |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| View commit             |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Push commit             |        ✓        |         ✓        |         ✗        | `prompts:update`    |
| List repo tags          |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Get all tags            |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Create tag              |        ✓        |         ✓        |         ✗        | `prompts:create`    |
| Update tag              |        ✓        |         ✓        |         ✗        | `prompts:update`    |
| Delete tag              |        ✓        |         ✓        |         ✗        | `prompts:delete`    |
| View events             |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| List comments           |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Create comment          |        ✓        |         ✓        |         ✗        | `prompts:read`      |
| Delete comment          |        ✓        |         ✓        |         ✗        | `prompts:read`      |
| Toggle like             |        ✓        |         ✓        |         ✗        | `prompts:read`      |
| Optimize prompt         |        ✓        |         ✓        |         ✗        | `prompts:update`    |
| List optimization jobs  |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Create optimization job |        ✓        |         ✓        |         ✗        | `prompts:create`    |
| Update optimization job |        ✓        |         ✓        |         ✗        | `prompts:update`    |
| Delete optimization job |        ✓        |         ✓        |         ✗        | `prompts:delete`    |
| Invoke prompt canvas    |        ✓        |         ✓        |         ✗        | `prompts:update`    |
| List quick actions      |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Create quick action     |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Delete quick action     |        ✓        |         ✓        |         ✓        | `prompts:read`      |
| Update quick action     |        ✓        |         ✓        |         ✓        | `prompts:read`      |

<Note>
  Some prompt operations support public access for shared prompts.
</Note>

Custom visualizations and dashboards.

| Operation               | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ----------------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| List charts             |        ✓        |         ✓        |         ✓        | `charts:read`       |
| Get chart by ID         |        ✓        |         ✓        |         ✓        | `charts:read`       |
| Create chart            |        ✓        |         ✓        |         ✗        | `charts:create`     |
| Update chart            |        ✓        |         ✓        |         ✗        | `charts:update`     |
| Delete chart            |        ✓        |         ✓        |         ✗        | `charts:delete`     |
| Render chart            |        ✓        |         ✓        |         ✓        | `charts:read`       |
| List chart sections     |        ✓        |         ✓        |         ✓        | `charts:read`       |
| Get chart section by ID |        ✓        |         ✓        |         ✓        | `charts:read`       |
| Create chart section    |        ✓        |         ✓        |         ✗        | `charts:create`     |
| Update chart section    |        ✓        |         ✓        |         ✗        | `charts:update`     |
| Delete chart section    |        ✓        |         ✓        |         ✗        | `charts:delete`     |
| Render chart section    |        ✓        |         ✓        |         ✓        | `charts:read`       |

[LangSmith Deployment](/langsmith/deployments) configurations.

| Operation         | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission  |
| ----------------- | :-------------: | :--------------: | :--------------: | -------------------- |
| Create deployment |        ✓        |         ✓        |         ✗        | `deployments:create` |
| View deployment   |        ✓        |         ✓        |         ✓        | `deployments:read`   |
| Update deployment |        ✓        |         ✓        |         ✗        | `deployments:update` |
| Delete deployment |        ✓        |         ✗        |         ✗        | `deployments:delete` |

### Workspace settings and management

| Operation                            | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ------------------------------------ | :-------------: | :--------------: | :--------------: | ------------------- |
| View workspace info                  |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| View workspace statistics            |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Update workspace (name, description) |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Delete workspace                     |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| View workspace members               |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| View active workspace members        |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| View pending workspace members       |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Add member to workspace              |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Add members (batch)                  |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Update workspace member role         |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Remove workspace member              |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Delete pending workspace member      |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| View usage limits                    |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| View shared entities                 |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Bulk unshare entities                |        ✓        |         ✗        |         ✗        | `workspaces:manage` |

| Operation                       | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ------------------------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| List tag keys                   |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Get tag key                     |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Create tag key                  |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Update tag key                  |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Delete tag key                  |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| List tag values                 |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Get tag value                   |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Create tag value                |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Update tag value                |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Delete tag value                |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| List tags                       |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| List tags for resource          |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| List tags for resources (batch) |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| List taggings                   |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Create tagging                  |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Delete tagging                  |        ✓        |         ✗        |         ✗        | `workspaces:manage` |

| Operation                      | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ------------------------------ | :-------------: | :--------------: | :--------------: | ------------------- |
| List bulk exports              |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Get bulk export                |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Create bulk export             |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Cancel bulk export             |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Get bulk export destinations   |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Get bulk export destination    |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Create bulk export destination |        ✓        |         ✗        |         ✗        | `workspaces:manage` |
| Get filtered export runs       |        ✓        |         ✓        |         ✓        | `workspaces:read`   |

Model Context Protocol servers for extended functionality.

| Operation         | Workspace Admin | Workspace Editor | Workspace Viewer | Required Permission |
| ----------------- | :-------------: | :--------------: | :--------------: | ------------------- |
| List MCP servers  |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Get MCP server    |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Create MCP server |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Update MCP server |        ✓        |         ✓        |         ✓        | `workspaces:read`   |
| Delete MCP server |        ✓        |         ✓        |         ✓        | `workspaces:read`   |

## User-level operations

These operations are available to all authenticated users and don't require specific workspace or organization permissions:

* View own user profile
* Update own user profile
* List organizations for user
* Create new organization
* List pending workspace invites
* Delete pending workspace invite
* Claim pending workspace invite
* List pending organization invites
* Delete pending organization invite
* Claim pending organization invite

## Permission inheritance

### Organization to workspace

* [Organization Admin](/langsmith/rbac#organization-admin) automatically has full permissions in all workspaces.
* [Organization User](/langsmith/rbac#organization-user) and [Organization Viewer](/langsmith/rbac#organization-viewer) only get workspace access when explicitly added to workspaces with workspace-level roles.

For detailed role definitions, refer to [Organization roles](/langsmith/rbac#organization-roles) and [Workspace roles](/langsmith/rbac#workspace-roles).

### Workspace role independence

* Users can have different workspace roles in different workspaces.
* A user might be a [Workspace Admin](/langsmith/rbac#workspace-admin) in one workspace and a [Workspace Viewer](/langsmith/rbac#workspace-viewer) in another.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/organization-workspace-operations.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Original call: add(a=2, b=3) becomes add(a=4, b=6)

**URL:** llms-txt#original-call:-add(a=2,-b=3)-becomes-add(a=4,-b=6)

python Dynamic header modification theme={null}
async def auth_header_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Add authentication headers based on the tool being called."""
    token = get_token_for_tool(request.name)
    modified_request = request.override(
        headers={"Authorization": f"Bearer {token}"}  # [!code highlight]
    )
    return await handler(modified_request)
python Composing multiple interceptors theme={null}
async def outer_interceptor(request, handler):
    print("outer: before")
    result = await handler(request)
    print("outer: after")
    return result

async def inner_interceptor(request, handler):
    print("inner: before")
    result = await handler(request)
    print("inner: after")
    return result

client = MultiServerMCPClient(
    {...},
    tool_interceptors=[outer_interceptor, inner_interceptor],  # [!code highlight]
)

**Examples:**

Example 1 (unknown):
```unknown
**Modifying headers at runtime**

Interceptors can modify HTTP headers dynamically based on the request context:
```

Example 2 (unknown):
```unknown
**Composing interceptors**

Multiple interceptors compose in "onion" order — the first interceptor in the list is the outermost layer:
```

---

## Or directly:

**URL:** llms-txt#or-directly:

**Contents:**
  - Code quality standards
- Testing and validation
  - Running tests locally
  - Test writing guidelines
- Getting help

uv run --group test pytest tests/unit_tests
bash  theme={null}
make integration_tests
python  theme={null}
    def process_documents(
        docs: list[Document],
        processor: DocumentProcessor,
        *,
        batch_size: int = 100
    ) -> ProcessingResult:
        """Process documents in batches.

Args:
            docs: List of documents to process.
            processor: Document processing instance.
            batch_size: Number of documents per batch.

Returns:
            Processing results with success/failure counts.
        """
    `python  theme={null}
        class ChatAnthropic(BaseChatModel):
            """Interface to Claude chat models.

See the [usage guide](https://docs.langchain.com/oss/python/integrations/chat/anthropic)
            for tutorials, feature walkthroughs, and examples.

Args:
                model: Model identifier (e.g., `'claude-sonnet-4-5-20250929'`).
                temperature: Sampling temperature between `0` and `1`.
                max_tokens: Maximum number of tokens to generate.
                api_key: Anthropic API key.

If not provided, reads from the `ANTHROPIC_API_KEY`
                    environment variable.
                timeout: Request timeout in seconds.
                max_retries: Maximum number of retries for failed requests.

Returns:
                A chat model instance that can be invoked with messages.

Raises:
                ValueError: If the model identifier is not recognized.
                AuthenticationError: If the API key is invalid.

Example:
                
            """
        python  theme={null}
          """
          ...

See the [extended thinking guide](https://docs.langchain.com/oss/integrations/chat/anthropic#extended-thinking)
          for configuration options.

...
          """
          python  theme={null}
          """
          Example:
              \`\`\`python
              message = HumanMessage(content=[
                  {"type": "image", "url": "https://example.com/image.jpg"}
              ])
              \`\`\`

See the [multimodal guide](https://docs.langchain.com/oss/integrations/chat/anthropic#multimodal)
          for all supported input formats.
          """
          bash  theme={null}
    make format  # Apply formatting
    make lint    # Check style and types
    bash  theme={null}
        make test
        bash  theme={null}
        make integration_tests
        bash  theme={null}
        make format
        make lint
        bash  theme={null}
        make type_check
        python  theme={null}
    def test_document_processor_handles_empty_input():
        """Test processor gracefully handles empty document list."""
        processor = DocumentProcessor()

result = processor.process([])

assert result.success
        assert result.processed_count == 0
        assert len(result.errors) == 0
    python  theme={null}
    @pytest.mark.requires("openai")
    def test_openai_chat_integration():
        """Test OpenAI chat integration with real API."""

chat = ChatOpenAI()
        response = chat.invoke("Hello")

assert isinstance(response.content, str)
        assert len(response.content) > 0
    python  theme={null}
    def test_retry_mechanism(mocker):
        """Test retry mechanism handles transient failures."""
        mock_client = mocker.Mock()
        mock_client.call.side_effect = [
            ConnectionError("Temporary failure"),
            {"result": "success"}
        ]

service = APIService(client=mock_client)
        result = service.call_with_retry()

assert result["result"] == "success"
        assert mock_client.call.call_count == 2
    ```
  </Tab>
</Tabs>

Our goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).

<Check>
  You're now ready to contribute high-quality code to LangChain!
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Integration tests

**Location**: `tests/integration_tests/`

Integration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.

Not every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.

**Requirements**:

* Test real integrations with external services
* Use environment variables for API keys
* Skip gracefully if credentials unavailable
```

Example 2 (unknown):
```unknown
### Code quality standards

Contributions must adhere to the following quality requirements:

<Tabs>
  <Tab title="Type hints">
    **Required**: Complete type annotations for all functions
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Documentation">
    **Required**: [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) for all public functions.

    **Guiding principle**: Docstrings describe "what"; docs on this site explain the "how" and "why."

    | Content type                | Location   | Purpose                           |
    | --------------------------- | ---------- | --------------------------------- |
    | Parameter descriptions      | Docstrings | Auto-generates into API reference |
    | Return types and exceptions | Docstrings | API reference                     |
    | Minimal usage example       | Docstrings | Show basic instantiation pattern  |
    | Feature tutorials           | This site  | In-depth walkthroughs             |
    | End-to-end examples         | This site  | Real-world usage patterns         |
    | Conceptual explanations     | This site  | Understanding and context         |

    **Docstrings should contain:**

    1. One-line summary of what the class/function does
    2. Link to this site for tutorials, guides, and usage patterns
    3. Parameter documentation with types and descriptions
    4. Return value description
    5. Exceptions that may be raised
    6. Single minimal example showing basic instantiation/usage as necessary

    <AccordionGroup>
      <Accordion title="Good docstring example">
```

Example 4 (python):
```python
from langchain_anthropic import ChatAnthropic

                model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
                response = model.invoke("Hello!")
```

---

## Or load specific resources by URI

**URL:** llms-txt#or-load-specific-resources-by-uri

**Contents:**
  - Prompts

blobs = await client.get_resources("server_name", uris=["file:///path/to/file.txt"])  # [!code highlight]

for blob in blobs:
    print(f"URI: {blob.metadata['uri']}, MIME type: {blob.mimetype}")
    print(blob.as_string())  # For text content
python  theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.resources import load_mcp_resources

client = MultiServerMCPClient({...})

async with client.session("server_name") as session:
    # Load all resources
    blobs = await load_mcp_resources(session)

# Or load specific resources by URI
    blobs = await load_mcp_resources(session, uris=["file:///path/to/file.txt"])
python  theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient

client = MultiServerMCPClient({...})

**Examples:**

Example 1 (unknown):
```unknown
You can also use [`load_mcp_resources`](/docs/reference/langchain-mcp-adapters#load_mcp_resources) directly with a session for more control:
```

Example 2 (unknown):
```unknown
### Prompts

[Prompts](https://modelcontextprotocol.io/docs/concepts/prompts) allow MCP servers to expose reusable prompt templates that can be retrieved and used by clients. LangChain converts MCP prompts into [messages](/docs/concepts/messages), making them easy to integrate into chat-based workflows.

#### Loading prompts

Use `client.get_prompt()` to load a prompt from an MCP server:
```

---

## Or run a specific test file

**URL:** llms-txt#or-run-a-specific-test-file

pnpm test src/tests/FILENAME_BEING_TESTED.test.ts

---

## Or run a specific test function

**URL:** llms-txt#or-run-a-specific-test-function

**Contents:**
  - Code quality standards
- Testing and validation
  - Running tests locally
  - Test writing guidelines
- Getting help

pnpm test -t "the test that should be run"
bash  theme={null}
pnpm test:int
typescript  theme={null}
    function processDocuments(
        docs: Document[],
        processor: DocumentProcessor,
        batchSize: number = 100
    ): ProcessingResult {
        // ...
    }
    typescript  theme={null}
    /**
     * Document processing instance.
     */
    interface FooDocumentProcessor {
        /**
         * Process documents in batches.
         *
         * @param docs - List of documents to process.
         * @returns Processing results with success/failure counts.
         */
        process(docs: Document[]): ProcessingResult;
    }

/**
     * Process documents in batches.
     *
     * @param docs - List of documents to process.
     * @param processor - Document processing instance.
     * @param batchSize - Number of documents per batch.
     * @returns Processing results with success/failure counts.
     */
    export function processDocuments(
        docs: Document[],
        processor: DocumentProcessor,
        batchSize: number = 100
    ): ProcessingResult {
        // ...
    }
    bash  theme={null}
    pnpm lint    # Check style and types
    pnpm format  # Apply formatting
    bash  theme={null}
        pnpm test
        bash  theme={null}
        pnpm test:int
        bash  theme={null}
        pnpm format
        pnpm lint
        typescript  theme={null}
    describe("DocumentProcessor", () => {
        it("Should handle empty document list", () => {
            const processor = new DocumentProcessor();
            const result = processor.process([]);

expect(result.success).toBe(true);
            expect(result.processedCount).toBe(0);
            expect(result.errors).toHaveLength(0);
        });
    });
    typescript  theme={null}
    describe("ChatOpenAI", () => {
        it("Should test with real API", () => {
            const chat = new ChatOpenAI();
            const response = chat.invoke("Hello");
        });
    });
    typescript  theme={null}
    describe("APIService", () => {
        it("Should call with retry", () => {
            const mockClient = new MockClient();
            const service = new APIService(client: mockClient);
            const result = service.callWithRetry();
        });
    });
    ```
  </Tab>
</Tabs>

Our goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).

<Check>
  You're now ready to contribute high-quality code to LangChain!
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Integration tests

**Location**: `src/tests/FILENAME_BEING_TESTED.int.test.ts`

Integration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default.

Not every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process.

**Requirements**:

* Test real integrations with external services
* Use environment variables for API keys
* Skip gracefully if credentials unavailable
```

Example 2 (unknown):
```unknown
### Code quality standards

Contributions must adhere to the following quality requirements:

<Tabs>
  <Tab title="Type hints">
    **Required**: Complete types for all functions
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Documentation">
    **Required**: [JSDocs](https://jsdoc.app/about-getting-started) for all exported functions and interfaces
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Code style">
    **Automated**: Formatting and linting:
```

---

## Our SQL queries will only work if we filter on the exact string values that are in the DB.

**URL:** llms-txt#our-sql-queries-will-only-work-if-we-filter-on-the-exact-string-values-that-are-in-the-db.

---

## outer: before -> inner: before -> tool execution -> inner: after -> outer: after

**URL:** llms-txt#outer:-before-->-inner:-before-->-tool-execution-->-inner:-after-->-outer:-after

**Contents:**
  - Progress notifications
  - Logging
  - Elicitation

python Retry on error theme={null}
import asyncio

async def retry_interceptor(
    request: MCPToolCallRequest,
    handler,
    max_retries: int = 3,
    delay: float = 1.0,
):
    """Retry failed tool calls with exponential backoff."""
    last_error = None
    for attempt in range(max_retries):
        try:
            return await handler(request)
        except Exception as e:
            last_error = e
            if attempt < max_retries - 1:
                wait_time = delay * (2 ** attempt)  # Exponential backoff
                print(f"Tool {request.name} failed (attempt {attempt + 1}), retrying in {wait_time}s...")
                await asyncio.sleep(wait_time)
    raise last_error

client = MultiServerMCPClient(
    {...},
    tool_interceptors=[retry_interceptor],  # [!code highlight]
)
python Error handling with fallback theme={null}
async def fallback_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Return a fallback value if tool execution fails."""
    try:
        return await handler(request)
    except TimeoutError:
        return f"Tool {request.name} timed out. Please try again later."
    except ConnectionError:
        return f"Could not connect to {request.name} service. Using cached data."
python Progress callback theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext

async def on_progress(
    progress: float,
    total: float | None,
    message: str | None,
    context: CallbackContext,
):
    """Handle progress updates from MCP servers."""
    percent = (progress / total * 100) if total else progress
    tool_info = f" ({context.tool_name})" if context.tool_name else ""
    print(f"[{context.server_name}{tool_info}] Progress: {percent:.1f}% - {message}")

client = MultiServerMCPClient(
    {...},
    callbacks=Callbacks(on_progress=on_progress),  # [!code highlight]
)
python Logging callback theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext
from mcp.types import LoggingMessageNotificationParams

async def on_logging_message(
    params: LoggingMessageNotificationParams,
    context: CallbackContext,
):
    """Handle log messages from MCP servers."""
    print(f"[{context.server_name}] {params.level}: {params.data}")

client = MultiServerMCPClient(
    {...},
    callbacks=Callbacks(on_logging_message=on_logging_message),  # [!code highlight]
)
python MCP server with elicitation theme={null}
from pydantic import BaseModel
from mcp.server.fastmcp import Context, FastMCP

server = FastMCP("Profile")

class UserDetails(BaseModel):
    email: str
    age: int

@server.tool()
async def create_profile(name: str, ctx: Context) -> str:
    """Create a user profile, requesting details via elicitation."""
    result = await ctx.elicit(  # [!code highlight]
        message=f"Please provide details for {name}'s profile:",  # [!code highlight]
        schema=UserDetails,  # [!code highlight]
    )  # [!code highlight]
    if result.action == "accept" and result.data:
        return f"Created profile for {name}: email={result.data.email}, age={result.data.age}"
    if result.action == "decline":
        return f"User declined. Created minimal profile for {name}."
    return "Profile creation cancelled."

if __name__ == "__main__":
    server.run(transport="http")
python Handling elicitation requests theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.callbacks import Callbacks, CallbackContext
from mcp.shared.context import RequestContext
from mcp.types import ElicitRequestParams, ElicitResult

async def on_elicitation(
    mcp_context: RequestContext,
    params: ElicitRequestParams,
    context: CallbackContext,
) -> ElicitResult:
    """Handle elicitation requests from MCP servers."""
    # In a real application, you would prompt the user for input
    # based on params.message and params.requestedSchema
    return ElicitResult(  # [!code highlight]
        action="accept",  # [!code highlight]
        content={"email": "user@example.com", "age": 25},  # [!code highlight]
    )  # [!code highlight]

client = MultiServerMCPClient(
    {
        "profile": {
            "url": "http://localhost:8000/mcp",
            "transport": "http",
        }
    },
    callbacks=Callbacks(on_elicitation=on_elicitation),  # [!code highlight]
)
python Response action examples theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Error handling**

Use interceptors to catch tool execution errors and implement retry logic:
```

Example 2 (unknown):
```unknown
You can also catch specific error types and return fallback values:
```

Example 3 (unknown):
```unknown
### Progress notifications

Subscribe to progress updates for long-running tool executions:
```

Example 4 (unknown):
```unknown
The `CallbackContext` provides:

* `server_name`: Name of the MCP server
* `tool_name`: Name of the tool being executed (available during tool calls)

### Logging

The MCP protocol supports [logging](https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#log-levels) notifications from servers. Use the `Callbacks` class to subscribe to these events.
```

---

## 'outputs' will come from your target function.

**URL:** llms-txt#'outputs'-will-come-from-your-target-function.

**Contents:**
- Example: Single LLM call
- Example: Non-LLM component
- Example: Application or agent

def evaluator_one(inputs: dict, outputs: dict) -> bool:
    return outputs["foo"] == 2

def evaluator_two(inputs: dict, outputs: dict) -> bool:
    return len(outputs["bar"]) < 3

client = Client()
results = client.evaluate(
    dummy_target,  # <-- target function
    data="your-dataset-name",
    evaluators=[evaluator_one, evaluator_two],
    ...
)
python Python theme={null}
  from langsmith import wrappers
  from openai import OpenAI

# Optionally wrap the OpenAI client to automatically
  # trace all model calls.
  oai_client = wrappers.wrap_openai(OpenAI())

def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a 'messages' key.
    # You can update to match your dataset schema.
    messages = inputs["messages"]
    response = oai_client.chat.completions.create(
        messages=messages,
        model="gpt-4o-mini",
    )
    return {"answer": response.choices[0].message.content}
  typescript TypeScript theme={null}
  import OpenAI from 'openai';
  import { wrapOpenAI } from "langsmith/wrappers";

const client = wrapOpenAI(new OpenAI());

// This is the function you will evaluate.
  const target = async(inputs) => {
    // This assumes your dataset has inputs with a `messages` key
    const messages = inputs.messages;
    const response = await client.chat.completions.create({
        messages: messages,
        model: 'gpt-4o-mini',
    });
    return { answer: response.choices[0].message.content };
  }
  python Python (LangChain) theme={null}
  from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4o-mini")

def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a `messages` key
    messages = inputs["messages"]
    response = model.invoke(messages)
    return {"answer": response.content}
  typescript TypeScript (LangChain) theme={null}
  import { ChatOpenAI } from '@langchain/openai';

// This is the function you will evaluate.
  const target = async(inputs) => {
    // This assumes your dataset has inputs with a `messages` key
    const messages = inputs.messages;
    const model = new ChatOpenAI({ model: "gpt-4o-mini" });
    const response = await model.invoke(messages);
    return {"answer": response.content};
  }
  python Python theme={null}
  from langsmith import traceable

# Optionally decorate with '@traceable' to trace all invocations of this function.
  @traceable
  def calculator_tool(operation: str, number1: float, number2: float) -> str:
    if operation == "add":
        return str(number1 + number2)
    elif operation == "subtract":
        return str(number1 - number2)
    elif operation == "multiply":
        return str(number1 * number2)
    elif operation == "divide":
        return str(number1 / number2)
    else:
        raise ValueError(f"Unrecognized operation: {operation}.")

# This is the function you will evaluate.
  def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys.
    operation = inputs["operation"]
    number1 = inputs["num1"]
    number2 = inputs["num2"]
    result = calculator_tool(operation, number1, number2)
    return {"result": result}
  typescript TypeScript theme={null}
  import { traceable } from "langsmith/traceable";

// Optionally wrap in 'traceable' to trace all invocations of this function.
  const calculatorTool = traceable(async ({ operation, number1, number2 }) => {
  // Functions must return strings
  if (operation === "add") {
    return (number1 + number2).toString();
  } else if (operation === "subtract") {
    return (number1 - number2).toString();
  } else if (operation === "multiply") {
    return (number1 * number2).toString();
  } else if (operation === "divide") {
    return (number1 / number2).toString();
  } else {
    throw new Error("Invalid operation.");
  }
  });

// This is the function you will evaluate.
  const target = async (inputs) => {
  // This assumes your dataset has inputs with `operation`, `num1`, and `num2` keys
  const result = await calculatorTool.invoke({
    operation: inputs.operation,
    number1: inputs.num1,
    number2: inputs.num2,
  });
  return { result };
  }
  python Python theme={null}
  from my_agent import agent

# This is the function you will evaluate.
  def target(inputs: dict) -> dict:
    # This assumes your dataset has inputs with a `messages` key
    messages = inputs["messages"]
    # Replace `invoke` with whatever you use to call your agent
    response = agent.invoke({"messages": messages})
    # This assumes your agent output is in the right format
    return response
  typescript TypeScript theme={null}
  import { agent } from 'my_agent';

// This is the function you will evaluate.
  const target = async(inputs) => {
  // This assumes your dataset has inputs with a `messages` key
  const messages = inputs.messages;
  // Replace `invoke` with whatever you use to call your agent
  const response = await agent.invoke({ messages });
  // This assumes your agent output is in the right format
  return response;
  }
  python  theme={null}
  from my_agent import agent
  from langsmith import Client
  client = Client()
  client.evaluate(agent, ...)
  ```
</Check>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/define-target-function.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Check>
  `evaluate()` will automatically trace your target function. This means that if you run any traceable code within your target function, this will also be traced as child runs of the target trace.
</Check>

## Example: Single LLM call

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Output from node_1 contains private data that is not part of the overall state

**URL:** llms-txt#output-from-node_1-contains-private-data-that-is-not-part-of-the-overall-state

class Node1Output(TypedDict):
    private_data: str

---

## over the generic handler for all actions on the "threads" resource

**URL:** llms-txt#over-the-generic-handler-for-all-actions-on-the-"threads"-resource

@auth.on.threads
async def on_thread(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create.value
):
    # Setting metadata on the thread being created
    # will ensure that the resource contains an "owner" field
    # Then any time a user tries to access this thread or runs within the thread,
    # we can filter by owner
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    return {"owner": ctx.user.identity}

---

## Parallel processing of multiple data sources

**URL:** llms-txt#parallel-processing-of-multiple-data-sources

workflow.add_node("fetch_news", fetch_news)
workflow.add_node("fetch_weather", fetch_weather)
workflow.add_node("fetch_stocks", fetch_stocks)
workflow.add_node("combine_data", combine_all_data)

---

## Parent graph

**URL:** llms-txt#parent-graph

**Contents:**
- View subgraph state
- Stream subgraph outputs

builder = StateGraph(State)
builder.add_node("node_1", subgraph)
builder.add_edge(START, "node_1")

checkpointer = MemorySaver()
graph = builder.compile(checkpointer=checkpointer)
python  theme={null}
subgraph_builder = StateGraph(...)
subgraph = subgraph_builder.compile(checkpointer=True)
python  theme={null}
  from langgraph.graph import START, StateGraph
  from langgraph.checkpoint.memory import MemorySaver
  from langgraph.types import interrupt, Command
  from typing_extensions import TypedDict

class State(TypedDict):
      foo: str

def subgraph_node_1(state: State):
      value = interrupt("Provide value:")
      return {"foo": state["foo"] + value}

subgraph_builder = StateGraph(State)
  subgraph_builder.add_node(subgraph_node_1)
  subgraph_builder.add_edge(START, "subgraph_node_1")

subgraph = subgraph_builder.compile()

builder = StateGraph(State)
  builder.add_node("node_1", subgraph)
  builder.add_edge(START, "node_1")

checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "1"}}

graph.invoke({"foo": ""}, config)
  parent_state = graph.get_state(config)

# This will be available only when the subgraph is interrupted.
  # Once you resume the graph, you won't be able to access the subgraph state.
  subgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state

# resume the subgraph
  graph.invoke(Command(resume="bar"), config)
  python  theme={null}
for chunk in graph.stream(
    {"foo": "foo"},
    subgraphs=True, # [!code highlight]
    stream_mode="updates",
):
    print(chunk)
python  theme={null}
  from typing_extensions import TypedDict
  from langgraph.graph.state import StateGraph, START

# Define subgraph
  class SubgraphState(TypedDict):
      foo: str
      bar: str

def subgraph_node_1(state: SubgraphState):
      return {"bar": "bar"}

def subgraph_node_2(state: SubgraphState):
      # note that this node is using a state key ('bar') that is only available in the subgraph
      # and is sending update on the shared state key ('foo')
      return {"foo": state["foo"] + state["bar"]}

subgraph_builder = StateGraph(SubgraphState)
  subgraph_builder.add_node(subgraph_node_1)
  subgraph_builder.add_node(subgraph_node_2)
  subgraph_builder.add_edge(START, "subgraph_node_1")
  subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2")
  subgraph = subgraph_builder.compile()

# Define parent graph
  class ParentState(TypedDict):
      foo: str

def node_1(state: ParentState):
      return {"foo": "hi! " + state["foo"]}

builder = StateGraph(ParentState)
  builder.add_node("node_1", node_1)
  builder.add_node("node_2", subgraph)
  builder.add_edge(START, "node_1")
  builder.add_edge("node_1", "node_2")
  graph = builder.compile()

for chunk in graph.stream(
      {"foo": "foo"},
      stream_mode="updates",
      subgraphs=True, # [!code highlight]
  ):
      print(chunk)
  
  ((), {'node_1': {'foo': 'hi! foo'}})
  (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})
  (('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})
  ((), {'node_2': {'foo': 'hi! foobar'}})
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-subgraphs.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If you want the subgraph to **have its own memory**, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/python/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories:
```

Example 2 (unknown):
```unknown
## View subgraph state

When you enable [persistence](/oss/python/langgraph/persistence), you can [inspect the graph state](/oss/python/langgraph/persistence#checkpoints) (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.

You can inspect the graph state via `graph.get_state(config)`. To view the subgraph state, you can use `graph.get_state(config, subgraphs=True)`.

<Warning>
  **Available **only** when interrupted**
  Subgraph state can only be viewed **when the subgraph is interrupted**. Once you resume the graph, you won't be able to access the subgraph state.
</Warning>

<Accordion title="View interrupted subgraph state">
```

Example 3 (unknown):
```unknown
1. This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.
</Accordion>

## Stream subgraph outputs

To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.
```

Example 4 (unknown):
```unknown
<Accordion title="Stream from subgraphs">
```

---

## Patch Assistant

**URL:** llms-txt#patch-assistant

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/patch-assistant

langsmith/agent-server-openapi.json patch /assistants/{assistant_id}
Update an assistant.

---

## Patch Deployment

**URL:** llms-txt#patch-deployment

Source: https://docs.langchain.com/api-reference/deployments-v2/patch-deployment

https://api.host.langchain.com/openapi.json patch /v2/deployments/{deployment_id}
Patch a deployment by ID.

---

## Patch Listener

**URL:** llms-txt#patch-listener

Source: https://docs.langchain.com/api-reference/listeners-v2/patch-listener

https://api.host.langchain.com/openapi.json patch /v2/listeners/{listener_id}
Patch a listener by ID.

---

## Patch Thread

**URL:** llms-txt#patch-thread

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/patch-thread

langsmith/agent-server-openapi.json patch /threads/{thread_id}
Update a thread.

---

## path/to/embedding_function.py

**URL:** llms-txt#path/to/embedding_function.py

**Contents:**
- Querying via the API

from openai import AsyncOpenAI

client = AsyncOpenAI()

async def aembed_texts(texts: list[str]) -> list[list[float]]:
    """Custom embedding function that must:
    1. Be async
    2. Accept a list of strings
    3. Return a list of float arrays (embeddings)
    """
    response = await client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    return [e.embedding for e in response.data]
python  theme={null}
from langgraph_sdk import get_client

async def search_store():
    client = get_client()
    results = await client.store.search_items(
        ("memory", "facts"),
        query="your search query",
        limit=3  # number of results to return
    )
    return results

**Examples:**

Example 1 (unknown):
```unknown
## Querying via the API

You can also query the store using the LangGraph SDK. Since the SDK uses async operations:
```

---

## Persistence

**URL:** llms-txt#persistence

**Contents:**
- Threads
- Checkpoints
  - Get state

Source: https://docs.langchain.com/oss/python/langgraph/persistence

LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which can be accessed after graph execution. Because `threads` allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=966566aaae853ed4d240c2d0d067467c" alt="Checkpoints" data-og-width="2316" width="2316" data-og-height="748" height="748" data-path="oss/images/checkpoints.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=7bb8525bfcd22b3903b3209aa7497f47 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=e8d07fc2899b9a13c7b00eb9b259c3c9 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=46a2f9ed3b131a7c78700711e8c314d6 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=c339bd49757810dad226e1846f066c94 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=8333dfdb9d766363f251132f2dfa08a1 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=33ba13937eed043ba4a7a87b36d3046f 2500w" />

<Info>
  **Agent Server handles checkpointing automatically**
  When using the [Agent Server](/langsmith/agent-server), you don't need to implement or configure checkpointers manually. The server handles all persistence infrastructure for you behind the scenes.
</Info>

A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of [runs](/langsmith/assistants#execution). When a run is executed, the [state](/oss/python/langgraph/graph-api#state) of the underlying graph of the assistant will be persisted to the thread.

When invoking a graph with a checkpointer, you **must** specify a `thread_id` as part of the `configurable` portion of the config:

A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the [API reference](https://reference.langchain.com/python/langsmith/) for more details.

The checkpointer uses `thread_id` as the primary key for storing and retrieving checkpoints. Without it, the checkpointer cannot save state or resume execution after an [interrupt](/oss/python/langgraph/interrupts), since the checkpointer uses `thread_id` to load the saved state.

The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:

* `config`: Config associated with this checkpoint.
* `metadata`: Metadata associated with this checkpoint.
* `values`: Values of the state channels at this point in time.
* `next` A tuple of the node names to execute next in the graph.
* `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](/oss/python/langgraph/interrupts#pause-using-interrupt) from within a node, tasks will contain additional data associated with interrupts.

Checkpoints are persisted and can be used to restore the state of a thread at a later time.

Let's see what checkpoints are saved when a simple graph is invoked as follows:

After we run the graph, we expect to see exactly 4 checkpoints:

* Empty checkpoint with [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) as the next node to be executed
* Checkpoint with the user input `{'foo': '', 'bar': []}` and `node_a` as the next node to be executed
* Checkpoint with the outputs of `node_a` `{'foo': 'a', 'bar': ['a']}` and `node_b` as the next node to be executed
* Checkpoint with the outputs of `node_b` `{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed

Note that we `bar` channel values contain outputs from both nodes as we have a reducer for `bar` channel.

When interacting with the saved graph state, you **must** specify a [thread identifier](#threads). You can view the *latest* state of the graph by calling `graph.get_state(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the [API reference](https://reference.langchain.com/python/langsmith/) for more details.

The checkpointer uses `thread_id` as the primary key for storing and retrieving checkpoints. Without it, the checkpointer cannot save state or resume execution after an [interrupt](/oss/python/langgraph/interrupts), since the checkpointer uses `thread_id` to load the saved state.

## Checkpoints

The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties:

* `config`: Config associated with this checkpoint.
* `metadata`: Metadata associated with this checkpoint.
* `values`: Values of the state channels at this point in time.
* `next` A tuple of the node names to execute next in the graph.
* `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](/oss/python/langgraph/interrupts#pause-using-interrupt) from within a node, tasks will contain additional data associated with interrupts.

Checkpoints are persisted and can be used to restore the state of a thread at a later time.

Let's see what checkpoints are saved when a simple graph is invoked as follows:
```

Example 2 (unknown):
```unknown
After we run the graph, we expect to see exactly 4 checkpoints:

* Empty checkpoint with [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) as the next node to be executed
* Checkpoint with the user input `{'foo': '', 'bar': []}` and `node_a` as the next node to be executed
* Checkpoint with the outputs of `node_a` `{'foo': 'a', 'bar': ['a']}` and `node_b` as the next node to be executed
* Checkpoint with the outputs of `node_b` `{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed

Note that we `bar` channel values contain outputs from both nodes as we have a reducer for `bar` channel.

### Get state

When interacting with the saved graph state, you **must** specify a [thread identifier](#threads). You can view the *latest* state of the graph by calling `graph.get_state(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.
```

---

## Persistent file (survives across threads)

**URL:** llms-txt#persistent-file-(survives-across-threads)

**Contents:**
- Cross-thread persistence

agent.invoke({
    "messages": [{"role": "user", "content": "Save final report to /memories/report.txt"}]
})
python  theme={null}
import uuid

**Examples:**

Example 1 (unknown):
```unknown
## Cross-thread persistence

Files in `/memories/` can be accessed from any thread:
```

---

## Pick a dataset id. In this case, we are using the dataset we created above.

**URL:** llms-txt#pick-a-dataset-id.-in-this-case,-we-are-using-the-dataset-we-created-above.

---

## Prevent logging of sensitive data in traces

**URL:** llms-txt#prevent-logging-of-sensitive-data-in-traces

**Contents:**
- Rule-based masking of inputs and outputs
- Processing Inputs & Outputs for a Single Function
- Quick starts
  - Regex

Source: https://docs.langchain.com/langsmith/mask-inputs-outputs

In some situations, you may need to prevent the inputs and outputs of your traces from being logged for privacy or security reasons. LangSmith provides a way to filter the inputs and outputs of your traces before they are sent to the LangSmith backend.

If you want to completely hide the inputs and outputs of your traces, you can set the following environment variables when running your application:

This works for both the LangSmith SDK (Python and TypeScript) and LangChain.

You can also customize and override this behavior for a given `Client` instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object (`hideInputs` and `hideOutputs` in TypeScript).

For the example below, we will simply return an empty object for both `hide_inputs` and `hide_outputs`, but you can customize this to your needs.

## Rule-based masking of inputs and outputs

<Info>
  This feature is available in the following LangSmith SDK versions:

* Python: 0.1.81 and above
  * TypeScript: 0.1.33 and above
</Info>

To mask specific data in inputs and outputs, you can use the `create_anonymizer` / `createAnonymizer` function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.

The anonymizer will be skipped for inputs if `LANGSMITH_HIDE_INPUTS = true`. Same applies for outputs if `LANGSMITH_HIDE_OUTPUTS = true`.

However, if inputs or outputs are to be sent to client, the `anonymizer` method will take precedence over functions found in `hide_inputs` and `hide_outputs`. By default, the `create_anonymizer` will only look at maximum of 10 nesting levels deep, which can be configured via the `max_depth` parameter.

Please note, that using the anonymizer might incur a performance hit with complex regular expressions or large payloads, as the anonymizer serializes the payload to JSON before processing.

<Note>
  Improving the performance of `anonymizer` API is on our roadmap! If you are encountering performance issues, please contact support via [support.langchain.com](https://support.langchain.com).
</Note>

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ac9ba9a6729029a7fa38da03e1466a1a" alt="Hide inputs outputs" data-og-width="1708" width="1708" data-og-height="717" height="717" data-path="langsmith/images/hide-inputs-outputs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7ded12c0345f47d55e9802083c5032d0 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8cbe74d09660d8c65e8a75dd78cdb24e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=8cb8c0b5c926e46522b9539b0262ee7a 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=9b8ef244796fad943ec76b0aa5733f80 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=87f35d63f42f05c49a220d5b8a87787a 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/hide-inputs-outputs.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=5173e30032c065646c13e9b9c6a95fb5 2500w" />

Older versions of LangSmith SDKs can use the `hide_inputs` and `hide_outputs` parameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well.

## Processing Inputs & Outputs for a Single Function

<Info>
  The `process_outputs` parameter is available in LangSmith SDK version 0.1.98 and above for Python.
</Info>

In addition to client-level input and output processing, LangSmith provides function-level processing through the `process_inputs` and `process_outputs` parameters of the `@traceable` decorator.

These parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function.

Here's an example of how to use `process_inputs` and `process_outputs`:

In this example, `process_inputs` creates a new dictionary with processed input data, and `process_outputs` transforms the output into a specific format before logging to LangSmith.

<Warning>
  It's recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data.
</Warning>

For asynchronous functions, the usage is similar:

These function-level processors take precedence over client-level processors (`hide_inputs` and `hide_outputs`) when both are defined.

You can combine rule-based masking with various anonymizers to scrub sensitive information from inputs and outputs. In this how-to-guide, we'll cover working with regex, Microsoft Presidio, and Amazon Comprehend.

<Info>
  The implementation below is not exhaustive and may miss some formats or edge cases. Test any implementation thoroughly before using it in production.
</Info>

You can use regex to mask inputs and outputs before they are sent to LangSmith. The implementation below masks email addresses, phone numbers, full names, credit card numbers, and SSNs.

```python  theme={null}
import re
import openai
from langsmith import Client
from langsmith.wrappers import wrap_openai

**Examples:**

Example 1 (unknown):
```unknown
This works for both the LangSmith SDK (Python and TypeScript) and LangChain.

You can also customize and override this behavior for a given `Client` instance. This can be done by setting the `hide_inputs` and `hide_outputs` parameters on the `Client` object (`hideInputs` and `hideOutputs` in TypeScript).

For the example below, we will simply return an empty object for both `hide_inputs` and `hide_outputs`, but you can customize this to your needs.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Rule-based masking of inputs and outputs

<Info>
  This feature is available in the following LangSmith SDK versions:

  * Python: 0.1.81 and above
  * TypeScript: 0.1.33 and above
</Info>

To mask specific data in inputs and outputs, you can use the `create_anonymizer` / `createAnonymizer` function and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.

The anonymizer will be skipped for inputs if `LANGSMITH_HIDE_INPUTS = true`. Same applies for outputs if `LANGSMITH_HIDE_OUTPUTS = true`.

However, if inputs or outputs are to be sent to client, the `anonymizer` method will take precedence over functions found in `hide_inputs` and `hide_outputs`. By default, the `create_anonymizer` will only look at maximum of 10 nesting levels deep, which can be configured via the `max_depth` parameter.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Pricing plans

**URL:** llms-txt#pricing-plans

Source: https://docs.langchain.com/langsmith/pricing-plans

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/pricing-plans.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Print the conversation

**URL:** llms-txt#print-the-conversation

for message in result["messages"]:
    if hasattr(message, 'pretty_print'):
        message.pretty_print()
    else:
        print(f"{message.type}: {message.content}")

================================ Human Message =================================

Write a SQL query to find all customers who made orders over $1000 in the last month
================================== Ai Message ==================================
Tool Calls:
  load_skill (call_abc123)
 Call ID: call_abc123
  Args:
    skill_name: sales_analytics
================================= Tool Message =================================
Name: load_skill

Loaded skill: sales_analytics

**Examples:**

Example 1 (unknown):
```unknown
Expected output:
```

---

## Proactive Approach (recommended) - using RemainingSteps

**URL:** llms-txt#proactive-approach-(recommended)---using-remainingsteps

def agent_with_monitoring(state: State) -> dict:
    """Proactively monitor and handle recursion within the graph"""
    remaining = state["remaining_steps"]

# Early detection - route to internal handling
    if remaining <= 2:
        return {
            "messages": ["Approaching limit, returning partial result"]
        }

# Normal processing
    return {"messages": [f"Processing... ({remaining} steps remaining)"]}

def route_decision(state: State) -> Literal["agent", END]:
    if state["remaining_steps"] <= 2:
        return END
    return "agent"

---

## Proactive: Graph completes gracefully

**URL:** llms-txt#proactive:-graph-completes-gracefully

result = graph.invoke({"messages": []}, {"recursion_limit": 10})

---

## Process all results after evaluation completes

**URL:** llms-txt#process-all-results-after-evaluation-completes

**Contents:**
- Related

for result in results:
    print("Input:", result["run"].inputs)
    print("Output:", result["run"].outputs)

# Access individual evaluation results
    for eval_result in result["evaluation_results"]["results"]:
        print(f"  {eval_result.key}: {eval_result.score}")
```

With `blocking=True`, your processing code runs only after all evaluations are complete, avoiding mixed output with evaluation logs.

For more information on running evaluations without uploading results, refer to [Run an evaluation locally](/langsmith/local).

* [Evaluate your LLM application](/langsmith/evaluate-llm-application)
* [Run an evaluation locally](/langsmith/local)
* [Fetch performance metrics from an experiment](/langsmith/fetch-perf-metrics-experiment)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/read-local-experiment-results.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Process final result

**URL:** llms-txt#process-final-result

**Contents:**
- Multiple tool calls
- Edit tool arguments
- Subagent interrupts
- Best practices
  - Always use a checkpointer
  - Use the same thread ID

print(result["messages"][-1].content)
python  theme={null}
config = {"configurable": {"thread_id": str(uuid.uuid4())}}

result = agent.invoke({
    "messages": [{
        "role": "user",
        "content": "Delete temp.txt and send an email to admin@example.com"
    }]
}, config=config)

if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]

# Two tools need approval
    assert len(action_requests) == 2

# Provide decisions in the same order as action_requests
    decisions = [
        {"type": "approve"},  # First tool: delete_file
        {"type": "reject"}    # Second tool: send_email
    ]

result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
python  theme={null}
if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_request = interrupts["action_requests"][0]

# Original args from the agent
    print(action_request["args"])  # {"to": "everyone@company.com", ...}

# User decides to edit the recipient
    decisions = [{
        "type": "edit",
        "edited_action": {
            "name": action_request["name"],  # Must include the tool name
            "args": {"to": "team@company.com", "subject": "...", "body": "..."}
        }
    }]

result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
python  theme={null}
agent = create_deep_agent(
    tools=[delete_file, read_file],
    interrupt_on={
        "delete_file": True,
        "read_file": False,
    },
    subagents=[{
        "name": "file-manager",
        "description": "Manages file operations",
        "system_prompt": "You are a file management assistant.",
        "tools": [delete_file, read_file],
        "interrupt_on": {
            # Override: require approval for reads in this subagent
            "delete_file": True,
            "read_file": True,  # Different from main agent!
        }
    }],
    checkpointer=checkpointer
)
python  theme={null}
from langgraph.checkpoint.memory import MemorySaver

checkpointer = MemorySaver()
agent = create_deep_agent(
    tools=[...],
    interrupt_on={...},
    checkpointer=checkpointer  # Required for HITL
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Multiple tool calls

When the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order.
```

Example 2 (unknown):
```unknown
## Edit tool arguments

When `"edit"` is in the allowed decisions, you can modify the tool arguments before execution:
```

Example 3 (unknown):
```unknown
## Subagent interrupts

Each subagent can have its own `interrupt_on` configuration that overrides the main agent's settings:
```

Example 4 (unknown):
```unknown
When a subagent triggers an interrupt, the handling is the same – check for `__interrupt__` and resume with `Command`.

## Best practices

### Always use a checkpointer

Human-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume:
```

---

## Prompt engineering concepts

**URL:** llms-txt#prompt-engineering-concepts

**Contents:**
- Why prompt engineering?
- Prompts vs. prompt templates
- Prompts in LangSmith
  - Chat vs Completion
  - F-string vs. mustache
  - Tools
  - Structured output
  - Model
- Prompt versioning
  - Commits

Source: https://docs.langchain.com/langsmith/prompt-engineering-concepts

While traditional software applications are built by writing code, AI applications often derive their logic from prompts.

This guide will walk through the key concepts of prompt engineering in LangSmith.

## Why prompt engineering?

A prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's behavior without changing its underlying capabilities. Just as telling an actor to "be a pirate" determines how they act, a prompt provides instructions, examples, and context that shape how the model responds.

Prompt engineering is important because it allows you to change the way the model behaves. While there are other ways to change the model's behavior (like fine-tuning), prompt engineering is usually the simplest to get started with and often provides the highest ROI.

We often see that prompt engineering is multi-disciplinary. Sometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager or another domain expert. It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.

## Prompts vs. prompt templates

Although we often use these terms interchangably, it is important to understand the difference between "prompts" and "prompt templates".

Prompts refer to the messages that are passed into the language model.

Prompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates can include variables for few shot examples, outside context, or any other external data that is needed in your prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=714ad4962c85cfb8847ebbf01559c217" alt="Prompt vs prompt template" data-og-width="1084" width="1084" data-og-height="450" height="450" data-path="langsmith/images/prompt-vs-prompt-template.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=81497578f297dd5a7311de6f1c06ef85 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=d13dfcf34430d876cdc1a2b77a5fd3c4 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=feec2461df390de635a2534b50eab8e6 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=e1851de62aa377fc3d70ec62dd91c5b2 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=6bccce7c02cb065f9ef8964fbe3112b6 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-vs-prompt-template.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=c3494cef2e70686d55493ded764cc8c0 2500w" />

## Prompts in LangSmith

You can store and version prompts templates in LangSmith. There are few key aspects of a prompt template to understand.

### Chat vs Completion

There are two different types of prompts: `chat` style prompts and `completion` style prompts.

Chat style prompts are a **list of messages**. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.

Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.

### F-string vs. mustache

You can format your prompt with input variables using either [f-string](https://realpython.com/python-f-strings/) or [mustache](https://mustache.github.io/mustache.5.html) format. Here is an example prompt with f-string format:

And here is one with mustache:

To add a conditional mustache prompt:

* The playground UI will pick up `is_logged_in` variable, but any nested variables you'll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:

<Check>
  The LangSmith Playground uses `f-string` as the default template format, but you can switch to `mustache` format in the prompt settings/template format section. `mustache` gives you more flexibility around conditional variables, loops, and nested keys. For conditional variables, you'll need to manually add json variables in the 'inputs' section. Read [the documentation](https://mustache.github.io/mustache.5.html)
</Check>

Tools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description, and JSON schema of arguments used to call the tool.

### Structured output

Structured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they stick to a specified schema. This may or may not use [Tools](#tools) under the hood.

<Check>
  Structured output is similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM **always** responds in this format. With tools, the LLM may select **multiple** tools; with structured output, only one response is generate.
</Check>

Optionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).

Verisioning is a key part of iterating and collaborating on your different prompts.

Every saved update to a prompt creates a new commit with a unique commit hash. This allows you to:

* View the full history of changes to a prompt.
* Review earlier versions.
* Revert to a previous state if needed.
* Reference specific versions in your code using the commit hash (e.g., `client.pull_prompt("prompt_name:commit_hash")`).

In the UI, you can compare a commit with its previous version by toggling **Show diff** in the top-right corner of the **Commits** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a045b239f92dc1c616c7e42ae282c2b9" alt="The commit hashes list for a prompt with the diff of one commit." data-og-width="2884" width="2884" data-og-height="1426" height="1426" data-path="langsmith/images/commit-diff.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5f1f1e66762226af9ec78e7dcd88a40e 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9170a56b51a619a3aea586942c6d1825 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=198ae20ca965b50d42178bb241bddf46 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5e56c9323edf28650c4c667def63d7d1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=25b8995161ed4523c09526eee44aef8a 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/commit-diff.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ac495d702a4df63f35ac47e5584a1c4b 2500w" />

Commit tags are human-readable labels that point to specific commits in your prompt's history. Unlike commit hashes, tags can be moved to point to different commits, allowing you to update which version your code references without changing the code itself.

Use cases for commit tags can include:

* **Environment-specific tags**: Mark commits for `production` or `staging` environments, which allows you to switch between different versions without changing your code.
* **Version control**: Mark stable versions of your prompts, for example, `v1`, `v2`, which lets you reference specific versions in your code and track changes over time.
* **Collaboration**: Mark versions ready for review, which enables you to share specific versions with collaborators and get feedback.

<Note>
  **Not to be confused with resource tags**: Commit tags reference specific prompt versions. [Resource tags](/langsmith/set-up-resource-tags) are key-value pairs used to organize workspace resources.
</Note>

For detailed information on creating and managing commit tags, see [Manage prompts](/langsmith/manage-prompts#commit-tags).

The prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.

In the playground you can:

* Change the model being used
* Change prompt template being used
* Change the output schema
* Change the tools available
* Enter the input variables to run through the prompt template
* Run the prompt through the model
* Observe the outputs

<Callout type="info" icon="bird">
  Use **[Polly](/langsmith/polly)** in the Playground to optimize prompts, generate tools, and create output schemas with AI assistance.
</Callout>

## Testing multiple prompts

You can add more prompts to your playground to easily compare outputs and decide which version is better:

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-prompt-to-playground.gif?s=1c6f0c32b45a3f480b16d704c09570fc" alt="Add prompt to playground" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/add-prompt-to-playground.gif" data-optimize="true" data-opv="3" />

## Testing over a dataset

To test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results are streamed back as well as how many repitions there are in the test.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/test-over-dataset-in-playground.gif?s=aaf0f90a0c61934a928f81d5e11e2c35" alt="Test over dataset in playground" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/test-over-dataset-in-playground.gif" data-optimize="true" data-opv="3" />

You can click on the "View Experiment" button to dive deeper into the results of the test.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/h4f6bIWGkog?si=IVJFfhldC7M3HL4G" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-engineering-concepts.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
And here is one with mustache:
```

Example 2 (unknown):
```unknown
To add a conditional mustache prompt:
```

Example 3 (unknown):
```unknown
* The playground UI will pick up `is_logged_in` variable, but any nested variables you'll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:
```

---

## Prompt engineering

**URL:** llms-txt#prompt-engineering

Source: https://docs.langchain.com/langsmith/prompt-engineering

The following sections help you create, manage, and optimize your prompts:

<Columns cols={3}>
  <Card title="Review core concepts" icon="circle-info" href="/langsmith/prompt-engineering-concepts" arrow="true">
    Read definitions and key terminology for prompt engineering in LangSmith.
  </Card>

<Card title="Create and update prompts" icon="pen-to-square" href="/langsmith/create-a-prompt" arrow="true">
    Build prompts via the UI or SDK, configure settings, use tools, add multimodal inputs, and connect model providers.
  </Card>

<Card title="Manage prompts" icon="tags" href="/langsmith/manage-prompts" arrow="true">
    Organize with tags, commit changes, trigger webhooks, and share through the public prompt hub.
  </Card>

<Card title="Explore the prompt hub" icon="folder-tree" href="/langsmith/manage-prompts#public-prompt-hub" arrow="true">
    Browse and manage prompt tags and discover community prompts from the LangChain Hub.
  </Card>

<Card title="Open the prompt playground" icon="vial" href="/langsmith/prompt-engineering-concepts#prompt-playground" arrow="true">
    Test and experiment with prompts using custom endpoints and model configurations.
  </Card>

<Card title="Follow tutorials" icon="book-open" href="/langsmith/optimize-classifier" arrow="true">
    Learn step-by-step techniques, like optimizing classifiers and advanced prompt engineering.
  </Card>
</Columns>

<Callout type="info" icon="bird">
  Use **[Polly](/langsmith/polly)** in the Prompt Playground to optimize prompts, generate tools, and create output schemas with AI-powered assistance.
</Callout>

<Note>
  To set up a LangSmith instance, visit the [Platform setup section](/langsmith/platform-setup) to choose between cloud, hybrid, or self-hosted. All options include observability, evaluation, prompt engineering, and deployment.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/prompt-engineering.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Property access

**URL:** llms-txt#property-access

---

## Provider-native format (e.g., OpenAI)

**URL:** llms-txt#provider-native-format-(e.g.,-openai)

human_message = HumanMessage(content=[
    {"type": "text", "text": "Hello, how are you?"},
    {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
])

---

## Provider-specific middleware

**URL:** llms-txt#provider-specific-middleware

Source: https://docs.langchain.com/oss/python/integrations/middleware/index

Middleware designed for specific providers. Learn more about [middleware](/oss/python/langchain/middleware/overview).

| Provider                                                   | Middleware available                                            |
| ---------------------------------------------------------- | --------------------------------------------------------------- |
| [Anthropic](/oss/python/integrations/middleware/anthropic) | Prompt caching, bash tool, text editor, memory, and file search |
| [OpenAI](/oss/python/integrations/middleware/openai)       | Content moderation                                              |

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/middleware/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Publicly available test files

**URL:** llms-txt#publicly-available-test-files

pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
wav_url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
img_url = "https://www.w3.org/Graphics/PNG/nurbcup2si.png"

---

## Publish an integration

**URL:** llms-txt#publish-an-integration

**Contents:**
- Publishing your package
  - Setup credentials
  - Build and publish
- Adding documentation
  - Writing docs
  - Submitting a PR
- Next steps

Source: https://docs.langchain.com/oss/python/contributing/publish-langchain

**Make your integration available to the community.**

<Warning>
  **Important: New integrations should be standalone packages, not PRs to the LangChain monorepo.**

While LangChain maintains a small subset of first-party and high-usage integrations (like OpenAI, Anthropic, and Ollama) in the main repository, **new integrations should be published as separate PyPI packages and repositories** (e.g., `langchain-yourservice`) that users install alongside the core LangChain packages. You **should not** submit a PR to add your integration directly to the main LangChain repository.
</Warning>

Now that your package is implemented and tested, you can publish it and add documentation to make it discoverable by the community.

## Publishing your package

<Info>
  This guide assumes you have already implemented your package and written tests for it. If you haven't, please refer to the [implementation guide](/oss/python/contributing/implement-langchain) and [testing guide](/oss/python/contributing/standard-tests-langchain).
</Info>

For the purposes of this guide, we'll be using PyPI as the package registry. You may choose to publish to other registries if you prefer; instructions will vary.

### Setup credentials

First, make sure you have a PyPI account:

<AccordionGroup>
  <Accordion title="How to create a PyPI Token" icon="key">
    <Steps>
      <Step title="Create account">
        Go to the [PyPI website](https://pypi.org/) and create an account
      </Step>

<Step title="Verify email">
        Verify your email address by clicking the link that PyPI emails to you
      </Step>

<Step title="Enable 2FA">
        Go to your account settings and click "Generate Recovery Codes" to enable 2FA. To generate an API token, you **must** have 2FA enabled
      </Step>

<Step title="Generate token">
        Go to your account settings and [generate a new API token](https://pypi.org/manage/account/token/)
      </Step>
    </Steps>
  </Accordion>
</AccordionGroup>

### Build and publish

<Card title="How to publish a package" icon="upload" href="https://docs.astral.sh/uv/guides/package/" arrow>
  Helpful guide from `uv` on how to build and publish a package to PyPI.
</Card>

## Adding documentation

To add documentation for your package to this site under the [integrations tab](/oss/python/integrations/providers/overview), you will need to create the relevant documentation pages and open a PR in the [LangChain docs repository](https://github.com/langchain-ai/docs).

Depending on the type of integration you have built, you will need to create different types of documentation pages. LangChain provides templates for different types of integrations to help you get started.

<CardGroup>
  <Card title="Chat models" icon="message" href="https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/chat/TEMPLATE.mdx" arrow />

<Card title="Tools/toolkits" icon="wrench" href="https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/tools/TEMPLATE.mdx" arrow />

<Card title="Retrievers" icon="magnifying-glass" href="https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/retrievers/TEMPLATE.mdx" arrow />

<Card title="Vector stores" icon="database" href="https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/vectorstores/TEMPLATE.mdx" arrow />

<Card title="Embedding models" icon="layer-group" href="https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/text_embedding/TEMPLATE.mdx" arrow />
</CardGroup>

<Tip>
  To reference existing documentation, you can look at the [list of integrations](/oss/python/integrations/providers/overview) and find similar ones to yours.

To view a given documentation page in raw markdown, use the dropdown button next to "Copy page" on the top right of the page and select "View as Markdown".
</Tip>

Make a fork of the [LangChain docs repository](https://github.com/langchain-ai/docs) under a personal GitHub account, and clone it locally. Create a new branch for your integration. Copy the template and modify them using your favorite markdown text editor. Make sure to refer to and follow the [documentation guide](/oss/python/contributing/documentation) when writing your documentation.

<Warning>
  We may reject PRs or ask for modification if:

* CI checks fail
  * Severe grammatical errors or typos are present
  * [Mintlify components](/oss/python/contributing/documentation#mintlify-components) are used incorrectly
  * Pages are missing a [frontmatter](/oss/python/contributing/documentation#page-structure)
  * [Localization](/oss/python/contributing/documentation#localization) is missing (where applicable)
  * [Code examples](/oss/python/contributing/documentation#in-code-documentation) do not run or have errors
  * [Quality standards](/oss/python/contributing/documentation#quality-standards) are not met
</Warning>

Please be patient as we handle a large volume of PRs. We will review your PR as soon as possible and provide feedback or merge it. **Do not repeatedly tag maintainers about your PR.**

**Congratulations!** Your integration is now published and documented, making it available to the entire LangChain community.

<Card title="Co-marketing" icon="bullhorn" href="/oss/python/contributing/comarketing" arrow>
  Get in touch with the LangChain marketing team to explore co-marketing opportunities.
</Card>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/publish-langchain.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Pull the images from the public registry

**URL:** llms-txt#pull-the-images-from-the-public-registry

**Contents:**
- Configuration

docker pull langchain/langsmith-backend:latest
docker tag langchain/langsmith-backend:latest <your-registry>/langsmith-backend:latest
docker push <your-registry>/langsmith-backend:latest
yaml Helm theme={null}
  images:
    imagePullSecrets: [] # Add your image pull secrets here if needed
    registry: "" # Set this to your registry URL if you mirrored all images to the same registry using our script. Then you can remove the repository prefix from the images below.
    aceBackendImage:
      repository: "(your-registry)/langchain/langsmith-ace-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    backendImage:
      repository: "(your-registry)/langchain/langsmith-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    frontendImage:
      repository: "(your-registry)/langchain/langsmith-frontend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    hostBackendImage:
      repository: "(your-registry)/langchain/hosted-langserve-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    operatorImage:
      repository: "(your-registry)/langchain/langgraph-operator"
      pullPolicy: IfNotPresent
      tag: "6cc83a8"
    platformBackendImage:
      repository: "(your-registry)/langchain/langsmith-go-backend"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    playgroundImage:
      repository: "(your-registry)/langchain/langsmith-playground"
      pullPolicy: IfNotPresent
      tag: "0.10.66"
    postgresImage:
      repository: "(your-registry)/postgres"
      pullPolicy: IfNotPresent
      tag: "14.7"
    redisImage:
      repository: "(your-registry)/redis"
      pullPolicy: IfNotPresent
      tag: "7"
    clickhouseImage:
      repository: "(your-registry)/clickhouse/clickhouse-server"
      pullPolicy: Always
      tag: "24.8"
  bash Docker theme={null}
  # In your .env file
  _REGISTRY=your-registry # Set this to your registry URL if you mirrored all images to the same registry using our script. Otherwise you will need to manually set the repository for each image in the compose file.
  ```
</CodeGroup>

Once configured, you will need to update your LangSmith installation. You can follow our upgrade guide here: [Upgrading LangSmith](/langsmith/self-host-upgrades).If your upgrade is successful, your LangSmith instance should now be using the mirrored images from your Docker registry.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-mirroring-images.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
You will need to repeat this for each image that you want to mirror.

## Configuration

Once the images are mirrored, you will need to configure your LangSmith installation to use the mirrored images. You can do this by modifying the `values.yaml` file for your LangSmith Helm Chart installation or the `.env` file for your Docker installation. Replace tag with the version you want to use, e.g. `0.10.66` for the latest version at the time of writing.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Push to your container registry

**URL:** llms-txt#push-to-your-container-registry

**Contents:**
  - Connect to Your Deployed Agent
  - Environment configuration

docker push my-agent:latest
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can push to any container registry (Docker Hub, AWS ECR, Azure ACR, Google GCR, etc.) that your deployment environment has access to.

**Supported deployments:**

* <Icon icon="cloud" /> **Cloud LangSmith**: Use the Control Plane API to create deployments from your GitHub repository
* <Icon icon="server" /> **Self-Hosted/Hybrid LangSmith**: Use the Control Plane API to create deployments from your container registry

See the [LangGraph CLI build documentation](/langsmith/cli#build) for more details.

### Connect to Your Deployed Agent

* <Icon icon="code" /> **[LangGraph SDK](https://langchain-ai.github.io/langgraph/cloud/reference/sdk/python_sdk_ref/#langgraph-sdk-python)**: Use the LangGraph SDK for programmatic integration.
* <Icon icon="project-diagram" /> **[RemoteGraph](/langsmith/use-remote-graph)**: Connect using RemoteGraph for remote graph connections (to use your graph in other graphs).
* <Icon icon="globe" /> **[REST API](/langsmith/server-api-ref)**: Use HTTP-based interactions with your deployed agent.
* <Icon icon="desktop" /> **[Studio](/langsmith/studio)**: Access the visual interface for testing and debugging.

### Environment configuration

#### Database & cache configuration

By default, LangSmith Deployment create PostgreSQL and Redis instances for you. To use external services, set the following environment variables in your new deployment or revision:
```

---

## Query traces (SDK)

**URL:** llms-txt#query-traces-(sdk)

**Contents:**
- Use filter arguments
  - List all runs in a project
  - List LLM and Chat runs in the last 24 hours
  - List root runs in a project
  - List runs without errors
  - List runs by run ID
- Use filter query language
  - List all root runs in a conversational thread
  - List all runs called "extractor" whose root of the trace was assigned feedback "user\_score" score of 1
  - List runs with "star\_rating" key whose score is greater than 4

Source: https://docs.langchain.com/langsmith/export-traces

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* [Run (span) data format](/langsmith/run-data-format)
  * <RegionalUrl type="api" suffix="/redoc" text="LangSmith API Reference" />
  * [LangSmith trace query syntax](/langsmith/trace-query-syntax)
</Tip>

<Note>
  **If you are looking to export a large volume of traces, we recommend that you use the [Bulk Data Export](./data-export) functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions.**
</Note>

The recommended way to query runs (the span data in LangSmith traces) is to use the `list_runs` method in the SDK or `/runs/query` endpoint in the API.

LangSmith stores traces in a simple format that is specified in the [Run (span) data format](/langsmith/run-data-format).

## Use filter arguments

For simple queries, you don't have to rely on our query syntax. You can use the filter arguments specified in the [filter arguments reference](/langsmith/trace-query-syntax#filter-arguments).

<Warning>
  **Prerequisites**

Initialize the client before running the below code snippets.
</Warning>

Below are some examples of ways to list runs using keyword arguments:

### List all runs in a project

### List LLM and Chat runs in the last 24 hours

### List root runs in a project

Root runs are runs that have no parents. These are assigned a value of `True` for `is_root`. You can use this to filter for root runs.

### List runs without errors

### List runs by run ID

<Warning>
  **Ignores Other Arguments**

If you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like `project_name`, `run_type`, etc. and directly return the runs matching the given IDs.
</Warning>

If you have a list of run IDs, you can list them directly:

## Use filter query language

For more complex queries, you can use the query language described in the [filter query language reference](/langsmith/trace-query-syntax#filter-query-language).

### List all root runs in a conversational thread

This is the way to fetch runs in a conversational thread. For more information on setting up threads, refer to our [how-to guide on setting up threads](./threads).
Threads are grouped by setting a shared thread ID. The LangSmith UI lets you use any one of the following three metadata keys: `session_id`, `conversation_id`, or `thread_id`. The session ID is also known as the tracing project ID. The following query matches on any of them.

### List all runs called "extractor" whose root of the trace was assigned feedback "user\_score" score of 1

### List runs with "star\_rating" key whose score is greater than 4

### List runs that took longer than 5 seconds to complete

### List all runs that have "error" not equal to null

### List all runs where start\_time is greater than a specific timestamp

### List all runs that contain the string "substring"

### List all runs that are tagged with the git hash "2aa1cf4"

### List all runs that started after a specific timestamp and either have "error" not equal to null or a "Correctness" feedback score equal to 0

### Complex query: List all runs where tags include "experimental" or "beta" and latency is greater than 2 seconds

### Search trace trees by full text

You can use the `search()` function without any specific field to do a full text search across all string fields in a run. This allows you to quickly find traces that match a search term.

### Check for presence of metadata

If you want to check for the presence of metadata, you can use the `eq` operator, optionally with an `and` statement to match by value. This is useful if you want to log more structured information about your runs.

### Check for environment details in metadata

A common pattern is to add environment information to your traces via metadata. If you want to filter for runs containing environment metadata, you can use the same pattern as above:

### Check for conversation ID in metadata

Another common way to associate traces in the same conversation is by using a shared conversation ID. If you want to filter runs based on a conversation ID in this way, you can search for that ID in the metadata.

### Negative filtering on key-value pairs

You can use negative filtering on metadata, input, and output key-value pairs to exclude specific runs from your results. Here are some examples for metadata key-value pairs but the same logic applies to input and output key-value pairs.

### Combine multiple filters

If you want to combine multiple conditions to refine your search, you can use the `and` operator along with other filtering functions. Here's how you can search for runs named "ChatOpenAI" that also have a specific `conversation_id` in their metadata:

List all runs named "RetrieveDocs" whose root run has a "user\_score" feedback of 1 and any run in the full trace is named "ExpandQuery".

This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.

### Advanced: export flattened trace view with child tool usage

The following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace.
This can be used to analyze the behavior of your agents across multiple traces.

This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information.

To optimize the query, the example:

1. Selects only the necessary fields when querying tool runs to reduce query time.
2. Fetches root runs in batches while processing tool runs concurrently.

<CodeGroup>
  
</CodeGroup>

### Advanced: export retriever IO for traces with feedback

This query is useful if you want to fine-tune embeddings or diagnose end-to-end system performance issues based on retriever behavior.
The following Python example demonstrates how to export retriever inputs and outputs within traces that have a specific feedback score.

<CodeGroup>
  
</CodeGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/export-traces.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

Below are some examples of ways to list runs using keyword arguments:

### List all runs in a project

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

### List LLM and Chat runs in the last 24 hours

<CodeGroup>
```

---

## Reactive Approach (fallback) - catching error externally

**URL:** llms-txt#reactive-approach-(fallback)---catching-error-externally

**Contents:**
- Visualization

try:
    result = graph.invoke({"messages": []}, {"recursion_limit": 10})
except GraphRecursionError as e:
    # Handle externally after graph execution fails
    result = {"messages": ["Fallback: recursion limit exceeded"]}
python  theme={null}
def inspect_metadata(state: dict, config: RunnableConfig) -> dict:
    metadata = config["metadata"]

print(f"Step: {metadata['langgraph_step']}")
    print(f"Node: {metadata['langgraph_node']}")
    print(f"Triggers: {metadata['langgraph_triggers']}")
    print(f"Path: {metadata['langgraph_path']}")
    print(f"Checkpoint NS: {metadata['langgraph_checkpoint_ns']}")

It's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](/oss/python/langgraph/use-graph-api#visualize-your-graph) for more info.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/graph-api.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The key differences between these approaches are:

| Approach                                  | Detection            | Handling                             | Control Flow                       |
| ----------------------------------------- | -------------------- | ------------------------------------ | ---------------------------------- |
| Proactive (using `RemainingSteps`)        | Before limit reached | Inside graph via conditional routing | Graph continues to completion node |
| Reactive (catching `GraphRecursionError`) | After limit exceeded | Outside graph in try/catch           | Graph execution terminated         |

**Proactive advantages:**

* Graceful degradation within the graph
* Can save intermediate state in checkpoints
* Better user experience with partial results
* Graph completes normally (no exception)

**Reactive advantages:**

* Simpler implementation
* No need to modify graph logic
* Centralized error handling

#### Other available metadata

Along with `langgraph_step`, the following metadata is also available in `config["metadata"]`:
```

---

## Reading a thread. Since this is also more specific than the generic @auth.on handler, and the @auth.on.threads handler,

**URL:** llms-txt#reading-a-thread.-since-this-is-also-more-specific-than-the-generic-@auth.on-handler,-and-the-@auth.on.threads-handler,

---

## "reasoning_output": True,

**URL:** llms-txt#"reasoning_output":-true,

---

## Rebuild graph at runtime

**URL:** llms-txt#rebuild-graph-at-runtime

**Contents:**
- Prerequisites
- Define graphs
  - No rebuild
  - Rebuild

Source: https://docs.langchain.com/langsmith/graph-rebuild

You might need to rebuild your graph with a different configuration for a new run. For example, you might need to use a different graph state or graph structure depending on the config. This guide shows how you can do this.

<Note>
  **Note**
  In most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it
</Note>

Make sure to check out [this how-to guide](/langsmith/setup-app-requirements-txt) on setting up your app for deployment first.

Let's say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:

where the graph is defined in `openai_agent.py`.

In the standard LangGraph API configuration, the server uses the compiled graph instance that's defined at the top level of `openai_agent.py`, which looks like the following:

To make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:

To make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:

```python  theme={null}
from typing import Annotated
from typing_extensions import TypedDict
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, MessageGraph
from langgraph.graph.state import StateGraph
from langgraph.graph.message import add_messages
from langchain.tools import tool
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage
from langchain_core.runnables import RunnableConfig

class State(TypedDict):
    messages: Annotated[list[BaseMessage], add_messages]

model = ChatOpenAI(temperature=0)

def make_default_graph():
    """Make a simple LLM agent"""
    graph_workflow = StateGraph(State)
    def call_model(state):
        return {"messages": [model.invoke(state["messages"])]}

graph_workflow.add_node("agent", call_model)
    graph_workflow.add_edge("agent", END)
    graph_workflow.add_edge(START, "agent")

agent = graph_workflow.compile()
    return agent

def make_alternative_graph():
    """Make a tool-calling agent"""

@tool
    def add(a: float, b: float):
        """Adds two numbers."""
        return a + b

tool_node = ToolNode([add])
    model_with_tools = model.bind_tools([add])
    def call_model(state):
        return {"messages": [model_with_tools.invoke(state["messages"])]}

def should_continue(state: State):
        if state["messages"][-1].tool_calls:
            return "tools"
        else:
            return END

graph_workflow = StateGraph(State)

graph_workflow.add_node("agent", call_model)
    graph_workflow.add_node("tools", tool_node)
    graph_workflow.add_edge("tools", "agent")
    graph_workflow.add_edge(START, "agent")
    graph_workflow.add_conditional_edges("agent", should_continue)

agent = graph_workflow.compile()
    return agent

**Examples:**

Example 1 (unknown):
```unknown
my-app/
|-- requirements.txt
|-- .env
|-- openai_agent.py     # code for your graph
```

Example 2 (unknown):
```unknown
To make the server aware of your graph, you need to specify a path to the variable that contains the [`CompiledStateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph) instance in your LangGraph API configuration (`langgraph.json`), e.g.:
```

Example 3 (unknown):
```unknown
### Rebuild

To make your graph rebuild on each new run with custom configuration, you need to rewrite `openai_agent.py` to instead provide a *function* that takes a config and returns a graph (or compiled graph) instance. Let's say we want to return our existing graph for user ID '1', and a tool-calling agent for other users. We can modify `openai_agent.py` as follows:
```

---

## Receivers

**URL:** llms-txt#receivers

**Contents:**
- Logs
- Metrics
  - Traces
- Processors
  - Recommended OTEL Processors
- Exporters

This is an example for a ***Sidecar*** collector to read logs from its own pod, excluding logs from non domain-specific containers. A Sidecar configuration is useful here because we require access to every container's filesystem. A DaemonSet can also be used.

<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods in the given namespace.**
</Info>

Metrics can be scraped using the Prometheus endpoints. A single instance ***Gateway*** collector can be be used to avoid duplication of queries when fetching metrics. The following config scrapes all of the default named LangSmith services:

<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods, services and endpoints in the given namespace.**
</Info>

For traces, you need to enable the OTLP receiver. The following configuration can be used to listen to HTTP traces on port 4318, and GRPC on port 4317:

### Recommended OTEL Processors

The following processors are recommended when using the OTel collector:

* [Batch Processor](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md): Groups the data into batches before sending to exporters.
* [Memory Limiter](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md): Prevents the collector from using too much memory and crashing. When the soft limit is crossed, the collector stops accepting new data.
* [Kubernetes Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor): Adds Kubernetes metadata such as pod name into the telemetry data.

Exporters just need to point to an external endpoint of your liking. The following configuration allows you to configure a separate endpoint for logs, metrics and traces:

<Note>
  **The OTel Collector also supports exporting directly to a [Datadog](https://docs.datadoghq.com/opentelemetry/setup/collector_exporter) endpoint.**
</Note>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods in the given namespace.**
</Info>

## Metrics

Metrics can be scraped using the Prometheus endpoints. A single instance ***Gateway*** collector can be be used to avoid duplication of queries when fetching metrics. The following config scrapes all of the default named LangSmith services:
```

Example 2 (unknown):
```unknown
<Info>
  **This configuration requires 'get', 'list', and 'watch' permissions on pods, services and endpoints in the given namespace.**
</Info>

### Traces

For traces, you need to enable the OTLP receiver. The following configuration can be used to listen to HTTP traces on port 4318, and GRPC on port 4317:
```

Example 3 (unknown):
```unknown
## Processors

### Recommended OTEL Processors

The following processors are recommended when using the OTel collector:

* [Batch Processor](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/batchprocessor/README.md): Groups the data into batches before sending to exporters.
* [Memory Limiter](https://github.com/open-telemetry/opentelemetry-collector/blob/main/processor/memorylimiterprocessor/README.md): Prevents the collector from using too much memory and crashing. When the soft limit is crossed, the collector stops accepting new data.
* [Kubernetes Attributes Processor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor): Adds Kubernetes metadata such as pod name into the telemetry data.

## Exporters

Exporters just need to point to an external endpoint of your liking. The following configuration allows you to configure a separate endpoint for logs, metrics and traces:
```

---

## Redefine the tool node to use the interrupt version

**URL:** llms-txt#redefine-the-tool-node-to-use-the-interrupt-version

**Contents:**
- Next steps

run_query_node = ToolNode([run_query_tool_with_interrupt], name="run_query") # [!code highlight]
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

def should_continue(state: MessagesState) -> Literal[END, "run_query"]:
    messages = state["messages"]
    last_message = messages[-1]
    if not last_message.tool_calls:
        return END
    else:
        return "run_query"

builder = StateGraph(MessagesState)
builder.add_node(list_tables)
builder.add_node(call_get_schema)
builder.add_node(get_schema_node, "get_schema")
builder.add_node(generate_query)
builder.add_node(run_query_node, "run_query")

builder.add_edge(START, "list_tables")
builder.add_edge("list_tables", "call_get_schema")
builder.add_edge("call_get_schema", "get_schema")
builder.add_edge("get_schema", "generate_query")
builder.add_conditional_edges(
    "generate_query",
    should_continue,
)
builder.add_edge("run_query", "generate_query")

checkpointer = InMemorySaver() # [!code highlight]
agent = builder.compile(checkpointer=checkpointer) # [!code highlight]
python  theme={null}
import json

config = {"configurable": {"thread_id": "1"}}

question = "Which genre on average has the longest tracks?"

for step in agent.stream(
    {"messages": [{"role": "user", "content": question}]},
    config,
    stream_mode="values",
):
    if "messages" in step:
        step["messages"][-1].pretty_print()
    elif "__interrupt__" in step:
        action = step["__interrupt__"][0]
        print("INTERRUPTED:")
        for request in action.value:
            print(json.dumps(request, indent=2))
    else:
        pass

INTERRUPTED:
{
  "action": "sql_db_query",
  "args": {
    "query": "SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;"
  },
  "description": "Please review the tool call"
}
python  theme={null}
from langgraph.types import Command

for step in agent.stream(
    Command(resume={"type": "accept"}),
    # Command(resume={"type": "edit", "args": {"query": "..."}}),
    config,
    stream_mode="values",
):
    if "messages" in step:
        step["messages"][-1].pretty_print()
    elif "__interrupt__" in step:
        action = step["__interrupt__"][0]
        print("INTERRUPTED:")
        for request in action.value:
            print(json.dumps(request, indent=2))
    else:
        pass

================================== Ai Message ==================================
Tool Calls:
  sql_db_query (call_t4yXkD6shwdTPuelXEmY3sAY)
 Call ID: call_t4yXkD6shwdTPuelXEmY3sAY
  Args:
    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;
================================= Tool Message =================================
Name: sql_db_query

[('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)]
================================== Ai Message ==================================

The genre with the longest average track length is "Sci Fi & Fantasy" with an average length of about 2,911,783 milliseconds. Other genres with long average track lengths include "Science Fiction," "Drama," "TV Shows," and "Comedy."
```

Refer to the [human-in-the-loop guide](/oss/python/langgraph/interrupts) for details.

Check out the [Evaluate a graph](/langsmith/evaluate-graph) guide for evaluating LangGraph applications, including SQL agents like this one, using LangSmith.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/sql-agent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  The above implementation follows the [tool interrupt example](/oss/python/langgraph/interrupts#configuring-interrupts) in the broader [human-in-the-loop](/oss/python/langgraph/interrupts) guide. Refer to that guide for details and alternatives.
</Note>

Let's now re-assemble our graph. We will replace the programmatic check with human review. Note that we now include a [checkpointer](/oss/python/langgraph/persistence); this is required to pause and resume the run.
```

Example 2 (unknown):
```unknown
We can invoke the graph as before. This time, execution is interrupted:
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
We can accept or edit the tool call using [Command](/oss/python/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command):
```

---

## Redeploy Revision

**URL:** llms-txt#redeploy-revision

Source: https://docs.langchain.com/api-reference/deployments-v2/redeploy-revision

https://api.host.langchain.com/openapi.json post /v2/deployments/{deployment_id}/revisions/{revision_id}/redeploy
Redeploy a specific revision ID.

---

## Regions FAQ

**URL:** llms-txt#regions-faq

**Contents:**
- Legal and compliance
- Features
- Plans and pricing

Source: https://docs.langchain.com/langsmith/regions-faq

<Note>
  See the [cloud architecture reference](/langsmith/cloud#architecture) for additional details.
</Note>

## Legal and compliance

#### *What privacy and data protection frameworks does LangSmith, including its EU instance, comply with?*

LangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at [trust.langchain.com](https://trust.langchain.com). If you would like to sign a Data Processing Addendum (DPA) with us, please contact support via [support.langchain.com](https://support.langchain.com). Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan.

#### *My company isn't based in the EU, can I still have my data hosted there?*

Yes, you can host your LangSmith data in the EU instance independent of your location.

#### *Do you have a legal entity in the EU that we can contract with?*

We do not have a legal entity in the EU for customer contracting today.

#### *Do different legal terms apply if I choose the EU region?*

The terms are the same for the EU and US regions.

#### *How do I use the EU instance?*

Follow the instructions [here](/langsmith/create-account-api-key) to create an account and an API key (make sure to change the region to EU in the dropdown)

#### *Are there any functional differences between US and EU cloud-managed LangSmith?*

There may be a small delay between launches to each region depending on the feature. Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa.

#### *Can an organization have workspaces in different regions?*

LangSmith does not support this at the moment, but if you are interested, please contact support via [support.langchain.com](https://support.langchain.com) and share your use case.

#### *Can I connect an EU organization to a US organization and share billing?*

LangSmith does not support this at the moment, but if you are interested, please contact support via [support.langchain.com](https://support.langchain.com) and share your use case.

#### *What data will be stored in my selected region?*

See the [cloud architecture reference](/langsmith/cloud#architecture) for details.

#### *How can I see my organization's region?*

Check your URL - any organizations on [https://eu.smith.langchain.com](https://eu.smith.langchain.com) are in the EU, and any on [https://smith.langchain.com](https://smith.langchain.com) are in the US.

#### *Can I switch my organization from the US to EU or vice versa?*

We do not support migration between regions at this time, but if you are interested in this feature, please contact support via [support.langchain.com](https://support.langchain.com).

#### *Is the EU region available on all LangSmith plans?*

Yes, you can sign up for the EU region on all plans including free plans.

#### *Is pricing different for the EU region compared to the US region?*

No, pricing is the same for the EU and US regions.

#### *What currency is used for payment if I use the EU region?*

All LangSmith plans are paid in USD.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/regions-faq.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Register with span processor

**URL:** llms-txt#register-with-span-processor

span_processor.register_turn_audio_recorder(conversation_id, turn_audio_recorder)

---

## Register with span processor for attachment to conversation span

**URL:** llms-txt#register-with-span-processor-for-attachment-to-conversation-span

span_processor.register_recording(
    conversation_id,
    str(recording_path),
    audio_recorder=audio_recorder
)

---

## Reject Concurrent

**URL:** llms-txt#reject-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/reject-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `reject` option for double texting, which rejects the new run of the graph by throwing an error and continues with the original run until completion. Below is a quick example of using the `reject` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now we can run a thread and try to run a second one with the "reject" option, which should fail since we have already started a run:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can verify that the original thread finished executing:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/reject-concurrent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Release policy

**URL:** llms-txt#release-policy

Source: https://docs.langchain.com/oss/python/release-policy

This page explains the LangChain and LangGraph release policies. Click on the tabs below to view the release policies for each:

<Tabs>
  <Tab title="LangChain">
    The LangChain ecosystem is composed of different component packages (e.g., `langchain-core`, `langchain`, `langchain-community`, partner packages, etc.)

With the release of LangChain 1.0, **minor** releases (e.g., from `1.0.x` to `1.1.0`) of `langchain` and `langchain-core` follow semantic versioning and may be released frequently. Minor releases contain new features and improvements but do not include breaking changes.

Patch versions are released frequently, up to a few times per week, as they contain bug fixes and minor improvements.

The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `langchain-core` will continue to evolve to better serve the needs of our users.

With LangChain 1.0's adoption of semantic versioning:

* Breaking changes to the public API will only occur in major version releases (e.g., `2.0.0`)
    * Minor version bumps (e.g., `1.0.0` to `1.1.0`) add new features without breaking changes
    * Patch version bumps (e.g., `1.0.0` to `1.0.1`) contain bug fixes and minor improvements

We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed.

### Stability of other packages

The stability of other packages in the LangChain ecosystem may vary:

* **Partner packages maintained by LangChain** (such as `langchain-openai` and `langchain-anthropic`) follow semantic versioning and are expected to be stable post 1.0. Other partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information.

* **`langchain-community`** is a community maintained package that contains 3rd party integrations. Due to the number of integrations there, `langchain-community` does not follow the same strict semantic versioning policy as `langchain` and `langchain-core`. See the "Special considerations" section under Long-term support for more details.

## Deprecation policy

We will generally avoid deprecating features until a better alternative is available.

With LangChain 1.0's semantic versioning approach, deprecated features will continue to work throughout the entire 1.x release series. Breaking changes, including the removal of deprecated features, will only occur in major version releases (e.g., 2.0).

When a feature is deprecated in `langchain` or `langchain-core`, we will:

* Clearly mark it as deprecated in the code and documentation
    * Provide migration guidance to the recommended alternative
    * Provide security updates for the deprecated feature through all 1.x minor releases

In some situations, we may allow deprecated features to remain in the code base even longer if they are not causing maintenance issues, to further reduce the burden on users.

## Long-term support (LTS)

LangChain follows a long-term support (LTS) policy to provide stability for production applications:

### Release status definitions

Packages are marked with one of the following statuses:

* **ACTIVE**: Current active development, includes bug fixes, security patches, and new features
    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features

### Current LTS releases

**LangChain 1.0** is designated as an LTS release:

* **Status**: ACTIVE until the release of 2.0
    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year
    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes

### Legacy version support

* **Status**: MAINTENANCE mode
    * **Support period**: Until December 2026
    * **Support includes**: Security patches and critical bug fixes

### Special considerations

**langchain-community 0.4**: Due to the nature of community contributions and third-party integrations, `langchain-community` may have breaking changes on minor releases. It has been released as version 0.4 to reflect this different stability policy.
  </Tab>

<Tab title="LangGraph">
    LangGraph follows a structured release policy to ensure stability and predictability for users building production applications.

We expect to space out **major** releases by at least 6-12 months to provide stability for production applications.

**Minor** releases are typically released every 1-2 months with new features and improvements.

**Patch** releases are released as needed, often weekly, to address bugs and security issues.

All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features within a major version.

Features marked as `beta` in the documentation are:

* Feature-complete and tested
    * Safe for production use with the understanding they may change
    * Subject to minor API adjustments based on user feedback

### Experimental features

Features marked as `experimental` or `alpha`:

* Are under active development
    * May change significantly or be removed
    * Should be used with caution in production

APIs prefixed with underscore (`_`) or explicitly marked as internal:

* Are not part of the public API
    * May change without notice
    * Should not be used directly

## Deprecation policy

When deprecating features:

1. **Deprecation Notice**: Features are marked as deprecated with clear migration guidance
    2. **Grace Period**: Deprecated features remain functional for at least one minor version
    3. **Removal**: Features are removed only in major version releases
    4. **Migration Support**: We provide migration guides and, when possible, automated tools

## Platform compatibility

* We support Python versions that are actively maintained by the Python Software Foundation
    * Python version requirements may change only in major releases
    * Currently requires Python 3.10 or later

Breaking changes are only introduced in major versions and include:

* Removal of deprecated APIs
    * Changes to required parameters
    * Changes to default behavior that affect existing applications
    * Minimum Python/Node.js version updates

For major version upgrades, we provide:

* Comprehensive migration guides
    * Automated migration scripts when feasible
    * Extended support period for the previous major version
    * Clear documentation of all breaking changes

## Long-term support (LTS)

LangGraph follows a long-term support (LTS) policy to provide stability for production applications:

### Release status definitions

Packages are marked with one of the following statuses:

* **ACTIVE**: Current active development, includes bug fixes, security patches, and new features
    * **MAINTENANCE**: Receives all security patches and critical bug fixes, but no new features

### Current LTS releases

**LangGraph 1.0** is designated as an LTS release:

* **Status**: ACTIVE until the release of 2.0
    * **Support period**: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year
    * **Semver compliance**: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes

### Legacy version support

* **Status**: MAINTENANCE mode
    * **Support period**: Until December 2026
    * **Support includes**: All security patches and critical bug fixes

* [Versioning](/oss/python/versioning) - Version numbering and support details
    * [Releases](/oss/python/releases) - Version-specific release notes and migration guides
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/release-policy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Release versions

**URL:** llms-txt#release-versions

**Contents:**
- Support levels
  - Active
  - Critical
  - End of life (EOL)
  - Deprecated
- Version support policy
  - Minor version support
  - Patch releases
- Recommendations
- Version compatibility

Source: https://docs.langchain.com/langsmith/release-versions

export const product_0 = "LangSmith"

{product_0} provides different support levels for different versions, which may include new features, bug fixes, or security patches.

There are four support levels:

* Active
* Critical
* End of life (EOL)
* Deprecated

Where N represents the latest minor version (e.g., 0.3, 0.4, etc.).

The current minor version (N) receives full support, including:

* New features and capabilities
* Bug fixes and regressions
* Security patches
* Quality-of-life improvements
* High confidence changes that are narrowly scoped

The previous minor version (N-1) receives limited support:

* Critical security fixes
* Installation fixes
* No new features or general bug fixes
* Transitioned from Active when a newer minor version is released

### End of life (EOL)

Versions older than N-2 (N-2, N-3, etc.) receive no support:

* No new patch releases
* No bug fixes, including known bugs
* No security updates
* Users should upgrade to a supported version

Versions that are no longer maintained:

* All versions prior to the first stable release
* Versions that have been explicitly deprecated
* No support or maintenance provided

## Version support policy

{product_0} follows an N-2 support policy for minor versions:

* **N (Current)**: Active support
* **N-1**: Critical support
* **N-2 and older**: End of Life

### Minor version support

Minor versions include new features and capabilities and are supported according to the N-2 policy. When we refer to a minor version, such as v0.3, we always mean its latest available patch release (v0.3.x).

During the support window for each version:

* **Active Support**: Regular patch releases with bug fixes, regressions, and new features
* **Critical Support**: Security-only releases for critical fixes related to security and installation
* **End of Life**: No new patches released

* **Stay Current**: We recommend upgrading to the latest minor version to receive full support and access to new features
* **Plan Upgrades**: Monitor the changelog for upcoming version changes and plan upgrades accordingly
* **Security**: Critical security fixes are only provided for Active and Critical support versions
* **Testing**: Test your applications with newer versions before upgrading in production

## Version compatibility

When upgrading between minor versions:

* Review the changelog for breaking changes
* Test your applications thoroughly
* Follow the upgrade guides provided in the documentation
* Consider the support timeline for your current version

## Current version support

To check the current supported versions and their support levels, refer to the [Agent Server Changelog](/langsmith/agent-server-changelog) for the latest release information.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/release-versions.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## RemainingSteps works with any recursion_limit

**URL:** llms-txt#remainingsteps-works-with-any-recursion_limit

result = graph.invoke({"messages": []}, {"recursion_limit": 10})
python  theme={null}
from typing import Annotated, Literal, TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.managed import RemainingSteps
from langgraph.errors import GraphRecursionError

class State(TypedDict):
    messages: Annotated[list, lambda x, y: x + y]
    remaining_steps: RemainingSteps

**Examples:**

Example 1 (unknown):
```unknown
#### Proactive vs reactive approaches

There are two main approaches to handling recursion limits: proactive (monitoring within the graph) and reactive (catching errors externally).
```

---

## Remember that langgraph graphs are also langchain runnables.

**URL:** llms-txt#remember-that-langgraph-graphs-are-also-langchain-runnables.

**Contents:**
- Evaluating intermediate steps
- Running and evaluating individual nodes
- Related
- Reference code

target = example_to_state | app

experiment_results = await aevaluate(
    target,
    data="weather agent",
    evaluators=[correct],
    max_concurrency=4,  # optional
    experiment_prefix="claude-3.5-baseline",  # optional
)
python  theme={null}
def right_tool(outputs: dict) -> bool:
    tool_calls = outputs["messages"][1].tool_calls
    return bool(tool_calls and tool_calls[0]["name"] == "search")

experiment_results = await aevaluate(
    target,
    data="weather agent",
    evaluators=[correct, right_tool],
    max_concurrency=4,  # optional
    experiment_prefix="claude-3.5-baseline",  # optional
)
python  theme={null}
from langsmith.schemas import Run, Example

def right_tool_from_run(run: Run, example: Example) -> dict:
    # Get documents and answer
    first_model_run = next(run for run in root_run.child_runs if run.name == "agent")
    tool_calls = first_model_run.outputs["messages"][-1].tool_calls
    right_tool = bool(tool_calls and tool_calls[0]["name"] == "search")
    return {"key": "right_tool", "value": right_tool}

experiment_results = await aevaluate(
    target,
    data="weather agent",
    evaluators=[correct, right_tool_from_run],
    max_concurrency=4,  # optional
    experiment_prefix="claude-3.5-baseline",  # optional
)
python  theme={null}
node_target = example_to_state | app.nodes["agent"]

node_experiment_results = await aevaluate(
    node_target,
    data="weather agent",
    evaluators=[right_tool_from_run],
    max_concurrency=4,  # optional
    experiment_prefix="claude-3.5-model-node",  # optional
)
python  theme={null}
  from typing import Annotated, Literal, TypedDict
  from langchain.chat_models import init_chat_model
  from langchain.tools import tool
  from langgraph.prebuilt import ToolNode
  from langgraph.graph import END, START, StateGraph
  from langgraph.graph.message import add_messages
  from langsmith import Client, aevaluate

# Define a graph
  class State(TypedDict):
      # Messages have the type "list". The 'add_messages' function
      # in the annotation defines how this state key should be updated
      # (in this case, it appends messages to the list, rather than overwriting them)
      messages: Annotated[list, add_messages]

# Define the tools for the agent to use
  @tool
  def search(query: str) -> str:
      """Call to surf the web."""
      # This is a placeholder, but don't tell the LLM that...
      if "sf" in query.lower() or "san francisco" in query.lower():
          return "It's 60 degrees and foggy."
      return "It's 90 degrees and sunny."

tools = [search]
  tool_node = ToolNode(tools)
  model = init_chat_model("claude-sonnet-4-5-20250929").bind_tools(tools)

# Define the function that determines whether to continue or not
  def should_continue(state: State) -> Literal["tools", END]:
      messages = state['messages']
      last_message = messages[-1]

# If the LLM makes a tool call, then we route to the "tools" node
      if last_message.tool_calls:
          return "tools"

# Otherwise, we stop (reply to the user)
      return END

# Define the function that calls the model
  def call_model(state: State):
      messages = state['messages']
      response = model.invoke(messages)
      # We return a list, because this will get added to the existing list
      return {"messages": [response]}

# Define a new graph
  workflow = StateGraph(State)

# Define the two nodes we will cycle between
  workflow.add_node("agent", call_model)
  workflow.add_node("tools", tool_node)

# Set the entrypoint as 'agent'
  # This means that this node is the first one called
  workflow.add_edge(START, "agent")

# We now add a conditional edge
  workflow.add_conditional_edges(
      # First, we define the start node. We use 'agent'.
      # This means these are the edges taken after the 'agent' node is called.
      "agent",
      # Next, we pass in the function that will determine which node is called next.
      should_continue,
  )

# We now add a normal edge from 'tools' to 'agent'.
  # This means that after 'tools' is called, 'agent' node is called next.
  workflow.add_edge("tools", 'agent')

# Finally, we compile it!
  # This compiles it into a LangChain Runnable,
  # meaning you can use it as you would any other runnable.
  # Note that we're (optionally) passing the memory when compiling the graph
  app = workflow.compile()

questions = [
      "what's the weather in sf",
      "whats the weather in san fran",
      "whats the weather in tangier"
  ]

answers = [
      "It's 60 degrees and foggy.",
      "It's 60 degrees and foggy.",
      "It's 90 degrees and sunny.",
  ]

# Create a dataset
  ls_client = Client()
  dataset = ls_client.create_dataset(
      "weather agent",
      inputs=[{"question": q} for q in questions],
      outputs=[{"answers": a} for a in answers],
  )

# Define evaluators
  async def correct(outputs: dict, reference_outputs: dict) -> bool:
      instructions = (
          "Given an actual answer and an expected answer, determine whether"
          " the actual answer contains all of the information in the"
          " expected answer. Respond with 'CORRECT' if the actual answer"
          " does contain all of the expected information and 'INCORRECT'"
          " otherwise. Do not include anything else in your response."
      )
      # Our graph outputs a State dictionary, which in this case means
      # we'll have a 'messages' key and the final message should
      # be our actual answer.
      actual_answer = outputs["messages"][-1].content
      expected_answer = reference_outputs["answer"]
      user_msg = (
          f"ACTUAL ANSWER: {actual_answer}"
          f"\n\nEXPECTED ANSWER: {expected_answer}"
      )
      response = await judge_llm.ainvoke(
          [
              {"role": "system", "content": instructions},
              {"role": "user", "content": user_msg}
          ]
      )
      return response.content.upper() == "CORRECT"

def right_tool(outputs: dict) -> bool:
      tool_calls = outputs["messages"][1].tool_calls
      return bool(tool_calls and tool_calls[0]["name"] == "search")

# Run evaluation
  experiment_results = await aevaluate(
      target,
      data="weather agent",
      evaluators=[correct, right_tool],
      max_concurrency=4,  # optional
      experiment_prefix="claude-3.5-baseline",  # optional
  )
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/evaluate-graph.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Evaluating intermediate steps

Often it is valuable to evaluate not only the final output of an agent but also the intermediate steps it has taken. What's nice about `langgraph` is that the output of a graph is a state object that often already carries information about the intermediate steps taken. Usually we can evaluate whatever we're interested in just by looking at the messages in our state. For example, we can look at the messages to assert that the model invoked the 'search' tool upon as a first step.

Requires `langsmith>=0.2.0`
```

Example 2 (unknown):
```unknown
If we need access to information about intermediate steps that isn't in state, we can look at the Run object. This contains the full traces for all node inputs and outputs:

<Check>
  See more about what arguments you can pass to custom evaluators in this [how-to guide](/langsmith/code-evaluator).
</Check>
```

Example 3 (unknown):
```unknown
## Running and evaluating individual nodes

Sometimes you want to evaluate a single node directly to save time and costs. `langgraph` makes it easy to do this. In this case we can even continue using the evaluators we've been using.
```

Example 4 (unknown):
```unknown
## Related

* [`langgraph` evaluation docs](https://langchain-ai.github.io/langgraph/tutorials/#evaluation)

## Reference code

<Accordion title="Click to see a consolidated code snippet">
```

---

## RemoteGraph

**URL:** llms-txt#remotegraph

Source: https://docs.langchain.com/langsmith/remote-graph

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/remote-graph.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## required environment variables

**URL:** llms-txt#required-environment-variables

CONTROL_PLANE_HOST = os.getenv("CONTROL_PLANE_HOST")
LANGSMITH_API_KEY = os.getenv("LANGSMITH_API_KEY")
WORKSPACE_ID = os.getenv("WORKSPACE_ID")
INTEGRATION_ID = os.getenv("INTEGRATION_ID")
MAX_WAIT_TIME = 1800  # 30 mins

def get_headers() -> dict:
    """Return common headers for requests to the control plane API."""
    return {
        "X-Api-Key": LANGSMITH_API_KEY,
        "X-Tenant-Id": WORKSPACE_ID,
    }

def create_deployment() -> str:
    """Create deployment. Return deployment ID."""
    headers = get_headers()
    headers["Content-Type"] = "application/json"

deployment_name = "my_deployment"

request_body = {
        "name": deployment_name,
        "source": "github",
        "source_config": {
            "integration_id": INTEGRATION_ID,
            "repo_url": "https://github.com/langchain-ai/langgraph-example",
            "deployment_type": "dev",
            "build_on_push": False,
            "custom_url": None,
            "resource_spec": None,
        },
        "source_revision_config": {
            "repo_ref": "main",
            "langgraph_config_path": "langgraph.json",
            "image_uri": None,
        },
        "secrets": [
            {
                "name": "OPENAI_API_KEY",
                "value": "test_openai_api_key",
            },
            {
                "name": "ANTHROPIC_API_KEY",
                "value": "test_anthropic_api_key",
            },
            {
                "name": "TAVILY_API_KEY",
                "value": "test_tavily_api_key",
            },
        ],
    }

response = requests.post(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments",
        headers=headers,
        json=request_body,
    )

if response.status_code != 201:
        raise Exception(f"Failed to create deployment: {response.text}")

deployment_id = response.json()["id"]
    print(f"Created deployment {deployment_name} ({deployment_id})")
    return deployment_id

def get_deployment(deployment_id: str) -> dict:
    """Get deployment."""
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(f"Failed to get deployment ID {deployment_id}: {response.text}")

return response.json()

def list_revisions(deployment_id: str) -> list[dict]:
    """List revisions.

Return list is sorted by created_at in descending order (latest first).
    """
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}/revisions",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(
            f"Failed to list revisions for deployment ID {deployment_id}: {response.text}"
        )

return response.json()

def get_revision(
    deployment_id: str,
    revision_id: str,
) -> dict:
    """Get revision."""
    response = requests.get(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}/revisions/{revision_id}",
        headers=get_headers(),
    )

if response.status_code != 200:
        raise Exception(f"Failed to get revision ID {revision_id}: {response.text}")

return response.json()

def patch_deployment(deployment_id: str) -> None:
    """Patch deployment."""
    headers = get_headers()
    headers["Content-Type"] = "application/json"

# This creates a new revision because source_revision_config is included
    response = requests.patch(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=headers,
        json={
            "source_config": {
                "build_on_push": True,
            },
            "source_revision_config": {
                "repo_ref": "main",
                "langgraph_config_path": "langgraph.json",
            },
        },
    )

if response.status_code != 200:
        raise Exception(f"Failed to patch deployment: {response.text}")

print(f"Patched deployment ID {deployment_id}")

def wait_for_deployment(deployment_id: str, revision_id: str) -> None:
    """Wait for revision status to be DEPLOYED."""
    start_time = time.time()
    revision, status = None, None
    while time.time() - start_time < MAX_WAIT_TIME:
        revision = get_revision(deployment_id, revision_id)
        status = revision["status"]
        if status == "DEPLOYED":
            break
        elif "FAILED" in status:
            raise Exception(f"Revision ID {revision_id} failed: {revision}")

print(f"Waiting for revision ID {revision_id} to be DEPLOYED...")
        time.sleep(60)

if status != "DEPLOYED":
        raise Exception(
            f"Timeout waiting for revision ID {revision_id} to be DEPLOYED: {revision}"
        )

def delete_deployment(deployment_id: str) -> None:
    """Delete deployment."""
    response = requests.delete(
        url=f"{CONTROL_PLANE_HOST}/v2/deployments/{deployment_id}",
        headers=get_headers(),
    )

if response.status_code != 204:
        raise Exception(
            f"Failed to delete deployment ID {deployment_id}: {response.text}"
        )

print(f"Deployment ID {deployment_id} deleted")

if __name__ == "__main__":
    # create deployment and get the latest revision
    deployment_id = create_deployment()
    revisions = list_revisions(deployment_id)
    latest_revision = revisions["resources"][0]
    latest_revision_id = latest_revision["id"]

# wait for latest revision to be DEPLOYED
    wait_for_deployment(deployment_id, latest_revision_id)

# patch the deployment and get the latest revision
    patch_deployment(deployment_id)
    revisions = list_revisions(deployment_id)
    latest_revision = revisions["resources"][0]
    latest_revision_id = latest_revision["id"]

# wait for latest revision to be DEPLOYED
    wait_for_deployment(deployment_id, latest_revision_id)

# delete the deployment
    delete_deployment(deployment_id)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/api-ref-control-plane.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## results in `Weather(temperature=70.0, condition='sunny')`

**URL:** llms-txt#results-in-`weather(temperature=70.0,-condition='sunny')`

**Contents:**
- Standard content blocks

python  theme={null}
from langchain_anthropic import ChatAnthropic

model = ChatAnthropic(model="claude-sonnet-4-5-20250929")
response = model.invoke("What's the capital of France?")

**Examples:**

Example 1 (unknown):
```unknown
**Error handling**: Control error handling via the `handle_errors` parameter to `ToolStrategy`:

* **Parsing errors**: Model generates data that doesn't match desired structure
* **Multiple tool calls**: Model generates 2+ tool calls for structured output schemas

***

## Standard content blocks

<Note>
  Content block support is currently only available for the following integrations:

  * [`langchain-anthropic`](https://pypi.org/project/langchain-anthropic/)
  * [`langchain-aws`](https://pypi.org/project/langchain-aws/)
  * [`langchain-openai`](https://pypi.org/project/langchain-openai/)
  * [`langchain-google-genai`](https://pypi.org/project/langchain-google-genai/)
  * [`langchain-ollama`](https://pypi.org/project/langchain-ollama/)

  Broader support for content blocks will be rolled out gradually across more providers.
</Note>

The new [`content_blocks`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.messages.BaseMessage.content_blocks) property introduces a standard representation for message content that works across providers:
```

---

## Resume (use same config)

**URL:** llms-txt#resume-(use-same-config)

**Contents:**
  - Match decision order to actions
  - Tailor configurations by risk

result = agent.invoke(Command(resume={...}), config=config)
python  theme={null}
if result.get("__interrupt__"):
    interrupts = result["__interrupt__"][0].value
    action_requests = interrupts["action_requests"]

# Create one decision per action, in order
    decisions = []
    for action in action_requests:
        decision = get_user_decision(action)  # Your logic
        decisions.append(decision)

result = agent.invoke(
        Command(resume={"decisions": decisions}),
        config=config
    )
python  theme={null}
interrupt_on = {
    # High risk: full control (approve, edit, reject)
    "delete_file": {"allowed_decisions": ["approve", "edit", "reject"]},
    "send_email": {"allowed_decisions": ["approve", "edit", "reject"]},

# Medium risk: no editing allowed
    "write_file": {"allowed_decisions": ["approve", "reject"]},

# Low risk: no interrupts
    "read_file": False,
    "list_files": False,
}
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/human-in-the-loop.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Match decision order to actions

The decisions list must match the order of `action_requests`:
```

Example 2 (unknown):
```unknown
### Tailor configurations by risk

Configure different tools based on their risk level:
```

---

## Resume with approval decision

**URL:** llms-txt#resume-with-approval-decision

**Contents:**
  - Decision types
- Streaming with human-in-the-loop

agent.invoke(
    Command( # [!code highlight]
        resume={"decisions": [{"type": "approve"}]}  # or "reject" [!code highlight]
    ), # [!code highlight]
    config=config # Same thread ID to resume the paused conversation
)
python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "approve",
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "edit",
                        # Edited action with tool name and args
                        "edited_action": {
                            # Tool name to call.
                            # Will usually be the same as the original action.
                            "name": "new_tool_name",
                            # Arguments to pass to the tool.
                            "args": {"key1": "new_value", "key2": "original_value"},
                        }
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    agent.invoke(
        Command(
            # Decisions are provided as a list, one per action under review.
            # The order of decisions must match the order of actions
            # listed in the `__interrupt__` request.
            resume={
                "decisions": [
                    {
                        "type": "reject",
                        # An explanation about why the action was rejected
                        "message": "No, this is wrong because ..., instead do this ...",
                    }
                ]
            }
        ),
        config=config  # Same thread ID to resume the paused conversation
    )
    python  theme={null}
    {
        "decisions": [
            {"type": "approve"},
            {
                "type": "edit",
                "edited_action": {
                    "name": "tool_name",
                    "args": {"param": "new_value"}
                }
            },
            {
                "type": "reject",
                "message": "This action is not allowed"
            }
        ]
    }
    python  theme={null}
from langgraph.types import Command

config = {"configurable": {"thread_id": "some_id"}}

**Examples:**

Example 1 (unknown):
```unknown
### Decision types

<Tabs>
  <Tab title="✅ approve">
    Use `approve` to approve the tool call as-is and execute it without changes.
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="✏️ edit">
    Use `edit` to modify the tool call before execution.
    Provide the edited action with the new tool name and arguments.
```

Example 3 (unknown):
```unknown
<Tip>
      When **editing** tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.
    </Tip>
  </Tab>

  <Tab title="❌ reject">
    Use `reject` to reject the tool call and provide feedback instead of execution.
```

Example 4 (unknown):
```unknown
The `message` is added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead.

    ***

    ### Multiple decisions

    When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt:
```

---

## Resume with streaming after human decision

**URL:** llms-txt#resume-with-streaming-after-human-decision

**Contents:**
- Execution lifecycle
- Custom HITL logic

for mode, chunk in agent.stream(
    Command(resume={"decisions": [{"type": "approve"}]}),
    config=config,
    stream_mode=["updates", "messages"],
):
    if mode == "messages":
        token, metadata = chunk
        if token.content:
            print(token.content, end="", flush=True)
```

See the [Streaming](/oss/python/langchain/streaming) guide for more details on stream modes.

## Execution lifecycle

The middleware defines an `after_model` hook that runs after the model generates a response but before any tool calls are executed:

1. The agent invokes the model to generate a response.
2. The middleware inspects the response for tool calls.
3. If any calls require human input, the middleware builds a `HITLRequest` with `action_requests` and `review_configs` and calls [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt).
4. The agent waits for human decisions.
5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes [ToolMessage](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage)'s for rejected calls, and resumes execution.

For more specialized workflows, you can build custom HITL logic directly using the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) primitive and [middleware](/oss/python/langchain/middleware) abstraction.

Review the [execution lifecycle](#execution-lifecycle) above to understand how to integrate interrupts into the agent's operation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/human-in-the-loop.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Resume with the human's response

**URL:** llms-txt#resume-with-the-human's-response

---

## Retrieval

**URL:** llms-txt#retrieval

**Contents:**
- Building a knowledge base
  - From retrieval to RAG
  - Retrieval Pipeline
  - Building Blocks
- RAG Architectures
  - 2-step RAG
  - Agentic RAG
  - Hybrid RAG

Source: https://docs.langchain.com/oss/python/langchain/retrieval

Large Language Models (LLMs) are powerful, but they have two key limitations:

* **Finite context** — they can’t ingest entire corpora at once.
* **Static knowledge** — their training data is frozen at a point in time.

Retrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of **Retrieval-Augmented Generation (RAG)**: enhancing an LLM’s answers with context-specific information.

## Building a knowledge base

A **knowledge base** is a repository of documents or structured data used during retrieval.

If you need a custom knowledge base, you can use LangChain’s document loaders and vector stores to build one from your own data.

<Note>
  If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do **not** need to rebuild it. You can:

* Connect it as a **tool** for an agent in Agentic RAG.
  * Query it and supply the retrieved content as context to the LLM [(2-Step RAG)](#2-step-rag).
</Note>

See the following tutorial to build a searchable knowledge base and minimal RAG workflow:

<Card title="Tutorial: Semantic search" icon="database" href="/oss/python/langchain/knowledge-base" arrow cta="Learn more">
  Learn how to create a searchable knowledge base from your own data using LangChain’s document loaders, embeddings, and vector stores.
  In this tutorial, you’ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You’ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.
</Card>

### From retrieval to RAG

Retrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they **integrate retrieval with generation** to produce grounded, context-aware answers.

This is the core idea behind **Retrieval-Augmented Generation (RAG)**. The retrieval pipeline becomes a foundation for a broader system that combines search with generation.

### Retrieval Pipeline

A typical retrieval workflow looks like this:

Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.

<Columns cols={2}>
  <Card title="Document loaders" icon="file-import" href="/oss/python/integrations/document_loaders" arrow cta="Learn more">
    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.
  </Card>

<Card title="Text splitters" icon="scissors" href="/oss/python/integrations/splitters" arrow cta="Learn more">
    Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.
  </Card>

<Card title="Embedding models" icon="diagram-project" href="/oss/python/integrations/text_embedding" arrow cta="Learn more">
    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.
  </Card>

<Card title="Vector stores" icon="database" href="/oss/python/integrations/vectorstores/" arrow cta="Learn more">
    Specialized databases for storing and searching embeddings.
  </Card>

<Card title="Retrievers" icon="binoculars" href="/oss/python/integrations/retrievers/" arrow cta="Learn more">
    A retriever is an interface that returns documents given an unstructured query.
  </Card>
</Columns>

RAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.

| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |
| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |
| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |
| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |
| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\&A with quality validation      |

<Info>
  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.
</Info>

In **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.

<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/python/langchain/rag#rag-chains" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

* A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.

<Tip>
  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.
</Tip>

<Expandable title="Extended example: Agentic RAG for LangGraph's llms.txt">
  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.

<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/python/langchain/rag" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

* A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

Hybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution.

Typical components include:

* **Query enhancement**: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.
* **Retrieval validation**: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.
* **Answer validation**: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.

The architecture often supports multiple iterations between these steps:

This architecture is suitable for:

* Applications with ambiguous or underspecified queries
* Systems that require validation or quality control steps
* Workflows involving multiple sources or iterative refinement

<Card title="Tutorial: Agentic RAG with Self-Correction" icon="robot" href="/oss/python/langgraph/agentic-rag" arrow cta="Learn more">
  An example of **Hybrid RAG** that combines agentic reasoning with retrieval and self-correction.
</Card>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/retrieval.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app’s logic.

### Building Blocks

<Columns cols={2}>
  <Card title="Document loaders" icon="file-import" href="/oss/python/integrations/document_loaders" arrow cta="Learn more">
    Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects.
  </Card>

  <Card title="Text splitters" icon="scissors" href="/oss/python/integrations/splitters" arrow cta="Learn more">
    Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.
  </Card>

  <Card title="Embedding models" icon="diagram-project" href="/oss/python/integrations/text_embedding" arrow cta="Learn more">
    An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.
  </Card>

  <Card title="Vector stores" icon="database" href="/oss/python/integrations/vectorstores/" arrow cta="Learn more">
    Specialized databases for storing and searching embeddings.
  </Card>

  <Card title="Retrievers" icon="binoculars" href="/oss/python/integrations/retrievers/" arrow cta="Learn more">
    A retriever is an interface that returns documents given an unstructured query.
  </Card>
</Columns>

## RAG Architectures

RAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below.

| Architecture    | Description                                                                | Control   | Flexibility | Latency    | Example Use Case                                  |
| --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- |
| **2-Step RAG**  | Retrieval always happens before generation. Simple and predictable         | ✅ High    | ❌ Low       | ⚡ Fast     | FAQs, documentation bots                          |
| **Agentic RAG** | An LLM-powered agent decides *when* and *how* to retrieve during reasoning | ❌ Low     | ✅ High      | ⏳ Variable | Research assistants with access to multiple tools |
| **Hybrid**      | Combines characteristics of both approaches with validation steps          | ⚖️ Medium | ⚖️ Medium   | ⏳ Variable | Domain-specific Q\&A with quality validation      |

<Info>
  **Latency**: Latency is generally more **predictable** in **2-Step RAG**, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps—such as API response times, network delays, or database queries—which can vary based on the tools and infrastructure in use.
</Info>

### 2-step RAG

In **2-Step RAG**, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.
```

Example 2 (unknown):
```unknown
<Card title="Tutorial: Retrieval-Augmented Generation (RAG)" icon="robot" href="/oss/python/langchain/rag#rag-chains" arrow cta="Learn more">
  See how to build a Q\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.
  This tutorial walks through two approaches:

  * A **RAG agent** that runs searches with a flexible tool—great for general-purpose use.
  * A **2-step RAG** chain that requires just one LLM call per query—fast and efficient for simpler tasks.
</Card>

### Agentic RAG

**Agentic Retrieval-Augmented Generation (RAG)** combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides **when** and **how** to retrieve information during the interaction.

<Tip>
  The only thing an agent needs to enable RAG behavior is access to one or more **tools** that can fetch external knowledge — such as documentation loaders, web APIs, or database queries.
</Tip>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
<Expandable title="Extended example: Agentic RAG for LangGraph's llms.txt">
  This example implements an **Agentic RAG system** to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch_documentation` tool to retrieve and process the relevant content based on the user’s question.
```

---

## Retrievers

**URL:** llms-txt#retrievers

**Contents:**
- Bring-your-own documents
- External index
- All retrievers

Source: https://docs.langchain.com/oss/python/integrations/retrievers/index

A [retriever](/oss/python/langchain/retrieval#building-blocks) is an interface that returns documents given an unstructured query.
It is more general than a vector store.
A retriever does not need to be able to store documents, only to return (or retrieve) them.
Retrievers can be created from vector stores, but are also broad enough to include [Wikipedia search](/oss/python/integrations/retrievers/wikipedia/) and [Amazon Kendra](/oss/python/integrations/retrievers/amazon_kendra_retriever/).

Retrievers accept a string query as input and return a list of [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects as output.

Note that all [vector stores](/oss/python/integrations/vectorstores) can be cast to retrievers. Refer to the vector store [integration docs](/oss/python/integrations/vectorstores/) for available vector stores.
This page lists custom retrievers, implemented via subclassing BaseRetriever.

## Bring-your-own documents

The below retrievers allow you to index and search a custom corpus of documents.

| Retriever                                                                                | Self-host | Cloud offering | Package                                                                                                                                                                               |
| ---------------------------------------------------------------------------------------- | --------- | -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`AmazonKnowledgeBasesRetriever`](/oss/python/integrations/retrievers/bedrock)           | ❌         | ✅              | [`langchain-aws`](https://python.langchain.com/api_reference/aws/retrievers/langchain_aws.retrievers.bedrock.AmazonKnowledgeBasesRetriever.html)                                      |
| [`AzureAISearchRetriever`](/oss/python/integrations/retrievers/azure_ai_search)          | ❌         | ✅              | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.azure_ai_search.AzureAISearchRetriever.html)                   |
| [`ElasticsearchRetriever`](/oss/python/integrations/retrievers/elasticsearch_retriever)  | ✅         | ✅              | [`langchain-elasticsearch`](https://python.langchain.com/api_reference/elasticsearch/retrievers/langchain_elasticsearch.retrievers.ElasticsearchRetriever.html)                       |
| [`VertexAISearchRetriever`](/oss/python/integrations/retrievers/google_vertex_ai_search) | ❌         | ✅              | [`langchain-google-community`](https://python.langchain.com/api_reference/google_community/vertex_ai_search/langchain_google_community.vertex_ai_search.VertexAISearchRetriever.html) |

The below retrievers will search over an external index (e.g., constructed from Internet data or similar).

| Retriever                                                                | Source                                                | Package                                                                                                                                                                 |
| ------------------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`ArxivRetriever`](/oss/python/integrations/retrievers/arxiv)            | Scholarly articles on [arxiv.org](https://arxiv.org/) | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.arxiv.ArxivRetriever.html)                       |
| [`TavilySearchAPIRetriever`](/oss/python/integrations/retrievers/tavily) | Internet search                                       | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.tavily_search_api.TavilySearchAPIRetriever.html) |
| [`WikipediaRetriever`](/oss/python/integrations/retrievers/wikipedia)    | [Wikipedia](https://www.wikipedia.org/) articles      | [`langchain-community`](https://python.langchain.com/api_reference/community/retrievers/langchain_community.retrievers.wikipedia.WikipediaRetriever.html)               |

<Columns cols={3}>
  <Card title="Activeloop Deep Memory" icon="link" href="/oss/python/integrations/retrievers/activeloop" arrow="true" cta="View guide" />

<Card title="Amazon Kendra" icon="link" href="/oss/python/integrations/retrievers/amazon_kendra_retriever" arrow="true" cta="View guide" />

<Card title="Arcee" icon="link" href="/oss/python/integrations/retrievers/arcee" arrow="true" cta="View guide" />

<Card title="Arxiv" icon="link" href="/oss/python/integrations/retrievers/arxiv" arrow="true" cta="View guide" />

<Card title="AskNews" icon="link" href="/oss/python/integrations/retrievers/asknews" arrow="true" cta="View guide" />

<Card title="Azure AI Search" icon="link" href="/oss/python/integrations/retrievers/azure_ai_search" arrow="true" cta="View guide" />

<Card title="Bedrock (Knowledge Bases)" icon="link" href="/oss/python/integrations/retrievers/bedrock" arrow="true" cta="View guide" />

<Card title="BM25" icon="link" href="/oss/python/integrations/retrievers/bm25" arrow="true" cta="View guide" />

<Card title="Box" icon="link" href="/oss/python/integrations/retrievers/box" arrow="true" cta="View guide" />

<Card title="BREEBS (Open Knowledge)" icon="link" href="/oss/python/integrations/retrievers/breebs" arrow="true" cta="View guide" />

<Card title="Chaindesk" icon="link" href="/oss/python/integrations/retrievers/chaindesk" arrow="true" cta="View guide" />

<Card title="ChatGPT plugin" icon="link" href="/oss/python/integrations/retrievers/chatgpt-plugin" arrow="true" cta="View guide" />

<Card title="Cognee" icon="link" href="/oss/python/integrations/retrievers/cognee" arrow="true" cta="View guide" />

<Card title="Cohere reranker" icon="link" href="/oss/python/integrations/retrievers/cohere-reranker" arrow="true" cta="View guide" />

<Card title="Cohere RAG" icon="link" href="/oss/python/integrations/retrievers/cohere" arrow="true" cta="View guide" />

<Card title="Contextual AI Reranker" icon="link" href="/oss/python/integrations/retrievers/contextual" arrow="true" cta="View guide" />

<Card title="Dappier" icon="link" href="/oss/python/integrations/retrievers/dappier" arrow="true" cta="View guide" />

<Card title="DocArray" icon="link" href="/oss/python/integrations/retrievers/docarray_retriever" arrow="true" cta="View guide" />

<Card title="Dria" icon="link" href="/oss/python/integrations/retrievers/dria_index" arrow="true" cta="View guide" />

<Card title="ElasticSearch BM25" icon="link" href="/oss/python/integrations/retrievers/elastic_search_bm25" arrow="true" cta="View guide" />

<Card title="Elasticsearch" icon="link" href="/oss/python/integrations/retrievers/elasticsearch_retriever" arrow="true" cta="View guide" />

<Card title="Embedchain" icon="link" href="/oss/python/integrations/retrievers/embedchain" arrow="true" cta="View guide" />

<Card title="FlashRank reranker" icon="link" href="/oss/python/integrations/retrievers/flashrank-reranker" arrow="true" cta="View guide" />

<Card title="Fleet AI Context" icon="link" href="/oss/python/integrations/retrievers/fleet_context" arrow="true" cta="View guide" />

<Card title="Galaxia" icon="link" href="/oss/python/integrations/retrievers/galaxia-retriever" arrow="true" cta="View guide" />

<Card title="Google Drive" icon="link" href="/oss/python/integrations/retrievers/google_drive" arrow="true" cta="View guide" />

<Card title="Google Vertex AI Search" icon="link" href="/oss/python/integrations/retrievers/google_vertex_ai_search" arrow="true" cta="View guide" />

<Card title="Graph RAG" icon="link" href="/oss/python/integrations/retrievers/graph_rag" arrow="true" cta="View guide" />

<Card title="GreenNode" icon="link" href="/oss/python/integrations/retrievers/greennode_reranker" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/python/integrations/retrievers/ibm_watsonx_ranker" arrow="true" cta="View guide" />

<Card title="JaguarDB Vector Database" icon="link" href="/oss/python/integrations/retrievers/jaguar" arrow="true" cta="View guide" />

<Card title="Kay.ai" icon="link" href="/oss/python/integrations/retrievers/kay" arrow="true" cta="View guide" />

<Card title="Kinetica Vectorstore" icon="link" href="/oss/python/integrations/retrievers/kinetica" arrow="true" cta="View guide" />

<Card title="kNN" icon="link" href="/oss/python/integrations/retrievers/knn" arrow="true" cta="View guide" />

<Card title="LinkupSearchRetriever" icon="link" href="/oss/python/integrations/retrievers/linkup_search" arrow="true" cta="View guide" />

<Card title="LLMLingua Document Compressor" icon="link" href="/oss/python/integrations/retrievers/llmlingua" arrow="true" cta="View guide" />

<Card title="LOTR (Merger Retriever)" icon="link" href="/oss/python/integrations/retrievers/merger_retriever" arrow="true" cta="View guide" />

<Card title="Metal" icon="link" href="/oss/python/integrations/retrievers/metal" arrow="true" cta="View guide" />

<Card title="NanoPQ (Product Quantization)" icon="link" href="/oss/python/integrations/retrievers/nanopq" arrow="true" cta="View guide" />

<Card title="Nebius" icon="link" href="/oss/python/integrations/retrievers/nebius" arrow="true" cta="View guide" />

<Card title="needle" icon="link" href="/oss/python/integrations/retrievers/needle" arrow="true" cta="View guide" />

<Card title="Nimble" icon="link" href="/oss/python/integrations/retrievers/nimble" arrow="true" cta="View guide" />

<Card title="Outline" icon="link" href="/oss/python/integrations/retrievers/outline" arrow="true" cta="View guide" />

<Card title="Permit" icon="link" href="/oss/python/integrations/retrievers/permit" arrow="true" cta="View guide" />

<Card title="Pinecone Hybrid Search" icon="link" href="/oss/python/integrations/retrievers/pinecone_hybrid_search" arrow="true" cta="View guide" />

<Card title="Pinecone Rerank" icon="link" href="/oss/python/integrations/retrievers/pinecone_rerank" arrow="true" cta="View guide" />

<Card title="PubMed" icon="link" href="/oss/python/integrations/retrievers/pubmed" arrow="true" cta="View guide" />

<Card title="Qdrant Sparse Vector" icon="link" href="/oss/python/integrations/retrievers/qdrant-sparse" arrow="true" cta="View guide" />

<Card title="RAGatouille" icon="link" href="/oss/python/integrations/retrievers/ragatouille" arrow="true" cta="View guide" />

<Card title="RePhraseQuery" icon="link" href="/oss/python/integrations/retrievers/re_phrase" arrow="true" cta="View guide" />

<Card title="Rememberizer" icon="link" href="/oss/python/integrations/retrievers/rememberizer" arrow="true" cta="View guide" />

<Card title="SEC filing" icon="link" href="/oss/python/integrations/retrievers/sec_filings" arrow="true" cta="View guide" />

<Card title="SVM" icon="link" href="/oss/python/integrations/retrievers/svm" arrow="true" cta="View guide" />

<Card title="TavilySearchAPI" icon="link" href="/oss/python/integrations/retrievers/tavily" arrow="true" cta="View guide" />

<Card title="TF-IDF" icon="link" href="/oss/python/integrations/retrievers/tf_idf" arrow="true" cta="View guide" />

<Card title="NeuralDB" icon="link" href="/oss/python/integrations/retrievers/thirdai_neuraldb" arrow="true" cta="View guide" />

<Card title="ValyuContext" icon="link" href="/oss/python/integrations/retrievers/valyu" arrow="true" cta="View guide" />

<Card title="Vectorize" icon="link" href="/oss/python/integrations/retrievers/vectorize" arrow="true" cta="View guide" />

<Card title="Vespa" icon="link" href="/oss/python/integrations/retrievers/vespa" arrow="true" cta="View guide" />

<Card title="Wikipedia" icon="link" href="/oss/python/integrations/retrievers/wikipedia" arrow="true" cta="View guide" />

<Card title="You.com" icon="link" href="/oss/python/integrations/retrievers/you-retriever" arrow="true" cta="View guide" />

<Card title="Zep Cloud" icon="link" href="/oss/python/integrations/retrievers/zep_cloud_memorystore" arrow="true" cta="View guide" />

<Card title="Zep Open Source" icon="link" href="/oss/python/integrations/retrievers/zep_memorystore" arrow="true" cta="View guide" />

<Card title="Zotero" icon="link" href="/oss/python/integrations/retrievers/zotero" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/retrievers/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Retrieve a single item.

**URL:** llms-txt#retrieve-a-single-item.

Source: https://docs.langchain.com/langsmith/agent-server-api/store/retrieve-a-single-item

langsmith/agent-server-openapi.json get /store/items

---

## Role-based access control

**URL:** llms-txt#role-based-access-control

**Contents:**
- Role types
  - Organization roles
  - Workspace roles
- Custom roles
  - Creating custom roles

Source: https://docs.langchain.com/langsmith/rbac

This reference explains LangSmith's Role-Based Access Control (RBAC) system for managing organization-level and workspace-level permissions.

<Note>
  RBAC (Role-Based Access Control) is an Enterprise feature for managing workspace-level permissions. If you are interested in this feature, [contact our sales team](https://www.langchain.com/contact-sales). Other plans default to using the Admin role for all users.
</Note>

LangSmith's RBAC system manages user permissions within workspaces. RBAC allows you to control who can access your LangSmith [workspace](/langsmith/administration-overview#workspaces) and what they can do within it.

In LangSmith, each user has:

* One [**organization role**](#organization-roles) that applies across the entire organization (separate from workspace RBAC).
  * The Organization User and Organization Viewer roles are only available in organizations on [plans](https://langchain.com/pricing) with multiple workspaces. In organizations limited to a single workspace, all users have the Organization Admin role.
* One [**workspace role**](#workspace-roles) per workspace they're a member of (requires Enterprise RBAC feature).

On Enterprise plans, organizations can create [custom workspace roles](#custom-roles) with granular permission combinations.

To learn how to set up RBAC and assign roles to users, refer to the [User Management guide](/langsmith/user-management#set-up-access-control).

<Note>
  For a comprehensive list of required permissions along with the operations and roles that can perform them, refer to the [Organization and workspace reference](/langsmith/organization-workspace-operations).
</Note>

### Organization roles

Organization roles are **distinct from the workspace RBAC feature** and are used to manage organization-wide capabilities. The roles are system-defined and cannot be modified or extended. These roles are available in multi-workspace organizations on [Plus and Enterprise plans](https://langchain.com/pricing).

| Role                                        | Description                                                                           |
| ------------------------------------------- | ------------------------------------------------------------------------------------- |
| [Organization Admin](#organization-admin)   | Full permissions to manage organization configuration, users, billing, and workspaces |
| [Organization User](#organization-user)     | Read access to organization information and ability to create personal access tokens  |
| [Organization Viewer](#organization-viewer) | Read-only access to organization information                                          |

<Note>
  In organizations limited to a single workspace, all users are [Organization Admins](#organization-admin).
</Note>

#### Organization Admin

**Description**: Full permissions to manage all organization configuration, users, billing, and workspaces.

* `organization:manage` - Full control over organization settings, SSO, security, billing
* `organization:read` - Read access to all organization information
* `organization:pats:create` - Create organization-level [personal access tokens](/langsmith/administration-overview#personal-access-tokens-pats)

For a comprehensive list of required permissions along with the operations and roles that can perform them, refer to the [Organization and workspace reference](/langsmith/organization-workspace-operations).

**Key Capabilities**:

* Manage [organization settings](/langsmith/set-up-a-workspace#set-up-an-organization) and branding
* Configure [SSO and authentication methods](/langsmith/user-management#set-up-saml-sso-for-your-organization)
* Manage [billing](/langsmith/billing) and subscription plans
* Create and delete [workspaces](/langsmith/set-up-a-workspace)
* Invite and remove organization members
* Assign organization and workspace roles to members
* Create and manage [custom roles](#custom-roles)
* Configure RBAC and ABAC (Attribute-Based Access Control) policies (Note that ABAC is in private preview)
* View organization [usage](/langsmith/administration-overview#usage-limits) and analytics

For details on setting up and managing your organization, refer to the [Administration Overview](/langsmith/administration-overview#organizations).

#### Organization User

**Description**: Read access to organization information and ability to create personal access tokens.

* `organization:read` - Read access to organization information
* `organization:pats:create` - Create personal access tokens

For a comprehensive list of required permissions along with the operations and roles that can perform them, refer to the [Organization and workspace reference](/langsmith/organization-workspace-operations).

**Key Capabilities**:

* View organization members and workspaces
* View organization settings (but not modify)
* Create [personal access tokens](/langsmith/administration-overview#personal-access-tokens-pats) for API access
* Join workspaces they're invited to

* Cannot modify organization settings
* Cannot manage billing or subscriptions
* Cannot create or delete workspaces
* Cannot invite or remove organization members
* Cannot manage roles or permissions

You can add an Organization User to a subset of workspaces and assigned workspace roles (if RBAC is enabled), which specify permissions at the workspace level.

#### Organization Viewer

**Description**: Read-only access to organization information.

* `organization:read` - Read access to organization information

For a comprehensive list of required permissions along with the operations and roles that can perform them, refer to the [Organization and workspace reference](/langsmith/organization-workspace-operations).

**Key Capabilities**:

* View organization members and workspaces
* View organization settings

* Cannot modify anything at the organization level
* Cannot create personal access tokens
* Cannot manage billing, workspaces, or members

Workspace roles are part of the **Enterprise RBAC feature** and control what users can do with resources inside a workspace:

| Role                                  | Description                                                                                       |
| ------------------------------------- | ------------------------------------------------------------------------------------------------- |
| [Workspace Admin](#workspace-admin)   | Full permissions for all resources and ability to manage workspace                                |
| [Workspace Editor](#workspace-editor) | Full permissions for most resources, cannot manage workspace settings or delete certain resources |
| [Workspace Viewer](#workspace-viewer) | Read-only access to all workspace resources                                                       |

<Note>
  RBAC (Role-Based Access Control) is a feature that is only available to [Enterprise](https://langchain.com/pricing) customers. If you are interested in this feature, [contact our sales team](https://www.langchain.com/contact-sales). Other plans default to using the Admin role for all users.
</Note>

**Description**: Role with full permissions for all resources and ability to manage workspace.

* All create, read, update, delete, and share permissions for all resource types
* Workspace management capabilities

For a comprehensive list of required permissions along with the operations and roles that can perform them, refer to the [Organization and workspace reference](/langsmith/organization-workspace-operations).

#### Workspace Editor

**Description**: Role with full permissions for most resources. Cannot manage workspace settings or delete certain critical resources.

**Key Differences from Admin**:

* Cannot delete [runs](/langsmith/observability#runs)
* Cannot manage workspace settings (add/remove members, change workspace name, etc.)

#### Workspace Viewer

**Description**: Read-only access to all workspace resources.

**Permissions**: Read-only access to all resource types.

For a comprehensive list of required permissions along with the operations and roles that can perform them, refer to the [Organization and workspace reference](/langsmith/organization-workspace-operations).

<Tip>
  For step-by-step instructions on assigning workspace roles to users, refer to the [User Management guide](/langsmith/user-management#assign-a-role-to-a-user).
</Tip>

<Info>Creating custom roles is available for organizations on the Enterprise plan.</Info>

[Organization Admins](#organization-admin) can create custom roles with specific combinations of permissions tailored to their organization's needs.

### Creating custom roles

Custom roles are created at the [organization](/langsmith/administration-overview#organizations) level and can be assigned to users in any [workspace](/langsmith/administration-overview#workspaces) within that organization.

1. Navigate to Organization **Settings** > **Roles**.
2. Click **Create Custom Role**.
3. Select the permissions to include in the role.
4. Assign the custom role to users in specific workspaces.

For details on which specific permissions are required for each operation, refer to the [Organization and workspace operations reference](/langsmith/organization-workspace-operations).

Note the following details on custom roles:

* Custom roles can only be created and managed by Organization Admins.
* Custom roles are organization-specific (not transferable between organizations).
* Each custom role can have any combination of workspace-level permissions.
* Custom roles cannot have organization-level permissions.
* Users can have different roles (including custom roles) in different workspaces.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rbac.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Rollback Concurrent

**URL:** llms-txt#rollback-concurrent

**Contents:**
- Setup
- Create runs
- View run results

Source: https://docs.langchain.com/langsmith/rollback-concurrent

This guide assumes knowledge of what double-texting is, which you can learn about in the [double-texting conceptual guide](/langsmith/double-texting).

The guide covers the `rollback` option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option is very similar to the `interrupt` option, but in this case the first run is completely deleted from the database and cannot be restarted. Below is a quick example of using the `rollback` option.

First, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):

<Tabs>
  <Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Now let's run a thread with the multitask parameter set to "rollback":

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

We can see that the thread has data only from the second run

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Verify that the original, rolled back run was deleted

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rollback-concurrent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

Now, let's import our required packages and instantiate our client, assistant, and thread.

<Tabs>
  <Tab title="Python">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

---

## Routing model with structured output

**URL:** llms-txt#routing-model-with-structured-output

router_llm = init_chat_model("gpt-4o-mini").with_structured_output(
    UserIntent, method="json_schema", strict=True
)

---

## Run all tests

**URL:** llms-txt#run-all-tests

uv run --group test pytest tests/unit_tests/
uv run --group test --group test_integration pytest -n auto tests/integration_tests/

---

## Run an evaluation from the prompt playground

**URL:** llms-txt#run-an-evaluation-from-the-prompt-playground

**Contents:**
- Create an experiment in the prompt playground[​](#create-an-experiment-in-the-prompt-playground "Direct link to Create an experiment in the prompt playground")
- Add evaluation scores to the experiment[​](#add-evaluation-scores-to-the-experiment "Direct link to Add evaluation scores to the experiment")

Source: https://docs.langchain.com/langsmith/run-evaluation-from-prompt-playground

LangSmith allows you to run evaluations directly in the UI. The [**Prompt Playground**](/langsmith/prompt-engineering#prompt-playground) allows you to test your prompt or model configuration over a series of inputs to see how well it scores across different contexts or scenarios, without having to write any code.

Before you run an evaluation, you need to have an [existing dataset](/langsmith/evaluation-concepts#datasets). Learn how to [create a dataset from the UI](/langsmith/manage-datasets-in-application#set-up-your-dataset).

If you prefer to run experiments in code, visit [run an evaluation using the SDK](/langsmith/evaluate-llm-application).

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/playground-experiment.gif?s=05ec3d2b1aa6590c443a033924fc6141" alt="Playground experiment" data-og-width="1358" width="1358" data-og-height="720" height="720" data-path="langsmith/images/playground-experiment.gif" data-optimize="true" data-opv="3" />

<Callout type="info" icon="bird">
  **[Polly](/langsmith/polly)** is available in the Playground to help you optimize prompts before running evaluations.
</Callout>

## Create an experiment in the prompt playground[​](#create-an-experiment-in-the-prompt-playground "Direct link to Create an experiment in the prompt playground")

1. **Navigate to the playground** by clicking **Playground** in the sidebar.
2. **Add a prompt** by selecting an existing saved a prompt or creating a new one.
3. **Select a dataset** from the **Test over dataset** dropdown

* Note that the keys in the dataset input must match the input variables of the prompt. For example, in the above video the selected dataset has inputs with the key "blog", which correctly match the input variable of the prompt.
* There is a maximum of 15 input variables allowed in the prompt playground.

4. **Start the experiment** by clicking on the **Start** or CMD+Enter. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. We recommend committing the prompt to the prompt hub before starting the experiment so that it can be easily referenced later when reviewing your experiment.
5. **View the full results** by clicking **View full experiment**. This will take you to the experiment details page where you can see the results of the experiment.

## Add evaluation scores to the experiment[​](#add-evaluation-scores-to-the-experiment "Direct link to Add evaluation scores to the experiment")

Evaluate your experiment over specific critera by adding evaluators. Add LLM-as-a-judge or custom code evaluators in the playground using the **+Evaluator** button.

To learn more about adding evaluators in via UI, visit [how to define an LLM-as-a-judge evaluator](/langsmith/llm-as-judge).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-evaluation-from-prompt-playground.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Run an evaluation with multimodal content

**URL:** llms-txt#run-an-evaluation-with-multimodal-content

**Contents:**
- SDK
  - 1. Create examples with attachments

Source: https://docs.langchain.com/langsmith/evaluate-with-attachments

LangSmith lets you create dataset examples with file attachments—like images, audio files, or documents—so you can reference them when evaluating an application that uses multimodal inputs or outputs.

While you can include multimodal data in your examples by base64 encoding it, this approach is inefficient - the encoded data takes up more space than the original binary files, resulting in slower transfers to and from LangSmith. Using attachments instead provides two key benefits:

1. Faster upload and download speeds due to more efficient binary file transfers
2. Enhanced visualization of different file types in the LangSmith UI

### 1. Create examples with attachments

To upload examples with attachments using the SDK, use the [create\_examples](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.create_examples) / [update\_examples](https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client#langsmith.client.Client.update_examples) Python methods or the [uploadExamplesMultipart](https://docs.smith.langchain.com/reference/js/classes/client.Client#uploadexamplesmultipart) / [updateExamplesMultipart](https://docs.smith.langchain.com/reference/js/classes/client.Client#updateexamplesmultipart) TypeScript methods.

Requires `langsmith>=0.3.13`

```python  theme={null}
import requests
import uuid
from pathlib import Path
from langsmith import Client

---

## Run a local server

**URL:** llms-txt#run-a-local-server

**Contents:**
- Prerequisites
- 1. Install the LangGraph CLI
- 2. Create a LangGraph app
- 3. Install dependencies
- 4. Create a `.env` file
- 5. Launch Agent server
- 6. Test your application in Studio
- 7. Test the API
- Next steps

Source: https://docs.langchain.com/oss/python/langgraph/local-server

This guide shows you how to run a LangGraph application locally.

Before you begin, ensure you have the following:

* An API key for [LangSmith](https://smith.langchain.com/settings) - free to sign up

## 1. Install the LangGraph CLI

## 2. Create a LangGraph app

Create a new app from the [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project). This template demonstrates a single-node application you can extend with your own logic.

<Tip>
  **Additional templates**
  If you use `langgraph new` without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.
</Tip>

## 3. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

## 4. Create a `.env` file

You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys:

## 5. Launch Agent server

Start the LangGraph API server locally:

The `langgraph dev` command starts Agent Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy Agent Server with access to a persistent storage backend. For more information, see the [Platform setup overview](/langsmith/platform-setup).

## 6. Test your application in Studio

[Studio](/langsmith/studio) is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the `langgraph dev` command:

For an Agent Server running on a custom host/port, update the `baseUrl` query parameter in the URL. For example, if your server is running on `http://myhost:3000`:

<Accordion title="Safari compatibility">
  Use the `--tunnel` flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:

<Tabs>
  <Tab title="Python SDK (async)">
    1. Install the LangGraph Python SDK:
       
    2. Send a message to the assistant (threadless run):
       
  </Tab>

<Tab title="Python SDK (sync)">
    1. Install the LangGraph Python SDK:
       
    2. Send a message to the assistant (threadless run):
       
  </Tab>

<Tab title="Rest API">
    
  </Tab>
</Tabs>

Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:

* [Deployment quickstart](/langsmith/deployment-quickstart): Deploy your LangGraph app using LangSmith.

* [LangSmith](/langsmith/home): Learn about foundational LangSmith concepts.

* [SDK Reference](https://reference.langchain.com/python/langsmith/deployment/sdk/): Explore the SDK API Reference.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/local-server.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## 2. Create a LangGraph app

Create a new app from the [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project). This template demonstrates a single-node application you can extend with your own logic.
```

Example 3 (unknown):
```unknown
<Tip>
  **Additional templates**
  If you use `langgraph new` without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.
</Tip>

## 3. Install dependencies

In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run a specific test file

**URL:** llms-txt#run-a-specific-test-file

uv run --group test pytest tests/integration_tests/test_chat_models.py

---

## Run a specific test function in a file

**URL:** llms-txt#run-a-specific-test-function-in-a-file

uv run --group test pytest tests/integration_tests/test_chat_models.py::test_chat_completions

---

## Run a specific test function within a class

**URL:** llms-txt#run-a-specific-test-function-within-a-class

**Contents:**
- Troubleshooting

uv run --group test pytest tests/integration_tests/test_chat_models.py::TestChatParrotLinkIntegration::test_chat_completions
```

For a full list of the standard test suites that are available, as well as information on which tests are included and how to troubleshoot common issues, see the [Standard Tests API Reference](https://reference.langchain.com/python/langchain_tests).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/standard-tests-langchain.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Run creation, streaming, updates, etc.

**URL:** llms-txt#run-creation,-streaming,-updates,-etc.

---

## Run evaluation

**URL:** llms-txt#run-evaluation

results = client.evaluate(
    my_application,
    data="my_test_dataset",
    evaluators=[accuracy_evaluator],
    blocking=False
)

---

## Run evaluation and wait for all results

**URL:** llms-txt#run-evaluation-and-wait-for-all-results

results = client.evaluate(
    target,
    data=dataset,
    evaluators=[evaluator],
    blocking=True  # Wait for all evaluations to complete
)

---

## Run evaluation with blocking=False to get an iterator

**URL:** llms-txt#run-evaluation-with-blocking=false-to-get-an-iterator

streamed_results = client.evaluate(
    target,
    data="MY_DATASET_NAME",
    evaluators=[evaluator],
    blocking=False
)

---

## Run pipeline

**URL:** llms-txt#run-pipeline

**Contents:**
- Troubleshooting
  - Spans not appearing in LangSmith
  - Messages not showing correctly
  - Audio not working
  - Import errors
  - Performance issues
  - Advanced: Audio recording troubleshooting

runner = PipelineRunner()
try:
    await runner.run(task)
finally:
    audio_recorder.save_recording()
```

### Spans not appearing in LangSmith

If traces aren't showing up in LangSmith:

1. **Verify environment variables**: Ensure `OTEL_EXPORTER_OTLP_ENDPOINT` and `OTEL_EXPORTER_OTLP_HEADERS` are set correctly in your `.env` file.
2. **Check API key**: Confirm your LangSmith API key has write permissions.
3. **Verify import**: Make sure you're importing `span_processor` from `langsmith_processor.py`.
4. **Check .env loading**: Ensure `load_dotenv()` is called before importing Pipecat components.

### Messages not showing correctly

If conversation messages aren't displaying properly:

1. **Check span processor**: Verify `langsmith_processor.py` is in your project directory and imported correctly.
2. **Verify conversation ID**: Ensure you're setting a unique `conversation_id` in `PipelineTask`.
3. **Enable turn tracking**: Make sure `enable_turn_tracking=True` is set in `PipelineTask`.

### Audio not working

If your microphone or speakers aren't working:

1. **Check permissions**: Ensure your terminal/IDE has microphone access.
2. **Test audio devices**: Verify your microphone and speakers work in other applications.
3. **VAD settings**: Try adjusting `SileroVADAnalyzer()` settings if speech isn't being detected.
4. **Check services**: Ensure OpenAI API key is valid and has access to Whisper and TTS.

If you're getting import errors:

1. **Install dependencies**: Run `pip install langsmith "pipecat-ai[whisper,openai,local]" opentelemetry-exporter-otlp python-dotenv`.
2. **Check Python version**: Ensure you're using Python 3.9 or higher.
3. **Verify langsmith\_processor**: Make sure `langsmith_processor.py` is downloaded and in the same directory as your `agent.py`.

### Performance issues

If responses are slow:

1. **Use faster models**: Switch to `gpt-4o-mini` for the LLM (already in the tutorial).
2. **Check network**: Ensure stable internet connection for API calls.
3. **Local STT**: Consider using local Whisper instead of API-based services.

### Advanced: Audio recording troubleshooting

For issues with the advanced audio recording features, see the [complete demo documentation](https://github.com/langchain-ai/voice-agents-tracing).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-pipecat.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Run (span) data format

**URL:** llms-txt#run-(span)-data-format

Source: https://docs.langchain.com/langsmith/run-data-format

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and runs](/langsmith/observability-concepts)
</Check>

LangSmith stores and processes trace data in a simple format that is easy to export and import.

Many of these fields are optional or not important to know about but are included for completeness. The **bolded** fields are the most important ones to know about.

| Field Name                    | Type             | Description                                                                                                                   |
| ----------------------------- | ---------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| **id**                        | UUID             | Unique identifier for the span.                                                                                               |
| **name**                      | string           | The name associated with the run.                                                                                             |
| **inputs**                    | object           | A map or set of inputs provided to the run.                                                                                   |
| **run\_type**                 | string           | Type of run, e.g., "llm", "chain", "tool".                                                                                    |
| **start\_time**               | datetime         | Start time of the run.                                                                                                        |
| **end\_time**                 | datetime         | End time of the run.                                                                                                          |
| **extra**                     | object           | Any extra information run.                                                                                                    |
| **error**                     | string           | Error message if the run encountered an error.                                                                                |
| **outputs**                   | object           | A map or set of outputs generated by the run.                                                                                 |
| **events**                    | array of objects | A list of event objects associated with the run. This is relevant for runs executed with streaming.                           |
| **tags**                      | array of strings | Tags or labels associated with the run.                                                                                       |
| **trace\_id**                 | UUID             | Unique identifier for the trace the run is a part of. This is also the `id` field of the root run of the trace                |
| **dotted\_order**             | string           | Ordering string, hierarchical. Format: `run_start_time`Z`run_uuid`.`child_run_start_time`Z`child_run_uuid`...                 |
| **status**                    | string           | Current status of the run execution, e.g., "error", "pending", "success"                                                      |
| **child\_run\_ids**           | array of UUIDs   | List of IDs for all child runs.                                                                                               |
| **direct\_child\_run\_ids**   | array of UUIDs   | List of IDs for direct children of this run.                                                                                  |
| **parent\_run\_ids**          | array of UUIDs   | List of IDs for all parent runs.                                                                                              |
| **feedback\_stats**           | object           | Aggregations of feedback statistics for this run                                                                              |
| **reference\_example\_id**    | UUID             | ID of a reference example associated with the run. This is usually only present for evaluation runs.                          |
| **total\_tokens**             | integer          | Total number of tokens processed by the run.                                                                                  |
| **prompt\_tokens**            | integer          | Number of tokens in the prompt of the run.                                                                                    |
| **completion\_tokens**        | integer          | Number of tokens in the completion of the run.                                                                                |
| **total\_cost**               | decimal          | Total cost associated with processing the run.                                                                                |
| **prompt\_cost**              | decimal          | Cost associated with the prompt part of the run.                                                                              |
| **completion\_cost**          | decimal          | Cost associated with the completion of the run.                                                                               |
| **first\_token\_time**        | datetime         | Time when the first token of a model output was generated. Only applies for runs with `run_type="llm"` and streaming enabled. |
| **session\_id**               | string           | Session identifier for the run, also known as the tracing project ID.                                                         |
| **in\_dataset**               | boolean          | Indicates whether the run is included in a dataset.                                                                           |
| **parent\_run\_id**           | UUID             | Unique identifier of the parent run.                                                                                          |
| execution\_order (deprecated) | integer          | The order in which this run was executed within the trace.                                                                    |
| serialized                    | object           | Serialized state of the object executing the run if applicable.                                                               |
| manifest\_id (deprecated)     | UUID             | Identifier for a manifest associated with the span.                                                                           |
| manifest\_s3\_id              | UUID             | S3 identifier for the manifest.                                                                                               |
| inputs\_s3\_urls              | object           | S3 URLs for the inputs.                                                                                                       |
| outputs\_s3\_urls             | object           | S3 URLs for the outputs.                                                                                                      |
| price\_model\_id              | UUID             | Identifier for the pricing model applied to the run.                                                                          |
| app\_path                     | string           | Application (UI) path for this run.                                                                                           |
| last\_queued\_at              | datetime         | Last time the span was queued.                                                                                                |
| share\_token                  | string           | Token for sharing access to the run's data.                                                                                   |

Here is an example of a JSON representation of a run in the above format:

#### What is `dotted_order`?

A run's dotted order is a sortable key that fully specifies its location within the tracing hierarchy.

Take the following example:

If you print out the IDs at each stage, you may get the following:

Note a few invariants:

* The "id" is equal to the last 36 characters of the dotted order (the suffix after the final "Z"). See `0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6` for example in the grandchild.
* The "trace\_id" is equal to the first UUID in the dotted order (i.e., `dotted_order.split('.')[0].split('Z')[1]`)
* If "parent\_run\_id" exists, it is the penultimate UUID in the dotted order. See `a8024e23-5b82-47fd-970e-f6a5ba3f5097` in the grandchild, for an example.
* If you split the dotted\_order on the dots, each segment is formatted as (`<run_start_time>Z<run_id>`)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/run-data-format.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### What is `dotted_order`?

A run's dotted order is a sortable key that fully specifies its location within the tracing hierarchy.

Take the following example:
```

Example 2 (unknown):
```unknown
If you print out the IDs at each stage, you may get the following:
```

---

## Run support queries against ClickHouse

**URL:** llms-txt#run-support-queries-against-clickhouse

**Contents:**
  - Prerequisites
  - Running the query script

Source: https://docs.langchain.com/langsmith/script-running-ch-support-queries

This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining query exception logs from Clickhouse).

This command takes a clickhouse connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the `ch_get_query_exceptions.sql` input file in the `support_queries/clickhouse` directory.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

2. Clickhouse database credentials

* Host
   * Port
   * Username
     * If using the bundled version, this is `default`
   * Password
     * If using the bundled version, this is `password`
   * Database name
     * If using the bundled version, this is `default`

3. Connectivity to the Clickhouse database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.
   * Run `kubectl port-forward svc/langsmith-clickhouse 8123:8123` to port forward the clickhouse service to your local machine.

4. The script to run a support query

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/run_support_query_ch.sh)

### Running the query script

Run the following command to run the desired query:

For example, if you are using the bundled version with port-forwarding, the command might look like:

which will output query logs for all queries that have thrown exceptions in Clickhouse in the last 7 days. To extract this to a file add the flag `--output path/to/file.csv`

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-running-ch-support-queries.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command might look like:
```

---

## Run support queries against PostgreSQL

**URL:** llms-txt#run-support-queries-against-postgresql

**Contents:**
- Prerequisites
- Running the query script
- Export usage data
  - Get customer information

Source: https://docs.langchain.com/langsmith/script-running-pg-support-queries

This Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining trace counts for multiple organizations in a single query).

This command takes a postgres connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the `pg_get_trace_counts_daily.sql` input file in the `support_queries/postgres` directory.

Ensure you have the following tools/items ready.

* [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/)

* [https://www.postgresql.org/download/](https://www.postgresql.org/download/)

3. PostgreSQL database connection:

* Host
   * Port
   * Username
     * If using the bundled version, this is `postgres`
   * Password
     * If using the bundled version, this is `postgres`
   * Database name
     * If using the bundled version, this is `postgres`

4. Connectivity to the PostgreSQL database from the machine you will be running the migration script on.

* If you are using the bundled version, you may need to port forward the postgresql service to your local machine.
   * Run `kubectl port-forward svc/langsmith-postgres 5432:5432` to port forward the postgresql service to your local machine.

5. The script to run a support query

* You can download the script from [here](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/run_support_query_pg.sh)

## Running the query script

Run the following command to run the desired query:

For example, if you are using the bundled version with port-forwarding, the command might look like:

which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag `--output path/to/file.csv`

<Note>
  Exporting usage data requires running Helm chart version 0.11.4 or later.
</Note>

### Get customer information

You need to retrieve your customer information from the LangSmith API before running the export scripts. This information is required as input for the export scripts.

```bash  theme={null}
curl https://<langsmith_url>/api/v1/info

**Examples:**

Example 1 (unknown):
```unknown
For example, if you are using the bundled version with port-forwarding, the command might look like:
```

Example 2 (unknown):
```unknown
which will output the count of daily traces by workspace ID and organization ID. To extract this to a file add the flag `--output path/to/file.csv`

## Export usage data

<Note>
  Exporting usage data requires running Helm chart version 0.11.4 or later.
</Note>

### Get customer information

You need to retrieve your customer information from the LangSmith API before running the export scripts. This information is required as input for the export scripts.
```

---

## Run tests to ensure your changes don't break existing functionality

**URL:** llms-txt#run-tests-to-ensure-your-changes-don't-break-existing-functionality

**Contents:**
- Documentation types
  - How-to guides
  - Conceptual guides
  - Reference
  - Tutorials
- Writing standards
  - Mintlify components
  - Page structure
  - Co-locate Python and JavaScript/TypeScript content
- Quality standards

make test
python`, ` yaml  theme={null}
---
title: "Clear, specific title"
sidebarTitle: "Short title for the sidebar (optional)"
---
mdx  theme={null}
:::python
Python-specific content. In real docs, the preceding backslash (before `python`) is omitted.
:::

:::js
JavaScript/TypeScript-specific content. In real docs, the preceding backslash (before `js`) is omitted.
:::

Content for both languages (not wrapped)
mdx  theme={null}
See @[`ChatAnthropic`] for all configuration options.

The @[`bind_tools`][ChatAnthropic.bind_tools] method accepts...
```

The build pipeline transforms these into proper markdown links based on the current language scope (Python or JavaScript). For example, `@[ChatAnthropic]` becomes a link to the Python or JS API reference page depending on which version of the docs is being built, **but only if an entry exists in the `link_map.py` file!** See below for details.

<Accordion title="How autolinks work">
  The `@[]` syntax is processed by [`handle_auto_links.py`](https://github.com/langchain-ai/docs/blob/main/pipeline/preprocessors/handle_auto_links.py). It looks up link keys in [`link_map.py`](https://github.com/langchain-ai/docs/blob/main/pipeline/preprocessors/link_map.py), which contains dictionary mappings for both Python and JavaScript scopes.

**Supported formats:**

| Syntax                   | Result                                                                                     |
  | ------------------------ | ------------------------------------------------------------------------------------------ |
  | `@[ChatAnthropic]`       | Link with "ChatAnthropic" as the displayed text                                            |
  | ``@[`ChatAnthropic`]``   | Link with `` `ChatAnthropic` `` (code formatted) as text                                   |
  | `@[text][ChatAnthropic]` | Link with "text" as text and `ChatAnthropic` as the key in the link map                    |
  | `\@[ChatAnthropic]`      | Escaped: renders as literal `@[ChatAnthropic]` (no link – what's being used on this page!) |

**Adding new links:**

If a link isn't found in the map, it will be left unchanged in the output. To add a new autolink:

1. Open `pipeline/preprocessors/link_map.py`
  2. Add an entry to the appropriate scope (`python` or `js`) in `LINK_MAPS`
  3. The key is the link name used in `@[key]` or `@[text][key]`, the value is the path relative to the reference host
</Accordion>

**From API reference stubs to OSS docs:**

See the [`README`](https://github.com/langchain-ai/docs/blob/main/reference/python/README.md) for more information on linking from API reference stubs to Python OSS docs. Specifically see the `mkdocstrings` cross-reference [linking syntax](https://github.com/langchain-ai/docs/blob/main/reference/python/README.md#mkdocsmkdocstrings-python-cross-reference-linking-syntax).

Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/). Internal team members can reach out in the [#documentation](https://langchain.slack.com/archives/C04GWPE38LV) Slack channel.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/documentation.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For more details, see the [available commands](https://github.com/langchain-ai/docs?tab=readme-ov-file#available-commands) section in the `README`.

<Important>
  All pull requests are automatically checked by CI/CD. The same linting and formatting standards will be enforced, and PRs cannot be merged if these checks fail.
</Important>

#### Publish to prod

<Note>
  Only internal team members can publish to production.
</Note>

<Accordion title="Instructions">
  Once your branch has been merged into `main`, you need to push the changes to `prod` for them to render on the live docs site. Use the [Publish documentation GH action](https://github.com/langchain-ai/docs/actions/workflows/publish.yml):

  1. Go to [Publish documentation](https://github.com/langchain-ai/docs/actions/workflows/publish.yml).
  2. Click the **Run workflow** button.
  3. Select the **main** branch to deploy.
  4. Click **Run workflow**.
</Accordion>

## Documentation types

All documentation falls under one of four categories:

<CardGroup cols={2}>
  <Card title="How-to guides" icon="wrench" href="#how-to-guides">
    Task-oriented instructions for users who know what they want to accomplish.
  </Card>

  <Card title="Conceptual guides" icon="lightbulb" href="#conceptual-guides">
    Explanations that provide deeper understanding and insights.
  </Card>

  <Card title="Reference" icon="book" href="#reference">
    Technical descriptions of APIs and implementation details.
  </Card>

  <Card title="Tutorials" icon="graduation-cap" href="#tutorials">
    Lessons that guide users through practical activities to build understanding.
  </Card>
</CardGroup>

<Note>
  Where applicable, all documentation must have both Python and JavaScript/TypeScript content. For more details, see the [co-locate Python and JavaScript/TypeScript content](#co-locate-python-and-javascripttypescript-content) section.
</Note>

### How-to guides

How-to guides are task-oriented instructions for users who know what they want to accomplish. Examples of how-to guides are on the [LangChain](/oss/python/langchain/overview) and [LangGraph](/oss/python/langgraph/overview) tabs.

<AccordionGroup>
  <Accordion title="Characteristics">
    * **Task-focused**: Focus on a specific task or problem
    * **Step-by-step**: Break down the task into smaller steps
    * **Hands-on**: Provide concrete examples and code snippets
  </Accordion>

  <Accordion title="Tips">
    * Focus on the **how** rather than the **why**
    * Use concrete examples and code snippets
    * Break down the task into smaller steps
    * Link to related conceptual guides and references
  </Accordion>

  <Accordion title="Examples">
    * [Messages](/oss/python/langchain/messages)
    * [Tools](/oss/python/langchain/tools)
    * [Streaming](/oss/python/langgraph/streaming)
  </Accordion>
</AccordionGroup>

### Conceptual guides

Conceptual guide cover core concepts abstractly, providing deep understanding.

<AccordionGroup>
  <Accordion title="Characteristics">
    * **Understanding-focused**: Explain why things work as they do
    * **Broad perspective**: Higher and wider view than other types
    * **Design-oriented**: Explain decisions and trade-offs
    * **Context-rich**: Use analogies and comparisons
  </Accordion>

  <Accordion title="Tips">
    * Focus on the **"why"** rather than the "how"
    * Provides supplementary information not necessarily required for feature usage
    * Can use analogies and reference alternatives
    * Avoid blending in too much reference content
    * Link to related tutorials and how-to guides
  </Accordion>

  <Accordion title="Examples">
    * [Memory](/oss/python/concepts/memory)
    * [Context](/oss/python/concepts/context)
    * [Graph API](/oss/python/langgraph/graph-api)
    * [Functional API](/oss/python/langgraph/functional-api)
  </Accordion>
</AccordionGroup>

### Reference

Reference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.

<CardGroup cols={2}>
  <Card title="Python reference" href="https://reference.langchain.com/python/" icon="python" arrow />

  <Card title="JavaScript/TypeScript reference" href="https://reference.langchain.com/javascript/" icon="js" arrow />
</CardGroup>

A good reference should:

* Describe what exists (all parameters, options, return values)
* Be comprehensive and structured for easy lookup
* Serve as the authoritative source for technical details

<AccordionGroup>
  <Accordion title="Contributing to references">
    See the contributing guide for [Python reference docs](https://github.com/langchain-ai/docs/blob/main/reference/python/README.md).
  </Accordion>

  <Accordion title="LangChain reference best practices">
    * **Be consistent**; follow existing patterns for provider-specific documentation
    * Include both basic usage (code snippets) and common edge cases/failure modes
    * Note when features require specific versions
  </Accordion>

  <Accordion title="When to create new reference documentation">
    * New integrations or providers need dedicated reference pages
    * Complex configuration options require detailed explanation
    * API changes introduce new parameters or behavior
    * Community frequently asks questions about specific functionality
  </Accordion>
</AccordionGroup>

### Tutorials

Tutorials are longer form step-by-step guides that builds upon itself and takes users through a specific practical activity to build understanding. Tutorials are typically found on the [Learn](/oss/python/learn) tab.

<Note>
  We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please [open a new issue](https://github.com/langchain-ai/docs/issues).
</Note>

<AccordionGroup>
  <Accordion title="Characteristics">
    * **Practical**: Focus on practical activities to build understanding.
    * **Step-by-step**: Break down the activity into smaller steps.
    * **Hands-on**: Provide sequential, working code snippets.
    * **Supplementary**: Provide additional context and information not necessarily required for feature usage.
  </Accordion>

  <Accordion title="Tips">
    * Code snippets should be sequential and working if the user follows the steps in order.
    * Provide some context for the activity, but link to related conceptual guides and references for more detailed information.
  </Accordion>

  <Accordion title="Examples">
    * [Semantic search](/oss/python/langchain/knowledge-base)
    * [RAG agent](/oss/python/langchain/rag)
  </Accordion>
</AccordionGroup>

## Writing standards

<Note>
  Reference documentation has different standards - see the [reference docs contributing guide](https://github.com/langchain-ai/docs/blob/main/reference/python/README.md) for details.
</Note>

### Mintlify components

Use [Mintlify components](https://mintlify.com/docs/text) to enhance readability:

<Tabs>
  <Tab title="Callouts">
    * `<Note>` for helpful supplementary information
    * `<Warning>` for important cautions and breaking changes
    * `<Tip>` for best practices and advice
    * `<Info>` for neutral contextual information
    * `<Check>` for success confirmations
  </Tab>

  <Tab title="Structure">
    * `<Steps>` for an overview of sequential procedures. **Not** for long lists of steps or tutorials.
    * `<Tabs>` for platform-specific content.
    * `<AccordionGroup>` and `<Accordion>` for nice-to-have information that can be collapsed by default (e.g., full code examples).
    * `<CardGroup>` and `<Card>` for highlighting content.
  </Tab>

  <Tab title="Code">
    * `<CodeGroup>` for multiple language examples.
    * Always specify language tags on code blocks (e.g., `
```

Example 2 (unknown):
```unknown
### Co-locate Python and JavaScript/TypeScript content

All documentation must be written in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:
```

Example 3 (unknown):
```unknown
This will generate two outputs (one for each language) at `/oss/python/concepts/foo.mdx` and `/oss/javascript/concepts/foo.mdx`. Each outputted page will need to be added to the `/src/docs.json` file to be included in the navigation.

<Note>
  We don't want a lack of parity to block contributions. If a feature is only available in one language, it's okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language.

  If you need help translating content between Python and JavaScript/TypeScript, please ask in the [community slack](https://www.langchain.com/join-community) or tag a maintainer in your PR.
</Note>

## Quality standards

### General guidelines

<AccordionGroup>
  <Accordion title="Avoid duplication">
    Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.
  </Accordion>

  <Accordion title="Link frequently">
    Documentation sections don't exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.
  </Accordion>

  <Accordion title="Be concise">
    Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.
  </Accordion>
</AccordionGroup>

### Accessibility requirements

Ensure documentation is accessible to all users:

* Structure content for easy scanning with headers and lists
* Use specific, actionable link text instead of "click here"
* Include descriptive alt text for all images and diagrams

### Cross-referencing

Use consistent cross-references to connect docs with API reference documentation.

**From docs to API reference:**

Use the `@[]` syntax to link to API reference pages:
```

---

## Run the entire test suite

**URL:** llms-txt#run-the-entire-test-suite

---

## Run the export script with customer information as variables

**URL:** llms-txt#run-the-export-script-with-customer-information-as-variables

**Contents:**
  - Status update

sh run_support_query_pg.sh <postgres_url> \
  --input support_queries/postgres/pg_usage_traces_backfill_export.sql \
  --output ls_export.csv \
  -v customer_id=$CUSTOMER_ID \
  -v customer_name=$CUSTOMER_NAME
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> \
  --input support_queries/postgres/pg_usage_nodes_backfill_export.sql \
  --output lgp_export.csv \
  -v customer_id=$CUSTOMER_ID \
  -v customer_name=$CUSTOMER_NAME
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> --input support_queries/postgres/pg_usage_traces_backfill_update.sql --output export.csv -v backfill_id=<backfill_id>
bash  theme={null}
sh run_support_query_pg.sh <postgres_url> --input support_queries/postgres/pg_usage_nodes_backfill_update.sql --output export.csv -v backfill_id=<backfill_id>
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/script-running-pg-support-queries.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To export LangSmith usage:
```

Example 2 (unknown):
```unknown
### Status update

These scripts update the status of usage events in your installation to reflect that the events have been successfully processed by LangChain.

The scripts require passing in the corresponding `backfill_id`, which will be confirmed by your LangChain rep.

To update LangSmith trace usage:
```

Example 3 (unknown):
```unknown
To update LangSmith usage:
```

---

## Run the graph

**URL:** llms-txt#run-the-graph

**Contents:**
- Use user-scoped MCP tools in your deployment
- Session behavior
- Authentication
- Disable MCP

print(graph.invoke({"question": "hi"}))
python  theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient

def mcp_tools_node(state, config):
    user = config["configurable"].get("langgraph_auth_user")
         , user["github_token"], user["email"], etc.

client = MultiServerMCPClient({
        "github": {
            "transport": "streamable_http", # (1)
            "url": "https://my-github-mcp-server/mcp", # (2)
            "headers": {
                "Authorization": f"Bearer {user['github_token']}"
            }
        }
    })
    tools = await client.get_tools() # (3)

# Your tool-calling logic here

tool_messages = ...
    return {"messages": tool_messages}
json  theme={null}
{
  "$schema": "https://langgra.ph/schema.json",
  "http": {
    "disable_mcp": true
  }
}
```

This will prevent the server from exposing the `/mcp` endpoint.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/server-mcp.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
For more details, see the [low-level concepts guide](/oss/python/langgraph/graph-api#state).

## Use user-scoped MCP tools in your deployment

<Tip>
  **Prerequisites**
  You have added your own [custom auth middleware](/langsmith/custom-auth) that populates the `langgraph_auth_user` object, making it accessible through configurable context for every node in your graph.
</Tip>

To make user-scoped tools available to your LangSmith deployment, start with implementing a snippet like the following:
```

Example 2 (unknown):
```unknown
1. MCP only supports adding headers to requests made to `streamable_http` and `sse` `transport` servers.
2. Your MCP server URL.
3. Get available tools from your MCP server.

*This can also be done by [rebuilding your graph at runtime](/langsmith/graph-rebuild) to have a different configuration for a new run*

## Session behavior

The current LangGraph MCP implementation does not support sessions. Each `/mcp` request is stateless and independent.

## Authentication

The `/mcp` endpoint uses the same authentication as the rest of the LangGraph API. Refer to the [authentication guide](/langsmith/auth) for setup details.

## Disable MCP

To disable the MCP endpoint, set `disable_mcp` to `true` in your `langgraph.json` configuration file:
```

---

## Run the graph until the interrupt is hit.

**URL:** llms-txt#run-the-graph-until-the-interrupt-is-hit.

result = agent.invoke(
    {
        "messages": [
            {
                "role": "user",
                "content": "Delete old records from the database",
            }
        ]
    },
    config=config # [!code highlight]
)

---

## Sales Analytics Schema

**URL:** llms-txt#sales-analytics-schema

**Contents:**
- Tables
  - customers
  - orders
- Business Logic
- 6. Advanced: Add constraints with custom state
- Complete example
- Implementation variations
- Progressive disclosure and context engineering
- Next steps

### customers
- customer_id (PRIMARY KEY)
- name
- email
- signup_date
- status (active/inactive)
- customer_tier (bronze/silver/gold/platinum)

### orders
- order_id (PRIMARY KEY)
- customer_id (FOREIGN KEY -> customers)
- order_date
- status (pending/completed/cancelled/refunded)
- total_amount
- sales_region (north/south/east/west)

[... rest of schema ...]

**High-value orders**: Orders with `total_amount > 1000`
**Revenue calculation**: Only count orders with `status = 'completed'`

================================== Ai Message ==================================

Here's a SQL query to find all customers who made orders over $1000 in the last month:

\`\`\`sql
SELECT DISTINCT
    c.customer_id,
    c.name,
    c.email,
    c.customer_tier
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
WHERE o.total_amount > 1000
  AND o.status = 'completed'
  AND o.order_date >= CURRENT_DATE - INTERVAL '1 month'
ORDER BY c.customer_id;
\`\`\`

This query:
- Joins customers with their orders
- Filters for high-value orders (>$1000) using the total_amount field
- Only includes completed orders (as per the business logic)
- Restricts to orders from the last month
- Returns distinct customers to avoid duplicates if they made multiple qualifying orders
python  theme={null}
  from langchain.agents.middleware import AgentState

class CustomState(AgentState):  # [!code highlight]
      skills_loaded: NotRequired[list[str]]  # Track which skills have been loaded  # [!code highlight]
  python  theme={null}
  from langgraph.types import Command  # [!code highlight]
  from langchain.tools import tool, ToolRuntime
  from langchain.messages import ToolMessage  # [!code highlight]

@tool
  def load_skill(skill_name: str, runtime: ToolRuntime) -> Command:  # [!code highlight]
      """Load the full content of a skill into the agent's context.

Use this when you need detailed information about how to handle a specific
      type of request. This will provide you with comprehensive instructions,
      policies, and guidelines for the skill area.

Args:
          skill_name: The name of the skill to load
      """
      # Find and return the requested skill
      for skill in SKILLS:
          if skill["name"] == skill_name:
              skill_content = f"Loaded skill: {skill_name}\n\n{skill['content']}"

# Update state to track loaded skill
              return Command(  # [!code highlight]
                  update={  # [!code highlight]
                      "messages": [  # [!code highlight]
                          ToolMessage(  # [!code highlight]
                              content=skill_content,  # [!code highlight]
                              tool_call_id=runtime.tool_call_id,  # [!code highlight]
                          )  # [!code highlight]
                      ],  # [!code highlight]
                      "skills_loaded": [skill_name],  # [!code highlight]
                  }  # [!code highlight]
              )  # [!code highlight]

# Skill not found
      available = ", ".join(s["name"] for s in SKILLS)
      return Command(
          update={
              "messages": [
                  ToolMessage(
                      content=f"Skill '{skill_name}' not found. Available skills: {available}",
                      tool_call_id=runtime.tool_call_id,
                  )
              ]
          }
      )
  `python  theme={null}
  @tool
  def write_sql_query(  # [!code highlight]
      query: str,
      vertical: str,
      runtime: ToolRuntime,
  ) -> str:
      """Write and validate a SQL query for a specific business vertical.

This tool helps format and validate SQL queries. You must load the
      appropriate skill first to understand the database schema.

Args:
          query: The SQL query to write
          vertical: The business vertical (sales_analytics or inventory_management)
      """
      # Check if the required skill has been loaded
      skills_loaded = runtime.state.get("skills_loaded", [])  # [!code highlight]

if vertical not in skills_loaded:  # [!code highlight]
          return (  # [!code highlight]
              f"Error: You must load the '{vertical}' skill first "  # [!code highlight]
              f"to understand the database schema before writing queries. "  # [!code highlight]
              f"Use load_skill('{vertical}') to load the schema."  # [!code highlight]
          )  # [!code highlight]

# Validate and format the query
      return (
          f"SQL Query for {vertical}:\n\n"
          f"\n\n"
          f"✓ Query validated against {vertical} schema\n"
          f"Ready to execute against the database."
      )
  python  theme={null}
  class SkillMiddleware(AgentMiddleware[CustomState]):  # [!code highlight]
      """Middleware that injects skill descriptions into the system prompt."""

state_schema = CustomState  # [!code highlight]
      tools = [load_skill, write_sql_query]  # [!code highlight]

# ... rest of the middleware implementation stays the same
  python  theme={null}
  agent = create_agent(
      model,
      system_prompt=(
          "You are a SQL query assistant that helps users "
          "write queries against business databases."
      ),
      middleware=[SkillMiddleware()],  # [!code highlight]
      checkpointer=InMemorySaver(),
  )
  python  theme={null}
  import uuid
  from typing import TypedDict, NotRequired
  from langchain.tools import tool
  from langchain.agents import create_agent
  from langchain.agents.middleware import ModelRequest, ModelResponse, AgentMiddleware
  from langchain.messages import SystemMessage
  from langgraph.checkpoint.memory import InMemorySaver
  from typing import Callable

# Define skill structure
  class Skill(TypedDict):
      """A skill that can be progressively disclosed to the agent."""
      name: str
      description: str
      content: str

# Define skills with schemas and business logic
  SKILLS: list[Skill] = [
      {
          "name": "sales_analytics",
          "description": "Database schema and business logic for sales data analysis including customers, orders, and revenue.",
          "content": """# Sales Analytics Schema

### customers
  - customer_id (PRIMARY KEY)
  - name
  - email
  - signup_date
  - status (active/inactive)
  - customer_tier (bronze/silver/gold/platinum)

### orders
  - order_id (PRIMARY KEY)
  - customer_id (FOREIGN KEY -> customers)
  - order_date
  - status (pending/completed/cancelled/refunded)
  - total_amount
  - sales_region (north/south/east/west)

### order_items
  - item_id (PRIMARY KEY)
  - order_id (FOREIGN KEY -> orders)
  - product_id
  - quantity
  - unit_price
  - discount_percent

**Active customers**: status = 'active' AND signup_date <= CURRENT_DATE - INTERVAL '90 days'

**Revenue calculation**: Only count orders with status = 'completed'. Use total_amount from orders table, which already accounts for discounts.

**Customer lifetime value (CLV)**: Sum of all completed order amounts for a customer.

**High-value orders**: Orders with total_amount > 1000

-- Get top 10 customers by revenue in the last quarter
  SELECT
      c.customer_id,
      c.name,
      c.customer_tier,
      SUM(o.total_amount) as total_revenue
  FROM customers c
  JOIN orders o ON c.customer_id = o.customer_id
  WHERE o.status = 'completed'
    AND o.order_date >= CURRENT_DATE - INTERVAL '3 months'
  GROUP BY c.customer_id, c.name, c.customer_tier
  ORDER BY total_revenue DESC
  LIMIT 10;
  """,
      },
      {
          "name": "inventory_management",
          "description": "Database schema and business logic for inventory tracking including products, warehouses, and stock levels.",
          "content": """# Inventory Management Schema

### products
  - product_id (PRIMARY KEY)
  - product_name
  - sku
  - category
  - unit_cost
  - reorder_point (minimum stock level before reordering)
  - discontinued (boolean)

### warehouses
  - warehouse_id (PRIMARY KEY)
  - warehouse_name
  - location
  - capacity

### inventory
  - inventory_id (PRIMARY KEY)
  - product_id (FOREIGN KEY -> products)
  - warehouse_id (FOREIGN KEY -> warehouses)
  - quantity_on_hand
  - last_updated

### stock_movements
  - movement_id (PRIMARY KEY)
  - product_id (FOREIGN KEY -> products)
  - warehouse_id (FOREIGN KEY -> warehouses)
  - movement_type (inbound/outbound/transfer/adjustment)
  - quantity (positive for inbound, negative for outbound)
  - movement_date
  - reference_number

**Available stock**: quantity_on_hand from inventory table where quantity_on_hand > 0

**Products needing reorder**: Products where total quantity_on_hand across all warehouses is less than or equal to the product's reorder_point

**Active products only**: Exclude products where discontinued = true unless specifically analyzing discontinued items

**Stock valuation**: quantity_on_hand * unit_cost for each product

-- Find products below reorder point across all warehouses
  SELECT
      p.product_id,
      p.product_name,
      p.reorder_point,
      SUM(i.quantity_on_hand) as total_stock,
      p.unit_cost,
      (p.reorder_point - SUM(i.quantity_on_hand)) as units_to_reorder
  FROM products p
  JOIN inventory i ON p.product_id = i.product_id
  WHERE p.discontinued = false
  GROUP BY p.product_id, p.product_name, p.reorder_point, p.unit_cost
  HAVING SUM(i.quantity_on_hand) <= p.reorder_point
  ORDER BY units_to_reorder DESC;
  """,
      },
  ]

# Create skill loading tool
  @tool
  def load_skill(skill_name: str) -> str:
      """Load the full content of a skill into the agent's context.

Use this when you need detailed information about how to handle a specific
      type of request. This will provide you with comprehensive instructions,
      policies, and guidelines for the skill area.

Args:
          skill_name: The name of the skill to load (e.g., "sales_analytics", "inventory_management")
      """
      # Find and return the requested skill
      for skill in SKILLS:
          if skill["name"] == skill_name:
              return f"Loaded skill: {skill_name}\n\n{skill['content']}"

# Skill not found
      available = ", ".join(s["name"] for s in SKILLS)
      return f"Skill '{skill_name}' not found. Available skills: {available}"

# Create skill middleware
  class SkillMiddleware(AgentMiddleware):
      """Middleware that injects skill descriptions into the system prompt."""

# Register the load_skill tool as a class variable
      tools = [load_skill]

def __init__(self):
          """Initialize and generate the skills prompt from SKILLS."""
          # Build skills prompt from the SKILLS list
          skills_list = []
          for skill in SKILLS:
              skills_list.append(
                  f"- **{skill['name']}**: {skill['description']}"
              )
          self.skills_prompt = "\n".join(skills_list)

def wrap_model_call(
          self,
          request: ModelRequest,
          handler: Callable[[ModelRequest], ModelResponse],
      ) -> ModelResponse:
          """Sync: Inject skill descriptions into system prompt."""
          # Build the skills addendum
          skills_addendum = (
              f"\n\n## Available Skills\n\n{self.skills_prompt}\n\n"
              "Use the load_skill tool when you need detailed information "
              "about handling a specific type of request."
          )

# Append to system message content blocks
          new_content = list(request.system_message.content_blocks) + [
              {"type": "text", "text": skills_addendum}
          ]
          new_system_message = SystemMessage(content=new_content)
          modified_request = request.override(system_message=new_system_message)
          return handler(modified_request)

# Initialize your chat model (replace with your model)
  # Example: from langchain_anthropic import ChatAnthropic
  # model = ChatAnthropic(model="claude-3-5-sonnet-20241022")
  from langchain_openai import ChatOpenAI
  model = ChatOpenAI(model="gpt-4")

# Create the agent with skill support
  agent = create_agent(
      model,
      system_prompt=(
          "You are a SQL query assistant that helps users "
          "write queries against business databases."
      ),
      middleware=[SkillMiddleware()],
      checkpointer=InMemorySaver(),
  )

# Example usage
  if __name__ == "__main__":
      # Configuration for this conversation thread
      thread_id = str(uuid.uuid4())
      config = {"configurable": {"thread_id": thread_id}}

# Ask for a SQL query
      result = agent.invoke(
          {
              "messages": [
                  {
                      "role": "user",
                      "content": (
                          "Write a SQL query to find all customers "
                          "who made orders over $1000 in the last month"
                      ),
                  }
              ]
          },
          config
      )

# Print the conversation
      for message in result["messages"]:
          if hasattr(message, 'pretty_print'):
              message.pretty_print()
          else:
              print(f"{message.type}: {message.content}")
  ```

This complete example includes:

* Skill definitions with full database schemas
  * The `load_skill` tool for on-demand loading
  * `SkillMiddleware` that injects skill descriptions into the system prompt
  * Agent creation with middleware and checkpointer
  * Example usage showing how the agent loads skills and writes SQL queries

To run this, you'll need to:

1. Install required packages: `pip install langchain langchain-openai langgraph`
  2. Set your API key (e.g., `export OPENAI_API_KEY=...`)
  3. Replace the model initialization with your preferred LLM provider
</Accordion>

## Implementation variations

<Accordion title="View implementation options and trade-offs">
  This tutorial implemented skills as in-memory Python dictionaries loaded through tool calls. However, there are several ways to implement progressive disclosure with skills:

**Storage backends:**

* **In-memory** (this tutorial): Skills defined as Python data structures, fast access, no I/O overhead
  * **File system** (Claude Code approach): Skills as directories with files, discovered via file operations like `read_file`
  * **Remote storage**: Skills in S3, databases, Notion, or APIs, fetched on-demand

**Skill discovery** (how the agent learns which skills exist):

* **System prompt listing**: Skill descriptions in system prompt (used in this tutorial)
  * **File-based**: Discover skills by scanning directories (Claude Code approach)
  * **Registry-based**: Query a skill registry service or API for available skills
  * **Dynamic lookup**: List available skills via a tool call

**Progressive disclosure strategies** (how skill content is loaded):

* **Single load**: Load entire skill content in one tool call (used in this tutorial)
  * **Paginated**: Load skill content in multiple pages/chunks for large skills
  * **Search-based**: Search within a specific skill's content for relevant sections (e.g., using grep/read operations on skill files)
  * **Hierarchical**: Load skill overview first, then drill into specific subsections

**Size considerations** (uncalibrated mental model - optimize for your system):

* **Small skills** (\< 1K tokens / \~750 words): Can be included directly in system prompt and cached with prompt caching for cost savings and faster responses
  * **Medium skills** (1-10K tokens / \~750-7.5K words): Benefit from on-demand loading to avoid context overhead (this tutorial)
  * **Large skills** (> 10K tokens / \~7.5K words, or > 5-10% of context window): Should use progressive disclosure techniques like pagination, search-based loading, or hierarchical exploration to avoid consuming excessive context

The choice depends on your requirements: in-memory is fastest but requires redeployment for skill updates, while file-based or remote storage enables dynamic skill management without code changes.
</Accordion>

## Progressive disclosure and context engineering

<Accordion title="Combining with few-shot prompting and other techniques">
  Progressive disclosure is fundamentally a **[context engineering](/oss/python/langchain/context-engineering) technique** - you're managing what information is available to the agent and when. This tutorial focused on loading database schemas, but the same principles apply to other types of context.

### Combining with few-shot prompting

For the SQL query use case, you could extend progressive disclosure to dynamically load **few-shot examples** that match the user's query:

**Example approach:**

1. User asks: "Find customers who haven't ordered in 6 months"
  2. Agent loads `sales_analytics` schema (as shown in this tutorial)
  3. Agent also loads 2-3 relevant example queries (via semantic search or tag-based lookup):
     * Query for finding inactive customers
     * Query with date-based filtering
     * Query joining customers and orders tables
  4. Agent writes query using both schema knowledge AND example patterns

This combination of progressive disclosure (loading schemas on-demand) and dynamic few-shot prompting (loading relevant examples) creates a powerful context engineering pattern that scales to large knowledge bases while providing high-quality, grounded outputs.
</Accordion>

* Learn about [middleware](/oss/python/langchain/middleware) for more dynamic agent behaviors
* Explore [context engineering](/oss/python/langchain/context-engineering) techniques for managing agent context
* Explore the [handoffs pattern](/oss/python/langchain/multi-agent/handoffs-customer-support) for sequential workflows
* Read the [subagents pattern](/oss/python/langchain/multi-agent/subagents-personal-assistant) for parallel task routing
* See [multi-agent patterns](/oss/python/langchain/multi-agent) for other approaches to specialized agents
* Use [LangSmith](https://smith.langchain.com) to debug and monitor skill loading

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent/skills-sql-assistant.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The agent saw the lightweight skill description in its system prompt, recognized the question required sales database knowledge, called `load_skill("sales_analytics")` to get the full schema and business logic, and then used that information to write a correct query following the database conventions.

## 6. Advanced: Add constraints with custom state

<Accordion title="Optional: Track loaded skills and enforce tool constraints">
  You can add constraints to enforce that certain tools are only available after specific skills have been loaded. This requires tracking which skills have been loaded in custom agent state.

  ### Define custom state

  First, extend the agent state to track loaded skills:
```

Example 2 (unknown):
```unknown
### Update load\_skill to modify state

  Modify the `load_skill` tool to update state when a skill is loaded:
```

Example 3 (unknown):
```unknown
### Create constrained tool

  Create a tool that's only usable after a specific skill has been loaded:
```

Example 4 (unknown):
```unknown
### Update middleware and agent

  Update the middleware to use the custom state schema:
```

---

## Scalability & resilience

**URL:** llms-txt#scalability-&-resilience

**Contents:**
- Server scalability
- Queue scalability
- Resilience
- Postgres resilience
- Redis resilience

Source: https://docs.langchain.com/langsmith/scalability-and-resilience

LangSmith is designed to scale horizontally with your workload. Each instance of the service is stateless, and keeps no resources in memory. The service is designed to gracefully handle new instances being added or removed, including hard shutdown cases.

## Server scalability

As you add more instances to a service, they will share the HTTP load as long as an appropriate load balancer mechanism is placed in front of them. In most deployment modalities we configure a load balancer for the service automatically. In the “self-hosted without control plane” modality it’s your responsibility to add a load balancer. Since the instances are stateless any load balancing strategy will work, no session stickiness is needed, or recommended. Any instance of the server can communicate with any queue instance (through Redis PubSub), meaning that requests to cancel or stream an in-progress run can be handled by any arbitrary instance.

As you add more instances to a service, they will increase run throughput linearly, as each instance is configured to handle a set number of concurrent runs (by default 10). Each attempt for each run will be handled by a single instance, with exactly-once semantics enforced through Postgres’s MVCC model (refer to section below for crash resilience details). Attempts that fail due to transient database errors are retried up to 3 times. We do not make use of long-lived transactions or locks, this enables us to make more efficient use of Postgres resources.

While a run is being handled by a queue instance, a periodic heartbeat timestamp will be recorded in Redis by that queue worker.

When a graceful shutdown request is received (SIGINT) an instance enters shutdown mode, which

* stops accepting new HTTP requests
* gives any in-progress runs a limited number of seconds to finish (if not finished it will be put back in the queue)
* stops the instance from picking up more runs from the queue

If a hard shutdown occurs due to a server crash or an infrastructure failure, any runs that were in progress will be picked up by an internal sweeper task that looks for in-progress runs that have breached their heartbeat window. The sweeper runs every 2 minutes and will put the runs back in the queue for another instance to pick them up.

## Postgres resilience

For deployment modalities where we manage the Postgres database, we have periodic backups and continuously replicated standby replicas for automatic failover. This Postgres configuration is available in the [Cloud deployment option](/langsmith/cloud) for [`Production` deployment types](/langsmith/control-plane#deployment-types) only.

All communication with Postgres implements retries for retry-able errors. If Postgres is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Postgres will render the Agent Server unavailable.

All data that requires durable storage is stored in Postgres, not Redis. Redis is used only for ephemeral metadata, and communication between instances. Therefore we place no durability requirements on Redis.

All communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the Agent Server unavailable.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/scalability-and-resilience.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Schema for routing user intent.

**URL:** llms-txt#schema-for-routing-user-intent.

---

## Schema for structured output

**URL:** llms-txt#schema-for-structured-output

from pydantic import BaseModel, Field

class SearchQuery(BaseModel):
    search_query: str = Field(None, description="Query that is optimized web search.")
    justification: str = Field(
        None, description="Why this query is relevant to the user's request."
    )

---

## Search Assistants

**URL:** llms-txt#search-assistants

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/search-assistants

langsmith/agent-server-openapi.json post /assistants/search
Search for assistants.

This endpoint also functions as the endpoint to list all assistants.

---

## Search Crons

**URL:** llms-txt#search-crons

Source: https://docs.langchain.com/langsmith/agent-server-api/crons-plus-tier/search-crons

langsmith/agent-server-openapi.json post /runs/crons/search
Search all active crons

---

## Search for items within a namespace prefix.

**URL:** llms-txt#search-for-items-within-a-namespace-prefix.

Source: https://docs.langchain.com/langsmith/agent-server-api/store/search-for-items-within-a-namespace-prefix

langsmith/agent-server-openapi.json post /store/items/search

---

## Search Threads

**URL:** llms-txt#search-threads

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/search-threads

langsmith/agent-server-openapi.json post /threads/search
Search for threads.

This endpoint also functions as the endpoint to list all threads.

---

## Second session: get user info

**URL:** llms-txt#second-session:-get-user-info

agent.invoke({
    "messages": [{"role": "user", "content": "Get user info for user with id 'abc123'"}]
})

---

## Section 1: Prometheus Exporters

**URL:** llms-txt#section-1:-prometheus-exporters

Use this section if you would like to only deploy metrics exporters for the components in your self hosted deployment, which you can then scrape using your telemetry. If you would like a full observability stack deployed for you, go to the [End-to-End Deployment Section](/langsmith/observability-stack#prerequisites).

The helm chart provides a set of Prometheus exporters to expose metrics from [Redis](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-redis-exporter), [Postgres](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-postgres-exporter), [Nginx](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-nginx-exporter), and [Kube state metrics](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics).

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/metric-exporters-only.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

This will allow you to scrape metrics at the following service endpoints:

* Postgres: `langsmith-observability-postgres-exporter:9187/metrics`
* Redis: `langsmith-observability-redis-exporter:9121/metrics`
* Nginx: `langsmith-observability-nginx-exporter:9113/metrics`
* KubeStateMetrics: `langsmith-observability-kube-state-metrics:8080/metrics`

You should see the following if the installation went through:

And if you run `kubectl get pods -n langsmith-observability`, you should see:

**Examples:**

Example 1 (unknown):
```unknown
And if you run `kubectl get pods -n langsmith-observability`, you should see:
```

---

## Section 2: Full Observability Stack

**URL:** llms-txt#section-2:-full-observability-stack

**Contents:**
- Prerequisites
  - 1. Compute Resources
  - 2. Cert-Manager
  - 3. OpenTelemetry Operator
- Installation
- Post-Installation
  - Enable Logs and Traces in LangSmith
- Grafana Usage

<Warning>
  **This is not a production observability stack. Use this to gain quick insight into logs, metrics and traces for your deployment. This is only made to handle a few dozen GB of data per day.**
</Warning>

This section will show you how to deploy the end-to-end observability stack for LangSmith, using the [Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith-observability).

This chart is built around the open-source LGTM Stack from Grafana. It consists of:

* [Loki](https://grafana.com/docs/loki/latest/) for logs.
* [Mimir](https://grafana.com/docs/mimir/latest/) for metrics + alerting.
* [Tempo](https://grafana.com/docs/tempo/latest/) for traces.
* [Grafana](https://grafana.com/docs/grafana/latest/) for monitoring UI.

As well as [OpenTelemetry Collectors](https://opentelemetry.io/docs/collector/) for gathering the telemetry data.

### 1. Compute Resources

The resource requests and limits for each part of the stack can be modified in the helm chart. Here are the current allocations (request/limit):

* Loki: `2vCPU/3vCPU + 2Gi/4Gi`
* Mimir: `1vCPU/2vCPU + 2Gi/4Gi`
* Tempo: `1vCPU/2vCPU + 4Gi/6Gi`

Make sure you have those resources allocated before bringing up the helm chart, or modify the resource values in your helm configuration file.

The helm chart uses the OpenTelemetry Operator to provision collectors. The operator require that you have [cert-manager](https://cert-manager.io/docs/installation/) installed in your Kubernetes cluster.

If you do not have it installed, you can run the following commands:

### 3. OpenTelemetry Operator

Use the following to install the OpenTelemetry Operator:

The following instructions will bring up OTel collectors, the LGTM stack, Grafana and Prometheus exporters.

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/e2e-stack.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

<Note>
  **You can selectively collect logs, metrics or traces by modifying the boolean values under `otelCollector` in your config file. You can also selectively bring up each respective piece of the backend (Loki, Mimir, Tempo).**
</Note>

You should see the following if the install went through:

And if you run `kubectl get pods -n langsmith-observability`, you should see:

### Enable Logs and Traces in LangSmith

Once you have installed the observability helm chart, you need to set the following values in your *LangSmith* helm configuration file to enable collection of logs and traces.

<Info>
  1. To get `${LANGSMITH_OTEL_CRD_NAME}`, you can run `kubectl get opentelemetrycollectors -n ${LANGSMITH_OBS_NAMESPACE}` and select the name of the one with MODE = `sidecar`
  2. To get `${GATEWAY_COLLECTOR_SERVICE_NAME}` name, run `kubectl get services -n ${LANGSMITH_OBS_NAMESPACE}` and select the one with Ports 4317/4318 AND a ClusterIP set. It should be something like `langsmith-observability-collector-gateway-collector`
</Info>

Now run `helm upgrade langsmith langchain/langsmith --values langsmith_config.yaml -n <langsmith-namespace> --wait --debug`

Once upgraded, if you run `kubectl get pods -n <langsmith-namespace>` you should see the following (note the 2/2 for sidecar collectors):

Once everything is installed, do the following: to get your Grafana password:

Then port-forward into the `langsmith-observability-grafana` container at port 3000, and open your browser as `localhost:3000`. Use the username `admin` and the password from the secret above to log into Grafana.

Once in Grafana, you can use the UI to monitor logs, metrics and traces. Grafana also comes pre-packaged with sets of dashboards for monitoring the main components of your deployment.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ee47243826737bab23944e01536dec71" alt="LangSmith Grafana Dashboards" data-og-width="1715" width="1715" data-og-height="1073" height="1073" data-path="langsmith/images/langsmith-grafana-dashboards.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=11e0d71897053d012e929ca49533d6dd 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=0e674be629c1c1e795e1b6e686ca28e2 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fb3bcdc4fa39e82fe9f3370834c7f7fa 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=80c401f8794ac0671207e1a260aac25b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3f5baa041ab9e5c39b7d8a62e2bfe1e5 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-grafana-dashboards.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7620f91021593db1d87c92fc43d8fe2b 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-stack.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### 3. OpenTelemetry Operator

Use the following to install the OpenTelemetry Operator:
```

Example 2 (unknown):
```unknown
## Installation

The following instructions will bring up OTel collectors, the LGTM stack, Grafana and Prometheus exporters.

1. Create a local file called `langsmith_obs_config.yaml`
2. Copy over the values from this [file](https://github.com/langchain-ai/helm/blob/main/charts/langsmith-observability/examples/e2e-stack.yaml) into `langsmith_obs_config.yaml`, making sure to modify the values to match your LangSmith deployment.
3. Find the latest version of the chart by running `helm search repo langchain/langsmith-observability --versions`.
4. Grab the latest version number, and run `helm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug`

<Note>
  **You can selectively collect logs, metrics or traces by modifying the boolean values under `otelCollector` in your config file. You can also selectively bring up each respective piece of the backend (Loki, Mimir, Tempo).**
</Note>

You should see the following if the install went through:
```

Example 3 (unknown):
```unknown
And if you run `kubectl get pods -n langsmith-observability`, you should see:
```

Example 4 (unknown):
```unknown
## Post-Installation

### Enable Logs and Traces in LangSmith

Once you have installed the observability helm chart, you need to set the following values in your *LangSmith* helm configuration file to enable collection of logs and traces.
```

---

## Security policy

**URL:** llms-txt#security-policy

**Contents:**
- Best practices
- Reporting OSS vulnerabilities
  - Bug bounty eligibility
  - Out-of-scope targets
- Reporting LangSmith Vulnerabilities
  - Other Security Concerns

Source: https://docs.langchain.com/oss/python/security-policy

LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.

When building such applications developers should remember to follow good security practices:

* [**Limit permissions**](https://en.wikipedia.org/wiki/Principle_of_least_privilege): Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc. as appropriate for your application.
* **Anticipate potential misuse**: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it's safest to assume that any LLM able to use those credentials may in fact delete data.
* [**Defense in depth**](https://en.wikipedia.org/wiki/Defense_in_depth_\(computing\)): No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It's best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.

Risks of not doing so include, but are not limited to:

* Data corruption or loss.
* Unauthorized access to confidential information.
* Compromised performance or availability of critical resources.

Example scenarios with mitigation strategies:

* A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container.
* A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse.
* A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.

If you're building applications that access external resources like file systems, APIs
or databases, consider speaking with your company's security team to determine how to best
design and secure your applications.

## Reporting OSS vulnerabilities

Please report security vulnerabilities associated with the LangChain open source projects using the following process:

1. **Submit a security advisory** on the Security tab in the GitHubrepository where the vulnerability exists.
2. **Send an email** to `security@langchain.dev` notifying us that you've filed a security issue and which repository it was filed in.

Before reporting a vulnerability, please review the [Best Practices](#best-practices) above to understand what we consider to be a security vulnerability vs. developer responsibility.

### Bug bounty eligibility

We welcome security vulnerability reports for all LangChain libraries. However, we may offer ad hoc bug bounties only for vulnerabilities in the following packages:

* Core libraries owned and maintained by the LangChain team: `langchain-core`, `langchain` (v1), `langgraph`, and related checkpointer packages (or their JavaScript equivalents)
* Popular integrations maintained by the LangChain team (e.g., `langchain-openai`, `langchain-anthropic`, etc., or their JavaScript equivalents)

The vulnerability must be in the library code itself, not in example code or example applications.

We welcome reports for all other LangChain packages and will address valid security concerns, but bug bounties will not be awarded for packages outside this scope. This includes `langchain-community`, which due to its community-driven nature is not eligible for bug bounties, though we will accept and address reports.

### Out-of-scope targets

The following are out-of-scope for security vulnerability reports:

* **langchain-experimental**: This repository is for experimental code and is not in scope for security reports (see [package warning](https://pypi.org/project/langchain-experimental/)).
* **Examples and example applications**: Example code and demo applications are not in scope for security reports.
* **Code documented with security notices**: This will be decided on a case-by-case basis, but likely will not be in scope as the code is already documented with guidelines for developers that should be followed for making their application secure.
* **LangSmith related repositories or APIs**: See [Reporting LangSmith Vulnerabilities](#reporting-langsmith-vulnerabilities) below.

## Reporting LangSmith Vulnerabilities

Please report security vulnerabilities associated with LangSmith by email to `security@langchain.dev`.

* LangSmith site: [https://smith.langchain.com](https://smith.langchain.com)
* SDK client: [https://github.com/langchain-ai/langsmith-sdk](https://github.com/langchain-ai/langsmith-sdk)

### Other Security Concerns

For any other security concerns, please contact us at `security@langchain.dev`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/security-policy.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## See trace: https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r

**URL:** llms-txt#see-trace:-https://smith.langchain.com/public/882f9ecf-5057-426a-ae98-0edf84fdcaf9/r

**Contents:**
- Ensure all traces are submitted before exiting
  - Using the LangSmith SDK
  - Using LangChain

MyClass(13).combine(29)
python Python theme={null}
  from langsmith import Client

@traceable(client=client)
  async def my_traced_func():
    # Your code here...
    pass

try:
    await my_traced_func()
  finally:
    await client.flush()
  typescript TypeScript theme={null}
  import { Client } from "langsmith";

const langsmithClient = new Client({});

const myTracedFunc = traceable(async () => {
    // Your code here...
  },{ client: langsmithClient });

try {
    await myTracedFunc();
  } finally {
    await langsmithClient.flush();
  }
  ```
</CodeGroup>

If you are using LangChain, please refer to our [LangChain tracing guide](/langsmith/trace-with-langchain#ensure-all-traces-are-submitted-before-exiting).

If you prefer a video tutorial, check out the [Tracing Basics video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotate-code.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Ensure all traces are submitted before exiting

LangSmith's tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. Here are some options for ensuring all traces are submitted before exiting your application.

### Using the LangSmith SDK

If you are using the LangSmith SDK standalone, you can use the `flush` method before exit:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Self-hosted LangSmith

**URL:** llms-txt#self-hosted-langsmith

**Contents:**
- Self-host LangSmith Observability and Evaluation
  - Services
  - Storage services
  - Setup methods
  - Setup guides
- Enable LangSmith Deployment
  - Workflow
- Standalone Server
  - Workflow
  - Supported compute platforms

Source: https://docs.langchain.com/langsmith/self-hosted

<Note>
  **Important**<br />
  Self-hosted LangSmith is an add-on to the Enterprise plan designed for our largest, most security-conscious customers. For more details, refer to [Pricing](https://www.langchain.com/pricing). [Contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Note>

LangSmith supports different self-hosted configurations depending on your scale, security, and infrastructure needs.

You can use LangSmith for [observability](/langsmith/observability) and [evaluation](/langsmith/evaluation) without agent deployment. Or, you can set up the **full self-hosted platform** for observability, evaluation, and [agent deployment](/langsmith/deployments). Alternatively, you can deploy agents directly without the [control plane](/langsmith/control-plane).

This page provides an overview of each self-hosted model:

<Columns cols={1}>
  <Card title="LangSmith Observability and Evaluation" icon="chart-line" href="#langsmith">
    Host an instance of LangSmith that includes observability, tracing, and evaluations in the UI and API. Best for teams who want self-hosted monitoring and evaluation without deploying agents.
  </Card>

<Card title="LangSmith Observability, Evaluation, and Deployment" icon="layer-group" href="#enable-langsmith-deployment">
    Enables deploying graphs to Agent Server via the control plane. The control plane and data plane provide the full LangSmith platform for running and monitoring agents. This includes observability, evaluation, and deployment.
  </Card>

<Card title="Standalone server" icon="server" href="#standalone-server">
    Host an Agent Server directly without the control plane UI. A lightweight option for running one or a few agents as independent services, with full control over scaling and integration.
  </Card>
</Columns>

| Model                                      | Includes                                                                                                                                                                                                                | Best for                                                                                                                                                                               | Methods                                                                                                                                               |
| ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Observability & Evaluation**             | <ul><li>LangSmith (UI + API)</li><li>Backend services (queue, playground, ACE)</li><li>Datastores: PostgreSQL, Redis, ClickHouse, optional blob storage</li></ul>                                                       | <ul><li>Teams who need self-hosted observability, tracing, and evaluation</li><li>Running LangSmith without deploying agents/graphs</li></ul>                                          | <ul><li>Docker Compose (dev/test)</li><li>Kubernetes + Helm (production)</li></ul>                                                                    |
| **Observability, Evaluation & Deployment** | <ul><li>Everything from Observability and Evaluation</li><li>Control plane (deployments UI, revision management, Studio)</li><li>Data plane (Agent Server pods)</li><li>Kubernetes operator for orchestration</li></ul> | <ul><li>Enterprise teams needing a private LangChain Cloud</li><li>Centralized UI/API for managing multiple agents/graphs</li><li>Integrated observability and orchestration</li></ul> | <ul><li>Kubernetes with Helm (required)</li><li>Runs on EKS, GKE, AKS, or self-managed clusters</li></ul>                                             |
| **Standalone server**                      | <ul><li>Agent Server container(s)</li><li>Requires PostgreSQL + Redis (shared or dedicated)</li><li>Optional LangSmith integration for tracing</li></ul>                                                                | <ul><li>Lightweight deployments of one or a few agents</li><li>Integrating Agent Servers as microservices</li><li>Teams preferring to manage scaling & CI/CD themselves</li></ul>      | <ul><li>Docker / Docker Compose (dev/test)</li><li>Kubernetes + Helm (production)</li><li>Any container runtime or VM (ECS, EC2, ACI, etc.)</li></ul> |

<Note>
  For setup guides, refer to:

* [Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform)
  * [Deploy Standalone Server](/langsmith/deploy-standalone-server)

Supported compute platforms: [Kubernetes](https://kubernetes.io/) (for LangSmith Deployment), any compute platform (for Standalone Server)
</Note>

## Self-host LangSmith Observability and Evaluation

Host an instance of LangSmith that includes observability, tracing, and evaluations in the UI and API, but **without** the ability to deploy agents through the control plane.

* LangSmith frontend UI
* LangSmith backend API
* LangSmith Platform backend
* LangSmith Playground
* LangSmith queue
* LangSmith ACE (Arbitrary Code Execution) backend

**Storage services:**

* ClickHouse (traces and feedback data)
* PostgreSQL (operational data)
* Redis (queuing and caching)
* Blob storage (optional, but recommended for production)

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=0790cbdf4fe131c74d1e60bb120834e3" alt="LangSmith architecture showing services and datastores" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=c04d8a044d221559fe2f7b9121275638 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a15351b254f11cc149ce237ba8853e91 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=d4a409e73830e588519cb1d0b2a17f3b 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=6dbeda77b57083efb988e15af38f0a6e 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=24aadbe2e79db02d76fd5deaea6564e1 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-light.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=a126aa1f02d36de0a8e391f0e1059b8e 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=767f3bc3dc73ffe1a806f54e0aaa428b" alt="LangSmith architecture showing services and datastores" data-og-width="2210" width="2210" data-og-height="1463" height="1463" data-path="langsmith/images/cloud-arch-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=280&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=f7367df5b782c821882605418c50563f 280w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=560&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=60759ef9e927ba0985e21e38acacae6d 560w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=840&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=383ac38ba52733548d8d97ffabfe384e 840w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1100&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=b045b8e19a9926d4d10ec8ad2d2767c1 1100w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=1650&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=23778aa891c1b42336b274ab1b2f8bec 1650w, https://mintcdn.com/langchain-5e9cc07a/rqYqeBEA_2oeiw17/langsmith/images/cloud-arch-dark.png?w=2500&fit=max&auto=format&n=rqYqeBEA_2oeiw17&q=85&s=5a64734b4e9fb5dd4af690edf3fa6248 2500w" />

To access the LangSmith UI and send API requests, you will need to expose the [LangSmith frontend](#langsmith-frontend) service. Depending on your installation method, this can be a load balancer or a port exposed on the host machine.

| Service                                                                                                        | Description                                                                                                                                                                                                                                                                                                                                              |
| -------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| <a id="langsmith-frontend" /> **LangSmith frontend**                                                           | The frontend uses Nginx to serve the LangSmith UI and route API requests to the other servers. This serves as the entrypoint for the application and is the only component that must be exposed to users.                                                                                                                                                |
| <a id="langsmith-backend" /> **LangSmith backend**                                                             | The backend is the main entrypoint for CRUD API requests and handles the majority of the business logic for the application. This includes handling requests from the frontend and SDK, preparing traces for ingestion, and supporting the hub API.                                                                                                      |
| <a id="langsmith-queue" /> **LangSmith queue**                                                                 | The queue handles incoming traces and feedback to ensure that they are ingested and persisted into the traces and feedback datastore asynchronously, handling checks for data integrity and ensuring successful insert into the datastore, handling retries in situations such as database errors or the temporary inability to connect to the database. |
| <a id="langsmith-platform-backend" /> **LangSmith platform backend**                                           | The platform backend is another critical service that primarily handles authentication, run ingestion, and other high-volume tasks.                                                                                                                                                                                                                      |
| <a id="langsmith-playground" /> **LangSmith playground**                                                       | The playground is a service that handles forwarding requests to various LLM APIs to support the LangSmith Playground feature. This can also be used to connect to your own custom model servers.                                                                                                                                                         |
| <a id="langsmith-ace-arbitrary-code-execution-backend" /> **LangSmith ACE (Arbitrary Code Execution) backend** | The ACE backend is a service that handles executing arbitrary code in a secure environment. This is used to support running custom code within LangSmith.                                                                                                                                                                                                |

<Note>
  LangSmith will bundle all storage services by default. You can configure it to use external versions of all storage services. In a production setting, we **strongly recommend using external storage services**.
</Note>

| Service                                  | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| <a id="clickhouse" /> **ClickHouse**     | [ClickHouse](https://clickhouse.com/docs/en/intro) is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP).<br /><br />LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data).                                                                                                                                                                                      |
| <a id="postgresql" /> **PostgreSQL**     | [PostgreSQL](https://www.postgresql.org/about/) is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads.<br /><br />LangSmith uses PostgreSQL as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback).                                                        |
| <a id="redis" /> **Redis**               | [Redis](https://github.com/redis/redis) is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching.<br /><br />LangSmith uses Redis to back queuing and caching operations.                                                                                                                                                                                                  |
| <a id="blob-storage" /> **Blob storage** | LangSmith supports several blob storage providers, including [AWS S3](https://aws.amazon.com/s3/), [Azure Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/), and [Google Cloud Storage](https://cloud.google.com/storage).<br /><br />LangSmith uses blob storage to store large files, such as trace artifacts, feedback attachments, and other large data objects. Blob storage is optional, but highly recommended for production deployments. |

* **Docker Compose** (development/testing only)
* **Kubernetes + Helm** (recommended for production)

* [Install on Kubernetes](/langsmith/kubernetes) (production)
* [Install with Docker](/langsmith/docker) (development only)

## Enable LangSmith Deployment

**LangSmith Deployment** is an optional add-on that can be enabled on your [LangSmith](#langsmith) instance. It's ideal for enterprise teams who want a centralized, UI-driven platform to deploy and manage multiple agents and graphs, with all infrastructure, data, and orchestration fully under their control.

This includes everything from [LangSmith](#langsmith), plus:

| Component                                                                                                | Responsibilities                                                                                                                                    | Where it runs | Who manages it |
| -------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- | -------------- |
| <Tooltip tip="The LangSmith UI and APIs for managing deployments.">Control plane</Tooltip>               | <ul><li>UI for creating deployments & revisions</li><li>APIs for deployment management</li></ul>                                                    | Your cloud    | You            |
| <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">Data plane</Tooltip> | <ul><li>Operator/listener to reconcile deployments</li><li>Agent Servers (agents/graphs)</li><li>Backing services (Postgres, Redis, etc.)</li></ul> | Your cloud    | You            |

You run both the control plane and the data plane entirely within your own infrastructure. You are responsible for provisioning and managing all components.

<Note>
  Learn more about the [control plane](/langsmith/control-plane) and [data plane](/langsmith/data-plane) architecture concepts.
</Note>

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=78861d689cf446190d0c1e80ed1a65dd" alt="Full platform architecture with control plane and data plane" data-og-width="2138" width="2138" data-og-height="2104" height="2104" data-path="langsmith/images/full-platform-with-deployment-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=28f2db3fb8d323baa1ed925fa39a0f82 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=724b17ae55c6fc19c8ced2d8e081bcb3 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=46d965e47f66ece91077b1dfdf56196b 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=d723ddff8b6942e8fcf15fe5523102b0 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=06932f510e15f96376c67c790dcb31e3 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-light.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=c482dbbe51709c394194bb3ea7842c09 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=f5253bab42b0b351e1d13489fea4311b" alt="Full platform architecture with control plane and data plane" data-og-width="2138" width="2138" data-og-height="2104" height="2104" data-path="langsmith/images/full-platform-with-deployment-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=280&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ba56323cd1e66109847f8adea173063c 280w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=560&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=6e529069bd427fde7c940ab944f0a1ba 560w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=840&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ee4ed77d09b804dc2de32af0ddedbfbe 840w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=1100&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=ce52832ec7c18f9dc374546651545c42 1100w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=1650&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=8c8781ac82576d68faa8fc11c4b47a83 1650w, https://mintcdn.com/langchain-5e9cc07a/JOyLr_spVEW0t2KF/langsmith/images/full-platform-with-deployment-dark.png?w=2500&fit=max&auto=format&n=JOyLr_spVEW0t2KF&q=85&s=a0b4f5a8a31a23136d03160744305700 2500w" />

If you want to self-host LangSmith for observability, evaluation, and agent deployment, follow these steps:

<Steps>
  <Step title="Install self-hosted LangSmith">
    You must already have a [self-hosted LangSmith instance](#langsmith) installed in your cloud with a Kubernetes cluster (required for control plane and data plane).
  </Step>

<Step title="Test your graph locally">
    Use `langgraph-cli` or [Studio](/langsmith/studio) to test your graph locally.
  </Step>

<Step title="Enable LangSmith Deployment">
    Follow the [setup guide](/langsmith/deploy-self-hosted-full-platform) to enable LangSmith Deployment on your LangSmith instance.
  </Step>
</Steps>

The **Standalone server** option is the most lightweight and flexible way to run LangSmith. Unlike the other models, you only manage a simplified <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">data plane</Tooltip> made up of Agent Servers and their required backing services (PostgreSQL, Redis, etc.).

| Component         | Responsibilities                                              | Where it runs | Who manages it |
| ----------------- | ------------------------------------------------------------- | ------------- | -------------- |
| **Control plane** | n/a                                                           | n/a           | n/a            |
| **Data plane**    | <ul><li>Agent Servers</li><li>Postgres, Redis, etc.</li></ul> | Your cloud    | You            |

This option gives you full control over scaling, deployment, and CI/CD pipelines, while still allowing optional integration with LangSmith for tracing and evaluation.

<Warning>
  Do not run standalone servers in serverless environments. Scale-to-zero may cause task loss and scaling up will not work reliably.
</Warning>

<img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=db67e2add4cf039b1ce2324fa1c1f244" alt="Standalone server architecture" data-og-width="752" width="752" data-og-height="821" height="821" data-path="langsmith/images/standalone-server-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=280&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=e28c7f91a45d2608e74febe474066b27 280w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=560&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=6956b33b01bd4fa69269d890d09fa571 560w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=840&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=9e4727b09b6c88780787c6d6ff7bd490 840w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=1100&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=e380af553cd4dd0ddec93a44aee6ce09 1100w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=1650&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=5a0da7b4a6b3da6b5ac6fe804fe219b4 1650w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-light.png?w=2500&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=11f1fbd4fdbb59dd77ef3ebde76422f3 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=57ede6682332db867f1900200f675a5f" alt="Standalone server architecture" data-og-width="752" width="752" data-og-height="821" height="821" data-path="langsmith/images/standalone-server-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=280&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=d38e2ee8deee27d3bbd2d465854bc850 280w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=560&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=19f7f77f74f6c11c1b40c0169796cbe3 560w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=840&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=68547f892a250fc426ce9f0dad79a80e 840w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=1100&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=66b5b9d43d2d08cae16755d9f510d418 1100w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=1650&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=1c045709ec08809225b084d8c850cdc2 1650w, https://mintcdn.com/langchain-5e9cc07a/Mwtbhvs2R50foe4Y/langsmith/images/standalone-server-dark.png?w=2500&fit=max&auto=format&n=Mwtbhvs2R50foe4Y&q=85&s=49c858d75788a600cdc558f78fa2dc41 2500w" />

1. Define and test your graph locally using the `langgraph-cli` or [Studio](/langsmith/studio)
2. Package your agent as a Docker image
3. Deploy the Agent Server to your compute platform of choice (Kubernetes, Docker, VM)
4. Optionally, configure LangSmith API keys and endpoints so the server reports traces and evaluations back to LangSmith (self-hosted or SaaS)

### Supported compute platforms

* **Kubernetes**: Use the LangSmith Helm chart to run Agent Servers in a Kubernetes cluster. This is the recommended option for production-grade deployments.

* **Docker**: Run in any Docker-supported compute platform (local dev machine, VM, ECS, etc.). This is best suited for development or small-scale workloads.

<Tip>
  To set up an [Agent Server](/langsmith/agent-server), refer to the [how-to guide](/langsmith/deploy-standalone-server) in the application deployment section.
</Tip>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-hosted.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Self-hosted LangSmith changelog

**URL:** llms-txt#self-hosted-langsmith-changelog

Source: https://docs.langchain.com/langsmith/self-hosted-changelog

<Callout icon="rss" color="#DFC5FE" iconType="regular">
  **Subscribe**: Our changelog includes an [RSS feed](https://docs.langchain.com/langsmith/self-hosted-changelog/rss.xml) that can integrate with [Slack](https://slack.com/help/articles/218688467-Add-RSS-feeds-to-Slack), [email](https://zapier.com/apps/email/integrations/rss/1441/send-new-rss-feed-entries-via-email), Discord bots like [Readybot](https://readybot.io/) or [RSS Feeds to Discord Bot](https://rss.app/en/bots/rssfeeds-discord-bot), and other subscription tools.
</Callout>

[Self-hosted LangSmith](/langsmith/self-hosted) is an add-on to the Enterprise plan designed for our largest, most security-conscious customers. For more details, refer to [Pricing](https://www.langchain.com/pricing). [Contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.

<Update label="2025-12-12" tags={["self-hosted"]}>
  ## langsmith-0.12.32

* Added IAM connection support for PostgreSQL (AWS only).
  * Added GPT-5.2 model support to the playground.
  * Added support for setting memory limits on executor pods.

**Download the Helm chart:** [`langsmith-0.12.32.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.32/langsmith-0.12.32.tgz)
</Update>

<Update label="2025-12-11" tags={["self-hosted"]}>
  ## langsmith-0.12.31

* Improved error messages for basic authentication misconfiguration.
  * Added organization operator role support.
  * Fixed issues with streaming datasets endpoint.

**Download the Helm chart:** [`langsmith-0.12.31.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.31/langsmith-0.12.31.tgz)
</Update>

<Update label="2025-12-09" tags={["self-hosted"]}>
  ## langsmith-0.12.30

* Fixed API Docs button not redirecting to the correct URL when using a sub path.
  * Performance improvements and bug fixes.

**Download the Helm chart:** [`langsmith-0.12.30.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.30/langsmith-0.12.30.tgz)
</Update>

<Update label="2025-12-08" tags={["self-hosted"]}>
  ## langsmith-0.12.29

* Added mTLS (mutual TLS) support for ClickHouse connections to enhance security for database communication.

**Download the Helm chart:** [`langsmith-0.12.29.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.29/langsmith-0.12.29.tgz)
</Update>

<Update label="2025-12-05" tags={["self-hosted"]}>
  ## langsmith-0.12.28

* Added mTLS (mutual TLS) support for PostgreSQL connections to enhance security for database communication.
  * Added mTLS support for ClickHouse clients.
  * Fixed Agent Builder onboarding and side navigation visibility when disabled in self-hosted deployments.

**Download the Helm chart:** [`langsmith-0.12.28.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.28/langsmith-0.12.28.tgz)
</Update>

<Update label="2025-12-04" tags={["self-hosted"]}>
  ## langsmith-0.12.27

* Added mTLS (mutual TLS) support for Redis connections to enhance security.
  * Added support for empty trigger server configuration in self-hosted deployments.
  * Improved incident banner styling and content.

**Download the Helm chart:** [`langsmith-0.12.27.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.27/langsmith-0.12.27.tgz)
</Update>

<Update label="2025-12-01" tags={["self-hosted"]}>
  ## langsmith-0.12.25

* Enabled Agent Builder UI feature flag for self-hosted deployments.
  * Added Redis Cluster support for improved scalability and high availability.

**Download the Helm chart:** [`langsmith-0.12.25.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.25/langsmith-0.12.25.tgz)
</Update>

<Update label="2025-11-27" tags={["self-hosted"]}>
  ## langsmith-0.12.24

* Added dequeue timeouts to all SAQ (Simple Async Queue) queues to improve reliability.
  * Performance improvements and bug fixes.

**Download the Helm chart:** [`langsmith-0.12.24.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.24/langsmith-0.12.24.tgz)
</Update>

<Update label="2025-11-26" tags={["self-hosted"]}>
  ## langsmith-0.12.22

* Added Claude Opus 4.5 model support to the playground.
  * Updated dataplane operator version.
  * Added `LANGCHAIN_ENDPOINT` environment variable when basePath is configured.

**Download the Helm chart:** [`langsmith-0.12.22.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.22/langsmith-0.12.22.tgz)
</Update>

<Update label="2025-11-26" tags={["self-hosted"]}>
  ## langsmith-0.12.21

* Added explicit `revisionHistoryLimit` configuration for operator deployment template.

**Download the Helm chart:** [`langsmith-0.12.21.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.21/langsmith-0.12.21.tgz)
</Update>

<Update label="2025-11-24" tags={["self-hosted"]}>
  ## langsmith-0.12.20

* Added support for self-hosted customers to opt into the pairwise annotation queue feature.
  * Updated operator to version 0.1.21 in LangSmith and data plane charts.

**Download the Helm chart:** [`langsmith-0.12.20.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.20/langsmith-0.12.20.tgz)
</Update>

<Update label="2025-11-24" tags={["self-hosted"]}>
  ## langsmith-0.12.19

* Fixed playground environment configuration to use correct default settings.

**Download the Helm chart:** [`langsmith-0.12.19.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.19/langsmith-0.12.19.tgz)
</Update>

<Update label="2025-11-20" tags={["self-hosted"]}>
  ## langsmith-0.12.18

* Internal updates and maintenance.

**Download the Helm chart:** [`langsmith-0.12.18.tgz`](https://github.com/langchain-ai/helm/releases/download/langsmith-0.12.18/langsmith-0.12.18.tgz)
</Update>

<Note>
  Additional Helm chart releases are available in the [`langchain-ai/helm` GitHub repository](https://github.com/langchain-ai/helm/releases).
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-hosted-changelog.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Self-host LangSmith on Kubernetes

**URL:** llms-txt#self-host-langsmith-on-kubernetes

**Contents:**
- Prerequisites
  - Databases
  - Kubernetes cluster requirements
- Configure your Helm Charts:
- Deploying to Kubernetes:
- Validate your deployment:
- Using LangSmith

Source: https://docs.langchain.com/langsmith/kubernetes

<Info>
  Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and [contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Info>

This page describes how to set up **LangSmith** (observability, tracing, and evaluation) in a Kubernetes cluster. You'll use Helm to install LangSmith and its dependencies.

After completing this page, you'll have:

* **LangSmith UI and APIs**: for [observability](/langsmith/observability), tracing, and [evaluation](/langsmith/evaluation).
* **Backend services**: (queue, playground, ACE).
* **Datastores**: (PostgreSQL, Redis, ClickHouse, optional blob storage).

For [agent deployment](/langsmith/deployments): To add deployment capabilities, complete this guide first, then follow [Enable LangSmith Deployment](/langsmith/deploy-self-hosted-full-platform).

LangChain has successfully tested LangSmith on the following Kubernetes distributions:

* Google Kubernetes Engine (GKE)
* Amazon Elastic Kubernetes Service (EKS): For architecture patterns and best practices, refer to [self-hosting on AWS](/langsmith/aws-self-hosted).
* Azure Kubernetes Service (AKS): For architecture patterns and best practices, refer to [self-hosting on AWS](/langsmith/azure-self-hosted).
* OpenShift (4.14+)
* Minikube and Kind (for development purposes)

<Note>
  LangChain provides Terraform modules to help provision infrastructure for LangSmith. These modules can quickly set up Kubernetes clusters, storage, and networking for your deployment.

* [AWS Terraform modules](https://github.com/langchain-ai/terraform/tree/main/modules/aws)
  * [Azure Terraform modules](https://github.com/langchain-ai/terraform/tree/main/modules/azure)

View the [full Terraform repository](https://github.com/langchain-ai/terraform) for documentation and additional resources.
</Note>

Ensure you have the following tools/items ready. Some items are marked optional:

1. LangSmith License Key

1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

3. JWT Secret (Optional but used for basic auth)

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

LangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider’s managed services.

For more information, refer to the following setup guides for external services:

* [PostgreSQL](/langsmith/self-host-external-postgres)
* [Redis](/langsmith/self-host-external-redis)
* [ClickHouse](/langsmith/self-host-external-clickhouse)

### Kubernetes cluster requirements

1. You will need a working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:

1. Recommended: At least 16 vCPUs, 64GB Memory available

* You may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found [here](/langsmith/self-host-scale).
      * We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.
      * We recommend setting up the metrics server so that autoscaling can be turned on.
      * If you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory **allocatable** as ClickHouse will request this amount of resources by default.

2. Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)

* To enable persistence, we will try to provision volumes for any database running in-cluster.
      * If using PVs in your cluster, we highly recommend setting up backups in a production environment.
      * **We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.**
      * On EKS, you may need to ensure you have the `ebs-csi-driver` installed and configured for dynamic provisioning. Refer to the [EBS CSI Driver documentation](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) for more information.

You can verify this by running:

The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:

<Note>
        We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.
      </Note>

Refer to the [Kubernetes documentation](https://kubernetes.io/do/langsmith/observability-concepts/storage/storage-classes/) for more information on storage classes.

1. To install `helm` refer to the [Helm documentation](https://helm.sh/docs/intro/install/)

3. Egress to `https://beacon.langchain.com` (if not running in offline mode)

1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

## Configure your Helm Charts:

1. Create a new file called `langsmith_config.yaml` with the configuration options from the previous step.
   1. There are several configuration options that you can set in the `langsmith_config.yaml` file. You can find more information on specific configuration options in the [Configuration](/langsmith/self-hosted) section.
   2. If you are new to Kubernetes or Helm, we’d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: [LangSmith helm chart examples](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/examples).
   3. You can see a full list of configuration options in the `values.yaml` file in the Helm Chart repository here: [LangSmith Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/values.yaml)

<Warning>
  Only override the settings you need in `langsmith_config.yaml`; don’t copy the entire `values.yaml`.
  Keeping your config minimal ensures you continue to inherit new defaults and upgrades from the Helm chart.
</Warning>

2. At a minimum, you will need to set the following configuration options (using basic auth):

You will also need to specify connection details for any external databases you are using.

## Deploying to Kubernetes:

1. Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace)

1. Run `kubectl get pods`

Output should look something like:

<Note>
     If you are using a namespace other than the default namespace, you will need to specify the namespace in the `helm` and `kubectl` commands by using the `-n <namespace>` flag.
   </Note>

2. Ensure you have the LangChain Helm repo added. (skip this step if you are using local charts)

3. Find the latest version of the chart. You can find the available versions in the [Helm Chart repository](https://github.com/langchain-ai/helm/releases).

* We generally recommend using the latest version.
   * You can also run `helm search repo langchain/langsmith --versions` to see the available versions. The output will look something like this:

4. Run `helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug`

* Replace `<namespace>` with the namespace you want to deploy LangSmith to.
   * Replace `<version>` with the version of LangSmith you want to install from the previous step. Most users should install the latest version available.

Once the `helm install` command runs and finishes successfully, you should see output similar to this:

This may take a few minutes to complete as it will create several Kubernetes resources and run several jobs to initialize the database and other services.

5. Run `kubectl get pods` Output should now look something like this (note the exact pod names may vary based on the version and configuration you used):

## Validate your deployment:

1. Run `kubectl get services`

Output should look something like:

2. Curl the external ip of the `langsmith-frontend` service:

3. Visit the external ip for the `langsmith-frontend` service on your browser

The LangSmith UI should be visible/operational

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675" alt="Langsmith ui" data-og-width="2886" width="2886" data-og-height="1698" height="1698" data-path="langsmith/images/langsmith-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w" />

Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](/langsmith/self-hosted).

Your LangSmith instance is now running but may not be fully setup yet.

If you used one of the basic configs, you will have a default admin user account created for you. You can log in with the email address and password you specified in the `langsmith_config.yaml` file.

As a next step, it is strongly recommended you work with your infrastructure administrators to:

* Setup DNS for your LangSmith instance to enable easier access
* Configure SSL to ensure in-transit encryption of traces submitted to LangSmith
* Configure LangSmith with [Single Sign-On](/langsmith/self-host-sso) to secure your LangSmith instance
* Connect LangSmith to external Postgres and Redis instances
* Set up [Blob Storage](/langsmith/self-host-blob-storage) for storing large files

Review our [configuration section](/langsmith/self-hosted) for more information on how to configure these options.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/kubernetes.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
3. JWT Secret (Optional but used for basic auth)

   1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:
```

Example 2 (unknown):
```unknown
### Databases

LangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider’s managed services.

For more information, refer to the following setup guides for external services:

* [PostgreSQL](/langsmith/self-host-external-postgres)
* [Redis](/langsmith/self-host-external-redis)
* [ClickHouse](/langsmith/self-host-external-clickhouse)

### Kubernetes cluster requirements

1. You will need a working Kubernetes cluster that you can access via `kubectl`. Your cluster should have the following minimum requirements:

   1. Recommended: At least 16 vCPUs, 64GB Memory available

      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found [here](/langsmith/self-host-scale).
      * We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage.
      * We recommend setting up the metrics server so that autoscaling can be turned on.
      * If you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory **allocatable** as ClickHouse will request this amount of resources by default.

   2. Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster)

      * To enable persistence, we will try to provision volumes for any database running in-cluster.
      * If using PVs in your cluster, we highly recommend setting up backups in a production environment.
      * **We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput.**
      * On EKS, you may need to ensure you have the `ebs-csi-driver` installed and configured for dynamic provisioning. Refer to the [EBS CSI Driver documentation](https://docs.aws.amazon.com/eks/latest/userguide/ebs-csi.html) for more information.

      You can verify this by running:
```

Example 3 (unknown):
```unknown
The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:
```

Example 4 (unknown):
```unknown
<Note>
        We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time.
      </Note>

      Refer to the [Kubernetes documentation](https://kubernetes.io/do/langsmith/observability-concepts/storage/storage-classes/) for more information on storage classes.

2. Helm

   1. To install `helm` refer to the [Helm documentation](https://helm.sh/docs/intro/install/)

3. Egress to `https://beacon.langchain.com` (if not running in offline mode)

   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

## Configure your Helm Charts:

1. Create a new file called `langsmith_config.yaml` with the configuration options from the previous step.
   1. There are several configuration options that you can set in the `langsmith_config.yaml` file. You can find more information on specific configuration options in the [Configuration](/langsmith/self-hosted) section.
   2. If you are new to Kubernetes or Helm, we’d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: [LangSmith helm chart examples](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/examples).
   3. You can see a full list of configuration options in the `values.yaml` file in the Helm Chart repository here: [LangSmith Helm Chart](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/values.yaml)

<Warning>
  Only override the settings you need in `langsmith_config.yaml`; don’t copy the entire `values.yaml`.
  Keeping your config minimal ensures you continue to inherit new defaults and upgrades from the Helm chart.
</Warning>

2. At a minimum, you will need to set the following configuration options (using basic auth):
```

---

## Self-host LangSmith with Docker

**URL:** llms-txt#self-host-langsmith-with-docker

**Contents:**
- Prerequisites
- Running via Docker Compose
  - 1. Fetch the LangSmith `docker-compose.yml` file
  - 2. Configure environment variables
  - 3. Start server
  - Validate your deployment:
  - Checking the logs
  - Stopping the server
- Using LangSmith

Source: https://docs.langchain.com/langsmith/docker

<Info>
  Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our [pricing page](https://www.langchain.com/pricing) for more detail, and [contact our sales team](https://www.langchain.com/contact-sales) if you want to get a license key to trial LangSmith in your environment.
</Info>

This guide provides instructions for running the **LangSmith platform** locally using Docker for development and testing purposes.

<Warning>
  **For development/testing only**. Do not use Docker Compose for production. For production deployments, use [Kubernetes](/langsmith/kubernetes).
</Warning>

<Note>
  This page describes how to install the base [LangSmith platform](/langsmith/self-hosted#langsmith) for local testing. It does **not** include deployment management features. For more details, review the [self-hosted options](/langsmith/self-hosted).
</Note>

Note that Docker Compose is limited to local development environments only and does not extend support to container services such as AWS Elastic Container Service, Azure Container Instances, and Google Cloud Run.

1. Ensure Docker is installed and running on your system. You can verify this by running:

If you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.

1. Recommended: At least 4 vCPUs, 16GB Memory available on your machine.
      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
   2. Disk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.

2. LangSmith License Key
   1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:

4. Egress to `https://beacon.langchain.com` (if not running in offline mode)
   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

5. Configuration
   1. There are several configuration options that you can set in the `.env` file. You can find more information on the available configuration options in the [Configuration](/langsmith/self-host-scale) section.

## Running via Docker Compose

The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. **In production, we highly recommend using a secured Kubernetes environment.**

### 1. Fetch the LangSmith `docker-compose.yml` file

You can find the `docker-compose.yml` file and related files in the LangSmith SDK repository here: [*LangSmith Docker Compose File*](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docker-compose/docker-compose.yaml)

Copy the `docker-compose.yml` file and all files in that directory from the LangSmith SDK to your project directory.

* Ensure that you copy the `users.xml` file as well.

### 2. Configure environment variables

1. Copy the `.env.example` file from the LangSmith SDK to your project directory and rename it to `.env`.
2. Configure the appropriate values in the `.env` file. You can find the available configuration options in the [Configuration](/langsmith/self-hosted) section.

You can also set these environment variables in the `docker-compose.yml` file directly or export them in your terminal. We recommend setting them in the `.env` file.

Start the LangSmith application by executing the following command in your terminal:

You can also run the server in the background by running:

### Validate your deployment:

1. Curl the exposed port of the `cli-langchain-frontend-1` container:

2. Visit the exposed port of the `cli-langchain-frontend-1` container on your browser

The LangSmith UI should be visible/operational at `http://localhost:1980`

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5310f686e7b9eebaaee4fe2a152a8675" alt=".langsmith_ui.png" data-og-width="2886" width="2886" data-og-height="1698" height="1698" data-path="langsmith/images/langsmith-ui.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f155ce778ca848f89fefff237b69bcb 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=1d55d4068a9f53387c129b4688b0971e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=feb20198d67249ece559e5fd0e6d8e98 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=3e5eba764d911e567d5aaa9e5702327b 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d45af56632578a8d1b05e546dfc8d01d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16a49517a6c224930fdb81c9ccde5527 2500w" />

### Checking the logs

If, at any point, you want to check if the server is running and see the logs, run

### Stopping the server

Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the [self-hosted usage guide](/langsmith/self-hosted).

Your LangSmith instance is now running but may not be fully setup yet.

If you used one of the basic configs, you may have deployed a no-auth configuration. In this state, there is no authentication or concept of user accounts nor API keys and traces can be submitted directly without an API key so long as the hostname is passed to the LangChain tracer/LangSmith SDK.

As a next step, it is strongly recommended you work with your infrastructure administrators to:

* Setup DNS for your LangSmith instance to enable easier access
* Configure SSL to ensure in-transit encryption of traces submitted to LangSmith
* Configure LangSmith for [oauth authentication](/langsmith/self-host-sso) or [basic authentication](/langsmith/self-host-basic-auth) to secure your LangSmith instance
* Secure access to your Docker environment to limit access to only the LangSmith frontend and API
* Connect LangSmith to secured Postgres and Redis instances

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/docker.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If you don't see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.

   1. Recommended: At least 4 vCPUs, 16GB Memory available on your machine.
      * You may need to tune resource requests/limits for all of our different services based off of organization size/usage
   2. Disk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.

2. LangSmith License Key
   1. You can get this from your LangChain representative. [Contact our sales team](https://www.langchain.com/contact-sales) for more information.

3. Api Key Salt

   1. This is a secret key that you can generate. It should be a random string of characters.
   2. You can generate this using the following command:
```

Example 2 (unknown):
```unknown
4. Egress to `https://beacon.langchain.com` (if not running in offline mode)
   1. LangSmith requires egress to `https://beacon.langchain.com` for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the [Egress](/langsmith/self-host-egress) section.

5. Configuration
   1. There are several configuration options that you can set in the `.env` file. You can find more information on the available configuration options in the [Configuration](/langsmith/self-host-scale) section.

## Running via Docker Compose

The following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. **In production, we highly recommend using a secured Kubernetes environment.**

### 1. Fetch the LangSmith `docker-compose.yml` file

You can find the `docker-compose.yml` file and related files in the LangSmith SDK repository here: [*LangSmith Docker Compose File*](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/docker-compose/docker-compose.yaml)

Copy the `docker-compose.yml` file and all files in that directory from the LangSmith SDK to your project directory.

* Ensure that you copy the `users.xml` file as well.

### 2. Configure environment variables

1. Copy the `.env.example` file from the LangSmith SDK to your project directory and rename it to `.env`.
2. Configure the appropriate values in the `.env` file. You can find the available configuration options in the [Configuration](/langsmith/self-hosted) section.

You can also set these environment variables in the `docker-compose.yml` file directly or export them in your terminal. We recommend setting them in the `.env` file.

### 3. Start server

Start the LangSmith application by executing the following command in your terminal:
```

Example 3 (unknown):
```unknown
You can also run the server in the background by running:
```

Example 4 (unknown):
```unknown
### Validate your deployment:

1. Curl the exposed port of the `cli-langchain-frontend-1` container:
```

---

## Send your API Key in the request headers

**URL:** llms-txt#send-your-api-key-in-the-request-headers

headers = {
    "x-api-key": os.environ["LANGSMITH_API_KEY"],
    "x-tenant-id": os.environ["LANGSMITH_WORKSPACE_ID"]
}

def post_run(run_id, name, run_type, inputs, parent_id=None):
    """Function to post a new run to the API."""
    data = {
        "id": run_id.hex,
        "name": name,
        "run_type": run_type,
        "inputs": inputs,
        "start_time": datetime.utcnow().isoformat(),
        # "session_name": "project-name",  # the name of the project to trace to
        # "session_id": "project-id",  # the ID of the project to trace to. specify one of session_name or session_id
    }
    if parent_id:
        data["parent_run_id"] = parent_id.hex

requests.post(
        "https://api.smith.langchain.com/runs",  # Update appropriately for self-hosted installations or the EU region
        json=data,
        headers=headers
    )

def patch_run(run_id, outputs):
    """Function to patch a run with outputs."""
    requests.patch(
        f"https://api.smith.langchain.com/runs/{run_id}",
        json={
            "outputs": outputs,
            "end_time": datetime.now(timezone.utc).isoformat(),
        },
        headers=headers,
    )

---

## Separate loop to avoid logging at the same time as logs from evaluate()

**URL:** llms-txt#separate-loop-to-avoid-logging-at-the-same-time-as-logs-from-evaluate()

**Contents:**
- Understand the result structure
- Examples
  - Implement a quality gate

for result in aggregated_results:
    print("Input:", result["run"].inputs)
    print("Output:", result["run"].outputs)
    print("Evaluation Results:", result["evaluation_results"]["results"])
    print("--------------------------------")

Input: {'input': 'MY INPUT'}
Output: {'output': 'MY OUTPUT'}
Evaluation Results: [EvaluationResult(key='randomness', score=1, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7ebb4900-91c0-40b0-bb10-f2f6a451fd3c'), target_run_id=None, extra=None)]
--------------------------------
python  theme={null}
from langsmith import Client
import sys

def my_application(inputs):
    # Your application logic
    return {"response": "..."}

def accuracy_evaluator(run, example):
    # Your evaluation logic
    is_correct = run.outputs["response"] == example.outputs["expected"]
    return {"key": "accuracy", "score": 1 if is_correct else 0}

**Examples:**

Example 1 (unknown):
```unknown
This produces output like:
```

Example 2 (unknown):
```unknown
## Understand the result structure

Each result in the iterator contains:

* `result["run"]`: The execution of your target function.
  * `result["run"].inputs`: The inputs from your [dataset](/langsmith/evaluation-concepts#datasets) example.
  * `result["run"].outputs`: The outputs produced by your target function.
  * `result["run"].id`: The unique ID for this run.

* `result["evaluation_results"]["results"]`: A list of `EvaluationResult` objects, one per evaluator.
  * `key`: The metric name (from your evaluator's return value).
  * `score`: The numeric score (typically 0-1 or boolean).
  * `comment`: Optional explanatory text.
  * `source_run_id`: The ID of the evaluator run.

* `result["example"]`: The dataset example that was evaluated.
  * `result["example"].inputs`: The input values.
  * `result["example"].outputs`: The reference outputs (if any).

## Examples

### Implement a quality gate

This example uses evaluation results to pass or fail a CI/CD build automatically based on quality thresholds. The script iterates through results, calculates an average accuracy score, and exits with a non-zero status code if the accuracy falls below 85%. This ensures that you can deploy code changes that meet quality standards.
```

---

## server.py

**URL:** llms-txt#server.py

import langsmith as ls
from fastapi import FastAPI, Request

@ls.traceable
async def my_application():
    ...

app = FastAPI()  # Or Flask, Django, or any other framework

@app.post("/my-route")
async def fake_route(request: Request):
    # request.headers:  {"langsmith-trace": "..."}
    # as well as optional metadata/tags in `baggage`
    with ls.tracing_context(parent=request.headers):
        return await my_application()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
The example above uses the `tracing_context` context manager. You can also directly specify the parent run context in the `langsmith_extra` parameter of a method wrapped with `@traceable`.
```

---

## Server Information

**URL:** llms-txt#server-information

Source: https://docs.langchain.com/langsmith/agent-server-api/system/server-information

langsmith/agent-server-openapi.json get /info
Get server version information, feature flags, and metadata.

---

## Service A: Create a span and propagate context to Service B

**URL:** llms-txt#service-a:-create-a-span-and-propagate-context-to-service-b

def service_a():
    with tracer.start_as_current_span("service_a_operation") as span:
        # Create a chain
        prompt = ChatPromptTemplate.from_template("Summarize: {text}")
        model = ChatOpenAI()
        chain = prompt | model

# Run the chain
        result = chain.invoke({"text": "OpenTelemetry is an observability framework"})

# Propagate context to Service B
        headers = {}
        inject(headers)  # Inject trace context into headers

# Call Service B with the trace context
        response = requests.post(
            "http://service-b.example.com/process",
            headers=headers,
            json={"summary": result.content}
        )
        return response.json()

---

## Service B: Extract the context and continue the trace

**URL:** llms-txt#service-b:-extract-the-context-and-continue-the-trace

from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route("/process", methods=["POST"])
def service_b_endpoint():
    # Extract the trace context from the request headers
    context = extract(request.headers)
    with tracer.start_as_current_span("service_b_operation", context=context) as span:
        data = request.json
        summary = data.get("summary", "")

# Process the summary with another LLM chain
        prompt = ChatPromptTemplate.from_template("Analyze the sentiment of: {text}")
        model = ChatOpenAI()
        chain = prompt | model
        result = chain.invoke({"text": summary})

return jsonify({"analysis": result.content})

if __name__ == "__main__":
    app.run(port=5000)
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-opentelemetry.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Setup recording directory

**URL:** llms-txt#setup-recording-directory

recordings_dir = Path(__file__).parent / "recordings"
recordings_dir.mkdir(exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
recording_path = recordings_dir / f"conversation_{timestamp}.wav"

---

## Set a sampling rate for traces

**URL:** llms-txt#set-a-sampling-rate-for-traces

**Contents:**
- Set a global sampling rate
- Set different sampling rates per client

Source: https://docs.langchain.com/langsmith/sample-traces

When working with high-volume applications, you may not want to log every trace to LangSmith. Sampling rates allow you to control what percentage of traces are logged, helping you balance observability needs with cost considerations.

## Set a global sampling rate

<Note>
  This section is relevant for those using the LangSmith SDK or LangChain, not for those logging directly with the LangSmith API.
</Note>

By default, all traces are logged to LangSmith. To down-sample the number of traces logged to LangSmith, set the `LANGSMITH_TRACING_SAMPLING_RATE` environment variable to any float between `0` (no traces) and `1` (all traces). For instance, setting the following environment variable will log 75% of the traces.

This works for the `traceable` decorator and `RunTree` objects.

## Set different sampling rates per client

You can also set sampling rates on specific `Client` instances and use the `tracing_context` context manager:

```python  theme={null}
from langsmith import Client, tracing_context

**Examples:**

Example 1 (unknown):
```unknown
This works for the `traceable` decorator and `RunTree` objects.

## Set different sampling rates per client

You can also set sampling rates on specific `Client` instances and use the `tracing_context` context manager:
```

---

## Set environment variables for external services

**URL:** llms-txt#set-environment-variables-for-external-services

**Contents:**
- Troubleshooting
  - Wrong API endpoints

export POSTGRES_URI_CUSTOM="postgresql://user:pass@host:5432/db"
export REDIS_URI_CUSTOM="redis://host:6379/0"
```

See the [environment variables documentation](/langsmith/env-var#postgres-uri-custom) for more details.

### Wrong API endpoints

If you're experiencing connection issues, verify you're using the correct endpoint format for your LangSmith instance. There are two different APIs with different endpoints:

#### LangSmith API (Traces, Ingestion, etc.)

For LangSmith API operations (traces, evaluations, datasets):

| Region | Endpoint                             |
| ------ | ------------------------------------ |
| US     | `https://api.smith.langchain.com`    |
| EU     | `https://eu.api.smith.langchain.com` |

For self-hosted LangSmith instances, use `http(s)://<langsmith-url>/api` where `<langsmith-url>` is your self-hosted instance URL.

<Note>
  If you're setting the endpoint in the `LANGSMITH_ENDPOINT` environment variable, you need to add `/v1` at the end (e.g., `https://api.smith.langchain.com/v1` or `http(s)://<langsmith-url>/api/v1` if self-hosted).
</Note>

#### LangSmith Deployment API (Deployments)

For LangSmith Deployment operations (deployments, revisions):

| Region | Endpoint                            |
| ------ | ----------------------------------- |
| US     | `https://api.host.langchain.com`    |
| EU     | `https://eu.api.host.langchain.com` |

For self-hosted LangSmith instances, use `http(s)://<langsmith-url>/api-host` where `<langsmith-url>` is your self-hosted instance URL.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cicd-pipeline-example.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Set environment variables for LangChain

**URL:** llms-txt#set-environment-variables-for-langchain

os.environ["LANGSMITH_OTEL_ENABLED"] = "true"
os.environ["LANGSMITH_TRACING"] = "true"

---

## Set Latest Assistant Version

**URL:** llms-txt#set-latest-assistant-version

Source: https://docs.langchain.com/langsmith/agent-server-api/assistants/set-latest-assistant-version

langsmith/agent-server-openapi.json post /assistants/{assistant_id}/latest
Set the latest version for an assistant.

---

## Set stream_mode="custom" to receive the custom data in the stream

**URL:** llms-txt#set-stream_mode="custom"-to-receive-the-custom-data-in-the-stream

**Contents:**
- Disable streaming for specific chat models
  - Async with Python \< 3.11

for chunk in graph.stream(
    {"topic": "cats"},
    stream_mode="custom",  # [!code highlight]

):
    # The chunk will contain the custom data streamed from the llm
    print(chunk)
python  theme={null}
  import operator
  import json

from typing import TypedDict
  from typing_extensions import Annotated
  from langgraph.graph import StateGraph, START

from openai import AsyncOpenAI

openai_client = AsyncOpenAI()
  model_name = "gpt-4o-mini"

async def stream_tokens(model_name: str, messages: list[dict]):
      response = await openai_client.chat.completions.create(
          messages=messages, model=model_name, stream=True
      )
      role = None
      async for chunk in response:
          delta = chunk.choices[0].delta

if delta.role is not None:
              role = delta.role

if delta.content:
              yield {"role": role, "content": delta.content}

# this is our tool
  async def get_items(place: str) -> str:
      """Use this tool to list items one might find in a place you're asked about."""
      writer = get_stream_writer()
      response = ""
      async for msg_chunk in stream_tokens(
          model_name,
          [
              {
                  "role": "user",
                  "content": (
                      "Can you tell me what kind of items "
                      f"i might find in the following place: '{place}'. "
                      "List at least 3 such items separating them by a comma. "
                      "And include a brief description of each item."
                  ),
              }
          ],
      ):
          response += msg_chunk["content"]
          writer(msg_chunk)

class State(TypedDict):
      messages: Annotated[list[dict], operator.add]

# this is the tool-calling graph node
  async def call_tool(state: State):
      ai_message = state["messages"][-1]
      tool_call = ai_message["tool_calls"][-1]

function_name = tool_call["function"]["name"]
      if function_name != "get_items":
          raise ValueError(f"Tool {function_name} not supported")

function_arguments = tool_call["function"]["arguments"]
      arguments = json.loads(function_arguments)

function_response = await get_items(**arguments)
      tool_message = {
          "tool_call_id": tool_call["id"],
          "role": "tool",
          "name": function_name,
          "content": function_response,
      }
      return {"messages": [tool_message]}

graph = (
      StateGraph(State)
      .add_node(call_tool)
      .add_edge(START, "call_tool")
      .compile()
  )
  python  theme={null}
  inputs = {
      "messages": [
          {
              "content": None,
              "role": "assistant",
              "tool_calls": [
                  {
                      "id": "1",
                      "function": {
                          "arguments": '{"place":"bedroom"}',
                          "name": "get_items",
                      },
                      "type": "function",
                  }
              ],
          }
      ]
  }

async for chunk in graph.astream(
      inputs,
      stream_mode="custom",
  ):
      print(chunk["content"], end="|", flush=True)
  python  theme={null}
    from langchain.chat_models import init_chat_model

model = init_chat_model(
        "claude-sonnet-4-5-20250929",
        # Set streaming=False to disable streaming for the chat model
        streaming=False  # [!code highlight]
    )
    python  theme={null}
    from langchain_openai import ChatOpenAI

# Set streaming=False to disable streaming for the chat model
    model = ChatOpenAI(model="o1-preview", streaming=False)
    python  theme={null}
  from typing import TypedDict
  from langgraph.graph import START, StateGraph
  from langchain.chat_models import init_chat_model

model = init_chat_model(model="gpt-4o-mini")

class State(TypedDict):
      topic: str
      joke: str

# Accept config as an argument in the async node function
  async def call_model(state, config):
      topic = state["topic"]
      print("Generating joke...")
      # Pass config to model.ainvoke() to ensure proper context propagation
      joke_response = await model.ainvoke(  # [!code highlight]
          [{"role": "user", "content": f"Write a joke about {topic}"}],
          config,
      )
      return {"joke": joke_response.content}

graph = (
      StateGraph(State)
      .add_node(call_model)
      .add_edge(START, "call_model")
      .compile()
  )

# Set stream_mode="messages" to stream LLM tokens
  async for chunk, metadata in graph.astream(
      {"topic": "ice cream"},
      stream_mode="messages",  # [!code highlight]
  ):
      if chunk.content:
          print(chunk.content, end="|", flush=True)
  python  theme={null}
  from typing import TypedDict
  from langgraph.types import StreamWriter

class State(TypedDict):
        topic: str
        joke: str

# Add writer as an argument in the function signature of the async node or tool
  # LangGraph will automatically pass the stream writer to the function
  async def generate_joke(state: State, writer: StreamWriter):  # [!code highlight]
        writer({"custom_key": "Streaming custom data while generating a joke"})
        return {"joke": f"This is a joke about {state['topic']}"}

graph = (
        StateGraph(State)
        .add_node(generate_joke)
        .add_edge(START, "generate_joke")
        .compile()
  )

# Set stream_mode="custom" to receive the custom data in the stream  # [!code highlight]
  async for chunk in graph.astream(
        {"topic": "ice cream"},
        stream_mode="custom",
  ):
        print(chunk)
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming arbitrary chat model">
```

Example 2 (unknown):
```unknown
Let's invoke the graph with an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) that includes a tool call:
```

Example 3 (unknown):
```unknown
</Accordion>

## Disable streaming for specific chat models

If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for
models that do not support it.

Set `streaming=False` when initializing the model.

<Tabs>
  <Tab title="init_chat_model">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Chat model interface">
```

---

## Set the project name to whichever project you'd like to be testing against

**URL:** llms-txt#set-the-project-name-to-whichever-project-you'd-like-to-be-testing-against

project_name = "Tweet Writing Task"
os.environ["LANGSMITH_PROJECT"] = project_name
os.environ["LANGSMITH_TRACING"] = "true"

if not os.environ.get("LANGSMITH_API_KEY"):
    os.environ["LANGSMITH_API_KEY"] = getpass.getpass("YOUR API KEY")

---

## set the same access key credentials and region as you used for the destination

**URL:** llms-txt#set-the-same-access-key-credentials-and-region-as-you-used-for-the-destination

> AWS Access Key ID: <access_key_id>
> AWS Secret Access Key: <secret_access_key>
> Default region name [us-east-1]: <region>

---

## Set up automation rules

**URL:** llms-txt#set-up-automation-rules

**Contents:**
- View automation rules
- Create a rule
- View logs for your automations
- Video guide

Source: https://docs.langchain.com/langsmith/rules

While you can manually sift through and process production logs from your LLM application, it often becomes difficult as your application scales to more users.
LangSmith provides a powerful feature called **Automations** that allow you to trigger certain actions on your trace data.
At a high level, automations are defined by a **filter**, **sampling rate**, and **action**.

Automation rules can trigger actions such as: adding traces to a dataset, adding to an annotation queue, triggering a webhook (e.g. for remote evaluations) or extending data retention. Some examples of automations you can set up:

* Send all traces with negative feedback to an annotation queue for human review
* Send 10% of all traces to an annotation queue for human review to spot check for issues
* Upgrade all traces with errors for extended data retention

<Info>
  To configure online evaluations, visit the [online evaluations](/langsmith/online-evaluations) page.
</Info>

<Note>If an automation rule matches any run within a trace, the trace will be auto-upgraded to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your automation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

## View automation rules

Head to the **Tracing Projects** tab and select a tracing project. To view existing automation rules for that tracing project, click on the **Automations** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=9206c95784e0d572adddf7ad60e58717" alt="View automation rules" data-og-width="1349" width="1349" data-og-height="521" height="521" data-path="langsmith/images/view-automation-rules.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=88988b3e687419b0494b36e20d424965 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8091094d8e938a31367fd456cdb16778 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=08bce1184a23c4345c41abc2ede29826 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=faf56babdc5b9396a38e780600a31b4a 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8996953f130ed137172b26af817a5c22 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-automation-rules.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fea915cf43defcf09e6b7161a2c2af8a 2500w" />

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aq-spot-check-rule.gif?s=0ea58303ab04ffa8ed16c49ab39701ef" alt="Aq spot check rule" data-og-width="1556" width="1556" data-og-height="1080" height="1080" data-path="langsmith/images/aq-spot-check-rule.gif" data-optimize="true" data-opv="3" />

#### 1. Navigate to rule creation

Head to the **Tracing Projects** tab and select a tracing project. Click on **+ New** in the top right corner of the tracing project page, then click on **New Automation**.

#### 2. Name your rule

#### 3. Create a filter

Automation rule filters work the same way as filters applied to traces in the project. For more information on filters, you can refer to [this guide](./filter-traces-in-application)

#### 4. Configure a sampling rate

Configure a sampling rate to control the percentage of filtered runs that trigger the automation action.

You can specify a sampling rate between 0 and 1 for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action.

#### 5. (Optional) Apply rule to past runs

Apply rule to past runs by toggling the **Apply to past runs** and entering a "Backfill from" date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately. In order to track progress of the backfill, you can [view logs for your automations](./rules#view-logs-for-your-automations)

#### 6. Select an action to trigger when the rule is applied.

There are four actions you can take with an automation rule:

* **Add to dataset**: Add the inputs and outputs of the trace to a [dataset](/langsmith/evaluation-concepts#datasets).
* **Add to annotation queue**: Add the trace to an [annotation queue](/langsmith/evaluation-concepts#annotation-queues).
* **Trigger webhook**: Trigger a webhook with the trace data. For more information on webhooks, you can refer to [this guide](./webhooks).
* **Extend data retention**: Extends the data retention period on matching traces that use base retention [(see data retention docs for more details)](/langsmith/administration-overview#data-retention).
  Note that all other rules will also extend data retention on matching traces through the
  auto-upgrade mechanism described in the aforementioned data retention docs,
  but this rule takes no additional action.

## View logs for your automations

Logs allow you to gain confidence that your rules are working as expected. You can view logs for your automations by heading to the **Automations** tab within a tracing project and clicking the **Logs** button for the rule you created.

The logs tab allows you to:

* View all runs processed by a given rule for the time period selected
* If a particular rule execution has triggered an error, you can view the error message by hovering over the error icon
* You can monitor the progress of a backfill job by filtering to the rule's creation timestamp. This is because the backfill starts from when the rule was created.
* Inspect the run that the automation rule applied to using the **View run** button. For rules that add runs as examples to datasets, you can view the example produced.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rule-logs.gif?s=2ec5562fea9e2079b687da149d1609b3" alt="Logs_Gif" data-og-width="1556" width="1556" data-og-height="1080" height="1080" data-path="langsmith/images/rule-logs.gif" data-optimize="true" data-opv="3" />

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/z69cBXTJFZ0?si=GBKQ9_muHR1zllLl" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/rules.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Set up a workspace

**URL:** llms-txt#set-up-a-workspace

**Contents:**
- Set up an organization
  - Create an organization
  - Manage and navigate workspaces
  - Manage users
- Set up a workspace
  - Create a workspace
  - Manage users
  - Configure workspace settings
  - Delete a workspace
  - Delete a workspace via the UI

Source: https://docs.langchain.com/langsmith/set-up-a-workspace

This page describes setting up and managing your LangSmith [*organization*](/langsmith/administration-overview#organizations) and [*workspaces*](/langsmith/administration-overview#workspaces):

* [Set up an organization](#set-up-an-organization): Create and manage organizations for team collaboration, including user management and role assignments.
* [Set up a workspace](#set-up-a-workspace): Set up and configure workspaces to organize your LangSmith resources, manage workspace members, and configure settings for team collaboration.

<Check>
  You may find it helpful to refer to the [overview on LangSmith resource hierarchy](/langsmith/administration-overview) before you read this setup page.
</Check>

## Set up an organization

<Note>
  If you're interested in managing your organization and workspaces programmatically, see [this how-to guide](/langsmith/manage-organization-by-api).
</Note>

### Create an organization

When you log in for the first time, LangSmith will create a personal organization for you automatically. If you'd like to collaborate with others, you can create a separate organization and invite your team members to join.

To do this, open the Organizations drawer by clicking your profile icon in the bottom left and click **+ New**. Shared organizations require a credit card before they can be used. You will need to [set up billing](/langsmith/billing#set-up-billing-for-your-account) to proceed.

### Manage and navigate workspaces

Once you've subscribed to a plan that allows for multiple users per organization, you can [set up workspaces](/langsmith/administration-overview#workspaces) to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=91c38be270a4e9f7d613fca83192dc6b" alt="Select workspace" data-og-width="2992" width="2992" data-og-height="478" height="478" data-path="langsmith/images/select-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9c1ba64d54177f72feab7394b4a8ff28 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=772aa2168f043813208bd0a9af628b8b 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=ae8d1e560af5d320a772ef6edb23f934 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=cea6e37801913f953c0fd750ba3ff2a3 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=e40edbdce2c538e64c4833f4a10b44fc 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/select-workspace.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b7598fd5dc01cb62f6fa22f9adfead8b 2500w" />

Manage membership in your shared organization in the **Members and roles** tabs on the [Settings page](https://smith.langchain.com/settings). Here you can:

* Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role.
* Edit a user's organization role.
* Remove users from your organization.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=7f6b85051e5dcca2f074ba0ef4801ddd" alt="Organization members and roles" data-og-width="3008" width="3008" data-og-height="890" height="890" data-path="langsmith/images/organization-members-and-roles.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=280&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=2285444015681aadc12dee88d8485294 280w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=560&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=5643ea639fa21f9df8faeb7bbcf4db4c 560w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=840&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=011efce2e1f2c89ef26e1c3c5c280d5b 840w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=1100&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=fe2dff2c95618365c5269c7fc0f6337f 1100w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=1650&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=cd20d709570a0e9e447e0a29b9f457c7 1650w, https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/organization-members-and-roles.png?w=2500&fit=max&auto=format&n=H9jA2WRyA-MV4-H0&q=85&s=8dba05df82360444c35d94fe90a64dd4 2500w" />

Organizations on the Enterprise plan may set up custom workspace roles in the **Roles** tab. For more details, refer to the [access control setup guide](/langsmith/user-management).

#### Organization roles

Organization-scoped roles are used to determine access to organization settings. The role selected also impacts workspace membership:

* `Organization Admin` grants full access to manage all organization configuration, users, billing, and workspaces. Any `Organization Admin` has `Admin` access to all workspaces in an organization.

- `Organization User` may read organization information, but cannot execute any write actions at the organization level. You can add an `Organization User` to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.

<Info>
  The `Organization User` role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are `Organization Admins`. Custom organization-scoped roles are not available.
</Info>

For a full list of permissions associated with each role, refer to the [Administration overview](/langsmith/administration-overview#organization-roles) page.

## Set up a workspace

When you log in for the first time, a default [workspace](/langsmith/administration-overview#workspaces) will be created for you in your personal organization. Workspaces are often used to separate resources between different teams or business units to establish clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) manages permissions and access levels, which ensures that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls.

To organize resources within a workspace, you can use [resource tags](/langsmith/set-up-resource-tags).

### Create a workspace

To create a new workspace, navigate to the [Settings page](https://smith.langchain.com/settings) **Workspaces** tab in your shared organization and click **Add Workspace**. Once you have created your workspace, you can manage its members and other configuration by selecting it on this page.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a26994889b28911c59daa8de557c7271" alt="Create workspace" data-og-width="3014" width="3014" data-og-height="532" height="532" data-path="langsmith/images/create-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e7542ce1dcc74278722aaa5b707eb7f8 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c30f0578754fa71812905d1b964c2ebb 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=79f02189dc33d5f730defa4792d89f19 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4138278a2a6f8b11c3df64a51d710b55 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1a5964c468159d57a812efe66e8bd822 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-workspace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e5efd5a083c29f17b9bddad1f7423fe9 2500w" />

<Note>
  Different plans have different limits placed on the number of workspaces that can be used in an organization. For more information, refer to the [pricing page](https://www.langchain.com/pricing-langsmith).
</Note>

<Info>
  Only workspace `Admins` can manage workspace membership and, if RBAC is enabled, change a user's workspace role.
</Info>

For users that are already members of an organization, a workspace `Admin` may add them to a workspace in the **Workspace members** tab under [Workspaces settings page](https://smith.langchain.com/settings/workspaces). Users may also be invited directly to one or more workspaces when they are [invited to an organization](#manage-users).

### Configure workspace settings

Workspace configuration exists in the [Workspaces settings page](https://smith.langchain.com/settings/workspaces) tab. Select the workspace to configure and then the desired configuration sub-tab. The following example shows the **API keys**, and other configuration options including secrets, models, and shared URLs are available here as well.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0b95739c014bc31f2950d9d586303cbb" alt="Workspace settings" data-og-width="3012" width="3012" data-og-height="1226" height="1226" data-path="langsmith/images/workspace-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=ddd4f1738c7142be44e6966b0079cad6 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a8dcfe014fc2584946019acebc59fd3b 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=659cffc42334b972d3a1f01f2926120b 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7fdae2696aed94f5e7391d3e88c92d49 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1071e84f39202761f635e141b7828a82 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/workspace-settings.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6edc496967555f2cfd3365bb846ce698 2500w" />

### Delete a workspace

<Warning>
  Deleting a workspace will permanently delete the workspace and all associated data. This action cannot be undone.
</Warning>

You can delete a workspace through the LangSmith UI or via [API](https://api.smith.langchain.com/redoc?#tag/workspaces/operation/delete_workspace_api_v1_workspaces__workspace_id__delete). You must be a workspace `Admin` in order to delete a workspace.

### Delete a workspace via the UI

1. Navigate to **Settings**.
2. Select the workspace you want to delete.
3. Click **Delete** in the top-right corner of the screen.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=33038784e813f06dae3c87e5d34a3dc1" alt="Delete a workspace" data-og-width="1106" width="1106" data-og-height="250" height="250" data-path="langsmith/images/delete-workspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=829f2ad5874457f3023bf4441e408203 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c907fade390eff674deb3fafc038e885 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=1b5c0c1dec248c82ef12cd61d4da9fed 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c9005738241ade3dc2516c6e9b395d39 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=19a256c90e69d4db054d4a360a84cd40 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-workspace.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5cfe1a5279c11bdd0e33812b58418e92 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-a-workspace.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Set up custom authentication

**URL:** llms-txt#set-up-custom-authentication

**Contents:**
- 1. Create your app
- 2. Add authentication

Source: https://docs.langchain.com/langsmith/set-up-custom-auth

In this tutorial, we will build a chatbot that only lets specific users access it. We'll start with the LangGraph template and add token-based security step by step. By the end, you'll have a working chatbot that checks for valid tokens before allowing access.

This is part 1 of our authentication series:

1. Set up custom authentication (you are here) - Control who can access your bot
2. [Make conversations private](/langsmith/resource-auth) - Let users have private conversations
3. [Connect an authentication provider](/langsmith/add-auth-server) - Add real user accounts and validate using OAuth2 for production

This guide assumes basic familiarity with the following concepts:

* [**Authentication & Access Control**](/langsmith/auth)
* [**LangSmith**](/langsmith/home)

<Note>
  Custom auth is only available for LangSmith SaaS deployments or Enterprise Self-Hosted deployments.
</Note>

## 1. Create your app

Create a new chatbot using the LangGraph starter template:

The template gives us a placeholder LangGraph app. Try it out by installing the local dependencies and running the development server:

The server will start and open [Studio](/langsmith/studio) in your browser:

If you were to self-host this on the public internet, anyone could access it.

<img src="https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=3ca2c9a8d65891ef71abfb7ad0aae7d3" alt="No authentication: the dev server is publicly reachable, anyone can access the bot if exposed to the internet." data-og-width="1974" width="1974" data-og-height="1412" height="1412" data-path="langsmith/images/no-auth.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=280&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=67bfe450ee04d2432e5e1b86cfa0af1c 280w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=560&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=087f8083981ae85eeca794bdca3e0e05 560w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=840&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=7967264c0243918be1a8561d4db89586 840w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=1100&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=fd9d62383850b137834bfc7e35bc4533 1100w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=1650&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=66100323459f51596c53b3d941e3f85d 1650w, https://mintcdn.com/langchain-5e9cc07a/N1xJUsnxxRqnrjxV/langsmith/images/no-auth.png?w=2500&fit=max&auto=format&n=N1xJUsnxxRqnrjxV&q=85&s=a7b9157f67b3094370cc5399d917a164 2500w" />

## 2. Add authentication

Now that you have a base LangGraph app, add authentication to it.

<Note>
  In this tutorial, you will start with a hard-coded token for example purposes. You will get to a "production-ready" authentication scheme in the third tutorial.
</Note>

The [Auth](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth) object lets you register an authentication function that the LangSmith deployment will run on every request. This function receives each request and decides whether to accept or reject.

Create a new file `src/security/auth.py`. This is where your code will live to check if users are allowed to access your bot:

```python {highlight={10,15-16}} title="src/security/auth.py" theme={null}
from langgraph_sdk import Auth

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

The template gives us a placeholder LangGraph app. Try it out by installing the local dependencies and running the development server:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Set up feedback criteria

**URL:** llms-txt#set-up-feedback-criteria

**Contents:**
- Continuous feedback
- Categorical feedback

Source: https://docs.langchain.com/langsmith/set-up-feedback-criteria

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on tracing and feedback](/langsmith/observability-concepts)
  * [Reference guide on feedback data format](/langsmith/feedback-data-format)
</Tip>

Feedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback.

To set up a new feedback criteria, follow [this link](https://smith.langchain.com/settings/workspaces/feedbacks) to view all existing tags for your workspace, then click **New Tag**.

## Continuous feedback

For continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=44798176648f0a65e873fddecc90d43d" alt="Cont feedback" data-og-width="350" width="350" data-og-height="529" height="529" data-path="langsmith/images/cont-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4181c432230e33e7b6a7839e64729efa 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d11740b1d6f782cb551b6c8e7af92b50 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c966a6835ae5e2aaf3a320cf9bb71c74 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=d6ea2414a3f698cb66cd5f336f4bac51 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=442f373f36d5e8aa7dcf0e9685eb29f5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/cont-feedback.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8e8d44fd982322dd6865a25af561bc24 2500w" />

## Categorical feedback

For categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score.
Both the category label and the score will be logged as feedback in `value` and `score` fields, respectively.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6ec5030c3ba55b1fb12d60bca91719f7" alt="Cat feedback" data-og-width="470" width="470" data-og-height="465" height="465" data-path="langsmith/images/cat-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a11c14d2e7361e9aebc7d5997944f4c8 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d5e1dcf94730da4f7664092c4582410a 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0c18cd85595c56d9cf795fc07902db38 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ba75e7685d5cbd1918789301333038e2 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=311246a21107c2f66b130b720ec67121 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/cat-feedback.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=bc625d7b4686fb291f4bf795f8bd0f6e 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-feedback-criteria.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Set up hybrid LangSmith

**URL:** llms-txt#set-up-hybrid-langsmith

**Contents:**
- Kubernetes
  - Prerequisites
  - Setup
  - Configuring additional data planes in the same cluster
- Next steps

Source: https://docs.langchain.com/langsmith/deploy-hybrid

<Info>
  **Important**
  The Hybrid deployment option requires an [Enterprise](https://langchain.com/pricing) plan.
</Info>

The [**hybrid**](/langsmith/hybrid) model lets you run the [data plane](/langsmith/data-plane)—your Agent Server deployments and agent workloads—in your own cloud, while LangChain hosts and manages the [control plane](/langsmith/control-plane) (the LangSmith UI and orchestration). This setup gives you the flexibility of self-hosting your runtime environments with the convenience of a managed LangSmith instance.

The following steps describe how to connect your self-hosted data plane to the managed LangSmith control plane.

1. `KEDA` is installed on your cluster.
   
2. A valid `Ingress` controller is installed on your cluster. For more information about configuring ingress for your deployment, refer to [Create an ingress for installations](/langsmith/self-host-ingress). We highly recommend using the modern [Gateway API](/langsmith/self-host-ingress#option-2%3A-gateway-api) in a production setup.
3. If you plan to have the listener watch multiple namespaces, you **MUST** use the [Gateway API](/langsmith/self-host-ingress#option-2%3A-gateway-api) or an [Istio Gateway](/langsmith/self-host-ingress#option-3%3A-istio-gateway) instead of the [standard ingress](/langsmith/self-host-ingress#option-1%3A-standard-ingress) resource. A standard ingress resource can only route traffic to services in the same namespace, whereas a Gateway or Istio Gateway can route traffic to services across multiple namespaces.
4. You have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
5. You will need to enable egress to two control plane URLs. The listener polls these endpoints for deployments:
   * [https://api.host.langchain.com](https://api.host.langchain.com)
   * [https://api.smith.langchain.com](https://api.smith.langchain.com)

1. Provide your LangSmith organization ID to us. Your LangSmith organization will be configured to deploy the data plane in your cloud.
2. Create a listener from the LangSmith UI. The `Listener` data model is configured for the actual ["listener" application](/langsmith/data-plane#”listener”-application).
   1. In the left-hand navigation, select `Deployments` > `Listeners`.
   2. In the top-right of the page, select `+ Create Listener`.
   3. Enter a unique `Compute ID` for the listener. The `Compute ID` is a user-defined identifier that should be unique across all listeners in the current LangSmith workspace. The `Compute ID` is displayed to end users when they are creating a new deployment. Ensure that the `Compute ID` provides context to the end user about where their Agent Server deployments will be deployed to. For example, a `Compute ID` can be set to `k8s-cluster-name-dev-01`. In this example, the name of the Kubernetes cluster is `k8s-cluster-name`, `dev` denotes that the cluster is reserved for "development" workloads, and `01` is a numerical suffix to reduce naming collisions.
   4. Enter one or more Kubernetes namespaces. Later, the "listener" application will be configured to deploy to each of these namespaces.
   5. In the top-right on the page, select `Submit`.
   6. After the listener is created, copy the listener ID. You will use it later when installing the actual "listener" application in the Kubernetes cluster (step 5).
   <Info>
     **Important**
     Creating a listener from the LangSmith UI does not install the "listener" application in the Kubernetes cluster.
   </Info>
3. A [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langgraph-dataplane) is provided to install the necesssary components in your Kubernetes cluster.
   * `langgraph-dataplane-listener`: This is a service that listens to LangChain's [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs. This is the ["listener" application](/langsmith/data-plane#”listener”-application).
   * `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith Deployment.
   * `langgraph-dataplane-operator`: This operator handles changes to your LangSmith CRDs.
   * `langgraph-dataplane-redis`: A Redis instance is used by the `langgraph-dataplane-listener` to manage various tasks (mainly creating and deleting deployments).
4. Configure your `langgraph-dataplane-values.yaml` file.
   
   * `config.langsmithApiKey`: The `langgraph-listener` deployment authenticates with LangChain's LangGraph control plane API with the `langsmithApiKey`.
   * `config.langsmithWorkspaceId`: The `langgraph-listener` deployment is coupled to Agent Server deployments in the LangSmith workspace. In other words, the `langgraph-listener` deployment can only manage Agent Server deployments in the specified LangSmith workspace ID.
   * `config.langgraphListenerId`: In addition to being coupled with a LangSmith workspace, the `langgraph-listener` deployment is also coupled to a listener. When a new Agent Server deployment is created, it is automatically coupled to a `langgraphListenerId`. Specifying `langgraphListenerId` ensures that the `langgraph-listener` deployment can only manage Agent Server deployments that are coupled to `langgraphListenerId`.
   * `config.watchNamespaces`: A comma-separated list of Kubernetes namespaces that the `langgraph-listener` deployment will deploy to. This list should match the list of namespaces specified in step 2d.
   * `config.enableLGPDeploymentHealthCheck`: To disable the Agent Server health check, set this to `false`.
   * `ingress.hostname`: As part of the deployment workflow, the `langgraph-listener` deployment attempts to call the Agent Server health check endpoint (`GET /ok`) to verify that the application has started up correctly. A typical setup involves creating a shared DNS record or domain for Agent Server deployments. This is not managed by LangSmith. Once created, set `ingress.hostname` to the domain, which will be used to complete the health check.
   * `operator.createCRDs`: Set this value to `false` if the Kubernetes cluster already has the `LangGraphPlatform CRD` installed. During installation, an error will occur if the CRD is already installed. This situation may occur if multiple listeners are deployed on the same Kubernetes cluster.
5. Deploy `langgraph-dataplane` Helm chart.
   
6. If successful, you will see three services start up in your namespace.

Your hybrid infrastructure is now ready to create deployments.

### Configuring additional data planes in the same cluster

To create a data plane in a different namespace in the same cluster, repeat the above steps and pass a `-n` option to `helm upgrade` to specify a different namespace.

**When installing multiple data planes in the same cluster, it is very important to follow the rules below:**

1. The `config.watchNamespaces` list should never intersect with other installations `config.watchNamespaces`. For example, if installation A is watching namespaces `foo,bar`, installation B cannot watch either `foo` or `bar`. Multiple operators or listeners watching the same namespace will lead to unexpected behavior. This means that multiple LangSmith workspaces cannot deploy to the same namespace! Please review the [cluster organization](/langsmith/hybrid#kubernetes-cluster-organization) section to understand this better.
2. It is required to use the [Gateway API](/langsmith/self-host-ingress#option-2%3A-gateway-api) or an [Istio Gateway](/langsmith/self-host-ingress#option-3%3A-istio-gateway). Relying on the [standard ingress](/langsmith/self-host-ingress#option-1%3A-standard-ingress) resource can cause conflicts with Ingress objects created by other data planes in the same cluster. Because behavior in these cases depends on the specific ingress controller, this may result in unpredictable or undesired outcomes.

Once your infrastructure is set up, you're ready to deploy applications. See the deployment guides in the [Deployment tab](/langsmith/deployments) for instructions on building and deploying your applications.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-hybrid.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
2. A valid `Ingress` controller is installed on your cluster. For more information about configuring ingress for your deployment, refer to [Create an ingress for installations](/langsmith/self-host-ingress). We highly recommend using the modern [Gateway API](/langsmith/self-host-ingress#option-2%3A-gateway-api) in a production setup.
3. If you plan to have the listener watch multiple namespaces, you **MUST** use the [Gateway API](/langsmith/self-host-ingress#option-2%3A-gateway-api) or an [Istio Gateway](/langsmith/self-host-ingress#option-3%3A-istio-gateway) instead of the [standard ingress](/langsmith/self-host-ingress#option-1%3A-standard-ingress) resource. A standard ingress resource can only route traffic to services in the same namespace, whereas a Gateway or Istio Gateway can route traffic to services across multiple namespaces.
4. You have slack space in your cluster for multiple deployments. `Cluster-Autoscaler` is recommended to automatically provision new nodes.
5. You will need to enable egress to two control plane URLs. The listener polls these endpoints for deployments:
   * [https://api.host.langchain.com](https://api.host.langchain.com)
   * [https://api.smith.langchain.com](https://api.smith.langchain.com)

### Setup

1. Provide your LangSmith organization ID to us. Your LangSmith organization will be configured to deploy the data plane in your cloud.
2. Create a listener from the LangSmith UI. The `Listener` data model is configured for the actual ["listener" application](/langsmith/data-plane#”listener”-application).
   1. In the left-hand navigation, select `Deployments` > `Listeners`.
   2. In the top-right of the page, select `+ Create Listener`.
   3. Enter a unique `Compute ID` for the listener. The `Compute ID` is a user-defined identifier that should be unique across all listeners in the current LangSmith workspace. The `Compute ID` is displayed to end users when they are creating a new deployment. Ensure that the `Compute ID` provides context to the end user about where their Agent Server deployments will be deployed to. For example, a `Compute ID` can be set to `k8s-cluster-name-dev-01`. In this example, the name of the Kubernetes cluster is `k8s-cluster-name`, `dev` denotes that the cluster is reserved for "development" workloads, and `01` is a numerical suffix to reduce naming collisions.
   4. Enter one or more Kubernetes namespaces. Later, the "listener" application will be configured to deploy to each of these namespaces.
   5. In the top-right on the page, select `Submit`.
   6. After the listener is created, copy the listener ID. You will use it later when installing the actual "listener" application in the Kubernetes cluster (step 5).
   <Info>
     **Important**
     Creating a listener from the LangSmith UI does not install the "listener" application in the Kubernetes cluster.
   </Info>
3. A [Helm chart](https://github.com/langchain-ai/helm/tree/main/charts/langgraph-dataplane) is provided to install the necesssary components in your Kubernetes cluster.
   * `langgraph-dataplane-listener`: This is a service that listens to LangChain's [control plane](/langsmith/control-plane) for changes to your deployments and creates/updates downstream CRDs. This is the ["listener" application](/langsmith/data-plane#”listener”-application).
   * `LangGraphPlatform CRD`: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith Deployment.
   * `langgraph-dataplane-operator`: This operator handles changes to your LangSmith CRDs.
   * `langgraph-dataplane-redis`: A Redis instance is used by the `langgraph-dataplane-listener` to manage various tasks (mainly creating and deleting deployments).
4. Configure your `langgraph-dataplane-values.yaml` file.
```

Example 2 (unknown):
```unknown
* `config.langsmithApiKey`: The `langgraph-listener` deployment authenticates with LangChain's LangGraph control plane API with the `langsmithApiKey`.
   * `config.langsmithWorkspaceId`: The `langgraph-listener` deployment is coupled to Agent Server deployments in the LangSmith workspace. In other words, the `langgraph-listener` deployment can only manage Agent Server deployments in the specified LangSmith workspace ID.
   * `config.langgraphListenerId`: In addition to being coupled with a LangSmith workspace, the `langgraph-listener` deployment is also coupled to a listener. When a new Agent Server deployment is created, it is automatically coupled to a `langgraphListenerId`. Specifying `langgraphListenerId` ensures that the `langgraph-listener` deployment can only manage Agent Server deployments that are coupled to `langgraphListenerId`.
   * `config.watchNamespaces`: A comma-separated list of Kubernetes namespaces that the `langgraph-listener` deployment will deploy to. This list should match the list of namespaces specified in step 2d.
   * `config.enableLGPDeploymentHealthCheck`: To disable the Agent Server health check, set this to `false`.
   * `ingress.hostname`: As part of the deployment workflow, the `langgraph-listener` deployment attempts to call the Agent Server health check endpoint (`GET /ok`) to verify that the application has started up correctly. A typical setup involves creating a shared DNS record or domain for Agent Server deployments. This is not managed by LangSmith. Once created, set `ingress.hostname` to the domain, which will be used to complete the health check.
   * `operator.createCRDs`: Set this value to `false` if the Kubernetes cluster already has the `LangGraphPlatform CRD` installed. During installation, an error will occur if the CRD is already installed. This situation may occur if multiple listeners are deployed on the same Kubernetes cluster.
5. Deploy `langgraph-dataplane` Helm chart.
```

Example 3 (unknown):
```unknown
6. If successful, you will see three services start up in your namespace.
```

---

## Set up LangSmith

**URL:** llms-txt#set-up-langsmith

**Contents:**
- Choose how to set up LangSmith
  - Comparison
  - Related

Source: https://docs.langchain.com/langsmith/platform-setup

This section covers how to host and manage LangSmith infrastructure. You can set up LangSmith for [observability](/langsmith/observability), [evaluation](/langsmith/evaluation), and [prompt engineering](/langsmith/prompt-engineering), or use the full platform experience with [LangSmith Deployment](/langsmith/deployments) to also deploy and manage your applications through the UI.

<Callout icon="building" color="#2563EB" iconType="regular">
  **Start here if you're setting up or maintaining LangSmith infrastructure.**

If you want to deploy an agent application, the [Deployment section](/langsmith/deployments) covers application structure and deployment configuration.
</Callout>

## Choose how to set up LangSmith

You can deploy LangSmith in one of three modes:

* [**Cloud**](/langsmith/cloud): fully managed by LangChain
* [**Hybrid**](/langsmith/hybrid): LangChain manages the <Tooltip tip="The LangSmith UI and APIs for managing deployments.">control plane</Tooltip>; you host the <Tooltip tip="The runtime environment where your Agent Servers and agents execute.">data plane</Tooltip>
* [**Self-hosted**](/langsmith/self-hosted): you manage the full stack within your infrastructure

<Columns cols={3}>
  <Card title="Cloud" icon="cloud" iconType="solid" href="/langsmith/cloud" cta="Get started">
    Fully managed observability, evaluation, prompt engineering, and application deployment. Deploy from GitHub with automated CI/CD.
  </Card>

<Card title="Hybrid" icon="cloud" href="/langsmith/hybrid" cta="Set up Hybrid">
    **(Enterprise)** Observability, evaluation, prompt engineering, and application deployment with your applications running in your infrastructure.
  </Card>

<Card title="Self-hosted" icon="server" iconType="solid" href="/langsmith/self-hosted" cta="Run self-hosted">
    **(Enterprise)** Full control with observability, evaluation, and prompt engineering. Enable the full platform experience with LangSmith Deployment or run standalone servers.
  </Card>
</Columns>

Refer to the following table for a comparison:

| Feature                                        | **Cloud**                           | **Hybrid**                                                        | **Self-Hosted**                           |
| ---------------------------------------------- | ----------------------------------- | ----------------------------------------------------------------- | ----------------------------------------- |
| **Infrastructure location**                    | LangChain's cloud                   | Split: Control plane in LangChain cloud, data plane in your cloud | Your cloud                                |
| **Who manages updates**                        | LangChain                           | LangChain (control plane), You (data plane)                       | You                                       |
| **Who manages CI/CD for your apps**            | LangChain                           | You                                                               | You                                       |
| **Can deploy applications?**                   | ✅ Yes                               | ✅ Yes                                                             | ✅ Yes (with LangSmith Deployment enabled) |
| **Observability data location**                | LangChain cloud                     | LangChain cloud                                                   | Your cloud                                |
| **[Pricing](https://www.langchain.com/plans)** | Plus tier                           | Enterprise                                                        | Enterprise                                |
| **Best for**                                   | Quick setup, managed infrastructure | Data residency requirements + managed control plane               | Full control, data isolation              |

<Tip>
  You can [run an Agent Server locally for free](/langsmith/local-server) for testing and development.
</Tip>

* [Plans](https://langchain.com/pricing)
* [Pricing](https://www.langchain.com/plans)
* [Observability](/langsmith/observability)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/platform-setup.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Set up LangSmith tracing

**URL:** llms-txt#set-up-langsmith-tracing

def setup_langsmith():
    """Setup OpenTelemetry tracing to export spans to LangSmith."""
    endpoint = os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT")
    headers = os.getenv("OTEL_EXPORTER_OTLP_HEADERS")

if not endpoint or not headers:
        print("⚠️  Warning: OTEL environment variables not set. Tracing disabled.")
        return

# Create tracer provider with custom span processor
    trace_provider = TracerProvider()
    trace_provider.add_span_processor(LangSmithSpanProcessor())

# Set as LiveKit's tracer provider
    set_tracer_provider(trace_provider)
    print("✅ LangSmith tracing enabled")

---

## Set up online evaluators

**URL:** llms-txt#set-up-online-evaluators

**Contents:**
- View online evaluators
- Configure online evaluators
  - Configure a LLM-as-a-judge online evaluator
  - Configure a custom code evaluator
  - Video guide
- Configure multi-turn online evaluators
  - Prerequisites
  - Configuration
  - Limits
  - Troubleshooting

Source: https://docs.langchain.com/langsmith/online-evaluations

<Tip>
  **Recommended Reading**

Before diving into this content, it might be helpful to read the following:

* Running [online evaluations](/langsmith/evaluation-concepts#online-evaluation)
</Tip>

Online evaluations provide real-time feedback on your production traces. This is useful to continuously monitor the performance of your application—to identify issues, measure improvements, and ensure consistent quality over time.

There are two types of online evaluations supported in LangSmith:

* **[LLM-as-a-judge](/langsmith/evaluation-concepts#llm-as-judge)**: Use an LLM to evaluate traces as a scalable substitute for human-like judgment (e.g., toxicity, hallucinations, correctness). Supports two different levels of granularity:
  * **Run level**: Evaluate a single run.
  * [**Thread level**](/langsmith/online-evaluations#configure-multi-turn-online-evaluators): Evaluate all traces in a thread.
* **Custom Code**: Write an evaluator in Python directly in LangSmith. Often used for validating structure or statistical properties of your data.

<Note>When an online evaluator runs on any run within a trace, the trace will be auto-upgraded to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

## View online evaluators

Head to the **Tracing Projects** tab and select a tracing project. To view existing online evaluators for that project, click on the **Evaluators** tab.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=471b55b0d23b6c54ea5044406f0c55f7" alt="View online evaluators" data-og-width="1350" width="1350" data-og-height="639" height="639" data-path="langsmith/images/view-evaluators.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=141082993aba37d45550bfff9da502df 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=211cc6c5359e00ab23f0cf55bd67fd93 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fcdae1f3bce28bfcdd91059e43f9e1be 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=93b239efbd10f6ab5013e91b08384df6 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b6e496bee86cfb221cccde72366f83bb 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-evaluators.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=79817ff090124e1ec7b5f25eb2ddd978 2500w" />

## Configure online evaluators

#### 1. Navigate to online evaluators

Head to the **Tracing Projects** tab and select a tracing project. Click on **+ New** in the top right corner of the tracing project page, then click on **New Evaluator**. Select the evaluator you want to configure.

#### 2. Name your evaluator

#### 3. Create a filter

For example, you may want to apply specific evaluators based on:

* Runs where a [user left feedback](/langsmith/attach-user-feedback) indicating the response was unsatisfactory.
* Runs that invoke a specific tool call. See [filtering for tool calls](/langsmith/filter-traces-in-application#example-filtering-for-tool-calls) for more information.
* Runs that match a particular piece of metadata (e.g. if you log traces with a `plan_type` and only want to run evaluations on traces from your enterprise customers). See [adding metadata to your traces](/langsmith/add-metadata-tags) for more information.

Filters on evaluators work the same way as when you're filtering traces in a project. For more information on filters, you can refer to [this guide](./filter-traces-in-application).

<Tip>
  It's often helpful to inspect runs as you're creating a filter for your evaluator. With the evaluator configuration panel open, you can inspect runs and apply filters to them. Any filters you apply to the runs table will automatically be reflected in filters on your evaluator.
</Tip>

#### 4. (Optional) Configure a sampling rate

Configure a sampling rate to control the percentage of filtered runs that trigger the automation action. For example, to control costs, you may want to set a filter to only apply the evaluator to 10% of traces. In order to do this, you would set the sampling rate to 0.1.

#### 5. (Optional) Apply rule to past runs

Apply rule to past runs by toggling the **Apply to past runs** and entering a "Backfill from" date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately.

In order to track progress of the backfill, you can view logs for your evaluator by heading to the **Evaluators** tab within a tracing project and clicking the Logs button for the evaluator you created. Online evaluator logs are similar to [automation rule logs](./rules#view-logs-for-your-automations).

* Add an evaluator name
* Optionally filter runs that you would like to apply your evaluator on or configure a sampling rate.
* Select **Apply Evaluator**

#### 6. Select evaluator type

* Configuring [LLM-as-a-judge evaluators](/langsmith/online-evaluations#configure-a-llm-as-a-judge-online-evaluator)
* Configuring [custom code evaluators](/langsmith/online-evaluations#configure-a-custom-code-evaluator)

### Configure a LLM-as-a-judge online evaluator

View this guide to configure an [LLM-as-a-judge evaluator](/langsmith/llm-as-judge?mode=ui#pre-built-evaluators-1).

### Configure a custom code evaluator

Select **custom code** evaluator.

#### Write your evaluation function

<Note>
  **Custom code evaluators restrictions.**

**Allowed Libraries**: You can import all standard library functions, as well as the following public packages:

**Network Access**: You cannot access the internet from a custom code evaluator.
</Note>

Custom code evaluators must be written inline. We recommend testing locally before setting up your custom code evaluator in LangSmith.

In the UI, you will see a panel that lets you write your code inline, with some starter code:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cf7b75691edb3afaa10652a79813e581" alt="Online eval custom code" data-og-width="2910" width="2910" data-og-height="902" height="902" data-path="langsmith/images/online-eval-custom-code.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=acf6e6f3be5751c93a7287971fb18907 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5df847b9d1f8171120853834d5d12f38 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=17d028d3e0709087a8be7e31343f0ab0 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=668fd653028ebe056fed1e3963d3dc8e 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=62465285a785c577d0b0537c5ad307ca 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/online-eval-custom-code.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=7c342ba749987066e8ae05129e6a4fb7 2500w" />

Custom code evaluators take in one argument:

* A `Run` ([reference](/langsmith/run-data-format)). This represents the sampled run to evaluate.

They return a single value:

* Feedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example, `{"correctness": 1, "silliness": 0}` would create two types of feedback on the run, one saying it is correct, and the other saying it is not silly.

In the below screenshot, you can see an example of a simple function that validates that each run in the experiment has a known json field:

#### Test and save your evaluation function

Before saving, you can test your evaluator function on a recent run by clicking **Test Code** to make sure that your code executes properly.

Once you **Save**, your online evaluator will run over newly sampled runs (or backfilled ones too if you chose the backfill option).

If you prefer a video tutorial, check out the [Online Evaluations video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/z69cBXTJFZ0?si=GBKQ9_muHR1zllLl" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

## Configure multi-turn online evaluators

Multi-turn online evaluators allow you to evaluate entire conversations between a human and an agent — not just individual exchanges. They measure end-to-end interaction quality across all turns in a thread.

You can use multi-turn evaluations to measure:

1. Semantic Intent: What the user was trying to do.
2. Semantic Outcome: What actually happened, did the task succeed.
3. Trajectory: How the conversation unfolded, including trajectory of tool calls.

<Note> Running multi-turn online evals will auto-upgrade each trace within a thread to [extended data retention](/langsmith/administration-overview#data-retention-auto-upgrades). This upgrade will impact trace pricing, but ensures that traces meeting your evaluation criteria (typically those most valuable for analysis) are preserved for investigation. </Note>

* Your tracing project must be using [threads](/langsmith/threads).
* The top-level inputs and outputs of each trace in a thread must have a `messages` key that contains a list of messages. We support messages in [LangChain](/langsmith/log-llm-trace#messages-format), [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat/create), and [Anthropic Messages](https://platform.claude.com/docs/en/api/messages) formats.
  * If the top-level inputs and outputs of each trace only contain the latest message in the conversation, LangSmith will automatically combine messages across turns into a thread.
  * If the top-level inputs and outputs of each trace contain the full conversation history, LangSmith will use that directly.

<Note>
  If your traces don't follow the format above, thread level evaluators won't work. You’ll need to update how you trace to LangSmith to ensure each trace’s top-level inputs and outputs contain a list of `messages`.

Please refer to the [troubleshooting](#troubleshooting) section for more information.
</Note>

1. Navigate to the **Tracing Projects** tab and select a tracing project.
2. Click **+ New** in the top right corner of the tracing project page >  **New Evaluator** > **Evaluate a multi-turn thread**.
3. **Name your evaluator**.
4. **Apply filters or a sampling rate**. <br />
   Use filters or sampling to control evaluator cost. For example, evaluate only threads under *N* turns or sample 10% of all threads.
5. **Configure an idle time**. <br />
   The first time you configure a thread level evaluator, you’ll define the idle time — the amount of time after the last trace in a thread before it’s considered complete and ready for evaluation. This value should reflect the expected length of user interactions in your app. It applies across all evaluators in the project.

<Tip>
  When first testing your evaluator, use a short idle time so you can see results quickly. Once validated, increase it to match the expected length of user interactions.
</Tip>

6. **Configure your model.**<br />
   Select the provider and model you want to use for your evaluator. Threads tend to get long, so you should use a model with a higher context window in order to avoid running into limits. For example, OpenAI's GPT-4.1 mini or Gemini 2.5 Flash are good options as they both have 1M+ token context windows.

7. **Configure your LLM-as-a-judge prompt.**<br />
   Define what you want to evaluate. This prompt will be used to evaluate the thread. You can also configure which parts of the `messages` list are passed to the evaluator to control the content it receives:
   * All messages: Send the full message list.
   * Human and AI pairs: Send only user and assistant messages (excluding system messages, tool calls, etc.).
   * First human and last AI: Send only the first user message and the last assistant reply.

8. **Set up your feedback configuration**.<br />
   Configure a name for the feedback key, the format for the feedback you want to collect and optionally enable reasoning on the feedback.

<Warning>
  We don't recommend using the same feedback key for a thread-level evaluator and a run-level evaluator as it can be hard to distinguish between the two.
</Warning>

8. **Save your evaluator.**

After saving, your evaluator will appear in the **Evaluators** tab. You can test it once the idle time has passed for any new threads created after saving.

These are the current limits for multi-turn online evaluators (subject to change). Please reach out if you are running into any of these limits.

* **Runs must be less than one week old**: When a thread becomes idle, only runs within the past 7 days are eligible for evaluation.
* **Maximum of 500 threads evaluated at once**: If you have more than 500 threads marked as idle in a five minute period, we will automatically sample beyond 500.
* **Maximum of 10 multi-turn online evaluators per workspace**

**Checking the status of your evaluator** <br />
You can check when your evaluator was last run by heading to the **Evaluators** tab within a tracing project and clicking the **Logs** button for the evaluator you created to view its run history.

**Inspect the data sent to the evaluator** <br />
Inspect the data sent to the evaluator by heading to the **Evaluators** tab within a tracing project, clicking on the evaluator you created and clicking the **Evaluator traces** tab.

In this tab, you can see the inputs passed into the LLM-as-a-judge evaluator. If your messages are not being passed in correctly, you will see blank values in the inputs. This can happen if your messages are not formatted in one of [the expected formats](/langsmith/online-evaluations#prerequisites).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/online-evaluations.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
numpy (v2.2.2): "numpy"
  pandas (v1.5.2): "pandas"
  jsonschema (v4.21.1): "jsonschema"
  scipy (v1.14.1): "scipy"
  sklearn (v1.26.4): "scikit-learn"
```

Example 2 (unknown):
```unknown

```

---

## Set up OpenTelemetry trace provider

**URL:** llms-txt#set-up-opentelemetry-trace-provider

provider = TracerProvider()
otlp_exporter = OTLPSpanExporter(
    endpoint="https://api.smith.langchain.com/otel/v1/traces",
    headers={"x-api-key": os.getenv("LANGSMITH_API_KEY"), "Langsmith-Project": "my_project"}
)
processor = BatchSpanProcessor(otlp_exporter)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)

---

## Set up resource tags

**URL:** llms-txt#set-up-resource-tags

**Contents:**
- Create a tag
- Assign a tag to a resource
- Delete a tag
- Filter resources by tags

Source: https://docs.langchain.com/langsmith/set-up-resource-tags

<Check>
  Before diving into this content, it might be helpful to read the following:

* [Conceptual guide on organizations and workspaces](/langsmith/administration-overview)
</Check>

<Info>
  Resource tags are available for Plus and Enterprise plans.
</Info>

While workspaces help separate trust boundaries and access control, tags help you organize resources within a workspace. Tags are key-value pairs that you can attach to resources.

<Note>
  **Not to be confused with commit tags**: Resource tags are key-value pairs used to organize and filter workspace resources (projects, datasets, prompts, etc.). [Commit tags](/langsmith/manage-prompts#commit-tags) are labels that reference specific versions in a prompt's commit history. While both types of tags can use similar terminology (like `prod` or `staging`), resource tags help you *organize resources* across your workspace, while commit tags control *which version* of a prompt is used in your code.
</Note>

To create a tag, head to the workspace settings and click on the "Resource Tags" tab. Here, you'll be able to see the existing tag values, grouped by key. Two keys `Application` and `Environment` are created by default.

To create a new tag, click on the "New Tag" button. You'll be prompted to enter a key and a value for the tag. Note that you can use an existing key or create a new one.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=7482c51de04fbfa54731159c9f44c4e7" alt="Create tag" data-og-width="1460" width="1460" data-og-height="1268" height="1268" data-path="langsmith/images/create-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5a5a406729553197c361b802663af22f 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5618e46f4895c858a234a62b2222b5d7 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0dc3a636385e4c0502d23c50013696d8 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=a138f20d0755211084e7352a6aa98dbc 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=0255e6c9c526c32ea01e46b80dde19c7 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=e35a3c90d4bd9584e773e2bd1cb28294 2500w" />

## Assign a tag to a resource

Within the same side panel for creating a new tag, you can also create assign resources to tags. Search for corresponding resources in the "Assign Resources" section and select the resources you want to tag.

<Note>
  You can only tag workspace-scoped resources with resource tags. This includes Tracing Projects, Annotation Queues, Deployments, Experiments, Datasets, and Prompts.
</Note>

You can also assign tags to resources from the resource's detail page. Click on the Resource tags button to open up the tag panel and assign tags.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=95e638f9d51af94017b4b9f077c319b4" alt="Assign tag" data-og-width="1460" width="1460" data-og-height="607" height="607" data-path="langsmith/images/assign-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6941ebf3198b8b959e23110c9605061e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=86c9dcba2b753beac4d229bf15c76aa2 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5997def1f1c4294ffa4152f87edf2579 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=09645f720dc1d2ba3484194d9a0876ee 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3bbee72af92ff7cbe1617ba6686603ce 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-tag.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d2833a4f8ef5aec89218ad22d619de7c 2500w" />

To un-assign a tag from a resource, click on the Trash icon next to the tag, both in the tag panel and the resource tag panel.

You can delete either a key or a value of a tag from the [workspace settings page](https://smith.langchain.com/settings/workspaces/resource_tags). To delete a key, click on the Trash icon next to the key. To delete a value, click on the Trash icon next to the value.

Note that if you delete a key, all values associated with that key will also be deleted. When you delete a value, you will lose all associations between that value and resources.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=545b2cd5178bd4da31db56cde48f9f12" alt="Delete tag" data-og-width="1175" width="1175" data-og-height="1030" height="1030" data-path="langsmith/images/delete-tag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=059964ddd0ec5fffbe63bdf775dbd114 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=4b2fd607ef48d43555ec30f3689075d9 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=fe5f0a9b4c6695f9b1041e692bdf1799 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6012f1f83a031d5d84f2921f28f3ae4c 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=485c5e41e66ef679ebed451e291ce3ca 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/delete-tag.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c172ee7ff2edc13aaead3fa392de80ef 2500w" />

## Filter resources by tags

You can use resource tags to organize your experience navigating resources in the workspace.

To filter resources by tags in your workspace, open up the left-hand side panel and click on the tags icon. Here, you can select the tags you want to filter by.

In the homepage, you can see updated counts for resources based on the tags you've selected.

As you navigate through the different product surfaces, you will *only* see resources that match the tags you've selected. At any time, you can clear the tags to see all resources in the workspace or select different tags to filter by.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=b71f3118eb60146b576a02d2b7ddb8cd" alt="Filter by tags" data-og-width="1459" width="1459" data-og-height="1265" height="1265" data-path="langsmith/images/filter-by-tags.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=30edd76e20e69234d15345fc94520d79 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=154f682c20bf3a50cf3773c6f0e4550f 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=21bb41e7cb0c4984ea98f07adcefcc09 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e544fd1d74706c9bc40f72a3a20edd2c 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=bb90fb78c5e6b646f3a77e3175aef80f 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filter-by-tags.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=32da0bb71893aedb79ab0b00cc028f3e 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/set-up-resource-tags.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Set up SSO with OAuth2.0 and OIDC

**URL:** llms-txt#set-up-sso-with-oauth2.0-and-oidc

**Contents:**
- Overview
- With Client Secret (Recommended)
  - Prerequisites
  - Configuration
  - Session length controls
  - Override Sub Claim
  - Google Workspace IdP setup
  - Okta IdP setup
- Without Client Secret (PKCE) (Deprecated)
  - Requirements

Source: https://docs.langchain.com/langsmith/self-host-sso

LangSmith Self-Hosted provides SSO via OAuth2.0 and OIDC. This will delegate authentication to your Identity Provider (IdP) to manage access to LangSmith.

Our implementation supports almost anything that is OIDC compliant, with a few exceptions. Once configured, you will see a login screen like this:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=bbe6447424ea3e97a486b67d21cd4f6b" alt="LangSmith UI with OAuth SSO" data-og-width="1596" width="1596" data-og-height="994" height="994" data-path="langsmith/images/langsmith-ui-sso.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=6046e421d56b069227060520ecf3c2a8 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=03fc4a6909af9f15dca14af7e3891cda 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=53693db675b2bdd6cefd7e9b8b605af6 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=4b2135a1a76bf2e34eb914b8b00f58dc 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=78141a4d748d77bc1cceede89e62087d 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langsmith-ui-sso.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8532b1891c98046f171f1bded2523f3a 2500w" />

<Note>
  You may upgrade a [basic auth](/langsmith/self-host-basic-auth) installation to this mode, but not a [none auth](/langsmith/authentication-methods#none) installation. In order to upgrade, simply remove the basic auth configuration and add the required configuration parameters as shown below. Users may then login via OAuth *only*. **In order to maintain access post-upgrade, you must have access to login via OAuth using an email address that previously logged in via basic auth.**
</Note>

<Warning>
  LangSmith does not support moving from SSO to basic auth mode in self-hosted at the moment. We also do not support moving from OAuth Mode with client secret to OAuth mode without a client secret and vice versa. Finally, we do not support having both basic auth and OAuth at the same time. Ensure you disable the basic auth configuration when enabling OAuth.
</Warning>

## With Client Secret (Recommended)

By default, LangSmith Self-Hosted supports the `Authorization Code` flow with `Client Secret`. In this version of the flow, your client secret is stored security in LangSmith (not on the frontend) and used for authentication and establishing auth sessions.

* You must be self-hosted and on an Enterprise plan.
* Your IdP must support the `Authorization Code` flow with `Client Secret`.
* Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.
* You must provide the `OIDC`, `email`, and `profile` scopes to LangSmith. We use these to fetch the necessary user information and email for your users.

<Note>
  LangSmith SSO is only supported over `https`.
</Note>

* You will need to set the callback URL in your IdP to `https://<host>/api/v1/oauth/custom-oidc/callback`, where `host` is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.
* You will need to provide the `oauthClientId`, `oauthClientSecret`, `hostname`, and `oauthIssuerUrl` in your `values.yaml` file. This is where you will configure your LangSmith instance.
* If you have **not** already configured Oauth with client secret or if you only have personal orgs, you must provide an email address to assign as the initial org admin for the newly provisioned SSO org. If you are upgrading from basic auth, your existing org will be reused instead.

### Session length controls

<Note>
  All of the environment variables in this section are for the `platform-backend` service and can be added using `platformBackend.deployment.extraEnv` in Helm.
</Note>

* By default, session length is controlled by the expiration of the identity token returned by the identity provider
* Most setups should use refresh tokens to enable session length extension beyond the identity token expiration up to `OAUTH_SESSION_MAX_SEC`, which may require including the `offline_access` scope by adding to `oauthScopes` (Helm) or `OAUTH_SCOPES` (Docker)
* `OAUTH_SESSION_MAX_SEC` (default 1 day) can be overridden to a maximum of one week (`604800`)
* For identity provider setups that don't support refresh tokens, setting `OAUTH_OVERRIDE_TOKEN_EXPIRY="true"` will take `OAUTH_SESSION_MAX_SEC` as the session length, ignoring the identity token expiration

### Override Sub Claim

In some scenarios, it may be necessary to override which claim is used as the `sub` claim from your identity provider.
For example, in SCIM, the resolved `sub` claim and SCIM `externalId` must match in order for login to succeed.
If there are restrictions on the source attribute of the `sub` claim and/or the SCIM `externalId`, set the `ISSUER_SUB_CLAIM_OVERRIDES` environment variable to select which OIDC JWT claim is used as the `sub`.

If an issuer URL **starts with** one of the URLs in this configuration, the `sub` claim is taken from the field name specified.
For example, with the following configuration, a token with the issuer `https://idp.yourdomain.com/application/uuid` would use the `customClaim` value as the `sub`:

If unset, the default value for this configuration uses the `oid` claim when Azure Entra ID is used as the identity provider:

### Google Workspace IdP setup

You can use Google Workspace as a single sign-on (SSO) provider using [OAuth2.0 and OIDC](https://developers.google.com/identity/openid-connect/openid-connect) without PKCE.

<Note>
  You must have administrator-level access to your organization's Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen.
</Note>

1. Create a new GCP project, see the Google documentation topic [creating and managing projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

2. After you have created the project, open the [Credentials](https://console.developers.google.com/apis/credentials) page in the Google API Console (making sure the project in the top left corner is correct)

3. Create new credentials: `Create Credentials → OAuth client ID`

4. Choose `Web application` as the `Application type` and enter a name for the application e.g. `LangSmith`

5. In `Authorized Javascript origins` put the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com`

6. In `Authorized redirect URIs` put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback` e.g. `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`

7. Click `Create`, then download the JSON or copy and save the `Client ID` (ends with `.apps.googleusercontent.com`) and `Client secret` somewhere secure. **You will be able to access these later if needed**.

8. Select `OAuth consent screen` from the navigation menu on the left

1. Choose the Application type as `Internal`. **If you select `Public`, anyone with a Google account can sign in.**
   2. Enter a descriptive `Application name`. This name is shown to users on the consent screen when they sign in. For example, use `LangSmith` or `<organization_name> SSO for LangSmith`.
   3. Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data.

9. (Optional) control who within your organization has access to LangSmith: [https://admin.google.com/ac/owl/list?tab=configuredApps](https://admin.google.com/ac/owl/list?tab=configuredApps). See [Google's documentation](https://support.google.com/a/answer/7281227?hl=en\&fl=1\&sjid=9554153972856467090-NA) for additional details.

10. Configure LangSmith to use this OAuth application. For examples, here are the `config`values that would be used for Kubernetes configuration:

1. `oauthClientId`: `Client ID` (ends with `.apps.googleusercontent.com`)
    2. `oauthClientSecret`: `Client secret`
    3. `hostname`: the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com` (no trailing slash)
    4. `oauthIssuerUrl`: `https://accounts.google.com`
    5. `oauth.enabled`: `true`
    6. `authType`: `mixed`

#### Supported features

* IdP-initiated SSO
* SP-initiated SSO
* Just-In-Time provisioning

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_oidc.htm).
If you have any questions or issues, please contact support via [support.langchain.com](https://support.langchain.com).

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

<Info>For details on SCIM setup, refer to [Set up SCIM for your organization](/langsmith/user-management#set-up-scim-for-your-organization).</Info>

<Note>
  This method of configuration is required in order to use SCIM with Okta.
</Note>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Fill in `ApiUrlBase`:
   * Your LangSmith API URL **without the protocol** (`https://`) formatted as `<langsmith_domain>/api/v1`, e.g., `langsmith.yourdomain.com/api/v1`.
   * If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `langsmith.yourdomain.com/prefix/api/v1`.
7. Leave `AuthHost` empty.
8. (Optional, if planning to use [SCIM](/langsmith/user-management#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`: The `<langsmith_url>` portion from above, e.g., `langsmith.yourdomain.com`.
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `OpenID Connect`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`.
    * `Update application username on`: `Create and update`.
    * `Allow users to securely see their password`: leave **unchecked**.
13. Click **Save**.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

<Info>For details on SCIM setup, refer to [Set up SCIM for your organization](/langsmith/user-management#set-up-scim-for-your-organization).</Info>

<div id="via-okta-custom-app-integration">
  <b>Via Custom App Integration</b>
</div>

<Warning>
  SCIM is not compatible with this method of configuration. Refer to [**Via Okta Integration Network**](#via-okta-integration-network).
</Warning>

1. Log in to Okta as an administrator, and go to the **Okta Admin console**.
2. Under **Applications** > **Applications** click **Create App Integration**.
3. Select **OIDC - OpenID Connect** as the Sign-in method and **Web Application** as the Application type, then click **Next**.
4. Enter an `App integration name` (e.g., `LangSmith`).
5. Recommended: Check **Core grants > Refresh Token** (see [session length controls](#session-length-controls)).
6. In **Sign-in redirect URIs** put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback`, e.g., `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`. If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `https://langsmith.yourdomain.com/prefix/api/v1/oauth/custom-oidc/callback`.
7. Remove the default URI under **Sign-out redirect URIs**.
8. Under **Trusted Origins > Base URIs** add your langsmith URL with the protocol, e.g., `https://langsmith.yourdomain.com`.
9. Select your desired option under **Assignments > Controlled access**:
   * Allow everyone in your organization to access.
   * Limit access to selected groups.
   * Skip group assignment for now.
10. Click **Save**.
11. Under **Sign On > OpenID Connect ID Token** set **Issuer** to **Okta URL**.
12. (Optional) Under **General > Login** set **Login initiated by** to `Either Okta or App` to enable IdP-initiated login.
13. (Recommended) Under **General > Login > Email verification experience** fill in the **Callback URI** with the LangSmith URL, e.g., `https://langsmith.yourdomain.com`.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

#### SP-initiated SSO

Users can sign in using the **Login via SSO** button on the LangSmith homepage.

## Without Client Secret (PKCE) (Deprecated)

We recommend running with a `Client Secret` if possible (previously we didn't support this). However, if your IdP does not support this, you can use the `Authorization Code with PKCE` flow.

This flow does *not* require a `Client Secret`. For the alternative workflow, refer to [With client secret](#with-client-secret-recommended).

There are a couple of requirements for using OAuth SSO with LangSmith:

* Your IdP must support the `Authorization Code with PKCE` [flow](https://www.oauth.com/oauth2-servers/pkce) (Google does not support this flow for example, but see [above](#with-client-secret-recommended) for an alternative configuration that Google supports). This is often displayed in your OAuth Provider as configuring a "Single Page Application (SPA)"
* Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP.
* You must provide the `OIDC`, `email`, and `profile` scopes to LangSmith. We use these to fetch the necessary user information and email for your users.
* You will need to set the callback URL in your IdP to `http://<host>/oauth-callback`, where host is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated.
* You will need to provide the `oauthClientId` and `oauthIssuerUrl` in your `values.yaml` file. This is where you will configure your LangSmith instance.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-sso.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

### Session length controls

<Note>
  All of the environment variables in this section are for the `platform-backend` service and can be added using `platformBackend.deployment.extraEnv` in Helm.
</Note>

* By default, session length is controlled by the expiration of the identity token returned by the identity provider
* Most setups should use refresh tokens to enable session length extension beyond the identity token expiration up to `OAUTH_SESSION_MAX_SEC`, which may require including the `offline_access` scope by adding to `oauthScopes` (Helm) or `OAUTH_SCOPES` (Docker)
* `OAUTH_SESSION_MAX_SEC` (default 1 day) can be overridden to a maximum of one week (`604800`)
* For identity provider setups that don't support refresh tokens, setting `OAUTH_OVERRIDE_TOKEN_EXPIRY="true"` will take `OAUTH_SESSION_MAX_SEC` as the session length, ignoring the identity token expiration

### Override Sub Claim

In some scenarios, it may be necessary to override which claim is used as the `sub` claim from your identity provider.
For example, in SCIM, the resolved `sub` claim and SCIM `externalId` must match in order for login to succeed.
If there are restrictions on the source attribute of the `sub` claim and/or the SCIM `externalId`, set the `ISSUER_SUB_CLAIM_OVERRIDES` environment variable to select which OIDC JWT claim is used as the `sub`.

If an issuer URL **starts with** one of the URLs in this configuration, the `sub` claim is taken from the field name specified.
For example, with the following configuration, a token with the issuer `https://idp.yourdomain.com/application/uuid` would use the `customClaim` value as the `sub`:
```

Example 3 (unknown):
```unknown
If unset, the default value for this configuration uses the `oid` claim when Azure Entra ID is used as the identity provider:
```

Example 4 (unknown):
```unknown
### Google Workspace IdP setup

You can use Google Workspace as a single sign-on (SSO) provider using [OAuth2.0 and OIDC](https://developers.google.com/identity/openid-connect/openid-connect) without PKCE.

<Note>
  You must have administrator-level access to your organization's Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen.
</Note>

1. Create a new GCP project, see the Google documentation topic [creating and managing projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)

2. After you have created the project, open the [Credentials](https://console.developers.google.com/apis/credentials) page in the Google API Console (making sure the project in the top left corner is correct)

3. Create new credentials: `Create Credentials → OAuth client ID`

4. Choose `Web application` as the `Application type` and enter a name for the application e.g. `LangSmith`

5. In `Authorized Javascript origins` put the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com`

6. In `Authorized redirect URIs` put the domain of your LangSmith instance followed by `/api/v1/oauth/custom-oidc/callback` e.g. `https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback`

7. Click `Create`, then download the JSON or copy and save the `Client ID` (ends with `.apps.googleusercontent.com`) and `Client secret` somewhere secure. **You will be able to access these later if needed**.

8. Select `OAuth consent screen` from the navigation menu on the left

   1. Choose the Application type as `Internal`. **If you select `Public`, anyone with a Google account can sign in.**
   2. Enter a descriptive `Application name`. This name is shown to users on the consent screen when they sign in. For example, use `LangSmith` or `<organization_name> SSO for LangSmith`.
   3. Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data.

9. (Optional) control who within your organization has access to LangSmith: [https://admin.google.com/ac/owl/list?tab=configuredApps](https://admin.google.com/ac/owl/list?tab=configuredApps). See [Google's documentation](https://support.google.com/a/answer/7281227?hl=en\&fl=1\&sjid=9554153972856467090-NA) for additional details.

10. Configure LangSmith to use this OAuth application. For examples, here are the `config`values that would be used for Kubernetes configuration:

    1. `oauthClientId`: `Client ID` (ends with `.apps.googleusercontent.com`)
    2. `oauthClientSecret`: `Client secret`
    3. `hostname`: the domain of your LangSmith instance e.g. `https://langsmith.yourdomain.com` (no trailing slash)
    4. `oauthIssuerUrl`: `https://accounts.google.com`
    5. `oauth.enabled`: `true`
    6. `authType`: `mixed`

### Okta IdP setup

#### Supported features

* IdP-initiated SSO
* SP-initiated SSO
* Just-In-Time provisioning

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_oidc.htm).
If you have any questions or issues, please contact support via [support.langchain.com](https://support.langchain.com).

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

<Info>For details on SCIM setup, refer to [Set up SCIM for your organization](/langsmith/user-management#set-up-scim-for-your-organization).</Info>

<Note>
  This method of configuration is required in order to use SCIM with Okta.
</Note>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Fill in `ApiUrlBase`:
   * Your LangSmith API URL **without the protocol** (`https://`) formatted as `<langsmith_domain>/api/v1`, e.g., `langsmith.yourdomain.com/api/v1`.
   * If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., `langsmith.yourdomain.com/prefix/api/v1`.
7. Leave `AuthHost` empty.
8. (Optional, if planning to use [SCIM](/langsmith/user-management#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`: The `<langsmith_url>` portion from above, e.g., `langsmith.yourdomain.com`.
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `OpenID Connect`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`.
    * `Update application username on`: `Create and update`.
    * `Allow users to securely see their password`: leave **unchecked**.
13. Click **Save**.
14. Configure LangSmith to use this OAuth application (see [general configuration section](#configuration) for details about `initialOrgAdminEmail`):

<CodeGroup>
```

---

## Set up TracerProvider manually

**URL:** llms-txt#set-up-tracerprovider-manually

provider = TracerProvider()
trace.set_tracer_provider(provider)

---

## Share or unshare a trace publicly

**URL:** llms-txt#share-or-unshare-a-trace-publicly

Source: https://docs.langchain.com/langsmith/share-trace

<Warning>
  **Sharing a trace publicly will make it accessible to anyone with the link. Make sure you're not sharing sensitive information.**

If your self-hosted or hybrid LangSmith deployment is within a VPC, then the public link is accessible only to members authenticated within your VPC. For enhanced security, we recommend configuring your instance with a private URL accessible only to users with access to your network.
</Warning>

To share a trace publicly, simply click on the **Share** button in the upper right hand side of any trace view.
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f4d51afcb8b75809a08cf254b1797172" alt="Share trace" data-og-width="2011" width="2011" data-og-height="1005" height="1005" data-path="langsmith/images/share-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2580f397804e880fa5772dd5541347b3 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=d73de3d28cddf8585257cc5671218af4 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9495226170662b9eb0c270e2c9443210 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=5f6f3b2a45a50a6610dd16f591651a82 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ef2e10b3ae15f87d85bf8b5bde7f9e02 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-trace.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a2fb5e488e6b18113b6dd82457fc1720 2500w" />

This will open a dialog where you can copy the link to the trace.

Shared traces will be accessible to anyone with the link, even if they don't have a LangSmith account. They will be able to view the trace, but not edit it.

To "unshare" a trace, either:

1. Click on **Unshare** by clicking on **Public** in the upper right hand corner of any publicly shared trace, then **Unshare** in the dialog.
   <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=30504d6c7fe0ee5d3c6bf9b52a9c3d77" alt="Unshare trace" data-og-width="750" width="750" data-og-height="223" height="223" data-path="langsmith/images/unshare-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fd6850366fbdadfe8b60af3d675b7e7a 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f992e5a88562a93b88a0ce114452d426 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=322987d1e066b41d6c5eef49ad95f4a7 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=0a9ff17d3ebb1e7ab9c3777bc9eea051 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=be39ee08bd403a1c890687feaa3eb870 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=99898693ec9d411c396104fc8dc9196d 2500w" />

2. Navigate to your organization's list of publicly shared traces, by clicking on **Settings** -> **Shared URLs**, then click on **Unshare** next to the trace you want to unshare.
   <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e139222bbde3e2b9530e92164e0e1efe" alt="Unshare trace list share" data-og-width="2294" width="2294" data-og-height="1113" height="1113" data-path="langsmith/images/unshare-trace-list-share.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=fcd07bdf6a4968cefb9d13ab1c447e17 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5010be488c821b690b0c63b3eeb47e1c 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=aba6af3a7889f20a7b7c00ca01633bde 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=efb8fd990042f594a07dde88d61a434c 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=309427d94d2db88d43930b7de9b8ac65 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list-share.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2b708410fd513598eb49e354aae33571 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/share-trace.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## shortlived: "604800"  # 7 days (default is 14 days)

**URL:** llms-txt#shortlived:-"604800"--#-7-days-(default-is-14-days)

frontend:
  deployment:
    replicas: 4 # OR enable autoscaling to this level (example below)

---

## Show the workflow

**URL:** llms-txt#show-the-workflow

display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))

---

## Simple data processing using Functional API

**URL:** llms-txt#simple-data-processing-using-functional-api

@entrypoint()
def data_processor(raw_data: dict) -> dict:
    cleaned = clean_data(raw_data).result()
    transformed = transform_data(cleaned).result()
    return transformed

---

## Since this is **more specific** than both the generic @auth.on handler and the @auth.on.threads handler,

**URL:** llms-txt#since-this-is-**more-specific**-than-both-the-generic-@auth.on-handler-and-the-@auth.on.threads-handler,

---

## Since this is **more specific** than the generic @auth.on handler, it will take precedence

**URL:** llms-txt#since-this-is-**more-specific**-than-the-generic-@auth.on-handler,-it-will-take-precedence

---

## songs by "prince" and our DB records the artist as "Prince", ideally when we query our

**URL:** llms-txt#songs-by-"prince"-and-our-db-records-the-artist-as-"prince",-ideally-when-we-query-our

---

## so the conversation can be paused and resumed (as is needed for human review).

**URL:** llms-txt#so-the-conversation-can-be-paused-and-resumed-(as-is-needed-for-human-review).

config = {"configurable": {"thread_id": "some_id"}} # [!code highlight]

---

## Start local development server with Studio

**URL:** llms-txt#start-local-development-server-with-studio

**Contents:**
  - Method 1: LangSmith Deployment UI
  - Method 2: Control Plane API

langgraph dev
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
This will:

* Spin up a local server with Studio.
* Allow you to visualize and interact with your graph.
* Validate that your agent works correctly before deployment.

<Note>
  If your agent runs locally without any errors, it means that deployment to LangSmith will likely succeed. This local testing helps catch configuration issues, dependency problems, and agent logic errors before attempting deployment.
</Note>

See the [LangGraph CLI documentation](/langsmith/cli#dev) for more details.

### Method 1: LangSmith Deployment UI

Deploy your agent using the LangSmith deployment interface:

1. Go to your [LangSmith dashboard](https://smith.langchain.com).
2. Navigate to the **Deployments** section.
3. Click the **+ New Deployment** button in the top right.
4. Select your GitHub repository containing your LangGraph agent from the dropdown menu.

**Supported deployments:**

* <Icon icon="cloud" /> **Cloud LangSmith**: Direct GitHub integration with dropdown menu
* <Icon icon="server" /> **Self-Hosted/Hybrid LangSmith**: Specify your image URI in the Image Path field (e.g., `docker.io/username/my-agent:latest`)

<Info>
  **Benefits:**

  * Simple UI-based deployment
  * Direct integration with your GitHub repository (cloud)
  * No manual Docker image management required (cloud)
</Info>

### Method 2: Control Plane API

Deploy using the Control Plane API with different approaches for each deployment type:

**For Cloud LangSmith:**

* Use the Control Plane API to create deployments by pointing to your GitHub repository
* No Docker image building required for cloud deployments

**For Self-Hosted/Hybrid LangSmith:**
```

---

## Stateless runs

**URL:** llms-txt#stateless-runs

**Contents:**
- Setup
- Stateless streaming
- Waiting for stateless results

Source: https://docs.langchain.com/langsmith/stateless-runs

Most of the time, you provide a `thread_id` to your client when you run your graph in order to keep track of prior runs through the persistent state implemented in LangSmith Deployment. However, if you don't need to persist the runs you don't need to use the built-in persistent state and can create stateless runs.

First, let's setup our client:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Stateless streaming

We can stream the results of a stateless run in an almost identical fashion to how we stream from a run with the state attribute, but instead of passing a value to the `thread_id` parameter, we pass `None`:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Waiting for stateless results

In addition to streaming, you can also wait for a stateless result by using the `.wait` function like follows:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/stateless-runs.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

## Stateless streaming

We can stream the results of a stateless run in an almost identical fashion to how we stream from a run with the state attribute, but instead of passing a value to the `thread_id` parameter, we pass `None`:

<Tabs>
  <Tab title="Python">
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

---

## Step configuration: maps step name to (prompt, tools, required_state)

**URL:** llms-txt#step-configuration:-maps-step-name-to-(prompt,-tools,-required_state)

**Contents:**
- 4. Create step-based middleware
- 5. Create the agent

STEP_CONFIG = {
    "warranty_collector": {
        "prompt": WARRANTY_COLLECTOR_PROMPT,
        "tools": [record_warranty_status],
        "requires": [],
    },
    "issue_classifier": {
        "prompt": ISSUE_CLASSIFIER_PROMPT,
        "tools": [record_issue_type],
        "requires": ["warranty_status"],
    },
    "resolution_specialist": {
        "prompt": RESOLUTION_SPECIALIST_PROMPT,
        "tools": [provide_solution, escalate_to_human],
        "requires": ["warranty_status", "issue_type"],
    },
}
python  theme={null}
from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse
from typing import Callable

@wrap_model_call  # [!code highlight]
def apply_step_config(
    request: ModelRequest,
    handler: Callable[[ModelRequest], ModelResponse],
) -> ModelResponse:
    """Configure agent behavior based on the current step."""
    # Get current step (defaults to warranty_collector for first interaction)
    current_step = request.state.get("current_step", "warranty_collector")  # [!code highlight]

# Look up step configuration
    stage_config = STEP_CONFIG[current_step]  # [!code highlight]

# Validate required state exists
    for key in stage_config["requires"]:
        if request.state.get(key) is None:
            raise ValueError(f"{key} must be set before reaching {current_step}")

# Format prompt with state values (supports {warranty_status}, {issue_type}, etc.)
    system_prompt = stage_config["prompt"].format(**request.state)

# Inject system prompt and step-specific tools
    request = request.override(  # [!code highlight]
        system_prompt=system_prompt,  # [!code highlight]
        tools=stage_config["tools"],  # [!code highlight]
    )

return handler(request)
python  theme={null}
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver

**Examples:**

Example 1 (unknown):
```unknown
This dictionary-based configuration makes it easy to:

* See all steps at a glance
* Add new steps (just add another entry)
* Understand the workflow dependencies (`requires` field)
* Use prompt templates with state variables (e.g., `{warranty_status}`)

## 4. Create step-based middleware

Create middleware that reads `current_step` from state and applies the appropriate configuration. We'll use the `@wrap_model_call` decorator for a clean implementation:
```

Example 2 (unknown):
```unknown
This middleware:

1. **Reads current step**: Gets `current_step` from state (defaults to "warranty\_collector").
2. **Looks up configuration**: Finds the matching entry in `STEP_CONFIG`.
3. **Validates dependencies**: Ensures required state fields exist.
4. **Formats prompt**: Injects state values into the prompt template.
5. **Applies configuration**: Overrides the system prompt and available tools.

The `request.override()` method is key - it allows us to dynamically change the agent's behavior based on state without creating separate agent instances.

## 5. Create the agent

Now create the agent with the step-based middleware and a checkpointer for state persistence:
```

---

## Store or update an item.

**URL:** llms-txt#store-or-update-an-item.

Source: https://docs.langchain.com/langsmith/agent-server-api/store/store-or-update-an-item

langsmith/agent-server-openapi.json put /store/items

---

## Store persists embeddings to the local filesystem

**URL:** llms-txt#store-persists-embeddings-to-the-local-filesystem

---

## Store without embedding (still retrievable, but not searchable)

**URL:** llms-txt#store-without-embedding-(still-retrievable,-but-not-searchable)

**Contents:**
  - Using in LangGraph

store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {"system_info": "Last updated: 2024-01-01"},
    index=False
)
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

**Examples:**

Example 1 (unknown):
```unknown
### Using in LangGraph

With this all in place, we use the `in_memory_store` in LangGraph. The `in_memory_store` works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the `in_memory_store` allows us to store arbitrary information for access *across* threads. We compile the graph with both the checkpointer and the `in_memory_store` as follows.
```

---

## Store with specific fields to embed

**URL:** llms-txt#store-with-specific-fields-to-embed

store.put(
    namespace_for_memory,
    str(uuid.uuid4()),
    {
        "food_preference": "I love Italian cuisine",
        "context": "Discussing dinner plans"
    },
    index=["food_preference"]  # Only embed "food_preferences" field
)

---

## Streaming

**URL:** llms-txt#streaming

**Contents:**
- Supported stream modes
- Basic usage example
- Stream multiple modes
- Stream graph state
- Stream subgraph outputs
  - Debugging
- LLM tokens

Source: https://docs.langchain.com/oss/python/langgraph/streaming

LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

What's possible with LangGraph streaming:

* <Icon icon="share-nodes" size={16} /> [**Stream graph state**](#stream-graph-state) — get state updates / values with `updates` and `values` modes.
* <Icon icon="square-poll-horizontal" size={16} /> [**Stream subgraph outputs**](#stream-subgraph-outputs) — include outputs from both the parent graph and any nested subgraphs.
* <Icon icon="square-binary" size={16} /> [**Stream LLM tokens**](#messages) — capture token streams from anywhere: inside nodes, subgraphs, or tools.
* <Icon icon="table" size={16} /> [**Stream custom data**](#stream-custom-data) — send custom updates or progress signals directly from tool functions.
* <Icon icon="layer-plus" size={16} /> [**Use multiple streaming modes**](#stream-multiple-modes) — choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces).

## Supported stream modes

Pass one or more of the following stream modes as a list to the [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods:

| Mode       | Description                                                                                                                                                                         |
| ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `values`   | Streams the full value of the state after each step of the graph.                                                                                                                   |
| `updates`  | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |
| `custom`   | Streams custom data from inside your graph nodes.                                                                                                                                   |
| `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.                                                                                                |
| `debug`    | Streams as much information as possible throughout the execution of the graph.                                                                                                      |

## Basic usage example

LangGraph graphs expose the [`stream`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) (sync) and [`astream`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.astream) (async) methods to yield streamed outputs as iterators.

<Accordion title="Extended example: streaming updates">

## Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.

<Tabs>
  <Tab title="updates">
    Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

<Tab title="values">
    Use this to stream the **full state** of the graph after each step.

## Stream subgraph outputs

To include outputs from [subgraphs](/oss/python/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

The outputs will be streamed as tuples `(namespace, data)`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.

<Accordion title="Extended example: streaming from subgraphs">

**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.
</Accordion>

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

Use the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

The streamed output from [`messages` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:

* `message_chunk`: the token or message segment from the LLM.
* `metadata`: a dictionary containing details about the graph node and LLM invocation.

> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.

<Warning>
  **Manual config required for async in Python \< 3.11**
  When using Python \< 3.11 with async code, you must explicitly pass [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) to `ainvoke()` to enable proper streaming. See [Async with Python \< 3.11](#async) for details or upgrade to Python 3.11+.
</Warning>

```python  theme={null}
from dataclasses import dataclass

from langchain.chat_models import init_chat_model
from langgraph.graph import StateGraph, START

@dataclass
class MyState:
    topic: str
    joke: str = ""

model = init_chat_model(model="gpt-4o-mini")

def call_model(state: MyState):
    """Call the LLM to generate a joke about a topic"""
    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream
    model_response = model.invoke(  # [!code highlight]
        [
            {"role": "user", "content": f"Generate a joke about {state.topic}"}
        ]
    )
    return {"joke": model_response.content}

graph = (
    StateGraph(MyState)
    .add_node(call_model)
    .add_edge(START, "call_model")
    .compile()
)

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming updates">
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</Accordion>

## Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.
```

Example 4 (unknown):
```unknown
## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.
```

---

## Streaming API

**URL:** llms-txt#streaming-api

**Contents:**
- Basic usage
  - Supported stream modes
  - Stream multiple modes
- Stream graph state
  - Stream Mode: `updates`
  - Stream Mode: `values`
- Subgraphs
- Debugging
- LLM tokens
  - Filter LLM tokens

Source: https://docs.langchain.com/langsmith/streaming

[LangGraph SDK](/langsmith/langgraph-python-sdk) allows you to [stream outputs](/oss/python/langgraph/streaming/) from the [LangSmith Deployment API](/langsmith/server-api-ref).

<Note>
  LangGraph SDK and Agent Server are a part of [LangSmith](/langsmith/home).
</Note>

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    Create a thread:

Create a streaming run:

<Accordion title="Extended example: streaming updates">
  This is an example graph you can run in the Agent Server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.

Once you have a running Agent Server, you can interact with it using
  [LangGraph SDK](/langsmith/langgraph-python-sdk)

<Tabs>
    <Tab title="Python">

1. The `client.runs.stream()` method returns an iterator that yields streamed outputs.
         2\. Set `stream_mode="updates"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.
    </Tab>

<Tab title="JavaScript">

1. The `client.runs.stream()` method returns an iterator that yields streamed outputs.
      2. Set `streamMode: "updates"` to stream only the updates to the graph state after each node. Other stream modes are also available. See [supported stream modes](#supported-stream-modes) for details.
    </Tab>

<Tab title="cURL">
      Create a thread:

Create a streaming run:

### Supported stream modes

| Mode                             | Description                                                                                                                                                                         | LangGraph Library Method                                                                                      |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| [`values`](#stream-graph-state)  | Stream the full graph state after each [super-step](/langsmith/graph-rebuild#graphs).                                                                                               | `.stream()` / `.astream()` with [`stream_mode="values"`](/oss/python/langgraph/streaming#stream-graph-state)  |
| [`updates`](#stream-graph-state) | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. | `.stream()` / `.astream()` with [`stream_mode="updates"`](/oss/python/langgraph/streaming#stream-graph-state) |
| [`messages-tuple`](#messages)    | Streams LLM tokens and metadata for the graph node where the LLM is invoked (useful for chat apps).                                                                                 | `.stream()` / `.astream()` with [`stream_mode="messages"`](/oss/python/langgraph/streaming#messages)          |
| [`debug`](#debug)                | Streams as much information as possible throughout the execution of the graph.                                                                                                      | `.stream()` / `.astream()` with [`stream_mode="debug"`](/oss/python/langgraph/streaming#stream-graph-state)   |
| [`custom`](#stream-custom-data)  | Streams custom data from inside your graph                                                                                                                                          | `.stream()` / `.astream()` with [`stream_mode="custom"`](/oss/python/langgraph/streaming#stream-custom-data)  |
| [`events`](#stream-events)       | Stream all events (including the state of the graph); mainly useful when migrating large LCEL apps.                                                                                 | `.astream_events()`                                                                                           |

### Stream multiple modes

You can pass a list as the `stream_mode` parameter to stream multiple modes at once.

The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

## Stream graph state

Use the stream modes `updates` and `values` to stream the state of the graph as it executes.

* `updates` streams the **updates** to the state after each step of the graph.
* `values` streams the **full value** of the state after each step of the graph.

<Accordion title="Example graph">
  
</Accordion>

<Note>
  **Stateful runs**
  Examples below assume that you want to **persist the outputs** of a streaming run in the [checkpointer](/oss/python/langgraph/persistence) DB and have created a thread. To create a thread:

<Tabs>
    <Tab title="Python">
      
    </Tab>

<Tab title="JavaScript">
      
    </Tab>

<Tab title="cURL">
      
    </Tab>
  </Tabs>

If you don't need to persist the outputs of a run, you can pass `None` instead of `thread_id` when streaming.
</Note>

### Stream Mode: `updates`

Use this to stream only the **state updates** returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### Stream Mode: `values`

Use this to stream the **full state** of the graph after each step.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

To include outputs from [subgraphs](/oss/python/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.

1. Set `stream_subgraphs=True` to stream outputs from subgraphs.

<Accordion title="Extended example: streaming from subgraphs">
  This is an example graph you can run in the Agent Server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.

Once you have a running Agent Server, you can interact with it using
  [LangGraph SDK](/langsmith/langgraph-python-sdk)

<Tabs>
    <Tab title="Python">

1. Set `stream_subgraphs=True` to stream outputs from subgraphs.
    </Tab>

<Tab title="JavaScript">

1. Set `streamSubgraphs: true` to stream outputs from subgraphs.
    </Tab>

<Tab title="cURL">
      Create a thread:

Create a streaming run:

**Note** that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.
</Accordion>

Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

Use the `messages-tuple` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.

The streamed output from [`messages-tuple` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:

* `message_chunk`: the token or message segment from the LLM.
* `metadata`: a dictionary containing details about the graph node and LLM invocation.

<Accordion title="Example graph">

1. Note that the message events are emitted even when the LLM is run using `invoke` rather than `stream`.
</Accordion>

<Tabs>
  <Tab title="Python">

1. The "messages-tuple" stream mode returns an iterator of tuples `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
  </Tab>

<Tab title="JavaScript">

1. The "messages-tuple" stream mode returns an iterator of tuples `(message_chunk, metadata)` where `message_chunk` is the token streamed by the LLM and `metadata` is a dictionary with information about the graph node where the LLM was called and other information.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### Filter LLM tokens

* To filter the streamed tokens by LLM invocation, you can [associate `tags` with LLM invocations](/oss/python/langgraph/streaming#filter-by-llm-invocation).
* To stream tokens only from specific nodes, use `stream_mode="messages"` and [filter the outputs by the `langgraph_node` field](/oss/python/langgraph/streaming#filter-by-node) in the streamed metadata.

## Stream custom data

To send **custom user-defined data**:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

To stream all events, including the state of the graph:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

If you don't want to **persist the outputs** of a streaming run in the [checkpointer](/oss/python/langgraph/persistence) DB, you can create a stateless run without creating a thread:

<Tabs>
  <Tab title="Python">

1. We are passing `None` instead of a `thread_id` UUID.
  </Tab>

<Tab title="JavaScript">

1. We are passing `None` instead of a `thread_id` UUID.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

LangSmith allows you to join an active [background run](/langsmith/background-run) and stream outputs from it. To do so, you can use [LangGraph SDK's](/langsmith/langgraph-python-sdk) `client.runs.join_stream` method:

<Tabs>
  <Tab title="Python">

1. This is the `run_id` of an existing run you want to join.
  </Tab>

<Tab title="JavaScript">

1. This is the `run_id` of an existing run you want to join.
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

<Warning>
  **Outputs not buffered**
  When you use `.join_stream`, output is not buffered, so any output produced before joining will not be received.
</Warning>

For API usage and implementation, refer to the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/thread-runs/POST/threads/\{thread_id}/runs/stream).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/streaming.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="cURL">
    Create a thread:
```

Example 3 (unknown):
```unknown
Create a streaming run:
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

<Accordion title="Extended example: streaming updates">
  This is an example graph you can run in the Agent Server.
  See [LangSmith quickstart](/langsmith/deployment-quickstart) for more details.
```

---

## String content

**URL:** llms-txt#string-content

human_message = HumanMessage("Hello, how are you?")

---

## Studio troubleshooting

**URL:** llms-txt#studio-troubleshooting

**Contents:**
- Safari Connection Issues
  - Solution 1: Use Cloudflare Tunnel
  - Solution 2: Use Chromium browser
- Chrome connection issues
  - Symptoms
  - Solution: Allow local network access in Chrome
  - Additional troubleshooting
- Brave Connection Issues
  - Solution 1: Disable Brave Shields
  - Solution 2: Use Cloudflare Tunnel

Source: https://docs.langchain.com/langsmith/troubleshooting-studio

## Safari Connection Issues

Safari blocks plain-HTTP traffic on localhost. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>
</Tabs>

The command outputs a URL in this format:

Use this URL in Safari to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

### Solution 2: Use Chromium browser

Chrome and other Chromium browsers allow HTTP on localhost. Use `langgraph dev` without additional configuration.

## Chrome connection issues

Starting with Chrome version 142, you may experience "Failed to initialize Studio" errors with "TypeError: Failed to fetch" when trying to connect [LangSmith Studio](/langsmith/studio) to your local development server via [`langgraph dev`](/langsmith/cli). This occurs even when the API server at `http://127.0.0.1:2024/docs` loads successfully.

**Root Cause:** Chrome 142 fully enforces the Private Network Access (PNA) specification with no fallback, which blocks HTTPS sites (like `https://smith.langchain.com`) from accessing HTTP localhost servers by default.

* Running `langgraph dev` starts the server successfully.
* Navigating to `http://127.0.0.1:2024/docs` shows the API documentation correctly.
* LangSmith Studio at `https://smith.langchain.com` shows: "Failed to initialize Studio - Please verify if the API server is running or accessible from the browser. TypeError: Failed to fetch".
* Browser console shows errors like: `Permission was denied for this request to access the 'unknown' address space`.

### Solution: Allow local network access in Chrome

1. Open LangSmith Studio at `https://smith.langchain.com` in Chrome.
2. Click the **lock icon** (or site information icon) to the left of the address bar.
3. Look for the **"Local network access"** option in the dropdown.
4. Change the setting from **"Ask (default)"** or **"Block"** to **"Allow"**.
5. Reload the page.

Studio should now connect to your local development server successfully.

### Additional troubleshooting

**Check for browser extension conflicts**

Browser extensions (especially Ollama Chrome extension or AI model extensions) can interfere with localhost connections:

1. Disable all browser extensions temporarily.
2. Restart Chrome.
3. Try connecting to Studio again.
4. If it works, re-enable extensions one by one to identify the culprit.

**Verify dependencies are up to date**

**Clear browser cache and site data**

1. In Chrome, go to **Settings** > **Privacy and Security** > **Site Settings**.
2. Find `https://smith.langchain.com` in the list.
3. Click **Clear data**.
4. Restart Chrome and try again.

## Brave Connection Issues

Brave blocks plain-HTTP traffic on localhost when Brave Shields are enabled. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Disable Brave Shields

Disable Brave Shields for LangSmith using the Brave icon in the URL bar.

### Solution 2: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JS">
    
  </Tab>
</Tabs>

The command outputs a URL in this format:

Use this URL in Brave to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

Undefined conditional edges may show unexpected connections in your graph. This is
because without proper definition, Studio assumes the conditional edge could access all other nodes. To address this, explicitly define the routing paths using one of these methods:

### Solution 1: Path map

Define a mapping between router outputs and target nodes:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>
</Tabs>

### Solution 2: Router type definition

Specify possible routing destinations using Python's `Literal` type:

## Experiment troubleshooting in Studio

### **Run experiment** button is disabled

* **Deployed application**: If your application is deployed on LangSmith, you may need to create a new revision to enable this feature.
* **Local development server**: If you are running your application locally, make sure you have upgraded to the latest version of the `langgraph-cli` (`pip install -U langgraph-cli`). Additionally, ensure you have tracing enabled by setting the `LANGSMITH_API_KEY` in your project's `.env` file.

### Evaluator results are missing

When you run an experiment, any attached evaluators are scheduled for execution in a queue. If you don't see results immediately, it likely means they are still pending.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting-studio.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JS">
```

Example 2 (unknown):
```unknown
</Tab>
</Tabs>

The command outputs a URL in this format:
```

Example 3 (unknown):
```unknown
Use this URL in Safari to load Studio. Here, the `baseUrl` parameter specifies your agent server endpoint.

### Solution 2: Use Chromium browser

Chrome and other Chromium browsers allow HTTP on localhost. Use `langgraph dev` without additional configuration.

## Chrome connection issues

Starting with Chrome version 142, you may experience "Failed to initialize Studio" errors with "TypeError: Failed to fetch" when trying to connect [LangSmith Studio](/langsmith/studio) to your local development server via [`langgraph dev`](/langsmith/cli). This occurs even when the API server at `http://127.0.0.1:2024/docs` loads successfully.

**Root Cause:** Chrome 142 fully enforces the Private Network Access (PNA) specification with no fallback, which blocks HTTPS sites (like `https://smith.langchain.com`) from accessing HTTP localhost servers by default.

### Symptoms

* Running `langgraph dev` starts the server successfully.
* Navigating to `http://127.0.0.1:2024/docs` shows the API documentation correctly.
* LangSmith Studio at `https://smith.langchain.com` shows: "Failed to initialize Studio - Please verify if the API server is running or accessible from the browser. TypeError: Failed to fetch".
* Browser console shows errors like: `Permission was denied for this request to access the 'unknown' address space`.

### Solution: Allow local network access in Chrome

1. Open LangSmith Studio at `https://smith.langchain.com` in Chrome.
2. Click the **lock icon** (or site information icon) to the left of the address bar.
3. Look for the **"Local network access"** option in the dropdown.
4. Change the setting from **"Ask (default)"** or **"Block"** to **"Allow"**.
5. Reload the page.

Studio should now connect to your local development server successfully.

### Additional troubleshooting

**Check for browser extension conflicts**

Browser extensions (especially Ollama Chrome extension or AI model extensions) can interfere with localhost connections:

1. Disable all browser extensions temporarily.
2. Restart Chrome.
3. Try connecting to Studio again.
4. If it works, re-enable extensions one by one to identify the culprit.

**Verify dependencies are up to date**
```

Example 4 (unknown):
```unknown
**Clear browser cache and site data**

1. In Chrome, go to **Settings** > **Privacy and Security** > **Site Settings**.
2. Find `https://smith.langchain.com` in the list.
3. Click **Clear data**.
4. Restart Chrome and try again.

## Brave Connection Issues

Brave blocks plain-HTTP traffic on localhost when Brave Shields are enabled. When running Studio with `langgraph dev`, you may see "Failed to load assistants" errors.

### Solution 1: Disable Brave Shields

Disable Brave Shields for LangSmith using the Brave icon in the URL bar.

### Solution 2: Use Cloudflare Tunnel

<Tabs>
  <Tab title="Python">
```

---

## Subgraphs

**URL:** llms-txt#subgraphs

**Contents:**
- Setup
- Invoke a graph from a node

Source: https://docs.langchain.com/oss/python/langgraph/use-subgraphs

This guide explains the mechanics of using subgraphs. A subgraph is a [graph](/oss/python/langgraph/graph-api#graphs) that is used as a [node](/oss/python/langgraph/graph-api#nodes) in another graph.

Subgraphs are useful for:

* Building [multi-agent systems](/oss/python/langchain/multi-agent)
* Re-using a set of nodes in multiple graphs
* Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph

When adding subgraphs, you need to define how the parent graph and the subgraph communicate:

* [Invoke a graph from a node](#invoke-a-graph-from-a-node) — subgraphs are called from inside a node in the parent graph
* [Add a graph as a node](#add-a-graph-as-a-node) — a subgraph is added directly as a node in the parent and **shares [state keys](/oss/python/langgraph/graph-api#state)** with the parent

<Tip>
  **Set up LangSmith for LangGraph development**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).
</Tip>

## Invoke a graph from a node

A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](/oss/python/langchain/multi-agent) system.

If that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.

```python  theme={null}
from typing_extensions import TypedDict
from langgraph.graph.state import StateGraph, START

class SubgraphState(TypedDict):
    bar: str

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Tip>
  **Set up LangSmith for LangGraph development**
  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started [here](https://docs.smith.langchain.com).
</Tip>

## Invoke a graph from a node

A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have **completely different schemas** from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](/oss/python/langchain/multi-agent) system.

If that's the case for your application, you need to define a node **function that invokes the subgraph**. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.
```

---

## Subgraph

**URL:** llms-txt#subgraph

def subgraph_node_1(state: State):
    return {"foo": state["foo"] + "bar"}

subgraph_builder = StateGraph(State)
subgraph_builder.add_node(subgraph_node_1)
subgraph_builder.add_edge(START, "subgraph_node_1")
subgraph = subgraph_builder.compile()

---

## Subsequent calls use the cache

**URL:** llms-txt#subsequent-calls-use-the-cache

**Contents:**
- All embedding models

tic = time.time()
print(cached_embedder.embed_query("Hello, world!"))
print(f"Second call took: {time.time() - tic:.2f} seconds")
```

In production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see [stores integrations](/oss/python/integrations/stores/) for options.

## All embedding models

<Columns cols={3}>
  <Card title="Aleph Alpha" icon="link" href="/oss/python/integrations/text_embedding/aleph_alpha" arrow="true" cta="View guide" />

<Card title="Anyscale" icon="link" href="/oss/python/integrations/text_embedding/anyscale" arrow="true" cta="View guide" />

<Card title="Ascend" icon="link" href="/oss/python/integrations/text_embedding/ascend" arrow="true" cta="View guide" />

<Card title="AI/ML API" icon="link" href="/oss/python/integrations/text_embedding/aimlapi" arrow="true" cta="View guide" />

<Card title="AwaDB" icon="link" href="/oss/python/integrations/text_embedding/awadb" arrow="true" cta="View guide" />

<Card title="AzureOpenAI" icon="link" href="/oss/python/integrations/text_embedding/azure_openai" arrow="true" cta="View guide" />

<Card title="Baichuan Text Embeddings" icon="link" href="/oss/python/integrations/text_embedding/baichuan" arrow="true" cta="View guide" />

<Card title="Baidu Qianfan" icon="link" href="/oss/python/integrations/text_embedding/baidu_qianfan_endpoint" arrow="true" cta="View guide" />

<Card title="Baseten" icon="link" href="/oss/python/integrations/text_embedding/baseten" arrow="true" cta="View guide" />

<Card title="Bedrock" icon="link" href="/oss/python/integrations/text_embedding/bedrock" arrow="true" cta="View guide" />

<Card title="BGE on Hugging Face" icon="link" href="/oss/python/integrations/text_embedding/bge_huggingface" arrow="true" cta="View guide" />

<Card title="Bookend AI" icon="link" href="/oss/python/integrations/text_embedding/bookend" arrow="true" cta="View guide" />

<Card title="Clarifai" icon="link" href="/oss/python/integrations/text_embedding/clarifai" arrow="true" cta="View guide" />

<Card title="Cloudflare Workers AI" icon="link" href="/oss/python/integrations/text_embedding/cloudflare_workersai" arrow="true" cta="View guide" />

<Card title="Clova Embeddings" icon="link" href="/oss/python/integrations/text_embedding/clova" arrow="true" cta="View guide" />

<Card title="Cohere" icon="link" href="/oss/python/integrations/text_embedding/cohere" arrow="true" cta="View guide" />

<Card title="DashScope" icon="link" href="/oss/python/integrations/text_embedding/dashscope" arrow="true" cta="View guide" />

<Card title="Databricks" icon="link" href="/oss/python/integrations/text_embedding/databricks" arrow="true" cta="View guide" />

<Card title="DeepInfra" icon="link" href="/oss/python/integrations/text_embedding/deepinfra" arrow="true" cta="View guide" />

<Card title="EDEN AI" icon="link" href="/oss/python/integrations/text_embedding/edenai" arrow="true" cta="View guide" />

<Card title="Elasticsearch" icon="link" href="/oss/python/integrations/text_embedding/elasticsearch" arrow="true" cta="View guide" />

<Card title="Embaas" icon="link" href="/oss/python/integrations/text_embedding/embaas" arrow="true" cta="View guide" />

<Card title="Fake Embeddings" icon="link" href="/oss/python/integrations/text_embedding/fake" arrow="true" cta="View guide" />

<Card title="FastEmbed by Qdrant" icon="link" href="/oss/python/integrations/text_embedding/fastembed" arrow="true" cta="View guide" />

<Card title="Fireworks" icon="link" href="/oss/python/integrations/text_embedding/fireworks" arrow="true" cta="View guide" />

<Card title="Google Gemini" icon="link" href="/oss/python/integrations/text_embedding/google_generative_ai" arrow="true" cta="View guide" />

<Card title="Google Vertex AI" icon="link" href="/oss/python/integrations/text_embedding/google_vertex_ai" arrow="true" cta="View guide" />

<Card title="GPT4All" icon="link" href="/oss/python/integrations/text_embedding/gpt4all" arrow="true" cta="View guide" />

<Card title="Gradient" icon="link" href="/oss/python/integrations/text_embedding/gradient" arrow="true" cta="View guide" />

<Card title="GreenNode" icon="link" href="/oss/python/integrations/text_embedding/greennode" arrow="true" cta="View guide" />

<Card title="Hugging Face" icon="link" href="/oss/python/integrations/text_embedding/huggingfacehub" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/python/integrations/text_embedding/ibm_watsonx" arrow="true" cta="View guide" />

<Card title="Infinity" icon="link" href="/oss/python/integrations/text_embedding/infinity" arrow="true" cta="View guide" />

<Card title="Instruct Embeddings" icon="link" href="/oss/python/integrations/text_embedding/instruct_embeddings" arrow="true" cta="View guide" />

<Card title="IPEX-LLM CPU" icon="link" href="/oss/python/integrations/text_embedding/ipex_llm" arrow="true" cta="View guide" />

<Card title="IPEX-LLM GPU" icon="link" href="/oss/python/integrations/text_embedding/ipex_llm_gpu" arrow="true" cta="View guide" />

<Card title="Isaacus" icon="link" href="/oss/python/integrations/text_embedding/isaacus" arrow="true" cta="View guide" />

<Card title="Intel Extension for Transformers" icon="link" href="/oss/python/integrations/text_embedding/itrex" arrow="true" cta="View guide" />

<Card title="Jina" icon="link" href="/oss/python/integrations/text_embedding/jina" arrow="true" cta="View guide" />

<Card title="John Snow Labs" icon="link" href="/oss/python/integrations/text_embedding/johnsnowlabs_embedding" arrow="true" cta="View guide" />

<Card title="LASER" icon="link" href="/oss/python/integrations/text_embedding/laser" arrow="true" cta="View guide" />

<Card title="Lindorm" icon="link" href="/oss/python/integrations/text_embedding/lindorm" arrow="true" cta="View guide" />

<Card title="Llama.cpp" icon="link" href="/oss/python/integrations/text_embedding/llamacpp" arrow="true" cta="View guide" />

<Card title="LLMRails" icon="link" href="/oss/python/integrations/text_embedding/llm_rails" arrow="true" cta="View guide" />

<Card title="LocalAI" icon="link" href="/oss/python/integrations/text_embedding/localai" arrow="true" cta="View guide" />

<Card title="MiniMax" icon="link" href="/oss/python/integrations/text_embedding/minimax" arrow="true" cta="View guide" />

<Card title="MistralAI" icon="link" href="/oss/python/integrations/text_embedding/mistralai" arrow="true" cta="View guide" />

<Card title="Model2Vec" icon="link" href="/oss/python/integrations/text_embedding/model2vec" arrow="true" cta="View guide" />

<Card title="ModelScope" icon="link" href="/oss/python/integrations/text_embedding/modelscope_embedding" arrow="true" cta="View guide" />

<Card title="MosaicML" icon="link" href="/oss/python/integrations/text_embedding/mosaicml" arrow="true" cta="View guide" />

<Card title="Naver" icon="link" href="/oss/python/integrations/text_embedding/naver" arrow="true" cta="View guide" />

<Card title="Nebius" icon="link" href="/oss/python/integrations/text_embedding/nebius" arrow="true" cta="View guide" />

<Card title="Netmind" icon="link" href="/oss/python/integrations/text_embedding/netmind" arrow="true" cta="View guide" />

<Card title="NLP Cloud" icon="link" href="/oss/python/integrations/text_embedding/nlp_cloud" arrow="true" cta="View guide" />

<Card title="Nomic" icon="link" href="/oss/python/integrations/text_embedding/nomic" arrow="true" cta="View guide" />

<Card title="NVIDIA NIMs" icon="link" href="/oss/python/integrations/text_embedding/nvidia_ai_endpoints" arrow="true" cta="View guide" />

<Card title="Oracle Cloud Infrastructure" icon="link" href="/oss/python/integrations/text_embedding/oci_generative_ai" arrow="true" cta="View guide" />

<Card title="Ollama" icon="link" href="/oss/python/integrations/text_embedding/ollama" arrow="true" cta="View guide" />

<Card title="OpenClip" icon="link" href="/oss/python/integrations/text_embedding/open_clip" arrow="true" cta="View guide" />

<Card title="OpenAI" icon="link" href="/oss/python/integrations/text_embedding/openai" arrow="true" cta="View guide" />

<Card title="OpenVINO" icon="link" href="/oss/python/integrations/text_embedding/openvino" arrow="true" cta="View guide" />

<Card title="Optimum Intel" icon="link" href="/oss/python/integrations/text_embedding/optimum_intel" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/text_embedding/oracleai" arrow="true" cta="View guide" />

<Card title="OVHcloud" icon="link" href="/oss/python/integrations/text_embedding/ovhcloud" arrow="true" cta="View guide" />

<Card title="Pinecone Embeddings" icon="link" href="/oss/python/integrations/text_embedding/pinecone" arrow="true" cta="View guide" />

<Card title="PredictionGuard" icon="link" href="/oss/python/integrations/text_embedding/predictionguard" arrow="true" cta="View guide" />

<Card title="PremAI" icon="link" href="/oss/python/integrations/text_embedding/premai" arrow="true" cta="View guide" />

<Card title="SageMaker" icon="link" href="/oss/python/integrations/text_embedding/sagemaker-endpoint" arrow="true" cta="View guide" />

<Card title="SambaNova" icon="link" href="/oss/python/integrations/text_embedding/sambanova" arrow="true" cta="View guide" />

<Card title="Self Hosted" icon="link" href="/oss/python/integrations/text_embedding/self-hosted" arrow="true" cta="View guide" />

<Card title="Sentence Transformers" icon="link" href="/oss/python/integrations/text_embedding/sentence_transformers" arrow="true" cta="View guide" />

<Card title="Solar" icon="link" href="/oss/python/integrations/text_embedding/solar" arrow="true" cta="View guide" />

<Card title="SpaCy" icon="link" href="/oss/python/integrations/text_embedding/spacy_embedding" arrow="true" cta="View guide" />

<Card title="SparkLLM" icon="link" href="/oss/python/integrations/text_embedding/sparkllm" arrow="true" cta="View guide" />

<Card title="TensorFlow Hub" icon="link" href="/oss/python/integrations/text_embedding/tensorflowhub" arrow="true" cta="View guide" />

<Card title="Text Embeddings Inference" icon="link" href="/oss/python/integrations/text_embedding/text_embeddings_inference" arrow="true" cta="View guide" />

<Card title="TextEmbed" icon="link" href="/oss/python/integrations/text_embedding/textembed" arrow="true" cta="View guide" />

<Card title="Titan Takeoff" icon="link" href="/oss/python/integrations/text_embedding/titan_takeoff" arrow="true" cta="View guide" />

<Card title="Together AI" icon="link" href="/oss/python/integrations/text_embedding/together" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/text_embedding/upstage" arrow="true" cta="View guide" />

<Card title="Volc Engine" icon="link" href="/oss/python/integrations/text_embedding/volcengine" arrow="true" cta="View guide" />

<Card title="Voyage AI" icon="link" href="/oss/python/integrations/text_embedding/voyageai" arrow="true" cta="View guide" />

<Card title="Xinference" icon="link" href="/oss/python/integrations/text_embedding/xinference" arrow="true" cta="View guide" />

<Card title="YandexGPT" icon="link" href="/oss/python/integrations/text_embedding/yandex" arrow="true" cta="View guide" />

<Card title="ZhipuAI" icon="link" href="/oss/python/integrations/text_embedding/zhipuai" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text_embedding/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## System Metrics

**URL:** llms-txt#system-metrics

Source: https://docs.langchain.com/langsmith/agent-server-api/system/system-metrics

langsmith/agent-server-openapi.json get /metrics
Get system metrics in Prometheus or JSON format for monitoring and observability.

---

## Target function

**URL:** llms-txt#target-function

async def run_graph(inputs: dict) -> dict:
    """Run graph and track the trajectory it takes along with the final response."""
    result = await graph.ainvoke({"messages": [
        { "role": "user", "content": inputs['question']},
    ]}, config={"env": "test"})
    return {"response": result["followup"]}

---

## Target function for running the relevant step

**URL:** llms-txt#target-function-for-running-the-relevant-step

async def run_intent_classifier(inputs: dict) -> dict:
    # Note that we can access and run the intent_classifier node of our graph directly.
    command = await graph.nodes['intent_classifier'].ainvoke(inputs)
    return {"route": command.goto}

---

## Terminate Session

**URL:** llms-txt#terminate-session

Source: https://docs.langchain.com/langsmith/agent-server-api/mcp/terminate-session

langsmith/agent-server-openapi.json delete /mcp/
Implemented according to the Streamable HTTP Transport specification.
Terminate an MCP session. The server implementation is stateless, so this is a no-op.

---

## Test

**URL:** llms-txt#test

**Contents:**
- Prerequisites
- Getting started
- Testing individual nodes and edges
- Partial execution

Source: https://docs.langchain.com/oss/python/langgraph/test

After you've prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests.

Note that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out [this section](/oss/python/langchain/test/) that uses LangChain's built-in [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) instead.

First, make sure you have [`pytest`](https://docs.pytest.org/) installed:

Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.

The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my_key`:

## Testing individual nodes and edges

Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:

For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to [restructure these sections as subgraphs](/oss/python/langgraph/use-subgraphs), which you can invoke in isolation as normal.

However, if you do not wish to make changes to your agent graph's overall structure, you can use LangGraph's persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:

1. Compile your agent with a checkpointer (the in-memory checkpointer [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) will suffice for testing).
2. Call your agent's [`update_state`](/oss/python/langgraph/use-time-travel) method with an [`as_node`](/oss/python/langgraph/persistence#as-node) parameter set to the name of the node *before* the one you want to start your test.
3. Invoke your agent with the same `thread_id` you used to update the state and an `interrupt_after` parameter set to the name of the node you want to stop at.

Here's an example that executes only the second and third nodes in a linear graph:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/test.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Getting started

Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance.

The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my_key`:
```

Example 2 (unknown):
```unknown
## Testing individual nodes and edges

Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:
```

Example 3 (unknown):
```unknown
## Partial execution

For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to [restructure these sections as subgraphs](/oss/python/langgraph/use-subgraphs), which you can invoke in isolation as normal.

However, if you do not wish to make changes to your agent graph's overall structure, you can use LangGraph's persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:

1. Compile your agent with a checkpointer (the in-memory checkpointer [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) will suffice for testing).
2. Call your agent's [`update_state`](/oss/python/langgraph/use-time-travel) method with an [`as_node`](/oss/python/langgraph/persistence#as-node) parameter set to the name of the node *before* the one you want to start your test.
3. Invoke your agent with the same `thread_id` you used to update the state and an `interrupt_after` parameter set to the name of the node you want to stop at.

Here's an example that executes only the second and third nodes in a linear graph:
```

---

## Test multi-turn conversations

**URL:** llms-txt#test-multi-turn-conversations

**Contents:**
- From an existing run
- From a dataset
- Manually
- Next steps

Source: https://docs.langchain.com/langsmith/multiple-messages

This how-to guide walks you through the various ways you can set up the playground for multi-turn conversations, which will allow you to test different tool configurations and system prompts against longer threads of messages.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8d468f069fe0ee6eac2e95c8942990aa" alt="Multiturn diagram" data-og-width="963" width="963" data-og-height="552" height="552" data-path="langsmith/images/multiturn-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ee61b9e81315c0b78ca5de6edb93f303 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ae0c7a1b4e68b46ef29469a2dc04610e 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9ce886cc977da99e442bf0ffeb82b683 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b4365db9abb545cf507c7d3333404ffa 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fd3ff5634b263980b0f9b9d97756d2b8 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-diagram.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c28271d3bc0988362e988a64c1f1b396 2500w" />

## From an existing run

First, ensure you have properly [traced](/langsmith/observability) a multi-turn conversation, and then navigate to your tracing project. Once you get to your tracing project simply open the run, select the LLM call, and open it in the playground as follows:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-from-run.gif?s=b4918bc6c6fac9c71859d962495db053" alt="Multiturn from run" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-from-run.gif" data-optimize="true" data-opv="3" />

You can then edit the system prompt, tweak the tools and/or output schema and observe how the output of the multi-turn conversation changes.

Before starting, make sure you have [set up your dataset](/langsmith/manage-datasets-in-application). Since you want to evaluate multi-turn conversations, make sure there is a key in your inputs that contains a list of messages.

Once you have created your dataset, head to the playground and [load your dataset](/langsmith/manage-datasets-in-application#from-the-prompt-playground) to evaluate.

Then, add a messages list variable to your prompt, making sure to name it the same as the key in your inputs that contains the list of messages:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-from-dataset.gif?s=42e2f11a348f50a7d2a0c8b6630c57e9" alt="Multiturn from dataset" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-from-dataset.gif" data-optimize="true" data-opv="3" />

When you run your prompt, the messages from each example will be added as a list in place of the 'Messages List' variable.

There are two ways to manually create multi-turn conversations. The first way is by simply appending messages to the prompt:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-manual.gif?s=1278a29854a66ee3dec92cc6f5059da0" alt="Multiturn manual" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-manual.gif" data-optimize="true" data-opv="3" />

This is helpful for quick iteration, but is rigid since the multi-turn conversation is hardcoded. Instead, if you want your prompt to work with any multi-turn conversation you can add a 'Messages List' variable and add your multi-turn conversation there:

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multiturn-manual-list.gif?s=12458af5558482bdfe40855c3117c02b" alt="Multiturn manual list" data-og-width="1632" width="1632" data-og-height="1080" height="1080" data-path="langsmith/images/multiturn-manual-list.gif" data-optimize="true" data-opv="3" />

This allows you to just tweak the system prompt or the tools, while allowing any multi-turn conversation to take the place of the `Messages List` variable, allowing you to reuse this prompt across various runs.

Now that you know how to set up the playground for multi-turn interactions, you can either manually inspect and judge the outputs, or you can [add evaluators](/langsmith/code-evaluator) to classify results.

You can also read [these how-to guides](/langsmith/create-a-prompt) to learn more about how to use the playground to run evaluations.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/multiple-messages.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Test the graph with a valid input

**URL:** llms-txt#test-the-graph-with-a-valid-input

**Contents:**
- Add runtime configuration

graph.invoke({"a": "hello"})
python  theme={null}
try:
    graph.invoke({"a": 123})  # Should be a string
except Exception as e:
    print("An exception was raised because `a` is an integer rather than a string.")
    print(e)

An exception was raised because `a` is an integer rather than a string.
1 validation error for OverallState
a
  Input should be a valid string [type=string_type, input_value=123, input_type=int]
    For further information visit https://errors.pydantic.dev/2.9/v/string_type
python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel

class NestedModel(BaseModel):
      value: str

class ComplexState(BaseModel):
      text: str
      count: int
      nested: NestedModel

def process_node(state: ComplexState):
      # Node receives a validated Pydantic object
      print(f"Input state type: {type(state)}")
      print(f"Nested type: {type(state.nested)}")
      # Return a dictionary update
      return {"text": state.text + " processed", "count": state.count + 1}

# Build the graph
  builder = StateGraph(ComplexState)
  builder.add_node("process", process_node)
  builder.add_edge(START, "process")
  builder.add_edge("process", END)
  graph = builder.compile()

# Create a Pydantic instance for input
  input_state = ComplexState(text="hello", count=0, nested=NestedModel(value="test"))
  print(f"Input object type: {type(input_state)}")

# Invoke graph with a Pydantic instance
  result = graph.invoke(input_state)
  print(f"Output type: {type(result)}")
  print(f"Output content: {result}")

# Convert back to Pydantic model if needed
  output_model = ComplexState(**result)
  print(f"Converted back to Pydantic: {type(output_model)}")
  python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel

class CoercionExample(BaseModel):
      # Pydantic will coerce string numbers to integers
      number: int
      # Pydantic will parse string booleans to bool
      flag: bool

def inspect_node(state: CoercionExample):
      print(f"number: {state.number} (type: {type(state.number)})")
      print(f"flag: {state.flag} (type: {type(state.flag)})")
      return {}

builder = StateGraph(CoercionExample)
  builder.add_node("inspect", inspect_node)
  builder.add_edge(START, "inspect")
  builder.add_edge("inspect", END)
  graph = builder.compile()

# Demonstrate coercion with string inputs that will be converted
  result = graph.invoke({"number": "42", "flag": "true"})

# This would fail with a validation error
  try:
      graph.invoke({"number": "not-a-number", "flag": "true"})
  except Exception as e:
      print(f"\nExpected validation error: {e}")
  python  theme={null}
  from langgraph.graph import StateGraph, START, END
  from pydantic import BaseModel
  from langchain.messages import HumanMessage, AIMessage, AnyMessage
  from typing import List

class ChatState(BaseModel):
      messages: List[AnyMessage]
      context: str

def add_message(state: ChatState):
      return {"messages": state.messages + [AIMessage(content="Hello there!")]}

builder = StateGraph(ChatState)
  builder.add_node("add_message", add_message)
  builder.add_edge(START, "add_message")
  builder.add_edge("add_message", END)
  graph = builder.compile()

# Create input with a message
  initial_state = ChatState(
      messages=[HumanMessage(content="Hi")], context="Customer support chat"
  )

result = graph.invoke(initial_state)
  print(f"Output: {result}")

# Convert back to Pydantic model to see message types
  output_model = ChatState(**result)
  for i, msg in enumerate(output_model.messages):
      print(f"Message {i}: {type(msg).__name__} - {msg.content}")
  python  theme={null}
from langgraph.graph import END, StateGraph, START
from langgraph.runtime import Runtime
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown
Invoke the graph with an **invalid** input
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
See below for additional features of Pydantic model state:

<Accordion title="Serialization Behavior">
  When using Pydantic models as state schemas, it's important to understand how serialization works, especially when:

  * Passing Pydantic objects as inputs
  * Receiving outputs from the graph
  * Working with nested Pydantic models

  Let's see these behaviors in action.
```

Example 4 (unknown):
```unknown
</Accordion>

<Accordion title="Runtime Type Coercion">
  Pydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you're not aware of it.
```

---

## test write permissions

**URL:** llms-txt#test-write-permissions

**Contents:**
  - Monitoring Runs
  - Common Errors

touch ./test.txt
aws s3 --endpoint-url=<endpoint_url> cp ./test.txt s3://<bucket-name>/tmp/test.txt
```

You can monitor your runs using the [List Runs API](#list-runs-for-an-export). If this is a known error, this will be added to the `errors` field of the run.

Here are some common errors:

| Error                              | Description                                                                                                                                                                                                                                                                                              |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Access denied                      | The blob store credentials or bucket are not valid. This error occurs when the provided access key and secret key combination doesn't have the necessary permissions to access the specified bucket or perform the required operations.                                                                  |
| Bucket is not valid                | The specified blob store bucket is not valid. This error is thrown when the bucket doesn't exist or there is not enough access to perform writes on the bucket.                                                                                                                                          |
| Key ID you provided does not exist | The blob store credentials provided are not valid. This error occurs when the access key ID used for authentication is not a valid key.                                                                                                                                                                  |
| Invalid endpoint                   | The endpoint\_url provided is invalid. This error is raised when the specified endpoint is an invalid endpoint. Only S3 compatible endpoints are supported, for example `https://storage.googleapis.com` for GCS, `https://play.min.io` for minio, etc. If using AWS, you should omit the endpoint\_url. |

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/data-export.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Text block

**URL:** llms-txt#text-block

text_block = {
    "type": "text",
    "text": "Hello world",
}

---

## Text splitters

**URL:** llms-txt#text-splitters

**Contents:**
- Text structure-based
- Length-based
- Document structure-based

Source: https://docs.langchain.com/oss/python/integrations/splitters/index

**Text splitters** break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.

There are several strategies for splitting documents, each with its own advantages.

<Tip>
  For most use cases, start with the [`RecursiveCharacterTextSplitter`](/oss/python/integrations/splitters/recursive_text_splitter). It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.
</Tip>

## Text structure-based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain's `RecursiveCharacterTextSplitter` implements this concept:

* The [`RecursiveCharacterTextSplitter`](/oss/python/integrations/splitters/recursive_text_splitter) attempts to keep larger units (e.g., paragraphs) intact.
* If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).
* This process continues down to the word level if necessary.

**Available text splitters**:

* [Recursively split text](/oss/python/integrations/splitters/recursive_text_splitter)

An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting:

* Straightforward implementation
* Consistent chunk sizes
* Easily adaptable to different model requirements

Types of length-based splitting:

* Token-based: Splits text based on the number of tokens, which is useful when working with language models.
* Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

**Available text splitters**:

* [Split by tokens](/oss/python/integrations/splitters/split_by_token)
* [Split by characters](/oss/python/integrations/splitters/character_text_splitter)

## Document structure-based

Some documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:

* Preserves the logical organization of the document
* Maintains context within each chunk
* Can be more effective for downstream tasks like retrieval or summarization

Examples of structure-based splitting:

* Markdown: Split based on headers (e.g., `#`, `##`, `###`)
* HTML: Split using tags
* JSON: Split by object or array elements
* Code: Split by functions, classes, or logical blocks

**Available text splitters**:

* [Split Markdown](/oss/python/integrations/splitters/markdown_header_metadata_splitter)
* [Split JSON](/oss/python/integrations/splitters/recursive_json_splitter)
* [Split code](/oss/python/integrations/splitters/code_splitter)
* [Split HTML](/oss/python/integrations/splitters/split_html)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/integrations/splitters/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

**Text splitters** break large docs into smaller chunks that will be retrievable individually and fit within model context window limit.

There are several strategies for splitting documents, each with its own advantages.

<Tip>
  For most use cases, start with the [`RecursiveCharacterTextSplitter`](/oss/python/integrations/splitters/recursive_text_splitter). It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.
</Tip>

## Text structure-based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain's `RecursiveCharacterTextSplitter` implements this concept:

* The [`RecursiveCharacterTextSplitter`](/oss/python/integrations/splitters/recursive_text_splitter) attempts to keep larger units (e.g., paragraphs) intact.
* If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).
* This process continues down to the word level if necessary.

Example usage:
```

Example 3 (unknown):
```unknown
**Available text splitters**:

* [Recursively split text](/oss/python/integrations/splitters/recursive_text_splitter)

## Length-based

An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting:

* Straightforward implementation
* Consistent chunk sizes
* Easily adaptable to different model requirements

Types of length-based splitting:

* Token-based: Splits text based on the number of tokens, which is useful when working with language models.
* Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.

Example implementation using LangChain's `CharacterTextSplitter` with token-based splitting:
```

---

## Then, update the runs with their end times and any outputs

**URL:** llms-txt#then,-update-the-runs-with-their-end-times-and-any-outputs

child_run_update = {
    **child_run,
    "end_time": datetime.now(timezone.utc).isoformat(),
    "outputs": {"answer": "Paris is the capital of France."},
}

parent_run_update = {
    **parent_run,
    "end_time": datetime.now(timezone.utc).isoformat(),
    "outputs": {"summary": "Discussion about France, including its capital."},
}

patches = [parent_run_update, child_run_update]
batch_ingest_runs(api_url, api_key, patches=patches)

---

## The

**URL:** llms-txt#the

---

## The above chain will be traced as a child run of the traceable function

**URL:** llms-txt#the-above-chain-will-be-traced-as-a-child-run-of-the-traceable-function

**Contents:**
- Interoperability between LangChain.JS and LangSmith SDK
  - Tracing LangChain objects inside `traceable` (JS only)
  - Tracing LangChain child runs via `traceable` / RunTree API (JS only)

@traceable(
    tags=["openai", "chat"],
    metadata={"foo": "bar"}
)
def invoke_runnnable(question, context):
    result = chain.invoke({"question": question, "context": context})
    return "The response is: " + result

invoke_runnnable("Can you summarize this morning's meetings?", "During this morning's meeting, we solved all world conflict.")
typescript  theme={null}
import { ChatOpenAI } from "@langchain/openai";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { getLangchainCallbacks } from "langsmith/langchain";

const prompt = ChatPromptTemplate.fromMessages([
  [
    "system",
    "You are a helpful assistant. Please respond to the user's request only based on the given context.",
  ],
  ["user", "Question: {question}\nContext: {context}"],
]);

const model = new ChatOpenAI({ modelName: "gpt-4o-mini" });
const outputParser = new StringOutputParser();
const chain = prompt.pipe(model).pipe(outputParser);

const main = traceable(
  async (input: { question: string; context: string }) => {
    const callbacks = await getLangchainCallbacks();
    const response = await chain.invoke(input, { callbacks });
    return response;
  },
  { name: "main" }
);
typescript  theme={null}
import { traceable } from "langsmith/traceable";
import { RunnableLambda } from "@langchain/core/runnables";
import { RunnableConfig } from "@langchain/core/runnables";

const tracedChild = traceable((input: string) => `Child Run: ${input}`, {
  name: "Child Run",
});

const parrot = new RunnableLambda({
  func: async (input: { text: string }, config?: RunnableConfig) => {
    return await tracedChild(input.text);
  },
});
typescript Traceable theme={null}
  import { traceable } from "langsmith/traceable";
  import { RunnableLambda } from "@langchain/core/runnables";
  import { RunnableConfig } from "@langchain/core/runnables";

const tracedChild = traceable((input: string) => `Child Run: ${input}`, {
    name: "Child Run",
  });

const parrot = new RunnableLambda({
    func: async (input: { text: string }, config?: RunnableConfig) => {
      // Pass the config to existing traceable function
      await tracedChild(config, input.text);
      return input.text;
    },
  });
  typescript Run Tree theme={null}
  import { RunTree } from "langsmith/run_trees";
  import { RunnableLambda } from "@langchain/core/runnables";
  import { RunnableConfig } from "@langchain/core/runnables";

const parrot = new RunnableLambda({
    func: async (input: { text: string }, config?: RunnableConfig) => {
      // create the RunTree from the RunnableConfig of the RunnableLambda
      const childRunTree = RunTree.fromRunnableConfig(config, {
        name: "Child Run",
      });

childRunTree.inputs = { input: input.text };
      await childRunTree.postRun();

childRunTree.outputs = { output: `Child Run: ${input.text}` };
      await childRunTree.patchRun();

return input.text;
    },
  });
  ```
</CodeGroup>

If you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langchain.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
This will produce the following trace tree: <img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=52c64fd784522c4b2d75886ae76f8c18" alt="Trace tree python interop" data-og-width="1334" width="1334" data-og-height="734" height="734" data-path="langsmith/images/trace-tree-python-interop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=21a424e2326767bb66a6b5a207390bec 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=72f1854193fd30317d1b69d8de433d73 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=34fa74aff75c11172b350d38319bf276 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4ebfb4252764af54033e62ad088f60b1 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2d783eecf72ba0680e78a0f122bd4411 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-python-interop.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9d977793e5281e5d255d962224bd70df 2500w" />

## Interoperability between LangChain.JS and LangSmith SDK

### Tracing LangChain objects inside `traceable` (JS only)

Starting with `langchain@0.2.x`, LangChain objects are traced automatically when used inside `@traceable` functions, inheriting the client, tags, metadata and project name of the traceable function.

For older versions of LangChain below `0.2.x`, you will need to manually pass an instance `LangChainTracer` created from the tracing context found in `@traceable`.
```

Example 2 (unknown):
```unknown
### Tracing LangChain child runs via `traceable` / RunTree API (JS only)

<Note>
  We're working on improving the interoperability between `traceable` and LangChain. The following limitations are present when using combining LangChain with `traceable`:

  1. Mutating RunTree obtained from `getCurrentRunTree()` of the RunnableLambda context will result in a no-op.
  2. It's discouraged to traverse the RunTree obtained from RunnableLambda via `getCurrentRunTree()` as it may not contain all the RunTree nodes.
  3. Different child runs may have the same `execution_order` and `child_execution_order` value. Thus in extreme circumstances, some runs may end up in a different order, depending on the `start_time`.
</Note>

In some uses cases, you might want to run `traceable` functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the `RunTree` API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke `traceable`-wrapped functions within RunnableLambda.
```

Example 3 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=7b117d3aa9b419fe2a314ec6d9cc7c16" alt="Trace Tree" data-og-width="2564" width="2564" data-og-height="1530" height="1530" data-path="langsmith/images/trace-tree-manual-tracing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=517f8a525908d5241c0d635726bf2da7 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2772298bff6569c12537d8b31cc90e78 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4b02aa33df7ef1d18a33bb36c3e2edfe 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e9d4021c62cf3bad95e01c8aa2895d44 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=02ef49a98151ad9fc63c742241542d94 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/trace-tree-manual-tracing.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=765dabfe056ce5f28b1b09ca7eb735d7 2500w" />

Alternatively, you can convert LangChain's [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) to a equivalent RunTree object by using `RunTree.fromRunnableConfig` or pass the [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) as the first argument of `traceable`-wrapped function.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The `authenticate` decorator tells LangGraph to call this function as middleware

**URL:** llms-txt#the-`authenticate`-decorator-tells-langgraph-to-call-this-function-as-middleware

---

## The "Auth" object is a container that LangGraph will use to mark our authentication function

**URL:** llms-txt#the-"auth"-object-is-a-container-that-langgraph-will-use-to-mark-our-authentication-function

---

## The default RetryPolicy is optimized for retrying specific network errors.

**URL:** llms-txt#the-default-retrypolicy-is-optimized-for-retrying-specific-network-errors.

**Contents:**
- Caching Tasks
- Resuming after an error

retry_policy = RetryPolicy(retry_on=ValueError)

@task(retry_policy=retry_policy)
def get_info():
    global attempts
    attempts += 1

if attempts < 2:
        raise ValueError('Failure')
    return "OK"

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def main(inputs, writer):
    return get_info().result()

config = {
    "configurable": {
        "thread_id": "1"
    }
}

main.invoke({'any_input': 'foobar'}, config=config)
pycon  theme={null}
'OK'
python  theme={null}
import time
from langgraph.cache.memory import InMemoryCache
from langgraph.func import entrypoint, task
from langgraph.types import CachePolicy

@task(cache_policy=CachePolicy(ttl=120))    # [!code highlight]
def slow_add(x: int) -> int:
    time.sleep(1)
    return x * 2

@entrypoint(cache=InMemoryCache())
def main(inputs: dict) -> dict[str, int]:
    result1 = slow_add(inputs["x"]).result()
    result2 = slow_add(inputs["x"]).result()
    return {"result1": result1, "result2": result2}

for chunk in main.stream({"x": 5}, stream_mode="updates"):
    print(chunk)

#> {'slow_add': 10}
#> {'slow_add': 10, '__metadata__': {'cached': True}}
#> {'main': {'result1': 10, 'result2': 10}}
python  theme={null}
import time
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import StreamWriter

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Caching Tasks
```

Example 3 (unknown):
```unknown
1. `ttl` is specified in seconds. The cache will be invalidated after this time.

## Resuming after an error
```

---

## the desired output.

**URL:** llms-txt#the-desired-output.

class UserIntent(TypedDict):
    """The user's current intent in the conversation"""

intent: Literal["refund", "question_answering"]

---

## The interrupt contains the full HITL request with action_requests and review_configs

**URL:** llms-txt#the-interrupt-contains-the-full-hitl-request-with-action_requests-and-review_configs

print(result['__interrupt__'])

---

## The "messages" stream mode returns an iterator of tuples (message_chunk, metadata)

**URL:** llms-txt#the-"messages"-stream-mode-returns-an-iterator-of-tuples-(message_chunk,-metadata)

---

## The "messages" stream mode returns a tuple of (message_chunk, metadata)

**URL:** llms-txt#the-"messages"-stream-mode-returns-a-tuple-of-(message_chunk,-metadata)

---

## The metadata contains information about the LLM invocation, including the tags

**URL:** llms-txt#the-metadata-contains-information-about-the-llm-invocation,-including-the-tags

async for msg, metadata in graph.astream(
    {"topic": "cats"},
    stream_mode="messages",  # [!code highlight]
):
    # Filter the streamed tokens by the tags field in the metadata to only include
    # the tokens from the LLM invocation with the "joke" tag
    if metadata["tags"] == ["joke"]:
        print(msg.content, end="|", flush=True)
python  theme={null}
  from typing import TypedDict

from langchain.chat_models import init_chat_model
  from langgraph.graph import START, StateGraph

# The joke_model is tagged with "joke"
  joke_model = init_chat_model(model="gpt-4o-mini", tags=["joke"])
  # The poem_model is tagged with "poem"
  poem_model = init_chat_model(model="gpt-4o-mini", tags=["poem"])

class State(TypedDict):
        topic: str
        joke: str
        poem: str

async def call_model(state, config):
        topic = state["topic"]
        print("Writing joke...")
        # Note: Passing the config through explicitly is required for python < 3.11
        # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
        # The config is passed through explicitly to ensure the context vars are propagated correctly
        # This is required for Python < 3.11 when using async code. Please see the async section for more details
        joke_response = await joke_model.ainvoke(
              [{"role": "user", "content": f"Write a joke about {topic}"}],
              config,
        )
        print("\n\nWriting poem...")
        poem_response = await poem_model.ainvoke(
              [{"role": "user", "content": f"Write a short poem about {topic}"}],
              config,
        )
        return {"joke": joke_response.content, "poem": poem_response.content}

graph = (
        StateGraph(State)
        .add_node(call_model)
        .add_edge(START, "call_model")
        .compile()
  )

# The stream_mode is set to "messages" to stream LLM tokens
  # The metadata contains information about the LLM invocation, including the tags
  async for msg, metadata in graph.astream(
        {"topic": "cats"},
        stream_mode="messages",
  ):
      if metadata["tags"] == ["joke"]:
          print(msg.content, end="|", flush=True)
  python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: filtering by tags">
```

Example 2 (unknown):
```unknown
</Accordion>

#### Filter by node

To stream tokens only from specific nodes, use `stream_mode="messages"` and filter the outputs by the `langgraph_node` field in the streamed metadata:
```

---

## The overall state of the graph (this is the public state shared across nodes)

**URL:** llms-txt#the-overall-state-of-the-graph-(this-is-the-public-state-shared-across-nodes)

class OverallState(BaseModel):
    a: str

def node(state: OverallState):
    return {"a": "goodbye"}

---

## The private data is only shared between node_1 and node_2

**URL:** llms-txt#the-private-data-is-only-shared-between-node_1-and-node_2

def node_1(state: OverallState) -> Node1Output:
    output = {"private_data": "set by node_1"}
    print(f"Entered node `node_1`:\n\tInput: {state}.\n\tReturned: {output}")
    return output

---

## The rest of your code

**URL:** llms-txt#the-rest-of-your-code

**Contents:**
- API reference

import langsmith
langsmith_client = langsmith.Client(
    api_key='<api_key>',
    api_url='http(s)://<host>/api/v1',
)
```

To access the API reference, navigate to `http://<host>/api/docs` in your browser.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-usage.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## The resume payload becomes the return value of interrupt() inside the node

**URL:** llms-txt#the-resume-payload-becomes-the-return-value-of-interrupt()-inside-the-node

**Contents:**
- Common patterns
  - Approve or reject

graph.invoke(Command(resume=True), config=config)
python  theme={null}
from typing import Literal
from langgraph.types import interrupt, Command

def approval_node(state: State) -> Command[Literal["proceed", "cancel"]]:
    # Pause execution; payload shows up under result["__interrupt__"]
    is_approved = interrupt({
        "question": "Do you want to proceed with this action?",
        "details": state["action_details"]
    })

# Route based on the response
    if is_approved:
        return Command(goto="proceed")  # Runs after the resume payload is provided
    else:
        return Command(goto="cancel")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Key points about resuming:**

* You must use the **same thread ID** when resuming that was used when the interrupt occurred
* The value passed to `Command(resume=...)` becomes the return value of the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call
* The node restarts from the beginning of the node where the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was called when resumed, so any code before the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) runs again
* You can pass any JSON-serializable value as the resume value

## Common patterns

The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:

* <Icon icon="check-circle" /> [Approval workflows](#approve-or-reject): Pause before executing critical actions (API calls, database changes, financial transactions)
* <Icon icon="pencil" /> [Review and edit](#review-and-edit-state): Let humans review and modify LLM outputs or tool calls before continuing
* <Icon icon="wrench" /> [Interrupting tool calls](#interrupts-in-tools): Pause before executing tool calls to review and edit the tool call before execution
* <Icon icon="shield-check" /> [Validating human input](#validating-human-input): Pause before proceeding to the next step to validate human input

### Approve or reject

One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.
```

Example 2 (unknown):
```unknown
When you resume the graph, pass `true` to approve or `false` to reject:
```

---

## The Secret Life of Socks in the Dryer

**URL:** llms-txt#the-secret-life-of-socks-in-the-dryer

**Contents:**
  - 2. Identify a checkpoint

I finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all—they've just eloped with someone else's socks from the laundromat to start new lives together.

My blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 2. Identify a checkpoint
```

---

## the server starts with OpenTelemetry instrumentation enabled.

**URL:** llms-txt#the-server-starts-with-opentelemetry-instrumentation-enabled.

OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=<target trace ingestion endpoint>
OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp.nr-data.net
OTEL_SERVICE_NAME=MY_LANGSMITH_DEPLOYMENT
OTEL_EXPORTER_OTLP_HEADERS=api-key=<YOUR_INGEST_LICENSE_KEY>

---

## The sky

**URL:** llms-txt#the-sky

---

## The sky is

**URL:** llms-txt#the-sky-is

---

## The sky is typically

**URL:** llms-txt#the-sky-is-typically

---

## The sky is typically blue

**URL:** llms-txt#the-sky-is-typically-blue

---

## The states are returned in reverse chronological order.

**URL:** llms-txt#the-states-are-returned-in-reverse-chronological-order.

states = list(graph.get_state_history(config))

for state in states:
    print(state.next)
    print(state.config["configurable"]["checkpoint_id"])
    print()

()
1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e

('write_joke',)
1f02ac4a-ce2a-6494-8001-cb2e2d651227

('generate_topic',)
1f02ac4a-a4e0-630d-8000-b73c254ba748

('__start__',)
1f02ac4a-a4dd-665e-bfff-e6c8c44315d9
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Output:**
```

Example 2 (unknown):
```unknown

```

---

## The stream_mode is set to "messages" to stream LLM tokens

**URL:** llms-txt#the-stream_mode-is-set-to-"messages"-to-stream-llm-tokens

---

## The trace produced will have its metadata present, but the inputs and outputs will be anonymized

**URL:** llms-txt#the-trace-produced-will-have-its-metadata-present,-but-the-inputs-and-outputs-will-be-anonymized

response_with_anonymization = openai_client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},
  ],
  langsmith_extra={"client": langsmith_client},
)

---

## The trace produced will not have anonymized inputs and outputs

**URL:** llms-txt#the-trace-produced-will-not-have-anonymized-inputs-and-outputs

response_without_anonymization = openai_client.chat.completions.create(
  model="gpt-4o-mini",
  messages=[
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "My name is Slim Shady, call me at 313-666-7440 or email me at real.slim.shady@gmail.com"},
  ],
)
```

The anonymized run will look like this in LangSmith: <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cea182d95ef02e614a6f1bbd7e3a2657" alt="Anonymized run" data-og-width="3180" width="3180" data-og-height="1616" height="1616" data-path="langsmith/images/aws-comprehend-anonymized.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d3c5a665e2ee726ad6dacf89ade8daea 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=77bccbc4ba3bcde3bd771866e44ce535 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=527a6563672cb66d28bf7ae3272c0c5e 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=108afc60434f26addae7525049850aac 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e0216a9846c6e7ff041bcccdb23ce98a 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-anonymized.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=cd4119b7e81420f54db86cad58db5426 2500w" />

The non-anonymized run will look like this in LangSmith: <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ec61e5c8d78268b5b34b6b9c184871cc" alt="Non-anonymized run" data-og-width="3180" width="3180" data-og-height="1648" height="1648" data-path="langsmith/images/aws-comprehend-not-anonymized.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=34ad34770c55fcea58caee9dfa7f856e 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=089c86d993f3dc8a5998bfc2adc8a75d 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7fcd25c38a94d366762cf74629dd07c7 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b82a670a0e25bc656475cccea5d1a50d 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c422ed47282a844783713e5dc291a7b0 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/aws-comprehend-not-anonymized.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a5a62a5bd72b2cf49c9495d14d3059fa 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/mask-inputs-outputs.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## This can be a user input to your app

**URL:** llms-txt#this-can-be-a-user-input-to-your-app

question = "Can you summarize this morning's meetings?"

---

## (This can be done after putting memories into the store)

**URL:** llms-txt#(this-can-be-done-after-putting-memories-into-the-store)

memories = store.search(
    namespace_for_memory,
    query="What does the user like to eat?",
    limit=3  # Return top 3 matches
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can control which parts of your memories get embedded by configuring the `fields` parameter or by specifying the `index` parameter when storing memories:
```

---

## This can be retrieved in a retrieval step

**URL:** llms-txt#this-can-be-retrieved-in-a-retrieval-step

context = "During this morning's meeting, we solved all world conflict."

messages = [
    {"role": "system", "content": "You are a helpful assistant. Please respond to the user's request only based on the given context."},
    {"role": "user", "content": f"Question: {question}\nContext: {context}"}
]

---

## This compiles it into a LangChain Runnable,

**URL:** llms-txt#this-compiles-it-into-a-langchain-runnable,

---

## This converts the runs to a dataset + experiment

**URL:** llms-txt#this-converts-the-runs-to-a-dataset-+-experiment

**Contents:**
- Benchmark against new system
  - Define evaluators
  - Evaluate baseline

convert_runs_to_test(
    prod_runs,
    # Name of the resulting dataset
    dataset_name=dataset_name,
    # Whether to include the run outputs as reference/ground truth
    include_outputs=False,
    # Whether to include the full traces in the resulting experiment
    # (default is to just include the root run)
    load_child_runs=True,
    # Name of the experiment so we can apply evalautors to it after
    test_project_name=baseline_experiment_name
)
python  theme={null}
import emoji
from pydantic import BaseModel, Field
from langchain_core.messages import convert_to_openai_messages

class Grade(BaseModel):
    """Grade whether a response is supported by some context."""
    grounded: bool = Field(..., description="Is the majority of the response supported by the retrieved context?")

grounded_instructions = f"""You have given somebody some contextual information and asked them to write a statement grounded in that context.

Grade whether their response is fully supported by the context you have provided. \
If any meaningful part of their statement is not backed up directly by the context you provided, then their response is not grounded. \
Otherwise it is grounded."""
grounded_model = init_chat_model(model="gpt-4o").with_structured_output(Grade)

def lt_280_chars(outputs: dict) -> bool:
    messages = convert_to_openai_messages(outputs["messages"])
    return len(messages[-1]['content']) <= 280

def gte_3_emojis(outputs: dict) -> bool:
    messages = convert_to_openai_messages(outputs["messages"])
    return len(emoji.emoji_list(messages[-1]['content'])) >= 3

async def is_grounded(outputs: dict) -> bool:
    context = ""
    messages = convert_to_openai_messages(outputs["messages"])
    for message in messages:
        if message["role"] == "tool":
            # Tool message outputs are the results returned from the Tavily/DuckDuckGo tool
            context += "\n\n" + message["content"]
    tweet = messages[-1]["content"]
    user = f"""CONTEXT PROVIDED:
    {context}

RESPONSE GIVEN:
    {tweet}"""
    grade = await grounded_model.ainvoke([
        {"role": "system", "content": grounded_instructions},
        {"role": "user", "content": user}
    ])
    return grade.grounded
python  theme={null}
baseline_results = await client.aevaluate(
    baseline_experiment_name,
    evaluators=[lt_280_chars, gte_3_emojis, is_grounded],
)

**Examples:**

Example 1 (unknown):
```unknown
Once this step is complete, you should see a new dataset in your LangSmith project called "Tweet Writing Task-backtesting TODAYS DATE", with a single experiment like so:

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=73b60a75d6b33f2830f5ed68464c586b" alt="Baseline experiment" data-og-width="3456" width="3456" data-og-height="1852" height="1852" data-path="langsmith/images/baseline-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e459a884bbec6e3741617830b9e70848 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6116dd6057709be29d84f0cbad32e7a1 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=a86ab5cffeac0ceace81adbc59dba649 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=e0058d68f4061df5fe4249c9e3954c38 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=270a7eb33f39fd612d1732aadaa0b373 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/baseline-experiment.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fe9dc037876fcf1c4eb22317d7bb3f45 2500w" />

## Benchmark against new system

Now we can start the process of benchmarking our production runs against a new system.

### Define evaluators

First let's define the evaluators we will use to compare the two systems. Note that we have no reference outputs, so we'll need to come up with evaluation metrics that only require the actual outputs.
```

Example 2 (unknown):
```unknown
### Evaluate baseline

Now, let's run our evaluators against the baseline experiment.
```

---

## This example uses OpenAI, but you can use any LLM provider of choice

**URL:** llms-txt#this-example-uses-openai,-but-you-can-use-any-llm-provider-of-choice

**Contents:**
  - 3. Log a trace

export OPENAI_API_KEY=<your-openai-api-key>
python Python theme={null}
  import json
  import openai
  import operator
  from langsmith import traceable
  from langsmith.wrappers import wrap_openai
  from typing import Annotated, Literal, TypedDict
  from langgraph.graph import StateGraph

class State(TypedDict):
      messages: Annotated[list, operator.add]

tool_schema = {
      "type": "function",
      "function": {
          "name": "search",
          "description": "Call to surf the web.",
          "parameters": {
              "type": "object",
              "properties": {"query": {"type": "string"}},
              "required": ["query"],
          },
      },
  }

# Decorating the tool function will automatically trace it with the correct context
  @traceable(run_type="tool", name="Search Tool")
  def search(query: str):
      """Call to surf the web."""
      if "sf" in query.lower() or "san francisco" in query.lower():
          return "It's 60 degrees and foggy."
      return "It's 90 degrees and sunny."

def call_tools(state):
      function_name_to_function = {"search": search}
      messages = state["messages"]
      tool_call = messages[-1]["tool_calls"][0]
      function_name = tool_call["function"]["name"]
      function_arguments = tool_call["function"]["arguments"]
      arguments = json.loads(function_arguments)
      function_response = function_name_to_function[function_name](**arguments)
      tool_message = {
          "tool_call_id": tool_call["id"],
          "role": "tool",
          "name": function_name,
          "content": function_response,
      }
      return {"messages": [tool_message]}

wrapped_client = wrap_openai(openai.Client())

def should_continue(state: State) -> Literal["tools", "__end__"]:
      messages = state["messages"]
      last_message = messages[-1]
      if last_message["tool_calls"]:
          return "tools"
      return "__end__"

def call_model(state: State):
      messages = state["messages"]
      # Calling the wrapped client will automatically infer the correct tracing context
      response = wrapped_client.chat.completions.create(
          messages=messages, model="gpt-4o-mini", tools=[tool_schema]
      )
      raw_tool_calls = response.choices[0].message.tool_calls
      tool_calls = [tool_call.to_dict() for tool_call in raw_tool_calls] if raw_tool_calls else []
      response_message = {
          "role": "assistant",
          "content": response.choices[0].message.content,
          "tool_calls": tool_calls,
      }
      return {"messages": [response_message]}

workflow = StateGraph(State)
  workflow.add_node("agent", call_model)
  workflow.add_node("tools", call_tools)
  workflow.add_edge("__start__", "agent")
  workflow.add_conditional_edges(
      "agent",
      should_continue,
  )
  workflow.add_edge("tools", 'agent')

app = workflow.compile()

final_state = app.invoke(
      {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
  )

final_state["messages"][-1]["content"]
  typescript TypeScript theme={null}
  **Note:** The below example requires `langsmith>=0.1.39` and `@langchain/langgraph>=0.0.31`

import OpenAI from "openai";
  import { StateGraph } from "@langchain/langgraph";
  import { wrapOpenAI } from "langsmith/wrappers/openai";
  import { traceable } from "langsmith/traceable";

type GraphState = {
    messages: OpenAI.ChatCompletionMessageParam[];
  };

const wrappedClient = wrapOpenAI(new OpenAI({}));

const toolSchema: OpenAI.ChatCompletionTool = {
    type: "function",
    function: {
      name: "search",
      description: "Use this tool to query the web.",
      parameters: {
        type: "object",
        properties: {
          query: {
            type: "string",
          },
        },
        required: ["query"],
      }
    }
  };

// Wrapping the tool function will automatically trace it with the correct context
  const search = traceable(async ({ query }: { query: string }) => {
    if (
      query.toLowerCase().includes("sf") ||
      query.toLowerCase().includes("san francisco")
    ) {
      return "It's 60 degrees and foggy.";
    }
    return "It's 90 degrees and sunny.";
  }, { run_type: "tool", name: "Search Tool" });

const callTools = async ({ messages }: GraphState) => {
    const mostRecentMessage = messages[messages.length - 1];
    const toolCalls = (mostRecentMessage as OpenAI.ChatCompletionAssistantMessageParam).tool_calls;
    if (toolCalls === undefined || toolCalls.length === 0) {
      throw new Error("No tool calls passed to node.");
    }
    const toolNameMap = {
      search,
    };
    const functionName = toolCalls[0].function.name;
    const functionArguments = JSON.parse(toolCalls[0].function.arguments);
    const response = await toolNameMap[functionName](functionArguments);
    const toolMessage = {
      tool_call_id: toolCalls[0].id,
      role: "tool",
      name: functionName,
      content: response,
    }
    return { messages: [toolMessage] };
  };

const callModel = async ({ messages }: GraphState) => {
    // Calling the wrapped client will automatically infer the correct tracing context
    const response = await wrappedClient.chat.completions.create({
      messages,
      model: "gpt-4o-mini",
      tools: [toolSchema],
    });
    const responseMessage = {
      role: "assistant",
      content: response.choices[0].message.content,
      tool_calls: response.choices[0].message.tool_calls ?? [],
    };
    return { messages: [responseMessage] };
  };

const shouldContinue = ({ messages }: GraphState) => {
    const lastMessage =
      messages[messages.length - 1] as OpenAI.ChatCompletionAssistantMessageParam;
    if (
      lastMessage?.tool_calls !== undefined &&
      lastMessage?.tool_calls.length > 0
    ) {
      return "tools";
    }
    return "__end__";
  }

const workflow = new StateGraph<GraphState>({
    channels: {
      messages: {
        reducer: (a: any, b: any) => a.concat(b),
      }
    }
  });

const graph = workflow
    .addNode("model", callModel)
    .addNode("tools", callTools)
    .addEdge("__start__", "model")
    .addConditionalEdges("model", shouldContinue, {
      tools: "tools",
      __end__: "__end__",
    })
    .addEdge("tools", "model")
    .compile();

await graph.invoke({
    messages: [{ role: "user", content: "what is the weather in sf" }]
  });
  ```
</CodeGroup>

An example trace from running the above code [looks like this](https://smith.langchain.com/public/353f27da-c221-4b67-b9ec-ede3777f3271/r):

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=abe0ae173d182563c343f6596e0ce4e2" alt="Trace tree for a LangGraph run without LangChain" data-og-width="3296" width="3296" data-og-height="1774" height="1774" data-path="langsmith/images/langgraph-without-langchain-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=794e9ce04677bbf721880ebb07ada7c6 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d6208cbec91ba187ba8f75f6cc916b3f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=418509190558d6a87363d3ba146b7722 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=a3c3e2e11cbdac8c32e80e7a895b1eb0 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ea893862eb3f9bc5910649cf0ccd2abe 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/langgraph-without-langchain-trace.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2b244ca0ec1296552991428d1efffde6 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-langgraph.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Info>
  If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:

  `export LANGCHAIN_CALLBACKS_BACKGROUND=true`

  If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:

  `export LANGCHAIN_CALLBACKS_BACKGROUND=false`

  See [this LangChain.js guide](https://js.langchain.com/docs/how_to/callbacks_serverless) for more information.
</Info>

### 3. Log a trace

Once you've set up your environment, [wrap or decorate the custom functions/SDKs](/langsmith/annotate-code#use-traceable--traceable) you want to trace. LangSmith will then infer the proper tracing config:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## This invocation will take ~1 second due to the slow_task execution

**URL:** llms-txt#this-invocation-will-take-~1-second-due-to-the-slow_task-execution

**Contents:**
- Human-in-the-loop
  - Basic human-in-the-loop workflow

try:
    # First invocation will raise an exception due to the `get_info` task failing
    main.invoke({'any_input': 'foobar'}, config=config)
except ValueError:
    pass  # Handle the failure gracefully
python  theme={null}
main.invoke(None, config=config)
pycon  theme={null}
'Ran slow task.'
python  theme={null}
from langgraph.func import entrypoint, task
from langgraph.types import Command, interrupt

@task
def step_1(input_query):
    """Append bar."""
    return f"{input_query} bar"

@task
def human_feedback(input_query):
    """Append user input."""
    feedback = interrupt(f"Please provide feedback: {input_query}")
    return f"{input_query} {feedback}"

@task
def step_3(input_query):
    """Append qux."""
    return f"{input_query} qux"
python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

@entrypoint(checkpointer=checkpointer)
def graph(input_query):
    result_1 = step_1(input_query).result()
    result_2 = human_feedback(result_1).result()
    result_3 = step_3(result_2).result()

return result_3
python  theme={null}
config = {"configurable": {"thread_id": "1"}}

for event in graph.stream("foo", config):
    print(event)
    print("\n")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
When we resume execution, we won't need to re-run the `slow_task` as its result is already saved in the checkpoint.
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
## Human-in-the-loop

The functional API supports [human-in-the-loop](/oss/python/langgraph/interrupts) workflows using the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function and the `Command` primitive.

### Basic human-in-the-loop workflow

We will create three [tasks](/oss/python/langgraph/functional-api#task):

1. Append `"bar"`.
2. Pause for human input. When resuming, append human input.
3. Append `"qux"`.
```

Example 4 (unknown):
```unknown
We can now compose these tasks in an [entrypoint](/oss/python/langgraph/functional-api#entrypoint):
```

---

## This isn't for production use, but is useful for local

**URL:** llms-txt#this-isn't-for-production-use,-but-is-useful-for-local

store = LocalFileStore("./cache/") # [!code highlight]

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings,
    store,
    namespace=underlying_embeddings.model
)

---

## This is loaded from the `.env` file you created above

**URL:** llms-txt#this-is-loaded-from-the-`.env`-file-you-created-above

SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_SERVICE_KEY = os.environ["SUPABASE_SERVICE_KEY"]

@auth.authenticate
async def get_current_user(authorization: str | None):
    """Validate JWT tokens and extract user information."""
    assert authorization
    scheme, token = authorization.split()
    assert scheme.lower() == "bearer"

try:
        # Verify token with auth provider
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{SUPABASE_URL}/auth/v1/user",
                headers={
                    "Authorization": authorization,
                    "apiKey": SUPABASE_SERVICE_KEY,
                },
            )
            assert response.status_code == 200
            user = response.json()
            return {
                "identity": user["id"],  # Unique user identifier
                "email": user["email"],
                "is_authenticated": True,
            }
    except Exception as e:
        raise Auth.exceptions.HTTPException(status_code=401, detail=str(e))

---

## This is our toy user database. Do not do this in production

**URL:** llms-txt#this-is-our-toy-user-database.-do-not-do-this-in-production

VALID_TOKENS = {
    "user1-token": {"id": "user1", "name": "Alice"},
    "user2-token": {"id": "user2", "name": "Bob"},
}

---

## this is supported

**URL:** llms-txt#this-is-supported

{"messages": [HumanMessage(content="message")]}

---

## this is the graph making function that will decide which graph to

**URL:** llms-txt#this-is-the-graph-making-function-that-will-decide-which-graph-to

---

## This is the state before last (states are listed in chronological order)

**URL:** llms-txt#this-is-the-state-before-last-(states-are-listed-in-chronological-order)

**Contents:**
  - 3. Update the state
  - 4. Resume execution from the checkpoint

selected_state = states[1]
print(selected_state.next)
print(selected_state.values)

('write_joke',)
{'topic': 'How about "The Secret Life of Socks in the Dryer"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\'t know about? There\\'s a lot of comedic potential in the everyday mystery that unites us all!'}
python  theme={null}
new_config = graph.update_state(selected_state.config, values={"topic": "chickens"})
print(new_config)

{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}
python  theme={null}
graph.invoke(None, new_config)
python  theme={null}
{'topic': 'chickens',
 'joke': 'Why did the chicken join a band?\n\nBecause it had excellent drumsticks!'}
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-time-travel.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
**Output:**
```

Example 2 (unknown):
```unknown
<a id="optional" />

### 3. Update the state

[`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state) will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.
```

Example 3 (unknown):
```unknown
**Output:**
```

Example 4 (unknown):
```unknown
### 4. Resume execution from the checkpoint
```

---

## This is your PUBLIC anon key (which is safe to use client-side)

**URL:** llms-txt#this-is-your-public-anon-key-(which-is-safe-to-use-client-side)

---

## This means that this node is the first one called

**URL:** llms-txt#this-means-that-this-node-is-the-first-one-called

workflow.add_edge(START, "agent")

---

## This takes precedenceover the generic @auth.on handler and the @auth.on.threads handler

**URL:** llms-txt#this-takes-precedenceover-the-generic-@auth.on-handler-and-the-@auth.on.threads-handler

@auth.on.threads.create_run
async def on_run_create(
    ctx: Auth.types.AuthContext,
    value: Auth.types.threads.create_run.value
):
    metadata = value.setdefault("metadata", {})
    metadata["owner"] = ctx.user.identity
    # Inherit thread's access control
    return {"owner": ctx.user.identity}

---

## This variable is just used for demonstration purposes to simulate a network failure.

**URL:** llms-txt#this-variable-is-just-used-for-demonstration-purposes-to-simulate-a-network-failure.

---

## This will become important when we're running our evaluations.

**URL:** llms-txt#this-will-become-important-when-we're-running-our-evaluations.

def refund(state: State, config: RunnableConfig) -> dict:
    # Whether to mock the deletion. True if the configurable var 'env' is set to 'test'.
    mock = config.get("configurable", {}).get("env", "prod") == "test"
    refunded = _refund(
        invoice_id=state["invoice_id"], invoice_line_ids=state["invoice_line_ids"], mock=mock
    )
    response = f"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?"
    return {
        "messages": [{"role": "assistant", "content": response}],
        "followup": response,
    }

---

## This WILL be traced

**URL:** llms-txt#this-will-be-traced

with ls.tracing_context(enabled=True):
    agent.invoke({"messages": [{"role": "user", "content": "Send a test email to alice@example.com"}]})

---

## This will NOT be traced (if LANGSMITH_TRACING is not set)

**URL:** llms-txt#this-will-not-be-traced-(if-langsmith_tracing-is-not-set)

**Contents:**
- Log to a project
- Add metadata to traces
- Use anonymizers to prevent logging of sensitive data in traces

agent.invoke({"messages": [{"role": "user", "content": "Send another email"}]})
bash  theme={null}
  export LANGSMITH_PROJECT=my-agent-project
  python  theme={null}
  import langsmith as ls

with ls.tracing_context(project_name="email-agent-test", enabled=True):
      response = agent.invoke({
          "messages": [{"role": "user", "content": "Send a welcome email"}]
      })
  python  theme={null}
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Send a welcome email"}]},
    config={
        "tags": ["production", "email-assistant", "v1.0"],
        "metadata": {
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        }
    }
)
python  theme={null}
with ls.tracing_context(
    project_name="email-agent-test",
    enabled=True,
    tags=["production", "email-assistant", "v1.0"],
    metadata={"user_id": "user_123", "session_id": "session_456", "environment": "production"}):
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "Send a welcome email"}]}
    )
python Python theme={null}
from langchain_core.tracers.langchain import LangChainTracer
from langgraph.graph import StateGraph, MessagesState
from langsmith import Client
from langsmith.anonymizer import create_anonymizer

anonymizer = create_anonymizer([
    # Matches SSNs
    { "pattern": r"\b\d{3}-?\d{2}-?\d{4}\b", "replace": "<ssn>" }
])

tracer_client = Client(anonymizer=anonymizer)
tracer = LangChainTracer(client=tracer_client)

**Examples:**

Example 1 (unknown):
```unknown
## Log to a project

<Accordion title="Statically">
  You can set a custom project name for your entire application by setting the `LANGSMITH_PROJECT` environment variable:
```

Example 2 (unknown):
```unknown
</Accordion>

<Accordion title="Dynamically">
  You can set the project name programmatically for specific operations:
```

Example 3 (unknown):
```unknown
</Accordion>

## Add metadata to traces

You can annotate your traces with custom metadata and tags:
```

Example 4 (unknown):
```unknown
`tracing_context` also accepts tags and metadata for fine-grained control:
```

---

## Thread 1: Write to long-term memory

**URL:** llms-txt#thread-1:-write-to-long-term-memory

config1 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
    "messages": [{"role": "user", "content": "Save my preferences to /memories/preferences.txt"}]
}, config=config1)

---

## Thread 2: Read from long-term memory (different conversation!)

**URL:** llms-txt#thread-2:-read-from-long-term-memory-(different-conversation!)

config2 = {"configurable": {"thread_id": str(uuid.uuid4())}}
agent.invoke({
    "messages": [{"role": "user", "content": "What are my preferences?"}]
}, config=config2)

---

## Thread creation. This will match only on thread create actions

**URL:** llms-txt#thread-creation.-this-will-match-only-on-thread-create-actions

---

## thread_id is the persistent pointer (stores a stable ID in production)

**URL:** llms-txt#thread_id-is-the-persistent-pointer-(stores-a-stable-id-in-production)

config = {"configurable": {"thread_id": "thread-1"}}
result = graph.invoke({"input": "data"}, config=config)

---

## Time travel using the server API

**URL:** llms-txt#time-travel-using-the-server-api

**Contents:**
- Use time travel in a workflow
  - 1. Run the graph
  - 2. Identify a checkpoint
  - 3. Update the state
  - 4. Resume execution from the checkpoint
- Learn more

Source: https://docs.langchain.com/langsmith/human-in-the-loop-time-travel

LangGraph provides the [**time travel**](/oss/python/langgraph/use-time-travel) functionality to resume execution from a prior checkpoint, either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.

To time travel using the LangSmith Deployment API (via the LangGraph SDK):

1. **Run the graph** with initial inputs using [LangGraph SDK](/langsmith/langgraph-python-sdk)'s [client.runs.wait](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.wait) or [client.runs.stream](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.stream) APIs.
2. **Identify a checkpoint in an existing thread**: Use [client.threads.get\_history](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_history) method to retrieve the execution history for a specific `thread_id` and locate the desired `checkpoint_id`.
   Alternatively, set a [breakpoint](/oss/python/langgraph/interrupts) before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint.
3. **(Optional) modify the graph state**: Use the [client.threads.update\_state](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.update_state) method to modify the graph’s state at the checkpoint and resume execution from alternative state.
4. **Resume execution from the checkpoint**: Use the [client.runs.wait](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.wait) or [client.runs.stream](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.RunsClient.stream) APIs with an input of `None` and the appropriate `thread_id` and `checkpoint_id`.

## Use time travel in a workflow

<Accordion title="Example graph">
  
</Accordion>

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    Create a thread:

### 2. Identify a checkpoint

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### 3. Update the state

[`update_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update_state) will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

### 4. Resume execution from the checkpoint

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="cURL">
    
  </Tab>
</Tabs>

* [**LangGraph time travel guide**](/oss/python/langgraph/use-time-travel): learn more about using time travel in LangGraph.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/human-in-the-loop-time-travel.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

### 1. Run the graph

<Tabs>
  <Tab title="Python">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="cURL">
    Create a thread:
```

Example 4 (unknown):
```unknown
Run the graph:
```

---

## Tools

**URL:** llms-txt#tools

from langchain.tools import tool

---

## Tools and toolkits

**URL:** llms-txt#tools-and-toolkits

**Contents:**
- Search
- Code Interpreter
- Productivity
- Web Browsing
- Database
- Finance
- Integration Platforms
- All tools and toolkits

Source: https://docs.langchain.com/oss/python/integrations/tools/index

[Tools](/oss/python/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.

A toolkit is a collection of tools meant to be used together.

The following table shows tools that execute online searches in some shape or form:

| Tool/Toolkit                                                      | Free/Paid                    | Return Data                                           |
| ----------------------------------------------------------------- | ---------------------------- | ----------------------------------------------------- |
| [Bing Search](/oss/python/integrations/tools/bing_search)         | Paid                         | URL, Snippet, Title                                   |
| [Brave Search](/oss/python/integrations/tools/brave_search)       | Free                         | URL, Snippet, Title                                   |
| [DuckDuckgoSearch](/oss/python/integrations/tools/ddg)            | Free                         | URL, Snippet, Title                                   |
| [Exa Search](/oss/python/integrations/tools/exa_search)           | 1000 free searches/month     | URL, Author, Title, Published Date                    |
| [Google Search](/oss/python/integrations/tools/google_search)     | Paid                         | URL, Snippet, Title                                   |
| [Google Serper](/oss/python/integrations/tools/google_serper)     | Free                         | URL, Snippet, Title, Search Rank, Site Links          |
| [Jina Search](/oss/python/integrations/tools/jina_search)         | 1M Response Tokens Free      | URL, Snippet, Title, Page Content                     |
| [Mojeek Search](/oss/python/integrations/tools/mojeek_search)     | Paid                         | URL, Snippet, Title                                   |
| [Parallel Search](/oss/python/integrations/tools/parallel_search) | Paid                         | URL, Title, Excerpts                                  |
| [SearchApi](/oss/python/integrations/tools/searchapi)             | 100 Free Searches on Sign Up | URL, Snippet, Title, Search Rank, Site Links, Authors |
| [SearxNG Search](/oss/python/integrations/tools/searx_search)     | Free                         | URL, Snippet, Title, Category                         |
| [SerpApi](/oss/python/integrations/tools/serpapi)                 | 250 Free Searches/Month      | Answer                                                |
| [Tavily Search](/oss/python/integrations/tools/tavily_search)     | 1000 free searches/month     | URL, Content, Title, Images, Answer                   |
| [You.com Search](/oss/python/integrations/tools/you)              | Free for 60 days             | URL, Title, Page Content                              |

The following table shows tools that can be used as code interpreters:

| Tool/Toolkit                                                                                   | Supported Languages           | Sandbox Lifetime    | Supports File Uploads | Return Types | Supports Self-Hosting |
| ---------------------------------------------------------------------------------------------- | ----------------------------- | ------------------- | --------------------- | ------------ | --------------------- |
| [Azure Container Apps dynamic sessions](/oss/python/integrations/tools/azure_dynamic_sessions) | Python                        | 1 Hour              | ✅                     | Text, Images | ❌                     |
| [Bearly Code Interpreter](/oss/python/integrations/tools/bearly)                               | Python                        | Resets on Execution | ✅                     | Text         | ❌                     |
| [Riza Code Interpreter](/oss/python/integrations/tools/riza)                                   | Python, JavaScript, PHP, Ruby | Resets on Execution | ✅                     | Text         | ✅                     |

The following table shows tools that can be used to automate tasks in productivity tools:

| Tool/Toolkit                                                  | Pricing                                                                                                |
| ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |
| [GitHub Toolkit](/oss/python/integrations/tools/github)       | Free                                                                                                   |
| [GitLab Toolkit](/oss/python/integrations/tools/gitlab)       | Free for personal project                                                                              |
| [Gmail Toolkit](/oss/python/integrations/tools/google_gmail)  | Free, with limit of 250 quota units per user per second                                                |
| [Infobip Tool](/oss/python/integrations/tools/infobip)        | Free trial, with variable pricing after                                                                |
| [Jira Toolkit](/oss/python/integrations/tools/jira)           | Free, with [rate limits](https://developer.atlassian.com/cloud/jira/platform/rate-limiting/)           |
| [Office365 Toolkit](/oss/python/integrations/tools/office365) | Free with Office365, includes [rate limits](https://learn.microsoft.com/en-us/graph/throttling-limits) |
| [Slack Toolkit](/oss/python/integrations/tools/slack)         | Free                                                                                                   |
| [Twilio Tool](/oss/python/integrations/tools/twilio)          | Free trial, with [pay-as-you-go pricing](https://www.twilio.com/en-us/pricing) after                   |

The following table shows tools that can be used to automate tasks in web browsers:

| Tool/Toolkit                                                                                        | Pricing                                                     | Supports Interacting with the Browser |
| --------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- | ------------------------------------- |
| [AgentQL Toolkit](/oss/python/integrations/tools/agentql)                                           | Free trial, with pay-as-you-go and flat rate plans after    | ✅                                     |
| [Hyperbrowser Browser Agent Tools](/oss/python/integrations/tools/hyperbrowser_browser_agent_tools) | Free trial, with flat rate plans and pre-paid credits after | ✅                                     |
| [Hyperbrowser Web Scraping Tools](/oss/python/integrations/tools/hyperbrowser_web_scraping_tools)   | Free trial, with flat rate plans and pre-paid credits after | ❌                                     |
| [MultiOn Toolkit](/oss/python/integrations/tools/multion)                                           | 40 free requests/day                                        | ✅                                     |
| [Oxylabs Web Scraper API](/oss/python/integrations/tools/oxylabs)                                   | Free trial, with flat rate plans and pre-paid credits after | ❌                                     |
| [PlayWright Browser Toolkit](/oss/python/integrations/tools/playwright)                             | Free                                                        | ✅                                     |
| [Requests Toolkit](/oss/python/integrations/tools/requests)                                         | Free                                                        | ❌                                     |

The following table shows tools that can be used to automate tasks in databases:

| Tool/Toolkit                                                                    | Allowed Operations              |
| ------------------------------------------------------------------------------- | ------------------------------- |
| [Cassandra Database Toolkit](/oss/python/integrations/tools/cassandra_database) | SELECT and schema introspection |
| [MCP Toolbox](/oss/python/integrations/tools/mcp_toolbox)                       | Any SQL operation               |
| [SQLDatabase Toolkit](/oss/python/integrations/tools/sql_database)              | Any SQL operation               |
| [Spark SQL Toolkit](/oss/python/integrations/tools/spark_sql)                   | Any SQL operation               |

The following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:

| Tool/Toolkit                                  | Pricing | Capabilities                                                                      |
| --------------------------------------------- | ------- | --------------------------------------------------------------------------------- |
| [GOAT](/oss/python/integrations/tools/goat)   | Free    | Create and receive payments, purchase physical goods, make investments, and more. |
| [Privy](/oss/python/integrations/tools/privy) | Free    | Create wallets with configurable permissions and execute transactions with speed. |

## Integration Platforms

The following platforms provide access to multiple tools and services through a unified interface:

| Tool/Toolkit                                        | Number of Integrations | Pricing             | Key Features                                               |
| --------------------------------------------------- | ---------------------- | ------------------- | ---------------------------------------------------------- |
| [Composio](/oss/python/integrations/tools/composio) | 500+                   | Free tier available | OAuth handling, event-driven workflows, multi-user support |

## All tools and toolkits

<Columns cols={3}>
  <Card title="ADS4GPTs" icon="link" href="/oss/python/integrations/tools/ads4gpts" arrow="true" cta="View guide" />

<Card title="AgentQL" icon="link" href="/oss/python/integrations/tools/agentql" arrow="true" cta="View guide" />

<Card title="AINetwork Toolkit" icon="link" href="/oss/python/integrations/tools/ainetwork" arrow="true" cta="View guide" />

<Card title="Alpha Vantage" icon="link" href="/oss/python/integrations/tools/alpha_vantage" arrow="true" cta="View guide" />

<Card title="Amadeus Toolkit" icon="link" href="/oss/python/integrations/tools/amadeus" arrow="true" cta="View guide" />

<Card title="Anchor Browser" icon="link" href="/oss/python/integrations/tools/anchor_browser" arrow="true" cta="View guide" />

<Card title="Apify Actor" icon="link" href="/oss/python/integrations/tools/apify_actors" arrow="true" cta="View guide" />

<Card title="ArXiv" icon="link" href="/oss/python/integrations/tools/arxiv" arrow="true" cta="View guide" />

<Card title="AskNews" icon="link" href="/oss/python/integrations/tools/asknews" arrow="true" cta="View guide" />

<Card title="AWS Lambda" icon="link" href="/oss/python/integrations/tools/awslambda" arrow="true" cta="View guide" />

<Card title="Azure AI Services Toolkit" icon="link" href="/oss/python/integrations/tools/azure_ai_services" arrow="true" cta="View guide" />

<Card title="Azure Cognitive Services Toolkit" icon="link" href="/oss/python/integrations/tools/azure_cognitive_services" arrow="true" cta="View guide" />

<Card title="Azure Container Apps Dynamic Sessions" icon="link" href="/oss/python/integrations/tools/azure_dynamic_sessions" arrow="true" cta="View guide" />

<Card title="Shell (bash)" icon="link" href="/oss/python/integrations/tools/bash" arrow="true" cta="View guide" />

<Card title="Bearly Code Interpreter" icon="link" href="/oss/python/integrations/tools/bearly" arrow="true" cta="View guide" />

<Card title="Bing Search" icon="link" href="/oss/python/integrations/tools/bing_search" arrow="true" cta="View guide" />

<Card title="Bodo DataFrames" icon="link" href="/oss/python/integrations/tools/bodo" arrow="true" cta="View guide" />

<Card title="Brave Search" icon="link" href="/oss/python/integrations/tools/brave_search" arrow="true" cta="View guide" />

<Card title="BrightData Web Scraper API" icon="link" href="/oss/python/integrations/tools/brightdata-webscraperapi" arrow="true" cta="View guide" />

<Card title="BrightData SERP" icon="link" href="/oss/python/integrations/tools/brightdata_serp" arrow="true" cta="View guide" />

<Card title="BrightData Unlocker" icon="link" href="/oss/python/integrations/tools/brightdata_unlocker" arrow="true" cta="View guide" />

<Card title="Cassandra Database Toolkit" icon="link" href="/oss/python/integrations/tools/cassandra_database" arrow="true" cta="View guide" />

<Card title="CDP" icon="link" href="/oss/python/integrations/tools/cdp_agentkit" arrow="true" cta="View guide" />

<Card title="ChatGPT Plugins" icon="link" href="/oss/python/integrations/tools/chatgpt_plugins" arrow="true" cta="View guide" />

<Card title="ClickUp Toolkit" icon="link" href="/oss/python/integrations/tools/clickup" arrow="true" cta="View guide" />

<Card title="Cogniswitch Toolkit" icon="link" href="/oss/python/integrations/tools/cogniswitch" arrow="true" cta="View guide" />

<Card title="Compass DeFi Toolkit" icon="link" href="/oss/python/integrations/tools/compass" arrow="true" cta="View guide" />

<Card title="Composio" icon="link" href="/oss/python/integrations/tools/composio" arrow="true" cta="View guide" />

<Card title="Connery Toolkit" icon="link" href="/oss/python/integrations/tools/connery" arrow="true" cta="View guide" />

<Card title="Dall-E Image Generator" icon="link" href="/oss/python/integrations/tools/dalle_image_generator" arrow="true" cta="View guide" />

<Card title="Dappier" icon="link" href="/oss/python/integrations/tools/dappier" arrow="true" cta="View guide" />

<Card title="Databricks Unity Catalog" icon="link" href="/oss/python/integrations/tools/databricks" arrow="true" cta="View guide" />

<Card title="DataForSEO" icon="link" href="/oss/python/integrations/tools/dataforseo" arrow="true" cta="View guide" />

<Card title="Dataherald" icon="link" href="/oss/python/integrations/tools/dataherald" arrow="true" cta="View guide" />

<Card title="Daytona Data Analysis" icon="link" href="/oss/python/integrations/tools/daytona_data_analysis" arrow="true" cta="View guide" />

<Card title="DuckDuckGo Search" icon="link" href="/oss/python/integrations/tools/ddg" arrow="true" cta="View guide" />

<Card title="Discord" icon="link" href="/oss/python/integrations/tools/discord" arrow="true" cta="View guide" />

<Card title="E2B Data Analysis" icon="link" href="/oss/python/integrations/tools/e2b_data_analysis" arrow="true" cta="View guide" />

<Card title="Eden AI" icon="link" href="/oss/python/integrations/tools/edenai_tools" arrow="true" cta="View guide" />

<Card title="ElevenLabs Text2Speech" icon="link" href="/oss/python/integrations/tools/eleven_labs_tts" arrow="true" cta="View guide" />

<Card title="Exa Search" icon="link" href="/oss/python/integrations/tools/exa_search" arrow="true" cta="View guide" />

<Card title="File System" icon="link" href="/oss/python/integrations/tools/filesystem" arrow="true" cta="View guide" />

<Card title="Financial Datasets Toolkit" icon="link" href="/oss/python/integrations/tools/financial_datasets" arrow="true" cta="View guide" />

<Card title="FMP Data" icon="link" href="/oss/python/integrations/tools/fmp-data" arrow="true" cta="View guide" />

<Card title="GitHub Toolkit" icon="link" href="/oss/python/integrations/tools/github" arrow="true" cta="View guide" />

<Card title="GitLab Toolkit" icon="link" href="/oss/python/integrations/tools/gitlab" arrow="true" cta="View guide" />

<Card title="Gmail Toolkit" icon="link" href="/oss/python/integrations/tools/google_gmail" arrow="true" cta="View guide" />

<Card title="GOAT" icon="link" href="/oss/python/integrations/tools/goat" arrow="true" cta="View guide" />

<Card title="Privy" icon="link" href="/oss/python/integrations/tools/privy" arrow="true" cta="View guide" />

<Card title="Golden Query" icon="link" href="/oss/python/integrations/tools/golden_query" arrow="true" cta="View guide" />

<Card title="Google Books" icon="link" href="/oss/python/integrations/tools/google_books" arrow="true" cta="View guide" />

<Card title="Google Calendar Toolkit" icon="link" href="/oss/python/integrations/tools/google_calendar" arrow="true" cta="View guide" />

<Card title="Google Cloud Text-to-Speech" icon="link" href="/oss/python/integrations/tools/google_cloud_texttospeech" arrow="true" cta="View guide" />

<Card title="Google Drive" icon="link" href="/oss/python/integrations/tools/google_drive" arrow="true" cta="View guide" />

<Card title="Google Finance" icon="link" href="/oss/python/integrations/tools/google_finance" arrow="true" cta="View guide" />

<Card title="Google Imagen" icon="link" href="/oss/python/integrations/tools/google_imagen" arrow="true" cta="View guide" />

<Card title="Google Jobs" icon="link" href="/oss/python/integrations/tools/google_jobs" arrow="true" cta="View guide" />

<Card title="Google Lens" icon="link" href="/oss/python/integrations/tools/google_lens" arrow="true" cta="View guide" />

<Card title="Google Places" icon="link" href="/oss/python/integrations/tools/google_places" arrow="true" cta="View guide" />

<Card title="Google Scholar" icon="link" href="/oss/python/integrations/tools/google_scholar" arrow="true" cta="View guide" />

<Card title="Google Search" icon="link" href="/oss/python/integrations/tools/google_search" arrow="true" cta="View guide" />

<Card title="Google Serper" icon="link" href="/oss/python/integrations/tools/google_serper" arrow="true" cta="View guide" />

<Card title="Google Trends" icon="link" href="/oss/python/integrations/tools/google_trends" arrow="true" cta="View guide" />

<Card title="Gradio" icon="link" href="/oss/python/integrations/tools/gradio_tools" arrow="true" cta="View guide" />

<Card title="GraphQL" icon="link" href="/oss/python/integrations/tools/graphql" arrow="true" cta="View guide" />

<Card title="HuggingFace Hub Tools" icon="link" href="/oss/python/integrations/tools/huggingface_tools" arrow="true" cta="View guide" />

<Card title="Human as a Tool" icon="link" href="/oss/python/integrations/tools/human_tools" arrow="true" cta="View guide" />

<Card title="Hyperbrowser Browser Agent Tools" icon="link" href="/oss/python/integrations/tools/hyperbrowser_browser_agent_tools" arrow="true" cta="View guide" />

<Card title="Hyperbrowser Web Scraping Tools" icon="link" href="/oss/python/integrations/tools/hyperbrowser_web_scraping_tools" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai" icon="link" href="/oss/python/integrations/tools/ibm_watsonx" arrow="true" cta="View guide" />

<Card title="IBM watsonx.ai (SQL)" icon="link" href="/oss/python/integrations/tools/ibm_watsonx_sql" arrow="true" cta="View guide" />

<Card title="IFTTT WebHooks" icon="link" href="/oss/python/integrations/tools/ifttt" arrow="true" cta="View guide" />

<Card title="Infobip" icon="link" href="/oss/python/integrations/tools/infobip" arrow="true" cta="View guide" />

<Card title="Ionic Shopping Tool" icon="link" href="/oss/python/integrations/tools/ionic_shopping" arrow="true" cta="View guide" />

<Card title="Jenkins" icon="link" href="/oss/python/integrations/tools/jenkins" arrow="true" cta="View guide" />

<Card title="Jina Search" icon="link" href="/oss/python/integrations/tools/jina_search" arrow="true" cta="View guide" />

<Card title="Jira Toolkit" icon="link" href="/oss/python/integrations/tools/jira" arrow="true" cta="View guide" />

<Card title="JSON Toolkit" icon="link" href="/oss/python/integrations/tools/json" arrow="true" cta="View guide" />

<Card title="Lemon Agent" icon="link" href="/oss/python/integrations/tools/lemonai" arrow="true" cta="View guide" />

<Card title="Linkup Search Tool" icon="link" href="/oss/python/integrations/tools/linkup_search" arrow="true" cta="View guide" />

<Card title="Memgraph" icon="link" href="/oss/python/integrations/tools/memgraph" arrow="true" cta="View guide" />

<Card title="Memorize" icon="link" href="/oss/python/integrations/tools/memorize" arrow="true" cta="View guide" />

<Card title="Mojeek Search" icon="link" href="/oss/python/integrations/tools/mojeek_search" arrow="true" cta="View guide" />

<Card title="MultiOn Toolkit" icon="link" href="/oss/python/integrations/tools/multion" arrow="true" cta="View guide" />

<Card title="NASA Toolkit" icon="link" href="/oss/python/integrations/tools/nasa" arrow="true" cta="View guide" />

<Card title="Naver Search" icon="link" href="/oss/python/integrations/tools/naver_search" arrow="true" cta="View guide" />

<Card title="Nuclia Understanding" icon="link" href="/oss/python/integrations/tools/nuclia" arrow="true" cta="View guide" />

<Card title="NVIDIA Riva" icon="link" href="/oss/python/integrations/tools/nvidia_riva" arrow="true" cta="View guide" />

<Card title="Office365 Toolkit" icon="link" href="/oss/python/integrations/tools/office365" arrow="true" cta="View guide" />

<Card title="OpenAPI Toolkit" icon="link" href="/oss/python/integrations/tools/openapi" arrow="true" cta="View guide" />

<Card title="Natural Language API Toolkits" icon="link" href="/oss/python/integrations/tools/openapi_nla" arrow="true" cta="View guide" />

<Card title="OpenGradient" icon="link" href="/oss/python/integrations/tools/opengradient_toolkit" arrow="true" cta="View guide" />

<Card title="OpenWeatherMap" icon="link" href="/oss/python/integrations/tools/openweathermap" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/tools/oracleai" arrow="true" cta="View guide" />

<Card title="Oxylabs" icon="link" href="/oss/python/integrations/tools/oxylabs" arrow="true" cta="View guide" />

<Card title="Pandas Dataframe" icon="link" href="/oss/python/integrations/tools/pandas" arrow="true" cta="View guide" />

<Card title="Passio NutritionAI" icon="link" href="/oss/python/integrations/tools/passio_nutrition_ai" arrow="true" cta="View guide" />

<Card title="Parallel Extract" icon="link" href="/oss/python/integrations/tools/parallel_extract" arrow="true" cta="View guide" />

<Card title="Parallel Search" icon="link" href="/oss/python/integrations/tools/parallel_search" arrow="true" cta="View guide" />

<Card title="Permit" icon="link" href="/oss/python/integrations/tools/permit" arrow="true" cta="View guide" />

<Card title="PlayWright Browser Toolkit" icon="link" href="/oss/python/integrations/tools/playwright" arrow="true" cta="View guide" />

<Card title="Polygon IO Toolkit" icon="link" href="/oss/python/integrations/tools/polygon" arrow="true" cta="View guide" />

<Card title="PowerBI Toolkit" icon="link" href="/oss/python/integrations/tools/powerbi" arrow="true" cta="View guide" />

<Card title="Prolog" icon="link" href="/oss/python/integrations/tools/prolog_tool" arrow="true" cta="View guide" />

<Card title="PubMed" icon="link" href="/oss/python/integrations/tools/pubmed" arrow="true" cta="View guide" />

<Card title="Python REPL" icon="link" href="/oss/python/integrations/tools/python" arrow="true" cta="View guide" />

<Card title="Reddit Search" icon="link" href="/oss/python/integrations/tools/reddit_search" arrow="true" cta="View guide" />

<Card title="Requests Toolkit" icon="link" href="/oss/python/integrations/tools/requests" arrow="true" cta="View guide" />

<Card title="Riza Code Interpreter" icon="link" href="/oss/python/integrations/tools/riza" arrow="true" cta="View guide" />

<Card title="Robocorp Toolkit" icon="link" href="/oss/python/integrations/tools/robocorp" arrow="true" cta="View guide" />

<Card title="Salesforce" icon="link" href="/oss/python/integrations/tools/salesforce" arrow="true" cta="View guide" />

<Card title="SceneXplain" icon="link" href="/oss/python/integrations/tools/sceneXplain" arrow="true" cta="View guide" />

<Card title="ScrapeGraph" icon="link" href="/oss/python/integrations/tools/scrapegraph" arrow="true" cta="View guide" />

<Card title="Scrapeless Crawl" icon="link" href="/oss/python/integrations/tools/scrapeless_crawl" arrow="true" cta="View guide" />

<Card title="Scrapeless Scraping API" icon="link" href="/oss/python/integrations/tools/scrapeless_scraping_api" arrow="true" cta="View guide" />

<Card title="Scrapeless Universal Scraping" icon="link" href="/oss/python/integrations/tools/scrapeless_universal_scraping" arrow="true" cta="View guide" />

<Card title="SearchApi" icon="link" href="/oss/python/integrations/tools/searchapi" arrow="true" cta="View guide" />

<Card title="SearxNG Search" icon="link" href="/oss/python/integrations/tools/searx_search" arrow="true" cta="View guide" />

<Card title="Semantic Scholar API" icon="link" href="/oss/python/integrations/tools/semanticscholar" arrow="true" cta="View guide" />

<Card title="SerpApi" icon="link" href="/oss/python/integrations/tools/serpapi" arrow="true" cta="View guide" />

<Card title="Slack Toolkit" icon="link" href="/oss/python/integrations/tools/slack" arrow="true" cta="View guide" />

<Card title="Spark SQL Toolkit" icon="link" href="/oss/python/integrations/tools/spark_sql" arrow="true" cta="View guide" />

<Card title="SQLDatabase Toolkit" icon="link" href="/oss/python/integrations/tools/sql_database" arrow="true" cta="View guide" />

<Card title="StackExchange" icon="link" href="/oss/python/integrations/tools/stackexchange" arrow="true" cta="View guide" />

<Card title="Steam Toolkit" icon="link" href="/oss/python/integrations/tools/steam" arrow="true" cta="View guide" />

<Card title="Stripe" icon="link" href="/oss/python/integrations/tools/stripe" arrow="true" cta="View guide" />

<Card title="Tableau" icon="link" href="/oss/python/integrations/tools/tableau" arrow="true" cta="View guide" />

<Card title="Taiga" icon="link" href="/oss/python/integrations/tools/taiga" arrow="true" cta="View guide" />

<Card title="Tavily Extract" icon="link" href="/oss/python/integrations/tools/tavily_extract" arrow="true" cta="View guide" />

<Card title="Tavily Search" icon="link" href="/oss/python/integrations/tools/tavily_search" arrow="true" cta="View guide" />

<Card title="Tilores" icon="link" href="/oss/python/integrations/tools/tilores" arrow="true" cta="View guide" />

<Card title="MCP Toolbox" icon="link" href="/oss/python/integrations/tools/mcp_toolbox" arrow="true" cta="View guide" />

<Card title="Twilio" icon="link" href="/oss/python/integrations/tools/twilio" arrow="true" cta="View guide" />

<Card title="Upstage" icon="link" href="/oss/python/integrations/tools/upstage_groundedness_check" arrow="true" cta="View guide" />

<Card title="Valthera" icon="link" href="/oss/python/integrations/tools/valthera" arrow="true" cta="View guide" />

<Card title="ValyuContext" icon="link" href="/oss/python/integrations/tools/valyu_search" arrow="true" cta="View guide" />

<Card title="Vectara" icon="link" href="/oss/python/integrations/tools/vectara" arrow="true" cta="View guide" />

<Card title="Wikidata" icon="link" href="/oss/python/integrations/tools/wikidata" arrow="true" cta="View guide" />

<Card title="Wikipedia" icon="link" href="/oss/python/integrations/tools/wikipedia" arrow="true" cta="View guide" />

<Card title="Wolfram Alpha" icon="link" href="/oss/python/integrations/tools/wolfram_alpha" arrow="true" cta="View guide" />

<Card title="WRITER Tools" icon="link" href="/oss/python/integrations/tools/writer" arrow="true" cta="View guide" />

<Card title="Yahoo Finance News" icon="link" href="/oss/python/integrations/tools/yahoo_finance_news" arrow="true" cta="View guide" />

<Card title="You.com Search" icon="link" href="/oss/python/integrations/tools/you" arrow="true" cta="View guide" />

<Card title="YouTube" icon="link" href="/oss/python/integrations/tools/youtube" arrow="true" cta="View guide" />

<Card title="Zapier Natural Language Actions" icon="link" href="/oss/python/integrations/tools/zapier" arrow="true" cta="View guide" />

<Card title="ZenGuard AI" icon="link" href="/oss/python/integrations/tools/zenguard" arrow="true" cta="View guide" />
</Columns>

<Info>
  If you'd like to contribute an integration, see [Contributing integrations](/oss/python/contributing#add-a-new-integration).
</Info>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/tools/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## tools = [DuckDuckGoSearchRun(rate_limiter=rate_limiter)]

**URL:** llms-txt#tools-=-[duckduckgosearchrun(rate_limiter=rate_limiter)]

**Contents:**
  - Simulate production data
- Convert Production Traces to Experiment
  - Select runs to backtest on

tools = [TavilySearchResults(max_results=5, rate_limiter=rate_limiter)]

agent = create_agent(gpt_3_5_turbo, tools=tools, system_prompt=instructions)
python  theme={null}
fake_production_inputs = [
    "Alan turing's early childhood",
    "Economic impacts of the European Union",
    "Underrated philosophers",
    "History of the Roxie theater in San Francisco",
    "ELI5: gravitational waves",
    "The arguments for and against a parliamentary system",
    "Pivotal moments in music history",
    "Big ideas in programming languages",
    "Big questions in biology",
    "The relationship between math and reality",
    "What makes someone funny",
]

agent.batch(
    [{"messages": [{"role": "user", "content": content}]} for content in fake_production_inputs],
)
python  theme={null}
from datetime import datetime, timedelta, timezone
from uuid import uuid4
from langsmith import Client
from langsmith.beta import convert_runs_to_test

**Examples:**

Example 1 (unknown):
```unknown
### Simulate production data

Now lets simulate some production data:
```

Example 2 (unknown):
```unknown
## Convert Production Traces to Experiment

The first step is to generate a dataset based on the production *inputs*. Then copy over all the traces to serve as a baseline experiment.

### Select runs to backtest on

You can select the runs to backtest on using the `filter` argument of `list_runs`. The `filter` argument uses the LangSmith [trace query syntax](/langsmith/trace-query-syntax) to select runs.
```

---

## "tool_calling": True,

**URL:** llms-txt#"tool_calling":-true,

---

## To approve

**URL:** llms-txt#to-approve

graph.invoke(Command(resume=True), config=config)

---

## To ensure this, we'll create vectorstore indexes for all of the artists, tracks and albums

**URL:** llms-txt#to-ensure-this,-we'll-create-vectorstore-indexes-for-all-of-the-artists,-tracks-and-albums

---

## To reject

**URL:** llms-txt#to-reject

**Contents:**
  - Review and edit state
  - Interrupts in tools
  - Validating human input
- Rules of interrupts
  - Do not wrap `interrupt` calls in try/except
  - Do not reorder `interrupt` calls within a node
  - Do not return complex values in `interrupt` calls
  - Side effects called before `interrupt` must be idempotent
- Using with subgraphs called as functions
- Debugging with interrupts

graph.invoke(Command(resume=False), config=config)
python  theme={null}
  from typing import Literal, Optional, TypedDict

from langgraph.checkpoint.memory import MemorySaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class ApprovalState(TypedDict):
      action_details: str
      status: Optional[Literal["pending", "approved", "rejected"]]

def approval_node(state: ApprovalState) -> Command[Literal["proceed", "cancel"]]:
      # Expose details so the caller can render them in a UI
      decision = interrupt({
          "question": "Approve this action?",
          "details": state["action_details"],
      })

# Route to the appropriate node after resume
      return Command(goto="proceed" if decision else "cancel")

def proceed_node(state: ApprovalState):
      return {"status": "approved"}

def cancel_node(state: ApprovalState):
      return {"status": "rejected"}

builder = StateGraph(ApprovalState)
  builder.add_node("approval", approval_node)
  builder.add_node("proceed", proceed_node)
  builder.add_node("cancel", cancel_node)
  builder.add_edge(START, "approval")
  builder.add_edge("proceed", END)
  builder.add_edge("cancel", END)

# Use a more durable checkpointer in production
  checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "approval-123"}}
  initial = graph.invoke(
      {"action_details": "Transfer $500", "status": "pending"},
      config=config,
  )
  print(initial["__interrupt__"])  # -> [Interrupt(value={'question': ..., 'details': ...})]

# Resume with the decision; True routes to proceed, False to cancel
  resumed = graph.invoke(Command(resume=True), config=config)
  print(resumed["status"])  # -> "approved"
  python  theme={null}
from langgraph.types import interrupt

def review_node(state: State):
    # Pause and show the current content for review (surfaces in result["__interrupt__"])
    edited_content = interrupt({
        "instruction": "Review and edit this content",
        "content": state["generated_text"]
    })

# Update the state with the edited version
    return {"generated_text": edited_content}
python  theme={null}
graph.invoke(
    Command(resume="The edited and improved text"),  # Value becomes the return from interrupt()
    config=config
)
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langgraph.checkpoint.memory import MemorySaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class ReviewState(TypedDict):
      generated_text: str

def review_node(state: ReviewState):
      # Ask a reviewer to edit the generated content
      updated = interrupt({
          "instruction": "Review and edit this content",
          "content": state["generated_text"],
      })
      return {"generated_text": updated}

builder = StateGraph(ReviewState)
  builder.add_node("review", review_node)
  builder.add_edge(START, "review")
  builder.add_edge("review", END)

checkpointer = MemorySaver()
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "review-42"}}
  initial = graph.invoke({"generated_text": "Initial draft"}, config=config)
  print(initial["__interrupt__"])  # -> [Interrupt(value={'instruction': ..., 'content': ...})]

# Resume with the edited text from the reviewer
  final_state = graph.invoke(
      Command(resume="Improved draft after review"),
      config=config,
  )
  print(final_state["generated_text"])  # -> "Improved draft after review"
  python  theme={null}
from langchain.tools import tool
from langgraph.types import interrupt

@tool
def send_email(to: str, subject: str, body: str):
    """Send an email to a recipient."""

# Pause before sending; payload surfaces in result["__interrupt__"]
    response = interrupt({
        "action": "send_email",
        "to": to,
        "subject": subject,
        "body": body,
        "message": "Approve sending this email?"
    })

if response.get("action") == "approve":
        # Resume value can override inputs before executing
        final_to = response.get("to", to)
        final_subject = response.get("subject", subject)
        final_body = response.get("body", body)
        return f"Email sent to {final_to} with subject '{final_subject}'"
    return "Email cancelled by user"
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langchain.tools import tool
  from langchain_anthropic import ChatAnthropic
  from langgraph.checkpoint.sqlite import SqliteSaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class AgentState(TypedDict):
      messages: list[dict]

@tool
  def send_email(to: str, subject: str, body: str):
      """Send an email to a recipient."""

# Pause before sending; payload surfaces in result["__interrupt__"]
      response = interrupt({
          "action": "send_email",
          "to": to,
          "subject": subject,
          "body": body,
          "message": "Approve sending this email?",
      })

if response.get("action") == "approve":
          final_to = response.get("to", to)
          final_subject = response.get("subject", subject)
          final_body = response.get("body", body)

# Actually send the email (your implementation here)
          print(f"[send_email] to={final_to} subject={final_subject} body={final_body}")
          return f"Email sent to {final_to}"

return "Email cancelled by user"

model = ChatAnthropic(model="claude-sonnet-4-5-20250929").bind_tools([send_email])

def agent_node(state: AgentState):
      # LLM may decide to call the tool; interrupt pauses before sending
      result = model.invoke(state["messages"])
      return {"messages": state["messages"] + [result]}

builder = StateGraph(AgentState)
  builder.add_node("agent", agent_node)
  builder.add_edge(START, "agent")
  builder.add_edge("agent", END)

checkpointer = SqliteSaver(sqlite3.connect("tool-approval.db"))
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "email-workflow"}}
  initial = graph.invoke(
      {
          "messages": [
              {"role": "user", "content": "Send an email to alice@example.com about the meeting"}
          ]
      },
      config=config,
  )
  print(initial["__interrupt__"])  # -> [Interrupt(value={'action': 'send_email', ...})]

# Resume with approval and optionally edited arguments
  resumed = graph.invoke(
      Command(resume={"action": "approve", "subject": "Updated subject"}),
      config=config,
  )
  print(resumed["messages"][-1])  # -> Tool result returned by send_email
  python  theme={null}
from langgraph.types import interrupt

def get_age_node(state: State):
    prompt = "What is your age?"

while True:
        answer = interrupt(prompt)  # payload surfaces in result["__interrupt__"]

# Validate the input
        if isinstance(answer, int) and answer > 0:
            # Valid input - continue
            break
        else:
            # Invalid input - ask again with a more specific prompt
            prompt = f"'{answer}' is not a valid age. Please enter a positive number."

return {"age": answer}
python  theme={null}
  import sqlite3
  from typing import TypedDict

from langgraph.checkpoint.sqlite import SqliteSaver
  from langgraph.graph import StateGraph, START, END
  from langgraph.types import Command, interrupt

class FormState(TypedDict):
      age: int | None

def get_age_node(state: FormState):
      prompt = "What is your age?"

while True:
          answer = interrupt(prompt)  # payload surfaces in result["__interrupt__"]

if isinstance(answer, int) and answer > 0:
              return {"age": answer}

prompt = f"'{answer}' is not a valid age. Please enter a positive number."

builder = StateGraph(FormState)
  builder.add_node("collect_age", get_age_node)
  builder.add_edge(START, "collect_age")
  builder.add_edge("collect_age", END)

checkpointer = SqliteSaver(sqlite3.connect("forms.db"))
  graph = builder.compile(checkpointer=checkpointer)

config = {"configurable": {"thread_id": "form-1"}}
  first = graph.invoke({"age": None}, config=config)
  print(first["__interrupt__"])  # -> [Interrupt(value='What is your age?', ...)]

# Provide invalid data; the node re-prompts
  retry = graph.invoke(Command(resume="thirty"), config=config)
  print(retry["__interrupt__"])  # -> [Interrupt(value="'thirty' is not a valid age...", ...)]

# Provide valid data; loop exits and state updates
  final = graph.invoke(Command(resume=30), config=config)
  print(final["age"])  # -> 30
  python Separating logic theme={null}
  def node_a(state: State):
      # ✅ Good: interrupting first, then handling
      # error conditions separately
      interrupt("What's your name?")
      try:
          fetch_data()  # This can fail
      except Exception as e:
          print(e)
      return state
  python Explicit exception handling theme={null}
  def node_a(state: State):
      # ✅ Good: catching specific exception types
      # will not catch the interrupt exception
      try:
          name = interrupt("What's your name?")
          fetch_data()  # This can fail
      except NetworkException as e:
          print(e)
      return state
  python  theme={null}
def node_a(state: State):
    # ❌ Bad: wrapping interrupt in bare try/except
    # will catch the interrupt exception
    try:
        interrupt("What's your name?")
    except Exception as e:
        print(e)
    return state
python  theme={null}
def node_a(state: State):
    # ✅ Good: interrupt calls happen in the same order every time
    name = interrupt("What's your name?")
    age = interrupt("What's your age?")
    city = interrupt("What's your city?")

return {
        "name": name,
        "age": age,
        "city": city
    }
python Skipping interrupts theme={null}
  def node_a(state: State):
      # ❌ Bad: conditionally skipping interrupts changes the order
      name = interrupt("What's your name?")

# On first run, this might skip the interrupt
      # On resume, it might not skip it - causing index mismatch
      if state.get("needs_age"):
          age = interrupt("What's your age?")

city = interrupt("What's your city?")

return {"name": name, "city": city}
  python Looping interrupts theme={null}
  def node_a(state: State):
      # ❌ Bad: looping based on non-deterministic data
      # The number of interrupts changes between executions
      results = []
      for item in state.get("dynamic_list", []):  # List might change between runs
          result = interrupt(f"Approve {item}?")
          results.append(result)

return {"results": results}
  python Simple values theme={null}
  def node_a(state: State):
      # ✅ Good: passing simple types that are serializable
      name = interrupt("What's your name?")
      count = interrupt(42)
      approved = interrupt(True)

return {"name": name, "count": count, "approved": approved}
  python Structured data theme={null}
  def node_a(state: State):
      # ✅ Good: passing dictionaries with simple values
      response = interrupt({
          "question": "Enter user details",
          "fields": ["name", "email", "age"],
          "current_values": state.get("user", {})
      })

return {"user": response}
  python Functions theme={null}
  def validate_input(value):
      return len(value) > 0

def node_a(state: State):
      # ❌ Bad: passing a function to interrupt
      # The function cannot be serialized
      response = interrupt({
          "question": "What's your name?",
          "validator": validate_input  # This will fail
      })
      return {"name": response}
  python Class instances theme={null}
  class DataProcessor:
      def __init__(self, config):
          self.config = config

def node_a(state: State):
      processor = DataProcessor({"mode": "strict"})

# ❌ Bad: passing a class instance to interrupt
      # The instance cannot be serialized
      response = interrupt({
          "question": "Enter data to process",
          "processor": processor  # This will fail
      })
      return {"result": response}
  python Idempotent operations theme={null}
  def node_a(state: State):
      # ✅ Good: using upsert operation which is idempotent
      # Running this multiple times will have the same result
      db.upsert_user(
          user_id=state["user_id"],
          status="pending_approval"
      )

approved = interrupt("Approve this change?")

return {"approved": approved}
  python Side effects after interrupt theme={null}
  def node_a(state: State):
      # ✅ Good: placing side effect after the interrupt
      # This ensures it only runs once after approval is received
      approved = interrupt("Approve this change?")

if approved:
          db.create_audit_log(
              user_id=state["user_id"],
              action="approved"
          )

return {"approved": approved}
  python Separating into different nodes theme={null}
  def approval_node(state: State):
      # ✅ Good: only handling the interrupt in this node
      approved = interrupt("Approve this change?")

return {"approved": approved}

def notification_node(state: State):
      # ✅ Good: side effect happens in a separate node
      # This runs after approval, so it only executes once
      if (state.approved):
          send_notification(
              user_id=state["user_id"],
              status="approved"
          )

return state
  python Creating records theme={null}
  def node_a(state: State):
      # ❌ Bad: creating a new record before interrupt
      # This will create duplicate records on each resume
      audit_id = db.create_audit_log({
          "user_id": state["user_id"],
          "action": "pending_approval",
          "timestamp": datetime.now()
      })

approved = interrupt("Approve this change?")

return {"approved": approved, "audit_id": audit_id}
  python Appending to lists theme={null}
  def node_a(state: State):
      # ❌ Bad: appending to a list before interrupt
      # This will add duplicate entries on each resume
      db.append_to_history(state["user_id"], "approval_requested")

approved = interrupt("Approve this change?")

return {"approved": approved}
  python  theme={null}
def node_in_parent_graph(state: State):
    some_code()  # <-- This will re-execute when resumed
    # Invoke a subgraph as a function.
    # The subgraph contains an `interrupt` call.
    subgraph_result = subgraph.invoke(some_input)
    # ...

def node_in_subgraph(state: State):
    some_other_code()  # <-- This will also re-execute when resumed
    result = interrupt("What's your name?")
    # ...
python  theme={null}
    graph = builder.compile(
        interrupt_before=["node_a"],  # [!code highlight]
        interrupt_after=["node_b", "node_c"],  # [!code highlight]
        checkpointer=checkpointer,
    )

# Pass a thread ID to the graph
    config = {
        "configurable": {
            "thread_id": "some_thread"
        }
    }

# Run the graph until the breakpoint
    graph.invoke(inputs, config=config)  # [!code highlight]

# Resume the graph
    graph.invoke(None, config=config)  # [!code highlight]
    python  theme={null}
    config = {
        "configurable": {
            "thread_id": "some_thread"
        }
    }

# Run the graph until the breakpoint
    graph.invoke(
        inputs,
        interrupt_before=["node_a"],  # [!code highlight]
        interrupt_after=["node_b", "node_c"],  # [!code highlight]
        config=config,
    )

# Resume the graph
    graph.invoke(None, config=config)  # [!code highlight]
    ```

1. `graph.invoke` is called with the `interrupt_before` and `interrupt_after` parameters. This is a run-time configuration and can be changed for every invocation.
    2. `interrupt_before` specifies the nodes where execution should pause before the node is executed.
    3. `interrupt_after` specifies the nodes where execution should pause after the node is executed.
    4. The graph is run until the first breakpoint is hit.
    5. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.
  </Tab>
</Tabs>

### Using LangGraph Studio

You can use [LangGraph Studio](/langsmith/studio) to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution.

<img src="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5aa4e7cea2ab147cef5b4e210dd6c4a1" alt="image" data-og-width="1252" width="1252" data-og-height="1040" height="1040" data-path="oss/images/static-interrupt.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=52d02b507d0a6a879f7fb88d9c6767d0 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=e363cd4980edff9bab422f4f1c0ee3c8 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=49d26a3641953c23ef3fbc51e828c305 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=2dba15683b3baa1a61bc3bcada35ae1e 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=9f9a2c0f2631c0e69cd248f6319933fe 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/static-interrupt.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=5a46b765b436ab5d0dc2f41c01ffad80 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/interrupts.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Full example">
```

Example 2 (unknown):
```unknown
</Accordion>

### Review and edit state

Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.
```

Example 3 (unknown):
```unknown
When resuming, provide the edited content:
```

Example 4 (unknown):
```unknown
<Accordion title="Full example">
```

---

## Traces: [OTel Example](/langsmith/langsmith-collector#traces)

**URL:** llms-txt#traces:-[otel-example](/langsmith/langsmith-collector#traces)

The LangSmith Backend, Platform Backend, Playground and LangSmith Queue deployments have been instrumented to emit [Otel](https://opentelemetry.io/do/langsmith/observability-concepts/signals/traces/) traces. Tracing is toggled off by default, and can be enabled for all LangSmith services with the following in your `langsmith_config.yaml` (or equivalent) file:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/export-backend.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Trace a RAG application tutorial

**URL:** llms-txt#trace-a-rag-application-tutorial

**Contents:**
- Prototyping
  - Set up your environment
  - Trace your LLM calls
  - Trace the whole chain
- Beta Testing
  - Collecting Feedback
  - Logging Metadata
- Production
  - Monitoring
  - A/B Testing

Source: https://docs.langchain.com/langsmith/observability-llm-tutorial

In this tutorial, we'll build a simple RAG application using the OpenAI SDK. We'll add observability to the application at each stage of development, from prototyping to production.

Having observability set up from the start can help you iterate **much** more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.

### Set up your environment

First, create an API key by navigating to the [settings page](https://smith.langchain.com/settings).

Next, install the LangSmith SDK:

Finally, set up the appropriate environment variables. This will log traces to the `default` project (though you can easily change that).

<Note>
  You may see these variables referenced as `LANGCHAIN_*` in other places. These are all equivalent, however the best practice is to use `LANGSMITH_TRACING`, `LANGSMITH_API_KEY`, `LANGSMITH_PROJECT`.

The `LANGSMITH_PROJECT` flag is only supported in JS SDK versions >= 0.2.16, use `LANGCHAIN_PROJECT` instead if you are using an older version.
</Note>

### Trace your LLM calls

The first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like:

Notice how we import `from langsmith.wrappers import wrap_openai` and use it to wrap the OpenAI client (`openai_client = wrap_openai(OpenAI())`).

What happens if you call it in the following way?

This will produce a trace of just the OpenAI call - it should look something like [this](https://smith.langchain.com/public/e7b7d256-10fe-4d49-a8d5-36ca8e5af0d2/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8b3ad3b0d00851bce313311efa4e8bbb" alt="Tracing tutorial openai" data-og-width="1027" width="1027" data-og-height="615" height="615" data-path="langsmith/images/tracing-tutorial-openai.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c4ee9e306124a884702a7c0f5685e279 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=995ebe3b7342ea797887a052f962919d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=24e6e8b84336197d23ed294d4d37c842 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=822b72e7b0ea10a96cc05eeeefad7b51 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=985061817d26e8f76b50d2638b34ecb7 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-openai.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=bf5bb01fa39af440f985666c39809cc5 2500w" />

### Trace the whole chain

Great - we've traced the LLM call. But it's often very informative to trace more than that. LangSmith is **built** for tracing the entire LLM pipeline - so let's do that! We can do this by modifying the code to now look something like this:

Notice how we import `from langsmith import traceable` and use it decorate the overall function (`@traceable`).

What happens if you call it in the following way?

This will produce a trace of the entire RAG pipeline - it should look something like [this](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=00afea1ffa117b90159d30a53aac5a7f" alt="Tracing tutorial chain" data-og-width="1016" width="1016" data-og-height="635" height="635" data-path="langsmith/images/tracing-tutorial-chain.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=01eb0588af8534c636796b1ffc673a14 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=38625d3b2d93cd41b344bc2610272ff4 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f713b97cfd693fec7ea51d85d06c1358 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=039262907891da16b2a13d69ce3650ac 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=97e50a697c1ac249ca91a38c42beaaa9 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-chain.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=871e76f1794d7c3d0d7fc3bb875fd613 2500w" />

The next stage of LLM application development is beta testing your application. This is when you release it to a few initial users. Having good observability set up here is crucial as often you don't know exactly how users will actually use your application, so this allows you get insights into how they do so. This also means that you probably want to make some changes to your tracing set up to better allow for that. This extends the observability you set up in the previous section

### Collecting Feedback

A huge part of having good observability during beta testing is collecting feedback. What feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start. After logging that feedback, you need to be able to easily associate it with the run that caused that. Luckily LangSmith makes it easy to do that.

First, you need to log the feedback from your app. An easy way to do this is to keep track of a run ID for each run, and then use that to log feedback. Keeping track of the run ID would look something like:

Associating feedback with that run would look something like:

Once the feedback is logged, you can then see it associated with each run by clicking into the `Metadata` tab when inspecting the run. It should look something like [this](https://smith.langchain.com/public/8cafba6a-1a6d-4a73-8565-483186f31c29/r)

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cb81a556fe40895ebb29d4428d4c62d9" alt="Tracing tutorial feedback" data-og-width="1025" width="1025" data-og-height="345" height="345" data-path="langsmith/images/tracing-tutorial-feedback.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=165c08ee4c4f96f9f3ebb6e8183dc539 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f50e70bda816d314ac233430fe5703be 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=579ca04b25401fd98dbd1109e55f4a8c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9763f2b2f5cbb347f796e2c1951102cf 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8c84bd91d25dbb0514385aabf8b2dbc2 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-feedback.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b9f44b64e395eb392e9a6a0348189e84 2500w" />

You can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table. You can do this by creating a filter like the following:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=57ebc19f2e5443c21353064c082971bc" alt="Tracing tutorial filtering" data-og-width="940" width="940" data-og-height="496" height="496" data-path="langsmith/images/tracing-tutorial-filtering.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=630d2ed8d85794026cbf07fcc186f791 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9bc6f8a5464f3c0bbdcf172ecb3e1f67 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=9daf8b6d1ecbc23112dba100a77bc0a6 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fa18baf671267ec4a1bfce3cfdc1c789 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=b3184ab4bfe8361d1d52b4e63c054e58 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-filtering.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=2e1aeade5945fe20527a58c1966b5a10 2500w" />

It is also a good idea to start logging metadata. This allows you to start keep track of different attributes of your app. This is important in allowing you to know what version or variant of your app was used to produce a given result.

For this example, we will log the LLM used. Oftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering. In order to do that, we can add it as such:

Notice we added `@traceable(metadata={"llm": "gpt-4o-mini"})` to the `rag` function.

Keeping track of metadata in this way assumes that it is known ahead of time. This is fine for LLM types, but less desirable for other types of information - like a User ID. In order to log information that, we can pass it in at run time with the run ID.

Now that we've logged these two pieces of metadata, we should be able to see them both show up in the UI [here](https://smith.langchain.com/public/37adf7e5-97aa-42d0-9850-99c0199bddf6/r).

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=db49738eba3e0ce26514df3c9b72f87c" alt="Tracing tutorial metadata" data-og-width="1016" width="1016" data-og-height="337" height="337" data-path="langsmith/images/tracing-tutorial-metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c9f46f9e36cc47a6b10cc37870e17ffc 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=dba6e396adec67b74df789cfd9cd2491 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e31ff00027f6c5741935cec40d997697 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=cf82f0eca41b0fb04ae181e38ae47ece 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=fd57501d821496d68d5921e3cb8ce457 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=568f6fb60e84333f677775817dbc79e8 2500w" />

We can filter for these pieces of information by constructing a filter like the following:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=3662dfc2d8fb2c274f622e3f67e14b34" alt="Tracing tutorial metadata filtering" data-og-width="932" width="932" data-og-height="436" height="436" data-path="langsmith/images/tracing-tutorial-metadata-filtering.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=561f6c48361533d9ddcfc005136ff6c3 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1ea92820bc977b16368e89d923a631f5 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=137d1faa53ec75e8aaecf3aa2da32378 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f9c7902430dbae7b10543bd9b747a5fc 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1494db66f66fceba483829a249a44a31 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-metadata-filtering.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=06c2f159f5ce8bf601640a8484be187f 2500w" />

Great - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well. Time to ship it to production! What new observability do you need to add?

First of all, let's note that the same observability you've already added will keep on providing value in production. You will continue to be able to drill down into particular runs.

In production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time. Luckily, LangSmith has a set of tools to help with observability in production.

If you click on the `Monitor` tab in a project, you will see a series of monitoring charts. Here we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc. You can view these over time across a few different time bins.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=74f49882d9e6323e2ed467b525b81b9a" alt="Tracing tutorial monitor" data-og-width="946" width="946" data-og-height="746" height="746" data-path="langsmith/images/tracing-tutorial-monitor.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d99a10225425733cc18b1da00c04ff27 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=88a2737937bfa46e5d44f665a757fb68 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=5825a3a92fc134fe8fb8f8f4a7b46e55 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=848482d02d40236a1911a9623c39a2f0 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=278ba4c130e01f1042d49069093a7285 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dcef7cc8a6be73d02b2030f2bb9dc783 2500w" />

<Note>
  Group-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key.
</Note>

You can also use this tab to perform a version of A/B Testing. In the previous tutorial we starting tracking a few different metadata attributes - one of which was `llm`. We can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time. This allows us to experiment with different LLMs (or prompts, or other) and track their performance over time.

In order to do this, we just need to click on the `Metadata` button at the top. This will give us a drop down of options to choose from to group by:

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=dc91e44dac01fcb3966c7b57b1f41d66" alt="Tracing tutorial monitor metadata" data-og-width="957" width="957" data-og-height="534" height="534" data-path="langsmith/images/tracing-tutorial-monitor-metadata.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8ddc7be16e3b0f6e09e83af55fbe917f 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=cbc8c4287707ff477695f12d9220c55f 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=70345f5ed014329e07046681734f412f 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=73662cff14eebb9498e295834e6b8c86 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7a5e5124ffbc658a5a97d372f41b96c0 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-metadata.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=24dcb9a46ea8684c2fedd0b3f136d8a0 2500w" />

Once we select this, we will start to see charts grouped by this attribute:

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a1bf8fb453d7721d85bca20fbd7cb431" alt="Tracing tutorial monitor grouped" data-og-width="973" width="973" data-og-height="621" height="621" data-path="langsmith/images/tracing-tutorial-monitor-grouped.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=896d99b5a7456e10d92aa58c8d3bb6d8 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=6cc318a5f30ed0e8e11accf1d6f7428d 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=892af43a1ce178ec34ed4124a7a0f5d4 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=7be53f1392b471c45b7d14febe0df6f9 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=61d68a5703c7e69fe4c96df6a861ab37 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/tracing-tutorial-monitor-grouped.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e0a3af90e154e85c45f3de1cb0a908d5 2500w" />

One of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify as problematic while looking at monitoring charts. In order to do this, you can simply hover over a datapoint in the monitoring chart. When you do this, you will be able to click the datapoint. This will lead you back to the runs table with a filtered view:

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1ca02e0473f1fdfff102f2ccba371828" alt="Tracing tutorial monitor drilldown" data-og-width="952" width="952" data-og-height="708" height="708" data-path="langsmith/images/tracing-tutorial-monitor-drilldown.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ddf0256ff1e85656a8339e16a652480d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=0fe248ccc799c0cef9e661c861e81605 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4c3f95dd9e31e1884f569bbf736b852c 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8ce416eec3bdc7b4289ba9fbedf6959a 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=a2f93d8d48a5cf33f2f4b664c72644a7 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tracing-tutorial-monitor-drilldown.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=506f485087e2e7ab91e7a53c44fd8205 2500w" />

In this tutorial you saw how to set up your LLM application with best-in-class observability. No matter what stage your application is in, you will still benefit from observability.

If you have more in-depth questions about observability, check out the [how-to section](/langsmith/observability-concepts) for guides on topics like testing, prompt management, and more.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/observability-llm-tutorial.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Prototyping

Having observability set up from the start can help you iterate **much** more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.

### Set up your environment

First, create an API key by navigating to the [settings page](https://smith.langchain.com/settings).

Next, install the LangSmith SDK:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

Finally, set up the appropriate environment variables. This will log traces to the `default` project (though you can easily change that).
```

---

## Trace Claude Code

**URL:** llms-txt#trace-claude-code

**Contents:**
- How it works
- Prerequisites
- 1. Create the hook script
- 2. Configure the global hook
- 3. Enable Tracing
- 4. Verify Setup
- Troubleshooting
  - No traces appearing in LangSmith
  - Permission errors
  - Required commands not found

Source: https://docs.langchain.com/langsmith/trace-claude-code

This guide shows you how to automatically send conversations from the [Claude Code CLI](https://code.claude.com/docs/en/overview) to LangSmith.

Once configured, you can opt-in to sending traces from Claude Code projects to LangSmith. Traces will include user messages, tool calls and assistant responses.

<div style={{ textAlign: 'center' }}>
  <img className="block dark:hidden" src="https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace.png?fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=d59e2e68fb03e868ddf5d6d2887c25e6" alt="LangSmith UI showing trace from Claude Code." data-og-width="2478" width="2478" data-og-height="1596" height="1596" data-path="langsmith/images/claude-code-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace.png?w=280&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=d1cec3f7adf1c3cbad885e77918f9017 280w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace.png?w=560&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=d3fceeffc8c9796cb3d2d2a95d9f9785 560w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace.png?w=840&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=531f7631f8c70fb5cbb021053bec3d29 840w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace.png?w=1100&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=37355ed63ffecac752419053857f4f68 1100w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace.png?w=1650&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=5bb693851b27f81483b25c06e917d582 1650w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace.png?w=2500&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=a6ad02ca2d3ba75f7a696040a5e8db11 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace-dark.png?fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=81330e8bbd01d5ef740a7b7472e5e173" alt="LangSmith UI showing trace from Claude Code." data-og-width="2480" width="2480" data-og-height="1564" height="1564" data-path="langsmith/images/claude-code-trace-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace-dark.png?w=280&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=c8d35a77173846ba56d3cf841a5b663a 280w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace-dark.png?w=560&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=61083b3e1bf841ed1e2df85233410fad 560w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace-dark.png?w=840&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=a782d8898b2a38c41a7ff5b89a2adaed 840w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace-dark.png?w=1100&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=e4e089ff508bc5e2bdf89052aa0fde5d 1100w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace-dark.png?w=1650&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=ea8be6805edcdb5cb3a2047c0dea216b 1650w, https://mintcdn.com/langchain-5e9cc07a/ibZBFaqiLl4UuJmc/langsmith/images/claude-code-trace-dark.png?w=2500&fit=max&auto=format&n=ibZBFaqiLl4UuJmc&q=85&s=a9266da4f4042d694b90474639cb2a29 2500w" />
</div>

1. A global "Stop" [hook](https://code.claude.com/docs/en/hooks-guide#get-started-with-claude-code-hooks) is configured to run each time Claude Code responds.
2. The hook reads Claude Code’s generated conversation transcripts.
3. Messages in the transcript are converted into LangSmith runs and sent to your LangSmith project.

<Note> Tracing is opt-in and is enabled per Claude Code project using environment variables. </Note>

Before setting up tracing, ensure you have:

* **Claude Code CLI** installed.
* **LangSmith API key** ([get it here](https://smith.langchain.com/settings/apikeys)).
* **Command-line tool** `jq` - JSON processor ([install guide](https://jqlang.github.io/jq/download/))

<Info>
  This guide currently only supports macOS.
</Info>

## 1. Create the hook script

`stop_hook.sh` processes Claude Code's generated conversation transcripts and sends traces to LangSmith. Create the file `~/.claude/hooks/stop_hook.sh` with the following script:

<sup>Last Updated: 2025-12-21</sup>

<Accordion title="`stop_hook.sh` file">
  
</Accordion>

## 2. Configure the global hook

Set up a global hook in `~/.claude/settings.json` that runs the `stop_hook.sh` script. The global setting enables you to easily trace any Claude Code CLI project.

In `~/.claude/settings.json`, add the `Stop` hook.

For each Claude Code project (a Claude Code project is a directory with Claude Code initialized) where you want tracing enabled, create or edit [Claude Code's project settings file](https://code.claude.com/docs/en/settings#:~:text=Project%20settings%20are%20saved%20in%20your%20project%20directory%3A) `.claude/settings.local.json` to include the following environment variables:

* `TRACE_TO_LANGSMITH: "true"` - Enables tracing for this project. Remove or set to `false` to disable tracing.
* `CC_LANGSMITH_API_KEY` - Your LangSmith API key
* `CC_LANGSMITH_PROJECT` - The LangSmith project name where traces are sent
* (optional) `CC_LANGSMITH_DEBUG: "true"` - Enables detailed debug logging. Remove or set to `false` to disable tracing.

<Note> Alternativley, to enable tracing to LangSmith for all Claude Code sessions, you can add the above JSON to your [global Claude Code settings.json](https://code.claude.com/docs/en/settings#:~:text=User%20settings%20are%20defined%20in%20~/.claude/settings.json%20and%20apply%20to%20all%20projects.) file. </Note>

Start a Claude Code session in your configured project. Traces will appear in LangSmith after Claude Code responds.

In LangSmith, you'll see:

* Each message to Claude Code appears as a trace.
* All turns from the same Claude Code session are grouped using a shared `thread_id` and can be viewed in the **Threads** tab of a project.

### No traces appearing in LangSmith

1. **Check the hook is running**:
   
   You should see log entries after each Claude response.

2. **Verify environment variables**:
   * Check that `TRACE_TO_LANGSMITH="true"` in your project's `.claude/settings.local.json`
   * Verify your API key is correct (starts with `lsv2_pt_`)
   * Ensure the project name exists in LangSmith

3. **Enable debug mode** to see detailed API activity:
   
   Then check logs for API calls and HTTP status codes.

### Permission errors

Make sure the hook script is executable:

### Required commands not found

Verify all required commands are installed:

* **macOS**: `brew install jq`
* **Ubuntu/Debian**: `sudo apt-get install jq`

### Managing log file size

The hook logs all activity to `~/.claude/state/hook.log`. With debug mode enabled, this file can grow large:

```bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
</Accordion>

Make it executable:
```

Example 2 (unknown):
```unknown
## 2. Configure the global hook

Set up a global hook in `~/.claude/settings.json` that runs the `stop_hook.sh` script. The global setting enables you to easily trace any Claude Code CLI project.

In `~/.claude/settings.json`, add the `Stop` hook.
```

Example 3 (unknown):
```unknown
## 3. Enable Tracing

For each Claude Code project (a Claude Code project is a directory with Claude Code initialized) where you want tracing enabled, create or edit [Claude Code's project settings file](https://code.claude.com/docs/en/settings#:~:text=Project%20settings%20are%20saved%20in%20your%20project%20directory%3A) `.claude/settings.local.json` to include the following environment variables:

* `TRACE_TO_LANGSMITH: "true"` - Enables tracing for this project. Remove or set to `false` to disable tracing.
* `CC_LANGSMITH_API_KEY` - Your LangSmith API key
* `CC_LANGSMITH_PROJECT` - The LangSmith project name where traces are sent
* (optional) `CC_LANGSMITH_DEBUG: "true"` - Enables detailed debug logging. Remove or set to `false` to disable tracing.
```

Example 4 (unknown):
```unknown
<Note> Alternativley, to enable tracing to LangSmith for all Claude Code sessions, you can add the above JSON to your [global Claude Code settings.json](https://code.claude.com/docs/en/settings#:~:text=User%20settings%20are%20defined%20in%20~/.claude/settings.json%20and%20apply%20to%20all%20projects.) file. </Note>

## 4. Verify Setup

Start a Claude Code session in your configured project. Traces will appear in LangSmith after Claude Code responds.

In LangSmith, you'll see:

* Each message to Claude Code appears as a trace.
* All turns from the same Claude Code session are grouped using a shared `thread_id` and can be viewed in the **Threads** tab of a project.

## Troubleshooting

### No traces appearing in LangSmith

1. **Check the hook is running**:
```

---

## Trace generator functions

**URL:** llms-txt#trace-generator-functions

**Contents:**
- Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

Source: https://docs.langchain.com/langsmith/trace-generator-functions

In most LLM applications, you will want to stream outputs to minimize the time to the first token seen by the user.

LangSmith's tracing functionality natively supports streamed outputs via `generator` functions. Below is an example.

## Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

By default, the `outputs` of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the `aggregate` option (`reduce_fn` in python). This is especially useful for aggregating streamed LLM outputs.

<Note>
  Aggregating outputs **only** impacts the traced representation of the outputs. It doesn not alter the values returned by your function.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-generator-functions.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Aggregate Results[](#aggregate-results "Direct link to Aggregate Results")

By default, the `outputs` of the traced function are aggregated into a single array in LangSmith. If you want to customize how it is stored (for instance, concatenating the outputs into a single string), you can use the `aggregate` option (`reduce_fn` in python). This is especially useful for aggregating streamed LLM outputs.

<Note>
  Aggregating outputs **only** impacts the traced representation of the outputs. It doesn not alter the values returned by your function.
</Note>

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Trace JS functions in serverless environments

**URL:** llms-txt#trace-js-functions-in-serverless-environments

**Contents:**
- Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

Source: https://docs.langchain.com/langsmith/serverless-environments

<Note>
  This section is relevant for those using the LangSmith JS SDK version 0.2.0 and higher. If you are tracing using LangChain.js or LangGraph.js in serverless environments, see [this guide](https://js.langchain.com/docs/how_to/callbacks_serverless).
</Note>

When tracing JavaScript functions, LangSmith will trace runs in the background by default to avoid adding latency. In serverless environments where the execution context may be terminated abruptly, it's important to ensure that all tracing data is properly flushed before the function completes.

To make sure this occurs, you can either:

* Set an environment variable named `LANGSMITH_TRACING_BACKGROUND` to `"false"`. This will cause your traced functions to wait for tracing to complete before returning.
  * Note that this is named differently from the [environment variable](https://js.langchain.com/docs/how_to/callbacks_serverless) in LangChain.js because LangSmith can be used without LangChain.
* Pass a custom client into your traced runs and `await` the `client.awaitPendingTraceBatches();` method.

Here's an example of using `awaitPendingTraceBatches` alongside the [`traceable`](/langsmith/annotate-code) method:

## Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

By default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.

This works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.

If you are seeing rate limit errors related to this, you can try setting `manualFlushMode: true` in your client like this:

And then manually calling `client.flush()` like this before your serverless function closes:

Note that this will prevent runs from appearing in the LangSmith UI until you call `.flush()`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/serverless-environments.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Rate limits at high concurrency[](#rate-limits-at-high-concurrency "Direct link to Rate limits at high concurrency")

By default, the LangSmith client will batch operations as your traced run executions, sending a new batch every few milliseconds.

This works well in most situations, but if your traced function is long-running and you have very high concurrency, you may also hit rate limits related to overall request count.

If you are seeing rate limit errors related to this, you can try setting `manualFlushMode: true` in your client like this:
```

Example 2 (unknown):
```unknown
And then manually calling `client.flush()` like this before your serverless function closes:
```

---

## Trace query syntax

**URL:** llms-txt#trace-query-syntax

**Contents:**
- Filter arguments
- Filter query language

Source: https://docs.langchain.com/langsmith/trace-query-syntax

Using the method in the SDK or endpoint in the API, you can filter runs to analyze and export.

| Keys                          | Description                                                                                                                                                                                                                    |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `project_id` / `project_name` | The project(s) to fetch runs from - can be a single project or a list of projects.                                                                                                                                             |
| `trace_id`                    | Fetch runs that are part of a specific trace.                                                                                                                                                                                  |
| `run_type`                    | The type of run to get, such as `llm`, `chain`, `tool`, `retriever`, etc.                                                                                                                                                      |
| `dataset_name` / `dataset_id` | Fetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset.                                                                              |
| `reference_example_id`        | Fetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input.                                                                                                   |
| `parent_run_id`               | Fetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory.                                                                  |
| `error`                       | Fetch runs that errored or did not error.                                                                                                                                                                                      |
| `run_ids`                     | Fetch runs with a given list of run ids. Note: **This will ignore all other filtering arguments.**                                                                                                                             |
| `filter`                      | Fetch runs that match a given structured filter statement. See the guide below for more information.                                                                                                                           |
| `trace_filter`                | Filter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of the root run within a trace.                            |
| `tree_filter`                 | Filter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular `filter` parameter to let you filter runs by attributes of any run within a trace. |
| `is_root`                     | Only return root runs.                                                                                                                                                                                                         |
| `select`                      | Select the fields to return in the response. By default, all fields are returned. See [run data format](/langsmith/run-data-format) for available fields.                                                                      |
| `query` (*experimental*)      | Natural language query, which translates your query into a filter statement.                                                                                                                                                   |

<Note>
  **Performance tip**: Passing the `select` parameter and excluding `inputs` and `outputs` from the list can significantly improve query performance and reduce response sizes, especially for large runs.
</Note>

## Filter query language

LangSmith supports powerful filtering capabilities with a filter query language to permit complex filtering operations when fetching runs.

The filtering grammar is based on common comparators on fields in the run object. Supported comparators include:

* `gte` (greater than or equal to)
* `gt` (greater than)
* `lte` (less than or equal to)
* `lt` (less than)
* `eq` (equal to)
* `neq` (not equal to)
* `has` (check if run contains a tag or metadata json blob)
* `search` (search for a substring in a string field)

Additionally, you can combine multiple comparisons through the `and` operator.

These can be applied on fields of the run object, such as its `id`, `name`, `run_type`, `start_time` / `end_time`, `latency`, `total_tokens`, `error`, `execution_order`, `tags`, and any associated feedback through `feedback_key` and `feedback_score`.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-query-syntax.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Trace without setting environment variables

**URL:** llms-txt#trace-without-setting-environment-variables

Source: https://docs.langchain.com/langsmith/trace-without-env-vars

As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:

* `LANGSMITH_TRACING`
* `LANGSMITH_API_KEY`
* `LANGSMITH_ENDPOINT`
* `LANGSMITH_PROJECT`

If you need to trace runs with a custom configuration, are working in an environment that doesn’t support typical environment variables (such as Cloudflare Workers), or would simply prefer not to rely on environment variables, LangSmith allows you to configure tracing programmatically.

<Warning>
  Due to a number of asks for finer-grained control of tracing using the `trace` context manager, **we changed the behavior** of `with trace` to honor the `LANGSMITH_TRACING` environment variable in version **0.1.95** of the Python SDK. You can find more details in the [release notes](https://github.com/langchain-ai/langsmith-sdk/releases/tag/v0.1.95). The recommended way to disable/enable tracing without setting environment variables is to use the `with tracing_context` context manager, as shown in the example below.
</Warning>

* Python: The recommended way to do this in Python is to use the `tracing_context` context manager. This works for both code annotated with `traceable` and code within the `trace` context manager.
* TypeScript: You can pass in both the client and the `tracingEnabled` flag to the `traceable` decorator.

If you prefer a video tutorial, check out the [Alternative Ways to Trace video](https://academy.langchain.com/pages/intro-to-langsmith-preview) from the Introduction to LangSmith Course.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-without-env-vars.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Trace with Anthropic

**URL:** llms-txt#trace-with-anthropic

Source: https://docs.langchain.com/langsmith/trace-anthropic

The `wrap_anthropic` methods in Python allows you to wrap your Anthropic client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_anthropic`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

```python  theme={null}
import anthropic
from langsmith import traceable
from langsmith.wrappers import wrap_anthropic

client = wrap_anthropic(anthropic.Anthropic())

---

## Trace with API

**URL:** llms-txt#trace-with-api

**Contents:**
- Basic tracing

Source: https://docs.langchain.com/langsmith/trace-with-api

Learn how to trace your LLM applications using the LangSmith API directly.

It is **highly** recommended to use our Python or TypeScript SDKs to send traces to LangSmith. We have designed these SDKs with optimizations like batching and backgrounding to ensure that your application's performance is not impacted by sending traces to LangSmith. However, if you are unable to use our SDKs, you can use the LangSmith REST API to send traces. Performance may be impacted if you send traces synchronously in your application. This guide will show you how to trace a request using the LangSmith REST API. Please view our API documentation  for a full list of endpoints and request/response schemas.

The simplest way to log runs is via the POST and PATCH `/runs` endpoint. These routes expect minimal contextual information about the tree structure to

<Note>
  When using the LangSmith REST API, you will need to provide your API key in the request headers as `"x-api-key"`.

If your API key is linked to multiple workspaces, you will need to specify the workspace being used in the header with `"x-tenant-id"`.

In the simple example, you do not need to set the `dotted_order` opr `trace_id` fields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.
</Note>

The following example shows how you might leverage our API directly in Python. The same principles apply to other languages.

```python  theme={null}
import openai
import os
import requests
from datetime import datetime, timezone
from langsmith import uuid7

---

## Trace with AutoGen

**URL:** llms-txt#trace-with-autogen

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-autogen

LangSmith can capture traces generated by [AutoGen](https://microsoft.github.io/autogen/stable/) using OpenInference's AutoGen instrumentation. This guide shows you how to automatically capture traces from your AutoGen multi-agent conversations and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your AutoGen application, import and configure the LangSmith OpenTelemetry integration along with the AutoGen and OpenAI instrumentors:

```python  theme={null}
from langsmith.integrations.otel import configure
from openinference.instrumentation.autogen import AutogenInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Info>
  Requires LangSmith Python SDK version `langsmith>=0.4.26` for optimal OpenTelemetry support.
</Info>

## Setup

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your AutoGen application, import and configure the LangSmith OpenTelemetry integration along with the AutoGen and OpenAI instrumentors:
```

---

## Trace with CrewAI

**URL:** llms-txt#trace-with-crewai

**Contents:**
- Installation
- Setup
  - 1. Configure environment variables
  - 2. Configure OpenTelemetry integration

Source: https://docs.langchain.com/langsmith/trace-with-crewai

LangSmith can capture traces generated by [CrewAI](https://github.com/crewAIInc/crewAI) using OpenInference's CrewAI instrumentation. This guide shows you how to automatically capture traces from your CrewAI multi-agent workflows and send them to LangSmith for monitoring and analysis.

Install the required packages using your preferred package manager:

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
  
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your CrewAI application, import and configure the LangSmith OpenTelemetry integration along with the CrewAI and OpenAI instrumentors:

```python  theme={null}
from langsmith.integrations.otel import OtelSpanProcessor
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from openinference.instrumentation.crewai import CrewAIInstrumentor
from openinference.instrumentation.openai import OpenAIInstrumentor

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Setup

### 1. Configure environment variables

Set your API keys and project name:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

### 2. Configure OpenTelemetry integration

In your CrewAI application, import and configure the LangSmith OpenTelemetry integration along with the CrewAI and OpenAI instrumentors:
```

---

## Trace with Instructor

**URL:** llms-txt#trace-with-instructor

Source: https://docs.langchain.com/langsmith/trace-with-instructor

LangSmith provides a convenient integration with [Instructor](https://python.useinstructor.com/), a popular open-source library for generating structured output with LLMs.

In order to use, you first need to set your LangSmith API key.

```shell  theme={null}
export LANGSMITH_API_KEY=<your-api-key>

---

## Trace with OpenAI

**URL:** llms-txt#trace-with-openai

Source: https://docs.langchain.com/langsmith/trace-openai

The `wrap_openai`/`wrapOpenAI` methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. Also note that the wrapper works seamlessly with the `@traceable` decorator or `traceable` function and you can use both in the same application.

<Note>
  The `LANGSMITH_TRACING` environment variable must be set to `'true'` in order for traces to be logged to LangSmith, even when using `wrap_openai` or `wrapOpenAI`. This allows you to toggle tracing on and off without changing your code.

Additionally, you will need to set the `LANGSMITH_API_KEY` environment variable to your API key (see [Setup](/) for more information).

If your LangSmith API key is linked to multiple workspaces, set the `LANGSMITH_WORKSPACE_ID` environment variable to specify which workspace to use.

By default, the traces will be logged to a project named `default`. To log traces to a different project, see [this section](/langsmith/log-traces-to-project).
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-openai.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Trace with OpenTelemetry

**URL:** llms-txt#trace-with-opentelemetry

**Contents:**
- Trace a LangChain application
- Trace a non-LangChain application
- Send traces to an alternate provider
  - Use environment variables for global configuration
  - Configure alternate OTLP endpoints

Source: https://docs.langchain.com/langsmith/trace-with-opentelemetry

LangSmith supports OpenTelemetry-based tracing, allowing you to send traces from any OpenTelemetry-compatible application. This guide covers both automatic instrumentation for LangChain applications and manual instrumentation for other frameworks.

Learn how to trace your LLM applications using OpenTelemetry with LangSmith.

<Note>
  Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use `eu.api.smith.langchain.com`.
</Note>

## Trace a LangChain application

If you're using LangChain or LangGraph, use the built-in integration to trace your application:

1. Install the LangSmith package with OpenTelemetry support:

<CodeGroup>
     
   </CodeGroup>

<Info>
     Requires Python SDK version `langsmith>=0.3.18`. We recommend `langsmith>=0.4.25` to benefit from important OpenTelemetry fixes.
   </Info>

2. In your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the `LANGSMITH_OTEL_ENABLED` environment variable:

<CodeGroup>
     
   </CodeGroup>

3. Create a LangChain application with tracing. For example:

4. View the traces in your LangSmith dashboard ([example](https://smith.langchain.com/public/a762af6c-b67d-4f22-90a0-728df16baeba/r)) once your application runs.

## Trace a non-LangChain application

For non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client. (We recommend **langsmith ≥ 0.4.25**.)

1. Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:

<CodeGroup>
     
   </CodeGroup>

2. Setup environment variables for the endpoint, substitute your specific values:

<CodeGroup>
     
   </CodeGroup>

<Note>
     Depending on how your otel exporter is configured, you may need to append `/v1/traces` to the endpoint if you are only sending traces.
   </Note>

<Note>
     If you're self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append `/api/v1`. For example: `OTEL_EXPORTER_OTLP_ENDPOINT=https://ai-company.com/api/v1/otel`
   </Note>

Optional: Specify a custom project name other than "default":

<CodeGroup>
     
   </CodeGroup>

This code sets up an OTEL tracer and exporter that will send traces to LangSmith. It then calls OpenAI and sends the required OpenTelemetry attributes.

4. View the trace in your LangSmith dashboard ([example](https://smith.langchain.com/public/4f2890b1-f105-44aa-a6cf-c777dcc27a37/r)).

## Send traces to an alternate provider

While LangSmith is the default destination for OpenTelemetry traces, you can also configure OpenTelemetry to send traces to other observability platforms.

<Info>
  Available in LangSmith Python SDK **≥ 0.4.1**. We recommend **≥ 0.4.25** for fixes that improve OTEL export and hybrid fan-out stability.
</Info>

### Use environment variables for global configuration

By default, the LangSmith OpenTelemetry exporter will send data to the LangSmith API OTEL endpoint, but this can be customized by setting standard OTEL environment variables:

LangSmith uses the HTTP trace exporter by default. If you'd like to use your own tracing provider, you can either:

1. Set the OTEL environment variables as shown above, or
2. Set a global trace provider before initializing LangChain components, which LangSmith will detect and use instead of creating its own.

### Configure alternate OTLP endpoints

To send traces to a different provider, configure the OTLP exporter with your provider's endpoint:

```python  theme={null}
import os
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

**Examples:**

Example 1 (unknown):
```unknown
</CodeGroup>

   <Info>
     Requires Python SDK version `langsmith>=0.3.18`. We recommend `langsmith>=0.4.25` to benefit from important OpenTelemetry fixes.
   </Info>

2. In your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the `LANGSMITH_OTEL_ENABLED` environment variable:

   <CodeGroup>
```

Example 2 (unknown):
```unknown
</CodeGroup>

3. Create a LangChain application with tracing. For example:
```

Example 3 (unknown):
```unknown
4. View the traces in your LangSmith dashboard ([example](https://smith.langchain.com/public/a762af6c-b67d-4f22-90a0-728df16baeba/r)) once your application runs.

## Trace a non-LangChain application

For non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client. (We recommend **langsmith ≥ 0.4.25**.)

1. Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package:

   <CodeGroup>
```

Example 4 (unknown):
```unknown
</CodeGroup>

2. Setup environment variables for the endpoint, substitute your specific values:

   <CodeGroup>
```

---

## Trace with the Vercel AI SDK (JS/TS only)

**URL:** llms-txt#trace-with-the-vercel-ai-sdk-(js/ts-only)

**Contents:**
- Installation
- Environment configuration
- Basic setup
  - With `traceable`
- Tracing in serverless environments
- Passing LangSmith config
- Redacting data

Source: https://docs.langchain.com/langsmith/trace-with-vercel-ai-sdk

You can use LangSmith to trace runs from the Vercel AI SDK. This guide will walk through an example.

<Note>
  This wrapper requires AI SDK v5 and `langsmith>=0.3.63`. If you are using an older version of the AI SDK or `langsmith`, see the OpenTelemetry (OTEL)
  based approach [on this page](/langsmith/legacy-trace-with-vercel-ai-sdk).
</Note>

Install the Vercel AI SDK. This guide uses Vercel's OpenAI integration for the code snippets below, but you can use any of their other options as well.

## Environment configuration

<CodeGroup>
  
</CodeGroup>

Import and wrap AI SDK methods, then use them as you normally would:

You should see a trace in your LangSmith dashboard [like this one](https://smith.langchain.com/public/4f0e689e-c801-44d3-8857-93b47ab100cc/r).

You can also trace runs with tool calls:

Which results in a trace like [this one](https://smith.langchain.com/public/6075fa2c-d255-4885-a66a-4fc798afaa9f/r).

You can use other AI SDK methods exactly as you usually would.

You can wrap `traceable` calls around AI SDK calls or within AI SDK tool calls. This is useful if you
want to group runs together in LangSmith:

The resulting trace will look [like this](https://smith.langchain.com/public/ff25bc26-9389-4798-8b91-2bdcc95d4a8e/r).

## Tracing in serverless environments

When tracing in serverless environments, you must wait for all runs to flush before your environment
shuts down. To do this, you can pass a LangSmith [`Client`](https://docs.smith.langchain.com/reference/js/classes/client.Client) instance when wrapping the AI SDK method,
then call `await client.awaitPendingTraceBatches()`.
Make sure to also pass it into any `traceable` wrappers you create as well:

If you are using `Next.js`, there is a convenient [`after`](https://nextjs.org/docs/app/api-reference/functions/after) hook
where you can put this logic:

See [this page](/langsmith/serverless-environments) for more detail, including information
around managing rate limits in serverless environments.

## Passing LangSmith config

You can pass LangSmith-specific config to your wrapper both when initially wrapping your
AI SDK methods and while running them via `providerOptions.langsmith`.
This includes metadata (which you can later use to filter runs in LangSmith), top-level run name,
tags, custom client instances, and more.

Config passed while wrapping will apply to all future calls you make with the wrapped method:

While passing config at runtime via `providerOptions.langsmith` will apply only to that run.
We suggest importing and wrapping your config in `createLangSmithProviderOptions` to ensure
proper typing:

You can customize what inputs and outputs the AI SDK sends to LangSmith by specifying custom input/output
processing functions. This is useful if you are dealing with sensitive data that you would like to
avoid sending to LangSmith.

Because output formats vary depending on which AI SDK method you are using, we suggest defining and passing config
individually into wrapped methods. You will also need to provide separate functions for child LLM runs within
AI SDK calls, since calling `generateText` at top level calls the LLM internally and can do so multiple times.

We also suggest passing a generic parameter into `createLangSmithProviderOptions` to get proper types for inputs and outputs.
Here's an example for `generateText`:

The actual return value will contain the original, non-redacted result but the trace in LangSmith
will be redacted. [Here's an example](https://smith.langchain.com/public/b4c69c8e-285b-4c0c-8492-e571e2cf562f/r).

For redacting tool input/output, wrap your `execute` method in a `traceable` like this:

The `traceable` return type is complex, which makes the cast necessary. You may also omit the AI SDK `tool` wrapper function
if you wish to avoid the cast.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-vercel-ai-sdk.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

## Environment configuration

<CodeGroup>
```

Example 4 (unknown):
```unknown
</CodeGroup>

## Basic setup

Import and wrap AI SDK methods, then use them as you normally would:
```

---

## Transient file (lost after thread ends)

**URL:** llms-txt#transient-file-(lost-after-thread-ends)

agent.invoke({
    "messages": [{"role": "user", "content": "Write draft to /draft.txt"}]
})

---

## Troubleshooting

**URL:** llms-txt#troubleshooting

**Contents:**
- Getting helpful information
- Common issues
  - *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*
  - *error: Dirty database version 'version'. Fix and force version*
  - *413 - Request Entity Too Large*
  - *Details: code: 497, message: default: Not enough privileges. To execute this query, it's necessary to have the grant CREATE ROW POLICY ON default.feedbacks\_rmt*
  - *ClickHouse fails to start up when running a cluster with AquaSec*

Source: https://docs.langchain.com/langsmith/troubleshooting

This guide will walk you through common issues you may encounter when running a self-hosted instance of LangSmith.

While running LangSmith, you may encounter unexpected 500 errors, slow performance, or other issues. This guide will help you diagnose and resolve these issues.

## Getting helpful information

To diagnose and resolve an issue, you will first need to retrieve some relevant information. The following sections explain how to do this for a Kubernetes or a Docker setup, and how to pull helpful browser information.

Generally, the main services you will want to analyze are the:

* `langsmith-backend`: Handles CRUD API requests, business logic, requests from the frontend and SDK, trace preparation for ingestion, and the hub API.
* `langsmith-platform-backend`: Handles authentication, run ingestion, and other high-volume tasks.
* `langsmith-queue`: Handles incoming traces and feedback, asynchronous ingestion and persistence into the datastore, data integrity checks, and retries during database errors or connection issues.

For more details on these services, refer to the [Architectural overview](/langsmith/architectural-overview).

The first step in troubleshooting is to gather important debugging information about your LangSmith deployment. Service logs, kubernetes events, and resource utilization of containers can help identify the root cause of an issue.

You can run our [k8s troubleshooting script](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/scripts/get_k8s_debugging_info.sh) which will pull all of the relevant kubernetes information and output it to a folder for investigation. The script also compresses this folder into a zip file for sharing. Here is an example of how to run this script, assuming your langsmith deployment was brought up in a `langsmith` namespace:

You can then inspect the contents of the produced folder for any relevant errors or information. If you would like the LangSmith team to assist in debugging, please share this zip file with the team.

If running on Docker, you can check the logs your deployment by running the following command:

If you are experiencing an issue that surfaces as a browser error, it may also be helpful to inspect a HAR file which may include key information. To get the HAR file, you can follow [this guide](https://support.langchain.com/articles/9042697994-how-to-generate-a-har-file-for-troubleshooting) which explains the short process for various browsers.

You can then use [Google's HAR analyzer](https://toolbox.googleapps.com/apps/har_analyzer/) to investigate. You can also send your HAR file to the LangSmith team to help with debugging.

### *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*

This error occurs when ClickHouse runs out of disk space. You will need to increase the disk space available to ClickHouse.

In Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:

1. Get the storage class of the PVC: `kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath='{.spec.storageClassName}'`

2. Ensure the storage class has AllowVolumeExpansion: true: `kubectl get sc <storage-class-name> -o jsonpath='{.allowVolumeExpansion}'`

* If it is false, some storage classes can be updated to allow volume expansion.
   * To update the storage class, you can run `kubectl patch sc <storage-class-name> -p '{"allowVolumeExpansion": true}'`
   * If this fails, you may need to create a new storage class with the correct settings.

3. Edit your pvc to have the new size: `kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace>` or `kubectl patch pvc data-langsmith-clickhouse-0 '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}' -n <namespace>`

4. Update your helm chart `langsmith_config.yaml` to new size(e.g `100 Gi`)

5. Delete the clickhouse statefulset `kubectl delete statefulset langsmith-clickhouse --cascade=orphan -n <namespace>`

6. Apply helm chart with updated size (You can follow the upgrade guide [here](/langsmith/self-host-upgrades))

7. Your pvc should now have the new size. Verify by running `kubectl get pvc` and `kubectl exec langsmith-clickhouse-0 -- bash -c "df"`

In Docker, you will need to increase the size of the ClickHouse volume. To achieve this, you can perform the following steps:

1. Stop your instance of LangSmith. `docker compose down`
2. If using bind mount, you will need to increase the size of the mount point.
3. If using a docker `volume`, you will need to allocate more space to the volume/docker.

### *error: Dirty database version 'version'. Fix and force version*

This error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.

1. Force migration to an earlier version, where version = dirty version - 1.

1. Rerun your upgrade/migrations.

1. Force migration to an earlier version, where version = dirty version - 1.

1. Rerun your upgrade/migrations.

### *413 - Request Entity Too Large*

This error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.

1. Edit your `langsmith_config.yaml` and increase the `frontend.maxBodySize` [value](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L519). This might look something like this:

2. Apply your changes to the cluster.

### *Details: code: 497, message: default: Not enough privileges. To execute this query, it's necessary to have the grant CREATE ROW POLICY ON default.feedbacks\_rmt*

This error occurs when your user does not have the necessary permissions to create row policies in Clickhouse. When deploying the Docker deployment, you need to copy the `users.xml` file from the github repo as well. This adds the `<access_management>` tag to the `users.xml` file, which allows the user to create row policies. Below is the default `users.xml` file that we expect to be used.

In some environments, your mount point may not be writable by the container. In these cases we suggest building a custom image with the `users.xml` file included.

Example `Dockerfile`:

Then take the following steps:

1. Build your custom image.

2. Update your `docker-compose.yaml` to use the custom image. Make sure to remove the users.xml mount point.

3. Restart your instance of LangSmith.

### *ClickHouse fails to start up when running a cluster with AquaSec*

In some environments, AquaSec may prevent ClickHouse from starting up correctly. This may manifest as the ClickHouse pod not emitting any logs and failing to get marked as ready.
Generally this is due to `LD_PRELOAD` being set by AquaSec, which interferes with ClickHouse. To resolve this, you can add the following environment variable to your ClickHouse deployment:

Edit your `langsmith_config.yaml` (or corresponding config file) and set the `AQUA_SKIP_LD_PRELOAD` environment variable:

Edit your `docker-compose.yaml` and set the `AQUA_SKIP_LD_PRELOAD` environment variable:

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
You can then inspect the contents of the produced folder for any relevant errors or information. If you would like the LangSmith team to assist in debugging, please share this zip file with the team.

#### Docker

If running on Docker, you can check the logs your deployment by running the following command:
```

Example 2 (unknown):
```unknown
#### Browser Errors

If you are experiencing an issue that surfaces as a browser error, it may also be helpful to inspect a HAR file which may include key information. To get the HAR file, you can follow [this guide](https://support.langchain.com/articles/9042697994-how-to-generate-a-har-file-for-troubleshooting) which explains the short process for various browsers.

You can then use [Google's HAR analyzer](https://toolbox.googleapps.com/apps/har_analyzer/) to investigate. You can also send your HAR file to the LangSmith team to help with debugging.

## Common issues

### *DB::Exception: Cannot reserve 1.00 MiB, not enough space: While executing WaitForAsyncInsert. (NOT\_ENOUGH\_SPACE)*

This error occurs when ClickHouse runs out of disk space. You will need to increase the disk space available to ClickHouse.

#### Kubernetes

In Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:

1. Get the storage class of the PVC: `kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath='{.spec.storageClassName}'`

2. Ensure the storage class has AllowVolumeExpansion: true: `kubectl get sc <storage-class-name> -o jsonpath='{.allowVolumeExpansion}'`

   * If it is false, some storage classes can be updated to allow volume expansion.
   * To update the storage class, you can run `kubectl patch sc <storage-class-name> -p '{"allowVolumeExpansion": true}'`
   * If this fails, you may need to create a new storage class with the correct settings.

3. Edit your pvc to have the new size: `kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace>` or `kubectl patch pvc data-langsmith-clickhouse-0 '{"spec":{"resources":{"requests":{"storage":"100Gi"}}}}' -n <namespace>`

4. Update your helm chart `langsmith_config.yaml` to new size(e.g `100 Gi`)

5. Delete the clickhouse statefulset `kubectl delete statefulset langsmith-clickhouse --cascade=orphan -n <namespace>`

6. Apply helm chart with updated size (You can follow the upgrade guide [here](/langsmith/self-host-upgrades))

7. Your pvc should now have the new size. Verify by running `kubectl get pvc` and `kubectl exec langsmith-clickhouse-0 -- bash -c "df"`

#### Docker

In Docker, you will need to increase the size of the ClickHouse volume. To achieve this, you can perform the following steps:

1. Stop your instance of LangSmith. `docker compose down`
2. If using bind mount, you will need to increase the size of the mount point.
3. If using a docker `volume`, you will need to allocate more space to the volume/docker.

### *error: Dirty database version 'version'. Fix and force version*

This error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.

#### Kubernetes

1. Force migration to an earlier version, where version = dirty version - 1.
```

Example 3 (unknown):
```unknown
1. Rerun your upgrade/migrations.

#### Docker

1. Force migration to an earlier version, where version = dirty version - 1.
```

Example 4 (unknown):
```unknown
1. Rerun your upgrade/migrations.

### *413 - Request Entity Too Large*

This error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.

#### Kubernetes

1. Edit your `langsmith_config.yaml` and increase the `frontend.maxBodySize` [value](https://github.com/langchain-ai/helm/blob/main/charts/langsmith/values.yaml#L519). This might look something like this:
```

---

## Troubleshoot trace nesting

**URL:** llms-txt#troubleshoot-trace-nesting

**Contents:**
- Python
  - Context propagation using asyncio
  - Context propagation using threading

Source: https://docs.langchain.com/langsmith/nest-traces

When tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.

If you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known "edge cases".

The following outlines common causes for "split" traces when building with python.

### Context propagation using asyncio

When using async calls (especially with streaming) in Python versions \< 3.11, you may encounter issues with trace nesting. This is because Python's `asyncio` only [added full support for passing context](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) in version 3.11.

LangChain and LangSmith SDK use [contextvars](https://docs.python.org/3/library/contextvars.html) to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), `asyncio` tasks lack proper `contextvar` support, which can lead to disconnected traces.

1. **Upgrade Python Version (Recommended)** If possible, upgrade to Python 3.11 or later for automatic context propagation.

2. **Manual Context Propagation** If upgrading isn't an option, you'll need to manually propagate the tracing context. The method varies depending on your setup:

a) **Using LangGraph or LangChain** Pass the parent `config` to the child call:

b) **Using LangSmith Directly** Pass the run tree directly:

c) **Combining Decorated Code with LangGraph/LangChain** Use a combination of techniques for manual handoff:

### Context propagation using threading

It's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib `ThreadPoolExecutor` by default breaks tracing.

Python's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:

1. **Using LangSmith's ContextThreadPoolExecutor**

LangSmith provides a `ContextThreadPoolExecutor` that automatically handles context propagation:

2. **Manually providing the parent run tree**

Alternatively, you can manually pass the parent run tree to the inner function:

In this approach, we use `get_current_run_tree()` to obtain the current run tree and pass it to the inner function using the `langsmith_extra` parameter.

Both methods ensure that the inner function calls are correctly aggregated under the initial trace stack, even when executed in separate threads.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/nest-traces.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
b) **Using LangSmith Directly** Pass the run tree directly:
```

Example 2 (unknown):
```unknown
c) **Combining Decorated Code with LangGraph/LangChain** Use a combination of techniques for manual handoff:
```

Example 3 (unknown):
```unknown
### Context propagation using threading

It's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib `ThreadPoolExecutor` by default breaks tracing.

#### Why

Python's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:

#### To resolve

1. **Using LangSmith's ContextThreadPoolExecutor**

   LangSmith provides a `ContextThreadPoolExecutor` that automatically handles context propagation:
```

Example 4 (unknown):
```unknown
2. **Manually providing the parent run tree**

   Alternatively, you can manually pass the parent run tree to the inner function:
```

---

## Troubleshoot variable caching

**URL:** llms-txt#troubleshoot-variable-caching

**Contents:**
- 1. Verify your environment variables
- 2. Clear the cache
- 3. Reload the environment variables

Source: https://docs.langchain.com/langsmith/troubleshooting-variable-caching

If you're not seeing traces in your tracing project or notice traces logged to the wrong project/workspace, the issue might be due to LangSmith's default environment variable caching. This is especially common when running LangSmith within a Jupyter notebook. Follow these steps to diagnose and resolve the issue:

## 1. Verify your environment variables

First, check that the environment variables are set correctly by running:

If the output does not match what's defined in your .env file, it's likely due to environment variable caching.

## 2. Clear the cache

Clear the cached environment variables with the following command:

## 3. Reload the environment variables

Reload your environment variables from the .env file by executing:

After reloading, your environment variables should be set correctly.

If you continue to experience issues, please reach out to us via a shared Slack channel or email support (available for Plus and Enterprise plans), or in the [LangChain Forum](https://forum.langchain.com/).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/troubleshooting-variable-caching.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
If the output does not match what's defined in your .env file, it's likely due to environment variable caching.

## 2. Clear the cache

Clear the cached environment variables with the following command:
```

Example 2 (unknown):
```unknown
## 3. Reload the environment variables

Reload your environment variables from the .env file by executing:
```

---

## Try creating an assistant. This should fail

**URL:** llms-txt#try-creating-an-assistant.-this-should-fail

try:
    await alice.assistants.create("agent")
    print("❌ Alice shouldn't be able to create assistants!")
except Exception as e:
    print("✅ Alice correctly denied access:", e)

---

## Try searching for assistants. This also should fail

**URL:** llms-txt#try-searching-for-assistants.-this-also-should-fail

try:
    await alice.assistants.search()
    print("❌ Alice shouldn't be able to search assistants!")
except Exception as e:
    print("✅ Alice correctly denied access to searching assistants:", e)

---

## Try to access user 1's thread as user 2

**URL:** llms-txt#try-to-access-user-1's-thread-as-user-2

**Contents:**
- Next steps

user2_token = await login(email2, password)
user2_client = get_client(
    url="http://localhost:2024", headers={"Authorization": f"Bearer {user2_token}"}
)

try:
    await user2_client.threads.get(thread["thread_id"])
    print("❌ User 2 shouldn't see User 1's thread!")
except Exception as e:
    print("✅ User 2 blocked from User 1's thread:", e)
shell  theme={null}
✅ User 1 created thread: d6af3754-95df-4176-aa10-dbd8dca40f1a
✅ Unauthenticated access blocked: Client error '403 Forbidden' for url 'http://localhost:2024/threads'
✅ User 2 blocked from User 1's thread: Client error '404 Not Found' for url 'http://localhost:2024/threads/d6af3754-95df-4176-aa10-dbd8dca40f1a'
```

Your authentication and authorization are working together:

1. Users must log in to access the bot
2. Each user can only see their own threads

All users are managed by the Supabase auth provider, so you don't need to implement any additional user management logic.

You've successfully built a production-ready authentication system for your LangGraph application! Let's review what you've accomplished:

1. Set up an authentication provider (Supabase in this case)
2. Added real user accounts with email/password authentication
3. Integrated JWT token validation into your Agent Server
4. Implemented proper authorization to ensure users can only access their own data
5. Created a foundation that's ready to handle your next authentication challenge

Now that you have production authentication, consider:

1. Building a web UI with your preferred framework (see the [Custom Auth](https://github.com/langchain-ai/custom-auth) template for an example)
2. Learn more about the other aspects of authentication and authorization in the [conceptual guide on authentication](/langsmith/auth).
3. Customize your handlers and setup further after reading the [reference docs](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.auth.Auth).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/add-auth-server.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
The output should look like this:
```

---

## Try to access without a token

**URL:** llms-txt#try-to-access-without-a-token

unauthenticated_client = get_client(url="http://localhost:2024")
try:
    await unauthenticated_client.threads.create()
    print("❌ Unauthenticated access should fail!")
except Exception as e:
    print("✅ Unauthenticated access blocked:", e)

---

## Try without a token (should fail)

**URL:** llms-txt#try-without-a-token-(should-fail)

client = get_client(url="http://localhost:2024")
try:
    thread = await client.threads.create()
    print("❌ Should have failed without token!")
except Exception as e:
    print("✅ Correctly blocked access:", e)

---

## Try with a valid token

**URL:** llms-txt#try-with-a-valid-token

client = get_client(
    url="http://localhost:2024", headers={"Authorization": "Bearer user1-token"}
)

---

## ttl:

**URL:** llms-txt#ttl:

---

## ttl_period_seconds:

**URL:** llms-txt#ttl_period_seconds:

---

## Turn 1: Initial message - starts with warranty_collector step

**URL:** llms-txt#turn-1:-initial-message---starts-with-warranty_collector-step

print("=== Turn 1: Warranty Collection ===")
result = agent.invoke(
    {"messages": [HumanMessage("Hi, my phone screen is cracked")]},
    config
)
for msg in result['messages']:
    msg.pretty_print()

---

## Turn 2: User responds about warranty

**URL:** llms-txt#turn-2:-user-responds-about-warranty

print("\n=== Turn 2: Warranty Response ===")
result = agent.invoke(
    {"messages": [HumanMessage("Yes, it's still under warranty")]},
    config
)
for msg in result['messages']:
    msg.pretty_print()
print(f"Current step: {result.get('current_step')}")

---

## Turn 3: User describes the issue

**URL:** llms-txt#turn-3:-user-describes-the-issue

print("\n=== Turn 3: Issue Description ===")
result = agent.invoke(
    {"messages": [HumanMessage("The screen is physically cracked from dropping it")]},
    config
)
for msg in result['messages']:
    msg.pretty_print()
print(f"Current step: {result.get('current_step')}")

---

## Turn 4: Resolution

**URL:** llms-txt#turn-4:-resolution

**Contents:**
- 7. Understanding state transitions
  - Turn 1: Initial message
  - Turn 2: After warranty recorded
  - Turn 3: After issue classified
- 8. Manage message history
- 9. Add flexibility: Go back

print("\n=== Turn 4: Resolution ===")
result = agent.invoke(
    {"messages": [HumanMessage("What should I do?")]},
    config
)
for msg in result['messages']:
    msg.pretty_print()
python  theme={null}
{
    "messages": [HumanMessage("Hi, my phone screen is cracked")],
    "current_step": "warranty_collector"  # Default value
}
python  theme={null}
Command(update={
    "warranty_status": "in_warranty",
    "current_step": "issue_classifier"  # State transition!
})
python  theme={null}
Command(update={
    "issue_type": "hardware",
    "current_step": "resolution_specialist"  # State transition!
})
python  theme={null}
from langchain.agents import create_agent
from langchain.agents.middleware import SummarizationMiddleware  # [!code highlight]
from langgraph.checkpoint.memory import InMemorySaver

agent = create_agent(
    model,
    tools=all_tools,
    state_schema=SupportState,
    middleware=[
        apply_step_config,
        SummarizationMiddleware(  # [!code highlight]
            model="gpt-4o-mini",
            trigger=("tokens", 4000),
            keep=("messages", 10)
        )
    ],
    checkpointer=InMemorySaver(),
)
python  theme={null}
@tool
def go_back_to_warranty() -> Command:  # [!code highlight]
    """Go back to warranty verification step."""
    return Command(update={"current_step": "warranty_collector"})  # [!code highlight]

@tool
def go_back_to_classification() -> Command:  # [!code highlight]
    """Go back to issue classification step."""
    return Command(update={"current_step": "issue_classifier"})  # [!code highlight]

**Examples:**

Example 1 (unknown):
```unknown
Expected flow:

1. **Warranty verification step**: Asks about warranty status
2. **Issue classification step**: Asks about the problem, determines it's hardware
3. **Resolution step**: Provides warranty repair instructions

## 7. Understanding state transitions

Let's trace what happens at each turn:

### Turn 1: Initial message
```

Example 2 (unknown):
```unknown
Middleware applies:

* System prompt: `WARRANTY_COLLECTOR_PROMPT`
* Tools: `[record_warranty_status]`

### Turn 2: After warranty recorded

Tool call: `record_warranty_status("in_warranty")` returns:
```

Example 3 (unknown):
```unknown
Next turn, middleware applies:

* System prompt: `ISSUE_CLASSIFIER_PROMPT` (formatted with `warranty_status="in_warranty"`)
* Tools: `[record_issue_type]`

### Turn 3: After issue classified

Tool call: `record_issue_type("hardware")` returns:
```

Example 4 (unknown):
```unknown
Next turn, middleware applies:

* System prompt: `RESOLUTION_SPECIALIST_PROMPT` (formatted with `warranty_status` and `issue_type`)
* Tools: `[provide_solution, escalate_to_human]`

The key insight: **Tools drive the workflow** by updating `current_step`, and **middleware responds** by applying the appropriate configuration on the next turn.

## 8. Manage message history

As the agent progresses through steps, message history grows. Use [summarization middleware](/oss/python/langchain/short-term-memory#summarize-messages) to compress earlier messages while preserving conversational context:
```

---

## Turn off LangSmith default tracing, as we want to only trace with OpenTelemetry

**URL:** llms-txt#turn-off-langsmith-default-tracing,-as-we-want-to-only-trace-with-opentelemetry

**Contents:**
- 4. Prepare for deployment

LANGSMITH_TRACING=false

OTEL_EXPORTER_OTLP_ENDPOINT = "https://api.smith.langchain.com/otel/"

OTEL_EXPORTER_OTLP_HEADERS = "x-api-key=your-langsmith-api-key,Langsmith-Project=your-tracing-project-name"
python  theme={null}
from strands.telemetry import StrandsTelemetry

strands_telemetry = StrandsTelemetry()
strands_telemetry.setup_otlp_exporter()
strands_telemetry.setup_meter()

my-strands-agent/
├── agent.py          # Your main agent code
├── requirements.txt  # Python dependencies
└── langgraph.json   # LangGraph configuration
```

To deploy your agent, follow the [Deploy to cloud](/langsmith/deploy-to-cloud) guide.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/deploy-other-frameworks.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  If you're [self-hosting LangSmith](/langsmith/self-hosted), replace the  `OTEL_EXPORTER_OTLP_ENDPOINT` endpoint with your LangSmith API endpoint and append `/api/v1/otel`. For example: `OTEL_EXPORTER_OTLP_ENDPOINT = "https://ai-company.com/api/v1/otel"`
</Note>

<Note>
  Strand's OTel tracing contains synchronous code. In this case, you may need to set `BG_JOB_ISOLATED_LOOPS=true` to execute background runs in an isolated event loop separate from the serving API event loop.
</Note>

In your main agent, set up the following:
```

Example 2 (unknown):
```unknown
## 4. Prepare for deployment

From here, to deploy to LangSmith, create a file structure like the following:
```

---

## TypedDict defines the structure of user information for the LLM

**URL:** llms-txt#typeddict-defines-the-structure-of-user-information-for-the-llm

class UserInfo(TypedDict):
    name: str

---

## {"type": "image", "base64": "...", "mime_type": "image/jpeg"},

**URL:** llms-txt#{"type":-"image",-"base64":-"...",-"mime_type":-"image/jpeg"},

---

## {"type": "text", "text": "Here's a picture of a cat"},

**URL:** llms-txt#{"type":-"text",-"text":-"here's-a-picture-of-a-cat"},

---

## [{"type": "text", "text": "The sky is typically blue..."}]

**URL:** llms-txt#[{"type":-"text",-"text":-"the-sky-is-typically-blue..."}]

**Contents:**
  - Batch
- Tool calling
- Structured output
- Supported models
- Advanced topics
  - Model profiles

python  theme={null}
    async for event in model.astream_events("Hello"):

if event["event"] == "on_chat_model_start":
            print(f"Input: {event['data']['input']}")

elif event["event"] == "on_chat_model_stream":
            print(f"Token: {event['data']['chunk'].text}")

elif event["event"] == "on_chat_model_end":
            print(f"Full message: {event['data']['output'].text}")

else:
            pass
    txt  theme={null}
    Input: Hello
    Token: Hi
    Token:  there
    Token: !
    Token:  How
    Token:  can
    Token:  I
    ...
    Full message: Hi there! How can I help today?
    python Batch theme={null}
responses = model.batch([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
])
for response in responses:
    print(response)
python Yield batch responses upon completion theme={null}
for response in model.batch_as_completed([
    "Why do parrots have colorful feathers?",
    "How do airplanes fly?",
    "What is quantum computing?"
]):
    print(response)
python Batch with max concurrency theme={null}
  model.batch(
      list_of_inputs,
      config={
          'max_concurrency': 5,  # Limit to 5 parallel calls
      }
  )
  mermaid  theme={null}
sequenceDiagram
    participant U as User
    participant M as Model
    participant T as Tools

U->>M: "What's the weather in SF and NYC?"
    M->>M: Analyze request & decide tools needed

par Parallel Tool Calls
        M->>T: get_weather("San Francisco")
        M->>T: get_weather("New York")
    end

par Tool Execution
        T-->>M: SF weather data
        T-->>M: NYC weather data
    end

M->>M: Process results & generate response
    M->>U: "SF: 72°F sunny, NYC: 68°F cloudy"
python Binding user tools theme={null}
from langchain.tools import tool

@tool
def get_weather(location: str) -> str:
    """Get the weather at a location."""
    return f"It's sunny in {location}."

model_with_tools = model.bind_tools([get_weather])  # [!code highlight]

response = model_with_tools.invoke("What's the weather like in Boston?")
for tool_call in response.tool_calls:
    # View tool calls made by the model
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
python Tool execution loop theme={null}
    # Bind (potentially multiple) tools to the model
    model_with_tools = model.bind_tools([get_weather])

# Step 1: Model generates tool calls
    messages = [{"role": "user", "content": "What's the weather in Boston?"}]
    ai_msg = model_with_tools.invoke(messages)
    messages.append(ai_msg)

# Step 2: Execute tools and collect results
    for tool_call in ai_msg.tool_calls:
        # Execute the tool with the generated arguments
        tool_result = get_weather.invoke(tool_call)
        messages.append(tool_result)

# Step 3: Pass results back to model for final response
    final_response = model_with_tools.invoke(messages)
    print(final_response.text)
    # "The current weather in Boston is 72°F and sunny."
    python Force use of any tool theme={null}
      model_with_tools = model.bind_tools([tool_1], tool_choice="any")
      python Force use of specific tools theme={null}
      model_with_tools = model.bind_tools([tool_1], tool_choice="tool_1")
      python Parallel tool calls theme={null}
    model_with_tools = model.bind_tools([get_weather])

response = model_with_tools.invoke(
        "What's the weather in Boston and Tokyo?"
    )

# The model may generate multiple tool calls
    print(response.tool_calls)
    # [
    #   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},
    #   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},
    # ]

# Execute all tools (can be done in parallel with async)
    results = []
    for tool_call in response.tool_calls:
        if tool_call['name'] == 'get_weather':
            result = get_weather.invoke(tool_call)
        ...
        results.append(result)
    python  theme={null}
      model.bind_tools([get_weather], parallel_tool_calls=False)
      python Streaming tool calls theme={null}
    for chunk in model_with_tools.stream(
        "What's the weather in Boston and Tokyo?"
    ):
        # Tool call chunks arrive progressively
        for tool_chunk in chunk.tool_call_chunks:
            if name := tool_chunk.get("name"):
                print(f"Tool: {name}")
            if id_ := tool_chunk.get("id"):
                print(f"ID: {id_}")
            if args := tool_chunk.get("args"):
                print(f"Args: {args}")

# Output:
    # Tool: get_weather
    # ID: call_SvMlU1TVIZugrFLckFE2ceRE
    # Args: {"lo
    # Args: catio
    # Args: n": "B
    # Args: osto
    # Args: n"}
    # Tool: get_weather
    # ID: call_QMZdy6qInx13oWKE7KhuhOLR
    # Args: {"lo
    # Args: catio
    # Args: n": "T
    # Args: okyo
    # Args: "}
    python Accumulate tool calls theme={null}
    gathered = None
    for chunk in model_with_tools.stream("What's the weather in Boston?"):
        gathered = chunk if gathered is None else gathered + chunk
        print(gathered.tool_calls)
    python  theme={null}
    from pydantic import BaseModel, Field

class Movie(BaseModel):
        """A movie with details."""
        title: str = Field(..., description="The title of the movie")
        year: int = Field(..., description="The year the movie was released")
        director: str = Field(..., description="The director of the movie")
        rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie)
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # Movie(title="Inception", year=2010, director="Christopher Nolan", rating=8.8)
    python  theme={null}
    from typing_extensions import TypedDict, Annotated

class MovieDict(TypedDict):
        """A movie with details."""
        title: Annotated[str, ..., "The title of the movie"]
        year: Annotated[int, ..., "The year the movie was released"]
        director: Annotated[str, ..., "The director of the movie"]
        rating: Annotated[float, ..., "The movie's rating out of 10"]

model_with_structure = model.with_structured_output(MovieDict)
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}
    python  theme={null}
    import json

json_schema = {
        "title": "Movie",
        "description": "A movie with details",
        "type": "object",
        "properties": {
            "title": {
                "type": "string",
                "description": "The title of the movie"
            },
            "year": {
                "type": "integer",
                "description": "The year the movie was released"
            },
            "director": {
                "type": "string",
                "description": "The director of the movie"
            },
            "rating": {
                "type": "number",
                "description": "The movie's rating out of 10"
            }
        },
        "required": ["title", "year", "director", "rating"]
    }

model_with_structure = model.with_structured_output(
        json_schema,
        method="json_schema",
    )
    response = model_with_structure.invoke("Provide details about the movie Inception")
    print(response)  # {'title': 'Inception', 'year': 2010, ...}
    python  theme={null}
  from pydantic import BaseModel, Field

class Movie(BaseModel):
      """A movie with details."""
      title: str = Field(..., description="The title of the movie")
      year: int = Field(..., description="The year the movie was released")
      director: str = Field(..., description="The director of the movie")
      rating: float = Field(..., description="The movie's rating out of 10")

model_with_structure = model.with_structured_output(Movie, include_raw=True)  # [!code highlight]
  response = model_with_structure.invoke("Provide details about the movie Inception")
  response
  # {
  #     "raw": AIMessage(...),
  #     "parsed": Movie(title=..., year=..., ...),
  #     "parsing_error": None,
  # }
  python Pydantic BaseModel theme={null}
    from pydantic import BaseModel, Field

class Actor(BaseModel):
        name: str
        role: str

class MovieDetails(BaseModel):
        title: str
        year: int
        cast: list[Actor]
        genres: list[str]
        budget: float | None = Field(None, description="Budget in millions USD")

model_with_structure = model.with_structured_output(MovieDetails)
    python TypedDict theme={null}
    from typing_extensions import Annotated, TypedDict

class Actor(TypedDict):
        name: str
        role: str

class MovieDetails(TypedDict):
        title: str
        year: int
        cast: list[Actor]
        genres: list[str]
        budget: Annotated[float | None, ..., "Budget in millions USD"]

model_with_structure = model.with_structured_output(MovieDetails)
    python  theme={null}
model.profile

**Examples:**

Example 1 (unknown):
```unknown
The resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) – for example, it can be aggregated into a message history and passed back to the model as conversational context.

<Warning>
  Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.
</Warning>

<Accordion title="Advanced streaming topics">
  <Accordion title="Streaming events">
    LangChain chat models can also stream semantic events using `astream_events()`.

    This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
<Tip>
      See the [`astream_events()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.astream_events) reference for event types and other details.
    </Tip>
  </Accordion>

  <Accordion title="&#x22;Auto-streaming&#x22; chat models">
    LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model.

    In [LangGraph agents](/oss/python/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.

    #### How it works

    When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking [`on_llm_new_token`](https://reference.langchain.com/python/langchain_core/callbacks/#langchain_core.callbacks.base.AsyncCallbackHandler.on_llm_new_token) events in LangChain's callback system.

    Callback events allow LangGraph `stream()` and `astream_events()` to surface the chat model's output in real-time.
  </Accordion>
</Accordion>

### Batch

Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:
```

Example 4 (unknown):
```unknown
<Note>
  This section describes a chat model method [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch), which parallelizes model calls client-side.

  It is **distinct** from batch APIs supported by inference providers, such as [OpenAI](https://platform.openai.com/docs/guides/batch) or [Anthropic](https://platform.claude.com/docs/en/build-with-claude/batch-processing#message-batches-api).
</Note>

By default, [`batch()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch) will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with [`batch_as_completed()`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel.batch_as_completed):
```

---

## Unified access to content blocks

**URL:** llms-txt#unified-access-to-content-blocks

**Contents:**
  - Benefits
- Simplified package
  - Namespace

for block in response.content_blocks:
    if block["type"] == "reasoning":
        print(f"Model reasoning: {block['reasoning']}")
    elif block["type"] == "text":
        print(f"Response: {block['text']}")
    elif block["type"] == "tool_call":
        print(f"Tool call: {block['name']}({block['args']})")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Benefits

* **Provider agnostic**: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider
* **Type safe**: Full type hints for all content block types
* **Backward compatible**: Standard content can be [loaded lazily](/oss/python/langchain/messages#standard-content-blocks), so there are no associated breaking changes

For more information, see our guide on [content blocks](/oss/python/langchain/messages#standard-content-blocks).

***

## Simplified package

LangChain v1 streamlines the [`langchain`](https://pypi.org/project/langchain/) package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:

### Namespace

| Module                                                                                | What's available                                                                                                                                                                                                                                                          | Notes                                                                                       |
| ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |
| [`langchain.agents`](https://reference.langchain.com/python/langchain/agents)         | [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState)                                                            | Core agent creation functionality                                                           |
| [`langchain.messages`](https://reference.langchain.com/python/langchain/messages)     | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim_messages)                               | Re-exported from [`langchain-core`](https://reference.langchain.com/python/langchain_core/) |
| [`langchain.tools`](https://reference.langchain.com/python/langchain/tools)           | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers                                                                | Re-exported from [`langchain-core`](https://reference.langchain.com/python/langchain_core/) |
| [`langchain.chat_models`](https://reference.langchain.com/python/langchain/models)    | [`init_chat_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat_models.init_chat_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.chat_models.BaseChatModel)   | Unified model initialization                                                                |
| [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`Embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings), [`init_embeddings`](https://reference.langchain.com/python/langchain_core/embeddings/#langchain_core.embeddings.embeddings.Embeddings) | Embedding models                                                                            |

Most of these are re-exported from `langchain-core` for convenience, which gives you a focused API surface for building agents.
```

---

## Update memory

**URL:** llms-txt#update-memory

@tool
def save_user_info(user_id: str, user_info: dict[str, Any], runtime: ToolRuntime) -> str:
    """Save user info."""
    store = runtime.store
    store.put(("users",), user_id, user_info)
    return "Successfully saved user info."

store = InMemoryStore()
agent = create_agent(
    model,
    tools=[get_user_info, save_user_info],
    store=store
)

---

## Update Oauth Provider

**URL:** llms-txt#update-oauth-provider

Source: https://docs.langchain.com/api-reference/auth-service-v2/update-oauth-provider

https://api.host.langchain.com/openapi.json patch /v2/auth/providers/{provider_id}
Update an OAuth provider.

---

## Update these values as needed to connect to your replicated clickhouse cluster.

**URL:** llms-txt#update-these-values-as-needed-to-connect-to-your-replicated-clickhouse-cluster.

clickhouse:
  external:
    # If using a 3 node replicated setup, each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory.
    enabled: true
    host: langsmith-ch-clickhouse-replicated.default.svc.cluster.local
    port: "8123"
    nativePort: "9000"
    user: "default"
    password: "password"
    database: "default"
    cluster: "replicated"

commonEnv:
  - name: "CLICKHOUSE_ASYNC_INSERT_WAIT_PCT_FLOAT"
    value: "0"
```

<Note>
  Ensure that the Kubernetes cluster is configured with sufficient resources to scale to the recommended size. After deployment, all of the pods in the Kubernetes cluster should be in a `Running` state. Pods stuck in `Pending` may indicate that you are reaching node pool limits or need larger nodes.

Also, ensure that any ingress controller deployed on the cluster is able to handle the desired load to prevent bottlenecks.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-scale.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Update the conversation history by removing all messages

**URL:** llms-txt#update-the-conversation-history-by-removing-all-messages

@tool
def clear_conversation() -> Command:
    """Clear the conversation history."""

return Command(
        update={
            "messages": [RemoveMessage(id=REMOVE_ALL_MESSAGES)],
        }
    )

---

## Update the resolution_specialist configuration to include these tools

**URL:** llms-txt#update-the-resolution_specialist-configuration-to-include-these-tools

STEP_CONFIG["resolution_specialist"]["tools"].extend([
    go_back_to_warranty,
    go_back_to_classification
])
python  theme={null}
RESOLUTION_SPECIALIST_PROMPT = """You are a customer support agent helping with device issues.

CURRENT STAGE: Resolution
CUSTOMER INFO: Warranty status is {warranty_status}, issue type is {issue_type}

At this step, you need to:
1. For SOFTWARE issues: provide troubleshooting steps using provide_solution
2. For HARDWARE issues:
   - If IN WARRANTY: explain warranty repair process using provide_solution
   - If OUT OF WARRANTY: escalate_to_human for paid repair options

If the customer indicates any information was wrong, use:
- go_back_to_warranty to correct warranty status
- go_back_to_classification to correct issue type

Be specific and helpful in your solutions."""
python  theme={null}
result = agent.invoke(
    {"messages": [HumanMessage("Actually, I made a mistake - my device is out of warranty")]},
    config
)

**Examples:**

Example 1 (unknown):
```unknown
Update the resolution specialist's prompt to mention these tools:
```

Example 2 (unknown):
```unknown
Now the agent can handle corrections:
```

---

## Update Thread State

**URL:** llms-txt#update-thread-state

Source: https://docs.langchain.com/langsmith/agent-server-api/threads/update-thread-state

langsmith/agent-server-openapi.json post /threads/{thread_id}/state
Add state to a thread.

---

## Upload files with traces

**URL:** llms-txt#upload-files-with-traces

**Contents:**
  - Python

Source: https://docs.langchain.com/langsmith/upload-files-with-traces

<Check>
  Before diving into this content, it would be helpful to read the following guides:

* [Trace with LangSmith using the traceable decorator or wrapper](/langsmith/annotate-code#use-traceable--traceable)
</Check>

<Note>
  The following features are available in the following SDK versions:

* Python SDK: >=0.1.141
  * JS/TS SDK: >=0.2.5
</Note>

LangSmith supports uploading binary files (such as images, audio, videos, PDFs, and CSVs) with your traces. This is particularly useful when working with LLM pipelines using multimodal inputs or outputs.

In both the Python and TypeScript SDKs, attachments can be added to your traces by specifying the MIME type and binary content of each file. This guide explains how to define and trace attachments using the `Attachment` type in Python and `Uint8Array` / `ArrayBuffer` in TypeScript.

In the Python SDK, you can use the `Attachment` type to add files to your traces. Each `Attachment` requires:

* `mime_type` (str): The MIME type of the file (e.g., `"image/png"`).
* `data` (bytes | Path): The binary content of the file, or the file path.

You can also define an attachment with a tuple tuple of the form `(mime_type, data)` for convenience.

Simply decorate a function with `@traceable` and include your `Attachment` instances as arguments. Note that to use the file path instead of the raw bytes, you need to set the `dangerously_allow_filesystem` flag to `True` in your traceable decorator.

```python Python theme={null}
from langsmith import traceable
from langsmith.schemas import Attachment
from pathlib import Path
import os

---

## User management

**URL:** llms-txt#user-management

**Contents:**
- Set up access control
  - Create a role
  - Assign a role to a user
- Set up SAML SSO for your organization
  - Just-in-time (JIT) provisioning
  - Login methods and access
  - Enforce SAML SSO only
  - Prerequisites
  - Initial configuration
  - Entra ID (Azure)

Source: https://docs.langchain.com/langsmith/user-management

This page covers user management features in LangSmith, including access control, authentication, and automated user provisioning:

* [Set up access control](#set-up-access-control): Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users.
* [SAML SSO (Enterprise plan)](#set-up-saml-sso-for-your-organization): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers.
* [SCIM User Provisioning (Enterprise plan)](#set-up-scim-for-your-organization): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM.

## Set up access control

<Note>
  RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, [contact our sales team](https://www.langchain.com/contact-sales). Other plans default to using the [`Admin` role](/langsmith/administration-overview) for all users.
</Note>

<Check>
  You may find it helpful to read the [Administration overview](/langsmith/administration-overview) page before setting up access control.
</Check>

LangSmith relies on RBAC to manage user permissions within a [workspace](/langsmith/administration-overview#workspaces). This allows you to control who can access your LangSmith workspace and what they can do within it. Only users with the `workspace:manage` permission can manage access control settings for a workspace.

For a complete reference of workspace roles and their permissions, refer to the [Role-based access control](/langsmith/rbac#workspace-roles) guide. For specific operations each role can perform, refer to the [Organization and workspace operations reference](/langsmith/organization-workspace-operations).

By default, LangSmith comes with a set of system roles:

* `Admin`: has full access to all resources within the workspace.
* `Viewer`: has read-only access to all resources within the workspace.
* `Editor`: has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys).

If these do not fit your access model, `Organization Admins` can create custom roles to suit your needs.

To create a role, navigate to the **Roles** tab in the **Members and roles** section of the [Organization settings page](https://smith.langchain.com/settings). Note that new roles that you create will be usable across all workspaces within your organization.

Click on the **Create Role** button to create a new role. A **Create role** form will open.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=6a413dbec076a37d680fa2ed8a91c495" alt="Create Role" data-og-width="3078" width="3078" data-og-height="1932" height="1932" data-path="langsmith/images/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9b92bb7e4743445999e92faac75163ec 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=9ca40498a6740d4b73a5856461088af8 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=77bde5767be13e160a54d06c5c016953 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c83fd7dfd2f046073d80682607c6b6e1 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=318019bbd5d5e272aa6bad083f6713d5 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-role.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=642e5b56a1dc12a29173f969ec08be45 2500w" />

Assign permissions for the different LangSmith resources that you want to control access to.

### Assign a role to a user

Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to the `Workspace members` tab in the `Workspaces` section of the [Organization settings page](https://smith.langchain.com/settings)

Each user will have a **Role** dropdown that you can use to assign a role to them.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ec5748a2c884693a24f984cc517a3860" alt="Assign Role" data-og-width="1888" width="1888" data-og-height="574" height="574" data-path="langsmith/images/assign-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=9530054e9b95d485534b328b13c6df69 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=864bd5501ab0f253fc5f6db15dec9205 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=520648bd2bef4ed100193c0031b9b3e9 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=53bbafe1b595daf0487a2e8daff8ce68 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b6b5ea2d4d814c28b243d170f0af6359 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/assign-role.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6ad29acab6d3c0e06b0f5ddbef793ca9 2500w" />

You can also invite new users with a given role.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3787cb55cbefcd5c95d6c01b6e9f6e75" alt="Invite User" data-og-width="1204" width="1204" data-og-height="886" height="886" data-path="langsmith/images/invite-user.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7a300a15bd2572a18b8cc2f37921ffb6 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=ea044cb3c58da8351dc60f2d3a040c23 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a3ab5fd8738cde3f2257e580daf96114 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=7e8b4ab3a1f758aaa93a8694da3ee9cf 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7642a1b08854289db7bb7939c23db52 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/invite-user.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=33205dfa57d9a51af02c4f53395fc7f6 2500w" />

## Set up SAML SSO for your organization

Single Sign-On (SSO) functionality is **available for Enterprise Cloud** customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure.

LangSmith's SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience.

SSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:

* Streamlines user management across systems for organization owners.
* Enables organizations to enforce their own security policies (e.g., MFA).
* Removes the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications.

### Just-in-time (JIT) provisioning

LangSmith supports Just-in-time provisioning when using SAML SSO. This allows someone signing in via SAML SSO to join the organization and selected workspaces automatically as a member.

<Note>
  JIT provisioning only runs for new users, that is, users who do not already have access to the organization with the same email address via a [different login method](/langsmith/authentication-methods#cloud).
</Note>

### Login methods and access

Once you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to [other login methods](/langsmith/authentication-methods#cloud), such as username/password or Google Authentication":

* When logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured.
* Users with SAML SSO as their only login method do not have [personal organizations](/langsmith/administration-overview#organizations).
* When logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of.

### Enforce SAML SSO only

<Note>
  User invites are not supported in organizations enforcing SAML SSO only. Initial workspace membership and role is determined by JIT provisioning, and changes afterwards can be managed in the UI.
  For additional flexibility in automated user management, LangSmith supports SCIM.
</Note>

To ensure users can only access the organization when logged in using SAML SSO and no other method, check the **Login via SSO only** checkbox and click **Save**. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking **Save**.

<Note>
  You must be logged in via SAML SSO in order to update this setting to `Only SAML SSO`. This is to ensure the SAML settings are valid and avoid locking users out of your organization.
</Note>

For troubleshooting, refer to the [SAML SSO FAQs](/langsmith/faq#saml-sso-faqs). If you have issues setting up SAML SSO, contact the LangChain support team via [support.langchain.com](https://support.langchain.com).

<Note>
  SAML SSO is available for organizations on the [Enterprise plan](https://www.langchain.com/pricing-langsmith). Please [contact sales](https://www.langchain.com/contact-sales) to learn more.
</Note>

* Your organization must be on an Enterprise plan.
* Your Identity Provider (IdP) must support the SAML 2.0 standard.
* Only [`Organization Admins`](/langsmith/organization-workspace-operations#sso-and-authentication) can configure SAML SSO.

For instructions on using SCIM along with SAML for user provisioning and deprovisioning, refer to the [SCIM setup](#set-up-scim-for-your-organization).

### Initial configuration

<Note>
  For IdP-specific configuration steps, refer to one of the following:

* [Entra ID](#entra-id-azure)
  * [Google](#google)
  * [Okta](#okta)
</Note>

1. In your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3.

<Note>
     The following URLs are different for the US and EU regions. Ensure you select the correct link.
   </Note>

1. Single sign-on URL (or ACS URL):
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. Audience URI (or SP Entity ID):
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. Name ID format: email address.
   4. Application username: email address.
   5. Required claims: `sub` and `email`.

2. In LangSmith: Go to **Settings** -> **Members and roles** -> **SSO Configuration**. Fill in the required information and submit to activate SSO login:

1. Fill in either the `SAML metadata URL` or `SAML metadata XML`.
   2. Select the `Default workspace role` and `Default workspaces`. New users logging in via SSO will be added to the specified workspaces with the selected role.

* `Default workspace role` and `Default workspaces` are editable. The updated settings will apply to new users only, not existing users.
* (Coming soon) `SAML metadata URL` and `SAML metadata XML` are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used.

For additional information, see Microsoft's [documentation](https://learn.microsoft.com/en-us/entra/identity/enterprise-apps/add-application-portal-setup-sso).

<div id="create-application-entra-id" />

**Step 1: Create a new Entra ID application integration**

1. Log in to the [Azure portal](https://portal.azure.com/#home) with a privileged role (e.g., `Global Administrator`). On the left navigation pane, select the `Entra ID` service.

2. Navigate to **Enterprise Applications** and then select **All Applications**.

3. Click **Create your own application**.

4. In the **Create your own application** window:

1. Enter a name for your application (e.g., `LangSmith`).
   2. Select **Integrate any other application you don't find in the gallery (Non-gallery)**.

**Step 2: Configure the Entra ID application and obtain the SAML Metadata**

1. Open the enterprise application that you created.

2. In the left-side navigation, select **Manage** > **Single sign-on**.

3. On the Single sign-on page, click **SAML**.

4. Update the **Basic SAML Configuration**:

1. `Identifier (Entity ID)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   2. `Reply URL (Assertion Consumer Service URL)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   3. Leave `Relay State`, `Logout Url`, and `Sign on URL` empty.
   4. Click **Save**.

5. Ensure required claims are present with **Namespace**: `http://schemas.xmlsoap.org/ws/2005/05/identity/claims`:

1. `sub`: `user.objectid`.
   2. `emailaddress`: `user.userprincipalname` or `user.mail` (if using the latter, ensure all users have the `Email` field filled in under `Contact Information`).
   3. (Optional) For SCIM, see the [setup documentation](/langsmith/user-management) for specific instructions about `Unique User Identifier (Name ID)`.

6. On the SAML-based Sign-on page, under **SAML Certificates**, copy the **App Federation Metadata URL**.

**Step 3: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the `Fill in required information` step, using the metadata URL from the previous step.

**Step 4: Verify the SSO setup**

1. Assign the application to users/groups in Entra ID:

1. Select **Manage** > **Users and groups**.

2. Click **Add user/group**.

3. In the **Add Assignment** window:

1. Under **Users**, click **None Selected**.
      2. Search for the user you want to assign to the enterprise application, and then click **Select**.
      3. Verify that the user is selected, and click **Assign**.

2. Have the user sign in via the unique login URL from the **SSO Configuration** page, or go to **Manage** > **Single sign-on** and select **Test single sign-on with (application name)**.

For additional information, see Google's [documentation](https://support.google.com/a/answer/6087519).

**Step 1: Create and configure the Google Workspace SAML application**

1. Make sure you're signed into an administrator account with the appropriate permissions.

2. In the Admin console, go to **Menu** -> **Apps** -> **Web and mobile apps**.

3. Click **Add App** and then **Add custom SAML app**.

4. Enter the app name and, optionally, upload an icon. Click **Continue**.

5. On the Google Identity Provider details page, download the **IDP metadata** and save it for Step 2. Click **Continue**.

6. In the `Service Provider Details` window, enter:

1. `ACS URL`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. `Entity ID`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. Leave `Start URL` and the `Signed response` box empty.
   4. Set `Name ID` format to `EMAIL` and leave `Name ID` as the default (`Basic Information > Primary email`).
   5. Click `Continue`.

7. Use `Add mapping` to ensure required claims are present:
   1. `Basic Information > Primary email` -> `email`

**Step 2: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the `Fill in required information` step, using the `IDP metadata` from the previous step as the metadata XML.

**Step 3: Turn on the SAML app in Google**

1. Select the SAML app under `Menu -> Apps -> Web and mobile apps`

2. Click `User access`.

3. Turn on the service:

1. To turn the service on for everyone in your organization, click `On for everyone`, and then click `Save`.

2. To turn the service on for an organizational unit:

1. At the left, select the organizational unit then `On`.
      2. If the Service status is set to `Inherited` and you want to keep the updated setting, even if the parent setting changes, click `Override`.
      3. If the Service status is set to `Overridden`, either click `Inherit` to revert to the same setting as its parent, or click `Save` to keep the new setting, even if the parent setting changes.

3. To turn on a service for a set of users across or within organizational units, select an access group. For details, go to [Use groups to customize service access](https://support.google.com/a/answer/9050643).

4. Ensure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain.

**Step 4: Verify the SSO setup**

Have a user with access sign in via the unique login URL from the **SSO Configuration** page, or go to the SAML application page in Google and click **TEST SAML LOGIN**.

#### Supported features

* IdP-initiated SSO (Single Sign-On)
* SP-initiated SSO
* Just-In-Time provisioning
* Enforce SSO only

#### Configuration steps

For additional information, see Okta's [documentation](https://help.okta.com/en-us/content/topics/apps/apps_app_integration_wizard_saml.htm).

**Step 1: Create and configure the Okta SAML application**

<div id="via-okta-integration-network">
  <b>Via Okta Integration Network (recommended)</b>
</div>

1. Sign in to [Okta](https://login.okta.com/).
2. In the upper-right corner, select Admin. The button is not visible from the Admin area.
3. Select `Browse App Integration Catalog`.
4. Find and select the LangSmith application.
5. On the application overview page, select Add Integration.
6. Leave `ApiUrlBase` empty.
7. Fill in `AuthHost`:
   * US: `auth.langchain.com`
   * EU: `eu.auth.langchain.com`
8. (Optional, if planning to use [SCIM](#set-up-scim-for-your-organization) as well) Fill in `LangSmithUrl`:
   * US: `api.smith.langchain.com`
   * EU: `eu.api.smith.langchain.com`
9. Under Application Visibility, keep the box unchecked.
10. Select Next.
11. Select `SAML 2.0`.
12. Fill in `Sign-On Options`:
    * `Application username format`: `Email`
    * `Update application username on`: `Create and update`
    * `Allow users to securely see their password`: leave **unchecked**.
13. Copy the **Metadata URL** from the **Sign On Options** page to use in the next step.

**Via Custom App Integration**

<Warning>
  SCIM is not compatible with this method of configuration. Refer to [**Via Okta Integration Network**](#via-okta-integration-network).
</Warning>

1. Log in to Okta as an administrator, and go to the **Okta Admin console**.

2. Under **Applications** > **Applications** click **Create App Integration**.

3. Select **SAML 2.0**.

4. Enter an `App name` (e.g., `LangSmith`) and optionally an **App logo**, then click **Next**.

5. Enter the following information in the **Configure SAML** page:

1. `Single sign-on URL` (`ACS URL`). Keep `Use this for Recipient URL and Destination URL` checked:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/acs](https://auth.langchain.com/auth/v1/sso/saml/acs)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/acs](https://eu.auth.langchain.com/auth/v1/sso/saml/acs)
   2. `Audience URI (SP Entity ID)`:
      * US: [https://auth.langchain.com/auth/v1/sso/saml/metadata](https://auth.langchain.com/auth/v1/sso/saml/metadata)
      * EU: [https://eu.auth.langchain.com/auth/v1/sso/saml/metadata](https://eu.auth.langchain.com/auth/v1/sso/saml/metadata)
   3. `Name ID format`: **Persistent**.
   4. `Application username`: `email`.
   5. Leave the rest of the fields empty or set to their default.
   6. Click **Next**.

7. Copy the **Metadata URL** from the **Sign On** page to use in the next step.

**Step 2: Set up LangSmith SSO Configuration**

Follow the instructions under [initial configuration](#initial-configuration) in the **Fill in required information** step, using the metadata URL from the previous step.

**Step 3: Assign users to LangSmith in Okta**

1. Under **Applications** > **Applications**, select the SAML application created in Step 1.
2. Under the **Assignments** tab, click **Assign** then either **Assign to People** or **Assign to Groups**.
3. Make the desired selection(s), then **Assign** and **Done**.

**Step 4: Verify the SSO setup**

Have a user with access sign in via the unique login URL from the `SSO Configuration` page, or have a user select the application from their Okta dashboard.

#### SP-initiated SSO

Once service-provider–initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under **Organization members and roles** then **SSO configuration**.

## Set up SCIM for your organization

System for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith [organization and workspaces](/langsmith/administration-overview), keeping user access synchronized with your organization's identity provider.

<Note>
  SCIM is available for organizations on the [Enterprise plan](https://www.langchain.com/pricing). [Contact sales](https://www.langchain.com/contact-sales) to learn more.

SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.

SCIM support is API-only (see instructions below).
</Note>

SCIM eliminates the need for manual user management and ensures that user access is always up-to-date with your organization's identity system. This allows for:

* **Automated user management**: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP.
* **Reduced administrative overhead**: No need to manage user access manually across multiple systems.
* **Improved security**: Users who leave your organization are automatically deprovisioned from LangSmith.
* **Consistent access control**: User attributes and group memberships are synchronized between systems.
* **Scaling team access control**: Efficiently manage large teams with many workspaces and custom roles.
* **Role assignment**: Select specific [Organization Roles](/langsmith/rbac#organization-roles) and [Workspace Roles](/langsmith/rbac#workspace-roles) for groups of users.

* Your organization must be on an Enterprise plan.
* Your Identity Provider (IdP) must support SCIM 2.0.
* Only [Organization Admins](/langsmith/administration-overview#organization-roles) can configure SCIM.
* For cloud customers: [SAML SSO](#set-up-saml-sso-for-your-organization) must be configurable for your organization.
* For self-hosted customers: [OAuth with Client Secret](/langsmith/self-host-sso#with-secret) authentication mode must be enabled.
* For self-hosted customers, network traffic must be allowed from the identity provider to LangSmith:
  * Microsoft Entra ID supports allowlisting IP ranges or an agent-based solution to provide connectivity.
    ([details](https://learn.microsoft.com/en-us/entra/identity/app-provisioning/use-scim-to-provision-users-and-groups#ip-ranges)).
  * Okta supports allow-listing IPs or domains ([details](https://help.okta.com/en-us/content/topics/security/ip-address-allow-listing.htm))
    or an agent-based solution ([details](https://help.okta.com/en-us/content/topics/provisioning/opp/opp-main.htm)) to provide connectivity.

When a user belongs to multiple groups for the same workspace, the following precedence applies:

1. **Organization Admin groups** take highest precedence. Users in these groups will be `Admin` in all workspaces.
2. **Most recently created workspace-specific group** takes precedence over other workspace groups.

<Note>
  When a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.

SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts.
</Note>

#### Email verification

In cloud only, creating a new user with SCIM triggers an email to the user.
They must verify their email address by clicking the link in this email.
The link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.

### Attributes and Mapping

#### Group Naming Convention

<Warning>
  Renaming groups is **not** supported via SCIM. Group names are persistent because they must match role names and/or workspace names in LangSmith.
</Warning>

Group membership maps to LangSmith workspace membership and workspace roles with a specific naming convention:

**Organization Admin Groups**

Format: `<optional_prefix>Organization Admin` or `<optional_prefix>Organization Admins`

* `LS:Organization Admins`
* `Groups-Organization Admins`
* `Organization Admin`

**Workspace-Specific Groups**

Format: `<optional_prefix><org_role_name>:<workspace_name>:<workspace_role_name>`

* `LS:Organization User:Production:Annotators`
* `Groups-Organization User:Engineering:Developers`
* `Organization User:Marketing:Viewers`

While specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:

| **LangSmith App Attribute**    | **Identity Provider Attribute**                       | **Matching Precedence** |
| ------------------------------ | ----------------------------------------------------- | ----------------------- |
| `userName`<sup>1</sup>         | email address                                         |                         |
| `active`                       | `!deactivated`                                        |                         |
| `emails[type eq "work"].value` | email address<sup>2</sup>                             |                         |
| `name.formatted`               | `displayName` OR `givenName + familyName`<sup>3</sup> |                         |
| `givenName`                    | `givenName`                                           |                         |
| `familyName`                   | `familyName`                                          |                         |
| `externalId`                   | `sub`<sup>4</sup>                                     | 1                       |

1. `userName` is not required by LangSmith
2. Email address is required
3. Use the computed expression if your `displayName` does not match the format of `Firstname Lastname`
4. To avoid inconsistency, this should match the SAML `NameID` assertion for cloud customers, or the `sub` OAuth2.0 claim for self-hosted.

#### Group Attributes

| **LangSmith App Attribute** | **Identity Provider Attribute** | **Matching Precedence** |
| --------------------------- | ------------------------------- | ----------------------- |
| `displayName`               | `displayName`<sup>1</sup>       | 1                       |
| `externalId`                | `objectId`                      |                         |
| `members`                   | `members`                       |                         |

1. Groups must follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
   If your company has a group naming policy, you should instead map from the `description` identity provider attribute and
   set the description based on the [Group Naming Convention](#group-naming-convention) section.

### Step 1 - Configure SAML SSO (Cloud only)

There are two scenarios for [SAML SSO](#set-up-saml-sso-for-your-organization) configuration:

1. If SAML SSO is already configured for your organization, you should skip the steps to initially add the application ([Add application from Okta Integration Network](#add-application-okta-oin) or [Create a new Entra ID application integration](#create-application-entra-id)), as you already have an application configured and just need to enable provisioning.
2. If you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to [set up SAML SSO](#set-up-saml-sso-for-your-organization), *then* follow the instructions here to enable SCIM.

LangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive.

1. Be unique to each user.
2. Be a persistent value that never changes, such as a randomly generated unique user ID.
3. Match exactly on each sign-in attempt. It should not rely on user input.

The NameID should not be an email address or username because email addresses and usernames are more likely to change over time and can be case-sensitive.

The NameID format must be `Persistent`, unless you are using a field, like email, that requires a different format.

### Step 2 - Disable JIT provisioning

Before enabling SCIM, disable [Just-in-time (JIT) provisioning](/langsmith/user-management#just-in-time-jit-provisioning) to prevent conflicts between automatic and manual user provisioning.

#### Disabling JIT for Cloud

Use the `PATCH /orgs/current/info` [endpoint](https://api.smith.langchain.com/redoc#tag/orgs/operation/update_current_organization_info_api_v1_orgs_current_info_patch):

#### Disabling JIT for Self-Hosted

As of LangSmith chart version **0.11.14**, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:

### Step 3 - Generate SCIM bearer token

<Note>
  In self-hosted environments, the full URL below may look like `https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens` (without a subdomain, note the `/api/v1` path prefix) or `https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens` (with a subdomain) - see the [ingress docs](/langsmith/self-host-ingress) for more details.
</Note>

Generate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:

Note that the SCIM Bearer Token value is not available outside of the response to this request. These additional endpoints are present:

* `GET /v1/platform/orgs/current/scim/tokens`
* `GET /v1/platform/orgs/current/scim/tokens/{scim_token_id}`
* `PATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id}` (only the `description` field is supported)
* `DELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id}`

### Step 4 - Configure your Identity Provider

<Note>
  If you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to [Azure Entra ID](#azure-entra-id-configuration-steps), [Okta](#okta)). The requirements and steps above are applicable for all identity providers.
</Note>

#### Azure Entra ID configuration steps

For additional information, see Microsoft's [documentation](https://learn.microsoft.com/en-us/entra/identity/app-provisioning/user-provisioning).

<Note>
  In self-hosted installations, the `oid` JWT claim is used as the `sub`.
  See [this Microsoft Learn link](https://learn.microsoft.com/en-us/answers/questions/5546297/how-to-link-oidc-users-with-scim)
  and [the related configuration instructions](/langsmith/self-host-sso#override-sub-claim) for additional details.
</Note>

**Step 1: Configure SCIM in your Enterprise Application**

1. Log in to the [Azure portal](https://portal.azure.com/#home) with a privileged role (e.g., `Global Administrator`).
2. Navigate to your existing LangSmith Enterprise Application.
3. In the left-side navigation, select **Manage** > **Provisioning**.
4. Click **Get started**.

**Step 2: Configure Admin credentials**

1. Under **Admin Credentials**:

* US: `https://api.smith.langchain.com/scim/v2`
     * EU: `https://eu.api.smith.langchain.com/scim/v2`
     * Self-hosted: `<langsmith_url>/scim/v2`

* **Secret Token**: Enter the SCIM Bearer Token generated in Step 3.

2. Click **Test Connection** to verify the configuration.

**Step 3: Configure Attribute Mappings**

Configure the following attribute mappings under `Mappings`:

Set **Target Object Actions** to `Create` and `Update` (start with `Delete` disabled for safety):

|   **LangSmith App Attribute**  |            **Microsoft Entra ID Attribute**           | **Matching Precedence** |
| :----------------------------: | :---------------------------------------------------: | :---------------------: |
|           `userName`           |                  `userPrincipalName`                  |                         |
|            `active`            |                 `Not([IsSoftDeleted])`                |                         |
| `emails[type eq "work"].value` |                        `mail`1                        |                         |
|        `name.formatted`        | `displayName` OR `Join(" ", [givenName], [surname])`2 |                         |
|          `externalId`          |                      `objectId`3                      |            1            |

1. User's email address must be present in Entra ID.
2. Use the `Join` expression if your `displayName` does not match the format of `Firstname Lastname`.
3. To avoid inconsistency, this should match the SAML NameID assertion and the `sub` OAuth2.0 claim. For SAML SSO in cloud, the `Unique User Identifier (Name ID)` required claim should be `user.objectID` and the `Name identifier format` should be `persistent`.

Set **Target Object Actions** to `Create` and `Update` only (start with `Delete` disabled for safety):

| **LangSmith App Attribute** | **Microsoft Entra ID Attribute** | **Matching Precedence** |
| :-------------------------: | :------------------------------: | :---------------------: |
|        `displayName`        |          `displayName`1          |            1            |
|         `externalId`        |            `objectId`            |                         |
|          `members`          |             `members`            |                         |

1. Groups must follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
   If your company has a group naming policy, you should instead map from the `description` Microsoft Entra ID Attribute and
   set the description based on the [Group Naming Convention](#group-naming-convention) section.

**Step 4: Assign Users and Groups**

1. Under **Applications** > **Applications**, select your LangSmith Enterprise Application.
2. Under the **Assignments** tab, click **Assign** then either **Assign to People** or **Assign to Groups**.
3. Make the desired selection(s), then **Assign** and **Done**.

**Step 5: Enable Provisioning**

1. Set **Provisioning Status** to `On` under **Provisioning**.
2. Monitor the initial sync to ensure users and groups are provisioned correctly.
3. Once verified, enable `Delete` actions for both User and Group mappings.

For troubleshooting, refer to the [SAML SSO FAQs](/langsmith/faq#saml-sso-faqs). If you have issues setting up SCIM, contact the LangChain support team via [support.langchain.com](https://support.langchain.com).

#### Okta configuration steps

<Note>
  You must use the [Okta Lifecycle Management](https://www.okta.com/products/lifecycle-management/) product. This product tier is required to use SCIM on Okta.
</Note>

<div id="supported-features">
  <b>Supported features</b>
</div>

* Create users
* Update user attributes
* Deactivate users
* Group push (**without group renaming**)
* Import users
* Import groups

<div id="add-application-okta-oin">
  <b>Step 1: Add application from Okta Integration Network</b>
</div>

<Note>
  If you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step.
</Note>

See [SAML SSO setup](#okta) for cloud or [OAuth2.0 setup](/langsmith/self-host-sso#okta-idp-setup) for self-hosted.

**Step 2: Configure API Integration**

1. In the General tab, ensure the `LangSmithUrl` is filled in according to the instructions from [Step 1](#add-application-okta-oin)
2. In the Provisioning tab, select `Integration`.
3. Select `Edit` then `Enable API integration`.
4. For API Token, paste the SCIM token you [generated above](#step-3-generate-scim-bearer-token).
5. Keep `Import Groups` checked.
6. To verify the configuration, select Test API Credentials.
7. Select Save.
8. After saving the API integration details, new settings tabs appear on the left. Select `To App`.
9. Select Edit.
10. Select the Enable checkbox for Create Users, Update Users, and Deactivate Users.
11. Select Save.
12. Assign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group.

**Step 3: Configure User Provisioning Settings**

1. Configure provisioning: under `Provisioning > To App > Provisioning to App`, click `Edit`, then check `Create Users`, `Update User Attributes`, and `Deactivate Users`.
2. Under `<application_name> Attribute Mappings`, set the user attribute mappings as shown below, and delete the rest:

<img src="https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=4951533e29e0f0f75e7aac74dcfab3bb" alt="SCIM Okta User Attributes Mapping" data-og-width="748" width="748" data-og-height="467" height="467" data-path="langsmith/images/scim_okta_user_attributes.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=280&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=f883ed0bf6e0876126e5a7f79dadfbb1 280w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=560&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=0dcd59e75cc4e7b5532dfc9ad81347fa 560w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=840&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=3c9ee64fb3d1357bb67685ca85ada751 840w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=1100&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=fb7274973d558c7973c6c00195d77341 1100w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=1650&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=7830d972b9ad3cb036d8c08623bc891b 1650w, https://mintcdn.com/langchain-5e9cc07a/Fd2sBZqE3DLR7pWt/langsmith/images/scim_okta_user_attributes.png?w=2500&fit=max&auto=format&n=Fd2sBZqE3DLR7pWt&q=85&s=f1e137b9faa8d6422cff5e3673cf98fb 2500w" />

**Step 4: Push Groups**

<Note>
  Okta does not support group attributes besides the group name itself, so group name *must* follow the naming convention described in the [Group Naming Convention](#group-naming-convention) section.
</Note>

Follow Okta's [Enable Group Push](https://help.okta.com/en-us/content/topics/users-groups-profiles/usgp-enable-group-push.htm) instructions to configure groups to push by name or by rule.

#### Other Identity Providers

Other identity providers have not been tested but may function depending on their SCIM implementation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/user-management.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Disabling JIT for Self-Hosted

As of LangSmith chart version **0.11.14**, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:
```

Example 2 (unknown):
```unknown
### Step 3 - Generate SCIM bearer token

<Note>
  In self-hosted environments, the full URL below may look like `https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens` (without a subdomain, note the `/api/v1` path prefix) or `https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens` (with a subdomain) - see the [ingress docs](/langsmith/self-host-ingress) for more details.
</Note>

Generate a SCIM Bearer Token for your organization. This token will be used by your IdP to authenticate SCIM API requests. Ensure env vars are set appropriately, for example:
```

---

## Use annotation queues

**URL:** llms-txt#use-annotation-queues

**Contents:**
- Single-run annotation queues
  - Create a single-run queue
  - Assign runs to a single-run queue
  - Review a single-run queue
- Pairwise annotation queues
  - Create a pairwise queue
  - Add more comparisons to a pairwise queue
  - Review a pairwise queue
- Video guide

Source: https://docs.langchain.com/langsmith/annotation-queues

*Annotation queues* provide a streamlined, directed view for human annotators to attach feedback to specific [runs](/langsmith/observability-concepts#runs). While you can always annotate [traces](/langsmith/observability-concepts#traces) inline, annotation queues provide a way to group runs together, prescribe rubrics, and track reviewer progress.

LangSmith supports two queue styles:

* [**Single-run annotation queues**](#single-run-annotation-queues) present one run at a time and let reviewers submit any rubric feedback you configure.
* [**Pairwise annotation queues (PAQs)**](#pairwise-annotation-queues) present two runs side-by-side so reviewers can quickly decide which output is better (or if they are equivalent) against the rubric items you define.

## Single-run annotation queues

Single-run queues present one run at a time and let reviewers submit any rubric feedback you configure. They can be created directly from the **Annotation queues** section in the [LangSmith UI](https://smith.langchain.com/).

### Create a single-run queue

1. Navigate to **Annotation queues** in the left navigation.
2. Click **+ New annotation queue** in the top-right corner.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=c5c28c10a5522af0a37f40236ed57510" alt="Create Annotation Queue form with Basic Details, Annotation Rubric, and Feedback sections." data-og-width="3456" width="3456" data-og-height="1912" height="1912" data-path="langsmith/images/create-annotation-queue-new.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=daa5c44976804eae5ca8bbfef1d0a9d0 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=167955e0202671425e6cd1476c31a756 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=71627eeab271c6d4581f00506731cc09 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=cd30341efa9d5eea82d85b63518b53a0 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=3e71d31b42b3411946f73d79e8735599 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-queue-new.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=832e7c8b99d332176bc9d9de702a6bac 2500w" />

1. Fill in the **Name** and **Description** of the queue.
2. Optionally assign a **default dataset** to streamline exporting reviewed runs into a dataset in your LangSmith [workspace](/langsmith/administration-overview#workspaces).

#### Annotation Rubric

1. Draft some high-level instructions for your annotators, which will be shown in the sidebar on every run.
2. Click **+ Desired Feedback** to add feedback keys to your annotation queue. Annotators will be presented with these feedback keys on each run.
3. Add a description for each, as well as a short description of each category, if the feedback is categorical.

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=8adfdba2649847f82543674978b0d1b1" alt="Annotation queue rubric form with instructions and desired feedback entered." data-og-width="3456" width="3456" data-og-height="1914" height="1914" data-path="langsmith/images/create-annotation-rubric.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=280&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=5d73a7688b61b3b9489aacac1223f7c6 280w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=560&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=ede9b22be4e3ce82e4feabf86575e8a5 560w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=840&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=768747aa9e314c66631f27e794d9174b 840w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=1100&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=f3061b6ba68c4d9cab997bbed2efe76e 1100w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=1650&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=27545c5b64b3b82ae9ebed853ff02168 1650w, https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/create-annotation-rubric.png?w=2500&fit=max&auto=format&n=aKRoUGXX6ygp4DlC&q=85&s=801397056da06004808b6c38df30c139 2500w" />

For example, with the descriptions in the previous screenshot, reviewers will see the **Annotation Rubric** details in the right-hand pane of the UI.

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=44452f7da89329acc06672beba4e4c0e" alt="The rendered rubric for reviewers from the example instructions." data-og-width="3456" width="3456" data-og-height="1912" height="1912" data-path="langsmith/images/rubric-for-annotators.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=fa18a86229854c27a85c341da2638501 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=67f5880b9d79e05e2dcd3703db92f79a 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=01436264374f87f9baab4ac4f3f8c161 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=80b8569a8ef3acc5b7d94a9276dc0d26 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=b4c6f43fe2e91f57de4a2790757a2083 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/rubric-for-annotators.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=aae4a68208b5b6838c3fdf77f2c57efe 2500w" />

#### Collaborator Settings

When there are multiple annotators for a run:

* **Number of reviewers per run**: This determines the number of reviewers that must mark a run as **Done** for it to be removed from the queue. If you check **All workspace members review each run**, then a run will remain in the queue until all [workspace](/langsmith/administration-overview#workspaces) members have marked their review as **Done**.

* Reviewers cannot view the feedback left by other reviewers.
  * Comments on runs are visible to all reviewers.

* **Enable reservations on runs**: When a reviewer views a run, the run is reserved for that reviewer for the specified **Reservation length**. If there are multiple reviewers per run as specified above, the run can be reserved by multiple reviewers (up to the number of reviewers per run) at the same time.

<Tip>
    We recommend enabling reservations. This will prevent multiple annotators from reviewing the same run at the same time.
  </Tip>

If a reviewer has viewed a run and then leaves the run without marking it **Done**, the reservation will expire after the specified **Reservation length**. The run is then released back into the queue and can be reserved by another reviewer.

<Note>
    Clicking **Requeue** for a run's annotation will only move the current run to the end of the current user's queue; it won't affect the queue order of any other user. It will also release the reservation that the current user has on that run.
  </Note>

Because of these settings, the number of runs visible to each reviewer can differ from the total queue size.

You can revisit the pencil icon <Icon icon="pencil" /> in **Annotation queues** to update any settings later.

### Assign runs to a single-run queue

There are several ways to populate a single-run queue with work items:

* **From a trace view**: Click **Add to Annotation Queue** in the top-right corner of any [trace](/langsmith/observability-concepts#traces) view. You can add any intermediate [run](/langsmith/observability-concepts#runs), but not the root span.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fc604c7f91bc8795dc688c4f9db73ce9" alt="Trace view with the Add to Annotation Queue button highlighted at the top of the screen." data-og-width="1373" width="1373" data-og-height="1028" height="1028" data-path="langsmith/images/add-to-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=0ff1545d09984dfb766067ad65ecbfb9 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=055554579c01cc8c48471e0d74fe27d6 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=10412c26d4042358e098631386cddbf2 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=be2c6162599ab18ef34975a439f16e93 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=4e0b51a16246b1e28d415f23d86643c7 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-to-annotation-queue.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c7744f75389cd270f60bbbee9571582a 2500w" />

* **From the runs table**: Select multiple runs, then click **Add to Annotation Queue** at the bottom of the page.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=c6781e6a7345ef7e16ea7a0bb306a474" alt="View of the runs table with runs selected. Add to Annotation Queue button at the botton of the page." data-og-width="1323" width="1323" data-og-height="1317" height="1317" data-path="langsmith/images/multi-select-annotation-queue.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=03fff2a1f8cc40bf86f4b9251dacc0e1 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=2cc50c1d4f1b9ec24f9e3bc4d5fabbff 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=b37ce7b181457582652a22405f240750 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=badc9ef126c220b9aa8f5e8212421b93 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=ae02333d194a955c1a6c86a2bf87c75f 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/multi-select-annotation-queue.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=9dce63a0eedfc4a02a5ad822cee67bce 2500w" />

* **Automation rules**: [Set up a rule](/langsmith/rules) to automatically assign runs that match a filter (for example, errors or low user scores) into a queue.

* **Datasets & experiments**: Select one or more [experiments](/langsmith/evaluation-concepts#experiment) within a dataset and click **<Icon icon="pencil" /> Annotate**. Choose an existing queue or create a new one, then confirm the (single-run) queue option.

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=7622e6db855711542de24270ddc129dc" alt="Selected experiments with the Annotate button at the bottom of the page." data-og-width="3456" width="3456" data-og-height="1914" height="1914" data-path="langsmith/images/annotate-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6bc0abf70504c439b413dfb6a3ff59f7 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c511826c68b2ecf9fb75addc50df48c3 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=450345b069f8de91a982d66bfe6ce8a9 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=fb67c24cd3f6fee0042ad3ef94fc9a59 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=83aab2b2cd3cc7a84440caa0d57a68be 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/annotate-experiment.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=08a045816ca42285ad46570280ee7061 2500w" />

### Review a single-run queue

1. Navigate to the **Annotation Queues** section through the left-hand navigation bar.
2. Click on the queue you want to review. This will take you to a focused, cyclical view of the runs in the queue that require review.
3. You can attach a comment, attach a score for a particular [feedback](/langsmith/observability-concepts#feedback) criteria, add the run to a dataset or mark the run as reviewed. You can also remove the run from the queue for all users, despite any current reservations or settings for the queue, by clicking the **Trash** icon <Icon icon="trash" /> next to **View run**.

<Tip>
     The keyboard shortcuts that are next to each option can help streamline the review process.
   </Tip>

<img src="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=9065d4b85e6165084b65d3908d61778a" alt="View or a run with the Annotate side panel. Keyboard shortcuts visible for options." data-og-width="1532" width="1532" data-og-height="1080" height="1080" data-path="langsmith/images/review-runs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=280&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=f69f94916e247ff498e1d9e5ed2755a2 280w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=560&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=64f16b8b83ffc56d1ad078ae1bfacd65 560w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=840&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=085b9e1ea9b3797bc10117c326a80018 840w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=1100&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=4321324e5d276d12864e446cd2a97c82 1100w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=1650&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=7eeb685e4d4064518613283e502c9395 1650w, https://mintcdn.com/langchain-5e9cc07a/Fr2lazPB4XVeEA7l/langsmith/images/review-runs.png?w=2500&fit=max&auto=format&n=Fr2lazPB4XVeEA7l&q=85&s=059f57fb238aa0cd1ebcf6a2fb0ede95 2500w" />

## Pairwise annotation queues

Pairwise annotation queues (PAQs) present two runs side-by-side so reviewers can quickly decide which output is better (or if they are equivalent) against the rubric items you define. They are designed for fast A/B comparisons between two experiments (often a baseline vs. a candidate model) and must be created from the **Datasets & Experiments** pages.

### Create a pairwise queue

1. Navigate to **Datasets & Experiments**, open a dataset, and select **exactly two experiments** you want to compare.

2. Click **Annotate**. In the popover, choose **Add to Pairwise Annotation Queue**. (The button is disabled until exactly two experiments are selected.)

<img src="https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-popup.png?fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=ef08b7166abce2d890ba4b9be8cae927" alt="Popover showing the &#x22;Add to Pairwise Annotation Queue&#x22; card highlighted after two experiments are selected." data-og-width="3456" width="3456" data-og-height="1980" height="1980" data-path="langsmith/images/pairwise-annotation-queue-popup.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-popup.png?w=280&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=96d0a67c3745542396ee4940228ffa1c 280w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-popup.png?w=560&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=fc02da4f82cab44767aa74c3e418de59 560w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-popup.png?w=840&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=8b719d329f260dfa8f0869f089a9caa5 840w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-popup.png?w=1100&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=2475caf18fde10b9f4539553d3963211 1100w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-popup.png?w=1650&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=c829586016d288d59e1c8158870a8ad1 1650w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-popup.png?w=2500&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=f91c18ad5c0c2e4b85092fe2a113a0af 2500w" />

3. Decide whether to send the experiments to an existing pairwise queue or create a new one.

4. Provide the queue details:
   * **Basic details** (name and description)
   * **Instructions & rubrics** tailored to pairwise scoring
   * **Collaborator settings** (reviewer count, reservations, reservation length)

5. Submit the form to create the queue. LangSmith immediately pairs runs from the two experiments and populates the queue.

Key differences for PAQs:

* **Experiments**: You must provide two experiment sessions up front. LangSmith automatically pairs their runs in chronological order and populates the queue during creation.
* **Rubric**: Pairwise rubric items only require a feedback key and (optionally) a description. Annotators decide whether Run A, Run B, or both are better for each rubric item.
* **Dataset**: Pairwise queues do not use a default dataset, because comparisons span two experiments.
* **Reservations & reviewers**: The same collaborator controls apply. Reservations help prevent two people from judging the same comparison simultaneously.

### Add more comparisons to a pairwise queue

If you need to add more comparisons later, return to **Datasets & Experiments**, select the two experiments again, and choose **Add to Pairwise Annotation Queue** to append new pairs.

Selecting two experiments and creating a PAQ automatically pairs the runs. When augmenting an existing PAQ, LangSmith preserves historical comparisons and appends new pairs to the queue.

### Review a pairwise queue

1. From **Annotation queues**, select the pairwise queue you want to review.
2. Each queue item displays Run A on the left and Run B on the right, along with your rubric.
3. For every rubric item:
   * Choose **A is better**, **B is better**, or **Equal**. The UI records binary feedback on both runs behind the scenes.
   * Use hotkeys `A`, `B`, or `E` to lock in your choice.
4. Once you finish all rubric items, press **Done** (or `Enter` on the final rubric item) to advance to the next comparison.
5. Optional actions:
   * Leave comments tied to either run.
   * Requeue the comparison if you need to revisit it later.
   * Open the full trace view for deeper debugging.

Reservations, reviewer thresholds, and comments behave identically to those in single-run queues, enabling teams to use different queue types without modifying their existing workflow.

<img src="https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-review-feedback-pane.png?fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=b144d168c4f4fd1f624c1d0fd5ce7e3e" alt="Pairwise review screen showing runs side-by-side with the feedback pane containing A/B/Equal buttons and keyboard shortcuts." data-og-width="3456" width="3456" data-og-height="1980" height="1980" data-path="langsmith/images/pairwise-annotation-queue-review-feedback-pane.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-review-feedback-pane.png?w=280&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=4398df0649f7941284f411e713b9324e 280w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-review-feedback-pane.png?w=560&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=077edf8001c6b308c618413707b0c10b 560w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-review-feedback-pane.png?w=840&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=51df52243ba575ef82ebc22ad1112d8a 840w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-review-feedback-pane.png?w=1100&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=612d906fe461da78958a624d3fb7ab0a 1100w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-review-feedback-pane.png?w=1650&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=66aa9cad6b4f2ce5c2fa02fd844c89dc 1650w, https://mintcdn.com/langchain-5e9cc07a/jimZt8pd1vc7LfPM/langsmith/images/pairwise-annotation-queue-review-feedback-pane.png?w=2500&fit=max&auto=format&n=jimZt8pd1vc7LfPM&q=85&s=913a8b6dc35b46af02da7673ca0d963d 2500w" />

<Check>
  Consider routing runs that already have user feedback (e.g., thumbs-down) into a single-run queue for triage and a pairwise queue for head-to-head comparisons against a stronger baseline. This helps you identify regressions quickly. To learn more about how to capture user feedback from your LLM application, follow the guide on [attaching user feedback](/langsmith/attach-user-feedback).
</Check>

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/rxKYHA-2KS0?si=V4EnrUmzJaUVJh0m" title="YouTube video player" frameBorder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/annotation-queues.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Use cron jobs

**URL:** llms-txt#use-cron-jobs

**Contents:**
- Setup
- Cron job on a thread
- Cron job stateless

Source: https://docs.langchain.com/langsmith/cron-jobs

There are many situations in which it is useful to run an assistant on a schedule.

For example, say that you're building an assistant that runs daily and sends an email summary
of the day's news. You could use a cron job to run the assistant every day at 8:00 PM.

LangSmith Deployment supports cron jobs, which run on a user-defined schedule. The user specifies a schedule, an assistant, and some input. After that, on the specified schedule, the server will:

* Create a new thread with the specified assistant
* Send the specified input to that thread

Note that this sends the same input to the thread every time.

The LangSmith Deployment API provides several endpoints for creating and managing cron jobs. See the [API reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref/) for more details.

Sometimes you don't want to run your graph based on user interaction, but rather you would like to schedule your graph to run on a schedule - for example if you wish for your graph to compose and send out a weekly email of to-dos for your team. LangSmith Deployment allows you to do this without having to write your own script by using the `Crons` client. To schedule a graph job, you need to pass a [cron expression](https://crontab.cronhub.io/) to inform the client when you want to run the graph. `Cron` jobs are run in the background and do not interfere with normal invocations of the graph.

First, let's set up our SDK client, assistant, and thread:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Cron job on a thread

To create a cron job associated with a specific thread, you can write:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Note that it is **very** important to delete `Cron` jobs that are no longer useful. Otherwise you could rack up unwanted API charges to the LLM! You can delete a `Cron` job using the following code:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Cron job stateless

You can also create stateless cron jobs by using the following code:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

Again, remember to delete your job once you are done with it!

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="Javascript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/cron-jobs.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Javascript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Output:
```

Example 4 (unknown):
```unknown
## Cron job on a thread

To create a cron job associated with a specific thread, you can write:

<Tabs>
  <Tab title="Python">
```

---

## Use different sampling rates for different operations

**URL:** llms-txt#use-different-sampling-rates-for-different-operations

with tracing_context(client=client_1):
    # Your code here - will be traced with 50% sampling rate
    agent_1.invoke(...)

with tracing_context(client=client_2):
    # Your code here - will be traced with 25% sampling rate
    agent_1.invoke(...)

with tracing_context(client=client_no_trace):
    # Your code here - will not be traced
    agent_1.invoke(...)
```

This allows you to control sampling rates at the operation level.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/sample-traces.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Use DuckDuckGo if you don't have a Tavily API key:

**URL:** llms-txt#use-duckduckgo-if-you-don't-have-a-tavily-api-key:

---

## Use environment variables for model providers

**URL:** llms-txt#use-environment-variables-for-model-providers

**Contents:**
- Requirements
- Configuration

Source: https://docs.langchain.com/langsmith/self-host-playground-environment-settings

<Note>
  This feature is only available on Helm chart versions 0.10.27 (application version 0.10.74) and later.
</Note>

Many model providers support setting credentials and other configuration options through environment variables. This is useful for self-hosted deployments where you want to avoid hardcoding sensitive information in your code or configuration files. In LangSmith, most model interactions are done through the `playground` service, which allows you to configure many of those environment variables directly on the pod itself. This can be useful to avoid having to set credentials in the UI.

* A self-hosted LangSmith instance with the `playground` service running.
* The provider you want to configure must support environment variables for configuration. Check the provider's Chat Model [documentation](https://python.langchain.com/docs/integrations/providers/) for more information.
* The secrets/roles you may want to attach to the `playground` service.
  * Note that for IRSA you may need to grant the `langsmith-playground` service account the necessary permissions to access the secrets or roles in your cloud provider.

With the parameters from above, you can configure your LangSmith instance to use environment variables for model providers. You can do this by modifying the `langsmith_config.yaml` file for your LangSmith Helm Chart installation or the `docker-compose.yaml` file for your Docker installation.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-playground-environment-settings.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Use HTTP headers for runtime configuration

**URL:** llms-txt#use-http-headers-for-runtime-configuration

**Contents:**
- Using within your graph

Source: https://docs.langchain.com/langsmith/configurable-headers

LangGraph allows runtime configuration to modify agent behavior and permissions dynamically. When using [LangSmith Deployment](/langsmith/deployment-quickstart), you can pass this configuration in the request body (`config`) or specific request headers. This enables adjustments based on user identity or other requests.

For privacy, control which headers are passed to the runtime configuration via the `http.configurable_headers` section in your [`langgraph.json`](/langsmith/application-structure#configuration-file) file.

Here's how to customize the included and excluded headers:

The `includes` and `excludes` lists accept exact header names or patterns using `*` to match any number of characters. For your security, no other regex patterns are supported.

## Using within your graph

You can access the included headers in your graph using the `config` argument of any node.

Or by fetching from context (useful in tools and or within other nested functions).

You can even use this to dynamically compile the graph.

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
The `includes` and `excludes` lists accept exact header names or patterns using `*` to match any number of characters. For your security, no other regex patterns are supported.

## Using within your graph

You can access the included headers in your graph using the `config` argument of any node.
```

Example 2 (unknown):
```unknown
Or by fetching from context (useful in tools and or within other nested functions).
```

Example 3 (unknown):
```unknown
You can even use this to dynamically compile the graph.
```

---

## Use in an async context

**URL:** llms-txt#use-in-an-async-context

results = await search_store()
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/semantic-search.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## use in our SQL queries.

**URL:** llms-txt#use-in-our-sql-queries.

def index_fields() -> tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore]: ...

track_store, artist_store, album_store = index_fields()

---

## Use in WebSocket endpoint

**URL:** llms-txt#use-in-websocket-endpoint

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

async def websocket_audio_stream():
        """Yield audio bytes from WebSocket."""
        while True:
            data = await websocket.receive_bytes()
            yield data

# Transform audio through pipeline
    output_stream = pipeline.atransform(websocket_audio_stream())

# Send TTS audio back to client
    async for event in output_stream:
        if event.type == "tts_chunk":
            await websocket.send_bytes(event.audio)
```

We use [RunnableGenerators](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.base.RunnableGenerator) to compose each step of the pipeline. This is an abstraction LangChain uses internally to manage [streaming across components](https://reference.langchain.com/python/langchain_core/runnables/).

Each stage processes events independently and concurrently: audio transcription begins as soon as audio arrives, the agent starts reasoning as soon as a transcript is available, and speech synthesis begins as soon as agent text is generated. This architecture can achieve sub-700ms latency to support natural conversation.

For more on building agents with LangChain, see the [Agents guide](/oss/python/langchain/agents).

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/voice-agent.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Use the functional API

**URL:** llms-txt#use-the-functional-api

**Contents:**
- Creating a simple workflow
- Parallel execution
- Calling graphs
- Call other entrypoints
- Streaming
- Retry policy

Source: https://docs.langchain.com/oss/python/langgraph/use-functional-api

The [**Functional API**](/oss/python/langgraph/functional-api) allows you to add LangGraph's key features — [persistence](/oss/python/langgraph/persistence), [memory](/oss/python/langgraph/add-memory), [human-in-the-loop](/oss/python/langgraph/interrupts), and [streaming](/oss/python/langgraph/streaming) — to your applications with minimal changes to your existing code.

<Tip>
  For conceptual information on the functional API, see [Functional API](/oss/python/langgraph/functional-api).
</Tip>

## Creating a simple workflow

When defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.

<Accordion title="Extended example: simple workflow">
  
</Accordion>

<Accordion title="Extended example: Compose an essay with an LLM">
  This example demonstrates how to use the `@task` and `@entrypoint` decorators
  syntactically. Given that a checkpointer is provided, the workflow results will
  be persisted in the checkpointer.

## Parallel execution

Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).

<Accordion title="Extended example: parallel LLM calls">
  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.

This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.
</Accordion>

The **Functional API** and the [**Graph API**](/oss/python/langgraph/graph-api) can be used together in the same application as they share the same underlying runtime.

<Accordion title="Extended example: calling a simple graph from the functional API">
  
</Accordion>

## Call other entrypoints

You can call other **entrypoints** from within an **entrypoint** or a **task**.

<Accordion title="Extended example: calling another entrypoint">
  
</Accordion>

The **Functional API** uses the same streaming mechanism as the **Graph API**. Please
read the [**streaming guide**](/oss/python/langgraph/streaming) section for more details.

Example of using the streaming API to stream both updates and custom data.

1. Import [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) from `langgraph.config`.
2. Obtain a stream writer instance within the entrypoint.
3. Emit custom data before computation begins.
4. Emit another custom message after computing the result.
5. Use `.stream()` to process streamed output.
6. Specify which streaming modes to use.

<Warning>
  **Async with Python \< 3.11**
  If using Python \< 3.11 and writing async code, using [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) will not work. Instead please
  use the `StreamWriter` class directly. See [Async with Python \< 3.11](/oss/python/langgraph/streaming#async) for more details.

```python  theme={null}
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.func import entrypoint, task
from langgraph.types import RetryPolicy

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: simple workflow">
```

Example 2 (unknown):
```unknown
</Accordion>

<Accordion title="Extended example: Compose an essay with an LLM">
  This example demonstrates how to use the `@task` and `@entrypoint` decorators
  syntactically. Given that a checkpointer is provided, the workflow results will
  be persisted in the checkpointer.
```

Example 3 (unknown):
```unknown
</Accordion>

## Parallel execution

Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).
```

Example 4 (unknown):
```unknown
<Accordion title="Extended example: parallel LLM calls">
  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.
```

---

## Use the functional API result in the graph

**URL:** llms-txt#use-the-functional-api-result-in-the-graph

**Contents:**
- Migration between APIs
  - From Functional to Graph API

def orchestrator_node(state):
    processed_data = data_processor.invoke(state["raw_data"])
    return {"processed_data": processed_data}
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Migration between APIs

### From Functional to Graph API

When your functional workflow grows complex, you can migrate to the Graph API:
```

---

## Use the graph API

**URL:** llms-txt#use-the-graph-api

**Contents:**
- Setup
- Define and update state
  - Define state
  - Update state
  - Process state updates with reducers
  - Bypass reducers with `Overwrite`
  - Define input and output schemas

Source: https://docs.langchain.com/oss/python/langgraph/use-graph-api

This guide demonstrates the basics of LangGraph's Graph API. It walks through [state](#define-and-update-state), as well as composing common graph structures such as [sequences](#create-a-sequence-of-steps), [branches](#create-branches), and [loops](#create-and-control-loops). It also covers LangGraph's control features, including the [Send API](#map-reduce-and-the-send-api) for map-reduce workflows and the [Command API](#combine-control-flow-and-state-updates-with-command) for combining state updates with "hops" across nodes.

<Tip>
  **Set up LangSmith for better debugging**

Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the [docs](/langsmith/observability).
</Tip>

## Define and update state

Here we show how to define and update [state](/oss/python/langgraph/graph-api#state) in LangGraph. We will demonstrate:

1. How to use state to define a graph's [schema](/oss/python/langgraph/graph-api#schema)
2. How to use [reducers](/oss/python/langgraph/graph-api#reducers) to control how state updates are processed.

[State](/oss/python/langgraph/graph-api#state) in LangGraph can be a `TypedDict`, `Pydantic` model, or dataclass. Below we will use `TypedDict`. See [this section](#use-pydantic-models-for-graph-state) for detail on using Pydantic.

By default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas.

Let's consider a simple example using [messages](/oss/python/langgraph/graph-api#messagesstate). This represents a versatile formulation of state for many LLM applications. See our [concepts page](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) for more detail.

This state tracks a list of [message](https://python.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field.

Let's build an example graph with a single node. Our [node](/oss/python/langgraph/graph-api#nodes) is just a Python function that reads our graph's state and makes updates to it. The first argument to this function will always be the state:

This node simply appends a message to our message list, and populates an extra field.

<Warning>
  Nodes should return updates to the state directly, instead of mutating the state.
</Warning>

Let's next define a simple graph containing this node. We use [`StateGraph`](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state. We then use [`add_node`](/oss/python/langgraph/graph-api#nodes) populate our graph.

LangGraph provides built-in utilities for visualizing your graph. Let's inspect our graph. See [this section](#visualize-your-graph) for detail on visualization.

<img src="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=cf3d978b707847e166d5ed15bc7cbbe4" alt="Simple graph with single node" data-og-width="107" width="107" data-og-height="134" height="134" data-path="oss/images/graph_api_image_1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=498bbdb0192eb26ab115d51b53fcb64c 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=94cbad4b92d5b887dff2bfbb6f8e0c6c 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d90d58640d49e3fd4e558ab56acf4817 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=cad59990b0c551a2aa96b684b102b953 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=318736f22c69f66c48f4189db3e39235 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/graph_api_image_1.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=6740141ec001a9a4275cecfac67b9c55 2500w" />

In this case, our graph just executes a single node. Let's proceed with a simple invocation:

* We kicked off invocation by updating a single key of the state.
* We receive the entire state in the invocation result.

For convenience, we frequently inspect the content of [message objects](https://python.langchain.com/docs/concepts/messages/) via pretty-print:

### Process state updates with reducers

Each key in the state can have its own independent [reducer](/oss/python/langgraph/graph-api#reducers) function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.

For `TypedDict` state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.

In the earlier example, our node updated the `"messages"` key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:

Now our node can be simplified:

In practice, there are additional considerations for updating lists of messages:

* We may wish to update an existing message in the state.
* We may want to accept short-hands for [message formats](/oss/python/langgraph/graph-api#using-messages-in-your-graph), such as [OpenAI format](https://python.langchain.com/docs/concepts/messages/#openai-format).

LangGraph includes a built-in reducer [`add_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add_messages) that handles these considerations:

This is a versatile representation of state for applications involving [chat models](https://python.langchain.com/docs/concepts/chat_models/). LangGraph includes a pre-built `MessagesState` for convenience, so that we can have:

### Bypass reducers with `Overwrite`

In some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the [`Overwrite`](https://reference.langchain.com/python/langgraph/types/) type for this purpose. When a node returns a value wrapped with `Overwrite`, the reducer is bypassed and the channel is set directly to that value.

This is useful when you want to reset or replace accumulated state rather than merge it with existing values.

You can also use JSON format with the special key `"__overwrite__"`:

<Warning>
  When nodes execute in parallel, only one node can use `Overwrite` on the same state key in a given super-step. If multiple nodes attempt to overwrite the same key in the same super-step, an `InvalidUpdateError` will be raised.
</Warning>

### Define input and output schemas

By default, `StateGraph` operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph.

When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.

Below, we'll see how to define distinct input and output schema.

```python  theme={null}
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<Tip>
  **Set up LangSmith for better debugging**

  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the [docs](/langsmith/observability).
</Tip>

## Define and update state

Here we show how to define and update [state](/oss/python/langgraph/graph-api#state) in LangGraph. We will demonstrate:

1. How to use state to define a graph's [schema](/oss/python/langgraph/graph-api#schema)
2. How to use [reducers](/oss/python/langgraph/graph-api#reducers) to control how state updates are processed.

### Define state

[State](/oss/python/langgraph/graph-api#state) in LangGraph can be a `TypedDict`, `Pydantic` model, or dataclass. Below we will use `TypedDict`. See [this section](#use-pydantic-models-for-graph-state) for detail on using Pydantic.

By default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas.

Let's consider a simple example using [messages](/oss/python/langgraph/graph-api#messagesstate). This represents a versatile formulation of state for many LLM applications. See our [concepts page](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) for more detail.
```

Example 3 (unknown):
```unknown
This state tracks a list of [message](https://python.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field.

### Update state

Let's build an example graph with a single node. Our [node](/oss/python/langgraph/graph-api#nodes) is just a Python function that reads our graph's state and makes updates to it. The first argument to this function will always be the state:
```

Example 4 (unknown):
```unknown
This node simply appends a message to our message list, and populates an extra field.

<Warning>
  Nodes should return updates to the state directly, instead of mutating the state.
</Warning>

Let's next define a simple graph containing this node. We use [`StateGraph`](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state. We then use [`add_node`](/oss/python/langgraph/graph-api#nodes) populate our graph.
```

---

## Use the messages in your workflow

**URL:** llms-txt#use-the-messages-in-your-workflow

**Contents:**
- Advanced features
  - Tool Interceptors

for message in messages:
    print(f"{message.type}: {message.content}")
python  theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.prompts import load_mcp_prompt

client = MultiServerMCPClient({...})

async with client.session("server_name") as session:
    # Load a prompt by name
    messages = await load_mcp_prompt(session, "summarize")

# Load a prompt with arguments
    messages = await load_mcp_prompt(
        session,
        "code_review",
        arguments={"language": "python", "focus": "security"}
    )
python Inject user context into MCP tool calls theme={null}
    from dataclasses import dataclass
    from langchain_mcp_adapters.client import MultiServerMCPClient
    from langchain_mcp_adapters.interceptors import MCPToolCallRequest
    from langchain.agents import create_agent

@dataclass
    class Context:
        user_id: str
        api_key: str

async def inject_user_context(
        request: MCPToolCallRequest,
        handler,
    ):
        """Inject user credentials into MCP tool calls."""
        runtime = request.runtime
        user_id = runtime.context.user_id  # [!code highlight]
        api_key = runtime.context.api_key  # [!code highlight]

# Add user context to tool arguments
        modified_request = request.override(
            args={**request.args, "user_id": user_id}
        )
        return await handler(modified_request)

client = MultiServerMCPClient(
        {...},
        tool_interceptors=[inject_user_context],
    )
    tools = await client.get_tools()
    agent = create_agent("gpt-4o", tools, context_schema=Context)

# Invoke with user context
    result = await agent.ainvoke(
        {"messages": [{"role": "user", "content": "Search my orders"}]},
        context={"user_id": "user_123", "api_key": "sk-..."}
    )
    python Access user preferences from store theme={null}
    from dataclasses import dataclass
    from langchain_mcp_adapters.client import MultiServerMCPClient
    from langchain_mcp_adapters.interceptors import MCPToolCallRequest
    from langchain.agents import create_agent
    from langgraph.store.memory import InMemoryStore

@dataclass
    class Context:
        user_id: str

async def personalize_search(
        request: MCPToolCallRequest,
        handler,
    ):
        """Personalize MCP tool calls using stored preferences."""
        runtime = request.runtime
        user_id = runtime.context.user_id
        store = runtime.store  # [!code highlight]

# Read user preferences from store
        prefs = store.get(("preferences",), user_id)  # [!code highlight]

if prefs and request.name == "search":
            # Apply user's preferred language and result limit
            modified_args = {
                **request.args,
                "language": prefs.value.get("language", "en"),
                "limit": prefs.value.get("result_limit", 10),
            }
            request = request.override(args=modified_args)

return await handler(request)

client = MultiServerMCPClient(
        {...},
        tool_interceptors=[personalize_search],
    )
    tools = await client.get_tools()
    agent = create_agent(
        "gpt-4o",
        tools,
        context_schema=Context,
        store=InMemoryStore()
    )
    python Filter tools based on authentication state theme={null}
    from langchain_mcp_adapters.client import MultiServerMCPClient
    from langchain_mcp_adapters.interceptors import MCPToolCallRequest
    from langchain.messages import ToolMessage

async def require_authentication(
        request: MCPToolCallRequest,
        handler,
    ):
        """Block sensitive MCP tools if user is not authenticated."""
        runtime = request.runtime
        state = runtime.state  # [!code highlight]
        is_authenticated = state.get("authenticated", False)  # [!code highlight]

sensitive_tools = ["delete_file", "update_settings", "export_data"]

if request.name in sensitive_tools and not is_authenticated:
            # Return error instead of calling tool
            return ToolMessage(
                content="Authentication required. Please log in first.",
                tool_call_id=runtime.tool_call_id,
            )

return await handler(request)

client = MultiServerMCPClient(
        {...},
        tool_interceptors=[require_authentication],
    )
    python Return custom responses with tool call ID theme={null}
    from langchain_mcp_adapters.client import MultiServerMCPClient
    from langchain_mcp_adapters.interceptors import MCPToolCallRequest
    from langchain.messages import ToolMessage

async def rate_limit_interceptor(
        request: MCPToolCallRequest,
        handler,
    ):
        """Rate limit expensive MCP tool calls."""
        runtime = request.runtime
        tool_call_id = runtime.tool_call_id  # [!code highlight]

# Check rate limit (simplified example)
        if is_rate_limited(request.name):
            return ToolMessage(
                content="Rate limit exceeded. Please try again later.",
                tool_call_id=tool_call_id,  # [!code highlight]
            )

result = await handler(request)

# Log successful tool call
        log_tool_execution(tool_call_id, request.name, success=True)

client = MultiServerMCPClient(
        {...},
        tool_interceptors=[rate_limit_interceptor],
    )
    python Mark task complete and switch agents theme={null}
from langchain.agents import AgentState, create_agent
from langchain_mcp_adapters.interceptors import MCPToolCallRequest
from langchain.messages import ToolMessage
from langgraph.types import Command

async def handle_task_completion(
    request: MCPToolCallRequest,
    handler,
):
    """Mark task complete and hand off to summary agent."""
    result = await handler(request)

if request.name == "submit_order":
        return Command(
            update={
                "messages": [result] if isinstance(result, ToolMessage) else [],
                "task_status": "completed",  # [!code highlight]
            },
            goto="summary_agent",  # [!code highlight]
        )

return result
python End agent run on completion theme={null}
async def end_on_success(
    request: MCPToolCallRequest,
    handler,
):
    """End agent run when task is marked complete."""
    result = await handler(request)

if request.name == "mark_complete":
        return Command(
            update={"messages": [result], "status": "done"},
            goto="__end__",  # [!code highlight]
        )

return result
python Basic interceptor pattern theme={null}
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.interceptors import MCPToolCallRequest

async def logging_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Log tool calls before and after execution."""
    print(f"Calling tool: {request.name} with args: {request.args}")
    result = await handler(request)
    print(f"Tool {request.name} returned: {result}")
    return result

client = MultiServerMCPClient(
    {"math": {"transport": "stdio", "command": "python", "args": ["/path/to/server.py"]}},
    tool_interceptors=[logging_interceptor],  # [!code highlight]
)
python Modifying tool arguments theme={null}
async def double_args_interceptor(
    request: MCPToolCallRequest,
    handler,
):
    """Double all numeric arguments before execution."""
    modified_args = {k: v * 2 for k, v in request.args.items()}
    modified_request = request.override(args=modified_args)  # [!code highlight]
    return await handler(modified_request)

**Examples:**

Example 1 (unknown):
```unknown
You can also use [`load_mcp_prompt`](/docs/reference/langchain-mcp-adapters#load_mcp_prompt) directly with a session for more control:
```

Example 2 (unknown):
```unknown
## Advanced features

### Tool Interceptors

MCP servers run as separate processes—they can't access LangGraph runtime information like the [store](/oss/python/langgraph/persistence#memory-store), [context](/oss/python/langchain/context-engineering), or agent state. **Interceptors bridge this gap** by giving you access to this runtime context during MCP tool execution.

Interceptors also provide middleware-like control over tool calls: you can modify requests, implement retries, add headers dynamically, or short-circuit execution entirely.

| Section                                                   | Description                                                                 |
| --------------------------------------------------------- | --------------------------------------------------------------------------- |
| [Accessing runtime context](#accessing-runtime-context)   | Read user IDs, API keys, store data, and agent state                        |
| [State updates and commands](#state-updates-and-commands) | Update agent state or control graph flow with `Command`                     |
| [Writing interceptors](#writing-interceptors)             | Patterns for modifying requests, composing interceptors, and error handling |

#### Accessing runtime context

When MCP tools are used within a LangChain agent (via `create_agent`), interceptors receive access to the `ToolRuntime` context. This provides access to the tool call ID, state, config, and store—enabling powerful patterns for accessing user data, persisting information, and controlling agent behavior.

<Tabs>
  <Tab title="Runtime context">
    Access user-specific configuration like user IDs, API keys, or permissions that are passed at invocation time:
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Store">
    Access long-term memory to retrieve user preferences or persist data across conversations:
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="State">
    Access conversation state to make decisions based on the current session:
```

---

## Use threads

**URL:** llms-txt#use-threads

**Contents:**
- Understand threads
- Create a thread
- List threads
- Inspect threads

Source: https://docs.langchain.com/langsmith/use-threads

This guide shows you how to create, view, and inspect *threads*. Threads work with [assistants](/langsmith/assistants) to enable [stateful](/oss/python/langgraph/persistence) execution of your [deployed graphs](/langsmith/deployments).

## Understand threads

A thread is a persistent conversation container that maintains state across multiple runs. Each time you execute a run on a thread, the graph processes the input with the thread's current state and updates that state with new information.

Threads enable stateful interactions by preserving conversation history and context between runs. Without threads, each run would be stateless, with no memory of previous interactions. Threads are particularly useful for:

* Multi-turn conversations where the assistant needs to remember what was discussed.
* Long-running tasks that require maintaining context across multiple steps.
* User-specific state management where each user has their own conversation history.

The diagram illustrates how a thread maintains state across two runs. The second run has access to the messages from the first run, allowing the assistant to understand that the context of "What about tomorrow?" refers to the weather query from the first run:

* A thread maintains a persistent conversation with a unique thread ID.
* Each run applies the assistant's configuration to the graph execution.
* State is updated after each run and persists for subsequent runs.
* Later runs have access to the full conversation history.

<Note>
  - **[Assistants](/langsmith/assistants)** define the configuration (model, prompts, tools) for how your graph executes. When creating a run, you can specify either a **graph ID** (e.g., `"agent"`) to use the default assistant, or an **assistant ID** (UUID) to use a specific configuration.
  - **Threads** maintain the state and conversation history.
  - **Runs** combine an assistant and thread to execute your graph with a specific configuration and state.
</Note>

To run your graph with state persistence, you must first create a thread:

<Tabs>
  <Tab title="SDK">
    ### Empty thread

To create a new thread, use one of:

For more information, refer to the [Python](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.create) and [JS](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.create) SDK docs, or the [REST API](/langsmith/agent-server-api/threads/create-thread) reference.

Alternatively, if you already have a thread in your application whose state you wish to copy, you can use the `copy` method. This will create an independent thread whose history is identical to the original thread at the time of the operation:

For more information, refer to the [Python](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.copy) and [JS](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.copy) SDK docs, or the [REST API](/langsmith/agent-server-api/threads/copy-thread) reference.

### Prepopulated State

You can create a thread with an arbitrary pre-defined state by providing a list of `supersteps` into the `create` method. The `supersteps` describe a sequence of state updates that establish the initial state of the thread. This is useful when you want to:

* Create a thread with existing conversation history.
    * Migrate conversations from another system.
    * Set up test scenarios with specific initial states.
    * Resume conversations from a previous session.

For more information on checkpoints and state management, refer to the [LangGraph persistence documentation](/oss/python/langgraph/persistence).

<Tab title="UI">
    You can also create threads directly from the [LangSmith UI](https://smith.langchain.com):

1. Navigate to your [deployment](/langsmith/deployments).
    2. Select the **Threads** tab.
    3. Click **+ New thread**.
    4. Optionally provide metadata or initial state for the thread.
    5. Click **Create thread**.

The newly created thread will appear in the threads table and can be used for runs immediately.
  </Tab>
</Tabs>

<Tabs>
  <Tab title="SDK">
    To list threads, use the `search` method. This will list the threads in the application that match the provided filters:

### Filter by thread status

Use the `status` field to filter threads based on their status. Supported values are `idle`, `busy`, `interrupted`, and `error`. For example, to view `idle` threads:

For more information, refer to the [Python](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.search) and [JS](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.search) SDK docs, or the [REST API](/langsmith/agent-server-api/threads/search-threads) reference.

### Filter by metadata

The `search` method allows you to filter on metadata. This is useful for finding threads associated with specific graphs, users, or custom metadata you've added to threads:

The SDK also supports sorting threads by `thread_id`, `status`, `created_at`, and `updated_at` using the `sort_by` and `sort_order` parameters.
  </Tab>

<Tab title="UI">
    You can also view and manage threads in a deployment via the [LangSmith UI](https://smith.langchain.com):

1. Navigate to your [deployment](/langsmith/deployments).
    2. Select the **Threads** tab.

This will load a table of all threads in your deployment.

**Filter by thread status:** Select a status in the top bar to filter threads by `idle`, `busy`, `interrupted`, or `error`.

**Sort threads:** Click on the arrow icon for any column header to sort by that property (`thread_id`, `status`, `created_at`, or `updated_at`).
  </Tab>
</Tabs>

<Tabs>
  <Tab title="SDK">
    ### Get Thread

To view a specific thread given its `thread_id`, use the [`get`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get) method:

For more information, refer to the [Python](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get) and [JS](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get) SDK docs, or the [REST API](/langsmith/agent-server-api/threads/get-thread) reference.

### Inspect thread state

To view the current state of a given thread, use the [`get_state`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_state) method. This returns the current values, next nodes to execute, and checkpoint information:

For more information, refer to the [Python](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_state) and [JS](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_state) SDK docs, or the [REST API](/langsmith/agent-server-api/threads/get-thread-state) reference.

Optionally, to view the state of a thread at a given checkpoint, pass in the checkpoint ID. This is useful for inspecting the thread state at a specific point in its execution history.

First, get the checkpoint ID from the thread's history:

Then use the checkpoint ID to get the state at that specific point:

### Inspect full thread history

To view a thread's history, use the [`get_history`](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_history) method. This returns a list of every state the thread experienced, allowing you to trace the full execution path:

This method is particularly useful for:

* Debugging execution flow by seeing how state evolved.
    * Understanding decision points in your graph's execution.
    * Auditing conversation history and state changes.
    * Replaying or analyzing past interactions.

For more information, refer to the [Python](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_history) and [JS](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.get_history) SDK docs, or the [REST API](/langsmith/agent-server-api/threads/get-thread-history) reference.
  </Tab>

<Tab title="UI">
    You can also view and inspect threads in the [LangSmith UI](https://smith.langchain.com):

1. Navigate to your [deployment](/langsmith/deployments).
    2. Select the **Threads** tab to view all threads.
    3. Click on a thread to inspect its current state.

To view the full thread history and perform detailed debugging, click **Open in Studio** to open the thread in [Studio](/langsmith/studio). Studio provides a visual interface for exploring the thread's execution history, state changes, and checkpoint details.
  </Tab>
</Tabs>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-threads.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
* A thread maintains a persistent conversation with a unique thread ID.
* Each run applies the assistant's configuration to the graph execution.
* State is updated after each run and persists for subsequent runs.
* Later runs have access to the full conversation history.

<Note>
  - **[Assistants](/langsmith/assistants)** define the configuration (model, prompts, tools) for how your graph executes. When creating a run, you can specify either a **graph ID** (e.g., `"agent"`) to use the default assistant, or an **assistant ID** (UUID) to use a specific configuration.
  - **Threads** maintain the state and conversation history.
  - **Runs** combine an assistant and thread to execute your graph with a specific configuration and state.
</Note>

## Create a thread

To run your graph with state persistence, you must first create a thread:

<Tabs>
  <Tab title="SDK">
    ### Empty thread

    To create a new thread, use one of:

    <CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

    For more information, refer to the [Python](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.create) and [JS](https://reference.langchain.com/python/langsmith/deployment/sdk/#langgraph_sdk.client.ThreadsClient.create) SDK docs, or the [REST API](/langsmith/agent-server-api/threads/create-thread) reference.

    Output:
```

---

## Use tools in a prompt

**URL:** llms-txt#use-tools-in-a-prompt

**Contents:**
- When to use tools
- Built-in tools
  - OpenAI Tools
  - Anthropic Tools
- Adding and using tools
  - Add a tool
  - Use a built-in tool
  - Create a custom tool
- Tool choice settings

Source: https://docs.langchain.com/langsmith/use-tools

Tools allow language models to interact with external systems and perform actions beyond just generating text. In the LangSmith playground, you can use two types of tools:

1. **Built-in tools**: Pre-configured tools provided by model providers (like OpenAI and Anthropic) that are ready to use. These include capabilities like web search, code interpretation, and more.

2. **Custom tools**: Functions you define to perform specific tasks. These are useful when you need to integrate with your own systems or create specialized functionality. When you define custom tools within the LangSmith Playground, you can verify that the model correctly identifies and calls these tools with the correct arguments. Soon we plan to support executing these custom tool calls directly.

* Use **built-in tools** when you need common capabilities like web search or code interpretation. These are built and maintained by the model providers.

* Use **custom tools** when you want to test and validate your own tool designs, including:

* Validating which tools the model chooses to use and seeing the specific arguments it provides in tool calls
  * Simulating tool interactions

The LangSmith Playground has native support for a variety of tools from OpenAI and Anthropic. If you want to use a tool that isn't explicitly listed in the Playground, you can still add it by manually specifying its `type` and any required arguments.

* **Web search**: [Search the web for real-time information](https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses)
* **Image generation**: [Generate images based on a text prompt](https://platform.openai.com/docs/guides/tools-image-generation)
* **MCP**: [Gives the model access to tools hosted on a remote MCP server](https://platform.openai.com/docs/guides/tools-remote-mcp)
* [View all OpenAI tools](https://platform.openai.com/docs/guides/tools?api-mode=responses)

* **Web search**: [Search the web for up-to-date information](https://platform.claude.com/docs/en/agents-and-tools/tool-use/web-search-tool)
* [View all Anthropic tools](https://platform.claude.com/docs/en/agents-and-tools/tool-use/overview)

## Adding and using tools

To add a tool to your prompt, click the `+ Tool` button at the bottom of the prompt editor. <img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=b922d9b219cff1bbc726bc2f6b82d6b6" alt="Add tool" data-og-width="753" width="753" data-og-height="351" height="351" data-path="langsmith/images/add-tool.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d08cf08da323465b8340f5395f93cb94 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=c54feb8a827ef4d7858061f252200fc1 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=59fbdea1f87fc501812417a5457ff249 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=f850e2a4dacb21cba0939bd017ce2106 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5b40ff17dd48d154c83b5063bab9f673 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/add-tool.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=6c48a7e9d6c921f206fc9ff777d94ed6 2500w" />

### Use a built-in tool

1. In the tool section, select the built-in tool you want to use. You'll only see the tools that are compatible with the provider and model you've chosen.
2. When the model calls the tool, the playground will display the response

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/web-search-tool.gif?s=2fb882f785abc26d0e5412557cc982ca" alt="Web search tool" data-og-width="1036" width="1036" data-og-height="720" height="720" data-path="langsmith/images/web-search-tool.gif" data-optimize="true" data-opv="3" />

### Create a custom tool

To create a custom tool, you'll need to provide:

* Name: A descriptive name for your tool
* Description: Clear explanation of what the tool does
* Arguments: The inputs your tool requires

<img src="https://mintcdn.com/langchain-5e9cc07a/aKRoUGXX6ygp4DlC/langsmith/images/custom-tool.gif?s=69638e515cd7c5be413cf13348e10974" alt="Custom tool" data-og-width="1028" width="1028" data-og-height="720" height="720" data-path="langsmith/images/custom-tool.gif" data-optimize="true" data-opv="3" />

Note: When running a custom tool in the playground, the model will respond with a JSON object containing the tool name and the tool call. Currently, there's no way to connect this to a hosted tool via MCP.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=44458ff5a2790122ffd7e8b62bf14032" alt="Tool call" data-og-width="1488" width="1488" data-og-height="747" height="747" data-path="langsmith/images/tool-call.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=add552eba8c2254b2b8631420e8a92ee 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=807f9c730d6386d884a8b263d07de8c3 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=6190625854497044bea5a486f155c1ab 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=eeee3567477764d44857923ff49d4d75 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=ec4aaf0657a0b200d8822b346762d443 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-call.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=1c6b56541dd156fdc5c06f3d9fb79648 2500w" />

## Tool choice settings

Some models provide control over which tools are called. To configure this:

1. Go to prompt settings
2. Navigate to tool settings
3. Select tool choice

To understand the available tool choice options, check the documentation for your specific provider. For example, [OpenAI's documentation on tool choice](https://platform.openai.com/docs/guides/function-calling/function-calling-behavior?api-mode=responses#tool-choice).

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=8685dbafdab37ed9529f8dbeceab72e6" alt="Tool choice" data-og-width="942" width="942" data-og-height="867" height="867" data-path="langsmith/images/tool-choice.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=280&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=4d040577bbfae3816d23de7d16b2017d 280w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=560&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=e8b1220627a1dd668a6c7e66bb8881b2 560w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=840&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=f99a56b4e513d33c4595b9665e5a3d0e 840w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=1100&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=c62de52814917a4d6f65a05c9b44a670 1100w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=1650&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=6d4ff53eb7fa66b94c24e285dc89ba5f 1650w, https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/tool-choice.png?w=2500&fit=max&auto=format&n=ImHGLQW1HnQYwnJV&q=85&s=591da66f1402ffa38645cec70ba12e0a 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-tools.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Use webhooks

**URL:** llms-txt#use-webhooks

**Contents:**
- Supported endpoints
- Set up your assistant and thread
- Use a webhook with a graph run
- Webhook payload
- Secure webhooks
- Disable webhooks
- Test webhooks

Source: https://docs.langchain.com/langsmith/use-webhooks

Webhooks enable event-driven communication from your LangSmith application to external services. For example, you may want to issue an update to a separate service once an API call to LangSmith has finished running.

Many LangSmith endpoints accept a `webhook` parameter. If this parameter is specified by an endpoint that can accept POST requests, LangSmith will send a request at the completion of a run.

When working with LangSmith, you may want to use webhooks to receive updates after an API call completes. Webhooks are useful for triggering actions in your service once a run has finished processing. To implement this, you need to expose an endpoint that can accept `POST` requests and pass this endpoint as a `webhook` parameter in your API request.

Currently, the SDK does not provide built-in support for defining webhook endpoints, but you can specify them manually using API requests.

## Supported endpoints

The following API endpoints accept a `webhook` parameter:

| Operation            | HTTP Method | Endpoint                          |
| -------------------- | ----------- | --------------------------------- |
| Create Run           | `POST`      | `/thread/{thread_id}/runs`        |
| Create Thread Cron   | `POST`      | `/thread/{thread_id}/runs/crons`  |
| Stream Run           | `POST`      | `/thread/{thread_id}/runs/stream` |
| Wait Run             | `POST`      | `/thread/{thread_id}/runs/wait`   |
| Create Cron          | `POST`      | `/runs/crons`                     |
| Stream Run Stateless | `POST`      | `/runs/stream`                    |
| Wait Run Stateless   | `POST`      | `/runs/wait`                      |

In this guide, we’ll show how to trigger a webhook after streaming a run.

## Set up your assistant and thread

Before making API calls, set up your assistant and thread.

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

## Use a webhook with a graph run

To use a webhook, specify the `webhook` parameter in your API request. When the run completes, LangSmith sends a `POST` request to the specified webhook URL.

For example, if your server listens for webhook events at `https://my-server.app/my-webhook-endpoint`, include this in your request:

<Tabs>
  <Tab title="Python">
    
  </Tab>

<Tab title="JavaScript">
    
  </Tab>

<Tab title="CURL">
    
  </Tab>
</Tabs>

LangSmith sends webhook notifications in the format of a [Run](/langsmith/assistants#execution). See the [API Reference](https://langchain-ai.github.io/langgraph/cloud/reference/api/api_ref.html#tag/assistants) for details. The request payload includes run input, configuration, and other metadata in the `kwargs` field.

To ensure only authorized requests hit your webhook endpoint, consider adding a security token as a query parameter:

Your server should extract and validate this token before processing requests.

As of `langgraph-api>=0.2.78`, developers can disable webhooks in the `langgraph.json` file:

This feature is primarily intended for self-hosted deployments, where platform administrators or developers may prefer to disable webhooks to simplify their security posture—especially if they are not configuring firewall rules or other network controls. Disabling webhooks helps prevent untrusted payloads from being sent to internal endpoints.

For full configuration details, refer to the [configuration file reference](/langsmith/cli?h=disable_webhooks#configuration-file).

You can test your webhook using online services like:

* **[Beeceptor](https://beeceptor.com/)** – Quickly create a test endpoint and inspect incoming webhook payloads.
* **[Webhook.site](https://webhook.site/)** – View, debug, and log incoming webhook requests in real time.

These tools help you verify that LangSmith is correctly triggering and sending webhooks to your service.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/use-webhooks.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="JavaScript">
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="CURL">
```

Example 3 (unknown):
```unknown
</Tab>
</Tabs>

Example response:
```

Example 4 (unknown):
```unknown
## Use a webhook with a graph run

To use a webhook, specify the `webhook` parameter in your API request. When the run completes, LangSmith sends a `POST` request to the specified webhook URL.

For example, if your server listens for webhook events at `https://my-server.app/my-webhook-endpoint`, include this in your request:

<Tabs>
  <Tab title="Python">
```

---

## Use with chat models

**URL:** llms-txt#use-with-chat-models

**Contents:**
  - Text prompts
  - Message prompts
  - Dictionary format
- Message types
  - System Message
  - Human Message
  - AI Message

messages = [system_msg, human_msg]
response = model.invoke(messages)  # Returns AIMessage
python  theme={null}
response = model.invoke("Write a haiku about spring")
python  theme={null}
from langchain.messages import SystemMessage, HumanMessage, AIMessage

messages = [
    SystemMessage("You are a poetry expert"),
    HumanMessage("Write a haiku about spring"),
    AIMessage("Cherry blossoms bloom...")
]
response = model.invoke(messages)
python  theme={null}
messages = [
    {"role": "system", "content": "You are a poetry expert"},
    {"role": "user", "content": "Write a haiku about spring"},
    {"role": "assistant", "content": "Cherry blossoms bloom..."}
]
response = model.invoke(messages)
python Basic instructions theme={null}
system_msg = SystemMessage("You are a helpful coding assistant.")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
response = model.invoke(messages)
python Detailed persona theme={null}
from langchain.messages import SystemMessage, HumanMessage

system_msg = SystemMessage("""
You are a senior Python developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
""")

messages = [
    system_msg,
    HumanMessage("How do I create a REST API?")
]
response = model.invoke(messages)
python Message object theme={null}
  response = model.invoke([
    HumanMessage("What is machine learning?")
  ])
  python String shortcut theme={null}
  # Using a string is a shortcut for a single HumanMessage
  response = model.invoke("What is machine learning?")
  python Add metadata theme={null}
human_msg = HumanMessage(
    content="Hello!",
    name="alice",  # Optional: identify different users
    id="msg_123",  # Optional: unique identifier for tracing
)
python  theme={null}
response = model.invoke("Explain AI")
print(type(response))  # <class 'langchain.messages.AIMessage'>
python  theme={null}
from langchain.messages import AIMessage, SystemMessage, HumanMessage

**Examples:**

Example 1 (unknown):
```unknown
### Text prompts

Text prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history.
```

Example 2 (unknown):
```unknown
**Use text prompts when:**

* You have a single, standalone request
* You don't need conversation history
* You want minimal code complexity

### Message prompts

Alternatively, you can pass in a list of messages to the model by providing a list of message objects.
```

Example 3 (unknown):
```unknown
**Use message prompts when:**

* Managing multi-turn conversations
* Working with multimodal content (images, audio, files)
* Including system instructions

### Dictionary format

You can also specify messages directly in OpenAI chat completions format.
```

Example 4 (unknown):
```unknown
## Message types

* <Icon icon="gear" size={16} /> [System message](#system-message) - Tells the model how to behave and provide context for interactions
* <Icon icon="user" size={16} /> [Human message](#human-message) - Represents user input and interactions with the model
* <Icon icon="robot" size={16} /> [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata
* <Icon icon="wrench" size={16} /> [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/python/langchain/models#tool-calling)

### System Message

A [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses.
```

---

## Using standard tests

**URL:** llms-txt#using-standard-tests

**Contents:**
- Setup
- Implementing standard tests

Source: https://docs.langchain.com/oss/python/contributing/standard-tests-langchain

**Standard tests ensure your integration works as expected.**

When creating either a custom class for yourself or to publish in a LangChain integration, it is necessary to add tests to ensure it works as expected. LangChain provides a comprehensive [set of tests](https://pypi.org/project/langchain-tests/) for each integration type for you. This guide will show you how to add LangChain's standard test suite to each integration type.

First, install the required dependencies:

<CardGroup cols={2}>
  <Card title="langchain-core" icon="cube" href="https://github.com/langchain-ai/langchain/tree/master/libs/core#readme" arrow>
    Defines the interfaces we want to import to define our custom components
  </Card>

<Card title="langchain-tests" icon="flask" href="https://github.com/langchain-ai/langchain/tree/master/libs/standard-tests#readme" arrow>
    Provides the standard tests and `pytest` plugins necessary to run them
  </Card>
</CardGroup>

<Warning>
  Because added tests in new versions of `langchain-tests` can break your CI/CD pipelines, we recommend pinning to the latest version of [`langchain-tests`](https://pypi.org/project/langchain-tests/#history) to avoid unexpected changes.
</Warning>

There are 2 namespaces in the `langchain-tests` package:

<AccordionGroup>
  <Accordion title="Unit tests" icon="gear">
    **Location**: `langchain_tests.unit_tests`

Designed to test the component in isolation and without access to external services

[View API reference](https://reference.langchain.com/python/langchain_tests/unit_tests)
  </Accordion>

<Accordion title="Integration tests" icon="network-wired">
    **Location**: `langchain_tests.integration_tests`

Designed to test the component with access to external services (in particular, the external service that the component is designed to interact with)

[View API reference](https://reference.langchain.com/python/langchain_tests/integration_tests)
  </Accordion>
</AccordionGroup>

Both types of tests are implemented as [`pytest`](https://docs.pytest.org/en/stable/) class-based test suites.

## Implementing standard tests

Depending on your integration type, you will need to implement either or both unit and integration tests.

By subclassing the standard test suite for your integration type, you get the full collection of standard tests for that type. For a test run to be successful, the a given test should pass only if the model supports the capability being tested. Otherwise, the test should be skipped.

Because different integrations offer unique sets of features, most standard tests provided by LangChain are **opt-in by default** to prevent false positives. Consequently, you will need to override properties to indicate which features your integration supports - see the below example for an illustration.

```python tests/integration_tests/test_standard.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

There are 2 namespaces in the `langchain-tests` package:

<AccordionGroup>
  <Accordion title="Unit tests" icon="gear">
    **Location**: `langchain_tests.unit_tests`

    Designed to test the component in isolation and without access to external services

    [View API reference](https://reference.langchain.com/python/langchain_tests/unit_tests)
  </Accordion>

  <Accordion title="Integration tests" icon="network-wired">
    **Location**: `langchain_tests.integration_tests`

    Designed to test the component with access to external services (in particular, the external service that the component is designed to interact with)

    [View API reference](https://reference.langchain.com/python/langchain_tests/integration_tests)
  </Accordion>
</AccordionGroup>

Both types of tests are implemented as [`pytest`](https://docs.pytest.org/en/stable/) class-based test suites.

## Implementing standard tests

Depending on your integration type, you will need to implement either or both unit and integration tests.

By subclassing the standard test suite for your integration type, you get the full collection of standard tests for that type. For a test run to be successful, the a given test should pass only if the model supports the capability being tested. Otherwise, the test should be skipped.

Because different integrations offer unique sets of features, most standard tests provided by LangChain are **opt-in by default** to prevent false positives. Consequently, you will need to override properties to indicate which features your integration supports - see the below example for an illustration.
```

---

## Vector stores

**URL:** llms-txt#vector-stores

**Contents:**
- Overview
  - Interface
  - Initialization
  - Adding documents
  - Deleting documents
  - Similarity search
  - Similarity metrics & indexing
  - Metadata filtering
- Top integrations
- All vector stores

Source: https://docs.langchain.com/oss/python/integrations/vectorstores/index

A vector store stores [embedded](/oss/python/integrations/text_embedding) data and performs similarity search.

LangChain provides a unified interface for vector stores, allowing you to:

* `add_documents` - Add documents to the store.
* `delete` - Remove stored documents by ID.
* `similarity_search` - Query for semantically similar documents.

This abstraction lets you switch between different implementations without altering your application logic.

To initialize a vector store, provide it with an embedding model:

Add [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects (holding `page_content` and optional metadata) like so:

### Deleting documents

Delete by specifying IDs:

### Similarity search

Issue a semantic query using `similarity_search`, which returns the closest embedded documents:

Many vector stores support parameters like:

* `k` — number of results to return
* `filter` — conditional filtering based on metadata

### Similarity metrics & indexing

Embedding similarity may be computed using:

* **Cosine similarity**
* **Euclidean distance**
* **Dot product**

Efficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store.

### Metadata filtering

Filtering by metadata (e.g., source, date) can refine search results:

<important>
  Support for metadata-based filtering varies between implementations.
  Check the documentation of your chosen vector store for details.
</important>

**Select embedding model:**

<AccordionGroup>
  <Accordion title="OpenAI">
    <CodeGroup>

<Accordion title="Azure">

<Accordion title="Google Gemini">

<Accordion title="Google Vertex">

<Accordion title="AWS">

<Accordion title="HuggingFace">

<Accordion title="Ollama">

<Accordion title="Cohere">

<Accordion title="Mistral AI">

<Accordion title="Nomic">

<Accordion title="NVIDIA">

<Accordion title="Voyage AI">

<Accordion title="IBM watsonx">

<Accordion title="Fake">

<Accordion title="xAI">

<Accordion title="Perplexity">

<Accordion title="DeepSeek">

</Accordion>
</AccordionGroup>

**Select vector store:**

<AccordionGroup>
  <Accordion title="In-memory">
    <CodeGroup>

<Accordion title="Amazon OpenSearch">

<Accordion title="Astra DB">
    <CodeGroup>

<Accordion title="Azure Cosmos DB NoSQL">
    <CodeGroup>

<Accordion title="Azure Cosmos DB Mongo vCore">
    <CodeGroup>

<Accordion title="Chroma">
    <CodeGroup>

<Accordion title="FAISS">

<Accordion title="Milvus">
    <CodeGroup>

<Accordion title="MongoDB">

<Accordion title="PGVector">
    <CodeGroup>

<Accordion title="PGVectorStore">
    <CodeGroup>

<Accordion title="Pinecone">
    <CodeGroup>

<Accordion title="Qdrant">
    <CodeGroup>

</Accordion>
</AccordionGroup>

| Vectorstore                                                                                                                                          | Delete by ID | Filtering | Search by Vector | Search with score | Async | Passes Standard Tests | Multi Tenancy | IDs in add Documents |
| ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | --------- | ---------------- | ----------------- | ----- | --------------------- | ------------- | -------------------- |
| [`AstraDBVectorStore`](/oss/python/integrations/vectorstores/astradb)                                                                                | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`AzureCosmosDBNoSqlVectorStore`](/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql)                                                      | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ✅             | ✅                    |
| [`AzureCosmosDBMongoVCoreVectorStore`](/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore)                                            | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ✅             | ✅                    |
| [`Chroma`](/oss/python/integrations/vectorstores/chroma)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`Clickhouse`](/oss/python/integrations/vectorstores/clickhouse)                                                                                     | ✅            | ✅         | ❌                | ✅                 | ❌     | ❌                     | ❌             | ✅                    |
| [`CouchbaseSearchVectorStore`](/oss/python/integrations/vectorstores/couchbase)                                                                      | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |
| [`DatabricksVectorSearch`](/oss/python/integrations/vectorstores/databricks_vector_search)                                                           | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`ElasticsearchStore`](/oss/python/integrations/vectorstores/elasticsearch)                                                                          | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`FAISS`](/oss/python/integrations/vectorstores/faiss)                                                                                               | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`InMemoryVectorStore`](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html) | ✅            | ✅         | ❌                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`Milvus`](/oss/python/integrations/vectorstores/milvus)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`Moorcheh`](/oss/python/integrations/vectorstores/moorcheh)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`MongoDBAtlasVectorSearch`](/oss/python/integrations/vectorstores/mongodb_atlas)                                                                    | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ✅             | ✅                    |
| [`openGauss`](/oss/python/integrations/vectorstores/opengauss)                                                                                       | ✅            | ✅         | ✅                | ✅                 | ❌     | ✅                     | ❌             | ✅                    |
| [`PGVector`](/oss/python/integrations/vectorstores/pgvector)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ❌             | ✅                    |
| [`PGVectorStore`](/oss/python/integrations/vectorstores/pgvectorstore)                                                                               | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ❌             | ✅                    |
| [`PineconeVectorStore`](/oss/python/integrations/vectorstores/pinecone)                                                                              | ✅            | ✅         | ✅                | ❌                 | ✅     | ❌                     | ❌             | ✅                    |
| [`QdrantVectorStore`](/oss/python/integrations/vectorstores/qdrant)                                                                                  | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |
| [`Weaviate`](/oss/python/integrations/vectorstores/weaviate)                                                                                         | ✅            | ✅         | ✅                | ✅                 | ✅     | ❌                     | ✅             | ✅                    |
| [`SQLServer`](/oss/python/integrations/vectorstores/sqlserver)                                                                                       | ✅            | ✅         | ✅                | ✅                 | ❌     | ❌                     | ❌             | ✅                    |
| [`ZeusDB`](/oss/python/integrations/vectorstores/zeusdb)                                                                                             | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ❌             | ✅                    |
| [`Oracle AI Vector Search`](/oss/python/integrations/vectorstores/oracle)                                                                            | ✅            | ✅         | ✅                | ✅                 | ✅     | ✅                     | ❌             | ✅                    |

<Columns cols={3}>
  <Card title="Activeloop Deep Lake" icon="link" href="/oss/python/integrations/vectorstores/activeloop_deeplake" arrow="true" cta="View guide" />

<Card title="Alibaba Cloud OpenSearch" icon="link" href="/oss/python/integrations/vectorstores/alibabacloud_opensearch" arrow="true" cta="View guide" />

<Card title="AnalyticDB" icon="link" href="/oss/python/integrations/vectorstores/analyticdb" arrow="true" cta="View guide" />

<Card title="Annoy" icon="link" href="/oss/python/integrations/vectorstores/annoy" arrow="true" cta="View guide" />

<Card title="Apache Doris" icon="link" href="/oss/python/integrations/vectorstores/apache_doris" arrow="true" cta="View guide" />

<Card title="ApertureDB" icon="link" href="/oss/python/integrations/vectorstores/aperturedb" arrow="true" cta="View guide" />

<Card title="Astra DB Vector Store" icon="link" href="/oss/python/integrations/vectorstores/astradb" arrow="true" cta="View guide" />

<Card title="Atlas" icon="link" href="/oss/python/integrations/vectorstores/atlas" arrow="true" cta="View guide" />

<Card title="AwaDB" icon="link" href="/oss/python/integrations/vectorstores/awadb" arrow="true" cta="View guide" />

<Card title="Azure Cosmos DB Mongo vCore" icon="link" href="/oss/python/integrations/vectorstores/azure_cosmos_db_mongo_vcore" arrow="true" cta="View guide" />

<Card title="Azure Cosmos DB No SQL" icon="link" href="/oss/python/integrations/vectorstores/azure_cosmos_db_no_sql" arrow="true" cta="View guide" />

<Card title="Azure Database for PostgreSQL - Flexible Server" icon="link" href="/oss/python/integrations/vectorstores/azure_db_for_postgresql" arrow="true" cta="View guide" />

<Card title="Azure AI Search" icon="link" href="/oss/python/integrations/vectorstores/azuresearch" arrow="true" cta="View guide" />

<Card title="Bagel" icon="link" href="/oss/python/integrations/vectorstores/bagel" arrow="true" cta="View guide" />

<Card title="BagelDB" icon="link" href="/oss/python/integrations/vectorstores/bageldb" arrow="true" cta="View guide" />

<Card title="Baidu Cloud ElasticSearch VectorSearch" icon="link" href="/oss/python/integrations/vectorstores/baiducloud_vector_search" arrow="true" cta="View guide" />

<Card title="Baidu VectorDB" icon="link" href="/oss/python/integrations/vectorstores/baiduvectordb" arrow="true" cta="View guide" />

<Card title="Apache Cassandra" icon="link" href="/oss/python/integrations/vectorstores/cassandra" arrow="true" cta="View guide" />

<Card title="Chroma" icon="link" href="/oss/python/integrations/vectorstores/chroma" arrow="true" cta="View guide" />

<Card title="Clarifai" icon="link" href="/oss/python/integrations/vectorstores/clarifai" arrow="true" cta="View guide" />

<Card title="ClickHouse" icon="link" href="/oss/python/integrations/vectorstores/clickhouse" arrow="true" cta="View guide" />

<Card title="Couchbase" icon="link" href="/oss/python/integrations/vectorstores/couchbase" arrow="true" cta="View guide" />

<Card title="DashVector" icon="link" href="/oss/python/integrations/vectorstores/dashvector" arrow="true" cta="View guide" />

<Card title="Databricks" icon="link" href="/oss/python/integrations/vectorstores/databricks_vector_search" arrow="true" cta="View guide" />

<Card title="IBM Db2" icon="link" href="/oss/python/integrations/vectorstores/db2" arrow="true" cta="View guide" />

<Card title="DingoDB" icon="link" href="/oss/python/integrations/vectorstores/dingo" arrow="true" cta="View guide" />

<Card title="DocArray HnswSearch" icon="link" href="/oss/python/integrations/vectorstores/docarray_hnsw" arrow="true" cta="View guide" />

<Card title="DocArray InMemorySearch" icon="link" href="/oss/python/integrations/vectorstores/docarray_in_memory" arrow="true" cta="View guide" />

<Card title="Amazon Document DB" icon="link" href="/oss/python/integrations/vectorstores/documentdb" arrow="true" cta="View guide" />

<Card title="DuckDB" icon="link" href="/oss/python/integrations/vectorstores/duckdb" arrow="true" cta="View guide" />

<Card title="China Mobile ECloud ElasticSearch" icon="link" href="/oss/python/integrations/vectorstores/ecloud_vector_search" arrow="true" cta="View guide" />

<Card title="Elasticsearch" icon="link" href="/oss/python/integrations/vectorstores/elasticsearch" arrow="true" cta="View guide" />

<Card title="Epsilla" icon="link" href="/oss/python/integrations/vectorstores/epsilla" arrow="true" cta="View guide" />

<Card title="Faiss" icon="link" href="/oss/python/integrations/vectorstores/faiss" arrow="true" cta="View guide" />

<Card title="Faiss (Async)" icon="link" href="/oss/python/integrations/vectorstores/faiss_async" arrow="true" cta="View guide" />

<Card title="FalkorDB" icon="link" href="/oss/python/integrations/vectorstores/falkordbvector" arrow="true" cta="View guide" />

<Card title="Gel" icon="link" href="/oss/python/integrations/vectorstores/gel" arrow="true" cta="View guide" />

<Card title="Google AlloyDB" icon="link" href="/oss/python/integrations/vectorstores/google_alloydb" arrow="true" cta="View guide" />

<Card title="Google BigQuery Vector Search" icon="link" href="/oss/python/integrations/vectorstores/google_bigquery_vector_search" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for MySQL" icon="link" href="/oss/python/integrations/vectorstores/google_cloud_sql_mysql" arrow="true" cta="View guide" />

<Card title="Google Cloud SQL for PostgreSQL" icon="link" href="/oss/python/integrations/vectorstores/google_cloud_sql_pg" arrow="true" cta="View guide" />

<Card title="Firestore" icon="link" href="/oss/python/integrations/vectorstores/google_firestore" arrow="true" cta="View guide" />

<Card title="Google Memorystore for Redis" icon="link" href="/oss/python/integrations/vectorstores/google_memorystore_redis" arrow="true" cta="View guide" />

<Card title="Google Spanner" icon="link" href="/oss/python/integrations/vectorstores/google_spanner" arrow="true" cta="View guide" />

<Card title="Google Vertex AI Feature Store" icon="link" href="/oss/python/integrations/vectorstores/google_vertex_ai_feature_store" arrow="true" cta="View guide" />

<Card title="Google Vertex AI Vector Search" icon="link" href="/oss/python/integrations/vectorstores/google_vertex_ai_vector_search" arrow="true" cta="View guide" />

<Card title="Hippo" icon="link" href="/oss/python/integrations/vectorstores/hippo" arrow="true" cta="View guide" />

<Card title="Hologres" icon="link" href="/oss/python/integrations/vectorstores/hologres" arrow="true" cta="View guide" />

<Card title="Jaguar Vector Database" icon="link" href="/oss/python/integrations/vectorstores/jaguar" arrow="true" cta="View guide" />

<Card title="Kinetica" icon="link" href="/oss/python/integrations/vectorstores/kinetica" arrow="true" cta="View guide" />

<Card title="LanceDB" icon="link" href="/oss/python/integrations/vectorstores/lancedb" arrow="true" cta="View guide" />

<Card title="Lantern" icon="link" href="/oss/python/integrations/vectorstores/lantern" arrow="true" cta="View guide" />

<Card title="Lindorm" icon="link" href="/oss/python/integrations/vectorstores/lindorm" arrow="true" cta="View guide" />

<Card title="LLMRails" icon="link" href="/oss/python/integrations/vectorstores/llm_rails" arrow="true" cta="View guide" />

<Card title="ManticoreSearch" icon="link" href="/oss/python/integrations/vectorstores/manticore_search" arrow="true" cta="View guide" />

<Card title="MariaDB" icon="link" href="/oss/python/integrations/vectorstores/mariadb" arrow="true" cta="View guide" />

<Card title="Marqo" icon="link" href="/oss/python/integrations/vectorstores/marqo" arrow="true" cta="View guide" />

<Card title="Meilisearch" icon="link" href="/oss/python/integrations/vectorstores/meilisearch" arrow="true" cta="View guide" />

<Card title="Amazon MemoryDB" icon="link" href="/oss/python/integrations/vectorstores/memorydb" arrow="true" cta="View guide" />

<Card title="Milvus" icon="link" href="/oss/python/integrations/vectorstores/milvus" arrow="true" cta="View guide" />

<Card title="Momento Vector Index" icon="link" href="/oss/python/integrations/vectorstores/momento_vector_index" arrow="true" cta="View guide" />

<Card title="Moorcheh" icon="link" href="/oss/python/integrations/vectorstores/moorcheh" arrow="true" cta="View guide" />

<Card title="MongoDB Atlas" icon="link" href="/oss/python/integrations/vectorstores/mongodb_atlas" arrow="true" cta="View guide" />

<Card title="MyScale" icon="link" href="/oss/python/integrations/vectorstores/myscale" arrow="true" cta="View guide" />

<Card title="Neo4j Vector Index" icon="link" href="/oss/python/integrations/vectorstores/neo4jvector" arrow="true" cta="View guide" />

<Card title="NucliaDB" icon="link" href="/oss/python/integrations/vectorstores/nucliadb" arrow="true" cta="View guide" />

<Card title="Oceanbase" icon="link" href="/oss/python/integrations/vectorstores/oceanbase" arrow="true" cta="View guide" />

<Card title="openGauss" icon="link" href="/oss/python/integrations/vectorstores/opengauss" arrow="true" cta="View guide" />

<Card title="OpenSearch" icon="link" href="/oss/python/integrations/vectorstores/opensearch" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/vectorstores/oracle" arrow="true" cta="View guide" />

<Card title="Pathway" icon="link" href="/oss/python/integrations/vectorstores/pathway" arrow="true" cta="View guide" />

<Card title="Postgres Embedding" icon="link" href="/oss/python/integrations/vectorstores/pgembedding" arrow="true" cta="View guide" />

<Card title="PGVecto.rs" icon="link" href="/oss/python/integrations/vectorstores/pgvecto_rs" arrow="true" cta="View guide" />

<Card title="PGVector" icon="link" href="/oss/python/integrations/vectorstores/pgvector" arrow="true" cta="View guide" />

<Card title="PGVectorStore" icon="link" href="/oss/python/integrations/vectorstores/pgvectorstore" arrow="true" cta="View guide" />

<Card title="Pinecone" icon="link" href="/oss/python/integrations/vectorstores/pinecone" arrow="true" cta="View guide" />

<Card title="Pinecone (sparse)" icon="link" href="/oss/python/integrations/vectorstores/pinecone_sparse" arrow="true" cta="View guide" />

<Card title="Qdrant" icon="link" href="/oss/python/integrations/vectorstores/qdrant" arrow="true" cta="View guide" />

<Card title="Relyt" icon="link" href="/oss/python/integrations/vectorstores/relyt" arrow="true" cta="View guide" />

<Card title="Rockset" icon="link" href="/oss/python/integrations/vectorstores/rockset" arrow="true" cta="View guide" />

<Card title="SAP HANA Cloud Vector Engine" icon="link" href="/oss/python/integrations/vectorstores/sap_hanavector" arrow="true" cta="View guide" />

<Card title="ScaNN" icon="link" href="/oss/python/integrations/vectorstores/google_scann" arrow="true" cta="View guide" />

<Card title="SemaDB" icon="link" href="/oss/python/integrations/vectorstores/semadb" arrow="true" cta="View guide" />

<Card title="SingleStore" icon="link" href="/oss/python/integrations/vectorstores/singlestore" arrow="true" cta="View guide" />

<Card title="scikit-learn" icon="link" href="/oss/python/integrations/vectorstores/sklearn" arrow="true" cta="View guide" />

<Card title="SQLiteVec" icon="link" href="/oss/python/integrations/vectorstores/sqlitevec" arrow="true" cta="View guide" />

<Card title="SQLite-VSS" icon="link" href="/oss/python/integrations/vectorstores/sqlitevss" arrow="true" cta="View guide" />

<Card title="SQLServer" icon="link" href="/oss/python/integrations/vectorstores/sqlserver" arrow="true" cta="View guide" />

<Card title="StarRocks" icon="link" href="/oss/python/integrations/vectorstores/starrocks" arrow="true" cta="View guide" />

<Card title="Supabase" icon="link" href="/oss/python/integrations/vectorstores/supabase" arrow="true" cta="View guide" />

<Card title="SurrealDB" icon="link" href="/oss/python/integrations/vectorstores/surrealdb" arrow="true" cta="View guide" />

<Card title="Tablestore" icon="link" href="/oss/python/integrations/vectorstores/tablestore" arrow="true" cta="View guide" />

<Card title="Tair" icon="link" href="/oss/python/integrations/vectorstores/tair" arrow="true" cta="View guide" />

<Card title="Tencent Cloud VectorDB" icon="link" href="/oss/python/integrations/vectorstores/tencentvectordb" arrow="true" cta="View guide" />

<Card title="Teradata VectorStore" icon="link" href="/oss/python/integrations/vectorstores/teradata" arrow="true" cta="View guide" />

<Card title="ThirdAI NeuralDB" icon="link" href="/oss/python/integrations/vectorstores/thirdai_neuraldb" arrow="true" cta="View guide" />

<Card title="TiDB Vector" icon="link" href="/oss/python/integrations/vectorstores/tidb_vector" arrow="true" cta="View guide" />

<Card title="Tigris" icon="link" href="/oss/python/integrations/vectorstores/tigris" arrow="true" cta="View guide" />

<Card title="TileDB" icon="link" href="/oss/python/integrations/vectorstores/tiledb" arrow="true" cta="View guide" />

<Card title="Timescale Vector" icon="link" href="/oss/python/integrations/vectorstores/timescalevector" arrow="true" cta="View guide" />

<Card title="Typesense" icon="link" href="/oss/python/integrations/vectorstores/typesense" arrow="true" cta="View guide" />

<Card title="Upstash Vector" icon="link" href="/oss/python/integrations/vectorstores/upstash" arrow="true" cta="View guide" />

<Card title="USearch" icon="link" href="/oss/python/integrations/vectorstores/usearch" arrow="true" cta="View guide" />

<Card title="Vald" icon="link" href="/oss/python/integrations/vectorstores/vald" arrow="true" cta="View guide" />

<Card title="VDMS" icon="link" href="/oss/python/integrations/vectorstores/vdms" arrow="true" cta="View guide" />

<Card title="Vearch" icon="link" href="/oss/python/integrations/vectorstores/vearch" arrow="true" cta="View guide" />

<Card title="Vectara" icon="link" href="/oss/python/integrations/vectorstores/vectara" arrow="true" cta="View guide" />

<Card title="Vespa" icon="link" href="/oss/python/integrations/vectorstores/vespa" arrow="true" cta="View guide" />

<Card title="viking DB" icon="link" href="/oss/python/integrations/vectorstores/vikingdb" arrow="true" cta="View guide" />

<Card title="vlite" icon="link" href="/oss/python/integrations/vectorstores/vlite" arrow="true" cta="View guide" />

<Card title="Weaviate" icon="link" href="/oss/python/integrations/vectorstores/weaviate" arrow="true" cta="View guide" />

<Card title="Xata" icon="link" href="/oss/python/integrations/vectorstores/xata" arrow="true" cta="View guide" />

<Card title="YDB" icon="link" href="/oss/python/integrations/vectorstores/ydb" arrow="true" cta="View guide" />

<Card title="Yellowbrick" icon="link" href="/oss/python/integrations/vectorstores/yellowbrick" arrow="true" cta="View guide" />

<Card title="Zep" icon="link" href="/oss/python/integrations/vectorstores/zep" arrow="true" cta="View guide" />

<Card title="Zep Cloud" icon="link" href="/oss/python/integrations/vectorstores/zep_cloud" arrow="true" cta="View guide" />

<Card title="ZeusDB" icon="link" href="/oss/python/integrations/vectorstores/zeusdb" arrow="true" cta="View guide" />

<Card title="Zilliz" icon="link" href="/oss/python/integrations/vectorstores/zilliz" arrow="true" cta="View guide" />

<Card title="Oracle AI Vector Search" icon="link" href="/oss/python/integrations/vectorstores/oracle" arrow="true" cta="View guide" />
</Columns>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/vectorstores/index.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Interface

LangChain provides a unified interface for vector stores, allowing you to:

* `add_documents` - Add documents to the store.
* `delete` - Remove stored documents by ID.
* `similarity_search` - Query for semantically similar documents.

This abstraction lets you switch between different implementations without altering your application logic.

### Initialization

To initialize a vector store, provide it with an embedding model:
```

Example 2 (unknown):
```unknown
### Adding documents

Add [`Document`](https://reference.langchain.com/python/langchain_core/documents/#langchain_core.documents.base.Document) objects (holding `page_content` and optional metadata) like so:
```

Example 3 (unknown):
```unknown
### Deleting documents

Delete by specifying IDs:
```

Example 4 (unknown):
```unknown
### Similarity search

Issue a semantic query using `similarity_search`, which returns the closest embedded documents:
```

---

## Verify the variables are set

**URL:** llms-txt#verify-the-variables-are-set

**Contents:**
  - Initial export

echo "Customer ID: $CUSTOMER_ID"
echo "Customer Name: $CUSTOMER_NAME"
bash  theme={null}
curl -s $LANGSMITH_URL/api/v1/info
export CUSTOMER_ID="<id>"
export CUSTOMER_NAME="<name>"
bash  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
You can then use these environment variables in your export scripts or other commands.

If you don't have `jq`, run these commands to set the environment variables based on the curl output:
```

Example 2 (unknown):
```unknown
### Initial export

These scripts export usage data to a CSV for reporting to LangChain. They additionally track the export by assigning a backfill ID and timestamp.

To export LangSmith trace usage:
```

---

## Versioning

**URL:** llms-txt#versioning

**Contents:**
- Version numbering
- API stability
  - Stable APIs
  - Beta APIs
  - Alpha APIs
  - Deprecated APIs
  - Internal APIs
- Release cycles
- Version support policy
  - Long-term support (LTS) releases

Source: https://docs.langchain.com/oss/python/versioning

Each LangChain and LangGraph version number follows the format: `MAJOR.MINOR.PATCH`

* **Major**: Breaking API updates that require code changes.
* **Minor**: New features and improvements that maintain backward compatibility.
* **Patch**: Bug fixes and minor improvements.

LangChain and LangGraph follow [Semantic Versioning](https://semver.org/) principles:

* `1.0.0`: First stable release with production-ready APIs
* `1.1.0`: New features added in a backward-compatible manner
* `1.0.1`: Backward-compatible bug fixes

We communicate the stability of our APIs as follows:

All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases.

APIs marked as `beta` are feature-complete but may undergo minor changes based on user feedback. They are safe for production use but may require small adjustments in future releases.

APIs marked as `alpha` are experimental and subject to significant changes. Use these with caution in production environments.

APIs marked as `deprecated` will be removed in future major releases. When possible, we specify the intended version of removal. To handle deprecations:

1. Switch to the recommended alternative API
2. Follow the migration guide (released alongside major releases)
3. Use automated migration tools when available

Certain APIs are explicitly marked as "internal" in a couple of ways:

* Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.
* Functions, methods, and other objects prefixed by a leading underscore (**`_`**). This is the standard Python convention of indicating that something is private; if any method starts with a single **`_`**, it's an internal API.
  * **Exception:** Certain methods are prefixed with `_` , but do not contain an implementation. These methods are *meant* to be overridden by sub-classes that provide the implementation. Such methods are generally part of the **Public API** of LangChain.

<AccordionGroup>
  <Accordion title="Major releases">
    Major releases (e.g., `1.0.0` → `2.0.0`) may include:

* Breaking API changes
    * Removal of deprecated features
    * Significant architectural improvements

* Detailed migration guides
    * Automated migration tools when possible
    * Extended support period for the previous major version
  </Accordion>

<Accordion title="Minor releases">
    Minor releases (e.g., `1.0.0` → `1.1.0`) include:

* New features and capabilities
    * Performance improvements
    * New optional parameters
    * Backward-compatible enhancements
  </Accordion>

<Accordion title="Patch releases">
    Patch releases (e.g., `1.0.0` → `1.0.1`) include:

* Bug fixes
    * Security updates
    * Documentation improvements
    * Performance optimizations without API changes
  </Accordion>
</AccordionGroup>

## Version support policy

* **Latest major version**: Full support with active development (ACTIVE status)
* **Previous major version**: Security updates and critical bug fixes for 12 months after the next major release (MAINTENANCE status)
* **Older versions**: Community support only

### Long-term support (LTS) releases

Both LangChain and LangGraph 1.0 are designated as LTS releases:

* Version 1.0 will remain in ACTIVE status until version 2.0 is released
* After version 2.0 is released, version 1.0 will enter MAINTENANCE mode for at least 1 year
* LTS releases follow semantic versioning (semver), allowing safe upgrades between minor versions
* Legacy versions (LangChain 0.3 and LangGraph 0.4) are in MAINTENANCE mode until December 2026

For detailed information about release status and support timelines, see the [Release policy](/oss/python/release-policy).

## Check your version

To check your installed version:

## Pre-release versions

We occasionally release alpha and beta versions for early testing:

* **Alpha** (e.g., `1.0.0a1`): Early preview, significant changes expected
* **Beta** (e.g., `1.0.0b1`): Feature-complete, minor changes possible
* **Release Candidate** (e.g., `1.0.0rc1`): Final testing before stable release

* [Release policy](/oss/python/release-policy) - Detailed release and deprecation policies
* [Releases](/oss/python/releases) - Version-specific release notes and migration guides

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/versioning.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Upgrade

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## View log file size

**URL:** llms-txt#view-log-file-size

ls -lh ~/.claude/state/hook.log

---

## View server logs for a trace

**URL:** llms-txt#view-server-logs-for-a-trace

**Contents:**
- Access server logs from trace view
- Server logs view
- Filtering logs by trace ID

Source: https://docs.langchain.com/langsmith/platform-logs

When viewing a trace that was generated by a run in LangSmith, you can access the associated server logs directly from the trace view.

<Note>
  Viewing server logs for a trace only works with the [Cloud SaaS](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/deployment_options/#cloud-saas) and [fully self-hosted](https://langchain-ai.github.io/langgra/langsmith/observability-concepts/deployment_options/#self-hosted-control-plane) deployment options.
</Note>

## Access server logs from trace view

In the trace view, use the **See Logs** button in the top right corner, next to the **Run in Studio** button.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=87e85d7f38169b5259ba9b335aa89d2f" alt="View server logs button" data-og-width="1595" width="1595" data-og-height="821" height="821" data-path="langsmith/images/view-server-logs-button.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=d70119d610bea0b05196653a2ab13cdd 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=2b395732952d78a269e9fa776b102465 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b69563779a026c19efed909fc1fc8cd3 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=13b31e6677cc41c18f5f7d6826489423 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3ea7803aa4ee13332c14809b953bb03a 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/view-server-logs-button.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=f260eb50eae4ee0e5c094391fa180f5c 2500w" />

Clicking this button will take you to the server logs view for the associated deployment in LangSmith.

The server logs view displays logs from both:

* **Agent Server's own operational logs** - Internal server operations, API calls, and system events
* **User application logs** - Logs written in your graph with:
  * Python: Use the `logging` or `structlog` libraries
  * JavaScript: Use the re-exported Winston logger from `@langchain/langgraph-sdk/logging`:

## Filtering logs by trace ID

When you navigate from the trace view, the **Filters** box will automatically pre-fill with the Trace ID from the trace you just viewed.

This allows you to quickly filter the logs to see only those related to your specific trace execution.

<img src="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=cb6f3c15ca3c8d462ee10c5fd190c73e" alt="Lgp server logs filters" data-og-width="1348" width="1348" data-og-height="681" height="681" data-path="langsmith/images/lgp-server-logs-filters.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=280&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=8eecae6d552d9e58b4b59949afe5cdf4 280w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=560&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=543817c9df78b3fa98d063956189ea6f 560w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=840&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=16989edf602b9f025464c0cd7b83bfa5 840w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=1100&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=5f109fbba34ebd01b856baf6be0d4d18 1100w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=1650&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=d456efb3eb004361f2d2a8c7c9d99e34 1650w, https://mintcdn.com/langchain-5e9cc07a/4kN8yiLrZX_amfFn/langsmith/images/lgp-server-logs-filters.png?w=2500&fit=max&auto=format&n=4kN8yiLrZX_amfFn&q=85&s=fb86476afe8c3f573bdb3a80d51075aa 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/platform-logs.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## View trace counts across your organization

**URL:** llms-txt#view-trace-counts-across-your-organization

**Contents:**
- Programmatically fetch trace counts
  - Method 1: Use the LangSmith REST API
  - Method 2: Use PostgreSQL support queries

Source: https://docs.langchain.com/langsmith/self-host-organization-charts

<Note>
  This feature is available on Helm chart versions 0.9.5 and later.
</Note>

LangSmith automatically generates and syncs organization usage charts for self-hosted installations.

These charts are available under `Settings > Usage and billing > Usage graph`:

* Usage by Workspace: this counts traces (root runs) by workspace
* Organization Usage: this counts all traces (root runs) for the organization

The charts are refreshed to include any new workspaces every 5 minutes. Note that the charts are not editable.

## Programmatically fetch trace counts

You can retrieve trace counts programmatically using two different methods:

### Method 1: Use the LangSmith REST API

If your self-hosted installation uses an online key, you can use the [LangSmith REST API](https://api.smith.langchain.com/redoc?_gl=1*w68t81*_gcl_au*MTgyNTQ5MDUxNy4xNzU2NzI3MDky*_ga*MTU3NDY5MzQyNC4xNzQyOTMyMTQ2*_ga_47WX3HKKY2*czE3NTgyMDAxMDAkbzM0MSRnMCR0MTc1ODIwMDEwMCRqNjAkbDAkaDA.#tag/orgs/operation/get_org_usage_api_v1_orgs_current_billing_usage_get) to fetch organization usage data.

### Method 2: Use PostgreSQL support queries

For installations using offline keys or when you need more detailed export capabilities, you can run support queries directly against the PostgreSQL database. All available scripts are in the [support queries repository](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/scripts/support_queries/postgres).

For more detailed information about running support queries, see the [Run support queries against PostgreSQL](/langsmith/script-running-pg-support-queries) guide.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/self-host-organization-charts.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
### Method 2: Use PostgreSQL support queries

For installations using offline keys or when you need more detailed export capabilities, you can run support queries directly against the PostgreSQL database. All available scripts are in the [support queries repository](https://github.com/langchain-ai/helm/tree/main/charts/langsmith/scripts/support_queries/postgres).
```

---

## Wait For Auth Completion

**URL:** llms-txt#wait-for-auth-completion

Source: https://docs.langchain.com/api-reference/auth-service-v2/wait-for-auth-completion

https://api.host.langchain.com/openapi.json get /v2/auth/wait/{auth_id}
Wait for OAuth authentication completion.

---

## We'll use structured output to enforce that the model returns only

**URL:** llms-txt#we'll-use-structured-output-to-enforce-that-the-model-returns-only

---

## we can add them as nodes directly.

**URL:** llms-txt#we-can-add-them-as-nodes-directly.

**Contents:**
- Evaluations
  - Final response evaluator

graph_builder.add_node("refund_agent", refund_graph)
graph_builder.add_node("question_answering_agent", qa_graph)
graph_builder.add_node(compile_followup)

graph_builder.set_entry_point("intent_classifier")
graph_builder.add_edge("refund_agent", "compile_followup")
graph_builder.add_edge("question_answering_agent", "compile_followup")
graph_builder.add_edge("compile_followup", END)

graph = graph_builder.compile()
python  theme={null}
display(Image(graph.get_graph().draw_mermaid_png()))
python  theme={null}
state = await graph.ainvoke(
    {"messages": [{"role": "user", "content": "what james brown songs do you have"}]}
)
print(state["followup"])

I found 20 James Brown songs in the database, all from the album "Sex Machine". Here they are: ...
python  theme={null}
state = await graph.ainvoke({"messages": [
    {
        "role": "user",
        "content": "my name is Aaron Mitchell and my number is +1 (204) 452-6452. I bought some songs by Led Zeppelin that i'd like refunded",
    }
]})
print(state["followup"])

Which of the following purchases would you like to be refunded for? ...
python  theme={null}
from langsmith import Client

**Examples:**

Example 1 (unknown):
```unknown
We can visualize our compiled parent graph including all of its subgraphs:
```

Example 2 (unknown):
```unknown
<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=619f9b540ea69b1662b2a599ce78241b" alt="graph" data-og-width="646" width="646" data-og-height="680" height="680" data-path="langsmith/images/agent-tutorial-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=227790d90780a4c56233650b957130df 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=30ae6a9b1bc367152a57d4a0c3e41af7 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=37f29b6e783cf2a80714c29ab0be3c5f 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=423ad48e0266ac257b6d76962697b45d 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=581821d6b377b98108507712d6b08c51 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/agent-tutorial-graph.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=126ef194ff5042f691c8f52cf3a1cb75 2500w" />

#### Try it out

Let's give our custom support agent a whirl!
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## We need this because we want to enable threads (conversations)

**URL:** llms-txt#we-need-this-because-we-want-to-enable-threads-(conversations)

checkpointer = InMemorySaver()

---

## We now add a conditional edge

**URL:** llms-txt#we-now-add-a-conditional-edge

workflow.add_conditional_edges(
    # First, we define the start node. We use 'agent'.
    # This means these are the edges taken after the 'agent' node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    should_continue,
)

---

## We set up a `secret` query parameter

**URL:** llms-txt#we-set-up-a-`secret`-query-parameter

**Contents:**
  - Hooking it up

def f(data: dict, secret: str = Query(...)):
    # You can import dependencies you don't have locally inside Modal functions
    from langsmith import Client

# First, we validate the secret key we pass
    import os

if secret != os.environ["LS_WEBHOOK"]:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect bearer token",
            headers={"WWW-Authenticate": "Bearer"},
        )

# This is where we put the logic for what should happen inside this webhook
    ls_client = Client()
    runs = data["runs"]
    ids = [r["id"] for r in runs]
    feedback = list(ls_client.list_feedback(run_ids=ids))
    for r, f in zip(runs, feedback):
        try:
            ls_client.create_example(
                inputs=r["inputs"],
                outputs={"output": f.correction},
                dataset_name="classifier-github-issues",
            )
        except Exception:
            raise ValueError(f"{r} and {f}")
    # Function body
    return "success!"

✓ Created objects.
├── 🔨 Created mount /Users/harrisonchase/workplace/langsmith-docs/example-webhook.py
├── 🔨 Created mount PythonPackage:langsmith
└── 🔨 Created f => https://hwchase17--auth-example-f.modal.run
✓ App deployed! 🎉

View Deployment: https://modal.com/apps/hwchase17/auth-example

https://hwchase17--auth-example-f-dev.modal.run?secret={SECRET}
```

Replace `{SECRET}` with the secret key you created to access the Modal service.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/webhooks.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
We can now deploy this easily with `modal deploy ...` (see docs [here](https://modal.com/docs/guide/managing-deployments)).

You should now get something like:
```

Example 2 (unknown):
```unknown
The important thing to remember is `https://hwchase17--auth-example-f.modal.run` - the function we created to run.
NOTE: this is NOT the final deployment URL, make sure not to accidentally use that.

### Hooking it up

We can now take the function URL we create above and add it as a webhook.
We have to remember to also pass in the secret key as a query parameter.
Putting it all together, it should look something like:
```

---

## We strongly recommend setting up a replicated clickhouse cluster for this load.

**URL:** llms-txt#we-strongly-recommend-setting-up-a-replicated-clickhouse-cluster-for-this-load.

---

## We use LCEL declarative syntax here.

**URL:** llms-txt#we-use-lcel-declarative-syntax-here.

---

## We want this to be a `POST` endpoint since we will post data here

**URL:** llms-txt#we-want-this-to-be-a-`post`-endpoint-since-we-will-post-data-here

@web_endpoint(method="POST")

---

## We will use GPT-3.5 Turbo as the baseline and compare against GPT-4o

**URL:** llms-txt#we-will-use-gpt-3.5-turbo-as-the-baseline-and-compare-against-gpt-4o

gpt_3_5_turbo = init_chat_model(
    "gpt-3.5-turbo",
    temperature=1,
    configurable_fields=("model", "model_provider"),
)

---

## What's new in LangGraph v1

**URL:** llms-txt#what's-new-in-langgraph-v1

**Contents:**
- Deprecation of `create_react_agent`
- Reporting issues
- Additional resources
- See also

Source: https://docs.langchain.com/oss/python/releases/langgraph-v1

**LangGraph v1 is a stability-focused release for the agent runtime.** It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics.

It's designed to work hand-in-hand with [LangChain v1](/oss/python/releases/langchain-v1) (whose `create_agent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.

<CardGroup cols={1}>
  <Card title="Stable core APIs" icon="diagram-project">
    Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.
  </Card>

<Card title="Reliability, by default" icon="database">
    Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.
  </Card>

<Card title="Seamless with LangChain v1" icon="link">
    LangChain's `create_agent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.
  </Card>
</CardGroup>

## Deprecation of `create_react_agent`

The LangGraph [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) prebuilt has been deprecated in favor of LangChain's [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent). It provides a simpler interface, and offers greater customization potential through the introduction of middleware.

* For information on the new [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent) API, see the [LangChain v1 release notes](/oss/python/releases/langchain-v1#create-agent).
* For information on migrating from [`create_react_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat_agent_executor.create_react_agent) to [`create_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create_agent), see the [LangChain v1 migration guide](/oss/python/migrate/langchain-v1#create-agent).

Please report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraph/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Av1).

## Additional resources

<CardGroup cols={3}>
  <Card title="LangGraph 1.0" icon="rocket" href="https://blog.langchain.com/langchain-langchain-1-0-alpha-releases/">
    Read the announcement
  </Card>

<Card title="Overview" icon="book" href="/oss/python/langgraph/overview" arrow>
    What LangGraph is and when to use it
  </Card>

<Card title="Graph API" icon="diagram-project" href="/oss/python/langgraph/graph-api" arrow>
    Build graphs with state, nodes, and edges
  </Card>

<Card title="LangChain Agents" icon="robot" href="/oss/python/langchain/agents" arrow>
    High-level agents built on LangGraph
  </Card>

<Card title="Migration guide" icon="arrow-right-arrow-left" href="/oss/python/migrate/langgraph-v1" arrow>
    How to migrate to LangGraph v1
  </Card>

<Card title="GitHub" icon="github" href="https://github.com/langchain-ai/langgraph">
    Report issues or contribute
  </Card>
</CardGroup>

* [Versioning](/oss/python/versioning) – Understanding version numbers
* [Release policy](/oss/python/release-policy) – Detailed release policies

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langgraph-v1.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown

```

---

## where message_chunk is the token streamed by the LLM and metadata is a dictionary

**URL:** llms-txt#where-message_chunk-is-the-token-streamed-by-the-llm-and-metadata-is-a-dictionary

---

## with information about the graph node where the LLM was called and other information

**URL:** llms-txt#with-information-about-the-graph-node-where-the-llm-was-called-and-other-information

**Contents:**
- Stream custom data
- Use with any LLM

for msg, metadata in graph.stream(
    inputs,
    stream_mode="messages",  # [!code highlight]
):
    # Filter the streamed tokens by the langgraph_node field in the metadata
    # to only include the tokens from the specified node
    if msg.content and metadata["langgraph_node"] == "some_node_name":
        ...
python  theme={null}
  from typing import TypedDict
  from langgraph.graph import START, StateGraph
  from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-4o-mini")

class State(TypedDict):
        topic: str
        joke: str
        poem: str

def write_joke(state: State):
        topic = state["topic"]
        joke_response = model.invoke(
              [{"role": "user", "content": f"Write a joke about {topic}"}]
        )
        return {"joke": joke_response.content}

def write_poem(state: State):
        topic = state["topic"]
        poem_response = model.invoke(
              [{"role": "user", "content": f"Write a short poem about {topic}"}]
        )
        return {"poem": poem_response.content}

graph = (
        StateGraph(State)
        .add_node(write_joke)
        .add_node(write_poem)
        # write both the joke and the poem concurrently
        .add_edge(START, "write_joke")
        .add_edge(START, "write_poem")
        .compile()
  )

# The "messages" stream mode returns a tuple of (message_chunk, metadata)
  # where message_chunk is the token streamed by the LLM and metadata is a dictionary
  # with information about the graph node where the LLM was called and other information
  for msg, metadata in graph.stream(
      {"topic": "cats"},
      stream_mode="messages",  # [!code highlight]
  ):
      # Filter the streamed tokens by the langgraph_node field in the metadata
      # to only include the tokens from the write_poem node
      if msg.content and metadata["langgraph_node"] == "write_poem":
          print(msg.content, end="|", flush=True)
  python  theme={null}
    from typing import TypedDict
    from langgraph.config import get_stream_writer
    from langgraph.graph import StateGraph, START

class State(TypedDict):
        query: str
        answer: str

def node(state: State):
        # Get the stream writer to send custom data
        writer = get_stream_writer()
        # Emit a custom key-value pair (e.g., progress update)
        writer({"custom_key": "Generating custom data inside node"})
        return {"answer": "some data"}

graph = (
        StateGraph(State)
        .add_node(node)
        .add_edge(START, "node")
        .compile()
    )

inputs = {"query": "example"}

# Set stream_mode="custom" to receive the custom data in the stream
    for chunk in graph.stream(inputs, stream_mode="custom"):
        print(chunk)
    python  theme={null}
    from langchain.tools import tool
    from langgraph.config import get_stream_writer

@tool
    def query_database(query: str) -> str:
        """Query the database."""
        # Access the stream writer to send custom data
        writer = get_stream_writer()  # [!code highlight]
        # Emit a custom key-value pair (e.g., progress update)
        writer({"data": "Retrieved 0/100 records", "type": "progress"})  # [!code highlight]
        # perform query
        # Emit another custom key-value pair
        writer({"data": "Retrieved 100/100 records", "type": "progress"})
        return "some-answer"

graph = ... # define a graph that uses this tool

# Set stream_mode="custom" to receive the custom data in the stream
    for chunk in graph.stream(inputs, stream_mode="custom"):
        print(chunk)
    python  theme={null}
from langgraph.config import get_stream_writer

def call_arbitrary_model(state):
    """Example node that calls an arbitrary model and streams the output"""
    # Get the stream writer to send custom data
    writer = get_stream_writer()  # [!code highlight]
    # Assume you have a streaming client that yields chunks
    # Generate LLM tokens using your custom streaming client
    for chunk in your_custom_streaming_client(state["topic"]):
        # Use the writer to send custom data to the stream
        writer({"custom_llm_chunk": chunk})  # [!code highlight]
    return {"result": "completed"}

graph = (
    StateGraph(State)
    .add_node(call_arbitrary_model)
    # Add other nodes and edges as needed
    .compile()
)

**Examples:**

Example 1 (unknown):
```unknown
<Accordion title="Extended example: streaming LLM tokens from specific nodes">
```

Example 2 (unknown):
```unknown
</Accordion>

## Stream custom data

To send **custom user-defined data** from inside a LangGraph node or tool, follow these steps:

1. Use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) to access the stream writer and emit custom data.
2. Set `stream_mode="custom"` when calling `.stream()` or `.astream()` to get the custom data in the stream. You can combine multiple modes (e.g., `["updates", "custom"]`), but at least one must be `"custom"`.

<Warning>
  **No [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) in async for Python \< 3.11**
  In async code running on Python \< 3.11, [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) will not work.
  Instead, add a `writer` parameter to your node or tool and pass it manually.
  See [Async with Python \< 3.11](#async) for usage examples.
</Warning>

<Tabs>
  <Tab title="node">
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="tool">
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

## Use with any LLM

You can use `stream_mode="custom"` to stream data from **any LLM API** — even if that API does **not** implement the LangChain chat model interface.

This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.
```

---

## Worker state

**URL:** llms-txt#worker-state

class WorkerState(TypedDict):
    section: Section
    completed_sections: Annotated[list, operator.add]

---

## Workflow execution configuration with a unique thread identifier

**URL:** llms-txt#workflow-execution-configuration-with-a-unique-thread-identifier

config = {
    "configurable": {
        "thread_id": "1"  # Unique identifier to track workflow execution
    }
}

---

## Wrap it as a tool  # [!code highlight]

**URL:** llms-txt#wrap-it-as-a-tool--#-[!code-highlight]

@tool("subagent_name", description="subagent_description")  # [!code highlight]
def call_subagent(query: str):  # [!code highlight]
    result = subagent.invoke({"messages": [{"role": "user", "content": query}]})
    return result["messages"][-1].content

---

## Wrap it as a tool

**URL:** llms-txt#wrap-it-as-a-tool

@tool("research", description="Research a topic and return findings")
def call_research_agent(query: str):
    result = subagent.invoke({"messages": [{"role": "user", "content": query}]})
    return result["messages"][-1].content

---

## Write sample data to the store using the put method

**URL:** llms-txt#write-sample-data-to-the-store-using-the-put-method

store.put( # [!code highlight]
    ("users",),  # Namespace to group related data together (users namespace for user data)
    "user_123",  # Key within the namespace (user ID as key)
    {
        "name": "John Smith",
        "language": "English",
    }  # Data to store for the given user
)

@tool
def get_user_info(runtime: ToolRuntime[Context]) -> str:
    """Look up user info."""
    # Access the store - same as that provided to `create_agent`
    store = runtime.store # [!code highlight]
    user_id = runtime.context.user_id
    # Retrieve data from store - returns StoreValue object with value and metadata
    user_info = store.get(("users",), user_id) # [!code highlight]
    return str(user_info.value) if user_info else "Unknown user"

agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[get_user_info],
    # Pass store to agent - enables agent to access store when running tools
    store=store, # [!code highlight]
    context_schema=Context
)

---

## Write your prompt with AI

**URL:** llms-txt#write-your-prompt-with-ai

**Contents:**
- Chat sidebar
- Quick actions
- Custom quick actions
- Diffing
- Saving and using prompts

Source: https://docs.langchain.com/langsmith/write-prompt-with-ai

The prompt canvas makes it easy to edit a prompt with the help of an LLM. This allows you to iterate faster on long prompts and also makes it easier to make overarching stylisting or tonal changes to your prompt. You can enter the promp canvas by clicking the glowing wand over any message in your prompt:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-open.gif?s=480b38abe2797436e0b7969e2d961e23" alt="Prompt canvas open" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-open.gif" data-optimize="true" data-opv="3" />

You can use the chat sidebar to ask questions about your prompt, or to give instructions in natural language to the LLM for how to rewrite your prompt.

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-rewrite.gif?s=91ba55b2e7b1e3a18a799265250dedb0" alt="Prompt canvas rewrite" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-rewrite.gif" data-optimize="true" data-opv="3" />

<Note>
  You can also edit the prompt directly - you don't **need** to use the LLM. This is useful if you know what edits you want to make and just want to make them directly
</Note>

There are quick actions to change the reading level or length of the prompt with a single mouse click:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-quick-actions.gif?s=6d2bb4ee78ec98fe551ce7f1a0e94ad9" alt="Prompt canvas quick actions" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-quick-actions.gif" data-optimize="true" data-opv="3" />

## Custom quick actions

You can also save your own custom quick actions, for ease of use across all the prompts you are working on in LangSmith:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-custom-quick-action.gif?s=3fd706f2fdd87abe339f83a2639ce708" alt="Prompt canvas custom quick action" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-custom-quick-action.gif" data-optimize="true" data-opv="3" />

You can also see the specific differences between each version of your prompt by selecting the diff slider in the top right of the canvas:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-diff.gif?s=be6259e6ac773c4a01c5b1ac4fe79e92" alt="Prompt canvas diff" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-diff.gif" data-optimize="true" data-opv="3" />

## Saving and using prompts

Lastly, you can save the prompt you have created in the canvas by clicking the "Use this Version" button in the bottom right:

<img src="https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/prompt-canvas-save.gif?s=f54b0d1b6374e6b69750a6489c787172" alt="Prompt canvas save" data-og-width="1000" width="1000" data-og-height="539" height="539" data-path="langsmith/images/prompt-canvas-save.gif" data-optimize="true" data-opv="3" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/write-prompt-with-ai.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## Your application code using multiple frameworks

**URL:** llms-txt#your-application-code-using-multiple-frameworks

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-semantic-kernel.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## You can access the store directly to get the value

**URL:** llms-txt#you-can-access-the-store-directly-to-get-the-value

store.get(("users",), "user_123").value
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/long-term-memory.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## You can also wrap the async client as well

**URL:** llms-txt#you-can-also-wrap-the-async-client-as-well

---

## You can customize the run name with the `name` keyword argument

**URL:** llms-txt#you-can-customize-the-run-name-with-the-`name`-keyword-argument

@traceable(name="Extract User Details")
def my_function(text: str) -> UserDetail:
    return client.chat.completions.create(
        model="gpt-4o-mini",
        response_model=UserDetail,
        messages=[
            {"role": "user", "content": f"Extract {text}"},
        ]
    )

my_function("Jason is 25 years old")
```

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/trace-with-instructor.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs

**URL:** llms-txt#you-can-iterate-over-the-runs-in-the-experiments-belonging-to-the-comparative-experiment-and-preferentially-rank-the-outputs

---

## You can tag a specific dataset version with a semantic name, like "prod"

**URL:** llms-txt#you-can-tag-a-specific-dataset-version-with-a-semantic-name,-like-"prod"

**Contents:**
- Evaluate on a specific dataset version
  - Use `list_examples`
- Evaluate on a split / filtered view of a dataset
  - Evaluate on a filtered view of a dataset
  - Evaluate on a dataset split
- Share a dataset
  - Share a dataset publicly
  - Unshare a dataset
- Export a dataset
- Export filtered traces from experiment to dataset

client.update_dataset_tag(
    dataset_name=toxic_dataset_name, as_of=initial_time, tag="prod"
)
python Python theme={null}
  from langsmith import Client

# Assumes actual outputs have a 'class' key.
  # Assumes example outputs have a 'label' key.
  def correct(outputs: dict, reference_outputs: dict) -> bool:
    return outputs["class"] == reference_outputs["label"]

results = ls_client.evaluate(
      lambda inputs: {"class": "Not toxic"},
      # Pass in filtered data here:
      data=ls_client.list_examples(
        dataset_name="Toxic Queries",
        as_of="latest",  # specify version here
      ),
      evaluators=[correct],
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      asOf: "latest",
    }),
    evaluators: [correctLabel],
  });
  python Python theme={null}
  from langsmith import evaluate

results = evaluate(
      lambda inputs: label_text(inputs["text"]),
      data=client.list_examples(dataset_name=dataset_name, metadata={"desired_key": "desired_value"}),
      evaluators=[correct_label],
      experiment_prefix="Toxic Queries",
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      metadata: {"desired_key": "desired_value"},
    }),
    evaluators: [correctLabel],
    experimentPrefix: "Toxic Queries",
  });
  python Python theme={null}
  from langsmith import evaluate

results = evaluate(
      lambda inputs: label_text(inputs["text"]),
      data=client.list_examples(dataset_name=dataset_name, splits=["test", "training"]),
      evaluators=[correct_label],
      experiment_prefix="Toxic Queries",
  )
  typescript TypeScript theme={null}
  import { evaluate } from "langsmith/evaluation";

await evaluate((inputs) => labelText(inputs["input"]), {
    data: langsmith.listExamples({
      datasetName: datasetName,
      splits: ["test", "training"],
    }),
    evaluators: [correctLabel],
    experimentPrefix: "Toxic Queries",
  });
  ```
</CodeGroup>

For more details on fetching views of a dataset, refer to the guide on [fetching datasets](/langsmith/manage-datasets-programmatically#fetch-datasets).

### Share a dataset publicly

<Warning>
  Sharing a dataset publicly will make the **dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link**, even if they don't have a LangSmith account. Make sure you're not sharing sensitive information.

This feature is only available in the cloud-hosted version of LangSmith.
</Warning>

From the **Dataset & Experiments** tab, select a dataset, click **⋮** (top right of the page), click **Share Dataset**. This will open a dialog where you can copy the link to the dataset.

<img src="https://mintcdn.com/langchain-5e9cc07a/ImHGLQW1HnQYwnJV/langsmith/images/share-dataset.gif?s=3788767dadf1c265968fe61d96bacc2d" alt="Share Dataset" data-og-width="1086" width="1086" data-og-height="720" height="720" data-path="langsmith/images/share-dataset.gif" data-optimize="true" data-opv="3" />

### Unshare a dataset

1. Click on **Unshare** by clicking on **Public** in the upper right hand corner of any publicly shared dataset, then **Unshare** in the dialog. <img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=98e1807ba9f9a510f56a435b7f81287c" alt="Unshare Dataset" data-og-width="1312" width="1312" data-og-height="803" height="803" data-path="langsmith/images/unshare-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=e5dc54489da489188e26e742fc797f71 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=356b04f4abe97829de039db79ddc5567 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=4bac2ca43d13295f5e16f29b9da90553 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=768b989e2e27f932d4611cfd5b1d3bc0 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=b588cc2aad65b4ad721e05563567e2e3 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-dataset.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=605d68739465a17ac63c9f40f00776f1 2500w" />

2. Navigate to your organization's list of publicly shared datasets, by clicking on **Settings** -> **Shared URLs** or [this link](https://smith.langchain.com/settings/shared), then click on **Unshare** next to the dataset you want to unshare.

<img src="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=8a7762947b85af17b36f2c71857badf7" alt="Unshare Trace List" data-og-width="1125" width="1125" data-og-height="519" height="519" data-path="langsmith/images/unshare-trace-list.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=280&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=1b4d7b5ec44a3ec6bcdad04df29bf417 280w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=560&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=c35f8f9690fd578920a74754ed65eec1 560w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=840&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3383e115479d7e7cab4721a571aa52e3 840w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=1100&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=a5a303b2de245b83369fdfd40390e00b 1100w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=1650&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=77b9562fb148ec7dac8f5ae04afc86ea 1650w, https://mintcdn.com/langchain-5e9cc07a/1RIJxfRpkszanJLL/langsmith/images/unshare-trace-list.png?w=2500&fit=max&auto=format&n=1RIJxfRpkszanJLL&q=85&s=3ddac9fb12453cb0647f6b0a6279b85c 2500w" />

You can export your LangSmith dataset to a CSV, JSONL, or [OpenAI's fine tuning format](https://platform.openai.com/docs/guides/fine-tuning#example-format) from the LangSmith UI.

From the **Dataset & Experiments** tab, select a dataset, click **⋮** (top right of the page), click **Download Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-dataset-button.gif?s=e71b7c55d70528df0a8985b8884f7597" alt="Export Dataset Button" data-og-width="1086" width="1086" data-og-height="720" height="720" data-path="langsmith/images/export-dataset-button.gif" data-optimize="true" data-opv="3" />

## Export filtered traces from experiment to dataset

After running an [offline evaluation](/langsmith/evaluation-concepts#offline-evaluation) in LangSmith, you may want to export [traces](/langsmith/observability-concepts#traces) that met some evaluation criteria to a dataset.

### View experiment traces

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d05263ae403f7f04e8a00ab956313c01" alt="Export filtered traces" data-og-width="3452" width="3452" data-og-height="1224" height="1224" data-path="langsmith/images/export-filtered-trace-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=165c814edfb48cb4f0ae996b654a8b1d 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d097b2b992b036e3f9b1c27a9a5a354e 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=d2e9824cdc9d6dccdb2b0e0d8cd09dab 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7b9df2d826e18396002aa52fabe9465 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=e160dbc0be517b4d9ba3e818a62fb628 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/export-filtered-trace-to-dataset.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=542ba924541f3c6cc970f37e84c07efb 2500w" />

To do so, first click on the arrow next to your experiment name. This will direct you to a project that contains the traces generated from your experiment.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6ef0c958b5af1f2fe113b8717e698584" alt="Export filtered traces" data-og-width="3452" width="3452" data-og-height="1638" height="1638" data-path="langsmith/images/experiment-tracing-project.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=384a1febf8595c2cbf87488a38f7b60a 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=66fbf297dad35d8379572614c17b0c0a 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=849acb7d46cf795441c5d983dc7034fb 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=31bdc68e882b0b94a86f3d28032f2eab 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=6567c99736d666c3a0e616b0cd474663 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/experiment-tracing-project.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=3331bc1c1eefe3189c6257be0a7f48b8 2500w" />

From there, you can filter the traces based on your evaluation criteria. In this example, we're filtering for all traces that received an accuracy score greater than 0.5.

<img src="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0a0edc120c230511d10285f80248dab0" alt="Export filtered traces" data-og-width="3438" width="3438" data-og-height="1844" height="1844" data-path="langsmith/images/filtered-traces-from-experiment.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=280&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=2bfcc0f940f0becbe41050683dd97787 280w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=560&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=702fcaaab265f22c7a6e86c4738aecc2 560w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=840&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=352cfc459b7b30c3aaa5b0968b8ce961 840w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=1100&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=173ce1d23621756731a4df05956117c9 1100w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=1650&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=a7f00715f4e840504d19b6f4daba41ef 1650w, https://mintcdn.com/langchain-5e9cc07a/0B2PFrFBMRWNccee/langsmith/images/filtered-traces-from-experiment.png?w=2500&fit=max&auto=format&n=0B2PFrFBMRWNccee&q=85&s=0f29eb2c1b85e37af88178ffe69eda39 2500w" />

After applying the filter on the project, we can multi-select runs to add to the dataset, and click **Add to Dataset**.

<img src="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=d2488fe04acef624c3528ad01c5bedaa" alt="Export filtered traces" data-og-width="3364" width="3364" data-og-height="1834" height="1834" data-path="langsmith/images/add-filtered-traces-to-dataset.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=280&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4e41c751ac2fec4a2fd5550af8e5538d 280w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=560&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=79e7789b3543800aec927e96e60f6530 560w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=840&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=5fa95115dec4ef56c924260dc3008aef 840w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=1100&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=4aed75b0e0ef7f1f365ba84860311019 1100w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=1650&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=84e44cd484135ba5718da7d7c5c68f81 1650w, https://mintcdn.com/langchain-5e9cc07a/Xbr8HuVd9jPi6qTU/langsmith/images/add-filtered-traces-to-dataset.png?w=2500&fit=max&auto=format&n=Xbr8HuVd9jPi6qTU&q=85&s=22468eea737aaec2ef2faa8e0e34f2ec 2500w" />

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/manage-datasets.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
To run an evaluation on a particular tagged version of a dataset, refer to the [Evaluate on a specific dataset version section](#evaluate-on-specific-dataset-version).

## Evaluate on a specific dataset version

<Check>
  You may find it helpful to refer to the following content before you read this section:

  * [Version a dataset](#version-a-dataset).
  * [Fetching examples](/langsmith/manage-datasets-programmatically#fetch-examples).
</Check>

### Use `list_examples`

You can use `evaluate` / `aevaluate` to pass in an iterable of examples to evaluate on a particular version of a dataset. Use `list_examples` / `listExamples` to fetch examples from a particular version tag using `as_of` / `asOf` and pass that into the `data` argument.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Learn more about how to fetch views of a dataset on the [Create and manage datasets programmatically](/langsmith/manage-datasets-programmatically#fetch-datasets) page.

## Evaluate on a split / filtered view of a dataset

<Check>
  You may find it helpful to refer to the following content before you read this section:

  * [Fetching examples](/langsmith/manage-datasets-programmatically#fetch-examples).
  * [Creating and managing dataset splits](/langsmith/manage-datasets-in-application#create-and-manage-dataset-splits).
</Check>

### Evaluate on a filtered view of a dataset

You can use the `list_examples` / `listExamples` method to [fetch](/langsmith/manage-datasets-programmatically#fetch-examples) a subset of examples from a dataset to evaluate on.

One common workflow is to fetch examples that have a certain metadata key-value pair.

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## You can then create edges to/from this node by referencing it as `"my_node"`

**URL:** llms-txt#you-can-then-create-edges-to/from-this-node-by-referencing-it-as-`"my_node"`

**Contents:**
  - `START` Node
  - `END` Node
  - Node Caching

python  theme={null}
from langgraph.graph import START

graph.add_edge(START, "node_a")
python  theme={null}
from langgraph.graph import END

graph.add_edge("node_a", END)
python  theme={null}
import time
from typing_extensions import TypedDict
from langgraph.graph import StateGraph
from langgraph.cache.memory import InMemoryCache
from langgraph.types import CachePolicy

class State(TypedDict):
    x: int
    result: int

builder = StateGraph(State)

def expensive_node(state: State) -> dict[str, int]:
    # expensive computation
    time.sleep(2)
    return {"result": state["x"] * 2}

builder.add_node("expensive_node", expensive_node, cache_policy=CachePolicy(ttl=3))
builder.set_entry_point("expensive_node")
builder.set_finish_point("expensive_node")

graph = builder.compile(cache=InMemoryCache())

print(graph.invoke({"x": 5}, stream_mode='updates'))    # [!code highlight]

**Examples:**

Example 1 (unknown):
```unknown
### `START` Node

The [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.
```

Example 2 (unknown):
```unknown
### `END` Node

The `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.
```

Example 3 (unknown):
```unknown
### Node Caching

LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:

* Specify a cache when compiling a graph (or specifying an entrypoint)
* Specify a cache policy for nodes. Each cache policy supports:
  * `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle.
  * `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.

For example:
```

---

## You must provide a thread ID to associate the execution with a conversation thread,

**URL:** llms-txt#you-must-provide-a-thread-id-to-associate-the-execution-with-a-conversation-thread,

---

## ]

**URL:** llms-txt#]

**Contents:**
  - Reasoning
  - Local models
  - Prompt caching
  - Server-side tool use
  - Rate limiting
  - Base URL or proxy
  - Log probabilities
  - Token usage
  - Invocation config
  - Configurable models

python Stream reasoning output theme={null}
  for chunk in model.stream("Why do parrots have colorful feathers?"):
      reasoning_steps = [r for r in chunk.content_blocks if r["type"] == "reasoning"]
      print(reasoning_steps if reasoning_steps else chunk.text)
  python Complete reasoning output theme={null}
  response = model.invoke("Why do parrots have colorful feathers?")
  reasoning_steps = [b for b in response.content_blocks if b["type"] == "reasoning"]
  print(" ".join(step["reasoning"] for step in reasoning_steps))
  python Invoke with server-side tool use theme={null}
from langchain.chat_models import init_chat_model

model = init_chat_model("gpt-4.1-mini")

tool = {"type": "web_search"}
model_with_tools = model.bind_tools([tool])

response = model_with_tools.invoke("What was a positive news story from today?")
response.content_blocks
python Result expandable theme={null}
[
    {
        "type": "server_tool_call",
        "name": "web_search",
        "args": {
            "query": "positive news stories today",
            "type": "search"
        },
        "id": "ws_abc123"
    },
    {
        "type": "server_tool_result",
        "tool_call_id": "ws_abc123",
        "status": "success"
    },
    {
        "type": "text",
        "text": "Here are some positive news stories from today...",
        "annotations": [
            {
                "end_index": 410,
                "start_index": 337,
                "title": "article title",
                "type": "citation",
                "url": "..."
            }
        ]
    }
]
python Define a rate limiter theme={null}
  from langchain_core.rate_limiters import InMemoryRateLimiter

rate_limiter = InMemoryRateLimiter(
      requests_per_second=0.1,  # 1 request every 10s
      check_every_n_seconds=0.1,  # Check every 100ms whether allowed to make a request
      max_bucket_size=10,  # Controls the maximum burst size.
  )

model = init_chat_model(
      model="gpt-5",
      model_provider="openai",
      rate_limiter=rate_limiter  # [!code highlight]
  )
  python  theme={null}
  model = init_chat_model(
      model="MODEL_NAME",
      model_provider="openai",
      base_url="BASE_URL",
      api_key="YOUR_API_KEY",
  )
  python  theme={null}
  from langchain_openai import ChatOpenAI

model = ChatOpenAI(
      model="gpt-4o",
      openai_proxy="http://proxy.example.com:8080"
  )
  python  theme={null}
model = init_chat_model(
    model="gpt-4o",
    model_provider="openai"
).bind(logprobs=True)

response = model.invoke("Why do parrots talk?")
print(response.response_metadata["logprobs"])
python  theme={null}
    from langchain.chat_models import init_chat_model
    from langchain_core.callbacks import UsageMetadataCallbackHandler

model_1 = init_chat_model(model="gpt-4o-mini")
    model_2 = init_chat_model(model="claude-haiku-4-5-20251001")

callback = UsageMetadataCallbackHandler()
    result_1 = model_1.invoke("Hello", config={"callbacks": [callback]})
    result_2 = model_2.invoke("Hello", config={"callbacks": [callback]})
    callback.usage_metadata
    python  theme={null}
    {
        'gpt-4o-mini-2024-07-18': {
            'input_tokens': 8,
            'output_tokens': 10,
            'total_tokens': 18,
            'input_token_details': {'audio': 0, 'cache_read': 0},
            'output_token_details': {'audio': 0, 'reasoning': 0}
        },
        'claude-haiku-4-5-20251001': {
            'input_tokens': 8,
            'output_tokens': 21,
            'total_tokens': 29,
            'input_token_details': {'cache_read': 0, 'cache_creation': 0}
        }
    }
    python  theme={null}
    from langchain.chat_models import init_chat_model
    from langchain_core.callbacks import get_usage_metadata_callback

model_1 = init_chat_model(model="gpt-4o-mini")
    model_2 = init_chat_model(model="claude-haiku-4-5-20251001")

with get_usage_metadata_callback() as cb:
        model_1.invoke("Hello")
        model_2.invoke("Hello")
        print(cb.usage_metadata)
    python  theme={null}
    {
        'gpt-4o-mini-2024-07-18': {
            'input_tokens': 8,
            'output_tokens': 10,
            'total_tokens': 18,
            'input_token_details': {'audio': 0, 'cache_read': 0},
            'output_token_details': {'audio': 0, 'reasoning': 0}
        },
        'claude-haiku-4-5-20251001': {
            'input_tokens': 8,
            'output_tokens': 21,
            'total_tokens': 29,
            'input_token_details': {'cache_read': 0, 'cache_creation': 0}
        }
    }
    python Invocation with config theme={null}
response = model.invoke(
    "Tell me a joke",
    config={
        "run_name": "joke_generation",      # Custom name for this run
        "tags": ["humor", "demo"],          # Tags for categorization
        "metadata": {"user_id": "123"},     # Custom metadata
        "callbacks": [my_callback_handler], # Callback handlers
    }
)
python  theme={null}
from langchain.chat_models import init_chat_model

configurable_model = init_chat_model(temperature=0)

configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "gpt-5-nano"}},  # Run with GPT-5-Nano
)
configurable_model.invoke(
    "what's your name",
    config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},  # Run with Claude
)
python  theme={null}
  first_model = init_chat_model(
          model="gpt-4.1-mini",
          temperature=0,
          configurable_fields=("model", "model_provider", "temperature", "max_tokens"),
          config_prefix="first",  # Useful when you have a chain with multiple models
  )

first_model.invoke("what's your name")
  python  theme={null}
  first_model.invoke(
      "what's your name",
      config={
          "configurable": {
              "first_model": "claude-sonnet-4-5-20250929",
              "first_temperature": 0.5,
              "first_max_tokens": 100,
          }
      },
  )
  python  theme={null}
  from pydantic import BaseModel, Field

class GetWeather(BaseModel):
      """Get the current weather in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

class GetPopulation(BaseModel):
      """Get the current population in a given location"""

location: str = Field(..., description="The city and state, e.g. San Francisco, CA")

model = init_chat_model(temperature=0)
  model_with_tools = model.bind_tools([GetWeather, GetPopulation])

model_with_tools.invoke(
      "what's bigger in 2024 LA or NYC", config={"configurable": {"model": "gpt-4.1-mini"}}
  ).tool_calls
  
  [
      {
          'name': 'GetPopulation',
          'args': {'location': 'Los Angeles, CA'},
          'id': 'call_Ga9m8FAArIyEjItHmztPYA22',
          'type': 'tool_call'
      },
      {
          'name': 'GetPopulation',
          'args': {'location': 'New York, NY'},
          'id': 'call_jh2dEvBaAHRaw5JUDthOs7rt',
          'type': 'tool_call'
      }
  ]
  python  theme={null}
  model_with_tools.invoke(
      "what's bigger in 2024 LA or NYC",
      config={"configurable": {"model": "claude-sonnet-4-5-20250929"}},
  ).tool_calls
  
  [
      {
          'name': 'GetPopulation',
          'args': {'location': 'Los Angeles, CA'},
          'id': 'toolu_01JMufPf4F4t2zLj7miFeqXp',
          'type': 'tool_call'
      },
      {
          'name': 'GetPopulation',
          'args': {'location': 'New York City, NY'},
          'id': 'toolu_01RQBHcE8kEEbYTuuS8WqY1u',
          'type': 'tool_call'
      }
  ]
  ```
</Accordion>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/models.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
See the [integrations page](/oss/python/integrations/providers/overview) for details on specific providers.

### Reasoning

Many models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps.

**If supported by the underlying model,** you can surface this reasoning process to better understand how the model arrived at its final answer.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical "tiers" of reasoning (e.g., `'low'` or `'high'`) or integer token budgets.

For details, see the [integrations page](/oss/python/integrations/providers/overview) or [reference](https://reference.langchain.com/python/integrations/) for your respective chat model.

### Local models

LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model.

[Ollama](/oss/python/integrations/chat/ollama) is one of the easiest ways to run models locally. See the full list of local integrations on the [integrations page](/oss/python/integrations/providers/overview).

### Prompt caching

Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be **implicit** or **explicit**:

* **Implicit prompt caching:** providers will automatically pass on cost savings if a request hits a cache. Examples: [OpenAI](/oss/python/integrations/chat/openai) and [Gemini](/oss/python/integrations/chat/google_generative_ai).
* **Explicit caching:** providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples:
  * [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain_openai/ChatOpenAI) (via `prompt_cache_key`)
  * Anthropic's [`AnthropicPromptCachingMiddleware`](/oss/python/integrations/chat/anthropic#prompt-caching)
  * [Gemini](https://python.langchain.com/api_reference/google_genai/chat_models/langchain_google_genai.chat_models.ChatGoogleGenerativeAI.html).
  * [AWS Bedrock](/oss/python/integrations/chat/bedrock#prompt-caching)

<Warning>
  Prompt caching is often only engaged above a minimum input token threshold. See [provider pages](/oss/python/integrations/chat) for details.
</Warning>

Cache usage will be reflected in the [usage metadata](/oss/python/langchain/messages#token-usage) of the model response.

### Server-side tool use

Some providers support server-side [tool-calling](#tool-calling) loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn.

If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the [content blocks](/oss/python/langchain/messages#standard-content-blocks) of the response will return the server-side tool calls and results in a provider-agnostic format:
```

Example 4 (unknown):
```unknown

```

---

## ...

**URL:** llms-txt#...

---

## }

**URL:** llms-txt#}

**Contents:**
  - Multimodal

python  theme={null}
  custom_profile = {
      "max_input_tokens": 100_000,
      "tool_calling": True,
      "structured_output": True,
      # ...
  }
  model = init_chat_model("...", profile=custom_profile)
  python  theme={null}
  new_profile = model.profile | {"key": "value"}
  model.model_copy(update={"profile": new_profile})
  bash  theme={null}
  pip install langchain-model-profiles
  bash  theme={null}
  langchain-profiles refresh --provider <provider> --data-dir <data_dir>
  bash  theme={null}
  uv run --with langchain-model-profiles --provider anthropic --data-dir langchain_anthropic/data
  python Multimodal output theme={null}
response = model.invoke("Create a picture of a cat")
print(response.content_blocks)

**Examples:**

Example 1 (unknown):
```unknown
Refer to the full set of fields in the [API reference](https://reference.langchain.com/python/langchain_core/language_models/#langchain_core.language_models.BaseChatModel.profile).

Much of the model profile data is powered by the [models.dev](https://github.com/sst/models.dev) project, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves.

Model profile data allow applications to work around model capabilities dynamically. For example:

1. [Summarization middleware](/oss/python/langchain/middleware/built-in#summarization) can trigger summarization based on a model's context window size.
2. [Structured output](/oss/python/langchain/structured-output) strategies in `create_agent` can be inferred automatically (e.g., by checking support for native structured output features).
3. Model inputs can be gated based on supported [modalities](#multimodal) and maximum input tokens.

<Accordion title="Updating or overwriting profile data">
  Model profile data can be changed if it is missing, stale, or incorrect.

  **Option 1 (quick fix)**

  You can instantiate a chat model with any valid profile:
```

Example 2 (unknown):
```unknown
The `profile` is also a regular `dict` and can be updated in place. If the model instance is shared, consider using `model_copy` to avoid mutating shared state.
```

Example 3 (unknown):
```unknown
**Option 2 (fix data upstream)**

  The primary source for the data is the [models.dev](https://models.dev/) project. This data is merged with additional fields and overrides in LangChain [integration packages](/oss/python/integrations/providers/overview) and are shipped with those packages.

  Model profile data can be updated through the following process:

  1. (If needed) update the source data at [models.dev](https://models.dev/) through a pull request to its [repository on GitHub](https://github.com/sst/models.dev).
  2. (If needed) update additional fields and overrides in `langchain_<package>/data/profile_augmentations.toml` through a pull request to the LangChain [integration package](/oss/python/integrations/providers/overview)\`.
  3. Use the [`langchain-model-profiles`](https://pypi.org/project/langchain-model-profiles/) CLI tool to pull the latest data from [models.dev](https://models.dev/), merge in the augmentations and update the profile data:
```

Example 4 (unknown):
```unknown

```

---

## >                'action_name': 'execute_sql',

**URL:** llms-txt#>----------------'action_name':-'execute_sql',

---

## >          'action_requests': [

**URL:** llms-txt#>----------'action_requests':-[

---

## ... add nodes and edges ...

**URL:** llms-txt#...-add-nodes-and-edges-...

**Contents:**
- Connect from the client
- Related

my_graph = builder.compile()

@contextlib.contextmanager
async def graph(config):
    configurable = config.get("configurable", {})
    parent_trace = configurable.get("langsmith-trace")
    parent_project = configurable.get("langsmith-project")
    # If you want to also include metadata and tags from the client
    metadata = configurable.get("langsmith-metadata")
    tags = configurable.get("langsmith-tags")
    with ls.tracing_context(parent=parent_trace, project_name=parent_project, metadata=metadata, tags=tags):
        yield my_graph
json  theme={null}
{
  "graphs": {
    "agent": "./src/agent.py:graph"
  }
}
python  theme={null}
    from langgraph.graph import StateGraph
    from langgraph.pregel.remote import RemoteGraph

remote_graph = RemoteGraph(
        "agent",
        url="<DEPLOYMENT_URL>",
        distributed_tracing=True,  # Enable trace propagation
    )

def subgraph_node(query: str):
        # Trace context is automatically propagated
        return remote_graph.invoke({
            "messages": [{"role": "user", "content": query}]
        })['messages'][-1]['content']

# The RemoteGraph is called in the context of some on going work.
    # This could be a parent LangGraph agent, code traced with `@ls.traceable`,
    # or any other instrumented code.
    graph = (
            StateGraph(str)
                .add_node(subgraph_node)
                .add_edge("__start__", "subgraph_node")
                .compile()
    )
    # The remote graph's execution will appear as a child of this trace
    result = graph.invoke("What's the weather in SF?")
    python  theme={null}
    from langgraph_sdk import get_client
    import langsmith as ls

client = get_client(url="<DEPLOYMENT_URL>")

with ls.trace("call_remote_agent", inputs={"query": query}) as rt:
        headers = rt.to_headers()
        async for chunk in client.runs.stream(
            thread_id=None,
            assistant_id="agent",
            input={"messages": [{"role": "user", "content": query}]},
            stream_mode="values",
            headers=headers,  # Pass trace headers
        ):
            pass
        return chunk

result = await call_remote_agent("What's the weather in SF?")
    ```
  </Tab>
</Tabs>

* [Distributed tracing](/langsmith/distributed-tracing): General distributed tracing concepts and patterns
* [RemoteGraph](/langsmith/use-remote-graph): Full guide to interacting with deployments using RemoteGraph

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/agent-server-distributed-tracing.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
Export this `graph` function in your `langgraph.json`:
```

Example 2 (unknown):
```unknown
## Connect from the client

<Tabs>
  <Tab title="RemoteGraph">
    Set `distributed_tracing=True` when initializing [`RemoteGraph`](https://reference.langchain.com/python/langsmith/deployment/remote_graph/). This automatically propagates trace headers on all requests.
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="SDK">
    If you're using the [LangGraph SDK](/langsmith/reference) directly, propagate trace headers manually using `run_tree.to_headers()`:
```

---

## ... add remaining nodes and edges

**URL:** llms-txt#...-add-remaining-nodes-and-edges

**Contents:**
  - From Graph to Functional API

**Examples:**

Example 1 (unknown):
```unknown
### From Graph to Functional API

When your graph becomes overly complex for simple linear processes:
```

---

## - Age: 25

**URL:** llms-txt#--age:-25

---

## >                'allowed_decisions': ['approve', 'reject']

**URL:** llms-txt#>----------------'allowed_decisions':-['approve',-'reject']

---

## >                'arguments': {'query': 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \'30 days\';'},

**URL:** llms-txt#>----------------'arguments':-{'query':-'delete-from-records-where-created_at-<-now()---interval-\'30-days\';'},

---

## ... can add custom routes if needed.

**URL:** llms-txt#...-can-add-custom-routes-if-needed.

**Contents:**
- Configure `langgraph.json`
- Start server
- Deploying
- Next steps

json  theme={null}
{
  "dependencies": ["."],
  "graphs": {
    "agent": "./src/agent/graph.py:graph"
  },
  "env": ".env",
  "http": {
    "app": "./src/agent/webapp.py:app"
  }
  // Other configuration options like auth, store, etc.
}
bash  theme={null}
langgraph dev --no-browser
```

You should see your startup message printed when the server starts, and your cleanup message when you stop it with `Ctrl+C`.

You can deploy your app as-is to cloud or to your self-hosted platform.

Now that you've added lifespan events to your deployment, you can use similar techniques to add [custom routes](/langsmith/custom-routes) or [custom middleware](/langsmith/custom-middleware) to further customize your server's behavior.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-lifespan.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Configure `langgraph.json`

Add the following to your `langgraph.json` configuration file. Make sure the path points to the `webapp.py` file you created above.
```

Example 2 (unknown):
```unknown
## Start server

Test the server out locally:
```

---

## {

**URL:** llms-txt#{

---

## ... database connection and query code

**URL:** llms-txt#...-database-connection-and-query-code

**Contents:**
  - Define the customer support agent

[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]
python  theme={null}
import sqlite3

def _refund(invoice_id: int | None, invoice_line_ids: list[int] | None, mock: bool = False) -> float:
    ...

def _lookup( ...
`python  theme={null}
from typing import Literal
import json

from langchain.chat_models import init_chat_model
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, StateGraph
from langgraph.graph.message import AnyMessage, add_messages
from langgraph.types import Command, interrupt
from tabulate import tabulate
from typing_extensions import Annotated, TypedDict

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
And here's the database schema (image from [https://github.com/lerocha/chinook-database](https://github.com/lerocha/chinook-database)):

<img src="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=5da2a8dcca68f02dfcec11f9c472d341" alt="Chinook DB" data-og-width="1672" width="1672" data-og-height="1132" height="1132" data-path="langsmith/images/chinook-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=280&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=ea7b3a27e9780b556aa90f6914dcef30 280w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=560&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=d9cf3ddad46562213014ffb1a77b1e45 560w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=840&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=2c9e1e70e9be2cf07111b2211e1ef9b7 840w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=1100&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=970f7f48c80222219b493211331ee22f 1100w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=1650&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=3188acb57c4abc0156f8687fa9e229d8 1650w, https://mintcdn.com/langchain-5e9cc07a/E8FdemkcQxROovD9/langsmith/images/chinook-diagram.png?w=2500&fit=max&auto=format&n=E8FdemkcQxROovD9&q=85&s=86b9655b9fd1bc38fcf9e054b11833df 2500w" />

### Define the customer support agent

We'll create a [LangGraph](https://langchain-ai.github.io/langgraph/) agent with limited access to our database. For demo purposes, our agent will support two basic types of requests:

* Lookup: The customer can look up song titles, artist names, and albums based on other identifying information. For example: "What songs do you have by Jimi Hendrix?"
* Refund: The customer can request a refund on their past purchases. For example: "My name is Claude Shannon and I'd like a refund on a purchase I made last week, could you help me?"

For simplicity in this demo, we'll implement refunds by deleting the corresponding database records. We'll skip implementing user authentication and other production security measures.

The agent's logic will be structured as two separate subgraphs (one for lookups and one for refunds), with a parent graph that routes requests to the appropriate subgraph.

#### Refund agent

Let's build the refund processing agent. This agent needs to:

1. Find the customer's purchase records in the database
2. Delete the relevant Invoice and InvoiceLine records to process the refund

We'll create two SQL helper functions:

1. A function to execute the refund by deleting records
2. A function to look up a customer's purchase history

To make testing easier, we'll add a "mock" mode to these functions. When mock mode is enabled, the functions will simulate database operations without actually modifying any data.
```

Example 3 (unknown):
```unknown
Now let's define our graph. We'll use a simple architecture with three main paths:

1. Extract customer and purchase information from the conversation

2. Route the request to one of three paths:

   * Refund path: If we have sufficient purchase details (Invoice ID or Invoice Line IDs) to process a refund
   * Lookup path: If we have enough customer information (name and phone) to search their purchase history
   * Response path: If we need more information, respond to the user requesting the specific details needed

The graph's state will track:

* The conversation history (messages between user and agent)
* All customer and purchase information extracted from the conversation
* The next message to send to the user (followup text)
```

---

## [

**URL:** llms-txt#[

---

## ... Define the graph ...

**URL:** llms-txt#...-define-the-graph-...

**Contents:**
- Capabilities
  - Human-in-the-loop
  - Memory
  - Time Travel
  - Fault-tolerance

graph.compile(
    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))
)
python  theme={null}
import sqlite3

from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.sqlite import SqliteSaver

serde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY
checkpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde)
python  theme={null}
from langgraph.checkpoint.serde.encrypted import EncryptedSerializer
from langgraph.checkpoint.postgres import PostgresSaver

serde = EncryptedSerializer.from_pycryptodome_aes()
checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde)
checkpointer.setup()
```

When running on LangSmith, encryption is automatically enabled whenever `LANGGRAPH_AES_KEY` is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing [`CipherProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol) and supplying it to [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer).

### Human-in-the-loop

First, checkpointers facilitate [human-in-the-loop workflows](/oss/python/langgraph/interrupts) by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See [the how-to guides](/oss/python/langgraph/interrupts) for examples.

Second, checkpointers allow for ["memory"](/oss/python/concepts/memory) between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See [Add memory](/oss/python/langgraph/add-memory) for information on how to add and manage conversation memory using checkpointers.

Third, checkpointers allow for ["time travel"](/oss/python/langgraph/use-time-travel), allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.

Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/persistence.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Encryption

Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer) to the `serde` argument of any [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) implementation. The easiest way to create an encrypted serializer is via [`from_pycryptodome_aes`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from_pycryptodome_aes), which reads the AES key from the `LANGGRAPH_AES_KEY` environment variable (or accepts a `key` argument):
```

Example 2 (unknown):
```unknown

```

---

## >                'description': 'Tool execution pending approval\n\nTool: execute_sql\nArgs: {...}'

**URL:** llms-txt#>----------------'description':-'tool-execution-pending-approval\n\ntool:-execute_sql\nargs:-{...}'

---

## - Email: foo@langchain.dev

**URL:** llms-txt#--email:-foo@langchain.dev

python  theme={null}
from langchain.tools import tool, ToolRuntime

@tool
def get_weather(city: str, runtime: ToolRuntime) -> str:
    """Get weather for a given city."""
    writer = runtime.stream_writer

# Stream custom updates as the tool executes
    writer(f"Looking up data for city: {city}")
    writer(f"Acquired data for city: {city}")

return f"It's always sunny in {city}!"
```

<Note>
  If you use `runtime.stream_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.
</Note>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
#### Stream Writer

Stream custom updates from tools as they execute using `runtime.stream_writer`. This is useful for providing real-time feedback to users about what a tool is doing.
```

---

## ✅ Good: Focused tool set

**URL:** llms-txt#✅-good:-focused-tool-set

email_agent = {
    "name": "email-sender",
    "tools": [send_email, validate_email],  # Only email-related
}

---

## > [Interrupt(value='Do you approve this action?')]

**URL:** llms-txt#>-[interrupt(value='do-you-approve-this-action?')]

---

## >    Interrupt(

**URL:** llms-txt#>----interrupt(

---

## >                'name': 'execute_sql',

**URL:** llms-txt#>----------------'name':-'execute_sql',

---

## - Name: Foo

**URL:** llms-txt#--name:-foo

---

## >          'review_configs': [

**URL:** llms-txt#>----------'review_configs':-[

---

## ... same as above

**URL:** llms-txt#...-same-as-above

**Contents:**
- Distributed tracing in TypeScript

@app.post("/my-route")
async def fake_route(request: Request):
    # request.headers:  {"langsmith-trace": "..."}
    my_application(langsmith_extra={"parent": request.headers})
typescript  theme={null}
// client.mts
import { getCurrentRunTree, traceable } from "langsmith/traceable";

const client = traceable(
    async () => {
        const runTree = getCurrentRunTree();
        return await fetch("...", {
            method: "POST",
            headers: runTree.toHeaders(),
        }).then((a) => a.text());
    },
    { name: "client" }
);

await client();
typescript Express.JS theme={null}
  // server.mts
  import { RunTree } from "langsmith";
  import { traceable, withRunTree } from "langsmith/traceable";
  import express from "express";
  import bodyParser from "body-parser";

const server = traceable(
          (text: string) => `Hello from the server! Received "${text}"`,
          { name: "server" }
      );

const app = express();
      app.use(bodyParser.text());

app.post("/", async (req, res) => {
      const runTree = RunTree.fromHeaders(req.headers);
      const result = await withRunTree(runTree, () => server(req.body));
      res.send(result);
  });
  typescript Hono theme={null}
  // server.mts
  import { RunTree } from "langsmith";
  import { traceable, withRunTree } from "langsmith/traceable";
  import { Hono } from "hono";

const server = traceable(
          (text: string) => `Hello from the server! Received "${text}"`,
          { name: "server" }
      );

const app = new Hono();

app.post("/", async (c) => {
      const body = await c.req.text();
      const runTree = RunTree.fromHeaders(c.req.raw.headers);
      const result = await withRunTree(runTree, () => server(body));
      return c.body(result);
  });
  ```
</CodeGroup>

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/distributed-tracing.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

**Examples:**

Example 1 (unknown):
```unknown
## Distributed tracing in TypeScript

<Note>
  Distributed tracing in TypeScript requires `langsmith` version `>=0.1.31`
</Note>

First, we obtain the current run tree from the client and convert it to `langsmith-trace` and `baggage` header values, which we can pass to the server:
```

Example 2 (unknown):
```unknown
Then, the server converts the headers back to a run tree, which it uses to further continue the tracing.

To pass the newly created run tree to a traceable function, we can use the `withRunTree` helper, which will ensure the run tree is propagated within traceable invocations.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## ... Same as before

**URL:** llms-txt#...-same-as-before

---

## ... Setup authenticate, etc.

**URL:** llms-txt#...-setup-authenticate,-etc.

**Contents:**
- Learn more

@auth.on
async def add_owner(
    ctx: Auth.types.AuthContext,
    value: dict  # The payload being sent to this access method
) -> dict:  # Returns a filter dict that restricts access to resources
    if is_studio_user(ctx.user):
        return {}

filters = {"owner": ctx.user.identity}
    metadata = value.setdefault("metadata", {})
    metadata.update(filters)
    return filters
```

Only use this if you want to permit developer access to a graph deployed on the managed LangSmith SaaS.

* [Authentication & Access Control](/langsmith/auth)
* [Setting up custom authentication tutorial](/langsmith/set-up-custom-auth)

<Callout icon="pen-to-square" iconType="regular">
  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/langsmith/custom-auth.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).
</Callout>

<Tip icon="terminal" iconType="regular">
  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.
</Tip>

---

## ... the rest is the same as before

**URL:** llms-txt#...-the-rest-is-the-same-as-before

---

## -- This code should be in a separate file or service --

**URL:** llms-txt#---this-code-should-be-in-a-separate-file-or-service---

**Contents:**
- Interoperability between LangChain (Python) and LangSmith SDK

@chain
def parent_chain(inputs):
    rt = get_current_run_tree()
    headers = rt.to_headers()
    # ... make a request to another service with the headers
    # The headers should be passed to the other service, eventually to the child_wrapper function

parent_chain.invoke({"test": 1})
python  theme={null}
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langsmith import traceable

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Please respond to the user's request only based on the given context."),
    ("user", "Question: {question}\nContext: {context}")
])

model = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()
chain = prompt | model | output_parser

**Examples:**

Example 1 (unknown):
```unknown
## Interoperability between LangChain (Python) and LangSmith SDK

If you are using LangChain for part of your application and the LangSmith SDK (see [this guide](/langsmith/annotate-code)) for other parts, you can still trace the entire application seamlessly.

LangChain objects will be traced when invoked within a `traceable` function and be bound as a child run of the `traceable` function.
```

---

## >       value={

**URL:** llms-txt#>-------value={

---

## >          ]

**URL:** llms-txt#>----------]

---

## >          ],

**URL:** llms-txt#>----------],

---

## > ]

**URL:** llms-txt#>-]

---

## >             {

**URL:** llms-txt#>-------------{

---

## >       }

**URL:** llms-txt#>-------}

---

## >             }

**URL:** llms-txt#>-------------}

---

## >    )

**URL:** llms-txt#>----)

---

## > [

**URL:** llms-txt#>-[

---

## __interrupt__ contains the payload that was passed to interrupt()

**URL:** llms-txt#__interrupt__-contains-the-payload-that-was-passed-to-interrupt()

print(result["__interrupt__"])

---
