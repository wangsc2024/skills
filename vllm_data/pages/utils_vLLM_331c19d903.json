{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
  "title": "utils - vLLM",
  "content": "A dataclass to hold a metadata store, and the rank, world_size of the group. Only use it to communicate metadata between processes. For data-plane communication, create NCCL-related objects.\n\nAll gather an object from all ranks.\n\nA robust barrier to synchronize all ranks.\n\nUses a multi-phase approach to ensure all processes reach the barrier before proceeding:\n\nEach process signals it has reached the barrier\n\nEach process signals that it has confirmed the arrival of all other ranks.\n\nRank 0 waits for all other ranks to signal their departure to ensure that all ranks have departed the barrier first.\n\nMaximum time in seconds to wait for each phase (in seconds)\n\nIf coordination fails or times out\n\nBroadcast an object from a source rank to all other ranks. It does not clean up after all ranks have received the object. Use it for limited times, e.g., for initialization.\n\nA replacement for torch.distributed.init_process_group that does not pollute the global state.\n\nIf we have process A and process B called torch.distributed.init_process_group to form a group, and then we want to form another group with process A, B, C, D, it is not possible in PyTorch, because process A and process B have already formed a group, and process C and process D cannot join that group. This function is a workaround for this issue.\n\ntorch.distributed.init_process_group is a global call, while this function is a stateless call. It will return a StatelessProcessGroup object that can be used for exchanging metadata. With this function, process A and process B can call StatelessProcessGroup.create to form a group, and then process A, B, C, and D can call StatelessProcessGroup.create to form another group.\n\nExpire data that is older than data_expiration_seconds seconds.\n\nReceive an object from a source rank.\n\nSend an object to a destination rank.\n\nEnsure that numerator is divisible by the denominator and return the division value.\n\nEnsure that numerator is divisible by the denominator.\n\nTry to evenly distribute layers across partitions.\n\nIf the number of layers is not divisible by the number of partitions, the remaining layers are evenly distributed across all but the last partition. The last partition is excluded because it often contains an additional norm layer and we are attempting to balance compute.\n\nIf pp_size > 2 and the number of remaining layers is 0 < x <= pp_size - 2 then the remaining layers are evenly distributed across the middle partitions. The first and last partitions are excluded because they contain the input and output embeddings respectively and we are attempting to reduce maximum memory consumption across partitions.\n\nStateless init ProcessGroup with gloo backend compatible with different torch versions.\n\nSplit a tensor along its last dimension.\n\nnumber of partitions to split the tensor\n\nIf True, make each chunk contiguous in memory.\n\nDestroy ProcessGroup returned by stateless_init_torch_distributed_process_group().\n\nA replacement for torch.distributed.init_process_group that does not pollute the global state. The created ProcessGroup object can be used for some operations such as allreduce, because it does not depend on the global rank. However, some operations such as broadcast cannot be used because it depends on the global rank.\n\nThis function is useful when we are not sure about the total number of processes in the process group. For example, we may have process 1, 2, ..., 8 who want to communicate, and process 9 might be the same process as process 1, or it might be a different process; process 10 might be the same process as process 5, or it might be a different process. In this case, how can we reliably form a communication channel within process 9 and 10, without affecting the communication channel within process 1, 2, ..., 8?\n\nOne possible solution is to figure out if process 9 and 10 are the same as process 1 and 5 beforehand, and then form a communication channel based on the information, adjusting the ranks and world_size etc. However, figuring out the information is not always easy, and it will interfere with the main communication channel.\n\nOur solution is to always form a communication channel with process 1, 2, ..., 8, and then use this function to form another communication channel with process 9 and 10. This way, regardless of whether process 9 and 10 are the same as process 1 and 5, the main communication channel is always formed with process 1, 2, ..., 8, and the additional communication channel is formed with process 9 and 10.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.distributed.utils ¶",
      "id": "vllm.distributed.utils"
    },
    {
      "level": "h2",
      "text": "USE_SCHED_YIELD module-attribute ¶",
      "id": "vllm.distributed.utils.USE_SCHED_YIELD"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.distributed.utils.logger"
    },
    {
      "level": "h2",
      "text": "StatelessProcessGroup dataclass ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup"
    },
    {
      "level": "h3",
      "text": "broadcast_recv_src_counter class-attribute instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.broadcast_recv_src_counter"
    },
    {
      "level": "h3",
      "text": "broadcast_send_counter class-attribute instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.broadcast_send_counter"
    },
    {
      "level": "h3",
      "text": "data_expiration_seconds class-attribute instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.data_expiration_seconds"
    },
    {
      "level": "h3",
      "text": "entries class-attribute instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.entries"
    },
    {
      "level": "h3",
      "text": "rank instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.rank"
    },
    {
      "level": "h3",
      "text": "recv_src_counter class-attribute instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.recv_src_counter"
    },
    {
      "level": "h3",
      "text": "send_dst_counter class-attribute instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.send_dst_counter"
    },
    {
      "level": "h3",
      "text": "socket instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.socket"
    },
    {
      "level": "h3",
      "text": "store instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.store"
    },
    {
      "level": "h3",
      "text": "world_size instance-attribute ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.world_size"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.__init__"
    },
    {
      "level": "h3",
      "text": "__post_init__ ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.__post_init__"
    },
    {
      "level": "h3",
      "text": "all_gather_obj ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.all_gather_obj"
    },
    {
      "level": "h3",
      "text": "barrier ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.barrier"
    },
    {
      "level": "h3",
      "text": "broadcast_obj ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.broadcast_obj"
    },
    {
      "level": "h3",
      "text": "create staticmethod ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.create"
    },
    {
      "level": "h3",
      "text": "expire_data ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.expire_data"
    },
    {
      "level": "h3",
      "text": "recv_obj ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.recv_obj"
    },
    {
      "level": "h3",
      "text": "send_obj ¶",
      "id": "vllm.distributed.utils.StatelessProcessGroup.send_obj"
    },
    {
      "level": "h2",
      "text": "divide ¶",
      "id": "vllm.distributed.utils.divide"
    },
    {
      "level": "h2",
      "text": "ensure_divisibility ¶",
      "id": "vllm.distributed.utils.ensure_divisibility"
    },
    {
      "level": "h2",
      "text": "get_pp_indices ¶",
      "id": "vllm.distributed.utils.get_pp_indices"
    },
    {
      "level": "h2",
      "text": "init_gloo_process_group ¶",
      "id": "vllm.distributed.utils.init_gloo_process_group"
    },
    {
      "level": "h2",
      "text": "sched_yield ¶",
      "id": "vllm.distributed.utils.sched_yield"
    },
    {
      "level": "h2",
      "text": "split_tensor_along_last_dim ¶",
      "id": "vllm.distributed.utils.split_tensor_along_last_dim"
    },
    {
      "level": "h2",
      "text": "stateless_destroy_torch_distributed_process_group ¶",
      "id": "vllm.distributed.utils.stateless_destroy_torch_distributed_process_group"
    },
    {
      "level": "h2",
      "text": "stateless_init_torch_distributed_process_group ¶",
      "id": "vllm.distributed.utils.stateless_init_torch_distributed_process_group"
    },
    {
      "level": "h3",
      "text": "TODO: ask for help from PyTorch team if we need the broadcast operation.¶",
      "id": "vllm.distributed.utils.stateless_init_torch_distributed_process_group--todo-ask-for-help-from-pytorch-team-if-we-need-the-broadcast-operation"
    }
  ],
  "code_samples": [
    {
      "code": "USE_SCHED_YIELD = (\n    version_info[:3] >= (3, 11, 1)\n    or version_info[:2] == (3, 10)\n    and version_info[2] >= 8\n)",
      "language": "json"
    },
    {
      "code": "USE_SCHED_YIELD = (\n    version_info[:3] >= (3, 11, 1)\n    or version_info[:2] == (3, 10)\n    and version_info[2] >= 8\n)",
      "language": "json"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418",
      "language": "unknown"
    },
    {
      "code": "@dataclasses.dataclass\nclass StatelessProcessGroup:\n    \"\"\"A dataclass to hold a metadata store, and the rank, world_size of the\n    group. Only use it to communicate metadata between processes.\n    For data-plane communication, create NCCL-related objects.\n    \"\"\"\n\n    rank: int\n    world_size: int\n    store: torch._C._distributed_c10d.Store\n\n    # stores a reference to the socket so that the file descriptor stays alive\n    socket: socket.socket | None\n\n    data_expiration_seconds: int = 3600  # 1 hour\n\n    # dst rank -> counter\n    send_dst_counter: dict[int, int] = dataclasses.field(default_factory=dict)\n    # src rank -> counter\n    recv_src_counter: dict[int, int] = dataclasses.field(default_factory=dict)\n    broadcast_send_counter: int = 0\n    broadcast_recv_src_counter: dict[int, int] = dataclasses.field(default_factory=dict)\n\n    # A deque to store the data entries, with key and timestamp.\n    entries: deque[tuple[str, float]] = dataclasses.field(default_factory=deque)\n\n    def __post_init__(self):\n        assert self.rank < self.world_size\n        self.send_dst_counter = {i: 0 for i in range(self.world_size)}\n        self.recv_src_counter = {i: 0 for i in range(self.world_size)}\n        self.broadcast_recv_src_counter = {i: 0 for i in range(self.world_size)}\n\n    def send_obj(self, obj: Any, dst: int):\n        \"\"\"Send an object to a destination rank.\"\"\"\n        self.expire_data()\n        key = f\"send_to/{dst}/{self.send_dst_counter[dst]}\"\n        self.store.set(key, pickle.dumps(obj))\n        self.send_dst_counter[dst] += 1\n        self.entries.append((key, time.time()))\n\n    def expire_data(self):\n        \"\"\"Expire data that is older than `data_expiration_seconds` seconds.\"\"\"\n        while self.entries:\n            # check the oldest entry\n            key, timestamp = self.entries[0]\n            if time.time() - timestamp > self.data_expiration_seconds:\n                self.store.delete_key(key)\n                self.entries.popleft()\n            else:\n                break\n\n    def recv_obj(self, src: int) -> Any:\n        \"\"\"Receive an object from a source rank.\"\"\"\n        obj = pickle.loads(\n            self.store.get(f\"send_to/{self.rank}/{self.recv_src_counter[src]}\")\n        )\n        self.recv_src_counter[src] += 1\n        return obj\n\n    def broadcast_obj(self, obj: Any | None, src: int) -> Any:\n        \"\"\"Broadcast an object from a source rank to all other ranks.\n        It does not clean up after all ranks have received the object.\n        Use it for limited times, e.g., for initialization.\n        \"\"\"\n        if self.rank == src:\n            self.expire_data()\n            key = f\"broadcast_from/{src}/{self.broadcast_send_counter}\"\n            self.store.set(key, pickle.dumps(obj))\n            self.broadcast_send_counter += 1\n            self.entries.append((key, time.time()))\n            return obj\n        else:\n            key = f\"broadcast_from/{src}/{self.broadcast_recv_src_counter[src]}\"\n            recv_obj = pickle.loads(self.store.get(key))\n            self.broadcast_recv_src_counter[src] += 1\n            return recv_obj\n\n    def all_gather_obj(self, obj: Any) -> list[Any]:\n        \"\"\"All gather an object from all ranks.\"\"\"\n        gathered_objs = []\n        for i in range(self.world_size):\n            if i == self.rank:\n                gathered_objs.append(obj)\n                self.broadcast_obj(obj, src=self.rank)\n            else:\n                recv_obj = self.broadcast_obj(None, src=i)\n                gathered_objs.append(recv_obj)\n        return gathered_objs\n\n    def barrier(self, timeout: float = 30.0):\n        \"\"\"A robust barrier to synchronize all ranks.\n\n\n        Uses a multi-phase approach to ensure all processes reach the barrier\n        before proceeding:\n\n        1. Each process signals it has reached the barrier\n\n        2. Each process signals that it has confirmed the arrival of all other\n        ranks.\n\n        3. Rank 0 waits for all other ranks to signal their departure to ensure\n        that all ranks have departed the barrier first.\n\n        Args:\n            timeout: Maximum time in seconds to wait for each phase (in seconds)\n\n\n        Raises:\n            RuntimeError: If coordination fails or times out\n        \"\"\"\n        # Generate a barrier ID that is globally unique\n        try:\n            if self.rank == 0:\n                barrier_id = f\"barrier_{uuid.uuid4()}\"\n                self.broadcast_obj(barrier_id, src=0)\n            else:\n                barrier_id = self.broadcast_obj(None, src=0)\n        except Exception as e:\n            raise RuntimeError(\"Failed to broadcast barrier_id\") from e\n\n        # Phase 1: Signal arrival at barrier\n        # Wait for all processes to arrive\n        # We need all ranks to confirm the arrival of all other ranks.\n        # This is the key synchronization point.\n        arrival_key = f\"arrival_{barrier_id}_{self.rank}\"\n        try:\n            self.store.set(arrival_key, b\"1\")\n        except Exception as e:\n            raise RuntimeError(\"Failed to signal barrier arrival\") from e\n\n        start_time = time.time()\n        processes_arrived: set[int] = set()\n\n        while len(processes_arrived) < self.world_size:\n            # Check for timeout\n            cur_time = time.time()\n            if cur_time - start_time > timeout:\n                raise RuntimeError(f\"Barrier timed out after {timeout:.2f} seconds\")\n\n            # Check for each process\n            for i in range(self.world_size):\n                if i in processes_arrived:\n                    continue\n\n                key = f\"arrival_{barrier_id}_{i}\"\n                try:\n                    # Try to get the key - if it exists, we'll get a value\n                    # If it doesn't exist, it will throw an exception\n                    self.store.get(key)\n                    processes_arrived.add(i)\n                except KeyError:\n                    # Key doesn't exist yet\n                    pass\n                except Exception as check_e:\n                    logger.debug(\"Error checking key existence: %s\", check_e)\n                    sched_yield()\n\n            # Short sleep to avoid tight polling\n            if len(processes_arrived) < self.world_size:\n                sched_yield()\n\n        # Phase 2: Signal departure from barrier\n        # We only care to block at this stage in rank 0, which runs the\n        # server side of the TCPStore. We want to make sure that all\n        # clients have departed the barrier before rank 0 in case the\n        # next thing after the barrier is a shutdown, including tearing\n        # down the TCPStore. Other ranks can exit the barrier immediately\n        # after signaling their departure.\n        departure_key = f\"departure_{barrier_id}_{self.rank}\"\n        try:\n            self.store.set(departure_key, b\"1\")\n        except Exception as e:\n            raise RuntimeError(\"Failed to signal barrier departure\") from e\n\n        if self.rank != 0:\n            return\n\n        # Make rank 0 wait for all processes to signal departure\n        start_time = time.time()\n        processes_departed: set[int] = set()\n\n        while len(processes_departed) < self.world_size:\n            # Check for timeout\n            if time.time() - start_time > timeout:\n                raise RuntimeError(\n                    f\"Barrier departure timed out after {timeout:.2f} seconds\"\n                )\n\n            # Check for each process\n            for i in range(self.world_size):\n                if i in processes_departed:\n                    continue\n\n                key = f\"departure_{barrier_id}_{i}\"\n                try:\n                    # Try to get the key - if it exists, we'll get a value\n                    # If it doesn't exist, it will throw an exception\n                    self.store.get(key)\n                    processes_departed.add(i)\n                except KeyError:\n                    # Key doesn't exist yet\n                    pass\n                except Exception as check_e:\n                    logger.debug(\"Error checking key existence: %s\", check_e)\n                    sched_yield()\n\n            # Short sleep to avoid tight polling\n            if len(processes_departed) < self.world_size:\n                sched_yield()\n\n        # Clean up keys to avoid leaking memory in the store\n        for i in range(self.world_size):\n            try:\n                self.store.delete_key(f\"arrival_{barrier_id}_{i}\")\n            except Exception:\n                logger.debug(\"Error deleting key: %s\", f\"arrival_{barrier_id}_{i}\")\n\n            try:\n                self.store.delete_key(f\"departure_{barrier_id}_{i}\")\n            except Exception:\n                logger.debug(\"Error deleting key: %s\", f\"departure_{barrier_id}_{i}\")\n\n    @staticmethod\n    def create(\n        host: str,\n        port: int,\n        rank: int,\n        world_size: int,\n        data_expiration_seconds: int = 3600,\n        store_timeout: int = 300,\n    ) -> \"StatelessProcessGroup\":\n        \"\"\"A replacement for `torch.distributed.init_process_group` that does not\n        pollute the global state.\n\n        If we have process A and process B called `torch.distributed.init_process_group`\n        to form a group, and then we want to form another group with process A, B, C,\n        D, it is not possible in PyTorch, because process A and process B have already\n        formed a group, and process C and process D cannot join that group. This\n        function is a workaround for this issue.\n\n        `torch.distributed.init_process_group` is a global call, while this function\n        is a stateless call. It will return a `StatelessProcessGroup` object that can be\n        used for exchanging metadata. With this function, process A and process B\n        can call `StatelessProcessGroup.create` to form a group, and then process A, B,\n        C, and D can call `StatelessProcessGroup.create` to form another group.\n        \"\"\"  # noqa\n        launch_server = rank == 0\n        if launch_server:\n            # listen on the specified interface (instead of 0.0.0.0)\n            listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            listen_socket.bind((host, port))\n            listen_socket.listen()\n            listen_fd = listen_socket.fileno()\n        else:\n            listen_socket = None\n            listen_fd = None\n\n        store = TCPStore(\n            host_name=host,\n            port=port,\n            world_size=world_size,\n            is_master=launch_server,\n            timeout=timedelta(seconds=store_timeout),\n            use_libuv=False,  # for now: github.com/pytorch/pytorch/pull/150215\n            master_listen_fd=listen_fd,\n        )\n\n        return StatelessProcessGroup(\n            rank=rank,\n            world_size=world_size,\n            store=store,\n            socket=listen_socket,\n            data_expiration_seconds=data_expiration_seconds,\n        )",
      "language": "python"
    },
    {
      "code": "@dataclasses.dataclass\nclass StatelessProcessGroup:\n    \"\"\"A dataclass to hold a metadata store, and the rank, world_size of the\n    group. Only use it to communicate metadata between processes.\n    For data-plane communication, create NCCL-related objects.\n    \"\"\"\n\n    rank: int\n    world_size: int\n    store: torch._C._distributed_c10d.Store\n\n    # stores a reference to the socket so that the file descriptor stays alive\n    socket: socket.socket | None\n\n    data_expiration_seconds: int = 3600  # 1 hour\n\n    # dst rank -> counter\n    send_dst_counter: dict[int, int] = dataclasses.field(default_factory=dict)\n    # src rank -> counter\n    recv_src_counter: dict[int, int] = dataclasses.field(default_factory=dict)\n    broadcast_send_counter: int = 0\n    broadcast_recv_src_counter: dict[int, int] = dataclasses.field(default_factory=dict)\n\n    # A deque to store the data entries, with key and timestamp.\n    entries: deque[tuple[str, float]] = dataclasses.field(default_factory=deque)\n\n    def __post_init__(self):\n        assert self.rank < self.world_size\n        self.send_dst_counter = {i: 0 for i in range(self.world_size)}\n        self.recv_src_counter = {i: 0 for i in range(self.world_size)}\n        self.broadcast_recv_src_counter = {i: 0 for i in range(self.world_size)}\n\n    def send_obj(self, obj: Any, dst: int):\n        \"\"\"Send an object to a destination rank.\"\"\"\n        self.expire_data()\n        key = f\"send_to/{dst}/{self.send_dst_counter[dst]}\"\n        self.store.set(key, pickle.dumps(obj))\n        self.send_dst_counter[dst] += 1\n        self.entries.append((key, time.time()))\n\n    def expire_data(self):\n        \"\"\"Expire data that is older than `data_expiration_seconds` seconds.\"\"\"\n        while self.entries:\n            # check the oldest entry\n            key, timestamp = self.entries[0]\n            if time.time() - timestamp > self.data_expiration_seconds:\n                self.store.delete_key(key)\n                self.entries.popleft()\n            else:\n                break\n\n    def recv_obj(self, src: int) -> Any:\n        \"\"\"Receive an object from a source rank.\"\"\"\n        obj = pickle.loads(\n            self.store.get(f\"send_to/{self.rank}/{self.recv_src_counter[src]}\")\n        )\n        self.recv_src_counter[src] += 1\n        return obj\n\n    def broadcast_obj(self, obj: Any | None, src: int) -> Any:\n        \"\"\"Broadcast an object from a source rank to all other ranks.\n        It does not clean up after all ranks have received the object.\n        Use it for limited times, e.g., for initialization.\n        \"\"\"\n        if self.rank == src:\n            self.expire_data()\n            key = f\"broadcast_from/{src}/{self.broadcast_send_counter}\"\n            self.store.set(key, pickle.dumps(obj))\n            self.broadcast_send_counter += 1\n            self.entries.append((key, time.time()))\n            return obj\n        else:\n            key = f\"broadcast_from/{src}/{self.broadcast_recv_src_counter[src]}\"\n            recv_obj = pickle.loads(self.store.get(key))\n            self.broadcast_recv_src_counter[src] += 1\n            return recv_obj\n\n    def all_gather_obj(self, obj: Any) -> list[Any]:\n        \"\"\"All gather an object from all ranks.\"\"\"\n        gathered_objs = []\n        for i in range(self.world_size):\n            if i == self.rank:\n                gathered_objs.append(obj)\n                self.broadcast_obj(obj, src=self.rank)\n            else:\n                recv_obj = self.broadcast_obj(None, src=i)\n                gathered_objs.append(recv_obj)\n        return gathered_objs\n\n    def barrier(self, timeout: float = 30.0):\n        \"\"\"A robust barrier to synchronize all ranks.\n\n\n        Uses a multi-phase approach to ensure all processes reach the barrier\n        before proceeding:\n\n        1. Each process signals it has reached the barrier\n\n        2. Each process signals that it has confirmed the arrival of all other\n        ranks.\n\n        3. Rank 0 waits for all other ranks to signal their departure to ensure\n        that all ranks have departed the barrier first.\n\n        Args:\n            timeout: Maximum time in seconds to wait for each phase (in seconds)\n\n\n        Raises:\n            RuntimeError: If coordination fails or times out\n        \"\"\"\n        # Generate a barrier ID that is globally unique\n        try:\n            if self.rank == 0:\n                barrier_id = f\"barrier_{uuid.uuid4()}\"\n                self.broadcast_obj(barrier_id, src=0)\n            else:\n                barrier_id = self.broadcast_obj(None, src=0)\n        except Exception as e:\n            raise RuntimeError(\"Failed to broadcast barrier_id\") from e\n\n        # Phase 1: Signal arrival at barrier\n        # Wait for all processes to arrive\n        # We need all ranks to confirm the arrival of all other ranks.\n        # This is the key synchronization point.\n        arrival_key = f\"arrival_{barrier_id}_{self.rank}\"\n        try:\n            self.store.set(arrival_key, b\"1\")\n        except Exception as e:\n            raise RuntimeError(\"Failed to signal barrier arrival\") from e\n\n        start_time = time.time()\n        processes_arrived: set[int] = set()\n\n        while len(processes_arrived) < self.world_size:\n            # Check for timeout\n            cur_time = time.time()\n            if cur_time - start_time > timeout:\n                raise RuntimeError(f\"Barrier timed out after {timeout:.2f} seconds\")\n\n            # Check for each process\n            for i in range(self.world_size):\n                if i in processes_arrived:\n                    continue\n\n                key = f\"arrival_{barrier_id}_{i}\"\n                try:\n                    # Try to get the key - if it exists, we'll get a value\n                    # If it doesn't exist, it will throw an exception\n                    self.store.get(key)\n                    processes_arrived.add(i)\n                except KeyError:\n                    # Key doesn't exist yet\n                    pass\n                except Exception as check_e:\n                    logger.debug(\"Error checking key existence: %s\", check_e)\n                    sched_yield()\n\n            # Short sleep to avoid tight polling\n            if len(processes_arrived) < self.world_size:\n                sched_yield()\n\n        # Phase 2: Signal departure from barrier\n        # We only care to block at this stage in rank 0, which runs the\n        # server side of the TCPStore. We want to make sure that all\n        # clients have departed the barrier before rank 0 in case the\n        # next thing after the barrier is a shutdown, including tearing\n        # down the TCPStore. Other ranks can exit the barrier immediately\n        # after signaling their departure.\n        departure_key = f\"departure_{barrier_id}_{self.rank}\"\n        try:\n            self.store.set(departure_key, b\"1\")\n        except Exception as e:\n            raise RuntimeError(\"Failed to signal barrier departure\") from e\n\n        if self.rank != 0:\n            return\n\n        # Make rank 0 wait for all processes to signal departure\n        start_time = time.time()\n        processes_departed: set[int] = set()\n\n        while len(processes_departed) < self.world_size:\n            # Check for timeout\n            if time.time() - start_time > timeout:\n                raise RuntimeError(\n                    f\"Barrier departure timed out after {timeout:.2f} seconds\"\n                )\n\n            # Check for each process\n            for i in range(self.world_size):\n                if i in processes_departed:\n                    continue\n\n                key = f\"departure_{barrier_id}_{i}\"\n                try:\n                    # Try to get the key - if it exists, we'll get a value\n                    # If it doesn't exist, it will throw an exception\n                    self.store.get(key)\n                    processes_departed.add(i)\n                except KeyError:\n                    # Key doesn't exist yet\n                    pass\n                except Exception as check_e:\n                    logger.debug(\"Error checking key existence: %s\", check_e)\n                    sched_yield()\n\n            # Short sleep to avoid tight polling\n            if len(processes_departed) < self.world_size:\n                sched_yield()\n\n        # Clean up keys to avoid leaking memory in the store\n        for i in range(self.world_size):\n            try:\n                self.store.delete_key(f\"arrival_{barrier_id}_{i}\")\n            except Exception:\n                logger.debug(\"Error deleting key: %s\", f\"arrival_{barrier_id}_{i}\")\n\n            try:\n                self.store.delete_key(f\"departure_{barrier_id}_{i}\")\n            except Exception:\n                logger.debug(\"Error deleting key: %s\", f\"departure_{barrier_id}_{i}\")\n\n    @staticmethod\n    def create(\n        host: str,\n        port: int,\n        rank: int,\n        world_size: int,\n        data_expiration_seconds: int = 3600,\n        store_timeout: int = 300,\n    ) -> \"StatelessProcessGroup\":\n        \"\"\"A replacement for `torch.distributed.init_process_group` that does not\n        pollute the global state.\n\n        If we have process A and process B called `torch.distributed.init_process_group`\n        to form a group, and then we want to form another group with process A, B, C,\n        D, it is not possible in PyTorch, because process A and process B have already\n        formed a group, and process C and process D cannot join that group. This\n        function is a workaround for this issue.\n\n        `torch.distributed.init_process_group` is a global call, while this function\n        is a stateless call. It will return a `StatelessProcessGroup` object that can be\n        used for exchanging metadata. With this function, process A and process B\n        can call `StatelessProcessGroup.create` to form a group, and then process A, B,\n        C, and D can call `StatelessProcessGroup.create` to form another group.\n        \"\"\"  # noqa\n        launch_server = rank == 0\n        if launch_server:\n            # listen on the specified interface (instead of 0.0.0.0)\n            listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            listen_socket.bind((host, port))\n            listen_socket.listen()\n            listen_fd = listen_socket.fileno()\n        else:\n            listen_socket = None\n            listen_fd = None\n\n        store = TCPStore(\n            host_name=host,\n            port=port,\n            world_size=world_size,\n            is_master=launch_server,\n            timeout=timedelta(seconds=store_timeout),\n            use_libuv=False,  # for now: github.com/pytorch/pytorch/pull/150215\n            master_listen_fd=listen_fd,\n        )\n\n        return StatelessProcessGroup(\n            rank=rank,\n            world_size=world_size,\n            store=store,\n            socket=listen_socket,\n            data_expiration_seconds=data_expiration_seconds,\n        )",
      "language": "python"
    },
    {
      "code": "broadcast_recv_src_counter: dict[int, int] = field(\n    default_factory=dict\n)",
      "language": "yaml"
    },
    {
      "code": "broadcast_recv_src_counter: dict[int, int] = field(\n    default_factory=dict\n)",
      "language": "yaml"
    },
    {
      "code": "broadcast_send_counter: int = 0",
      "language": "typescript"
    },
    {
      "code": "broadcast_send_counter: int = 0",
      "language": "typescript"
    },
    {
      "code": "data_expiration_seconds: int = 3600",
      "language": "typescript"
    },
    {
      "code": "data_expiration_seconds: int = 3600",
      "language": "typescript"
    },
    {
      "code": "entries: deque[tuple[str, float]] = field(\n    default_factory=deque\n)",
      "language": "yaml"
    },
    {
      "code": "entries: deque[tuple[str, float]] = field(\n    default_factory=deque\n)",
      "language": "yaml"
    },
    {
      "code": "recv_src_counter: dict[int, int] = field(\n    default_factory=dict\n)",
      "language": "yaml"
    },
    {
      "code": "recv_src_counter: dict[int, int] = field(\n    default_factory=dict\n)",
      "language": "yaml"
    },
    {
      "code": "send_dst_counter: dict[int, int] = field(\n    default_factory=dict\n)",
      "language": "yaml"
    },
    {
      "code": "send_dst_counter: dict[int, int] = field(\n    default_factory=dict\n)",
      "language": "yaml"
    },
    {
      "code": "socket: socket | None",
      "language": "yaml"
    },
    {
      "code": "socket: socket | None",
      "language": "yaml"
    },
    {
      "code": "store: Store",
      "language": "yaml"
    },
    {
      "code": "store: Store",
      "language": "yaml"
    },
    {
      "code": "world_size: int",
      "language": "yaml"
    },
    {
      "code": "world_size: int",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    rank: int,\n    world_size: int,\n    store: Store,\n    socket: socket | None,\n    data_expiration_seconds: int = 3600,\n    send_dst_counter: dict[int, int] = dict(),\n    recv_src_counter: dict[int, int] = dict(),\n    broadcast_send_counter: int = 0,\n    broadcast_recv_src_counter: dict[int, int] = dict(),\n    entries: deque[tuple[str, float]] = deque(),\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    rank: int,\n    world_size: int,\n    store: Store,\n    socket: socket | None,\n    data_expiration_seconds: int = 3600,\n    send_dst_counter: dict[int, int] = dict(),\n    recv_src_counter: dict[int, int] = dict(),\n    broadcast_send_counter: int = 0,\n    broadcast_recv_src_counter: dict[int, int] = dict(),\n    entries: deque[tuple[str, float]] = deque(),\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__post_init__()",
      "language": "unknown"
    },
    {
      "code": "__post_init__()",
      "language": "unknown"
    },
    {
      "code": "169\n170\n171\n172\n173",
      "language": "unknown"
    },
    {
      "code": "def __post_init__(self):\n    assert self.rank < self.world_size\n    self.send_dst_counter = {i: 0 for i in range(self.world_size)}\n    self.recv_src_counter = {i: 0 for i in range(self.world_size)}\n    self.broadcast_recv_src_counter = {i: 0 for i in range(self.world_size)}",
      "language": "python"
    },
    {
      "code": "def __post_init__(self):\n    assert self.rank < self.world_size\n    self.send_dst_counter = {i: 0 for i in range(self.world_size)}\n    self.recv_src_counter = {i: 0 for i in range(self.world_size)}\n    self.broadcast_recv_src_counter = {i: 0 for i in range(self.world_size)}",
      "language": "python"
    },
    {
      "code": "all_gather_obj(obj: Any) -> list[Any]",
      "language": "php"
    },
    {
      "code": "all_gather_obj(obj: Any) -> list[Any]",
      "language": "php"
    },
    {
      "code": "220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230",
      "language": "unknown"
    },
    {
      "code": "def all_gather_obj(self, obj: Any) -> list[Any]:\n    \"\"\"All gather an object from all ranks.\"\"\"\n    gathered_objs = []\n    for i in range(self.world_size):\n        if i == self.rank:\n            gathered_objs.append(obj)\n            self.broadcast_obj(obj, src=self.rank)\n        else:\n            recv_obj = self.broadcast_obj(None, src=i)\n            gathered_objs.append(recv_obj)\n    return gathered_objs",
      "language": "python"
    },
    {
      "code": "def all_gather_obj(self, obj: Any) -> list[Any]:\n    \"\"\"All gather an object from all ranks.\"\"\"\n    gathered_objs = []\n    for i in range(self.world_size):\n        if i == self.rank:\n            gathered_objs.append(obj)\n            self.broadcast_obj(obj, src=self.rank)\n        else:\n            recv_obj = self.broadcast_obj(None, src=i)\n            gathered_objs.append(recv_obj)\n    return gathered_objs",
      "language": "python"
    },
    {
      "code": "barrier(timeout: float = 30.0)",
      "language": "typescript"
    },
    {
      "code": "barrier(timeout: float = 30.0)",
      "language": "typescript"
    },
    {
      "code": "232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364",
      "language": "unknown"
    },
    {
      "code": "def barrier(self, timeout: float = 30.0):\n    \"\"\"A robust barrier to synchronize all ranks.\n\n\n    Uses a multi-phase approach to ensure all processes reach the barrier\n    before proceeding:\n\n    1. Each process signals it has reached the barrier\n\n    2. Each process signals that it has confirmed the arrival of all other\n    ranks.\n\n    3. Rank 0 waits for all other ranks to signal their departure to ensure\n    that all ranks have departed the barrier first.\n\n    Args:\n        timeout: Maximum time in seconds to wait for each phase (in seconds)\n\n\n    Raises:\n        RuntimeError: If coordination fails or times out\n    \"\"\"\n    # Generate a barrier ID that is globally unique\n    try:\n        if self.rank == 0:\n            barrier_id = f\"barrier_{uuid.uuid4()}\"\n            self.broadcast_obj(barrier_id, src=0)\n        else:\n            barrier_id = self.broadcast_obj(None, src=0)\n    except Exception as e:\n        raise RuntimeError(\"Failed to broadcast barrier_id\") from e\n\n    # Phase 1: Signal arrival at barrier\n    # Wait for all processes to arrive\n    # We need all ranks to confirm the arrival of all other ranks.\n    # This is the key synchronization point.\n    arrival_key = f\"arrival_{barrier_id}_{self.rank}\"\n    try:\n        self.store.set(arrival_key, b\"1\")\n    except Exception as e:\n        raise RuntimeError(\"Failed to signal barrier arrival\") from e\n\n    start_time = time.time()\n    processes_arrived: set[int] = set()\n\n    while len(processes_arrived) < self.world_size:\n        # Check for timeout\n        cur_time = time.time()\n        if cur_time - start_time > timeout:\n            raise RuntimeError(f\"Barrier timed out after {timeout:.2f} seconds\")\n\n        # Check for each process\n        for i in range(self.world_size):\n            if i in processes_arrived:\n                continue\n\n            key = f\"arrival_{barrier_id}_{i}\"\n            try:\n                # Try to get the key - if it exists, we'll get a value\n                # If it doesn't exist, it will throw an exception\n                self.store.get(key)\n                processes_arrived.add(i)\n            except KeyError:\n                # Key doesn't exist yet\n                pass\n            except Exception as check_e:\n                logger.debug(\"Error checking key existence: %s\", check_e)\n                sched_yield()\n\n        # Short sleep to avoid tight polling\n        if len(processes_arrived) < self.world_size:\n            sched_yield()\n\n    # Phase 2: Signal departure from barrier\n    # We only care to block at this stage in rank 0, which runs the\n    # server side of the TCPStore. We want to make sure that all\n    # clients have departed the barrier before rank 0 in case the\n    # next thing after the barrier is a shutdown, including tearing\n    # down the TCPStore. Other ranks can exit the barrier immediately\n    # after signaling their departure.\n    departure_key = f\"departure_{barrier_id}_{self.rank}\"\n    try:\n        self.store.set(departure_key, b\"1\")\n    except Exception as e:\n        raise RuntimeError(\"Failed to signal barrier departure\") from e\n\n    if self.rank != 0:\n        return\n\n    # Make rank 0 wait for all processes to signal departure\n    start_time = time.time()\n    processes_departed: set[int] = set()\n\n    while len(processes_departed) < self.world_size:\n        # Check for timeout\n        if time.time() - start_time > timeout:\n            raise RuntimeError(\n                f\"Barrier departure timed out after {timeout:.2f} seconds\"\n            )\n\n        # Check for each process\n        for i in range(self.world_size):\n            if i in processes_departed:\n                continue\n\n            key = f\"departure_{barrier_id}_{i}\"\n            try:\n                # Try to get the key - if it exists, we'll get a value\n                # If it doesn't exist, it will throw an exception\n                self.store.get(key)\n                processes_departed.add(i)\n            except KeyError:\n                # Key doesn't exist yet\n                pass\n            except Exception as check_e:\n                logger.debug(\"Error checking key existence: %s\", check_e)\n                sched_yield()\n\n        # Short sleep to avoid tight polling\n        if len(processes_departed) < self.world_size:\n            sched_yield()\n\n    # Clean up keys to avoid leaking memory in the store\n    for i in range(self.world_size):\n        try:\n            self.store.delete_key(f\"arrival_{barrier_id}_{i}\")\n        except Exception:\n            logger.debug(\"Error deleting key: %s\", f\"arrival_{barrier_id}_{i}\")\n\n        try:\n            self.store.delete_key(f\"departure_{barrier_id}_{i}\")\n        except Exception:\n            logger.debug(\"Error deleting key: %s\", f\"departure_{barrier_id}_{i}\")",
      "language": "python"
    },
    {
      "code": "def barrier(self, timeout: float = 30.0):\n    \"\"\"A robust barrier to synchronize all ranks.\n\n\n    Uses a multi-phase approach to ensure all processes reach the barrier\n    before proceeding:\n\n    1. Each process signals it has reached the barrier\n\n    2. Each process signals that it has confirmed the arrival of all other\n    ranks.\n\n    3. Rank 0 waits for all other ranks to signal their departure to ensure\n    that all ranks have departed the barrier first.\n\n    Args:\n        timeout: Maximum time in seconds to wait for each phase (in seconds)\n\n\n    Raises:\n        RuntimeError: If coordination fails or times out\n    \"\"\"\n    # Generate a barrier ID that is globally unique\n    try:\n        if self.rank == 0:\n            barrier_id = f\"barrier_{uuid.uuid4()}\"\n            self.broadcast_obj(barrier_id, src=0)\n        else:\n            barrier_id = self.broadcast_obj(None, src=0)\n    except Exception as e:\n        raise RuntimeError(\"Failed to broadcast barrier_id\") from e\n\n    # Phase 1: Signal arrival at barrier\n    # Wait for all processes to arrive\n    # We need all ranks to confirm the arrival of all other ranks.\n    # This is the key synchronization point.\n    arrival_key = f\"arrival_{barrier_id}_{self.rank}\"\n    try:\n        self.store.set(arrival_key, b\"1\")\n    except Exception as e:\n        raise RuntimeError(\"Failed to signal barrier arrival\") from e\n\n    start_time = time.time()\n    processes_arrived: set[int] = set()\n\n    while len(processes_arrived) < self.world_size:\n        # Check for timeout\n        cur_time = time.time()\n        if cur_time - start_time > timeout:\n            raise RuntimeError(f\"Barrier timed out after {timeout:.2f} seconds\")\n\n        # Check for each process\n        for i in range(self.world_size):\n            if i in processes_arrived:\n                continue\n\n            key = f\"arrival_{barrier_id}_{i}\"\n            try:\n                # Try to get the key - if it exists, we'll get a value\n                # If it doesn't exist, it will throw an exception\n                self.store.get(key)\n                processes_arrived.add(i)\n            except KeyError:\n                # Key doesn't exist yet\n                pass\n            except Exception as check_e:\n                logger.debug(\"Error checking key existence: %s\", check_e)\n                sched_yield()\n\n        # Short sleep to avoid tight polling\n        if len(processes_arrived) < self.world_size:\n            sched_yield()\n\n    # Phase 2: Signal departure from barrier\n    # We only care to block at this stage in rank 0, which runs the\n    # server side of the TCPStore. We want to make sure that all\n    # clients have departed the barrier before rank 0 in case the\n    # next thing after the barrier is a shutdown, including tearing\n    # down the TCPStore. Other ranks can exit the barrier immediately\n    # after signaling their departure.\n    departure_key = f\"departure_{barrier_id}_{self.rank}\"\n    try:\n        self.store.set(departure_key, b\"1\")\n    except Exception as e:\n        raise RuntimeError(\"Failed to signal barrier departure\") from e\n\n    if self.rank != 0:\n        return\n\n    # Make rank 0 wait for all processes to signal departure\n    start_time = time.time()\n    processes_departed: set[int] = set()\n\n    while len(processes_departed) < self.world_size:\n        # Check for timeout\n        if time.time() - start_time > timeout:\n            raise RuntimeError(\n                f\"Barrier departure timed out after {timeout:.2f} seconds\"\n            )\n\n        # Check for each process\n        for i in range(self.world_size):\n            if i in processes_departed:\n                continue\n\n            key = f\"departure_{barrier_id}_{i}\"\n            try:\n                # Try to get the key - if it exists, we'll get a value\n                # If it doesn't exist, it will throw an exception\n                self.store.get(key)\n                processes_departed.add(i)\n            except KeyError:\n                # Key doesn't exist yet\n                pass\n            except Exception as check_e:\n                logger.debug(\"Error checking key existence: %s\", check_e)\n                sched_yield()\n\n        # Short sleep to avoid tight polling\n        if len(processes_departed) < self.world_size:\n            sched_yield()\n\n    # Clean up keys to avoid leaking memory in the store\n    for i in range(self.world_size):\n        try:\n            self.store.delete_key(f\"arrival_{barrier_id}_{i}\")\n        except Exception:\n            logger.debug(\"Error deleting key: %s\", f\"arrival_{barrier_id}_{i}\")\n\n        try:\n            self.store.delete_key(f\"departure_{barrier_id}_{i}\")\n        except Exception:\n            logger.debug(\"Error deleting key: %s\", f\"departure_{barrier_id}_{i}\")",
      "language": "python"
    },
    {
      "code": "broadcast_obj(obj: Any | None, src: int) -> Any",
      "language": "rust"
    },
    {
      "code": "broadcast_obj(obj: Any | None, src: int) -> Any",
      "language": "rust"
    },
    {
      "code": "202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218",
      "language": "unknown"
    },
    {
      "code": "def broadcast_obj(self, obj: Any | None, src: int) -> Any:\n    \"\"\"Broadcast an object from a source rank to all other ranks.\n    It does not clean up after all ranks have received the object.\n    Use it for limited times, e.g., for initialization.\n    \"\"\"\n    if self.rank == src:\n        self.expire_data()\n        key = f\"broadcast_from/{src}/{self.broadcast_send_counter}\"\n        self.store.set(key, pickle.dumps(obj))\n        self.broadcast_send_counter += 1\n        self.entries.append((key, time.time()))\n        return obj\n    else:\n        key = f\"broadcast_from/{src}/{self.broadcast_recv_src_counter[src]}\"\n        recv_obj = pickle.loads(self.store.get(key))\n        self.broadcast_recv_src_counter[src] += 1\n        return recv_obj",
      "language": "python"
    },
    {
      "code": "def broadcast_obj(self, obj: Any | None, src: int) -> Any:\n    \"\"\"Broadcast an object from a source rank to all other ranks.\n    It does not clean up after all ranks have received the object.\n    Use it for limited times, e.g., for initialization.\n    \"\"\"\n    if self.rank == src:\n        self.expire_data()\n        key = f\"broadcast_from/{src}/{self.broadcast_send_counter}\"\n        self.store.set(key, pickle.dumps(obj))\n        self.broadcast_send_counter += 1\n        self.entries.append((key, time.time()))\n        return obj\n    else:\n        key = f\"broadcast_from/{src}/{self.broadcast_recv_src_counter[src]}\"\n        recv_obj = pickle.loads(self.store.get(key))\n        self.broadcast_recv_src_counter[src] += 1\n        return recv_obj",
      "language": "python"
    },
    {
      "code": "create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n    store_timeout: int = 300,\n) -> StatelessProcessGroup",
      "language": "typescript"
    },
    {
      "code": "create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n    store_timeout: int = 300,\n) -> StatelessProcessGroup",
      "language": "typescript"
    },
    {
      "code": "366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n    store_timeout: int = 300,\n) -> \"StatelessProcessGroup\":\n    \"\"\"A replacement for `torch.distributed.init_process_group` that does not\n    pollute the global state.\n\n    If we have process A and process B called `torch.distributed.init_process_group`\n    to form a group, and then we want to form another group with process A, B, C,\n    D, it is not possible in PyTorch, because process A and process B have already\n    formed a group, and process C and process D cannot join that group. This\n    function is a workaround for this issue.\n\n    `torch.distributed.init_process_group` is a global call, while this function\n    is a stateless call. It will return a `StatelessProcessGroup` object that can be\n    used for exchanging metadata. With this function, process A and process B\n    can call `StatelessProcessGroup.create` to form a group, and then process A, B,\n    C, and D can call `StatelessProcessGroup.create` to form another group.\n    \"\"\"  # noqa\n    launch_server = rank == 0\n    if launch_server:\n        # listen on the specified interface (instead of 0.0.0.0)\n        listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        listen_socket.bind((host, port))\n        listen_socket.listen()\n        listen_fd = listen_socket.fileno()\n    else:\n        listen_socket = None\n        listen_fd = None\n\n    store = TCPStore(\n        host_name=host,\n        port=port,\n        world_size=world_size,\n        is_master=launch_server,\n        timeout=timedelta(seconds=store_timeout),\n        use_libuv=False,  # for now: github.com/pytorch/pytorch/pull/150215\n        master_listen_fd=listen_fd,\n    )\n\n    return StatelessProcessGroup(\n        rank=rank,\n        world_size=world_size,\n        store=store,\n        socket=listen_socket,\n        data_expiration_seconds=data_expiration_seconds,\n    )",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef create(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    data_expiration_seconds: int = 3600,\n    store_timeout: int = 300,\n) -> \"StatelessProcessGroup\":\n    \"\"\"A replacement for `torch.distributed.init_process_group` that does not\n    pollute the global state.\n\n    If we have process A and process B called `torch.distributed.init_process_group`\n    to form a group, and then we want to form another group with process A, B, C,\n    D, it is not possible in PyTorch, because process A and process B have already\n    formed a group, and process C and process D cannot join that group. This\n    function is a workaround for this issue.\n\n    `torch.distributed.init_process_group` is a global call, while this function\n    is a stateless call. It will return a `StatelessProcessGroup` object that can be\n    used for exchanging metadata. With this function, process A and process B\n    can call `StatelessProcessGroup.create` to form a group, and then process A, B,\n    C, and D can call `StatelessProcessGroup.create` to form another group.\n    \"\"\"  # noqa\n    launch_server = rank == 0\n    if launch_server:\n        # listen on the specified interface (instead of 0.0.0.0)\n        listen_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        listen_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        listen_socket.bind((host, port))\n        listen_socket.listen()\n        listen_fd = listen_socket.fileno()\n    else:\n        listen_socket = None\n        listen_fd = None\n\n    store = TCPStore(\n        host_name=host,\n        port=port,\n        world_size=world_size,\n        is_master=launch_server,\n        timeout=timedelta(seconds=store_timeout),\n        use_libuv=False,  # for now: github.com/pytorch/pytorch/pull/150215\n        master_listen_fd=listen_fd,\n    )\n\n    return StatelessProcessGroup(\n        rank=rank,\n        world_size=world_size,\n        store=store,\n        socket=listen_socket,\n        data_expiration_seconds=data_expiration_seconds,\n    )",
      "language": "python"
    },
    {
      "code": "expire_data()",
      "language": "unknown"
    },
    {
      "code": "expire_data()",
      "language": "unknown"
    },
    {
      "code": "183\n184\n185\n186\n187\n188\n189\n190\n191\n192",
      "language": "unknown"
    },
    {
      "code": "def expire_data(self):\n    \"\"\"Expire data that is older than `data_expiration_seconds` seconds.\"\"\"\n    while self.entries:\n        # check the oldest entry\n        key, timestamp = self.entries[0]\n        if time.time() - timestamp > self.data_expiration_seconds:\n            self.store.delete_key(key)\n            self.entries.popleft()\n        else:\n            break",
      "language": "python"
    },
    {
      "code": "def expire_data(self):\n    \"\"\"Expire data that is older than `data_expiration_seconds` seconds.\"\"\"\n    while self.entries:\n        # check the oldest entry\n        key, timestamp = self.entries[0]\n        if time.time() - timestamp > self.data_expiration_seconds:\n            self.store.delete_key(key)\n            self.entries.popleft()\n        else:\n            break",
      "language": "python"
    },
    {
      "code": "recv_obj(src: int) -> Any",
      "language": "php"
    },
    {
      "code": "recv_obj(src: int) -> Any",
      "language": "php"
    },
    {
      "code": "194\n195\n196\n197\n198\n199\n200",
      "language": "unknown"
    },
    {
      "code": "def recv_obj(self, src: int) -> Any:\n    \"\"\"Receive an object from a source rank.\"\"\"\n    obj = pickle.loads(\n        self.store.get(f\"send_to/{self.rank}/{self.recv_src_counter[src]}\")\n    )\n    self.recv_src_counter[src] += 1\n    return obj",
      "language": "python"
    },
    {
      "code": "def recv_obj(self, src: int) -> Any:\n    \"\"\"Receive an object from a source rank.\"\"\"\n    obj = pickle.loads(\n        self.store.get(f\"send_to/{self.rank}/{self.recv_src_counter[src]}\")\n    )\n    self.recv_src_counter[src] += 1\n    return obj",
      "language": "python"
    },
    {
      "code": "send_obj(obj: Any, dst: int)",
      "language": "unknown"
    },
    {
      "code": "send_obj(obj: Any, dst: int)",
      "language": "unknown"
    },
    {
      "code": "175\n176\n177\n178\n179\n180\n181",
      "language": "unknown"
    },
    {
      "code": "def send_obj(self, obj: Any, dst: int):\n    \"\"\"Send an object to a destination rank.\"\"\"\n    self.expire_data()\n    key = f\"send_to/{dst}/{self.send_dst_counter[dst]}\"\n    self.store.set(key, pickle.dumps(obj))\n    self.send_dst_counter[dst] += 1\n    self.entries.append((key, time.time()))",
      "language": "python"
    },
    {
      "code": "def send_obj(self, obj: Any, dst: int):\n    \"\"\"Send an object to a destination rank.\"\"\"\n    self.expire_data()\n    key = f\"send_to/{dst}/{self.send_dst_counter[dst]}\"\n    self.store.set(key, pickle.dumps(obj))\n    self.send_dst_counter[dst] += 1\n    self.entries.append((key, time.time()))",
      "language": "python"
    },
    {
      "code": "divide(numerator, denominator)",
      "language": "unknown"
    },
    {
      "code": "divide(numerator, denominator)",
      "language": "unknown"
    },
    {
      "code": "60\n61\n62\n63\n64",
      "language": "unknown"
    },
    {
      "code": "def divide(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator",
      "language": "python"
    },
    {
      "code": "def divide(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator",
      "language": "python"
    },
    {
      "code": "ensure_divisibility(numerator, denominator)",
      "language": "unknown"
    },
    {
      "code": "ensure_divisibility(numerator, denominator)",
      "language": "unknown"
    },
    {
      "code": "53\n54\n55\n56\n57",
      "language": "unknown"
    },
    {
      "code": "def ensure_divisibility(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n        numerator, denominator\n    )",
      "language": "python"
    },
    {
      "code": "def ensure_divisibility(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n        numerator, denominator\n    )",
      "language": "python"
    },
    {
      "code": "get_pp_indices(\n    num_hidden_layers: int, pp_rank: int, pp_size: int\n) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "get_pp_indices(\n    num_hidden_layers: int, pp_rank: int, pp_size: int\n) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140",
      "language": "unknown"
    },
    {
      "code": "def get_pp_indices(\n    num_hidden_layers: int, pp_rank: int, pp_size: int\n) -> tuple[int, int]:\n    \"\"\"Try to evenly distribute layers across partitions.\n\n    If the number of layers is not divisible by the number of partitions,\n    the remaining layers are evenly distributed across all but the last\n    partition. The last partition is excluded because it often contains an\n    additional norm layer and we are attempting to balance compute.\n\n    If `pp_size > 2` and the number of remaining layers is\n    `0 < x <= pp_size - 2` then the remaining layers are evenly distributed\n    across the middle partitions. The first and last partitions are excluded\n    because they contain the input and output embeddings respectively and we\n    are attempting to reduce maximum memory consumption across partitions.\n    \"\"\"\n    partition_list_str = envs.VLLM_PP_LAYER_PARTITION\n    if partition_list_str is not None:\n        try:\n            partitions = [int(layer) for layer in partition_list_str.split(\",\")]\n        except ValueError as err:\n            raise ValueError(\n                \"Invalid partition string: {}\".format(partition_list_str)\n            ) from err\n        if len(partitions) != pp_size:\n            raise ValueError(f\"{len(partitions)=} does not match {pp_size=}.\")\n        if sum(partitions) != num_hidden_layers:\n            raise ValueError(f\"{sum(partitions)=} does not match {num_hidden_layers=}.\")\n    else:\n        layers_per_partition = num_hidden_layers // pp_size\n        partitions = [layers_per_partition for _ in range(pp_size)]\n\n        if remaining_layers := num_hidden_layers % pp_size:\n            for i in range(2, remaining_layers + 2):\n                partitions[-i] += 1\n            logger.info(\n                \"Hidden layers were unevenly partitioned: [%s]. \"\n                \"This can be manually overridden using the \"\n                \"VLLM_PP_LAYER_PARTITION environment variable\",\n                \",\".join(str(p) for p in partitions),\n            )\n\n    start_layer = sum(partitions[:pp_rank])\n    end_layer = start_layer + partitions[pp_rank]\n\n    return (start_layer, end_layer)",
      "language": "typescript"
    },
    {
      "code": "def get_pp_indices(\n    num_hidden_layers: int, pp_rank: int, pp_size: int\n) -> tuple[int, int]:\n    \"\"\"Try to evenly distribute layers across partitions.\n\n    If the number of layers is not divisible by the number of partitions,\n    the remaining layers are evenly distributed across all but the last\n    partition. The last partition is excluded because it often contains an\n    additional norm layer and we are attempting to balance compute.\n\n    If `pp_size > 2` and the number of remaining layers is\n    `0 < x <= pp_size - 2` then the remaining layers are evenly distributed\n    across the middle partitions. The first and last partitions are excluded\n    because they contain the input and output embeddings respectively and we\n    are attempting to reduce maximum memory consumption across partitions.\n    \"\"\"\n    partition_list_str = envs.VLLM_PP_LAYER_PARTITION\n    if partition_list_str is not None:\n        try:\n            partitions = [int(layer) for layer in partition_list_str.split(\",\")]\n        except ValueError as err:\n            raise ValueError(\n                \"Invalid partition string: {}\".format(partition_list_str)\n            ) from err\n        if len(partitions) != pp_size:\n            raise ValueError(f\"{len(partitions)=} does not match {pp_size=}.\")\n        if sum(partitions) != num_hidden_layers:\n            raise ValueError(f\"{sum(partitions)=} does not match {num_hidden_layers=}.\")\n    else:\n        layers_per_partition = num_hidden_layers // pp_size\n        partitions = [layers_per_partition for _ in range(pp_size)]\n\n        if remaining_layers := num_hidden_layers % pp_size:\n            for i in range(2, remaining_layers + 2):\n                partitions[-i] += 1\n            logger.info(\n                \"Hidden layers were unevenly partitioned: [%s]. \"\n                \"This can be manually overridden using the \"\n                \"VLLM_PP_LAYER_PARTITION environment variable\",\n                \",\".join(str(p) for p in partitions),\n            )\n\n    start_layer = sum(partitions[:pp_rank])\n    end_layer = start_layer + partitions[pp_rank]\n\n    return (start_layer, end_layer)",
      "language": "typescript"
    },
    {
      "code": "init_gloo_process_group(\n    prefix_store: PrefixStore,\n    group_rank: int,\n    group_size: int,\n    timeout: timedelta,\n) -> ProcessGroup",
      "language": "php"
    },
    {
      "code": "init_gloo_process_group(\n    prefix_store: PrefixStore,\n    group_rank: int,\n    group_size: int,\n    timeout: timedelta,\n) -> ProcessGroup",
      "language": "php"
    },
    {
      "code": "421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459",
      "language": "unknown"
    },
    {
      "code": "def init_gloo_process_group(\n    prefix_store: PrefixStore,\n    group_rank: int,\n    group_size: int,\n    timeout: timedelta,\n) -> ProcessGroup:\n    \"\"\"\n    Stateless init ProcessGroup with gloo backend compatible with\n    different torch versions.\n    \"\"\"\n    with suppress_stdout():\n        if is_torch_equal_or_newer(\"2.6\"):\n            pg = ProcessGroup(\n                prefix_store,\n                group_rank,\n                group_size,\n            )\n        else:\n            options = ProcessGroup.Options(backend=\"gloo\")\n            pg = ProcessGroup(\n                prefix_store,\n                group_rank,\n                group_size,\n                options,\n            )\n        from torch.distributed.distributed_c10d import ProcessGroupGloo\n\n        backend_class = ProcessGroupGloo(\n            prefix_store, group_rank, group_size, timeout=timeout\n        )\n        backend_type = ProcessGroup.BackendType.GLOO\n        device = torch.device(\"cpu\")\n        if is_torch_equal_or_newer(\"2.6\"):\n            # _set_default_backend is supported in torch >= 2.6\n            pg._set_default_backend(backend_type)\n        backend_class._set_sequence_number_for_group()\n\n        pg._register_backend(device, backend_type, backend_class)\n    return pg",
      "language": "python"
    },
    {
      "code": "def init_gloo_process_group(\n    prefix_store: PrefixStore,\n    group_rank: int,\n    group_size: int,\n    timeout: timedelta,\n) -> ProcessGroup:\n    \"\"\"\n    Stateless init ProcessGroup with gloo backend compatible with\n    different torch versions.\n    \"\"\"\n    with suppress_stdout():\n        if is_torch_equal_or_newer(\"2.6\"):\n            pg = ProcessGroup(\n                prefix_store,\n                group_rank,\n                group_size,\n            )\n        else:\n            options = ProcessGroup.Options(backend=\"gloo\")\n            pg = ProcessGroup(\n                prefix_store,\n                group_rank,\n                group_size,\n                options,\n            )\n        from torch.distributed.distributed_c10d import ProcessGroupGloo\n\n        backend_class = ProcessGroupGloo(\n            prefix_store, group_rank, group_size, timeout=timeout\n        )\n        backend_type = ProcessGroup.BackendType.GLOO\n        device = torch.device(\"cpu\")\n        if is_torch_equal_or_newer(\"2.6\"):\n            # _set_default_backend is supported in torch >= 2.6\n            pg._set_default_backend(backend_type)\n        backend_class._set_sequence_number_for_group()\n\n        pg._register_backend(device, backend_type, backend_class)\n    return pg",
      "language": "python"
    },
    {
      "code": "sched_yield()",
      "language": "unknown"
    },
    {
      "code": "sched_yield()",
      "language": "unknown"
    },
    {
      "code": "46\n47\n48\n49\n50",
      "language": "unknown"
    },
    {
      "code": "def sched_yield():\n    if USE_SCHED_YIELD:\n        os.sched_yield()\n    else:\n        time.sleep(0)",
      "language": "python"
    },
    {
      "code": "def sched_yield():\n    if USE_SCHED_YIELD:\n        os.sched_yield()\n    else:\n        time.sleep(0)",
      "language": "python"
    },
    {
      "code": "split_tensor_along_last_dim(\n    tensor: Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -> Sequence[Tensor]",
      "language": "typescript"
    },
    {
      "code": "split_tensor_along_last_dim(\n    tensor: Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -> Sequence[Tensor]",
      "language": "typescript"
    },
    {
      "code": "67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92",
      "language": "unknown"
    },
    {
      "code": "def split_tensor_along_last_dim(\n    tensor: torch.Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -> Sequence[torch.Tensor]:\n    \"\"\"Split a tensor along its last dimension.\n\n    Arguments:\n        tensor: input tensor.\n        num_partitions: number of partitions to split the tensor\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                 in memory.\n\n    Returns:\n        A list of Tensors\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # NOTE: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list",
      "language": "python"
    },
    {
      "code": "def split_tensor_along_last_dim(\n    tensor: torch.Tensor,\n    num_partitions: int,\n    contiguous_split_chunks: bool = False,\n) -> Sequence[torch.Tensor]:\n    \"\"\"Split a tensor along its last dimension.\n\n    Arguments:\n        tensor: input tensor.\n        num_partitions: number of partitions to split the tensor\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                 in memory.\n\n    Returns:\n        A list of Tensors\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # NOTE: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list",
      "language": "python"
    },
    {
      "code": "stateless_destroy_torch_distributed_process_group(\n    pg: ProcessGroup,\n) -> None",
      "language": "rust"
    },
    {
      "code": "stateless_destroy_torch_distributed_process_group(\n    pg: ProcessGroup,\n) -> None",
      "language": "rust"
    },
    {
      "code": "532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545",
      "language": "unknown"
    },
    {
      "code": "def stateless_destroy_torch_distributed_process_group(pg: ProcessGroup) -> None:\n    \"\"\"\n    Destroy ProcessGroup returned by\n        stateless_init_torch_distributed_process_group().\n    \"\"\"\n    if is_torch_equal_or_newer(\"2.7\"):\n        pg.shutdown()\n    else:\n        # Lazy import for non-CUDA backends.\n        from torch.distributed.distributed_c10d import _shutdown_backend\n\n        _shutdown_backend(pg)\n\n    _unregister_process_group(pg.group_name)",
      "language": "python"
    },
    {
      "code": "def stateless_destroy_torch_distributed_process_group(pg: ProcessGroup) -> None:\n    \"\"\"\n    Destroy ProcessGroup returned by\n        stateless_init_torch_distributed_process_group().\n    \"\"\"\n    if is_torch_equal_or_newer(\"2.7\"):\n        pg.shutdown()\n    else:\n        # Lazy import for non-CUDA backends.\n        from torch.distributed.distributed_c10d import _shutdown_backend\n\n        _shutdown_backend(pg)\n\n    _unregister_process_group(pg.group_name)",
      "language": "python"
    },
    {
      "code": "stateless_init_torch_distributed_process_group(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    backend: str,\n) -> ProcessGroup",
      "language": "php"
    },
    {
      "code": "stateless_init_torch_distributed_process_group(\n    host: str,\n    port: int,\n    rank: int,\n    world_size: int,\n    backend: str,\n) -> ProcessGroup",
      "language": "php"
    },
    {
      "code": "462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529",
      "language": "unknown"
    },
    {
      "code": "def stateless_init_torch_distributed_process_group(\n    host: str, port: int, rank: int, world_size: int, backend: str\n) -> ProcessGroup:\n    \"\"\"\n    A replacement for `torch.distributed.init_process_group` that does not\n    pollute the global state. The created ProcessGroup object can be used for\n    some operations such as `allreduce`, because it does not depend on the\n    global rank. However, some operations such as `broadcast` cannot be used\n    because it depends on the global rank.\n\n    # TODO: ask for help from PyTorch team if we need the `broadcast` operation.\n\n    This function is useful when we are not sure about the total number of\n    processes in the process group. For example, we may have process\n    1, 2, ..., 8 who want to communicate, and process 9 might be the same\n    process as process 1, or it might be a different process; process 10\n    might be the same process as process 5, or it might be a different process.\n    In this case, how can we reliably form a communication channel within\n    process 9 and 10, without affecting the communication channel within\n    process 1, 2, ..., 8?\n\n    One possible solution is to figure out if process 9 and 10 are the same\n    as process 1 and 5 beforehand, and then form a communication channel\n    based on the information, adjusting the ranks and world_size etc. However,\n    figuring out the information is not always easy, and it will interfere\n    with the main communication channel.\n\n    Our solution is to always form a communication channel with process 1, 2,\n    ..., 8, and then use this function to form another communication channel\n    with process 9 and 10. This way, regardless of whether process 9 and 10\n    are the same as process 1 and 5, the main communication channel is\n    always formed with process 1, 2, ..., 8, and the additional communication\n    channel is formed with process 9 and 10.\n    \"\"\"\n    init_method = get_tcp_uri(host, port)\n    backend = Backend(backend)  # it is basically string\n    timeout = _get_default_timeout(backend)\n\n    store, rank, world_size = next(\n        rendezvous(init_method, rank, world_size, timeout=timeout)\n    )\n    store.set_timeout(timeout)\n\n    group_rank = rank\n    group_size = world_size\n\n    # Use a PrefixStore to avoid accidental overrides of keys used by\n    # different systems (e.g. RPC) in case the store is multi-tenant.\n    prefix_store = PrefixStore(init_method, store)\n    try:\n        from vllm.platforms import current_platform\n\n        return current_platform.stateless_init_device_torch_dist_pg(\n            backend=backend,\n            prefix_store=prefix_store,\n            group_rank=group_rank,\n            group_size=group_size,\n            timeout=timeout,\n        )\n    except NotImplementedError:\n        # If platform doesn't implement stateless_init_device_torch_dist_pg, it\n        # will raise a NotImplementedError. In this case, we fall back to gloo.\n        return init_gloo_process_group(\n            prefix_store=prefix_store,\n            group_rank=group_rank,\n            group_size=group_size,\n            timeout=timeout,\n        )",
      "language": "python"
    },
    {
      "code": "def stateless_init_torch_distributed_process_group(\n    host: str, port: int, rank: int, world_size: int, backend: str\n) -> ProcessGroup:\n    \"\"\"\n    A replacement for `torch.distributed.init_process_group` that does not\n    pollute the global state. The created ProcessGroup object can be used for\n    some operations such as `allreduce`, because it does not depend on the\n    global rank. However, some operations such as `broadcast` cannot be used\n    because it depends on the global rank.\n\n    # TODO: ask for help from PyTorch team if we need the `broadcast` operation.\n\n    This function is useful when we are not sure about the total number of\n    processes in the process group. For example, we may have process\n    1, 2, ..., 8 who want to communicate, and process 9 might be the same\n    process as process 1, or it might be a different process; process 10\n    might be the same process as process 5, or it might be a different process.\n    In this case, how can we reliably form a communication channel within\n    process 9 and 10, without affecting the communication channel within\n    process 1, 2, ..., 8?\n\n    One possible solution is to figure out if process 9 and 10 are the same\n    as process 1 and 5 beforehand, and then form a communication channel\n    based on the information, adjusting the ranks and world_size etc. However,\n    figuring out the information is not always easy, and it will interfere\n    with the main communication channel.\n\n    Our solution is to always form a communication channel with process 1, 2,\n    ..., 8, and then use this function to form another communication channel\n    with process 9 and 10. This way, regardless of whether process 9 and 10\n    are the same as process 1 and 5, the main communication channel is\n    always formed with process 1, 2, ..., 8, and the additional communication\n    channel is formed with process 9 and 10.\n    \"\"\"\n    init_method = get_tcp_uri(host, port)\n    backend = Backend(backend)  # it is basically string\n    timeout = _get_default_timeout(backend)\n\n    store, rank, world_size = next(\n        rendezvous(init_method, rank, world_size, timeout=timeout)\n    )\n    store.set_timeout(timeout)\n\n    group_rank = rank\n    group_size = world_size\n\n    # Use a PrefixStore to avoid accidental overrides of keys used by\n    # different systems (e.g. RPC) in case the store is multi-tenant.\n    prefix_store = PrefixStore(init_method, store)\n    try:\n        from vllm.platforms import current_platform\n\n        return current_platform.stateless_init_device_torch_dist_pg(\n            backend=backend,\n            prefix_store=prefix_store,\n            group_rank=group_rank,\n            group_size=group_size,\n            timeout=timeout,\n        )\n    except NotImplementedError:\n        # If platform doesn't implement stateless_init_device_torch_dist_pg, it\n        # will raise a NotImplementedError. In this case, we fall back to gloo.\n        return init_gloo_process_group(\n            prefix_store=prefix_store,\n            group_rank=group_rank,\n            group_size=group_size,\n            timeout=timeout,\n        )",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}