{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
  "title": "protocol - vLLM",
  "content": "Bases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: ChatCompletionLogProb\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nDeprecated: use reasoning instead.\n\nCopy reasoning to reasoning_content for backward compatibility.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nDeprecated: use reasoning instead.\n\nCopy reasoning to reasoning_content for backward compatibility.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nThe processed MM inputs for the model.\n\nThe sampling parameters for the model.\n\nThe token ids to generate text from.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: ResponseCompletedEvent\n\nBases: ResponseCreatedEvent\n\nBases: OpenAIBaseModel\n\nBases: ResponseInProgressEvent\n\nBases: OpenAIBaseModel\n\nClass to show the raw message. If message / tokens diverge, tokens is the source of truth\n\nBases: OpenAIBaseModel\n\nThe index of the content part that is done.\n\nThe ID of the output item that the content part was added to.\n\nThe index of the output item that the content part was added to.\n\nThe content part that is done.\n\nThe sequence number of this event.\n\nThe type of the event. Always response.reasoning_part.added.\n\nBases: OpenAIBaseModel\n\nThe index of the content part that is done.\n\nThe ID of the output item that the content part was added to.\n\nThe index of the output item that the content part was added to.\n\nThe content part that is done.\n\nThe sequence number of this event.\n\nThe type of the event. Always response.reasoning_part.done.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nParse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages.\n\nCheck if the request includes output logprobs.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nResponse containing tokenizer configuration equivalent to tokenizer_config.json\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nThe audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n\nThe frequency penalty to use for sampling.\n\nThe language of the input audio.\n\nSupplying the input language in ISO-639-1 format will improve accuracy and latency.\n\nThe maximum number of tokens to generate.\n\nFilters out tokens with a probability lower than min_p, ensuring a minimum likelihood threshold during sampling.\n\nID of the model to use.\n\nThe presence penalty to use for sampling.\n\nAn optional text to guide the model's style or continue a previous audio segment.\n\nThe prompt should match the audio language.\n\nThe repetition penalty to use for sampling.\n\nThe format of the output, in one of these options: json, text, srt, verbose_json, or vtt.\n\nThe seed to use for sampling.\n\nWhen set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint.\n\nThe sampling temperature, between 0 and 1.\n\nHigher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\n\nThe timestamp granularities to populate for this transcription.\n\nresponse_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.\n\nThe language of the output audio we transcribe to.\n\nPlease note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api.\n\nLimits sampling to the k most probable tokens at each step.\n\nEnables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds p.\n\nBases: OpenAIBaseModel\n\nThe transcribed text.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nThe duration of the input audio.\n\nThe language of the input audio.\n\nSegments of the transcribed text and their corresponding details.\n\nThe transcribed text.\n\nExtracted words and their corresponding timestamps.\n\nBases: OpenAIBaseModel\n\nAverage logprob of the segment.\n\nIf the value is lower than -1, consider the logprobs failed.\n\nCompression ratio of the segment.\n\nIf the value is greater than 2.4, consider the compression failed.\n\nEnd time of the segment in seconds.\n\nUnique identifier of the segment.\n\nProbability of no speech in the segment.\n\nIf the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent.\n\nSeek offset of the segment.\n\nStart time of the segment in seconds.\n\nTemperature parameter used for generating the segment.\n\nText content of the segment.\n\nArray of token IDs for the text content.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nEnd time of the word in seconds.\n\nStart time of the word in seconds.\n\nThe text content of the word.\n\nBases: OpenAIBaseModel\n\nThe audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n\nThe language of the input audio we translate from.\n\nSupplying the input language in ISO-639-1 format will improve accuracy.\n\nThe maximum number of tokens to generate.\n\nID of the model to use.\n\nAn optional text to guide the model's style or continue a previous audio segment.\n\nThe prompt should match the audio language.\n\nThe format of the output, in one of these options: json, text, srt, verbose_json, or vtt.\n\nThe seed to use for sampling.\n\nCustom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint.\n\nThe sampling temperature, between 0 and 1.\n\nHigher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.\n\nThe language of the input audio we translate to.\n\nPlease note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports to_language=en.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nThe duration of the input audio.\n\nThe language of the input audio.\n\nSegments of the translated text and their corresponding details.\n\nExtracted words and their corresponding timestamps.\n\nBases: OpenAIBaseModel\n\nAverage logprob of the segment.\n\nIf the value is lower than -1, consider the logprobs failed.\n\nCompression ratio of the segment.\n\nIf the value is greater than 2.4, consider the compression failed.\n\nEnd time of the segment in seconds.\n\nUnique identifier of the segment.\n\nProbability of no speech in the segment.\n\nIf the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent.\n\nSeek offset of the segment.\n\nStart time of the segment in seconds.\n\nTemperature parameter used for generating the segment.\n\nText content of the segment.\n\nArray of token IDs for the text content.\n\nBases: OpenAIBaseModel\n\nBases: OpenAIBaseModel\n\nEnd time of the word in seconds.\n\nStart time of the word in seconds.\n\nThe text content of the word.\n\nBases: OpenAIBaseModel\n\nvLLM-specific validation error for request validation failures.\n\nThe error message describing the validation failure.\n\nOptional parameter name that failed validation.\n\nOptional value that was rejected during validation.\n\nSerializes a single message\n\nSerializes multiple messages",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.entrypoints.openai.protocol ¶",
      "id": "vllm.entrypoints.openai.protocol"
    },
    {
      "level": "h2",
      "text": "AnyResponseFormat module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.AnyResponseFormat"
    },
    {
      "level": "h2",
      "text": "AnyStructuralTagResponseFormat module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.AnyStructuralTagResponseFormat"
    },
    {
      "level": "h2",
      "text": "AudioResponseFormat module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.AudioResponseFormat"
    },
    {
      "level": "h2",
      "text": "LogitsProcessors module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LogitsProcessors"
    },
    {
      "level": "h2",
      "text": "ResponseInputOutputItem module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseInputOutputItem"
    },
    {
      "level": "h2",
      "text": "ResponseInputOutputMessage module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseInputOutputMessage"
    },
    {
      "level": "h2",
      "text": "StreamingResponsesResponse module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.StreamingResponsesResponse"
    },
    {
      "level": "h2",
      "text": "TokenizeRequest module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeRequest"
    },
    {
      "level": "h2",
      "text": "TranscriptionResponseVariant module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseVariant"
    },
    {
      "level": "h2",
      "text": "TranslationResponseVariant module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseVariant"
    },
    {
      "level": "h2",
      "text": "_LONG_INFO module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol._LONG_INFO"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.logger"
    },
    {
      "level": "h2",
      "text": "ChatCompletionLogProb ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProb"
    },
    {
      "level": "h3",
      "text": "bytes class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProb.bytes"
    },
    {
      "level": "h3",
      "text": "logprob class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProb.logprob"
    },
    {
      "level": "h3",
      "text": "token instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProb.token"
    },
    {
      "level": "h2",
      "text": "ChatCompletionLogProbs ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProbs"
    },
    {
      "level": "h3",
      "text": "content class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProbs.content"
    },
    {
      "level": "h2",
      "text": "ChatCompletionLogProbsContent ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProbsContent"
    },
    {
      "level": "h3",
      "text": "field_names class-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProbsContent.field_names"
    },
    {
      "level": "h3",
      "text": "top_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionLogProbsContent.top_logprobs"
    },
    {
      "level": "h2",
      "text": "ChatCompletionNamedFunction ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionNamedFunction"
    },
    {
      "level": "h3",
      "text": "name instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionNamedFunction.name"
    },
    {
      "level": "h2",
      "text": "ChatCompletionNamedToolChoiceParam ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionNamedToolChoiceParam"
    },
    {
      "level": "h3",
      "text": "function instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionNamedToolChoiceParam.function"
    },
    {
      "level": "h3",
      "text": "type class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionNamedToolChoiceParam.type"
    },
    {
      "level": "h2",
      "text": "ChatCompletionRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest"
    },
    {
      "level": "h3",
      "text": "_DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest._DEFAULT_SAMPLING_PARAMS"
    },
    {
      "level": "h3",
      "text": "add_generation_prompt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.add_generation_prompt"
    },
    {
      "level": "h3",
      "text": "add_special_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.add_special_tokens"
    },
    {
      "level": "h3",
      "text": "allowed_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.allowed_token_ids"
    },
    {
      "level": "h3",
      "text": "bad_words class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.bad_words"
    },
    {
      "level": "h3",
      "text": "cache_salt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.cache_salt"
    },
    {
      "level": "h3",
      "text": "chat_template class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.chat_template"
    },
    {
      "level": "h3",
      "text": "chat_template_kwargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.chat_template_kwargs"
    },
    {
      "level": "h3",
      "text": "continue_final_message class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.continue_final_message"
    },
    {
      "level": "h3",
      "text": "documents class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.documents"
    },
    {
      "level": "h3",
      "text": "echo class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.echo"
    },
    {
      "level": "h3",
      "text": "frequency_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.frequency_penalty"
    },
    {
      "level": "h3",
      "text": "ignore_eos class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.ignore_eos"
    },
    {
      "level": "h3",
      "text": "include_reasoning class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.include_reasoning"
    },
    {
      "level": "h3",
      "text": "include_stop_str_in_output class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.include_stop_str_in_output"
    },
    {
      "level": "h3",
      "text": "kv_transfer_params class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.kv_transfer_params"
    },
    {
      "level": "h3",
      "text": "length_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.length_penalty"
    },
    {
      "level": "h3",
      "text": "logit_bias class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.logit_bias"
    },
    {
      "level": "h3",
      "text": "logits_processors class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.logits_processors"
    },
    {
      "level": "h3",
      "text": "logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.logprobs"
    },
    {
      "level": "h3",
      "text": "max_completion_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.max_completion_tokens"
    },
    {
      "level": "h3",
      "text": "max_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.max_tokens"
    },
    {
      "level": "h3",
      "text": "messages instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.messages"
    },
    {
      "level": "h3",
      "text": "min_p class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.min_p"
    },
    {
      "level": "h3",
      "text": "min_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.min_tokens"
    },
    {
      "level": "h3",
      "text": "mm_processor_kwargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.mm_processor_kwargs"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.model"
    },
    {
      "level": "h3",
      "text": "n class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.n"
    },
    {
      "level": "h3",
      "text": "parallel_tool_calls class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.parallel_tool_calls"
    },
    {
      "level": "h3",
      "text": "presence_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.presence_penalty"
    },
    {
      "level": "h3",
      "text": "priority class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.priority"
    },
    {
      "level": "h3",
      "text": "prompt_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.prompt_logprobs"
    },
    {
      "level": "h3",
      "text": "reasoning_effort class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.reasoning_effort"
    },
    {
      "level": "h3",
      "text": "repetition_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.repetition_penalty"
    },
    {
      "level": "h3",
      "text": "request_id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.request_id"
    },
    {
      "level": "h3",
      "text": "response_format class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.response_format"
    },
    {
      "level": "h3",
      "text": "return_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.return_token_ids"
    },
    {
      "level": "h3",
      "text": "return_tokens_as_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.return_tokens_as_token_ids"
    },
    {
      "level": "h3",
      "text": "seed class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.seed"
    },
    {
      "level": "h3",
      "text": "skip_special_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.skip_special_tokens"
    },
    {
      "level": "h3",
      "text": "spaces_between_special_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.spaces_between_special_tokens"
    },
    {
      "level": "h3",
      "text": "stop class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.stop"
    },
    {
      "level": "h3",
      "text": "stop_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.stop_token_ids"
    },
    {
      "level": "h3",
      "text": "stream class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.stream"
    },
    {
      "level": "h3",
      "text": "stream_options class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.stream_options"
    },
    {
      "level": "h3",
      "text": "structured_outputs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.structured_outputs"
    },
    {
      "level": "h3",
      "text": "temperature class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.temperature"
    },
    {
      "level": "h3",
      "text": "tool_choice class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.tool_choice"
    },
    {
      "level": "h3",
      "text": "tools class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.tools"
    },
    {
      "level": "h3",
      "text": "top_k class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.top_k"
    },
    {
      "level": "h3",
      "text": "top_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.top_logprobs"
    },
    {
      "level": "h3",
      "text": "top_p class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.top_p"
    },
    {
      "level": "h3",
      "text": "truncate_prompt_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.truncate_prompt_tokens"
    },
    {
      "level": "h3",
      "text": "use_beam_search class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.use_beam_search"
    },
    {
      "level": "h3",
      "text": "user class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.user"
    },
    {
      "level": "h3",
      "text": "vllm_xargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.vllm_xargs"
    },
    {
      "level": "h3",
      "text": "check_cache_salt_support classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.check_cache_salt_support"
    },
    {
      "level": "h3",
      "text": "check_generation_prompt classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.check_generation_prompt"
    },
    {
      "level": "h3",
      "text": "check_logprobs classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.check_logprobs"
    },
    {
      "level": "h3",
      "text": "check_structured_outputs_count classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.check_structured_outputs_count"
    },
    {
      "level": "h3",
      "text": "check_tool_usage classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.check_tool_usage"
    },
    {
      "level": "h3",
      "text": "to_beam_search_params ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.to_beam_search_params"
    },
    {
      "level": "h3",
      "text": "to_sampling_params ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.to_sampling_params"
    },
    {
      "level": "h3",
      "text": "validate_stream_options classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionRequest.validate_stream_options"
    },
    {
      "level": "h2",
      "text": "ChatCompletionResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse"
    },
    {
      "level": "h3",
      "text": "choices instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.choices"
    },
    {
      "level": "h3",
      "text": "created class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.created"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.id"
    },
    {
      "level": "h3",
      "text": "kv_transfer_params class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.kv_transfer_params"
    },
    {
      "level": "h3",
      "text": "model instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.model"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.object"
    },
    {
      "level": "h3",
      "text": "prompt_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.prompt_logprobs"
    },
    {
      "level": "h3",
      "text": "prompt_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.prompt_token_ids"
    },
    {
      "level": "h3",
      "text": "service_tier class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.service_tier"
    },
    {
      "level": "h3",
      "text": "system_fingerprint class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.system_fingerprint"
    },
    {
      "level": "h3",
      "text": "usage instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponse.usage"
    },
    {
      "level": "h2",
      "text": "ChatCompletionResponseChoice ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseChoice"
    },
    {
      "level": "h3",
      "text": "finish_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseChoice.finish_reason"
    },
    {
      "level": "h3",
      "text": "index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseChoice.index"
    },
    {
      "level": "h3",
      "text": "logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseChoice.logprobs"
    },
    {
      "level": "h3",
      "text": "message instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseChoice.message"
    },
    {
      "level": "h3",
      "text": "stop_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseChoice.stop_reason"
    },
    {
      "level": "h3",
      "text": "token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseChoice.token_ids"
    },
    {
      "level": "h2",
      "text": "ChatCompletionResponseStreamChoice ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseStreamChoice"
    },
    {
      "level": "h3",
      "text": "delta instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseStreamChoice.delta"
    },
    {
      "level": "h3",
      "text": "finish_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseStreamChoice.finish_reason"
    },
    {
      "level": "h3",
      "text": "index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseStreamChoice.index"
    },
    {
      "level": "h3",
      "text": "logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseStreamChoice.logprobs"
    },
    {
      "level": "h3",
      "text": "stop_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseStreamChoice.stop_reason"
    },
    {
      "level": "h3",
      "text": "token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionResponseStreamChoice.token_ids"
    },
    {
      "level": "h2",
      "text": "ChatCompletionStreamResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionStreamResponse"
    },
    {
      "level": "h3",
      "text": "choices instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionStreamResponse.choices"
    },
    {
      "level": "h3",
      "text": "created class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionStreamResponse.created"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionStreamResponse.id"
    },
    {
      "level": "h3",
      "text": "model instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionStreamResponse.model"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionStreamResponse.object"
    },
    {
      "level": "h3",
      "text": "prompt_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionStreamResponse.prompt_token_ids"
    },
    {
      "level": "h3",
      "text": "usage class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionStreamResponse.usage"
    },
    {
      "level": "h2",
      "text": "ChatCompletionToolsParam ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionToolsParam"
    },
    {
      "level": "h3",
      "text": "function instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionToolsParam.function"
    },
    {
      "level": "h3",
      "text": "type class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatCompletionToolsParam.type"
    },
    {
      "level": "h2",
      "text": "ChatMessage ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage"
    },
    {
      "level": "h3",
      "text": "annotations class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.annotations"
    },
    {
      "level": "h3",
      "text": "audio class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.audio"
    },
    {
      "level": "h3",
      "text": "content class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.content"
    },
    {
      "level": "h3",
      "text": "function_call class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.function_call"
    },
    {
      "level": "h3",
      "text": "reasoning class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.reasoning"
    },
    {
      "level": "h3",
      "text": "reasoning_content class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.reasoning_content"
    },
    {
      "level": "h3",
      "text": "refusal class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.refusal"
    },
    {
      "level": "h3",
      "text": "role instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.role"
    },
    {
      "level": "h3",
      "text": "tool_calls class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.tool_calls"
    },
    {
      "level": "h3",
      "text": "handle_deprecated_reasoning_content ¶",
      "id": "vllm.entrypoints.openai.protocol.ChatMessage.handle_deprecated_reasoning_content"
    },
    {
      "level": "h2",
      "text": "CompletionLogProbs ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionLogProbs"
    },
    {
      "level": "h3",
      "text": "text_offset class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionLogProbs.text_offset"
    },
    {
      "level": "h3",
      "text": "token_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionLogProbs.token_logprobs"
    },
    {
      "level": "h3",
      "text": "tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionLogProbs.tokens"
    },
    {
      "level": "h3",
      "text": "top_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionLogProbs.top_logprobs"
    },
    {
      "level": "h2",
      "text": "CompletionRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest"
    },
    {
      "level": "h3",
      "text": "_DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest._DEFAULT_SAMPLING_PARAMS"
    },
    {
      "level": "h3",
      "text": "add_special_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.add_special_tokens"
    },
    {
      "level": "h3",
      "text": "allowed_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.allowed_token_ids"
    },
    {
      "level": "h3",
      "text": "cache_salt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.cache_salt"
    },
    {
      "level": "h3",
      "text": "echo class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.echo"
    },
    {
      "level": "h3",
      "text": "frequency_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.frequency_penalty"
    },
    {
      "level": "h3",
      "text": "ignore_eos class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.ignore_eos"
    },
    {
      "level": "h3",
      "text": "include_stop_str_in_output class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.include_stop_str_in_output"
    },
    {
      "level": "h3",
      "text": "kv_transfer_params class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.kv_transfer_params"
    },
    {
      "level": "h3",
      "text": "length_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.length_penalty"
    },
    {
      "level": "h3",
      "text": "logit_bias class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.logit_bias"
    },
    {
      "level": "h3",
      "text": "logits_processors class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.logits_processors"
    },
    {
      "level": "h3",
      "text": "logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.logprobs"
    },
    {
      "level": "h3",
      "text": "max_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.max_tokens"
    },
    {
      "level": "h3",
      "text": "min_p class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.min_p"
    },
    {
      "level": "h3",
      "text": "min_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.min_tokens"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.model"
    },
    {
      "level": "h3",
      "text": "n class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.n"
    },
    {
      "level": "h3",
      "text": "presence_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.presence_penalty"
    },
    {
      "level": "h3",
      "text": "priority class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.priority"
    },
    {
      "level": "h3",
      "text": "prompt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.prompt"
    },
    {
      "level": "h3",
      "text": "prompt_embeds class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.prompt_embeds"
    },
    {
      "level": "h3",
      "text": "prompt_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.prompt_logprobs"
    },
    {
      "level": "h3",
      "text": "repetition_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.repetition_penalty"
    },
    {
      "level": "h3",
      "text": "request_id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.request_id"
    },
    {
      "level": "h3",
      "text": "response_format class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.response_format"
    },
    {
      "level": "h3",
      "text": "return_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.return_token_ids"
    },
    {
      "level": "h3",
      "text": "return_tokens_as_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.return_tokens_as_token_ids"
    },
    {
      "level": "h3",
      "text": "seed class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.seed"
    },
    {
      "level": "h3",
      "text": "skip_special_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.skip_special_tokens"
    },
    {
      "level": "h3",
      "text": "spaces_between_special_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.spaces_between_special_tokens"
    },
    {
      "level": "h3",
      "text": "stop class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.stop"
    },
    {
      "level": "h3",
      "text": "stop_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.stop_token_ids"
    },
    {
      "level": "h3",
      "text": "stream class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.stream"
    },
    {
      "level": "h3",
      "text": "stream_options class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.stream_options"
    },
    {
      "level": "h3",
      "text": "structured_outputs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.structured_outputs"
    },
    {
      "level": "h3",
      "text": "suffix class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.suffix"
    },
    {
      "level": "h3",
      "text": "temperature class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.temperature"
    },
    {
      "level": "h3",
      "text": "top_k class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.top_k"
    },
    {
      "level": "h3",
      "text": "top_p class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.top_p"
    },
    {
      "level": "h3",
      "text": "truncate_prompt_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.truncate_prompt_tokens"
    },
    {
      "level": "h3",
      "text": "use_beam_search class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.use_beam_search"
    },
    {
      "level": "h3",
      "text": "user class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.user"
    },
    {
      "level": "h3",
      "text": "vllm_xargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.vllm_xargs"
    },
    {
      "level": "h3",
      "text": "check_cache_salt_support classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.check_cache_salt_support"
    },
    {
      "level": "h3",
      "text": "check_logprobs classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.check_logprobs"
    },
    {
      "level": "h3",
      "text": "check_structured_outputs_count classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.check_structured_outputs_count"
    },
    {
      "level": "h3",
      "text": "to_beam_search_params ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.to_beam_search_params"
    },
    {
      "level": "h3",
      "text": "to_sampling_params ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.to_sampling_params"
    },
    {
      "level": "h3",
      "text": "validate_prompt_and_prompt_embeds classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.validate_prompt_and_prompt_embeds"
    },
    {
      "level": "h3",
      "text": "validate_stream_options classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionRequest.validate_stream_options"
    },
    {
      "level": "h2",
      "text": "CompletionResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse"
    },
    {
      "level": "h3",
      "text": "choices instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.choices"
    },
    {
      "level": "h3",
      "text": "created class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.created"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.id"
    },
    {
      "level": "h3",
      "text": "kv_transfer_params class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.kv_transfer_params"
    },
    {
      "level": "h3",
      "text": "model instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.model"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.object"
    },
    {
      "level": "h3",
      "text": "service_tier class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.service_tier"
    },
    {
      "level": "h3",
      "text": "system_fingerprint class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.system_fingerprint"
    },
    {
      "level": "h3",
      "text": "usage instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponse.usage"
    },
    {
      "level": "h2",
      "text": "CompletionResponseChoice ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice"
    },
    {
      "level": "h3",
      "text": "finish_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice.finish_reason"
    },
    {
      "level": "h3",
      "text": "index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice.index"
    },
    {
      "level": "h3",
      "text": "logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice.logprobs"
    },
    {
      "level": "h3",
      "text": "prompt_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice.prompt_logprobs"
    },
    {
      "level": "h3",
      "text": "prompt_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice.prompt_token_ids"
    },
    {
      "level": "h3",
      "text": "stop_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice.stop_reason"
    },
    {
      "level": "h3",
      "text": "text instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice.text"
    },
    {
      "level": "h3",
      "text": "token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseChoice.token_ids"
    },
    {
      "level": "h2",
      "text": "CompletionResponseStreamChoice ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseStreamChoice"
    },
    {
      "level": "h3",
      "text": "finish_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseStreamChoice.finish_reason"
    },
    {
      "level": "h3",
      "text": "index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseStreamChoice.index"
    },
    {
      "level": "h3",
      "text": "logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseStreamChoice.logprobs"
    },
    {
      "level": "h3",
      "text": "prompt_token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseStreamChoice.prompt_token_ids"
    },
    {
      "level": "h3",
      "text": "stop_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseStreamChoice.stop_reason"
    },
    {
      "level": "h3",
      "text": "text instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseStreamChoice.text"
    },
    {
      "level": "h3",
      "text": "token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionResponseStreamChoice.token_ids"
    },
    {
      "level": "h2",
      "text": "CompletionStreamResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionStreamResponse"
    },
    {
      "level": "h3",
      "text": "choices instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionStreamResponse.choices"
    },
    {
      "level": "h3",
      "text": "created class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionStreamResponse.created"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionStreamResponse.id"
    },
    {
      "level": "h3",
      "text": "model instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionStreamResponse.model"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionStreamResponse.object"
    },
    {
      "level": "h3",
      "text": "usage class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.CompletionStreamResponse.usage"
    },
    {
      "level": "h2",
      "text": "DeltaFunctionCall ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaFunctionCall"
    },
    {
      "level": "h3",
      "text": "arguments class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaFunctionCall.arguments"
    },
    {
      "level": "h3",
      "text": "name class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaFunctionCall.name"
    },
    {
      "level": "h2",
      "text": "DeltaMessage ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaMessage"
    },
    {
      "level": "h3",
      "text": "content class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaMessage.content"
    },
    {
      "level": "h3",
      "text": "reasoning class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaMessage.reasoning"
    },
    {
      "level": "h3",
      "text": "reasoning_content class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaMessage.reasoning_content"
    },
    {
      "level": "h3",
      "text": "role class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaMessage.role"
    },
    {
      "level": "h3",
      "text": "tool_calls class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaMessage.tool_calls"
    },
    {
      "level": "h3",
      "text": "handle_deprecated_reasoning_content ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaMessage.handle_deprecated_reasoning_content"
    },
    {
      "level": "h2",
      "text": "DeltaToolCall ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaToolCall"
    },
    {
      "level": "h3",
      "text": "function class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaToolCall.function"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaToolCall.id"
    },
    {
      "level": "h3",
      "text": "index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaToolCall.index"
    },
    {
      "level": "h3",
      "text": "type class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DeltaToolCall.type"
    },
    {
      "level": "h2",
      "text": "DetokenizeRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.DetokenizeRequest"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DetokenizeRequest.model"
    },
    {
      "level": "h3",
      "text": "tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DetokenizeRequest.tokens"
    },
    {
      "level": "h2",
      "text": "DetokenizeResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.DetokenizeResponse"
    },
    {
      "level": "h3",
      "text": "prompt instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.DetokenizeResponse.prompt"
    },
    {
      "level": "h2",
      "text": "ErrorInfo ¶",
      "id": "vllm.entrypoints.openai.protocol.ErrorInfo"
    },
    {
      "level": "h3",
      "text": "code instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ErrorInfo.code"
    },
    {
      "level": "h3",
      "text": "message instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ErrorInfo.message"
    },
    {
      "level": "h3",
      "text": "param class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ErrorInfo.param"
    },
    {
      "level": "h3",
      "text": "type instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ErrorInfo.type"
    },
    {
      "level": "h2",
      "text": "ErrorResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.ErrorResponse"
    },
    {
      "level": "h3",
      "text": "error instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ErrorResponse.error"
    },
    {
      "level": "h2",
      "text": "ExtractedToolCallInformation ¶",
      "id": "vllm.entrypoints.openai.protocol.ExtractedToolCallInformation"
    },
    {
      "level": "h3",
      "text": "content class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ExtractedToolCallInformation.content"
    },
    {
      "level": "h3",
      "text": "tool_calls instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ExtractedToolCallInformation.tool_calls"
    },
    {
      "level": "h3",
      "text": "tools_called instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ExtractedToolCallInformation.tools_called"
    },
    {
      "level": "h2",
      "text": "FunctionCall ¶",
      "id": "vllm.entrypoints.openai.protocol.FunctionCall"
    },
    {
      "level": "h3",
      "text": "arguments instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.FunctionCall.arguments"
    },
    {
      "level": "h3",
      "text": "name instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.FunctionCall.name"
    },
    {
      "level": "h2",
      "text": "FunctionDefinition ¶",
      "id": "vllm.entrypoints.openai.protocol.FunctionDefinition"
    },
    {
      "level": "h3",
      "text": "description class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.FunctionDefinition.description"
    },
    {
      "level": "h3",
      "text": "name instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.FunctionDefinition.name"
    },
    {
      "level": "h3",
      "text": "parameters class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.FunctionDefinition.parameters"
    },
    {
      "level": "h2",
      "text": "GenerateRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest"
    },
    {
      "level": "h3",
      "text": "cache_salt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.cache_salt"
    },
    {
      "level": "h3",
      "text": "features class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.features"
    },
    {
      "level": "h3",
      "text": "kv_transfer_params class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.kv_transfer_params"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.model"
    },
    {
      "level": "h3",
      "text": "priority class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.priority"
    },
    {
      "level": "h3",
      "text": "request_id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.request_id"
    },
    {
      "level": "h3",
      "text": "sampling_params instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.sampling_params"
    },
    {
      "level": "h3",
      "text": "stream class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.stream"
    },
    {
      "level": "h3",
      "text": "stream_options class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.stream_options"
    },
    {
      "level": "h3",
      "text": "token_ids instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateRequest.token_ids"
    },
    {
      "level": "h2",
      "text": "GenerateResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponse"
    },
    {
      "level": "h3",
      "text": "choices instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponse.choices"
    },
    {
      "level": "h3",
      "text": "kv_transfer_params class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponse.kv_transfer_params"
    },
    {
      "level": "h3",
      "text": "prompt_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponse.prompt_logprobs"
    },
    {
      "level": "h3",
      "text": "request_id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponse.request_id"
    },
    {
      "level": "h2",
      "text": "GenerateResponseChoice ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponseChoice"
    },
    {
      "level": "h3",
      "text": "finish_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponseChoice.finish_reason"
    },
    {
      "level": "h3",
      "text": "index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponseChoice.index"
    },
    {
      "level": "h3",
      "text": "logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponseChoice.logprobs"
    },
    {
      "level": "h3",
      "text": "token_ids class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.GenerateResponseChoice.token_ids"
    },
    {
      "level": "h2",
      "text": "InputTokensDetails ¶",
      "id": "vllm.entrypoints.openai.protocol.InputTokensDetails"
    },
    {
      "level": "h3",
      "text": "cached_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.InputTokensDetails.cached_tokens"
    },
    {
      "level": "h3",
      "text": "cached_tokens_per_turn class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.InputTokensDetails.cached_tokens_per_turn"
    },
    {
      "level": "h3",
      "text": "input_tokens_per_turn class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.InputTokensDetails.input_tokens_per_turn"
    },
    {
      "level": "h2",
      "text": "JsonSchemaResponseFormat ¶",
      "id": "vllm.entrypoints.openai.protocol.JsonSchemaResponseFormat"
    },
    {
      "level": "h3",
      "text": "description class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.JsonSchemaResponseFormat.description"
    },
    {
      "level": "h3",
      "text": "json_schema class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.JsonSchemaResponseFormat.json_schema"
    },
    {
      "level": "h3",
      "text": "name instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.JsonSchemaResponseFormat.name"
    },
    {
      "level": "h3",
      "text": "strict class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.JsonSchemaResponseFormat.strict"
    },
    {
      "level": "h2",
      "text": "LegacyStructuralTag ¶",
      "id": "vllm.entrypoints.openai.protocol.LegacyStructuralTag"
    },
    {
      "level": "h3",
      "text": "begin instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LegacyStructuralTag.begin"
    },
    {
      "level": "h3",
      "text": "end instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LegacyStructuralTag.end"
    },
    {
      "level": "h3",
      "text": "structural_tag_schema class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LegacyStructuralTag.structural_tag_schema"
    },
    {
      "level": "h2",
      "text": "LegacyStructuralTagResponseFormat ¶",
      "id": "vllm.entrypoints.openai.protocol.LegacyStructuralTagResponseFormat"
    },
    {
      "level": "h3",
      "text": "structures instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LegacyStructuralTagResponseFormat.structures"
    },
    {
      "level": "h3",
      "text": "triggers instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LegacyStructuralTagResponseFormat.triggers"
    },
    {
      "level": "h3",
      "text": "type instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LegacyStructuralTagResponseFormat.type"
    },
    {
      "level": "h2",
      "text": "LoadLoRAAdapterRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.LoadLoRAAdapterRequest"
    },
    {
      "level": "h3",
      "text": "lora_name instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LoadLoRAAdapterRequest.lora_name"
    },
    {
      "level": "h3",
      "text": "lora_path instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LoadLoRAAdapterRequest.lora_path"
    },
    {
      "level": "h2",
      "text": "LogitsProcessorConstructor ¶",
      "id": "vllm.entrypoints.openai.protocol.LogitsProcessorConstructor"
    },
    {
      "level": "h3",
      "text": "args class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LogitsProcessorConstructor.args"
    },
    {
      "level": "h3",
      "text": "kwargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LogitsProcessorConstructor.kwargs"
    },
    {
      "level": "h3",
      "text": "model_config class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LogitsProcessorConstructor.model_config"
    },
    {
      "level": "h3",
      "text": "qualname instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.LogitsProcessorConstructor.qualname"
    },
    {
      "level": "h2",
      "text": "ModelCard ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard"
    },
    {
      "level": "h3",
      "text": "created class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard.created"
    },
    {
      "level": "h3",
      "text": "id instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard.id"
    },
    {
      "level": "h3",
      "text": "max_model_len class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard.max_model_len"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard.object"
    },
    {
      "level": "h3",
      "text": "owned_by class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard.owned_by"
    },
    {
      "level": "h3",
      "text": "parent class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard.parent"
    },
    {
      "level": "h3",
      "text": "permission class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard.permission"
    },
    {
      "level": "h3",
      "text": "root class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelCard.root"
    },
    {
      "level": "h2",
      "text": "ModelList ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelList"
    },
    {
      "level": "h3",
      "text": "data class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelList.data"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelList.object"
    },
    {
      "level": "h2",
      "text": "ModelPermission ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission"
    },
    {
      "level": "h3",
      "text": "allow_create_engine class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.allow_create_engine"
    },
    {
      "level": "h3",
      "text": "allow_fine_tuning class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.allow_fine_tuning"
    },
    {
      "level": "h3",
      "text": "allow_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.allow_logprobs"
    },
    {
      "level": "h3",
      "text": "allow_sampling class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.allow_sampling"
    },
    {
      "level": "h3",
      "text": "allow_search_indices class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.allow_search_indices"
    },
    {
      "level": "h3",
      "text": "allow_view class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.allow_view"
    },
    {
      "level": "h3",
      "text": "created class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.created"
    },
    {
      "level": "h3",
      "text": "group class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.group"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.id"
    },
    {
      "level": "h3",
      "text": "is_blocking class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.is_blocking"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.object"
    },
    {
      "level": "h3",
      "text": "organization class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ModelPermission.organization"
    },
    {
      "level": "h2",
      "text": "OpenAIBaseModel ¶",
      "id": "vllm.entrypoints.openai.protocol.OpenAIBaseModel"
    },
    {
      "level": "h3",
      "text": "field_names class-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.OpenAIBaseModel.field_names"
    },
    {
      "level": "h3",
      "text": "model_config class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.OpenAIBaseModel.model_config"
    },
    {
      "level": "h3",
      "text": "__log_extra_fields__ classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.OpenAIBaseModel.__log_extra_fields__"
    },
    {
      "level": "h2",
      "text": "OutputTokensDetails ¶",
      "id": "vllm.entrypoints.openai.protocol.OutputTokensDetails"
    },
    {
      "level": "h3",
      "text": "output_tokens_per_turn class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.OutputTokensDetails.output_tokens_per_turn"
    },
    {
      "level": "h3",
      "text": "reasoning_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.OutputTokensDetails.reasoning_tokens"
    },
    {
      "level": "h3",
      "text": "tool_output_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.OutputTokensDetails.tool_output_tokens"
    },
    {
      "level": "h3",
      "text": "tool_output_tokens_per_turn class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.OutputTokensDetails.tool_output_tokens_per_turn"
    },
    {
      "level": "h2",
      "text": "PromptTokenUsageInfo ¶",
      "id": "vllm.entrypoints.openai.protocol.PromptTokenUsageInfo"
    },
    {
      "level": "h3",
      "text": "cached_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.PromptTokenUsageInfo.cached_tokens"
    },
    {
      "level": "h2",
      "text": "RequestResponseMetadata ¶",
      "id": "vllm.entrypoints.openai.protocol.RequestResponseMetadata"
    },
    {
      "level": "h3",
      "text": "final_usage_info class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.RequestResponseMetadata.final_usage_info"
    },
    {
      "level": "h3",
      "text": "request_id instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.RequestResponseMetadata.request_id"
    },
    {
      "level": "h2",
      "text": "ResponseCompletedEvent ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseCompletedEvent"
    },
    {
      "level": "h3",
      "text": "response instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseCompletedEvent.response"
    },
    {
      "level": "h2",
      "text": "ResponseCreatedEvent ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseCreatedEvent"
    },
    {
      "level": "h3",
      "text": "response instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseCreatedEvent.response"
    },
    {
      "level": "h2",
      "text": "ResponseFormat ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseFormat"
    },
    {
      "level": "h3",
      "text": "json_schema class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseFormat.json_schema"
    },
    {
      "level": "h3",
      "text": "type instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseFormat.type"
    },
    {
      "level": "h2",
      "text": "ResponseInProgressEvent ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseInProgressEvent"
    },
    {
      "level": "h3",
      "text": "response instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseInProgressEvent.response"
    },
    {
      "level": "h2",
      "text": "ResponseRawMessageAndToken ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseRawMessageAndToken"
    },
    {
      "level": "h3",
      "text": "message instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseRawMessageAndToken.message"
    },
    {
      "level": "h3",
      "text": "tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseRawMessageAndToken.tokens"
    },
    {
      "level": "h3",
      "text": "type class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseRawMessageAndToken.type"
    },
    {
      "level": "h2",
      "text": "ResponseReasoningPartAddedEvent ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartAddedEvent"
    },
    {
      "level": "h3",
      "text": "content_index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartAddedEvent.content_index"
    },
    {
      "level": "h3",
      "text": "item_id instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartAddedEvent.item_id"
    },
    {
      "level": "h3",
      "text": "output_index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartAddedEvent.output_index"
    },
    {
      "level": "h3",
      "text": "part instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartAddedEvent.part"
    },
    {
      "level": "h3",
      "text": "sequence_number instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartAddedEvent.sequence_number"
    },
    {
      "level": "h3",
      "text": "type instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartAddedEvent.type"
    },
    {
      "level": "h2",
      "text": "ResponseReasoningPartDoneEvent ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartDoneEvent"
    },
    {
      "level": "h3",
      "text": "content_index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartDoneEvent.content_index"
    },
    {
      "level": "h3",
      "text": "item_id instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartDoneEvent.item_id"
    },
    {
      "level": "h3",
      "text": "output_index instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartDoneEvent.output_index"
    },
    {
      "level": "h3",
      "text": "part instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartDoneEvent.part"
    },
    {
      "level": "h3",
      "text": "sequence_number instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartDoneEvent.sequence_number"
    },
    {
      "level": "h3",
      "text": "type instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseReasoningPartDoneEvent.type"
    },
    {
      "level": "h2",
      "text": "ResponseUsage ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseUsage"
    },
    {
      "level": "h3",
      "text": "input_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseUsage.input_tokens"
    },
    {
      "level": "h3",
      "text": "input_tokens_details instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseUsage.input_tokens_details"
    },
    {
      "level": "h3",
      "text": "output_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseUsage.output_tokens"
    },
    {
      "level": "h3",
      "text": "output_tokens_details instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseUsage.output_tokens_details"
    },
    {
      "level": "h3",
      "text": "total_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponseUsage.total_tokens"
    },
    {
      "level": "h2",
      "text": "ResponsesRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest"
    },
    {
      "level": "h3",
      "text": "_DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest._DEFAULT_SAMPLING_PARAMS"
    },
    {
      "level": "h3",
      "text": "background class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.background"
    },
    {
      "level": "h3",
      "text": "cache_salt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.cache_salt"
    },
    {
      "level": "h3",
      "text": "enable_response_messages class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.enable_response_messages"
    },
    {
      "level": "h3",
      "text": "include class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.include"
    },
    {
      "level": "h3",
      "text": "input instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.input"
    },
    {
      "level": "h3",
      "text": "instructions class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.instructions"
    },
    {
      "level": "h3",
      "text": "logit_bias class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.logit_bias"
    },
    {
      "level": "h3",
      "text": "max_output_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.max_output_tokens"
    },
    {
      "level": "h3",
      "text": "max_tool_calls class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.max_tool_calls"
    },
    {
      "level": "h3",
      "text": "metadata class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.metadata"
    },
    {
      "level": "h3",
      "text": "mm_processor_kwargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.mm_processor_kwargs"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.model"
    },
    {
      "level": "h3",
      "text": "parallel_tool_calls class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.parallel_tool_calls"
    },
    {
      "level": "h3",
      "text": "previous_input_messages class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.previous_input_messages"
    },
    {
      "level": "h3",
      "text": "previous_response_id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.previous_response_id"
    },
    {
      "level": "h3",
      "text": "priority class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.priority"
    },
    {
      "level": "h3",
      "text": "prompt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.prompt"
    },
    {
      "level": "h3",
      "text": "reasoning class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.reasoning"
    },
    {
      "level": "h3",
      "text": "request_id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.request_id"
    },
    {
      "level": "h3",
      "text": "service_tier class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.service_tier"
    },
    {
      "level": "h3",
      "text": "store class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.store"
    },
    {
      "level": "h3",
      "text": "stream class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.stream"
    },
    {
      "level": "h3",
      "text": "temperature class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.temperature"
    },
    {
      "level": "h3",
      "text": "text class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.text"
    },
    {
      "level": "h3",
      "text": "tool_choice class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.tool_choice"
    },
    {
      "level": "h3",
      "text": "tools class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.tools"
    },
    {
      "level": "h3",
      "text": "top_k class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.top_k"
    },
    {
      "level": "h3",
      "text": "top_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.top_logprobs"
    },
    {
      "level": "h3",
      "text": "top_p class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.top_p"
    },
    {
      "level": "h3",
      "text": "truncation class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.truncation"
    },
    {
      "level": "h3",
      "text": "user class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.user"
    },
    {
      "level": "h3",
      "text": "check_cache_salt_support ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.check_cache_salt_support"
    },
    {
      "level": "h3",
      "text": "function_call_parsing ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.function_call_parsing"
    },
    {
      "level": "h3",
      "text": "is_include_output_logprobs ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.is_include_output_logprobs"
    },
    {
      "level": "h3",
      "text": "to_sampling_params ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.to_sampling_params"
    },
    {
      "level": "h3",
      "text": "validate_background ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.validate_background"
    },
    {
      "level": "h3",
      "text": "validate_prompt ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesRequest.validate_prompt"
    },
    {
      "level": "h2",
      "text": "ResponsesResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse"
    },
    {
      "level": "h3",
      "text": "background instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.background"
    },
    {
      "level": "h3",
      "text": "created_at class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.created_at"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.id"
    },
    {
      "level": "h3",
      "text": "incomplete_details class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.incomplete_details"
    },
    {
      "level": "h3",
      "text": "input_messages class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.input_messages"
    },
    {
      "level": "h3",
      "text": "instructions class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.instructions"
    },
    {
      "level": "h3",
      "text": "max_output_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.max_output_tokens"
    },
    {
      "level": "h3",
      "text": "max_tool_calls class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.max_tool_calls"
    },
    {
      "level": "h3",
      "text": "metadata class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.metadata"
    },
    {
      "level": "h3",
      "text": "model instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.model"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.object"
    },
    {
      "level": "h3",
      "text": "output instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.output"
    },
    {
      "level": "h3",
      "text": "output_messages class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.output_messages"
    },
    {
      "level": "h3",
      "text": "parallel_tool_calls instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.parallel_tool_calls"
    },
    {
      "level": "h3",
      "text": "previous_response_id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.previous_response_id"
    },
    {
      "level": "h3",
      "text": "prompt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.prompt"
    },
    {
      "level": "h3",
      "text": "reasoning class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.reasoning"
    },
    {
      "level": "h3",
      "text": "service_tier instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.service_tier"
    },
    {
      "level": "h3",
      "text": "status instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.status"
    },
    {
      "level": "h3",
      "text": "temperature instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.temperature"
    },
    {
      "level": "h3",
      "text": "text class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.text"
    },
    {
      "level": "h3",
      "text": "tool_choice instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.tool_choice"
    },
    {
      "level": "h3",
      "text": "tools instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.tools"
    },
    {
      "level": "h3",
      "text": "top_logprobs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.top_logprobs"
    },
    {
      "level": "h3",
      "text": "top_p instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.top_p"
    },
    {
      "level": "h3",
      "text": "truncation instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.truncation"
    },
    {
      "level": "h3",
      "text": "usage class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.usage"
    },
    {
      "level": "h3",
      "text": "user class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.user"
    },
    {
      "level": "h3",
      "text": "from_request classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.from_request"
    },
    {
      "level": "h3",
      "text": "serialize_input_messages ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.serialize_input_messages"
    },
    {
      "level": "h3",
      "text": "serialize_output_messages ¶",
      "id": "vllm.entrypoints.openai.protocol.ResponsesResponse.serialize_output_messages"
    },
    {
      "level": "h2",
      "text": "StreamOptions ¶",
      "id": "vllm.entrypoints.openai.protocol.StreamOptions"
    },
    {
      "level": "h3",
      "text": "continuous_usage_stats class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.StreamOptions.continuous_usage_stats"
    },
    {
      "level": "h3",
      "text": "include_usage class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.StreamOptions.include_usage"
    },
    {
      "level": "h2",
      "text": "StructuralTagResponseFormat ¶",
      "id": "vllm.entrypoints.openai.protocol.StructuralTagResponseFormat"
    },
    {
      "level": "h3",
      "text": "format instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.StructuralTagResponseFormat.format"
    },
    {
      "level": "h3",
      "text": "type instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.StructuralTagResponseFormat.type"
    },
    {
      "level": "h2",
      "text": "TokenizeChatRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest"
    },
    {
      "level": "h3",
      "text": "add_generation_prompt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.add_generation_prompt"
    },
    {
      "level": "h3",
      "text": "add_special_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.add_special_tokens"
    },
    {
      "level": "h3",
      "text": "chat_template class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.chat_template"
    },
    {
      "level": "h3",
      "text": "chat_template_kwargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.chat_template_kwargs"
    },
    {
      "level": "h3",
      "text": "continue_final_message class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.continue_final_message"
    },
    {
      "level": "h3",
      "text": "messages instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.messages"
    },
    {
      "level": "h3",
      "text": "mm_processor_kwargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.mm_processor_kwargs"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.model"
    },
    {
      "level": "h3",
      "text": "return_token_strs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.return_token_strs"
    },
    {
      "level": "h3",
      "text": "tools class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.tools"
    },
    {
      "level": "h3",
      "text": "check_generation_prompt classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeChatRequest.check_generation_prompt"
    },
    {
      "level": "h2",
      "text": "TokenizeCompletionRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeCompletionRequest"
    },
    {
      "level": "h3",
      "text": "add_special_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeCompletionRequest.add_special_tokens"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeCompletionRequest.model"
    },
    {
      "level": "h3",
      "text": "prompt instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeCompletionRequest.prompt"
    },
    {
      "level": "h3",
      "text": "return_token_strs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeCompletionRequest.return_token_strs"
    },
    {
      "level": "h2",
      "text": "TokenizeResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeResponse"
    },
    {
      "level": "h3",
      "text": "count instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeResponse.count"
    },
    {
      "level": "h3",
      "text": "max_model_len instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeResponse.max_model_len"
    },
    {
      "level": "h3",
      "text": "token_strs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeResponse.token_strs"
    },
    {
      "level": "h3",
      "text": "tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizeResponse.tokens"
    },
    {
      "level": "h2",
      "text": "TokenizerInfoResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizerInfoResponse"
    },
    {
      "level": "h3",
      "text": "model_config class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizerInfoResponse.model_config"
    },
    {
      "level": "h3",
      "text": "tokenizer_class instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TokenizerInfoResponse.tokenizer_class"
    },
    {
      "level": "h2",
      "text": "ToolCall ¶",
      "id": "vllm.entrypoints.openai.protocol.ToolCall"
    },
    {
      "level": "h3",
      "text": "function instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ToolCall.function"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ToolCall.id"
    },
    {
      "level": "h3",
      "text": "type class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.ToolCall.type"
    },
    {
      "level": "h2",
      "text": "TranscriptionRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest"
    },
    {
      "level": "h3",
      "text": "_DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest._DEFAULT_SAMPLING_PARAMS"
    },
    {
      "level": "h3",
      "text": "file instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.file"
    },
    {
      "level": "h3",
      "text": "frequency_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.frequency_penalty"
    },
    {
      "level": "h3",
      "text": "language class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.language"
    },
    {
      "level": "h3",
      "text": "max_completion_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.max_completion_tokens"
    },
    {
      "level": "h3",
      "text": "min_p class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.min_p"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.model"
    },
    {
      "level": "h3",
      "text": "presence_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.presence_penalty"
    },
    {
      "level": "h3",
      "text": "prompt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.prompt"
    },
    {
      "level": "h3",
      "text": "repetition_penalty class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.repetition_penalty"
    },
    {
      "level": "h3",
      "text": "response_format class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.response_format"
    },
    {
      "level": "h3",
      "text": "seed class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.seed"
    },
    {
      "level": "h3",
      "text": "stream class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.stream"
    },
    {
      "level": "h3",
      "text": "stream_continuous_usage_stats class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.stream_continuous_usage_stats"
    },
    {
      "level": "h3",
      "text": "stream_include_usage class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.stream_include_usage"
    },
    {
      "level": "h3",
      "text": "temperature class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.temperature"
    },
    {
      "level": "h3",
      "text": "timestamp_granularities class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.timestamp_granularities"
    },
    {
      "level": "h3",
      "text": "to_language class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.to_language"
    },
    {
      "level": "h3",
      "text": "top_k class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.top_k"
    },
    {
      "level": "h3",
      "text": "top_p class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.top_p"
    },
    {
      "level": "h3",
      "text": "vllm_xargs class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.vllm_xargs"
    },
    {
      "level": "h3",
      "text": "to_sampling_params ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.to_sampling_params"
    },
    {
      "level": "h3",
      "text": "validate_transcription_request classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionRequest.validate_transcription_request"
    },
    {
      "level": "h2",
      "text": "TranscriptionResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponse"
    },
    {
      "level": "h3",
      "text": "text instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponse.text"
    },
    {
      "level": "h3",
      "text": "usage instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponse.usage"
    },
    {
      "level": "h2",
      "text": "TranscriptionResponseStreamChoice ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseStreamChoice"
    },
    {
      "level": "h3",
      "text": "delta instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseStreamChoice.delta"
    },
    {
      "level": "h3",
      "text": "finish_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseStreamChoice.finish_reason"
    },
    {
      "level": "h3",
      "text": "stop_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseStreamChoice.stop_reason"
    },
    {
      "level": "h2",
      "text": "TranscriptionResponseVerbose ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseVerbose"
    },
    {
      "level": "h3",
      "text": "duration instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseVerbose.duration"
    },
    {
      "level": "h3",
      "text": "language instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseVerbose.language"
    },
    {
      "level": "h3",
      "text": "segments class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseVerbose.segments"
    },
    {
      "level": "h3",
      "text": "text instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseVerbose.text"
    },
    {
      "level": "h3",
      "text": "words class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionResponseVerbose.words"
    },
    {
      "level": "h2",
      "text": "TranscriptionSegment ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment"
    },
    {
      "level": "h3",
      "text": "avg_logprob class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.avg_logprob"
    },
    {
      "level": "h3",
      "text": "compression_ratio class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.compression_ratio"
    },
    {
      "level": "h3",
      "text": "end instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.end"
    },
    {
      "level": "h3",
      "text": "id instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.id"
    },
    {
      "level": "h3",
      "text": "no_speech_prob class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.no_speech_prob"
    },
    {
      "level": "h3",
      "text": "seek instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.seek"
    },
    {
      "level": "h3",
      "text": "start instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.start"
    },
    {
      "level": "h3",
      "text": "temperature instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.temperature"
    },
    {
      "level": "h3",
      "text": "text instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.text"
    },
    {
      "level": "h3",
      "text": "tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionSegment.tokens"
    },
    {
      "level": "h2",
      "text": "TranscriptionStreamResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionStreamResponse"
    },
    {
      "level": "h3",
      "text": "choices instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionStreamResponse.choices"
    },
    {
      "level": "h3",
      "text": "created class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionStreamResponse.created"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionStreamResponse.id"
    },
    {
      "level": "h3",
      "text": "model instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionStreamResponse.model"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionStreamResponse.object"
    },
    {
      "level": "h3",
      "text": "usage class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionStreamResponse.usage"
    },
    {
      "level": "h2",
      "text": "TranscriptionUsageAudio ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionUsageAudio"
    },
    {
      "level": "h3",
      "text": "seconds instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionUsageAudio.seconds"
    },
    {
      "level": "h3",
      "text": "type class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionUsageAudio.type"
    },
    {
      "level": "h2",
      "text": "TranscriptionWord ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionWord"
    },
    {
      "level": "h3",
      "text": "end instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionWord.end"
    },
    {
      "level": "h3",
      "text": "start instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionWord.start"
    },
    {
      "level": "h3",
      "text": "word instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranscriptionWord.word"
    },
    {
      "level": "h2",
      "text": "TranslationRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest"
    },
    {
      "level": "h3",
      "text": "_DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest._DEFAULT_SAMPLING_PARAMS"
    },
    {
      "level": "h3",
      "text": "file instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.file"
    },
    {
      "level": "h3",
      "text": "language class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.language"
    },
    {
      "level": "h3",
      "text": "max_completion_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.max_completion_tokens"
    },
    {
      "level": "h3",
      "text": "model class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.model"
    },
    {
      "level": "h3",
      "text": "prompt class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.prompt"
    },
    {
      "level": "h3",
      "text": "response_format class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.response_format"
    },
    {
      "level": "h3",
      "text": "seed class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.seed"
    },
    {
      "level": "h3",
      "text": "stream class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.stream"
    },
    {
      "level": "h3",
      "text": "stream_continuous_usage_stats class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.stream_continuous_usage_stats"
    },
    {
      "level": "h3",
      "text": "stream_include_usage class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.stream_include_usage"
    },
    {
      "level": "h3",
      "text": "temperature class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.temperature"
    },
    {
      "level": "h3",
      "text": "to_language class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.to_language"
    },
    {
      "level": "h3",
      "text": "to_sampling_params ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.to_sampling_params"
    },
    {
      "level": "h3",
      "text": "validate_stream_options classmethod ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationRequest.validate_stream_options"
    },
    {
      "level": "h2",
      "text": "TranslationResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponse"
    },
    {
      "level": "h3",
      "text": "text instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponse.text"
    },
    {
      "level": "h2",
      "text": "TranslationResponseStreamChoice ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseStreamChoice"
    },
    {
      "level": "h3",
      "text": "delta instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseStreamChoice.delta"
    },
    {
      "level": "h3",
      "text": "finish_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseStreamChoice.finish_reason"
    },
    {
      "level": "h3",
      "text": "stop_reason class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseStreamChoice.stop_reason"
    },
    {
      "level": "h2",
      "text": "TranslationResponseVerbose ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseVerbose"
    },
    {
      "level": "h3",
      "text": "duration instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseVerbose.duration"
    },
    {
      "level": "h3",
      "text": "language instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseVerbose.language"
    },
    {
      "level": "h3",
      "text": "segments class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseVerbose.segments"
    },
    {
      "level": "h3",
      "text": "text instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseVerbose.text"
    },
    {
      "level": "h3",
      "text": "words class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationResponseVerbose.words"
    },
    {
      "level": "h2",
      "text": "TranslationSegment ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment"
    },
    {
      "level": "h3",
      "text": "avg_logprob class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.avg_logprob"
    },
    {
      "level": "h3",
      "text": "compression_ratio class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.compression_ratio"
    },
    {
      "level": "h3",
      "text": "end instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.end"
    },
    {
      "level": "h3",
      "text": "id instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.id"
    },
    {
      "level": "h3",
      "text": "no_speech_prob class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.no_speech_prob"
    },
    {
      "level": "h3",
      "text": "seek instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.seek"
    },
    {
      "level": "h3",
      "text": "start instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.start"
    },
    {
      "level": "h3",
      "text": "temperature instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.temperature"
    },
    {
      "level": "h3",
      "text": "text instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.text"
    },
    {
      "level": "h3",
      "text": "tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationSegment.tokens"
    },
    {
      "level": "h2",
      "text": "TranslationStreamResponse ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationStreamResponse"
    },
    {
      "level": "h3",
      "text": "choices instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationStreamResponse.choices"
    },
    {
      "level": "h3",
      "text": "created class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationStreamResponse.created"
    },
    {
      "level": "h3",
      "text": "id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationStreamResponse.id"
    },
    {
      "level": "h3",
      "text": "model instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationStreamResponse.model"
    },
    {
      "level": "h3",
      "text": "object class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationStreamResponse.object"
    },
    {
      "level": "h3",
      "text": "usage class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationStreamResponse.usage"
    },
    {
      "level": "h2",
      "text": "TranslationWord ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationWord"
    },
    {
      "level": "h3",
      "text": "end instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationWord.end"
    },
    {
      "level": "h3",
      "text": "start instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationWord.start"
    },
    {
      "level": "h3",
      "text": "word instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.TranslationWord.word"
    },
    {
      "level": "h2",
      "text": "UnloadLoRAAdapterRequest ¶",
      "id": "vllm.entrypoints.openai.protocol.UnloadLoRAAdapterRequest"
    },
    {
      "level": "h3",
      "text": "lora_int_id class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.UnloadLoRAAdapterRequest.lora_int_id"
    },
    {
      "level": "h3",
      "text": "lora_name instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.UnloadLoRAAdapterRequest.lora_name"
    },
    {
      "level": "h2",
      "text": "UsageInfo ¶",
      "id": "vllm.entrypoints.openai.protocol.UsageInfo"
    },
    {
      "level": "h3",
      "text": "completion_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.UsageInfo.completion_tokens"
    },
    {
      "level": "h3",
      "text": "prompt_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.UsageInfo.prompt_tokens"
    },
    {
      "level": "h3",
      "text": "prompt_tokens_details class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.UsageInfo.prompt_tokens_details"
    },
    {
      "level": "h3",
      "text": "total_tokens class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.UsageInfo.total_tokens"
    },
    {
      "level": "h2",
      "text": "VLLMValidationError ¶",
      "id": "vllm.entrypoints.openai.protocol.VLLMValidationError"
    },
    {
      "level": "h3",
      "text": "parameter instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.VLLMValidationError.parameter"
    },
    {
      "level": "h3",
      "text": "value instance-attribute ¶",
      "id": "vllm.entrypoints.openai.protocol.VLLMValidationError.value"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.protocol.VLLMValidationError.__init__"
    },
    {
      "level": "h3",
      "text": "__str__ ¶",
      "id": "vllm.entrypoints.openai.protocol.VLLMValidationError.__str__"
    },
    {
      "level": "h2",
      "text": "get_logits_processors ¶",
      "id": "vllm.entrypoints.openai.protocol.get_logits_processors"
    },
    {
      "level": "h2",
      "text": "serialize_message ¶",
      "id": "vllm.entrypoints.openai.protocol.serialize_message"
    },
    {
      "level": "h2",
      "text": "serialize_messages ¶",
      "id": "vllm.entrypoints.openai.protocol.serialize_messages"
    }
  ],
  "code_samples": [
    {
      "code": "AnyResponseFormat: TypeAlias = (\n    ResponseFormat\n    | StructuralTagResponseFormat\n    | LegacyStructuralTagResponseFormat\n)",
      "language": "typescript"
    },
    {
      "code": "AnyResponseFormat: TypeAlias = (\n    ResponseFormat\n    | StructuralTagResponseFormat\n    | LegacyStructuralTagResponseFormat\n)",
      "language": "typescript"
    },
    {
      "code": "AnyStructuralTagResponseFormat: TypeAlias = (\n    LegacyStructuralTagResponseFormat\n    | StructuralTagResponseFormat\n)",
      "language": "typescript"
    },
    {
      "code": "AnyStructuralTagResponseFormat: TypeAlias = (\n    LegacyStructuralTagResponseFormat\n    | StructuralTagResponseFormat\n)",
      "language": "typescript"
    },
    {
      "code": "AudioResponseFormat: TypeAlias = Literal[\n    \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"\n]",
      "language": "typescript"
    },
    {
      "code": "AudioResponseFormat: TypeAlias = Literal[\n    \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"\n]",
      "language": "typescript"
    },
    {
      "code": "LogitsProcessors = list[str | LogitsProcessorConstructor]",
      "language": "unknown"
    },
    {
      "code": "LogitsProcessors = list[str | LogitsProcessorConstructor]",
      "language": "unknown"
    },
    {
      "code": "ResponseInputOutputItem: TypeAlias = (\n    ResponseInputItemParam | ResponseOutputItem\n)",
      "language": "typescript"
    },
    {
      "code": "ResponseInputOutputItem: TypeAlias = (\n    ResponseInputItemParam | ResponseOutputItem\n)",
      "language": "typescript"
    },
    {
      "code": "ResponseInputOutputMessage: TypeAlias = (\n    list[ChatCompletionMessageParam]\n    | list[ResponseRawMessageAndToken]\n)",
      "language": "typescript"
    },
    {
      "code": "ResponseInputOutputMessage: TypeAlias = (\n    list[ChatCompletionMessageParam]\n    | list[ResponseRawMessageAndToken]\n)",
      "language": "typescript"
    },
    {
      "code": "StreamingResponsesResponse: TypeAlias = (\n    ResponseCreatedEvent\n    | ResponseInProgressEvent\n    | ResponseCompletedEvent\n    | ResponseOutputItemAddedEvent\n    | ResponseOutputItemDoneEvent\n    | ResponseContentPartAddedEvent\n    | ResponseContentPartDoneEvent\n    | ResponseReasoningTextDeltaEvent\n    | ResponseReasoningTextDoneEvent\n    | ResponseReasoningPartAddedEvent\n    | ResponseReasoningPartDoneEvent\n    | ResponseCodeInterpreterCallInProgressEvent\n    | ResponseCodeInterpreterCallCodeDeltaEvent\n    | ResponseWebSearchCallInProgressEvent\n    | ResponseWebSearchCallSearchingEvent\n    | ResponseWebSearchCallCompletedEvent\n    | ResponseCodeInterpreterCallCodeDoneEvent\n    | ResponseCodeInterpreterCallInterpretingEvent\n    | ResponseCodeInterpreterCallCompletedEvent\n    | ResponseMcpCallArgumentsDeltaEvent\n    | ResponseMcpCallArgumentsDoneEvent\n    | ResponseMcpCallInProgressEvent\n    | ResponseMcpCallCompletedEvent\n)",
      "language": "typescript"
    },
    {
      "code": "StreamingResponsesResponse: TypeAlias = (\n    ResponseCreatedEvent\n    | ResponseInProgressEvent\n    | ResponseCompletedEvent\n    | ResponseOutputItemAddedEvent\n    | ResponseOutputItemDoneEvent\n    | ResponseContentPartAddedEvent\n    | ResponseContentPartDoneEvent\n    | ResponseReasoningTextDeltaEvent\n    | ResponseReasoningTextDoneEvent\n    | ResponseReasoningPartAddedEvent\n    | ResponseReasoningPartDoneEvent\n    | ResponseCodeInterpreterCallInProgressEvent\n    | ResponseCodeInterpreterCallCodeDeltaEvent\n    | ResponseWebSearchCallInProgressEvent\n    | ResponseWebSearchCallSearchingEvent\n    | ResponseWebSearchCallCompletedEvent\n    | ResponseCodeInterpreterCallCodeDoneEvent\n    | ResponseCodeInterpreterCallInterpretingEvent\n    | ResponseCodeInterpreterCallCompletedEvent\n    | ResponseMcpCallArgumentsDeltaEvent\n    | ResponseMcpCallArgumentsDoneEvent\n    | ResponseMcpCallInProgressEvent\n    | ResponseMcpCallCompletedEvent\n)",
      "language": "typescript"
    },
    {
      "code": "TokenizeRequest: TypeAlias = (\n    TokenizeCompletionRequest | TokenizeChatRequest\n)",
      "language": "typescript"
    },
    {
      "code": "TokenizeRequest: TypeAlias = (\n    TokenizeCompletionRequest | TokenizeChatRequest\n)",
      "language": "typescript"
    },
    {
      "code": "TranscriptionResponseVariant: TypeAlias = (\n    TranscriptionResponse | TranscriptionResponseVerbose\n)",
      "language": "typescript"
    },
    {
      "code": "TranscriptionResponseVariant: TypeAlias = (\n    TranscriptionResponse | TranscriptionResponseVerbose\n)",
      "language": "typescript"
    },
    {
      "code": "TranslationResponseVariant: TypeAlias = (\n    TranslationResponse | TranslationResponseVerbose\n)",
      "language": "typescript"
    },
    {
      "code": "TranslationResponseVariant: TypeAlias = (\n    TranslationResponse | TranslationResponseVerbose\n)",
      "language": "typescript"
    },
    {
      "code": "_LONG_INFO = iinfo(long)",
      "language": "unknown"
    },
    {
      "code": "_LONG_INFO = iinfo(long)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "1535\n1536\n1537\n1538",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionLogProb(OpenAIBaseModel):\n    token: str\n    logprob: float = -9999.0\n    bytes: list[int] | None = None",
      "language": "typescript"
    },
    {
      "code": "class ChatCompletionLogProb(OpenAIBaseModel):\n    token: str\n    logprob: float = -9999.0\n    bytes: list[int] | None = None",
      "language": "typescript"
    },
    {
      "code": "bytes: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "bytes: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "logprob: float = -9999.0",
      "language": "typescript"
    },
    {
      "code": "logprob: float = -9999.0",
      "language": "typescript"
    },
    {
      "code": "class ChatCompletionLogProbs(OpenAIBaseModel):\n    content: list[ChatCompletionLogProbsContent] | None = None",
      "language": "php"
    },
    {
      "code": "class ChatCompletionLogProbs(OpenAIBaseModel):\n    content: list[ChatCompletionLogProbsContent] | None = None",
      "language": "php"
    },
    {
      "code": "content: list[ChatCompletionLogProbsContent] | None = None",
      "language": "yaml"
    },
    {
      "code": "content: list[ChatCompletionLogProbsContent] | None = None",
      "language": "yaml"
    },
    {
      "code": "1541\n1542\n1543\n1544\n1545",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionLogProbsContent(ChatCompletionLogProb):\n    # Workaround: redefine fields name cache so that it's not\n    # shared with the super class.\n    field_names: ClassVar[set[str] | None] = None\n    top_logprobs: list[ChatCompletionLogProb] = Field(default_factory=list)",
      "language": "php"
    },
    {
      "code": "class ChatCompletionLogProbsContent(ChatCompletionLogProb):\n    # Workaround: redefine fields name cache so that it's not\n    # shared with the super class.\n    field_names: ClassVar[set[str] | None] = None\n    top_logprobs: list[ChatCompletionLogProb] = Field(default_factory=list)",
      "language": "php"
    },
    {
      "code": "field_names: set[str] | None = None",
      "language": "yaml"
    },
    {
      "code": "field_names: set[str] | None = None",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: list[ChatCompletionLogProb] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: list[ChatCompletionLogProb] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "class ChatCompletionNamedFunction(OpenAIBaseModel):\n    name: str",
      "language": "php"
    },
    {
      "code": "class ChatCompletionNamedFunction(OpenAIBaseModel):\n    name: str",
      "language": "php"
    },
    {
      "code": "275\n276\n277",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionNamedToolChoiceParam(OpenAIBaseModel):\n    function: ChatCompletionNamedFunction\n    type: Literal[\"function\"] = \"function\"",
      "language": "php"
    },
    {
      "code": "class ChatCompletionNamedToolChoiceParam(OpenAIBaseModel):\n    function: ChatCompletionNamedFunction\n    type: Literal[\"function\"] = \"function\"",
      "language": "php"
    },
    {
      "code": "function: ChatCompletionNamedFunction",
      "language": "yaml"
    },
    {
      "code": "function: ChatCompletionNamedFunction",
      "language": "yaml"
    },
    {
      "code": "type: Literal['function'] = 'function'",
      "language": "yaml"
    },
    {
      "code": "type: Literal['function'] = 'function'",
      "language": "yaml"
    },
    {
      "code": "558\n 559\n 560\n 561\n 562\n 563\n 564\n 565\n 566\n 567\n 568\n 569\n 570\n 571\n 572\n 573\n 574\n 575\n 576\n 577\n 578\n 579\n 580\n 581\n 582\n 583\n 584\n 585\n 586\n 587\n 588\n 589\n 590\n 591\n 592\n 593\n 594\n 595\n 596\n 597\n 598\n 599\n 600\n 601\n 602\n 603\n 604\n 605\n 606\n 607\n 608\n 609\n 610\n 611\n 612\n 613\n 614\n 615\n 616\n 617\n 618\n 619\n 620\n 621\n 622\n 623\n 624\n 625\n 626\n 627\n 628\n 629\n 630\n 631\n 632\n 633\n 634\n 635\n 636\n 637\n 638\n 639\n 640\n 641\n 642\n 643\n 644\n 645\n 646\n 647\n 648\n 649\n 650\n 651\n 652\n 653\n 654\n 655\n 656\n 657\n 658\n 659\n 660\n 661\n 662\n 663\n 664\n 665\n 666\n 667\n 668\n 669\n 670\n 671\n 672\n 673\n 674\n 675\n 676\n 677\n 678\n 679\n 680\n 681\n 682\n 683\n 684\n 685\n 686\n 687\n 688\n 689\n 690\n 691\n 692\n 693\n 694\n 695\n 696\n 697\n 698\n 699\n 700\n 701\n 702\n 703\n 704\n 705\n 706\n 707\n 708\n 709\n 710\n 711\n 712\n 713\n 714\n 715\n 716\n 717\n 718\n 719\n 720\n 721\n 722\n 723\n 724\n 725\n 726\n 727\n 728\n 729\n 730\n 731\n 732\n 733\n 734\n 735\n 736\n 737\n 738\n 739\n 740\n 741\n 742\n 743\n 744\n 745\n 746\n 747\n 748\n 749\n 750\n 751\n 752\n 753\n 754\n 755\n 756\n 757\n 758\n 759\n 760\n 761\n 762\n 763\n 764\n 765\n 766\n 767\n 768\n 769\n 770\n 771\n 772\n 773\n 774\n 775\n 776\n 777\n 778\n 779\n 780\n 781\n 782\n 783\n 784\n 785\n 786\n 787\n 788\n 789\n 790\n 791\n 792\n 793\n 794\n 795\n 796\n 797\n 798\n 799\n 800\n 801\n 802\n 803\n 804\n 805\n 806\n 807\n 808\n 809\n 810\n 811\n 812\n 813\n 814\n 815\n 816\n 817\n 818\n 819\n 820\n 821\n 822\n 823\n 824\n 825\n 826\n 827\n 828\n 829\n 830\n 831\n 832\n 833\n 834\n 835\n 836\n 837\n 838\n 839\n 840\n 841\n 842\n 843\n 844\n 845\n 846\n 847\n 848\n 849\n 850\n 851\n 852\n 853\n 854\n 855\n 856\n 857\n 858\n 859\n 860\n 861\n 862\n 863\n 864\n 865\n 866\n 867\n 868\n 869\n 870\n 871\n 872\n 873\n 874\n 875\n 876\n 877\n 878\n 879\n 880\n 881\n 882\n 883\n 884\n 885\n 886\n 887\n 888\n 889\n 890\n 891\n 892\n 893\n 894\n 895\n 896\n 897\n 898\n 899\n 900\n 901\n 902\n 903\n 904\n 905\n 906\n 907\n 908\n 909\n 910\n 911\n 912\n 913\n 914\n 915\n 916\n 917\n 918\n 919\n 920\n 921\n 922\n 923\n 924\n 925\n 926\n 927\n 928\n 929\n 930\n 931\n 932\n 933\n 934\n 935\n 936\n 937\n 938\n 939\n 940\n 941\n 942\n 943\n 944\n 945\n 946\n 947\n 948\n 949\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/chat/create\n    messages: list[ChatCompletionMessageParam]\n    model: str | None = None\n    frequency_penalty: float | None = 0.0\n    logit_bias: dict[str, float] | None = None\n    logprobs: bool | None = False\n    top_logprobs: int | None = 0\n    max_tokens: int | None = Field(\n        default=None,\n        deprecated=\"max_tokens is deprecated in favor of \"\n        \"the max_completion_tokens field\",\n    )\n    max_completion_tokens: int | None = None\n    n: int | None = 1\n    presence_penalty: float | None = 0.0\n    response_format: AnyResponseFormat | None = None\n    seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    stop: str | list[str] | None = []\n    stream: bool | None = False\n    stream_options: StreamOptions | None = None\n    temperature: float | None = None\n    top_p: float | None = None\n    tools: list[ChatCompletionToolsParam] | None = None\n    tool_choice: (\n        Literal[\"none\"]\n        | Literal[\"auto\"]\n        | Literal[\"required\"]\n        | ChatCompletionNamedToolChoiceParam\n        | None\n    ) = \"none\"\n    reasoning_effort: Literal[\"low\", \"medium\", \"high\"] | None = None\n    include_reasoning: bool = True\n    parallel_tool_calls: bool | None = True\n\n    # NOTE this will be ignored by vLLM\n    user: str | None = None\n\n    # --8<-- [start:chat-completion-sampling-params]\n    use_beam_search: bool = False\n    top_k: int | None = None\n    min_p: float | None = None\n    repetition_penalty: float | None = None\n    length_penalty: float = 1.0\n    stop_token_ids: list[int] | None = []\n    include_stop_str_in_output: bool = False\n    ignore_eos: bool = False\n    min_tokens: int = 0\n    skip_special_tokens: bool = True\n    spaces_between_special_tokens: bool = True\n    truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None\n    prompt_logprobs: int | None = None\n    allowed_token_ids: list[int] | None = None\n    bad_words: list[str] = Field(default_factory=list)\n    # --8<-- [end:chat-completion-sampling-params]\n\n    # --8<-- [start:chat-completion-extra-params]\n    echo: bool = Field(\n        default=False,\n        description=(\n            \"If true, the new message will be prepended with the last message \"\n            \"if they belong to the same role.\"\n        ),\n    )\n    add_generation_prompt: bool = Field(\n        default=True,\n        description=(\n            \"If true, the generation prompt will be added to the chat template. \"\n            \"This is a parameter used by chat template in tokenizer config of the \"\n            \"model.\"\n        ),\n    )\n    continue_final_message: bool = Field(\n        default=False,\n        description=(\n            \"If this is set, the chat will be formatted so that the final \"\n            \"message in the chat is open-ended, without any EOS tokens. The \"\n            \"model will continue this message rather than starting a new one. \"\n            'This allows you to \"prefill\" part of the model\\'s response for it. '\n            \"Cannot be used at the same time as `add_generation_prompt`.\"\n        ),\n    )\n    add_special_tokens: bool = Field(\n        default=False,\n        description=(\n            \"If true, special tokens (e.g. BOS) will be added to the prompt \"\n            \"on top of what is added by the chat template. \"\n            \"For most models, the chat template takes care of adding the \"\n            \"special tokens so this should be set to false (as is the \"\n            \"default).\"\n        ),\n    )\n    documents: list[dict[str, str]] | None = Field(\n        default=None,\n        description=(\n            \"A list of dicts representing documents that will be accessible to \"\n            \"the model if it is performing RAG (retrieval-augmented generation).\"\n            \" If the template does not support RAG, this argument will have no \"\n            \"effect. We recommend that each document should be a dict containing \"\n            '\"title\" and \"text\" keys.'\n        ),\n    )\n    chat_template: str | None = Field(\n        default=None,\n        description=(\n            \"A Jinja template to use for this conversion. \"\n            \"As of transformers v4.44, default chat template is no longer \"\n            \"allowed, so you must provide a chat template if the tokenizer \"\n            \"does not define one.\"\n        ),\n    )\n    chat_template_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\n            \"Additional keyword args to pass to the template renderer. \"\n            \"Will be accessible by the chat template.\"\n        ),\n    )\n    mm_processor_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the HF processor.\"),\n    )\n    structured_outputs: StructuredOutputsParams | None = Field(\n        default=None,\n        description=\"Additional kwargs for structured outputs\",\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"\n        ),\n    )\n    request_id: str = Field(\n        default_factory=random_uuid,\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    logits_processors: LogitsProcessors | None = Field(\n        default=None,\n        description=(\n            \"A list of either qualified names of logits processors, or \"\n            \"constructor objects, to apply when sampling. A constructor is \"\n            \"a JSON object with a required 'qualname' field specifying the \"\n            \"qualified name of the processor class/factory, and optional \"\n            \"'args' and 'kwargs' fields containing positional and keyword \"\n            \"arguments. For example: {'qualname': \"\n            \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \"\n            \"{'param': 'value'}}.\"\n        ),\n    )\n    return_tokens_as_token_ids: bool | None = Field(\n        default=None,\n        description=(\n            \"If specified with 'logprobs', tokens are represented \"\n            \" as strings of the form 'token_id:{token_id}' so that tokens \"\n            \"that are not JSON-encodable can be identified.\"\n        ),\n    )\n    return_token_ids: bool | None = Field(\n        default=None,\n        description=(\n            \"If specified, the result will include token IDs alongside the \"\n            \"generated text. In streaming mode, prompt_token_ids is included \"\n            \"only in the first chunk, and token_ids contains the delta tokens \"\n            \"for each chunk. This is useful for debugging or when you \"\n            \"need to map generated text back to input tokens.\"\n        ),\n    )\n    cache_salt: str | None = Field(\n        default=None,\n        description=(\n            \"If specified, the prefix cache will be salted with the provided \"\n            \"string to prevent an attacker to guess prompts in multi-user \"\n            \"environments. The salt should be random, protected from \"\n            \"access by 3rd parties, and long enough to be \"\n            \"unpredictable (e.g., 43 characters base64-encoded, corresponding \"\n            \"to 256 bit).\"\n        ),\n    )\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None,\n        description=\"KVTransfer parameters used for disaggregated serving.\",\n    )\n\n    vllm_xargs: dict[str, str | int | float | list[str | int | float]] | None = Field(\n        default=None,\n        description=(\n            \"Additional request parameters with (list of) string or \"\n            \"numeric values, used by custom extensions.\"\n        ),\n    )\n\n    # --8<-- [end:chat-completion-extra-params]\n\n    # Default sampling parameters for chat completion requests\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"repetition_penalty\": 1.0,\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": 0,\n        \"min_p\": 0.0,\n    }\n\n    def to_beam_search_params(\n        self, max_tokens: int, default_sampling_params: dict\n    ) -> BeamSearchParams:\n        n = self.n if self.n is not None else 1\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n\n        return BeamSearchParams(\n            beam_width=n,\n            max_tokens=max_tokens,\n            ignore_eos=self.ignore_eos,\n            temperature=temperature,\n            length_penalty=self.length_penalty,\n            include_stop_str_in_output=self.include_stop_str_in_output,\n        )\n\n    def to_sampling_params(\n        self,\n        max_tokens: int,\n        logits_processor_pattern: str | None,\n        default_sampling_params: dict,\n    ) -> SamplingParams:\n        # Default parameters\n        if (repetition_penalty := self.repetition_penalty) is None:\n            repetition_penalty = default_sampling_params.get(\n                \"repetition_penalty\",\n                self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n            )\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n            )\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n            )\n        if (min_p := self.min_p) is None:\n            min_p = default_sampling_params.get(\n                \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n            )\n\n        prompt_logprobs = self.prompt_logprobs\n        if prompt_logprobs is None and self.echo:\n            prompt_logprobs = self.top_logprobs\n\n        response_format = self.response_format\n        if response_format is not None:\n            # If structured outputs wasn't already enabled,\n            # we must enable it for these features to work\n            if self.structured_outputs is None:\n                self.structured_outputs = StructuredOutputsParams()\n\n            # Set structured output params for response format\n            if response_format.type == \"json_object\":\n                self.structured_outputs.json_object = True\n            elif response_format.type == \"json_schema\":\n                json_schema = response_format.json_schema\n                assert json_schema is not None\n                self.structured_outputs.json = json_schema.json_schema\n            elif response_format.type == \"structural_tag\":\n                structural_tag = response_format\n                assert structural_tag is not None and isinstance(\n                    structural_tag,\n                    (\n                        LegacyStructuralTagResponseFormat,\n                        StructuralTagResponseFormat,\n                    ),\n                )\n                s_tag_obj = structural_tag.model_dump(by_alias=True)\n                self.structured_outputs.structural_tag = json.dumps(s_tag_obj)\n\n        extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {}\n        if self.kv_transfer_params:\n            # Pass in kv_transfer_params via extra_args\n            extra_args[\"kv_transfer_params\"] = self.kv_transfer_params\n        return SamplingParams.from_optional(\n            n=self.n,\n            presence_penalty=self.presence_penalty,\n            frequency_penalty=self.frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            min_p=min_p,\n            seed=self.seed,\n            stop=self.stop,\n            stop_token_ids=self.stop_token_ids,\n            logprobs=self.top_logprobs if self.logprobs else None,\n            prompt_logprobs=prompt_logprobs,\n            ignore_eos=self.ignore_eos,\n            max_tokens=max_tokens,\n            min_tokens=self.min_tokens,\n            skip_special_tokens=self.skip_special_tokens,\n            spaces_between_special_tokens=self.spaces_between_special_tokens,\n            logits_processors=get_logits_processors(\n                self.logits_processors, logits_processor_pattern\n            ),\n            include_stop_str_in_output=self.include_stop_str_in_output,\n            truncate_prompt_tokens=self.truncate_prompt_tokens,\n            output_kind=RequestOutputKind.DELTA\n            if self.stream\n            else RequestOutputKind.FINAL_ONLY,\n            structured_outputs=self.structured_outputs,\n            logit_bias=self.logit_bias,\n            bad_words=self.bad_words,\n            allowed_token_ids=self.allowed_token_ids,\n            extra_args=extra_args or None,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_stream_options(cls, data):\n        if data.get(\"stream_options\") and not data.get(\"stream\"):\n            raise VLLMValidationError(\n                \"Stream options can only be defined when `stream=True`.\",\n                parameter=\"stream_options\",\n            )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_logprobs(cls, data):\n        if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n            if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1):\n                raise VLLMValidationError(\n                    \"`prompt_logprobs` are not available when `stream=True`.\",\n                    parameter=\"prompt_logprobs\",\n                )\n\n            if prompt_logprobs < 0 and prompt_logprobs != -1:\n                raise VLLMValidationError(\n                    \"`prompt_logprobs` must be a positive value or -1.\",\n                    parameter=\"prompt_logprobs\",\n                    value=prompt_logprobs,\n                )\n        if (top_logprobs := data.get(\"top_logprobs\")) is not None:\n            if top_logprobs < 0 and top_logprobs != -1:\n                raise VLLMValidationError(\n                    \"`top_logprobs` must be a positive value or -1.\",\n                    parameter=\"top_logprobs\",\n                    value=top_logprobs,\n                )\n\n            if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"):\n                raise VLLMValidationError(\n                    \"when using `top_logprobs`, `logprobs` must be set to true.\",\n                    parameter=\"top_logprobs\",\n                )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_structured_outputs_count(cls, data):\n        if isinstance(data, ValueError):\n            raise data\n\n        if data.get(\"structured_outputs\", None) is None:\n            return data\n\n        structured_outputs_kwargs = data[\"structured_outputs\"]\n        count = sum(\n            structured_outputs_kwargs.get(k) is not None\n            for k in (\"json\", \"regex\", \"choice\")\n        )\n        # you can only use one kind of constraints for structured outputs\n        if count > 1:\n            raise ValueError(\n                \"You can only use one kind of constraints for structured \"\n                \"outputs ('json', 'regex' or 'choice').\"\n            )\n        # you can only either use structured outputs or tools, not both\n        if count > 1 and data.get(\"tool_choice\", \"none\") not in (\n            \"none\",\n            \"auto\",\n            \"required\",\n        ):\n            raise ValueError(\n                \"You can only either use constraints for structured outputs \"\n                \"or tools, not both.\"\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_tool_usage(cls, data):\n        # if \"tool_choice\" is not specified but tools are provided,\n        # default to \"auto\" tool_choice\n        if \"tool_choice\" not in data and data.get(\"tools\"):\n            data[\"tool_choice\"] = \"auto\"\n\n        # if \"tool_choice\" is \"none\" -- no validation is needed for tools\n        if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\":\n            return data\n\n        # if \"tool_choice\" is specified -- validation\n        if \"tool_choice\" in data and data[\"tool_choice\"] is not None:\n            # ensure that if \"tool choice\" is specified, tools are present\n            if \"tools\" not in data or data[\"tools\"] is None:\n                raise ValueError(\"When using `tool_choice`, `tools` must be set.\")\n\n            # make sure that tool choice is either a named tool\n            # OR that it's set to \"auto\" or \"required\"\n            if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance(\n                data[\"tool_choice\"], dict\n            ):\n                raise ValueError(\n                    f\"Invalid value for `tool_choice`: {data['tool_choice']}! \"\n                    'Only named tools, \"none\", \"auto\" or \"required\" '\n                    \"are supported.\"\n                )\n\n            # if tool_choice is \"required\" but the \"tools\" list is empty,\n            # override the data to behave like \"none\" to align with\n            # OpenAI’s behavior.\n            if (\n                data[\"tool_choice\"] == \"required\"\n                and isinstance(data[\"tools\"], list)\n                and len(data[\"tools\"]) == 0\n            ):\n                data[\"tool_choice\"] = \"none\"\n                del data[\"tools\"]\n                return data\n\n            # ensure that if \"tool_choice\" is specified as an object,\n            # it matches a valid tool\n            correct_usage_message = (\n                'Correct usage: `{\"type\": \"function\",'\n                ' \"function\": {\"name\": \"my_function\"}}`'\n            )\n            if isinstance(data[\"tool_choice\"], dict):\n                valid_tool = False\n                function = data[\"tool_choice\"].get(\"function\")\n                if not isinstance(function, dict):\n                    raise ValueError(\n                        f\"Invalid value for `function`: `{function}` in \"\n                        f\"`tool_choice`! {correct_usage_message}\"\n                    )\n                if \"name\" not in function:\n                    raise ValueError(\n                        f\"Expected field `name` in `function` in \"\n                        f\"`tool_choice`! {correct_usage_message}\"\n                    )\n                function_name = function[\"name\"]\n                if not isinstance(function_name, str) or len(function_name) == 0:\n                    raise ValueError(\n                        f\"Invalid `name` in `function`: `{function_name}`\"\n                        f\" in `tool_choice`! {correct_usage_message}\"\n                    )\n                for tool in data[\"tools\"]:\n                    if tool[\"function\"][\"name\"] == function_name:\n                        valid_tool = True\n                        break\n                if not valid_tool:\n                    raise ValueError(\n                        \"The tool specified in `tool_choice` does not match any\"\n                        \" of the specified `tools`\"\n                    )\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_generation_prompt(cls, data):\n        if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"):\n            raise ValueError(\n                \"Cannot set both `continue_final_message` and \"\n                \"`add_generation_prompt` to True.\"\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_cache_salt_support(cls, data):\n        if data.get(\"cache_salt\") is not None and (\n            not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n        ):\n            raise ValueError(\n                \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n            )\n        return data",
      "language": "python"
    },
    {
      "code": "class ChatCompletionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/chat/create\n    messages: list[ChatCompletionMessageParam]\n    model: str | None = None\n    frequency_penalty: float | None = 0.0\n    logit_bias: dict[str, float] | None = None\n    logprobs: bool | None = False\n    top_logprobs: int | None = 0\n    max_tokens: int | None = Field(\n        default=None,\n        deprecated=\"max_tokens is deprecated in favor of \"\n        \"the max_completion_tokens field\",\n    )\n    max_completion_tokens: int | None = None\n    n: int | None = 1\n    presence_penalty: float | None = 0.0\n    response_format: AnyResponseFormat | None = None\n    seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    stop: str | list[str] | None = []\n    stream: bool | None = False\n    stream_options: StreamOptions | None = None\n    temperature: float | None = None\n    top_p: float | None = None\n    tools: list[ChatCompletionToolsParam] | None = None\n    tool_choice: (\n        Literal[\"none\"]\n        | Literal[\"auto\"]\n        | Literal[\"required\"]\n        | ChatCompletionNamedToolChoiceParam\n        | None\n    ) = \"none\"\n    reasoning_effort: Literal[\"low\", \"medium\", \"high\"] | None = None\n    include_reasoning: bool = True\n    parallel_tool_calls: bool | None = True\n\n    # NOTE this will be ignored by vLLM\n    user: str | None = None\n\n    # --8<-- [start:chat-completion-sampling-params]\n    use_beam_search: bool = False\n    top_k: int | None = None\n    min_p: float | None = None\n    repetition_penalty: float | None = None\n    length_penalty: float = 1.0\n    stop_token_ids: list[int] | None = []\n    include_stop_str_in_output: bool = False\n    ignore_eos: bool = False\n    min_tokens: int = 0\n    skip_special_tokens: bool = True\n    spaces_between_special_tokens: bool = True\n    truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None\n    prompt_logprobs: int | None = None\n    allowed_token_ids: list[int] | None = None\n    bad_words: list[str] = Field(default_factory=list)\n    # --8<-- [end:chat-completion-sampling-params]\n\n    # --8<-- [start:chat-completion-extra-params]\n    echo: bool = Field(\n        default=False,\n        description=(\n            \"If true, the new message will be prepended with the last message \"\n            \"if they belong to the same role.\"\n        ),\n    )\n    add_generation_prompt: bool = Field(\n        default=True,\n        description=(\n            \"If true, the generation prompt will be added to the chat template. \"\n            \"This is a parameter used by chat template in tokenizer config of the \"\n            \"model.\"\n        ),\n    )\n    continue_final_message: bool = Field(\n        default=False,\n        description=(\n            \"If this is set, the chat will be formatted so that the final \"\n            \"message in the chat is open-ended, without any EOS tokens. The \"\n            \"model will continue this message rather than starting a new one. \"\n            'This allows you to \"prefill\" part of the model\\'s response for it. '\n            \"Cannot be used at the same time as `add_generation_prompt`.\"\n        ),\n    )\n    add_special_tokens: bool = Field(\n        default=False,\n        description=(\n            \"If true, special tokens (e.g. BOS) will be added to the prompt \"\n            \"on top of what is added by the chat template. \"\n            \"For most models, the chat template takes care of adding the \"\n            \"special tokens so this should be set to false (as is the \"\n            \"default).\"\n        ),\n    )\n    documents: list[dict[str, str]] | None = Field(\n        default=None,\n        description=(\n            \"A list of dicts representing documents that will be accessible to \"\n            \"the model if it is performing RAG (retrieval-augmented generation).\"\n            \" If the template does not support RAG, this argument will have no \"\n            \"effect. We recommend that each document should be a dict containing \"\n            '\"title\" and \"text\" keys.'\n        ),\n    )\n    chat_template: str | None = Field(\n        default=None,\n        description=(\n            \"A Jinja template to use for this conversion. \"\n            \"As of transformers v4.44, default chat template is no longer \"\n            \"allowed, so you must provide a chat template if the tokenizer \"\n            \"does not define one.\"\n        ),\n    )\n    chat_template_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\n            \"Additional keyword args to pass to the template renderer. \"\n            \"Will be accessible by the chat template.\"\n        ),\n    )\n    mm_processor_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the HF processor.\"),\n    )\n    structured_outputs: StructuredOutputsParams | None = Field(\n        default=None,\n        description=\"Additional kwargs for structured outputs\",\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"\n        ),\n    )\n    request_id: str = Field(\n        default_factory=random_uuid,\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    logits_processors: LogitsProcessors | None = Field(\n        default=None,\n        description=(\n            \"A list of either qualified names of logits processors, or \"\n            \"constructor objects, to apply when sampling. A constructor is \"\n            \"a JSON object with a required 'qualname' field specifying the \"\n            \"qualified name of the processor class/factory, and optional \"\n            \"'args' and 'kwargs' fields containing positional and keyword \"\n            \"arguments. For example: {'qualname': \"\n            \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \"\n            \"{'param': 'value'}}.\"\n        ),\n    )\n    return_tokens_as_token_ids: bool | None = Field(\n        default=None,\n        description=(\n            \"If specified with 'logprobs', tokens are represented \"\n            \" as strings of the form 'token_id:{token_id}' so that tokens \"\n            \"that are not JSON-encodable can be identified.\"\n        ),\n    )\n    return_token_ids: bool | None = Field(\n        default=None,\n        description=(\n            \"If specified, the result will include token IDs alongside the \"\n            \"generated text. In streaming mode, prompt_token_ids is included \"\n            \"only in the first chunk, and token_ids contains the delta tokens \"\n            \"for each chunk. This is useful for debugging or when you \"\n            \"need to map generated text back to input tokens.\"\n        ),\n    )\n    cache_salt: str | None = Field(\n        default=None,\n        description=(\n            \"If specified, the prefix cache will be salted with the provided \"\n            \"string to prevent an attacker to guess prompts in multi-user \"\n            \"environments. The salt should be random, protected from \"\n            \"access by 3rd parties, and long enough to be \"\n            \"unpredictable (e.g., 43 characters base64-encoded, corresponding \"\n            \"to 256 bit).\"\n        ),\n    )\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None,\n        description=\"KVTransfer parameters used for disaggregated serving.\",\n    )\n\n    vllm_xargs: dict[str, str | int | float | list[str | int | float]] | None = Field(\n        default=None,\n        description=(\n            \"Additional request parameters with (list of) string or \"\n            \"numeric values, used by custom extensions.\"\n        ),\n    )\n\n    # --8<-- [end:chat-completion-extra-params]\n\n    # Default sampling parameters for chat completion requests\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"repetition_penalty\": 1.0,\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": 0,\n        \"min_p\": 0.0,\n    }\n\n    def to_beam_search_params(\n        self, max_tokens: int, default_sampling_params: dict\n    ) -> BeamSearchParams:\n        n = self.n if self.n is not None else 1\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n\n        return BeamSearchParams(\n            beam_width=n,\n            max_tokens=max_tokens,\n            ignore_eos=self.ignore_eos,\n            temperature=temperature,\n            length_penalty=self.length_penalty,\n            include_stop_str_in_output=self.include_stop_str_in_output,\n        )\n\n    def to_sampling_params(\n        self,\n        max_tokens: int,\n        logits_processor_pattern: str | None,\n        default_sampling_params: dict,\n    ) -> SamplingParams:\n        # Default parameters\n        if (repetition_penalty := self.repetition_penalty) is None:\n            repetition_penalty = default_sampling_params.get(\n                \"repetition_penalty\",\n                self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n            )\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n            )\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n            )\n        if (min_p := self.min_p) is None:\n            min_p = default_sampling_params.get(\n                \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n            )\n\n        prompt_logprobs = self.prompt_logprobs\n        if prompt_logprobs is None and self.echo:\n            prompt_logprobs = self.top_logprobs\n\n        response_format = self.response_format\n        if response_format is not None:\n            # If structured outputs wasn't already enabled,\n            # we must enable it for these features to work\n            if self.structured_outputs is None:\n                self.structured_outputs = StructuredOutputsParams()\n\n            # Set structured output params for response format\n            if response_format.type == \"json_object\":\n                self.structured_outputs.json_object = True\n            elif response_format.type == \"json_schema\":\n                json_schema = response_format.json_schema\n                assert json_schema is not None\n                self.structured_outputs.json = json_schema.json_schema\n            elif response_format.type == \"structural_tag\":\n                structural_tag = response_format\n                assert structural_tag is not None and isinstance(\n                    structural_tag,\n                    (\n                        LegacyStructuralTagResponseFormat,\n                        StructuralTagResponseFormat,\n                    ),\n                )\n                s_tag_obj = structural_tag.model_dump(by_alias=True)\n                self.structured_outputs.structural_tag = json.dumps(s_tag_obj)\n\n        extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {}\n        if self.kv_transfer_params:\n            # Pass in kv_transfer_params via extra_args\n            extra_args[\"kv_transfer_params\"] = self.kv_transfer_params\n        return SamplingParams.from_optional(\n            n=self.n,\n            presence_penalty=self.presence_penalty,\n            frequency_penalty=self.frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            min_p=min_p,\n            seed=self.seed,\n            stop=self.stop,\n            stop_token_ids=self.stop_token_ids,\n            logprobs=self.top_logprobs if self.logprobs else None,\n            prompt_logprobs=prompt_logprobs,\n            ignore_eos=self.ignore_eos,\n            max_tokens=max_tokens,\n            min_tokens=self.min_tokens,\n            skip_special_tokens=self.skip_special_tokens,\n            spaces_between_special_tokens=self.spaces_between_special_tokens,\n            logits_processors=get_logits_processors(\n                self.logits_processors, logits_processor_pattern\n            ),\n            include_stop_str_in_output=self.include_stop_str_in_output,\n            truncate_prompt_tokens=self.truncate_prompt_tokens,\n            output_kind=RequestOutputKind.DELTA\n            if self.stream\n            else RequestOutputKind.FINAL_ONLY,\n            structured_outputs=self.structured_outputs,\n            logit_bias=self.logit_bias,\n            bad_words=self.bad_words,\n            allowed_token_ids=self.allowed_token_ids,\n            extra_args=extra_args or None,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_stream_options(cls, data):\n        if data.get(\"stream_options\") and not data.get(\"stream\"):\n            raise VLLMValidationError(\n                \"Stream options can only be defined when `stream=True`.\",\n                parameter=\"stream_options\",\n            )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_logprobs(cls, data):\n        if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n            if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1):\n                raise VLLMValidationError(\n                    \"`prompt_logprobs` are not available when `stream=True`.\",\n                    parameter=\"prompt_logprobs\",\n                )\n\n            if prompt_logprobs < 0 and prompt_logprobs != -1:\n                raise VLLMValidationError(\n                    \"`prompt_logprobs` must be a positive value or -1.\",\n                    parameter=\"prompt_logprobs\",\n                    value=prompt_logprobs,\n                )\n        if (top_logprobs := data.get(\"top_logprobs\")) is not None:\n            if top_logprobs < 0 and top_logprobs != -1:\n                raise VLLMValidationError(\n                    \"`top_logprobs` must be a positive value or -1.\",\n                    parameter=\"top_logprobs\",\n                    value=top_logprobs,\n                )\n\n            if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"):\n                raise VLLMValidationError(\n                    \"when using `top_logprobs`, `logprobs` must be set to true.\",\n                    parameter=\"top_logprobs\",\n                )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_structured_outputs_count(cls, data):\n        if isinstance(data, ValueError):\n            raise data\n\n        if data.get(\"structured_outputs\", None) is None:\n            return data\n\n        structured_outputs_kwargs = data[\"structured_outputs\"]\n        count = sum(\n            structured_outputs_kwargs.get(k) is not None\n            for k in (\"json\", \"regex\", \"choice\")\n        )\n        # you can only use one kind of constraints for structured outputs\n        if count > 1:\n            raise ValueError(\n                \"You can only use one kind of constraints for structured \"\n                \"outputs ('json', 'regex' or 'choice').\"\n            )\n        # you can only either use structured outputs or tools, not both\n        if count > 1 and data.get(\"tool_choice\", \"none\") not in (\n            \"none\",\n            \"auto\",\n            \"required\",\n        ):\n            raise ValueError(\n                \"You can only either use constraints for structured outputs \"\n                \"or tools, not both.\"\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_tool_usage(cls, data):\n        # if \"tool_choice\" is not specified but tools are provided,\n        # default to \"auto\" tool_choice\n        if \"tool_choice\" not in data and data.get(\"tools\"):\n            data[\"tool_choice\"] = \"auto\"\n\n        # if \"tool_choice\" is \"none\" -- no validation is needed for tools\n        if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\":\n            return data\n\n        # if \"tool_choice\" is specified -- validation\n        if \"tool_choice\" in data and data[\"tool_choice\"] is not None:\n            # ensure that if \"tool choice\" is specified, tools are present\n            if \"tools\" not in data or data[\"tools\"] is None:\n                raise ValueError(\"When using `tool_choice`, `tools` must be set.\")\n\n            # make sure that tool choice is either a named tool\n            # OR that it's set to \"auto\" or \"required\"\n            if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance(\n                data[\"tool_choice\"], dict\n            ):\n                raise ValueError(\n                    f\"Invalid value for `tool_choice`: {data['tool_choice']}! \"\n                    'Only named tools, \"none\", \"auto\" or \"required\" '\n                    \"are supported.\"\n                )\n\n            # if tool_choice is \"required\" but the \"tools\" list is empty,\n            # override the data to behave like \"none\" to align with\n            # OpenAI’s behavior.\n            if (\n                data[\"tool_choice\"] == \"required\"\n                and isinstance(data[\"tools\"], list)\n                and len(data[\"tools\"]) == 0\n            ):\n                data[\"tool_choice\"] = \"none\"\n                del data[\"tools\"]\n                return data\n\n            # ensure that if \"tool_choice\" is specified as an object,\n            # it matches a valid tool\n            correct_usage_message = (\n                'Correct usage: `{\"type\": \"function\",'\n                ' \"function\": {\"name\": \"my_function\"}}`'\n            )\n            if isinstance(data[\"tool_choice\"], dict):\n                valid_tool = False\n                function = data[\"tool_choice\"].get(\"function\")\n                if not isinstance(function, dict):\n                    raise ValueError(\n                        f\"Invalid value for `function`: `{function}` in \"\n                        f\"`tool_choice`! {correct_usage_message}\"\n                    )\n                if \"name\" not in function:\n                    raise ValueError(\n                        f\"Expected field `name` in `function` in \"\n                        f\"`tool_choice`! {correct_usage_message}\"\n                    )\n                function_name = function[\"name\"]\n                if not isinstance(function_name, str) or len(function_name) == 0:\n                    raise ValueError(\n                        f\"Invalid `name` in `function`: `{function_name}`\"\n                        f\" in `tool_choice`! {correct_usage_message}\"\n                    )\n                for tool in data[\"tools\"]:\n                    if tool[\"function\"][\"name\"] == function_name:\n                        valid_tool = True\n                        break\n                if not valid_tool:\n                    raise ValueError(\n                        \"The tool specified in `tool_choice` does not match any\"\n                        \" of the specified `tools`\"\n                    )\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_generation_prompt(cls, data):\n        if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"):\n            raise ValueError(\n                \"Cannot set both `continue_final_message` and \"\n                \"`add_generation_prompt` to True.\"\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_cache_salt_support(cls, data):\n        if data.get(\"cache_salt\") is not None and (\n            not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n        ):\n            raise ValueError(\n                \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n            )\n        return data",
      "language": "python"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS: dict = {\n    \"repetition_penalty\": 1.0,\n    \"temperature\": 1.0,\n    \"top_p\": 1.0,\n    \"top_k\": 0,\n    \"min_p\": 0.0,\n}",
      "language": "json"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS: dict = {\n    \"repetition_penalty\": 1.0,\n    \"temperature\": 1.0,\n    \"top_p\": 1.0,\n    \"top_k\": 0,\n    \"min_p\": 0.0,\n}",
      "language": "json"
    },
    {
      "code": "add_generation_prompt: bool = Field(\n    default=True,\n    description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\",\n)",
      "language": "typescript"
    },
    {
      "code": "add_generation_prompt: bool = Field(\n    default=True,\n    description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\",\n)",
      "language": "typescript"
    },
    {
      "code": "add_special_tokens: bool = Field(\n    default=False,\n    description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\",\n)",
      "language": "typescript"
    },
    {
      "code": "add_special_tokens: bool = Field(\n    default=False,\n    description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\",\n)",
      "language": "typescript"
    },
    {
      "code": "allowed_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "allowed_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "bad_words: list[str] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "bad_words: list[str] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "cache_salt: str | None = Field(\n    default=None,\n    description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\",\n)",
      "language": "sql"
    },
    {
      "code": "cache_salt: str | None = Field(\n    default=None,\n    description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\",\n)",
      "language": "sql"
    },
    {
      "code": "chat_template: str | None = Field(\n    default=None,\n    description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\",\n)",
      "language": "yaml"
    },
    {
      "code": "chat_template: str | None = Field(\n    default=None,\n    description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\",\n)",
      "language": "yaml"
    },
    {
      "code": "chat_template_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\",\n)",
      "language": "csharp"
    },
    {
      "code": "chat_template_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\",\n)",
      "language": "csharp"
    },
    {
      "code": "continue_final_message: bool = Field(\n    default=False,\n    description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.',\n)",
      "language": "typescript"
    },
    {
      "code": "continue_final_message: bool = Field(\n    default=False,\n    description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.',\n)",
      "language": "typescript"
    },
    {
      "code": "documents: list[dict[str, str]] | None = Field(\n    default=None,\n    description='A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.',\n)",
      "language": "yaml"
    },
    {
      "code": "documents: list[dict[str, str]] | None = Field(\n    default=None,\n    description='A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.',\n)",
      "language": "yaml"
    },
    {
      "code": "echo: bool = Field(\n    default=False,\n    description=\"If true, the new message will be prepended with the last message if they belong to the same role.\",\n)",
      "language": "typescript"
    },
    {
      "code": "echo: bool = Field(\n    default=False,\n    description=\"If true, the new message will be prepended with the last message if they belong to the same role.\",\n)",
      "language": "typescript"
    },
    {
      "code": "frequency_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "frequency_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "ignore_eos: bool = False",
      "language": "typescript"
    },
    {
      "code": "ignore_eos: bool = False",
      "language": "typescript"
    },
    {
      "code": "include_reasoning: bool = True",
      "language": "typescript"
    },
    {
      "code": "include_reasoning: bool = True",
      "language": "typescript"
    },
    {
      "code": "include_stop_str_in_output: bool = False",
      "language": "typescript"
    },
    {
      "code": "include_stop_str_in_output: bool = False",
      "language": "typescript"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None,\n    description=\"KVTransfer parameters used for disaggregated serving.\",\n)",
      "language": "yaml"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None,\n    description=\"KVTransfer parameters used for disaggregated serving.\",\n)",
      "language": "yaml"
    },
    {
      "code": "length_penalty: float = 1.0",
      "language": "typescript"
    },
    {
      "code": "length_penalty: float = 1.0",
      "language": "typescript"
    },
    {
      "code": "logit_bias: dict[str, float] | None = None",
      "language": "yaml"
    },
    {
      "code": "logit_bias: dict[str, float] | None = None",
      "language": "yaml"
    },
    {
      "code": "logits_processors: LogitsProcessors | None = Field(\n    default=None,\n    description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\",\n)",
      "language": "yaml"
    },
    {
      "code": "logits_processors: LogitsProcessors | None = Field(\n    default=None,\n    description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\",\n)",
      "language": "yaml"
    },
    {
      "code": "logprobs: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "logprobs: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "max_completion_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_completion_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_tokens: int | None = Field(\n    default=None,\n    deprecated=\"max_tokens is deprecated in favor of the max_completion_tokens field\",\n)",
      "language": "yaml"
    },
    {
      "code": "max_tokens: int | None = Field(\n    default=None,\n    deprecated=\"max_tokens is deprecated in favor of the max_completion_tokens field\",\n)",
      "language": "yaml"
    },
    {
      "code": "messages: list[ChatCompletionMessageParam]",
      "language": "yaml"
    },
    {
      "code": "messages: list[ChatCompletionMessageParam]",
      "language": "yaml"
    },
    {
      "code": "min_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "min_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "min_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "min_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "mm_processor_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional kwargs to pass to the HF processor.\",\n)",
      "language": "yaml"
    },
    {
      "code": "mm_processor_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional kwargs to pass to the HF processor.\",\n)",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "n: int | None = 1",
      "language": "yaml"
    },
    {
      "code": "n: int | None = 1",
      "language": "yaml"
    },
    {
      "code": "parallel_tool_calls: bool | None = True",
      "language": "yaml"
    },
    {
      "code": "parallel_tool_calls: bool | None = True",
      "language": "yaml"
    },
    {
      "code": "presence_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "presence_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "priority: int = Field(\n    default=0,\n    description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\",\n)",
      "language": "typescript"
    },
    {
      "code": "priority: int = Field(\n    default=0,\n    description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\",\n)",
      "language": "typescript"
    },
    {
      "code": "prompt_logprobs: int | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: int | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning_effort: (\n    Literal[\"low\", \"medium\", \"high\"] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "reasoning_effort: (\n    Literal[\"low\", \"medium\", \"high\"] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "repetition_penalty: float | None = None",
      "language": "yaml"
    },
    {
      "code": "repetition_penalty: float | None = None",
      "language": "yaml"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=random_uuid,\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=random_uuid,\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "response_format: AnyResponseFormat | None = None",
      "language": "yaml"
    },
    {
      "code": "response_format: AnyResponseFormat | None = None",
      "language": "yaml"
    },
    {
      "code": "return_token_ids: bool | None = Field(\n    default=None,\n    description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_token_ids: bool | None = Field(\n    default=None,\n    description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_tokens_as_token_ids: bool | None = Field(\n    default=None,\n    description=\"If specified with 'logprobs', tokens are represented  as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_tokens_as_token_ids: bool | None = Field(\n    default=None,\n    description=\"If specified with 'logprobs', tokens are represented  as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\",\n)",
      "language": "yaml"
    },
    {
      "code": "seed: int | None = Field(None, ge=min, le=max)",
      "language": "yaml"
    },
    {
      "code": "seed: int | None = Field(None, ge=min, le=max)",
      "language": "yaml"
    },
    {
      "code": "skip_special_tokens: bool = True",
      "language": "typescript"
    },
    {
      "code": "skip_special_tokens: bool = True",
      "language": "typescript"
    },
    {
      "code": "spaces_between_special_tokens: bool = True",
      "language": "typescript"
    },
    {
      "code": "spaces_between_special_tokens: bool = True",
      "language": "typescript"
    },
    {
      "code": "stop: str | list[str] | None = []",
      "language": "yaml"
    },
    {
      "code": "stop: str | list[str] | None = []",
      "language": "yaml"
    },
    {
      "code": "stop_token_ids: list[int] | None = []",
      "language": "yaml"
    },
    {
      "code": "stop_token_ids: list[int] | None = []",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_options: StreamOptions | None = None",
      "language": "yaml"
    },
    {
      "code": "stream_options: StreamOptions | None = None",
      "language": "yaml"
    },
    {
      "code": "structured_outputs: StructuredOutputsParams | None = Field(\n    default=None,\n    description=\"Additional kwargs for structured outputs\",\n)",
      "language": "yaml"
    },
    {
      "code": "structured_outputs: StructuredOutputsParams | None = Field(\n    default=None,\n    description=\"Additional kwargs for structured outputs\",\n)",
      "language": "yaml"
    },
    {
      "code": "temperature: float | None = None",
      "language": "yaml"
    },
    {
      "code": "temperature: float | None = None",
      "language": "yaml"
    },
    {
      "code": "tool_choice: (\n    Literal[\"none\"]\n    | Literal[\"auto\"]\n    | Literal[\"required\"]\n    | ChatCompletionNamedToolChoiceParam\n    | None\n) = \"none\"",
      "language": "yaml"
    },
    {
      "code": "tool_choice: (\n    Literal[\"none\"]\n    | Literal[\"auto\"]\n    | Literal[\"required\"]\n    | ChatCompletionNamedToolChoiceParam\n    | None\n) = \"none\"",
      "language": "yaml"
    },
    {
      "code": "tools: list[ChatCompletionToolsParam] | None = None",
      "language": "yaml"
    },
    {
      "code": "tools: list[ChatCompletionToolsParam] | None = None",
      "language": "yaml"
    },
    {
      "code": "top_k: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_k: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: int | None = 0",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: int | None = 0",
      "language": "yaml"
    },
    {
      "code": "top_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "top_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "truncate_prompt_tokens: (\n    Annotated[int, Field(ge=-1)] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "truncate_prompt_tokens: (\n    Annotated[int, Field(ge=-1)] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "use_beam_search: bool = False",
      "language": "typescript"
    },
    {
      "code": "use_beam_search: bool = False",
      "language": "typescript"
    },
    {
      "code": "user: str | None = None",
      "language": "yaml"
    },
    {
      "code": "user: str | None = None",
      "language": "yaml"
    },
    {
      "code": "vllm_xargs: (\n    dict[str, str | int | float | list[str | int | float]]\n    | None\n) = Field(\n    default=None,\n    description=\"Additional request parameters with (list of) string or numeric values, used by custom extensions.\",\n)",
      "language": "yaml"
    },
    {
      "code": "vllm_xargs: (\n    dict[str, str | int | float | list[str | int | float]]\n    | None\n) = Field(\n    default=None,\n    description=\"Additional request parameters with (list of) string or numeric values, used by custom extensions.\",\n)",
      "language": "yaml"
    },
    {
      "code": "check_cache_salt_support(data)",
      "language": "unknown"
    },
    {
      "code": "check_cache_salt_support(data)",
      "language": "unknown"
    },
    {
      "code": "1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_cache_salt_support(cls, data):\n    if data.get(\"cache_salt\") is not None and (\n        not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n    ):\n        raise ValueError(\n            \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_cache_salt_support(cls, data):\n    if data.get(\"cache_salt\") is not None and (\n        not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n    ):\n        raise ValueError(\n            \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "check_generation_prompt(data)",
      "language": "unknown"
    },
    {
      "code": "check_generation_prompt(data)",
      "language": "unknown"
    },
    {
      "code": "1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_generation_prompt(cls, data):\n    if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"):\n        raise ValueError(\n            \"Cannot set both `continue_final_message` and \"\n            \"`add_generation_prompt` to True.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_generation_prompt(cls, data):\n    if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"):\n        raise ValueError(\n            \"Cannot set both `continue_final_message` and \"\n            \"`add_generation_prompt` to True.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "check_logprobs(data)",
      "language": "unknown"
    },
    {
      "code": "check_logprobs(data)",
      "language": "unknown"
    },
    {
      "code": "894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_logprobs(cls, data):\n    if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n        if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1):\n            raise VLLMValidationError(\n                \"`prompt_logprobs` are not available when `stream=True`.\",\n                parameter=\"prompt_logprobs\",\n            )\n\n        if prompt_logprobs < 0 and prompt_logprobs != -1:\n            raise VLLMValidationError(\n                \"`prompt_logprobs` must be a positive value or -1.\",\n                parameter=\"prompt_logprobs\",\n                value=prompt_logprobs,\n            )\n    if (top_logprobs := data.get(\"top_logprobs\")) is not None:\n        if top_logprobs < 0 and top_logprobs != -1:\n            raise VLLMValidationError(\n                \"`top_logprobs` must be a positive value or -1.\",\n                parameter=\"top_logprobs\",\n                value=top_logprobs,\n            )\n\n        if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"):\n            raise VLLMValidationError(\n                \"when using `top_logprobs`, `logprobs` must be set to true.\",\n                parameter=\"top_logprobs\",\n            )\n\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_logprobs(cls, data):\n    if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n        if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1):\n            raise VLLMValidationError(\n                \"`prompt_logprobs` are not available when `stream=True`.\",\n                parameter=\"prompt_logprobs\",\n            )\n\n        if prompt_logprobs < 0 and prompt_logprobs != -1:\n            raise VLLMValidationError(\n                \"`prompt_logprobs` must be a positive value or -1.\",\n                parameter=\"prompt_logprobs\",\n                value=prompt_logprobs,\n            )\n    if (top_logprobs := data.get(\"top_logprobs\")) is not None:\n        if top_logprobs < 0 and top_logprobs != -1:\n            raise VLLMValidationError(\n                \"`top_logprobs` must be a positive value or -1.\",\n                parameter=\"top_logprobs\",\n                value=top_logprobs,\n            )\n\n        if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"):\n            raise VLLMValidationError(\n                \"when using `top_logprobs`, `logprobs` must be set to true.\",\n                parameter=\"top_logprobs\",\n            )\n\n    return data",
      "language": "python"
    },
    {
      "code": "check_structured_outputs_count(data)",
      "language": "unknown"
    },
    {
      "code": "check_structured_outputs_count(data)",
      "language": "unknown"
    },
    {
      "code": "926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_structured_outputs_count(cls, data):\n    if isinstance(data, ValueError):\n        raise data\n\n    if data.get(\"structured_outputs\", None) is None:\n        return data\n\n    structured_outputs_kwargs = data[\"structured_outputs\"]\n    count = sum(\n        structured_outputs_kwargs.get(k) is not None\n        for k in (\"json\", \"regex\", \"choice\")\n    )\n    # you can only use one kind of constraints for structured outputs\n    if count > 1:\n        raise ValueError(\n            \"You can only use one kind of constraints for structured \"\n            \"outputs ('json', 'regex' or 'choice').\"\n        )\n    # you can only either use structured outputs or tools, not both\n    if count > 1 and data.get(\"tool_choice\", \"none\") not in (\n        \"none\",\n        \"auto\",\n        \"required\",\n    ):\n        raise ValueError(\n            \"You can only either use constraints for structured outputs \"\n            \"or tools, not both.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_structured_outputs_count(cls, data):\n    if isinstance(data, ValueError):\n        raise data\n\n    if data.get(\"structured_outputs\", None) is None:\n        return data\n\n    structured_outputs_kwargs = data[\"structured_outputs\"]\n    count = sum(\n        structured_outputs_kwargs.get(k) is not None\n        for k in (\"json\", \"regex\", \"choice\")\n    )\n    # you can only use one kind of constraints for structured outputs\n    if count > 1:\n        raise ValueError(\n            \"You can only use one kind of constraints for structured \"\n            \"outputs ('json', 'regex' or 'choice').\"\n        )\n    # you can only either use structured outputs or tools, not both\n    if count > 1 and data.get(\"tool_choice\", \"none\") not in (\n        \"none\",\n        \"auto\",\n        \"required\",\n    ):\n        raise ValueError(\n            \"You can only either use constraints for structured outputs \"\n            \"or tools, not both.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "check_tool_usage(data)",
      "language": "unknown"
    },
    {
      "code": "check_tool_usage(data)",
      "language": "unknown"
    },
    {
      "code": "958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_tool_usage(cls, data):\n    # if \"tool_choice\" is not specified but tools are provided,\n    # default to \"auto\" tool_choice\n    if \"tool_choice\" not in data and data.get(\"tools\"):\n        data[\"tool_choice\"] = \"auto\"\n\n    # if \"tool_choice\" is \"none\" -- no validation is needed for tools\n    if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\":\n        return data\n\n    # if \"tool_choice\" is specified -- validation\n    if \"tool_choice\" in data and data[\"tool_choice\"] is not None:\n        # ensure that if \"tool choice\" is specified, tools are present\n        if \"tools\" not in data or data[\"tools\"] is None:\n            raise ValueError(\"When using `tool_choice`, `tools` must be set.\")\n\n        # make sure that tool choice is either a named tool\n        # OR that it's set to \"auto\" or \"required\"\n        if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance(\n            data[\"tool_choice\"], dict\n        ):\n            raise ValueError(\n                f\"Invalid value for `tool_choice`: {data['tool_choice']}! \"\n                'Only named tools, \"none\", \"auto\" or \"required\" '\n                \"are supported.\"\n            )\n\n        # if tool_choice is \"required\" but the \"tools\" list is empty,\n        # override the data to behave like \"none\" to align with\n        # OpenAI’s behavior.\n        if (\n            data[\"tool_choice\"] == \"required\"\n            and isinstance(data[\"tools\"], list)\n            and len(data[\"tools\"]) == 0\n        ):\n            data[\"tool_choice\"] = \"none\"\n            del data[\"tools\"]\n            return data\n\n        # ensure that if \"tool_choice\" is specified as an object,\n        # it matches a valid tool\n        correct_usage_message = (\n            'Correct usage: `{\"type\": \"function\",'\n            ' \"function\": {\"name\": \"my_function\"}}`'\n        )\n        if isinstance(data[\"tool_choice\"], dict):\n            valid_tool = False\n            function = data[\"tool_choice\"].get(\"function\")\n            if not isinstance(function, dict):\n                raise ValueError(\n                    f\"Invalid value for `function`: `{function}` in \"\n                    f\"`tool_choice`! {correct_usage_message}\"\n                )\n            if \"name\" not in function:\n                raise ValueError(\n                    f\"Expected field `name` in `function` in \"\n                    f\"`tool_choice`! {correct_usage_message}\"\n                )\n            function_name = function[\"name\"]\n            if not isinstance(function_name, str) or len(function_name) == 0:\n                raise ValueError(\n                    f\"Invalid `name` in `function`: `{function_name}`\"\n                    f\" in `tool_choice`! {correct_usage_message}\"\n                )\n            for tool in data[\"tools\"]:\n                if tool[\"function\"][\"name\"] == function_name:\n                    valid_tool = True\n                    break\n            if not valid_tool:\n                raise ValueError(\n                    \"The tool specified in `tool_choice` does not match any\"\n                    \" of the specified `tools`\"\n                )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_tool_usage(cls, data):\n    # if \"tool_choice\" is not specified but tools are provided,\n    # default to \"auto\" tool_choice\n    if \"tool_choice\" not in data and data.get(\"tools\"):\n        data[\"tool_choice\"] = \"auto\"\n\n    # if \"tool_choice\" is \"none\" -- no validation is needed for tools\n    if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\":\n        return data\n\n    # if \"tool_choice\" is specified -- validation\n    if \"tool_choice\" in data and data[\"tool_choice\"] is not None:\n        # ensure that if \"tool choice\" is specified, tools are present\n        if \"tools\" not in data or data[\"tools\"] is None:\n            raise ValueError(\"When using `tool_choice`, `tools` must be set.\")\n\n        # make sure that tool choice is either a named tool\n        # OR that it's set to \"auto\" or \"required\"\n        if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance(\n            data[\"tool_choice\"], dict\n        ):\n            raise ValueError(\n                f\"Invalid value for `tool_choice`: {data['tool_choice']}! \"\n                'Only named tools, \"none\", \"auto\" or \"required\" '\n                \"are supported.\"\n            )\n\n        # if tool_choice is \"required\" but the \"tools\" list is empty,\n        # override the data to behave like \"none\" to align with\n        # OpenAI’s behavior.\n        if (\n            data[\"tool_choice\"] == \"required\"\n            and isinstance(data[\"tools\"], list)\n            and len(data[\"tools\"]) == 0\n        ):\n            data[\"tool_choice\"] = \"none\"\n            del data[\"tools\"]\n            return data\n\n        # ensure that if \"tool_choice\" is specified as an object,\n        # it matches a valid tool\n        correct_usage_message = (\n            'Correct usage: `{\"type\": \"function\",'\n            ' \"function\": {\"name\": \"my_function\"}}`'\n        )\n        if isinstance(data[\"tool_choice\"], dict):\n            valid_tool = False\n            function = data[\"tool_choice\"].get(\"function\")\n            if not isinstance(function, dict):\n                raise ValueError(\n                    f\"Invalid value for `function`: `{function}` in \"\n                    f\"`tool_choice`! {correct_usage_message}\"\n                )\n            if \"name\" not in function:\n                raise ValueError(\n                    f\"Expected field `name` in `function` in \"\n                    f\"`tool_choice`! {correct_usage_message}\"\n                )\n            function_name = function[\"name\"]\n            if not isinstance(function_name, str) or len(function_name) == 0:\n                raise ValueError(\n                    f\"Invalid `name` in `function`: `{function_name}`\"\n                    f\" in `tool_choice`! {correct_usage_message}\"\n                )\n            for tool in data[\"tools\"]:\n                if tool[\"function\"][\"name\"] == function_name:\n                    valid_tool = True\n                    break\n            if not valid_tool:\n                raise ValueError(\n                    \"The tool specified in `tool_choice` does not match any\"\n                    \" of the specified `tools`\"\n                )\n    return data",
      "language": "python"
    },
    {
      "code": "to_beam_search_params(\n    max_tokens: int, default_sampling_params: dict\n) -> BeamSearchParams",
      "language": "php"
    },
    {
      "code": "to_beam_search_params(\n    max_tokens: int, default_sampling_params: dict\n) -> BeamSearchParams",
      "language": "php"
    },
    {
      "code": "767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783",
      "language": "unknown"
    },
    {
      "code": "def to_beam_search_params(\n    self, max_tokens: int, default_sampling_params: dict\n) -> BeamSearchParams:\n    n = self.n if self.n is not None else 1\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n\n    return BeamSearchParams(\n        beam_width=n,\n        max_tokens=max_tokens,\n        ignore_eos=self.ignore_eos,\n        temperature=temperature,\n        length_penalty=self.length_penalty,\n        include_stop_str_in_output=self.include_stop_str_in_output,\n    )",
      "language": "python"
    },
    {
      "code": "def to_beam_search_params(\n    self, max_tokens: int, default_sampling_params: dict\n) -> BeamSearchParams:\n    n = self.n if self.n is not None else 1\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n\n    return BeamSearchParams(\n        beam_width=n,\n        max_tokens=max_tokens,\n        ignore_eos=self.ignore_eos,\n        temperature=temperature,\n        length_penalty=self.length_penalty,\n        include_stop_str_in_output=self.include_stop_str_in_output,\n    )",
      "language": "python"
    },
    {
      "code": "to_sampling_params(\n    max_tokens: int,\n    logits_processor_pattern: str | None,\n    default_sampling_params: dict,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "to_sampling_params(\n    max_tokens: int,\n    logits_processor_pattern: str | None,\n    default_sampling_params: dict,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881",
      "language": "unknown"
    },
    {
      "code": "def to_sampling_params(\n    self,\n    max_tokens: int,\n    logits_processor_pattern: str | None,\n    default_sampling_params: dict,\n) -> SamplingParams:\n    # Default parameters\n    if (repetition_penalty := self.repetition_penalty) is None:\n        repetition_penalty = default_sampling_params.get(\n            \"repetition_penalty\",\n            self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n        )\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n    if (top_p := self.top_p) is None:\n        top_p = default_sampling_params.get(\n            \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n        )\n    if (top_k := self.top_k) is None:\n        top_k = default_sampling_params.get(\n            \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n        )\n    if (min_p := self.min_p) is None:\n        min_p = default_sampling_params.get(\n            \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n        )\n\n    prompt_logprobs = self.prompt_logprobs\n    if prompt_logprobs is None and self.echo:\n        prompt_logprobs = self.top_logprobs\n\n    response_format = self.response_format\n    if response_format is not None:\n        # If structured outputs wasn't already enabled,\n        # we must enable it for these features to work\n        if self.structured_outputs is None:\n            self.structured_outputs = StructuredOutputsParams()\n\n        # Set structured output params for response format\n        if response_format.type == \"json_object\":\n            self.structured_outputs.json_object = True\n        elif response_format.type == \"json_schema\":\n            json_schema = response_format.json_schema\n            assert json_schema is not None\n            self.structured_outputs.json = json_schema.json_schema\n        elif response_format.type == \"structural_tag\":\n            structural_tag = response_format\n            assert structural_tag is not None and isinstance(\n                structural_tag,\n                (\n                    LegacyStructuralTagResponseFormat,\n                    StructuralTagResponseFormat,\n                ),\n            )\n            s_tag_obj = structural_tag.model_dump(by_alias=True)\n            self.structured_outputs.structural_tag = json.dumps(s_tag_obj)\n\n    extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {}\n    if self.kv_transfer_params:\n        # Pass in kv_transfer_params via extra_args\n        extra_args[\"kv_transfer_params\"] = self.kv_transfer_params\n    return SamplingParams.from_optional(\n        n=self.n,\n        presence_penalty=self.presence_penalty,\n        frequency_penalty=self.frequency_penalty,\n        repetition_penalty=repetition_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        min_p=min_p,\n        seed=self.seed,\n        stop=self.stop,\n        stop_token_ids=self.stop_token_ids,\n        logprobs=self.top_logprobs if self.logprobs else None,\n        prompt_logprobs=prompt_logprobs,\n        ignore_eos=self.ignore_eos,\n        max_tokens=max_tokens,\n        min_tokens=self.min_tokens,\n        skip_special_tokens=self.skip_special_tokens,\n        spaces_between_special_tokens=self.spaces_between_special_tokens,\n        logits_processors=get_logits_processors(\n            self.logits_processors, logits_processor_pattern\n        ),\n        include_stop_str_in_output=self.include_stop_str_in_output,\n        truncate_prompt_tokens=self.truncate_prompt_tokens,\n        output_kind=RequestOutputKind.DELTA\n        if self.stream\n        else RequestOutputKind.FINAL_ONLY,\n        structured_outputs=self.structured_outputs,\n        logit_bias=self.logit_bias,\n        bad_words=self.bad_words,\n        allowed_token_ids=self.allowed_token_ids,\n        extra_args=extra_args or None,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "def to_sampling_params(\n    self,\n    max_tokens: int,\n    logits_processor_pattern: str | None,\n    default_sampling_params: dict,\n) -> SamplingParams:\n    # Default parameters\n    if (repetition_penalty := self.repetition_penalty) is None:\n        repetition_penalty = default_sampling_params.get(\n            \"repetition_penalty\",\n            self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n        )\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n    if (top_p := self.top_p) is None:\n        top_p = default_sampling_params.get(\n            \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n        )\n    if (top_k := self.top_k) is None:\n        top_k = default_sampling_params.get(\n            \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n        )\n    if (min_p := self.min_p) is None:\n        min_p = default_sampling_params.get(\n            \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n        )\n\n    prompt_logprobs = self.prompt_logprobs\n    if prompt_logprobs is None and self.echo:\n        prompt_logprobs = self.top_logprobs\n\n    response_format = self.response_format\n    if response_format is not None:\n        # If structured outputs wasn't already enabled,\n        # we must enable it for these features to work\n        if self.structured_outputs is None:\n            self.structured_outputs = StructuredOutputsParams()\n\n        # Set structured output params for response format\n        if response_format.type == \"json_object\":\n            self.structured_outputs.json_object = True\n        elif response_format.type == \"json_schema\":\n            json_schema = response_format.json_schema\n            assert json_schema is not None\n            self.structured_outputs.json = json_schema.json_schema\n        elif response_format.type == \"structural_tag\":\n            structural_tag = response_format\n            assert structural_tag is not None and isinstance(\n                structural_tag,\n                (\n                    LegacyStructuralTagResponseFormat,\n                    StructuralTagResponseFormat,\n                ),\n            )\n            s_tag_obj = structural_tag.model_dump(by_alias=True)\n            self.structured_outputs.structural_tag = json.dumps(s_tag_obj)\n\n    extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {}\n    if self.kv_transfer_params:\n        # Pass in kv_transfer_params via extra_args\n        extra_args[\"kv_transfer_params\"] = self.kv_transfer_params\n    return SamplingParams.from_optional(\n        n=self.n,\n        presence_penalty=self.presence_penalty,\n        frequency_penalty=self.frequency_penalty,\n        repetition_penalty=repetition_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        min_p=min_p,\n        seed=self.seed,\n        stop=self.stop,\n        stop_token_ids=self.stop_token_ids,\n        logprobs=self.top_logprobs if self.logprobs else None,\n        prompt_logprobs=prompt_logprobs,\n        ignore_eos=self.ignore_eos,\n        max_tokens=max_tokens,\n        min_tokens=self.min_tokens,\n        skip_special_tokens=self.skip_special_tokens,\n        spaces_between_special_tokens=self.spaces_between_special_tokens,\n        logits_processors=get_logits_processors(\n            self.logits_processors, logits_processor_pattern\n        ),\n        include_stop_str_in_output=self.include_stop_str_in_output,\n        truncate_prompt_tokens=self.truncate_prompt_tokens,\n        output_kind=RequestOutputKind.DELTA\n        if self.stream\n        else RequestOutputKind.FINAL_ONLY,\n        structured_outputs=self.structured_outputs,\n        logit_bias=self.logit_bias,\n        bad_words=self.bad_words,\n        allowed_token_ids=self.allowed_token_ids,\n        extra_args=extra_args or None,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "validate_stream_options(data)",
      "language": "unknown"
    },
    {
      "code": "validate_stream_options(data)",
      "language": "unknown"
    },
    {
      "code": "883\n884\n885\n886\n887\n888\n889\n890\n891\n892",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_stream_options(cls, data):\n    if data.get(\"stream_options\") and not data.get(\"stream\"):\n        raise VLLMValidationError(\n            \"Stream options can only be defined when `stream=True`.\",\n            parameter=\"stream_options\",\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_stream_options(cls, data):\n    if data.get(\"stream_options\") and not data.get(\"stream\"):\n        raise VLLMValidationError(\n            \"Stream options can only be defined when `stream=True`.\",\n            parameter=\"stream_options\",\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "1565\n1566\n1567\n1568\n1569\n1570\n1571\n1572\n1573\n1574\n1575\n1576\n1577\n1578\n1579\n1580",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\")\n    object: Literal[\"chat.completion\"] = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[ChatCompletionResponseChoice]\n    service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None\n    system_fingerprint: str | None = None\n    usage: UsageInfo\n\n    # vLLM-specific fields that are not in OpenAI spec\n    prompt_logprobs: list[dict[int, Logprob] | None] | None = None\n    prompt_token_ids: list[int] | None = None\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None, description=\"KVTransfer parameters.\"\n    )",
      "language": "typescript"
    },
    {
      "code": "class ChatCompletionResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\")\n    object: Literal[\"chat.completion\"] = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[ChatCompletionResponseChoice]\n    service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None\n    system_fingerprint: str | None = None\n    usage: UsageInfo\n\n    # vLLM-specific fields that are not in OpenAI spec\n    prompt_logprobs: list[dict[int, Logprob] | None] | None = None\n    prompt_token_ids: list[int] | None = None\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None, description=\"KVTransfer parameters.\"\n    )",
      "language": "typescript"
    },
    {
      "code": "choices: list[ChatCompletionResponseChoice]",
      "language": "yaml"
    },
    {
      "code": "choices: list[ChatCompletionResponseChoice]",
      "language": "yaml"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"chatcmpl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"chatcmpl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None, description=\"KVTransfer parameters.\"\n)",
      "language": "yaml"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None, description=\"KVTransfer parameters.\"\n)",
      "language": "yaml"
    },
    {
      "code": "object: Literal['chat.completion'] = 'chat.completion'",
      "language": "yaml"
    },
    {
      "code": "object: Literal['chat.completion'] = 'chat.completion'",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: list[dict[int, Logprob] | None] | None = (\n    None\n)",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: list[dict[int, Logprob] | None] | None = (\n    None\n)",
      "language": "yaml"
    },
    {
      "code": "prompt_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "service_tier: (\n    Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]\n    | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "service_tier: (\n    Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]\n    | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "system_fingerprint: str | None = None",
      "language": "yaml"
    },
    {
      "code": "system_fingerprint: str | None = None",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo",
      "language": "yaml"
    },
    {
      "code": "1552\n1553\n1554\n1555\n1556\n1557\n1558\n1559\n1560\n1561\n1562",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionResponseChoice(OpenAIBaseModel):\n    index: int\n    message: ChatMessage\n    logprobs: ChatCompletionLogProbs | None = None\n    # per OpenAI spec this is the default\n    finish_reason: str | None = \"stop\"\n    # not part of the OpenAI spec but included in vLLM for legacy reasons\n    stop_reason: int | str | None = None\n    # not part of the OpenAI spec but is useful for tracing the tokens\n    # in agent scenarios\n    token_ids: list[int] | None = None",
      "language": "php"
    },
    {
      "code": "class ChatCompletionResponseChoice(OpenAIBaseModel):\n    index: int\n    message: ChatMessage\n    logprobs: ChatCompletionLogProbs | None = None\n    # per OpenAI spec this is the default\n    finish_reason: str | None = \"stop\"\n    # not part of the OpenAI spec but included in vLLM for legacy reasons\n    stop_reason: int | str | None = None\n    # not part of the OpenAI spec but is useful for tracing the tokens\n    # in agent scenarios\n    token_ids: list[int] | None = None",
      "language": "php"
    },
    {
      "code": "finish_reason: str | None = 'stop'",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = 'stop'",
      "language": "yaml"
    },
    {
      "code": "logprobs: ChatCompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: ChatCompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "message: ChatMessage",
      "language": "yaml"
    },
    {
      "code": "message: ChatMessage",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "1598\n1599\n1600\n1601\n1602\n1603\n1604\n1605",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionResponseStreamChoice(OpenAIBaseModel):\n    index: int\n    delta: DeltaMessage\n    logprobs: ChatCompletionLogProbs | None = None\n    finish_reason: str | None = None\n    stop_reason: int | str | None = None\n    # not part of the OpenAI spec but for tracing the tokens\n    token_ids: list[int] | None = None",
      "language": "php"
    },
    {
      "code": "class ChatCompletionResponseStreamChoice(OpenAIBaseModel):\n    index: int\n    delta: DeltaMessage\n    logprobs: ChatCompletionLogProbs | None = None\n    finish_reason: str | None = None\n    stop_reason: int | str | None = None\n    # not part of the OpenAI spec but for tracing the tokens\n    token_ids: list[int] | None = None",
      "language": "php"
    },
    {
      "code": "delta: DeltaMessage",
      "language": "yaml"
    },
    {
      "code": "delta: DeltaMessage",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: ChatCompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: ChatCompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "1608\n1609\n1610\n1611\n1612\n1613\n1614\n1615\n1616",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\")\n    object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[ChatCompletionResponseStreamChoice]\n    usage: UsageInfo | None = Field(default=None)\n    # not part of the OpenAI spec but for tracing the tokens\n    prompt_token_ids: list[int] | None = None",
      "language": "typescript"
    },
    {
      "code": "class ChatCompletionStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\")\n    object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[ChatCompletionResponseStreamChoice]\n    usage: UsageInfo | None = Field(default=None)\n    # not part of the OpenAI spec but for tracing the tokens\n    prompt_token_ids: list[int] | None = None",
      "language": "typescript"
    },
    {
      "code": "choices: list[ChatCompletionResponseStreamChoice]",
      "language": "yaml"
    },
    {
      "code": "choices: list[ChatCompletionResponseStreamChoice]",
      "language": "yaml"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"chatcmpl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"chatcmpl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "object: Literal[\"chat.completion.chunk\"] = (\n    \"chat.completion.chunk\"\n)",
      "language": "yaml"
    },
    {
      "code": "object: Literal[\"chat.completion.chunk\"] = (\n    \"chat.completion.chunk\"\n)",
      "language": "yaml"
    },
    {
      "code": "prompt_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "266\n267\n268",
      "language": "unknown"
    },
    {
      "code": "class ChatCompletionToolsParam(OpenAIBaseModel):\n    type: Literal[\"function\"] = \"function\"\n    function: FunctionDefinition",
      "language": "php"
    },
    {
      "code": "class ChatCompletionToolsParam(OpenAIBaseModel):\n    type: Literal[\"function\"] = \"function\"\n    function: FunctionDefinition",
      "language": "php"
    },
    {
      "code": "function: FunctionDefinition",
      "language": "yaml"
    },
    {
      "code": "function: FunctionDefinition",
      "language": "yaml"
    },
    {
      "code": "type: Literal['function'] = 'function'",
      "language": "yaml"
    },
    {
      "code": "type: Literal['function'] = 'function'",
      "language": "yaml"
    },
    {
      "code": "1514\n1515\n1516\n1517\n1518\n1519\n1520\n1521\n1522\n1523\n1524\n1525\n1526\n1527\n1528\n1529\n1530\n1531\n1532",
      "language": "unknown"
    },
    {
      "code": "class ChatMessage(OpenAIBaseModel):\n    role: str\n    content: str | None = None\n    refusal: str | None = None\n    annotations: OpenAIAnnotation | None = None\n    audio: OpenAIChatCompletionAudio | None = None\n    function_call: FunctionCall | None = None\n    tool_calls: list[ToolCall] = Field(default_factory=list)\n\n    # vLLM-specific fields that are not in OpenAI spec\n    reasoning: str | None = None\n    reasoning_content: str | None = None\n    \"\"\"Deprecated: use `reasoning` instead.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def handle_deprecated_reasoning_content(self):\n        \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\"\n        self.reasoning_content = self.reasoning\n        return self",
      "language": "python"
    },
    {
      "code": "class ChatMessage(OpenAIBaseModel):\n    role: str\n    content: str | None = None\n    refusal: str | None = None\n    annotations: OpenAIAnnotation | None = None\n    audio: OpenAIChatCompletionAudio | None = None\n    function_call: FunctionCall | None = None\n    tool_calls: list[ToolCall] = Field(default_factory=list)\n\n    # vLLM-specific fields that are not in OpenAI spec\n    reasoning: str | None = None\n    reasoning_content: str | None = None\n    \"\"\"Deprecated: use `reasoning` instead.\"\"\"\n\n    @model_validator(mode=\"after\")\n    def handle_deprecated_reasoning_content(self):\n        \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\"\n        self.reasoning_content = self.reasoning\n        return self",
      "language": "python"
    },
    {
      "code": "annotations: Annotation | None = None",
      "language": "yaml"
    },
    {
      "code": "annotations: Annotation | None = None",
      "language": "yaml"
    },
    {
      "code": "audio: ChatCompletionAudio | None = None",
      "language": "yaml"
    },
    {
      "code": "audio: ChatCompletionAudio | None = None",
      "language": "yaml"
    },
    {
      "code": "content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "function_call: FunctionCall | None = None",
      "language": "yaml"
    },
    {
      "code": "function_call: FunctionCall | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning: str | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning: str | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning_content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning_content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "refusal: str | None = None",
      "language": "yaml"
    },
    {
      "code": "refusal: str | None = None",
      "language": "yaml"
    },
    {
      "code": "tool_calls: list[ToolCall] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "tool_calls: list[ToolCall] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "handle_deprecated_reasoning_content()",
      "language": "unknown"
    },
    {
      "code": "handle_deprecated_reasoning_content()",
      "language": "unknown"
    },
    {
      "code": "1528\n1529\n1530\n1531\n1532",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"after\")\ndef handle_deprecated_reasoning_content(self):\n    \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\"\n    self.reasoning_content = self.reasoning\n    return self",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"after\")\ndef handle_deprecated_reasoning_content(self):\n    \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\"\n    self.reasoning_content = self.reasoning\n    return self",
      "language": "python"
    },
    {
      "code": "1409\n1410\n1411\n1412\n1413",
      "language": "unknown"
    },
    {
      "code": "class CompletionLogProbs(OpenAIBaseModel):\n    text_offset: list[int] = Field(default_factory=list)\n    token_logprobs: list[float | None] = Field(default_factory=list)\n    tokens: list[str] = Field(default_factory=list)\n    top_logprobs: list[dict[str, float] | None] = Field(default_factory=list)",
      "language": "php"
    },
    {
      "code": "class CompletionLogProbs(OpenAIBaseModel):\n    text_offset: list[int] = Field(default_factory=list)\n    token_logprobs: list[float | None] = Field(default_factory=list)\n    tokens: list[str] = Field(default_factory=list)\n    top_logprobs: list[dict[str, float] | None] = Field(default_factory=list)",
      "language": "php"
    },
    {
      "code": "text_offset: list[int] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "text_offset: list[int] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "token_logprobs: list[float | None] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "token_logprobs: list[float | None] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "tokens: list[str] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "tokens: list[str] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: list[dict[str, float] | None] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: list[dict[str, float] | None] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406",
      "language": "unknown"
    },
    {
      "code": "class CompletionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/completions/create\n    model: str | None = None\n    prompt: list[int] | list[list[int]] | str | list[str] | None = None\n    echo: bool | None = False\n    frequency_penalty: float | None = 0.0\n    logit_bias: dict[str, float] | None = None\n    logprobs: int | None = None\n    max_tokens: int | None = 16\n    n: int = 1\n    presence_penalty: float | None = 0.0\n    seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    stop: str | list[str] | None = []\n    stream: bool | None = False\n    stream_options: StreamOptions | None = None\n    suffix: str | None = None\n    temperature: float | None = None\n    top_p: float | None = None\n    user: str | None = None\n\n    # --8<-- [start:completion-sampling-params]\n    use_beam_search: bool = False\n    top_k: int | None = None\n    min_p: float | None = None\n    repetition_penalty: float | None = None\n    length_penalty: float = 1.0\n    stop_token_ids: list[int] | None = []\n    include_stop_str_in_output: bool = False\n    ignore_eos: bool = False\n    min_tokens: int = 0\n    skip_special_tokens: bool = True\n    spaces_between_special_tokens: bool = True\n    truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None\n    allowed_token_ids: list[int] | None = None\n    prompt_logprobs: int | None = None\n    # --8<-- [end:completion-sampling-params]\n\n    # --8<-- [start:completion-extra-params]\n    prompt_embeds: bytes | list[bytes] | None = None\n    add_special_tokens: bool = Field(\n        default=True,\n        description=(\n            \"If true (the default), special tokens (e.g. BOS) will be added to \"\n            \"the prompt.\"\n        ),\n    )\n    response_format: AnyResponseFormat | None = Field(\n        default=None,\n        description=(\n            \"Similar to chat completion, this parameter specifies the format \"\n            \"of output. Only {'type': 'json_object'}, {'type': 'json_schema'}\"\n            \", {'type': 'structural_tag'}, or {'type': 'text' } is supported.\"\n        ),\n    )\n    structured_outputs: StructuredOutputsParams | None = Field(\n        default=None,\n        description=\"Additional kwargs for structured outputs\",\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"\n        ),\n    )\n    request_id: str = Field(\n        default_factory=random_uuid,\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    logits_processors: LogitsProcessors | None = Field(\n        default=None,\n        description=(\n            \"A list of either qualified names of logits processors, or \"\n            \"constructor objects, to apply when sampling. A constructor is \"\n            \"a JSON object with a required 'qualname' field specifying the \"\n            \"qualified name of the processor class/factory, and optional \"\n            \"'args' and 'kwargs' fields containing positional and keyword \"\n            \"arguments. For example: {'qualname': \"\n            \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \"\n            \"{'param': 'value'}}.\"\n        ),\n    )\n\n    return_tokens_as_token_ids: bool | None = Field(\n        default=None,\n        description=(\n            \"If specified with 'logprobs', tokens are represented \"\n            \" as strings of the form 'token_id:{token_id}' so that tokens \"\n            \"that are not JSON-encodable can be identified.\"\n        ),\n    )\n    return_token_ids: bool | None = Field(\n        default=None,\n        description=(\n            \"If specified, the result will include token IDs alongside the \"\n            \"generated text. In streaming mode, prompt_token_ids is included \"\n            \"only in the first chunk, and token_ids contains the delta tokens \"\n            \"for each chunk. This is useful for debugging or when you \"\n            \"need to map generated text back to input tokens.\"\n        ),\n    )\n\n    cache_salt: str | None = Field(\n        default=None,\n        description=(\n            \"If specified, the prefix cache will be salted with the provided \"\n            \"string to prevent an attacker to guess prompts in multi-user \"\n            \"environments. The salt should be random, protected from \"\n            \"access by 3rd parties, and long enough to be \"\n            \"unpredictable (e.g., 43 characters base64-encoded, corresponding \"\n            \"to 256 bit).\"\n        ),\n    )\n\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None,\n        description=\"KVTransfer parameters used for disaggregated serving.\",\n    )\n\n    vllm_xargs: dict[str, str | int | float] | None = Field(\n        default=None,\n        description=(\n            \"Additional request parameters with string or \"\n            \"numeric values, used by custom extensions.\"\n        ),\n    )\n\n    # --8<-- [end:completion-extra-params]\n\n    # Default sampling parameters for completion requests\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"repetition_penalty\": 1.0,\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": 0,\n        \"min_p\": 0.0,\n    }\n\n    def to_beam_search_params(\n        self,\n        max_tokens: int,\n        default_sampling_params: dict | None = None,\n    ) -> BeamSearchParams:\n        if default_sampling_params is None:\n            default_sampling_params = {}\n        n = self.n if self.n is not None else 1\n\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\"temperature\", 1.0)\n\n        return BeamSearchParams(\n            beam_width=n,\n            max_tokens=max_tokens,\n            ignore_eos=self.ignore_eos,\n            temperature=temperature,\n            length_penalty=self.length_penalty,\n            include_stop_str_in_output=self.include_stop_str_in_output,\n        )\n\n    def to_sampling_params(\n        self,\n        max_tokens: int,\n        logits_processor_pattern: str | None,\n        default_sampling_params: dict | None = None,\n    ) -> SamplingParams:\n        if default_sampling_params is None:\n            default_sampling_params = {}\n\n        # Default parameters\n        if (repetition_penalty := self.repetition_penalty) is None:\n            repetition_penalty = default_sampling_params.get(\n                \"repetition_penalty\",\n                self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n            )\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n            )\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n            )\n        if (min_p := self.min_p) is None:\n            min_p = default_sampling_params.get(\n                \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n            )\n\n        prompt_logprobs = self.prompt_logprobs\n        if prompt_logprobs is None and self.echo:\n            prompt_logprobs = self.logprobs\n\n        echo_without_generation = self.echo and self.max_tokens == 0\n\n        response_format = self.response_format\n        if response_format is not None:\n            # If structured outputs wasn't already enabled,\n            # we must enable it for these features to work\n            if self.structured_outputs is None:\n                self.structured_outputs = StructuredOutputsParams()\n\n            # Set structured output params for response format\n            if response_format.type == \"json_object\":\n                self.structured_outputs.json_object = True\n            elif response_format.type == \"json_schema\":\n                json_schema = response_format.json_schema\n                assert json_schema is not None\n                self.structured_outputs.json = json_schema.json_schema\n            elif response_format.type == \"structural_tag\":\n                structural_tag = response_format\n                assert structural_tag is not None and isinstance(\n                    structural_tag,\n                    (\n                        LegacyStructuralTagResponseFormat,\n                        StructuralTagResponseFormat,\n                    ),\n                )\n                s_tag_obj = structural_tag.model_dump(by_alias=True)\n                self.structured_outputs.structural_tag = json.dumps(s_tag_obj)\n\n        extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {}\n        if self.kv_transfer_params:\n            # Pass in kv_transfer_params via extra_args\n            extra_args[\"kv_transfer_params\"] = self.kv_transfer_params\n        return SamplingParams.from_optional(\n            n=self.n,\n            presence_penalty=self.presence_penalty,\n            frequency_penalty=self.frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            min_p=min_p,\n            seed=self.seed,\n            stop=self.stop,\n            stop_token_ids=self.stop_token_ids,\n            logprobs=self.logprobs,\n            ignore_eos=self.ignore_eos,\n            max_tokens=max_tokens if not echo_without_generation else 1,\n            min_tokens=self.min_tokens,\n            prompt_logprobs=prompt_logprobs,\n            skip_special_tokens=self.skip_special_tokens,\n            spaces_between_special_tokens=self.spaces_between_special_tokens,\n            include_stop_str_in_output=self.include_stop_str_in_output,\n            logits_processors=get_logits_processors(\n                self.logits_processors, logits_processor_pattern\n            ),\n            truncate_prompt_tokens=self.truncate_prompt_tokens,\n            output_kind=RequestOutputKind.DELTA\n            if self.stream\n            else RequestOutputKind.FINAL_ONLY,\n            structured_outputs=self.structured_outputs,\n            logit_bias=self.logit_bias,\n            allowed_token_ids=self.allowed_token_ids,\n            extra_args=extra_args or None,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_structured_outputs_count(cls, data):\n        if data.get(\"structured_outputs\", None) is None:\n            return data\n\n        structured_outputs_kwargs = data[\"structured_outputs\"]\n        count = sum(\n            structured_outputs_kwargs.get(k) is not None\n            for k in (\"json\", \"regex\", \"choice\")\n        )\n        if count > 1:\n            raise VLLMValidationError(\n                \"You can only use one kind of constraints for structured \"\n                \"outputs ('json', 'regex' or 'choice').\",\n                parameter=\"structured_outputs\",\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_logprobs(cls, data):\n        if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n            if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1):\n                raise VLLMValidationError(\n                    \"`prompt_logprobs` are not available when `stream=True`.\",\n                    parameter=\"prompt_logprobs\",\n                )\n\n            if prompt_logprobs < 0 and prompt_logprobs != -1:\n                raise VLLMValidationError(\n                    \"`prompt_logprobs` must be a positive value or -1.\",\n                    parameter=\"prompt_logprobs\",\n                    value=prompt_logprobs,\n                )\n        if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0:\n            raise VLLMValidationError(\n                \"`logprobs` must be a positive value.\",\n                parameter=\"logprobs\",\n                value=logprobs,\n            )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_stream_options(cls, data):\n        if data.get(\"stream_options\") and not data.get(\"stream\"):\n            raise VLLMValidationError(\n                \"Stream options can only be defined when `stream=True`.\",\n                parameter=\"stream_options\",\n            )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_prompt_and_prompt_embeds(cls, data):\n        prompt = data.get(\"prompt\")\n        prompt_embeds = data.get(\"prompt_embeds\")\n\n        prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\")\n        embeds_is_empty = prompt_embeds is None or (\n            isinstance(prompt_embeds, list) and len(prompt_embeds) == 0\n        )\n\n        if prompt_is_empty and embeds_is_empty:\n            raise ValueError(\n                \"Either prompt or prompt_embeds must be provided and non-empty.\"\n            )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_cache_salt_support(cls, data):\n        if data.get(\"cache_salt\") is not None and (\n            not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n        ):\n            raise ValueError(\n                \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n            )\n        return data",
      "language": "python"
    },
    {
      "code": "class CompletionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/completions/create\n    model: str | None = None\n    prompt: list[int] | list[list[int]] | str | list[str] | None = None\n    echo: bool | None = False\n    frequency_penalty: float | None = 0.0\n    logit_bias: dict[str, float] | None = None\n    logprobs: int | None = None\n    max_tokens: int | None = 16\n    n: int = 1\n    presence_penalty: float | None = 0.0\n    seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    stop: str | list[str] | None = []\n    stream: bool | None = False\n    stream_options: StreamOptions | None = None\n    suffix: str | None = None\n    temperature: float | None = None\n    top_p: float | None = None\n    user: str | None = None\n\n    # --8<-- [start:completion-sampling-params]\n    use_beam_search: bool = False\n    top_k: int | None = None\n    min_p: float | None = None\n    repetition_penalty: float | None = None\n    length_penalty: float = 1.0\n    stop_token_ids: list[int] | None = []\n    include_stop_str_in_output: bool = False\n    ignore_eos: bool = False\n    min_tokens: int = 0\n    skip_special_tokens: bool = True\n    spaces_between_special_tokens: bool = True\n    truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None\n    allowed_token_ids: list[int] | None = None\n    prompt_logprobs: int | None = None\n    # --8<-- [end:completion-sampling-params]\n\n    # --8<-- [start:completion-extra-params]\n    prompt_embeds: bytes | list[bytes] | None = None\n    add_special_tokens: bool = Field(\n        default=True,\n        description=(\n            \"If true (the default), special tokens (e.g. BOS) will be added to \"\n            \"the prompt.\"\n        ),\n    )\n    response_format: AnyResponseFormat | None = Field(\n        default=None,\n        description=(\n            \"Similar to chat completion, this parameter specifies the format \"\n            \"of output. Only {'type': 'json_object'}, {'type': 'json_schema'}\"\n            \", {'type': 'structural_tag'}, or {'type': 'text' } is supported.\"\n        ),\n    )\n    structured_outputs: StructuredOutputsParams | None = Field(\n        default=None,\n        description=\"Additional kwargs for structured outputs\",\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"\n        ),\n    )\n    request_id: str = Field(\n        default_factory=random_uuid,\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    logits_processors: LogitsProcessors | None = Field(\n        default=None,\n        description=(\n            \"A list of either qualified names of logits processors, or \"\n            \"constructor objects, to apply when sampling. A constructor is \"\n            \"a JSON object with a required 'qualname' field specifying the \"\n            \"qualified name of the processor class/factory, and optional \"\n            \"'args' and 'kwargs' fields containing positional and keyword \"\n            \"arguments. For example: {'qualname': \"\n            \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \"\n            \"{'param': 'value'}}.\"\n        ),\n    )\n\n    return_tokens_as_token_ids: bool | None = Field(\n        default=None,\n        description=(\n            \"If specified with 'logprobs', tokens are represented \"\n            \" as strings of the form 'token_id:{token_id}' so that tokens \"\n            \"that are not JSON-encodable can be identified.\"\n        ),\n    )\n    return_token_ids: bool | None = Field(\n        default=None,\n        description=(\n            \"If specified, the result will include token IDs alongside the \"\n            \"generated text. In streaming mode, prompt_token_ids is included \"\n            \"only in the first chunk, and token_ids contains the delta tokens \"\n            \"for each chunk. This is useful for debugging or when you \"\n            \"need to map generated text back to input tokens.\"\n        ),\n    )\n\n    cache_salt: str | None = Field(\n        default=None,\n        description=(\n            \"If specified, the prefix cache will be salted with the provided \"\n            \"string to prevent an attacker to guess prompts in multi-user \"\n            \"environments. The salt should be random, protected from \"\n            \"access by 3rd parties, and long enough to be \"\n            \"unpredictable (e.g., 43 characters base64-encoded, corresponding \"\n            \"to 256 bit).\"\n        ),\n    )\n\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None,\n        description=\"KVTransfer parameters used for disaggregated serving.\",\n    )\n\n    vllm_xargs: dict[str, str | int | float] | None = Field(\n        default=None,\n        description=(\n            \"Additional request parameters with string or \"\n            \"numeric values, used by custom extensions.\"\n        ),\n    )\n\n    # --8<-- [end:completion-extra-params]\n\n    # Default sampling parameters for completion requests\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"repetition_penalty\": 1.0,\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": 0,\n        \"min_p\": 0.0,\n    }\n\n    def to_beam_search_params(\n        self,\n        max_tokens: int,\n        default_sampling_params: dict | None = None,\n    ) -> BeamSearchParams:\n        if default_sampling_params is None:\n            default_sampling_params = {}\n        n = self.n if self.n is not None else 1\n\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\"temperature\", 1.0)\n\n        return BeamSearchParams(\n            beam_width=n,\n            max_tokens=max_tokens,\n            ignore_eos=self.ignore_eos,\n            temperature=temperature,\n            length_penalty=self.length_penalty,\n            include_stop_str_in_output=self.include_stop_str_in_output,\n        )\n\n    def to_sampling_params(\n        self,\n        max_tokens: int,\n        logits_processor_pattern: str | None,\n        default_sampling_params: dict | None = None,\n    ) -> SamplingParams:\n        if default_sampling_params is None:\n            default_sampling_params = {}\n\n        # Default parameters\n        if (repetition_penalty := self.repetition_penalty) is None:\n            repetition_penalty = default_sampling_params.get(\n                \"repetition_penalty\",\n                self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n            )\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n            )\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n            )\n        if (min_p := self.min_p) is None:\n            min_p = default_sampling_params.get(\n                \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n            )\n\n        prompt_logprobs = self.prompt_logprobs\n        if prompt_logprobs is None and self.echo:\n            prompt_logprobs = self.logprobs\n\n        echo_without_generation = self.echo and self.max_tokens == 0\n\n        response_format = self.response_format\n        if response_format is not None:\n            # If structured outputs wasn't already enabled,\n            # we must enable it for these features to work\n            if self.structured_outputs is None:\n                self.structured_outputs = StructuredOutputsParams()\n\n            # Set structured output params for response format\n            if response_format.type == \"json_object\":\n                self.structured_outputs.json_object = True\n            elif response_format.type == \"json_schema\":\n                json_schema = response_format.json_schema\n                assert json_schema is not None\n                self.structured_outputs.json = json_schema.json_schema\n            elif response_format.type == \"structural_tag\":\n                structural_tag = response_format\n                assert structural_tag is not None and isinstance(\n                    structural_tag,\n                    (\n                        LegacyStructuralTagResponseFormat,\n                        StructuralTagResponseFormat,\n                    ),\n                )\n                s_tag_obj = structural_tag.model_dump(by_alias=True)\n                self.structured_outputs.structural_tag = json.dumps(s_tag_obj)\n\n        extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {}\n        if self.kv_transfer_params:\n            # Pass in kv_transfer_params via extra_args\n            extra_args[\"kv_transfer_params\"] = self.kv_transfer_params\n        return SamplingParams.from_optional(\n            n=self.n,\n            presence_penalty=self.presence_penalty,\n            frequency_penalty=self.frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            min_p=min_p,\n            seed=self.seed,\n            stop=self.stop,\n            stop_token_ids=self.stop_token_ids,\n            logprobs=self.logprobs,\n            ignore_eos=self.ignore_eos,\n            max_tokens=max_tokens if not echo_without_generation else 1,\n            min_tokens=self.min_tokens,\n            prompt_logprobs=prompt_logprobs,\n            skip_special_tokens=self.skip_special_tokens,\n            spaces_between_special_tokens=self.spaces_between_special_tokens,\n            include_stop_str_in_output=self.include_stop_str_in_output,\n            logits_processors=get_logits_processors(\n                self.logits_processors, logits_processor_pattern\n            ),\n            truncate_prompt_tokens=self.truncate_prompt_tokens,\n            output_kind=RequestOutputKind.DELTA\n            if self.stream\n            else RequestOutputKind.FINAL_ONLY,\n            structured_outputs=self.structured_outputs,\n            logit_bias=self.logit_bias,\n            allowed_token_ids=self.allowed_token_ids,\n            extra_args=extra_args or None,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_structured_outputs_count(cls, data):\n        if data.get(\"structured_outputs\", None) is None:\n            return data\n\n        structured_outputs_kwargs = data[\"structured_outputs\"]\n        count = sum(\n            structured_outputs_kwargs.get(k) is not None\n            for k in (\"json\", \"regex\", \"choice\")\n        )\n        if count > 1:\n            raise VLLMValidationError(\n                \"You can only use one kind of constraints for structured \"\n                \"outputs ('json', 'regex' or 'choice').\",\n                parameter=\"structured_outputs\",\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_logprobs(cls, data):\n        if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n            if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1):\n                raise VLLMValidationError(\n                    \"`prompt_logprobs` are not available when `stream=True`.\",\n                    parameter=\"prompt_logprobs\",\n                )\n\n            if prompt_logprobs < 0 and prompt_logprobs != -1:\n                raise VLLMValidationError(\n                    \"`prompt_logprobs` must be a positive value or -1.\",\n                    parameter=\"prompt_logprobs\",\n                    value=prompt_logprobs,\n                )\n        if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0:\n            raise VLLMValidationError(\n                \"`logprobs` must be a positive value.\",\n                parameter=\"logprobs\",\n                value=logprobs,\n            )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_stream_options(cls, data):\n        if data.get(\"stream_options\") and not data.get(\"stream\"):\n            raise VLLMValidationError(\n                \"Stream options can only be defined when `stream=True`.\",\n                parameter=\"stream_options\",\n            )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_prompt_and_prompt_embeds(cls, data):\n        prompt = data.get(\"prompt\")\n        prompt_embeds = data.get(\"prompt_embeds\")\n\n        prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\")\n        embeds_is_empty = prompt_embeds is None or (\n            isinstance(prompt_embeds, list) and len(prompt_embeds) == 0\n        )\n\n        if prompt_is_empty and embeds_is_empty:\n            raise ValueError(\n                \"Either prompt or prompt_embeds must be provided and non-empty.\"\n            )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_cache_salt_support(cls, data):\n        if data.get(\"cache_salt\") is not None and (\n            not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n        ):\n            raise ValueError(\n                \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n            )\n        return data",
      "language": "python"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS: dict = {\n    \"repetition_penalty\": 1.0,\n    \"temperature\": 1.0,\n    \"top_p\": 1.0,\n    \"top_k\": 0,\n    \"min_p\": 0.0,\n}",
      "language": "json"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS: dict = {\n    \"repetition_penalty\": 1.0,\n    \"temperature\": 1.0,\n    \"top_p\": 1.0,\n    \"top_k\": 0,\n    \"min_p\": 0.0,\n}",
      "language": "json"
    },
    {
      "code": "add_special_tokens: bool = Field(\n    default=True,\n    description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\",\n)",
      "language": "typescript"
    },
    {
      "code": "add_special_tokens: bool = Field(\n    default=True,\n    description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\",\n)",
      "language": "typescript"
    },
    {
      "code": "allowed_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "allowed_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "cache_salt: str | None = Field(\n    default=None,\n    description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\",\n)",
      "language": "sql"
    },
    {
      "code": "cache_salt: str | None = Field(\n    default=None,\n    description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\",\n)",
      "language": "sql"
    },
    {
      "code": "echo: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "echo: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "frequency_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "frequency_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "ignore_eos: bool = False",
      "language": "typescript"
    },
    {
      "code": "ignore_eos: bool = False",
      "language": "typescript"
    },
    {
      "code": "include_stop_str_in_output: bool = False",
      "language": "typescript"
    },
    {
      "code": "include_stop_str_in_output: bool = False",
      "language": "typescript"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None,\n    description=\"KVTransfer parameters used for disaggregated serving.\",\n)",
      "language": "yaml"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None,\n    description=\"KVTransfer parameters used for disaggregated serving.\",\n)",
      "language": "yaml"
    },
    {
      "code": "length_penalty: float = 1.0",
      "language": "typescript"
    },
    {
      "code": "length_penalty: float = 1.0",
      "language": "typescript"
    },
    {
      "code": "logit_bias: dict[str, float] | None = None",
      "language": "yaml"
    },
    {
      "code": "logit_bias: dict[str, float] | None = None",
      "language": "yaml"
    },
    {
      "code": "logits_processors: LogitsProcessors | None = Field(\n    default=None,\n    description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\",\n)",
      "language": "yaml"
    },
    {
      "code": "logits_processors: LogitsProcessors | None = Field(\n    default=None,\n    description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\",\n)",
      "language": "yaml"
    },
    {
      "code": "logprobs: int | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_tokens: int | None = 16",
      "language": "yaml"
    },
    {
      "code": "max_tokens: int | None = 16",
      "language": "yaml"
    },
    {
      "code": "min_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "min_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "min_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "min_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "presence_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "presence_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "priority: int = Field(\n    default=0,\n    description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\",\n)",
      "language": "typescript"
    },
    {
      "code": "priority: int = Field(\n    default=0,\n    description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\",\n)",
      "language": "typescript"
    },
    {
      "code": "prompt: (\n    list[int] | list[list[int]] | str | list[str] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "prompt: (\n    list[int] | list[list[int]] | str | list[str] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "prompt_embeds: bytes | list[bytes] | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_embeds: bytes | list[bytes] | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: int | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: int | None = None",
      "language": "yaml"
    },
    {
      "code": "repetition_penalty: float | None = None",
      "language": "yaml"
    },
    {
      "code": "repetition_penalty: float | None = None",
      "language": "yaml"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=random_uuid,\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=random_uuid,\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "response_format: AnyResponseFormat | None = Field(\n    default=None,\n    description=\"Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'}, {'type': 'json_schema'}, {'type': 'structural_tag'}, or {'type': 'text' } is supported.\",\n)",
      "language": "yaml"
    },
    {
      "code": "response_format: AnyResponseFormat | None = Field(\n    default=None,\n    description=\"Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'}, {'type': 'json_schema'}, {'type': 'structural_tag'}, or {'type': 'text' } is supported.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_token_ids: bool | None = Field(\n    default=None,\n    description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_token_ids: bool | None = Field(\n    default=None,\n    description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_tokens_as_token_ids: bool | None = Field(\n    default=None,\n    description=\"If specified with 'logprobs', tokens are represented  as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_tokens_as_token_ids: bool | None = Field(\n    default=None,\n    description=\"If specified with 'logprobs', tokens are represented  as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\",\n)",
      "language": "yaml"
    },
    {
      "code": "seed: int | None = Field(None, ge=min, le=max)",
      "language": "yaml"
    },
    {
      "code": "seed: int | None = Field(None, ge=min, le=max)",
      "language": "yaml"
    },
    {
      "code": "skip_special_tokens: bool = True",
      "language": "typescript"
    },
    {
      "code": "skip_special_tokens: bool = True",
      "language": "typescript"
    },
    {
      "code": "spaces_between_special_tokens: bool = True",
      "language": "typescript"
    },
    {
      "code": "spaces_between_special_tokens: bool = True",
      "language": "typescript"
    },
    {
      "code": "stop: str | list[str] | None = []",
      "language": "yaml"
    },
    {
      "code": "stop: str | list[str] | None = []",
      "language": "yaml"
    },
    {
      "code": "stop_token_ids: list[int] | None = []",
      "language": "yaml"
    },
    {
      "code": "stop_token_ids: list[int] | None = []",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_options: StreamOptions | None = None",
      "language": "yaml"
    },
    {
      "code": "stream_options: StreamOptions | None = None",
      "language": "yaml"
    },
    {
      "code": "structured_outputs: StructuredOutputsParams | None = Field(\n    default=None,\n    description=\"Additional kwargs for structured outputs\",\n)",
      "language": "yaml"
    },
    {
      "code": "structured_outputs: StructuredOutputsParams | None = Field(\n    default=None,\n    description=\"Additional kwargs for structured outputs\",\n)",
      "language": "yaml"
    },
    {
      "code": "suffix: str | None = None",
      "language": "yaml"
    },
    {
      "code": "suffix: str | None = None",
      "language": "yaml"
    },
    {
      "code": "temperature: float | None = None",
      "language": "yaml"
    },
    {
      "code": "temperature: float | None = None",
      "language": "yaml"
    },
    {
      "code": "top_k: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_k: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "top_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "truncate_prompt_tokens: (\n    Annotated[int, Field(ge=-1)] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "truncate_prompt_tokens: (\n    Annotated[int, Field(ge=-1)] | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "use_beam_search: bool = False",
      "language": "typescript"
    },
    {
      "code": "use_beam_search: bool = False",
      "language": "typescript"
    },
    {
      "code": "user: str | None = None",
      "language": "yaml"
    },
    {
      "code": "user: str | None = None",
      "language": "yaml"
    },
    {
      "code": "vllm_xargs: dict[str, str | int | float] | None = Field(\n    default=None,\n    description=\"Additional request parameters with string or numeric values, used by custom extensions.\",\n)",
      "language": "yaml"
    },
    {
      "code": "vllm_xargs: dict[str, str | int | float] | None = Field(\n    default=None,\n    description=\"Additional request parameters with string or numeric values, used by custom extensions.\",\n)",
      "language": "yaml"
    },
    {
      "code": "check_cache_salt_support(data)",
      "language": "unknown"
    },
    {
      "code": "check_cache_salt_support(data)",
      "language": "unknown"
    },
    {
      "code": "1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_cache_salt_support(cls, data):\n    if data.get(\"cache_salt\") is not None and (\n        not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n    ):\n        raise ValueError(\n            \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_cache_salt_support(cls, data):\n    if data.get(\"cache_salt\") is not None and (\n        not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n    ):\n        raise ValueError(\n            \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "check_logprobs(data)",
      "language": "unknown"
    },
    {
      "code": "check_logprobs(data)",
      "language": "unknown"
    },
    {
      "code": "1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_logprobs(cls, data):\n    if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n        if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1):\n            raise VLLMValidationError(\n                \"`prompt_logprobs` are not available when `stream=True`.\",\n                parameter=\"prompt_logprobs\",\n            )\n\n        if prompt_logprobs < 0 and prompt_logprobs != -1:\n            raise VLLMValidationError(\n                \"`prompt_logprobs` must be a positive value or -1.\",\n                parameter=\"prompt_logprobs\",\n                value=prompt_logprobs,\n            )\n    if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0:\n        raise VLLMValidationError(\n            \"`logprobs` must be a positive value.\",\n            parameter=\"logprobs\",\n            value=logprobs,\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_logprobs(cls, data):\n    if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n        if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1):\n            raise VLLMValidationError(\n                \"`prompt_logprobs` are not available when `stream=True`.\",\n                parameter=\"prompt_logprobs\",\n            )\n\n        if prompt_logprobs < 0 and prompt_logprobs != -1:\n            raise VLLMValidationError(\n                \"`prompt_logprobs` must be a positive value or -1.\",\n                parameter=\"prompt_logprobs\",\n                value=prompt_logprobs,\n            )\n    if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0:\n        raise VLLMValidationError(\n            \"`logprobs` must be a positive value.\",\n            parameter=\"logprobs\",\n            value=logprobs,\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "check_structured_outputs_count(data)",
      "language": "unknown"
    },
    {
      "code": "check_structured_outputs_count(data)",
      "language": "unknown"
    },
    {
      "code": "1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_structured_outputs_count(cls, data):\n    if data.get(\"structured_outputs\", None) is None:\n        return data\n\n    structured_outputs_kwargs = data[\"structured_outputs\"]\n    count = sum(\n        structured_outputs_kwargs.get(k) is not None\n        for k in (\"json\", \"regex\", \"choice\")\n    )\n    if count > 1:\n        raise VLLMValidationError(\n            \"You can only use one kind of constraints for structured \"\n            \"outputs ('json', 'regex' or 'choice').\",\n            parameter=\"structured_outputs\",\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_structured_outputs_count(cls, data):\n    if data.get(\"structured_outputs\", None) is None:\n        return data\n\n    structured_outputs_kwargs = data[\"structured_outputs\"]\n    count = sum(\n        structured_outputs_kwargs.get(k) is not None\n        for k in (\"json\", \"regex\", \"choice\")\n    )\n    if count > 1:\n        raise VLLMValidationError(\n            \"You can only use one kind of constraints for structured \"\n            \"outputs ('json', 'regex' or 'choice').\",\n            parameter=\"structured_outputs\",\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "to_beam_search_params(\n    max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> BeamSearchParams",
      "language": "rust"
    },
    {
      "code": "to_beam_search_params(\n    max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> BeamSearchParams",
      "language": "rust"
    },
    {
      "code": "1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220",
      "language": "unknown"
    },
    {
      "code": "def to_beam_search_params(\n    self,\n    max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> BeamSearchParams:\n    if default_sampling_params is None:\n        default_sampling_params = {}\n    n = self.n if self.n is not None else 1\n\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\"temperature\", 1.0)\n\n    return BeamSearchParams(\n        beam_width=n,\n        max_tokens=max_tokens,\n        ignore_eos=self.ignore_eos,\n        temperature=temperature,\n        length_penalty=self.length_penalty,\n        include_stop_str_in_output=self.include_stop_str_in_output,\n    )",
      "language": "python"
    },
    {
      "code": "def to_beam_search_params(\n    self,\n    max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> BeamSearchParams:\n    if default_sampling_params is None:\n        default_sampling_params = {}\n    n = self.n if self.n is not None else 1\n\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\"temperature\", 1.0)\n\n    return BeamSearchParams(\n        beam_width=n,\n        max_tokens=max_tokens,\n        ignore_eos=self.ignore_eos,\n        temperature=temperature,\n        length_penalty=self.length_penalty,\n        include_stop_str_in_output=self.include_stop_str_in_output,\n    )",
      "language": "python"
    },
    {
      "code": "to_sampling_params(\n    max_tokens: int,\n    logits_processor_pattern: str | None,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "to_sampling_params(\n    max_tokens: int,\n    logits_processor_pattern: str | None,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322",
      "language": "unknown"
    },
    {
      "code": "def to_sampling_params(\n    self,\n    max_tokens: int,\n    logits_processor_pattern: str | None,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams:\n    if default_sampling_params is None:\n        default_sampling_params = {}\n\n    # Default parameters\n    if (repetition_penalty := self.repetition_penalty) is None:\n        repetition_penalty = default_sampling_params.get(\n            \"repetition_penalty\",\n            self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n        )\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n    if (top_p := self.top_p) is None:\n        top_p = default_sampling_params.get(\n            \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n        )\n    if (top_k := self.top_k) is None:\n        top_k = default_sampling_params.get(\n            \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n        )\n    if (min_p := self.min_p) is None:\n        min_p = default_sampling_params.get(\n            \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n        )\n\n    prompt_logprobs = self.prompt_logprobs\n    if prompt_logprobs is None and self.echo:\n        prompt_logprobs = self.logprobs\n\n    echo_without_generation = self.echo and self.max_tokens == 0\n\n    response_format = self.response_format\n    if response_format is not None:\n        # If structured outputs wasn't already enabled,\n        # we must enable it for these features to work\n        if self.structured_outputs is None:\n            self.structured_outputs = StructuredOutputsParams()\n\n        # Set structured output params for response format\n        if response_format.type == \"json_object\":\n            self.structured_outputs.json_object = True\n        elif response_format.type == \"json_schema\":\n            json_schema = response_format.json_schema\n            assert json_schema is not None\n            self.structured_outputs.json = json_schema.json_schema\n        elif response_format.type == \"structural_tag\":\n            structural_tag = response_format\n            assert structural_tag is not None and isinstance(\n                structural_tag,\n                (\n                    LegacyStructuralTagResponseFormat,\n                    StructuralTagResponseFormat,\n                ),\n            )\n            s_tag_obj = structural_tag.model_dump(by_alias=True)\n            self.structured_outputs.structural_tag = json.dumps(s_tag_obj)\n\n    extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {}\n    if self.kv_transfer_params:\n        # Pass in kv_transfer_params via extra_args\n        extra_args[\"kv_transfer_params\"] = self.kv_transfer_params\n    return SamplingParams.from_optional(\n        n=self.n,\n        presence_penalty=self.presence_penalty,\n        frequency_penalty=self.frequency_penalty,\n        repetition_penalty=repetition_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        min_p=min_p,\n        seed=self.seed,\n        stop=self.stop,\n        stop_token_ids=self.stop_token_ids,\n        logprobs=self.logprobs,\n        ignore_eos=self.ignore_eos,\n        max_tokens=max_tokens if not echo_without_generation else 1,\n        min_tokens=self.min_tokens,\n        prompt_logprobs=prompt_logprobs,\n        skip_special_tokens=self.skip_special_tokens,\n        spaces_between_special_tokens=self.spaces_between_special_tokens,\n        include_stop_str_in_output=self.include_stop_str_in_output,\n        logits_processors=get_logits_processors(\n            self.logits_processors, logits_processor_pattern\n        ),\n        truncate_prompt_tokens=self.truncate_prompt_tokens,\n        output_kind=RequestOutputKind.DELTA\n        if self.stream\n        else RequestOutputKind.FINAL_ONLY,\n        structured_outputs=self.structured_outputs,\n        logit_bias=self.logit_bias,\n        allowed_token_ids=self.allowed_token_ids,\n        extra_args=extra_args or None,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "def to_sampling_params(\n    self,\n    max_tokens: int,\n    logits_processor_pattern: str | None,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams:\n    if default_sampling_params is None:\n        default_sampling_params = {}\n\n    # Default parameters\n    if (repetition_penalty := self.repetition_penalty) is None:\n        repetition_penalty = default_sampling_params.get(\n            \"repetition_penalty\",\n            self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n        )\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n    if (top_p := self.top_p) is None:\n        top_p = default_sampling_params.get(\n            \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n        )\n    if (top_k := self.top_k) is None:\n        top_k = default_sampling_params.get(\n            \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n        )\n    if (min_p := self.min_p) is None:\n        min_p = default_sampling_params.get(\n            \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n        )\n\n    prompt_logprobs = self.prompt_logprobs\n    if prompt_logprobs is None and self.echo:\n        prompt_logprobs = self.logprobs\n\n    echo_without_generation = self.echo and self.max_tokens == 0\n\n    response_format = self.response_format\n    if response_format is not None:\n        # If structured outputs wasn't already enabled,\n        # we must enable it for these features to work\n        if self.structured_outputs is None:\n            self.structured_outputs = StructuredOutputsParams()\n\n        # Set structured output params for response format\n        if response_format.type == \"json_object\":\n            self.structured_outputs.json_object = True\n        elif response_format.type == \"json_schema\":\n            json_schema = response_format.json_schema\n            assert json_schema is not None\n            self.structured_outputs.json = json_schema.json_schema\n        elif response_format.type == \"structural_tag\":\n            structural_tag = response_format\n            assert structural_tag is not None and isinstance(\n                structural_tag,\n                (\n                    LegacyStructuralTagResponseFormat,\n                    StructuralTagResponseFormat,\n                ),\n            )\n            s_tag_obj = structural_tag.model_dump(by_alias=True)\n            self.structured_outputs.structural_tag = json.dumps(s_tag_obj)\n\n    extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {}\n    if self.kv_transfer_params:\n        # Pass in kv_transfer_params via extra_args\n        extra_args[\"kv_transfer_params\"] = self.kv_transfer_params\n    return SamplingParams.from_optional(\n        n=self.n,\n        presence_penalty=self.presence_penalty,\n        frequency_penalty=self.frequency_penalty,\n        repetition_penalty=repetition_penalty,\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        min_p=min_p,\n        seed=self.seed,\n        stop=self.stop,\n        stop_token_ids=self.stop_token_ids,\n        logprobs=self.logprobs,\n        ignore_eos=self.ignore_eos,\n        max_tokens=max_tokens if not echo_without_generation else 1,\n        min_tokens=self.min_tokens,\n        prompt_logprobs=prompt_logprobs,\n        skip_special_tokens=self.skip_special_tokens,\n        spaces_between_special_tokens=self.spaces_between_special_tokens,\n        include_stop_str_in_output=self.include_stop_str_in_output,\n        logits_processors=get_logits_processors(\n            self.logits_processors, logits_processor_pattern\n        ),\n        truncate_prompt_tokens=self.truncate_prompt_tokens,\n        output_kind=RequestOutputKind.DELTA\n        if self.stream\n        else RequestOutputKind.FINAL_ONLY,\n        structured_outputs=self.structured_outputs,\n        logit_bias=self.logit_bias,\n        allowed_token_ids=self.allowed_token_ids,\n        extra_args=extra_args or None,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "validate_prompt_and_prompt_embeds(data)",
      "language": "unknown"
    },
    {
      "code": "validate_prompt_and_prompt_embeds(data)",
      "language": "unknown"
    },
    {
      "code": "1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_prompt_and_prompt_embeds(cls, data):\n    prompt = data.get(\"prompt\")\n    prompt_embeds = data.get(\"prompt_embeds\")\n\n    prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\")\n    embeds_is_empty = prompt_embeds is None or (\n        isinstance(prompt_embeds, list) and len(prompt_embeds) == 0\n    )\n\n    if prompt_is_empty and embeds_is_empty:\n        raise ValueError(\n            \"Either prompt or prompt_embeds must be provided and non-empty.\"\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_prompt_and_prompt_embeds(cls, data):\n    prompt = data.get(\"prompt\")\n    prompt_embeds = data.get(\"prompt_embeds\")\n\n    prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\")\n    embeds_is_empty = prompt_embeds is None or (\n        isinstance(prompt_embeds, list) and len(prompt_embeds) == 0\n    )\n\n    if prompt_is_empty and embeds_is_empty:\n        raise ValueError(\n            \"Either prompt or prompt_embeds must be provided and non-empty.\"\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "validate_stream_options(data)",
      "language": "unknown"
    },
    {
      "code": "validate_stream_options(data)",
      "language": "unknown"
    },
    {
      "code": "1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_stream_options(cls, data):\n    if data.get(\"stream_options\") and not data.get(\"stream\"):\n        raise VLLMValidationError(\n            \"Stream options can only be defined when `stream=True`.\",\n            parameter=\"stream_options\",\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_stream_options(cls, data):\n    if data.get(\"stream_options\") and not data.get(\"stream\"):\n        raise VLLMValidationError(\n            \"Stream options can only be defined when `stream=True`.\",\n            parameter=\"stream_options\",\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "1434\n1435\n1436\n1437\n1438\n1439\n1440\n1441\n1442\n1443\n1444\n1445\n1446\n1447",
      "language": "unknown"
    },
    {
      "code": "class CompletionResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\")\n    object: Literal[\"text_completion\"] = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[CompletionResponseChoice]\n    service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None\n    system_fingerprint: str | None = None\n    usage: UsageInfo\n\n    # vLLM-specific fields that are not in OpenAI spec\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None, description=\"KVTransfer parameters.\"\n    )",
      "language": "typescript"
    },
    {
      "code": "class CompletionResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\")\n    object: Literal[\"text_completion\"] = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[CompletionResponseChoice]\n    service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None\n    system_fingerprint: str | None = None\n    usage: UsageInfo\n\n    # vLLM-specific fields that are not in OpenAI spec\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None, description=\"KVTransfer parameters.\"\n    )",
      "language": "typescript"
    },
    {
      "code": "choices: list[CompletionResponseChoice]",
      "language": "yaml"
    },
    {
      "code": "choices: list[CompletionResponseChoice]",
      "language": "yaml"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"cmpl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"cmpl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None, description=\"KVTransfer parameters.\"\n)",
      "language": "yaml"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None, description=\"KVTransfer parameters.\"\n)",
      "language": "yaml"
    },
    {
      "code": "object: Literal['text_completion'] = 'text_completion'",
      "language": "yaml"
    },
    {
      "code": "object: Literal['text_completion'] = 'text_completion'",
      "language": "yaml"
    },
    {
      "code": "service_tier: (\n    Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]\n    | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "service_tier: (\n    Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]\n    | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "system_fingerprint: str | None = None",
      "language": "yaml"
    },
    {
      "code": "system_fingerprint: str | None = None",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo",
      "language": "yaml"
    },
    {
      "code": "1416\n1417\n1418\n1419\n1420\n1421\n1422\n1423\n1424\n1425\n1426\n1427\n1428\n1429\n1430\n1431",
      "language": "unknown"
    },
    {
      "code": "class CompletionResponseChoice(OpenAIBaseModel):\n    index: int\n    text: str\n    logprobs: CompletionLogProbs | None = None\n    finish_reason: str | None = None\n    stop_reason: int | str | None = Field(\n        default=None,\n        description=(\n            \"The stop string or token id that caused the completion \"\n            \"to stop, None if the completion finished for some other reason \"\n            \"including encountering the EOS token\"\n        ),\n    )\n    token_ids: list[int] | None = None  # For response\n    prompt_logprobs: list[dict[int, Logprob] | None] | None = None\n    prompt_token_ids: list[int] | None = None  # For prompt",
      "language": "php"
    },
    {
      "code": "class CompletionResponseChoice(OpenAIBaseModel):\n    index: int\n    text: str\n    logprobs: CompletionLogProbs | None = None\n    finish_reason: str | None = None\n    stop_reason: int | str | None = Field(\n        default=None,\n        description=(\n            \"The stop string or token id that caused the completion \"\n            \"to stop, None if the completion finished for some other reason \"\n            \"including encountering the EOS token\"\n        ),\n    )\n    token_ids: list[int] | None = None  # For response\n    prompt_logprobs: list[dict[int, Logprob] | None] | None = None\n    prompt_token_ids: list[int] | None = None  # For prompt",
      "language": "php"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: CompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: CompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: list[dict[int, Logprob] | None] | None = (\n    None\n)",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: list[dict[int, Logprob] | None] | None = (\n    None\n)",
      "language": "yaml"
    },
    {
      "code": "prompt_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = Field(\n    default=None,\n    description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\",\n)",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = Field(\n    default=None,\n    description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\",\n)",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "1450\n1451\n1452\n1453\n1454\n1455\n1456\n1457\n1458\n1459\n1460\n1461\n1462\n1463\n1464\n1465\n1466",
      "language": "unknown"
    },
    {
      "code": "class CompletionResponseStreamChoice(OpenAIBaseModel):\n    index: int\n    text: str\n    logprobs: CompletionLogProbs | None = None\n    finish_reason: str | None = None\n    stop_reason: int | str | None = Field(\n        default=None,\n        description=(\n            \"The stop string or token id that caused the completion \"\n            \"to stop, None if the completion finished for some other reason \"\n            \"including encountering the EOS token\"\n        ),\n    )\n    # not part of the OpenAI spec but for tracing the tokens\n    # prompt tokens is put into choice to align with CompletionResponseChoice\n    prompt_token_ids: list[int] | None = None\n    token_ids: list[int] | None = None",
      "language": "php"
    },
    {
      "code": "class CompletionResponseStreamChoice(OpenAIBaseModel):\n    index: int\n    text: str\n    logprobs: CompletionLogProbs | None = None\n    finish_reason: str | None = None\n    stop_reason: int | str | None = Field(\n        default=None,\n        description=(\n            \"The stop string or token id that caused the completion \"\n            \"to stop, None if the completion finished for some other reason \"\n            \"including encountering the EOS token\"\n        ),\n    )\n    # not part of the OpenAI spec but for tracing the tokens\n    # prompt tokens is put into choice to align with CompletionResponseChoice\n    prompt_token_ids: list[int] | None = None\n    token_ids: list[int] | None = None",
      "language": "php"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: CompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: CompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = Field(\n    default=None,\n    description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\",\n)",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = Field(\n    default=None,\n    description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\",\n)",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "1469\n1470\n1471\n1472\n1473\n1474\n1475",
      "language": "unknown"
    },
    {
      "code": "class CompletionStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[CompletionResponseStreamChoice]\n    usage: UsageInfo | None = Field(default=None)",
      "language": "typescript"
    },
    {
      "code": "class CompletionStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[CompletionResponseStreamChoice]\n    usage: UsageInfo | None = Field(default=None)",
      "language": "typescript"
    },
    {
      "code": "choices: list[CompletionResponseStreamChoice]",
      "language": "yaml"
    },
    {
      "code": "choices: list[CompletionResponseStreamChoice]",
      "language": "yaml"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"cmpl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"cmpl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "object: str = 'text_completion'",
      "language": "typescript"
    },
    {
      "code": "object: str = 'text_completion'",
      "language": "typescript"
    },
    {
      "code": "usage: UsageInfo | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "1489\n1490\n1491",
      "language": "unknown"
    },
    {
      "code": "class DeltaFunctionCall(BaseModel):\n    name: str | None = None\n    arguments: str | None = None",
      "language": "php"
    },
    {
      "code": "class DeltaFunctionCall(BaseModel):\n    name: str | None = None\n    arguments: str | None = None",
      "language": "php"
    },
    {
      "code": "arguments: str | None = None",
      "language": "yaml"
    },
    {
      "code": "arguments: str | None = None",
      "language": "yaml"
    },
    {
      "code": "name: str | None = None",
      "language": "yaml"
    },
    {
      "code": "name: str | None = None",
      "language": "yaml"
    },
    {
      "code": "1583\n1584\n1585\n1586\n1587\n1588\n1589\n1590\n1591\n1592\n1593\n1594\n1595",
      "language": "unknown"
    },
    {
      "code": "class DeltaMessage(OpenAIBaseModel):\n    role: str | None = None\n    content: str | None = None\n    reasoning: str | None = None\n    reasoning_content: str | None = None\n    \"\"\"Deprecated: use `reasoning` instead.\"\"\"\n    tool_calls: list[DeltaToolCall] = Field(default_factory=list)\n\n    @model_validator(mode=\"after\")\n    def handle_deprecated_reasoning_content(self):\n        \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\"\n        self.reasoning_content = self.reasoning\n        return self",
      "language": "python"
    },
    {
      "code": "class DeltaMessage(OpenAIBaseModel):\n    role: str | None = None\n    content: str | None = None\n    reasoning: str | None = None\n    reasoning_content: str | None = None\n    \"\"\"Deprecated: use `reasoning` instead.\"\"\"\n    tool_calls: list[DeltaToolCall] = Field(default_factory=list)\n\n    @model_validator(mode=\"after\")\n    def handle_deprecated_reasoning_content(self):\n        \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\"\n        self.reasoning_content = self.reasoning\n        return self",
      "language": "python"
    },
    {
      "code": "content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning: str | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning: str | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning_content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning_content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "role: str | None = None",
      "language": "yaml"
    },
    {
      "code": "role: str | None = None",
      "language": "yaml"
    },
    {
      "code": "tool_calls: list[DeltaToolCall] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "tool_calls: list[DeltaToolCall] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "handle_deprecated_reasoning_content()",
      "language": "unknown"
    },
    {
      "code": "handle_deprecated_reasoning_content()",
      "language": "unknown"
    },
    {
      "code": "1591\n1592\n1593\n1594\n1595",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"after\")\ndef handle_deprecated_reasoning_content(self):\n    \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\"\n    self.reasoning_content = self.reasoning\n    return self",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"after\")\ndef handle_deprecated_reasoning_content(self):\n    \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\"\n    self.reasoning_content = self.reasoning\n    return self",
      "language": "python"
    },
    {
      "code": "1495\n1496\n1497\n1498\n1499",
      "language": "unknown"
    },
    {
      "code": "class DeltaToolCall(OpenAIBaseModel):\n    id: str | None = None\n    type: Literal[\"function\"] | None = None\n    index: int\n    function: DeltaFunctionCall | None = None",
      "language": "php"
    },
    {
      "code": "class DeltaToolCall(OpenAIBaseModel):\n    id: str | None = None\n    type: Literal[\"function\"] | None = None\n    index: int\n    function: DeltaFunctionCall | None = None",
      "language": "php"
    },
    {
      "code": "function: DeltaFunctionCall | None = None",
      "language": "yaml"
    },
    {
      "code": "function: DeltaFunctionCall | None = None",
      "language": "yaml"
    },
    {
      "code": "id: str | None = None",
      "language": "yaml"
    },
    {
      "code": "id: str | None = None",
      "language": "yaml"
    },
    {
      "code": "type: Literal['function'] | None = None",
      "language": "yaml"
    },
    {
      "code": "type: Literal['function'] | None = None",
      "language": "yaml"
    },
    {
      "code": "1987\n1988\n1989",
      "language": "unknown"
    },
    {
      "code": "class DetokenizeRequest(OpenAIBaseModel):\n    model: str | None = None\n    tokens: list[int]",
      "language": "php"
    },
    {
      "code": "class DetokenizeRequest(OpenAIBaseModel):\n    model: str | None = None\n    tokens: list[int]",
      "language": "php"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "class DetokenizeResponse(OpenAIBaseModel):\n    prompt: str",
      "language": "php"
    },
    {
      "code": "class DetokenizeResponse(OpenAIBaseModel):\n    prompt: str",
      "language": "php"
    },
    {
      "code": "prompt: str",
      "language": "yaml"
    },
    {
      "code": "prompt: str",
      "language": "yaml"
    },
    {
      "code": "123\n124\n125\n126\n127",
      "language": "unknown"
    },
    {
      "code": "class ErrorInfo(OpenAIBaseModel):\n    message: str\n    type: str\n    param: str | None = None\n    code: int",
      "language": "php"
    },
    {
      "code": "class ErrorInfo(OpenAIBaseModel):\n    message: str\n    type: str\n    param: str | None = None\n    code: int",
      "language": "php"
    },
    {
      "code": "message: str",
      "language": "yaml"
    },
    {
      "code": "message: str",
      "language": "yaml"
    },
    {
      "code": "param: str | None = None",
      "language": "yaml"
    },
    {
      "code": "param: str | None = None",
      "language": "yaml"
    },
    {
      "code": "class ErrorResponse(OpenAIBaseModel):\n    error: ErrorInfo",
      "language": "php"
    },
    {
      "code": "class ErrorResponse(OpenAIBaseModel):\n    error: ErrorInfo",
      "language": "php"
    },
    {
      "code": "error: ErrorInfo",
      "language": "yaml"
    },
    {
      "code": "error: ErrorInfo",
      "language": "yaml"
    },
    {
      "code": "1502\n1503\n1504\n1505\n1506\n1507\n1508\n1509\n1510\n1511",
      "language": "unknown"
    },
    {
      "code": "class ExtractedToolCallInformation(BaseModel):\n    # indicate if tools were called\n    tools_called: bool\n\n    # extracted tool calls\n    tool_calls: list[ToolCall]\n\n    # content - per OpenAI spec, content AND tool calls can be returned rarely\n    # But some models will do this intentionally\n    content: str | None = None",
      "language": "php"
    },
    {
      "code": "class ExtractedToolCallInformation(BaseModel):\n    # indicate if tools were called\n    tools_called: bool\n\n    # extracted tool calls\n    tool_calls: list[ToolCall]\n\n    # content - per OpenAI spec, content AND tool calls can be returned rarely\n    # But some models will do this intentionally\n    content: str | None = None",
      "language": "php"
    },
    {
      "code": "content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "content: str | None = None",
      "language": "yaml"
    },
    {
      "code": "tool_calls: list[ToolCall]",
      "language": "yaml"
    },
    {
      "code": "tool_calls: list[ToolCall]",
      "language": "yaml"
    },
    {
      "code": "tools_called: bool",
      "language": "yaml"
    },
    {
      "code": "tools_called: bool",
      "language": "yaml"
    },
    {
      "code": "1478\n1479\n1480",
      "language": "unknown"
    },
    {
      "code": "class FunctionCall(OpenAIBaseModel):\n    name: str\n    arguments: str",
      "language": "php"
    },
    {
      "code": "class FunctionCall(OpenAIBaseModel):\n    name: str\n    arguments: str",
      "language": "php"
    },
    {
      "code": "arguments: str",
      "language": "yaml"
    },
    {
      "code": "arguments: str",
      "language": "yaml"
    },
    {
      "code": "260\n261\n262\n263",
      "language": "unknown"
    },
    {
      "code": "class FunctionDefinition(OpenAIBaseModel):\n    name: str\n    description: str | None = None\n    parameters: dict[str, Any] | None = None",
      "language": "php"
    },
    {
      "code": "class FunctionDefinition(OpenAIBaseModel):\n    name: str\n    description: str | None = None\n    parameters: dict[str, Any] | None = None",
      "language": "php"
    },
    {
      "code": "description: str | None = None",
      "language": "yaml"
    },
    {
      "code": "description: str | None = None",
      "language": "yaml"
    },
    {
      "code": "parameters: dict[str, Any] | None = None",
      "language": "yaml"
    },
    {
      "code": "parameters: dict[str, Any] | None = None",
      "language": "yaml"
    },
    {
      "code": "2518\n2519\n2520\n2521\n2522\n2523\n2524\n2525\n2526\n2527\n2528\n2529\n2530\n2531\n2532\n2533\n2534\n2535\n2536\n2537\n2538\n2539\n2540\n2541\n2542\n2543\n2544\n2545\n2546\n2547\n2548\n2549\n2550\n2551\n2552\n2553\n2554\n2555\n2556\n2557\n2558\n2559\n2560\n2561\n2562\n2563\n2564",
      "language": "unknown"
    },
    {
      "code": "class GenerateRequest(BaseModel):\n    request_id: str = Field(\n        default_factory=random_uuid,\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    token_ids: list[int]\n    \"\"\"The token ids to generate text from.\"\"\"\n\n    # features: MultiModalFeatureSpec\n    # TODO (NickLucche): implement once Renderer work is completed\n    features: str | None = None\n    \"\"\"The processed MM inputs for the model.\"\"\"\n\n    sampling_params: SamplingParams\n    \"\"\"The sampling parameters for the model.\"\"\"\n\n    model: str | None = None\n\n    stream: bool | None = False\n    stream_options: StreamOptions | None = None\n    cache_salt: str | None = Field(\n        default=None,\n        description=(\n            \"If specified, the prefix cache will be salted with the provided \"\n            \"string to prevent an attacker to guess prompts in multi-user \"\n            \"environments. The salt should be random, protected from \"\n            \"access by 3rd parties, and long enough to be \"\n            \"unpredictable (e.g., 43 characters base64-encoded, corresponding \"\n            \"to 256 bit).\"\n        ),\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"\n        ),\n    )\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None,\n        description=\"KVTransfer parameters used for disaggregated serving.\",\n    )",
      "language": "csharp"
    },
    {
      "code": "class GenerateRequest(BaseModel):\n    request_id: str = Field(\n        default_factory=random_uuid,\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    token_ids: list[int]\n    \"\"\"The token ids to generate text from.\"\"\"\n\n    # features: MultiModalFeatureSpec\n    # TODO (NickLucche): implement once Renderer work is completed\n    features: str | None = None\n    \"\"\"The processed MM inputs for the model.\"\"\"\n\n    sampling_params: SamplingParams\n    \"\"\"The sampling parameters for the model.\"\"\"\n\n    model: str | None = None\n\n    stream: bool | None = False\n    stream_options: StreamOptions | None = None\n    cache_salt: str | None = Field(\n        default=None,\n        description=(\n            \"If specified, the prefix cache will be salted with the provided \"\n            \"string to prevent an attacker to guess prompts in multi-user \"\n            \"environments. The salt should be random, protected from \"\n            \"access by 3rd parties, and long enough to be \"\n            \"unpredictable (e.g., 43 characters base64-encoded, corresponding \"\n            \"to 256 bit).\"\n        ),\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"\n        ),\n    )\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None,\n        description=\"KVTransfer parameters used for disaggregated serving.\",\n    )",
      "language": "csharp"
    },
    {
      "code": "cache_salt: str | None = Field(\n    default=None,\n    description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\",\n)",
      "language": "sql"
    },
    {
      "code": "cache_salt: str | None = Field(\n    default=None,\n    description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\",\n)",
      "language": "sql"
    },
    {
      "code": "features: str | None = None",
      "language": "yaml"
    },
    {
      "code": "features: str | None = None",
      "language": "yaml"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None,\n    description=\"KVTransfer parameters used for disaggregated serving.\",\n)",
      "language": "yaml"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None,\n    description=\"KVTransfer parameters used for disaggregated serving.\",\n)",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "priority: int = Field(\n    default=0,\n    description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\",\n)",
      "language": "typescript"
    },
    {
      "code": "priority: int = Field(\n    default=0,\n    description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\",\n)",
      "language": "typescript"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=random_uuid,\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=random_uuid,\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "sampling_params: SamplingParams",
      "language": "yaml"
    },
    {
      "code": "sampling_params: SamplingParams",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_options: StreamOptions | None = None",
      "language": "yaml"
    },
    {
      "code": "stream_options: StreamOptions | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "2575\n2576\n2577\n2578\n2579\n2580\n2581\n2582\n2583\n2584\n2585\n2586\n2587\n2588\n2589\n2590\n2591",
      "language": "unknown"
    },
    {
      "code": "class GenerateResponse(BaseModel):\n    request_id: str = Field(\n        default_factory=random_uuid,\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    choices: list[GenerateResponseChoice]\n\n    prompt_logprobs: list[dict[int, Logprob] | None] | None = None\n\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None,\n        description=\"KVTransfer parameters used for disaggregated serving.\",\n    )",
      "language": "typescript"
    },
    {
      "code": "class GenerateResponse(BaseModel):\n    request_id: str = Field(\n        default_factory=random_uuid,\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    choices: list[GenerateResponseChoice]\n\n    prompt_logprobs: list[dict[int, Logprob] | None] | None = None\n\n    kv_transfer_params: dict[str, Any] | None = Field(\n        default=None,\n        description=\"KVTransfer parameters used for disaggregated serving.\",\n    )",
      "language": "typescript"
    },
    {
      "code": "choices: list[GenerateResponseChoice]",
      "language": "yaml"
    },
    {
      "code": "choices: list[GenerateResponseChoice]",
      "language": "yaml"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None,\n    description=\"KVTransfer parameters used for disaggregated serving.\",\n)",
      "language": "yaml"
    },
    {
      "code": "kv_transfer_params: dict[str, Any] | None = Field(\n    default=None,\n    description=\"KVTransfer parameters used for disaggregated serving.\",\n)",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: list[dict[int, Logprob] | None] | None = (\n    None\n)",
      "language": "yaml"
    },
    {
      "code": "prompt_logprobs: list[dict[int, Logprob] | None] | None = (\n    None\n)",
      "language": "yaml"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=random_uuid,\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=random_uuid,\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "2567\n2568\n2569\n2570\n2571\n2572",
      "language": "unknown"
    },
    {
      "code": "class GenerateResponseChoice(BaseModel):\n    index: int\n    logprobs: ChatCompletionLogProbs | None = None\n    # per OpenAI spec this is the default\n    finish_reason: str | None = \"stop\"\n    token_ids: list[int] | None = None",
      "language": "php"
    },
    {
      "code": "class GenerateResponseChoice(BaseModel):\n    index: int\n    logprobs: ChatCompletionLogProbs | None = None\n    # per OpenAI spec this is the default\n    finish_reason: str | None = \"stop\"\n    token_ids: list[int] | None = None",
      "language": "php"
    },
    {
      "code": "finish_reason: str | None = 'stop'",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = 'stop'",
      "language": "yaml"
    },
    {
      "code": "logprobs: ChatCompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "logprobs: ChatCompletionLogProbs | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int] | None = None",
      "language": "yaml"
    },
    {
      "code": "1634\n1635\n1636\n1637",
      "language": "unknown"
    },
    {
      "code": "class InputTokensDetails(OpenAIBaseModel):\n    cached_tokens: int\n    input_tokens_per_turn: list[int] = Field(default_factory=list)\n    cached_tokens_per_turn: list[int] = Field(default_factory=list)",
      "language": "php"
    },
    {
      "code": "class InputTokensDetails(OpenAIBaseModel):\n    cached_tokens: int\n    input_tokens_per_turn: list[int] = Field(default_factory=list)\n    cached_tokens_per_turn: list[int] = Field(default_factory=list)",
      "language": "php"
    },
    {
      "code": "cached_tokens: int",
      "language": "yaml"
    },
    {
      "code": "cached_tokens: int",
      "language": "yaml"
    },
    {
      "code": "cached_tokens_per_turn: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "cached_tokens_per_turn: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "input_tokens_per_turn: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "input_tokens_per_turn: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "211\n212\n213\n214\n215\n216\n217",
      "language": "unknown"
    },
    {
      "code": "class JsonSchemaResponseFormat(OpenAIBaseModel):\n    name: str\n    description: str | None = None\n    # schema is the field in openai but that causes conflicts with pydantic so\n    # instead use json_schema with an alias\n    json_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\")\n    strict: bool | None = None",
      "language": "php"
    },
    {
      "code": "class JsonSchemaResponseFormat(OpenAIBaseModel):\n    name: str\n    description: str | None = None\n    # schema is the field in openai but that causes conflicts with pydantic so\n    # instead use json_schema with an alias\n    json_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\")\n    strict: bool | None = None",
      "language": "php"
    },
    {
      "code": "description: str | None = None",
      "language": "yaml"
    },
    {
      "code": "description: str | None = None",
      "language": "yaml"
    },
    {
      "code": "json_schema: dict[str, Any] | None = Field(\n    default=None, alias=\"schema\"\n)",
      "language": "yaml"
    },
    {
      "code": "json_schema: dict[str, Any] | None = Field(\n    default=None, alias=\"schema\"\n)",
      "language": "yaml"
    },
    {
      "code": "strict: bool | None = None",
      "language": "yaml"
    },
    {
      "code": "strict: bool | None = None",
      "language": "yaml"
    },
    {
      "code": "220\n221\n222\n223\n224\n225",
      "language": "unknown"
    },
    {
      "code": "class LegacyStructuralTag(OpenAIBaseModel):\n    begin: str\n    # schema is the field, but that causes conflicts with pydantic so\n    # instead use structural_tag_schema with an alias\n    structural_tag_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\")\n    end: str",
      "language": "php"
    },
    {
      "code": "class LegacyStructuralTag(OpenAIBaseModel):\n    begin: str\n    # schema is the field, but that causes conflicts with pydantic so\n    # instead use structural_tag_schema with an alias\n    structural_tag_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\")\n    end: str",
      "language": "php"
    },
    {
      "code": "structural_tag_schema: dict[str, Any] | None = Field(\n    default=None, alias=\"schema\"\n)",
      "language": "yaml"
    },
    {
      "code": "structural_tag_schema: dict[str, Any] | None = Field(\n    default=None, alias=\"schema\"\n)",
      "language": "yaml"
    },
    {
      "code": "228\n229\n230\n231",
      "language": "unknown"
    },
    {
      "code": "class LegacyStructuralTagResponseFormat(OpenAIBaseModel):\n    type: Literal[\"structural_tag\"]\n    structures: list[LegacyStructuralTag]\n    triggers: list[str]",
      "language": "php"
    },
    {
      "code": "class LegacyStructuralTagResponseFormat(OpenAIBaseModel):\n    type: Literal[\"structural_tag\"]\n    structures: list[LegacyStructuralTag]\n    triggers: list[str]",
      "language": "php"
    },
    {
      "code": "structures: list[LegacyStructuralTag]",
      "language": "yaml"
    },
    {
      "code": "structures: list[LegacyStructuralTag]",
      "language": "yaml"
    },
    {
      "code": "triggers: list[str]",
      "language": "yaml"
    },
    {
      "code": "triggers: list[str]",
      "language": "yaml"
    },
    {
      "code": "type: Literal['structural_tag']",
      "language": "yaml"
    },
    {
      "code": "type: Literal['structural_tag']",
      "language": "yaml"
    },
    {
      "code": "2006\n2007\n2008",
      "language": "unknown"
    },
    {
      "code": "class LoadLoRAAdapterRequest(BaseModel):\n    lora_name: str\n    lora_path: str",
      "language": "php"
    },
    {
      "code": "class LoadLoRAAdapterRequest(BaseModel):\n    lora_name: str\n    lora_path: str",
      "language": "php"
    },
    {
      "code": "lora_name: str",
      "language": "yaml"
    },
    {
      "code": "lora_name: str",
      "language": "yaml"
    },
    {
      "code": "lora_path: str",
      "language": "yaml"
    },
    {
      "code": "lora_path: str",
      "language": "yaml"
    },
    {
      "code": "282\n283\n284\n285\n286\n287",
      "language": "unknown"
    },
    {
      "code": "class LogitsProcessorConstructor(BaseModel):\n    qualname: str\n    args: list[Any] | None = None\n    kwargs: dict[str, Any] | None = None\n\n    model_config = ConfigDict(extra=\"forbid\")",
      "language": "php"
    },
    {
      "code": "class LogitsProcessorConstructor(BaseModel):\n    qualname: str\n    args: list[Any] | None = None\n    kwargs: dict[str, Any] | None = None\n\n    model_config = ConfigDict(extra=\"forbid\")",
      "language": "php"
    },
    {
      "code": "args: list[Any] | None = None",
      "language": "yaml"
    },
    {
      "code": "args: list[Any] | None = None",
      "language": "yaml"
    },
    {
      "code": "kwargs: dict[str, Any] | None = None",
      "language": "yaml"
    },
    {
      "code": "kwargs: dict[str, Any] | None = None",
      "language": "yaml"
    },
    {
      "code": "model_config = ConfigDict(extra='forbid')",
      "language": "unknown"
    },
    {
      "code": "model_config = ConfigDict(extra='forbid')",
      "language": "unknown"
    },
    {
      "code": "qualname: str",
      "language": "yaml"
    },
    {
      "code": "qualname: str",
      "language": "yaml"
    },
    {
      "code": "179\n180\n181\n182\n183\n184\n185\n186\n187",
      "language": "unknown"
    },
    {
      "code": "class ModelCard(OpenAIBaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"vllm\"\n    root: str | None = None\n    parent: str | None = None\n    max_model_len: int | None = None\n    permission: list[ModelPermission] = Field(default_factory=list)",
      "language": "typescript"
    },
    {
      "code": "class ModelCard(OpenAIBaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"vllm\"\n    root: str | None = None\n    parent: str | None = None\n    max_model_len: int | None = None\n    permission: list[ModelPermission] = Field(default_factory=list)",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "max_model_len: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_model_len: int | None = None",
      "language": "yaml"
    },
    {
      "code": "object: str = 'model'",
      "language": "typescript"
    },
    {
      "code": "object: str = 'model'",
      "language": "typescript"
    },
    {
      "code": "owned_by: str = 'vllm'",
      "language": "typescript"
    },
    {
      "code": "owned_by: str = 'vllm'",
      "language": "typescript"
    },
    {
      "code": "parent: str | None = None",
      "language": "yaml"
    },
    {
      "code": "parent: str | None = None",
      "language": "yaml"
    },
    {
      "code": "permission: list[ModelPermission] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "permission: list[ModelPermission] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "root: str | None = None",
      "language": "yaml"
    },
    {
      "code": "root: str | None = None",
      "language": "yaml"
    },
    {
      "code": "190\n191\n192",
      "language": "unknown"
    },
    {
      "code": "class ModelList(OpenAIBaseModel):\n    object: str = \"list\"\n    data: list[ModelCard] = Field(default_factory=list)",
      "language": "typescript"
    },
    {
      "code": "class ModelList(OpenAIBaseModel):\n    object: str = \"list\"\n    data: list[ModelCard] = Field(default_factory=list)",
      "language": "typescript"
    },
    {
      "code": "data: list[ModelCard] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "data: list[ModelCard] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "object: str = 'list'",
      "language": "typescript"
    },
    {
      "code": "object: str = 'list'",
      "language": "typescript"
    },
    {
      "code": "164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176",
      "language": "unknown"
    },
    {
      "code": "class ModelPermission(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\")\n    object: str = \"model_permission\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    allow_create_engine: bool = False\n    allow_sampling: bool = True\n    allow_logprobs: bool = True\n    allow_search_indices: bool = False\n    allow_view: bool = True\n    allow_fine_tuning: bool = False\n    organization: str = \"*\"\n    group: str | None = None\n    is_blocking: bool = False",
      "language": "typescript"
    },
    {
      "code": "class ModelPermission(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\")\n    object: str = \"model_permission\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    allow_create_engine: bool = False\n    allow_sampling: bool = True\n    allow_logprobs: bool = True\n    allow_search_indices: bool = False\n    allow_view: bool = True\n    allow_fine_tuning: bool = False\n    organization: str = \"*\"\n    group: str | None = None\n    is_blocking: bool = False",
      "language": "typescript"
    },
    {
      "code": "allow_create_engine: bool = False",
      "language": "typescript"
    },
    {
      "code": "allow_create_engine: bool = False",
      "language": "typescript"
    },
    {
      "code": "allow_fine_tuning: bool = False",
      "language": "typescript"
    },
    {
      "code": "allow_fine_tuning: bool = False",
      "language": "typescript"
    },
    {
      "code": "allow_logprobs: bool = True",
      "language": "typescript"
    },
    {
      "code": "allow_logprobs: bool = True",
      "language": "typescript"
    },
    {
      "code": "allow_sampling: bool = True",
      "language": "typescript"
    },
    {
      "code": "allow_sampling: bool = True",
      "language": "typescript"
    },
    {
      "code": "allow_search_indices: bool = False",
      "language": "typescript"
    },
    {
      "code": "allow_search_indices: bool = False",
      "language": "typescript"
    },
    {
      "code": "allow_view: bool = True",
      "language": "typescript"
    },
    {
      "code": "allow_view: bool = True",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "group: str | None = None",
      "language": "yaml"
    },
    {
      "code": "group: str | None = None",
      "language": "yaml"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"modelperm-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"modelperm-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "is_blocking: bool = False",
      "language": "typescript"
    },
    {
      "code": "is_blocking: bool = False",
      "language": "typescript"
    },
    {
      "code": "object: str = 'model_permission'",
      "language": "typescript"
    },
    {
      "code": "object: str = 'model_permission'",
      "language": "typescript"
    },
    {
      "code": "organization: str = '*'",
      "language": "typescript"
    },
    {
      "code": "organization: str = '*'",
      "language": "typescript"
    },
    {
      "code": "91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120",
      "language": "unknown"
    },
    {
      "code": "class OpenAIBaseModel(BaseModel):\n    # OpenAI API does allow extra fields\n    model_config = ConfigDict(extra=\"allow\")\n\n    # Cache class field names\n    field_names: ClassVar[set[str] | None] = None\n\n    @model_validator(mode=\"wrap\")\n    @classmethod\n    def __log_extra_fields__(cls, data, handler):\n        result = handler(data)\n        if not isinstance(data, dict):\n            return result\n        field_names = cls.field_names\n        if field_names is None:\n            # Get all class field names and their potential aliases\n            field_names = set()\n            for field_name, field in cls.model_fields.items():\n                field_names.add(field_name)\n                if alias := getattr(field, \"alias\", None):\n                    field_names.add(alias)\n            cls.field_names = field_names\n\n        # Compare against both field names and aliases\n        if any(k not in field_names for k in data):\n            logger.warning(\n                \"The following fields were present in the request but ignored: %s\",\n                data.keys() - field_names,\n            )\n        return result",
      "language": "python"
    },
    {
      "code": "class OpenAIBaseModel(BaseModel):\n    # OpenAI API does allow extra fields\n    model_config = ConfigDict(extra=\"allow\")\n\n    # Cache class field names\n    field_names: ClassVar[set[str] | None] = None\n\n    @model_validator(mode=\"wrap\")\n    @classmethod\n    def __log_extra_fields__(cls, data, handler):\n        result = handler(data)\n        if not isinstance(data, dict):\n            return result\n        field_names = cls.field_names\n        if field_names is None:\n            # Get all class field names and their potential aliases\n            field_names = set()\n            for field_name, field in cls.model_fields.items():\n                field_names.add(field_name)\n                if alias := getattr(field, \"alias\", None):\n                    field_names.add(alias)\n            cls.field_names = field_names\n\n        # Compare against both field names and aliases\n        if any(k not in field_names for k in data):\n            logger.warning(\n                \"The following fields were present in the request but ignored: %s\",\n                data.keys() - field_names,\n            )\n        return result",
      "language": "python"
    },
    {
      "code": "field_names: set[str] | None = None",
      "language": "yaml"
    },
    {
      "code": "field_names: set[str] | None = None",
      "language": "yaml"
    },
    {
      "code": "model_config = ConfigDict(extra='allow')",
      "language": "unknown"
    },
    {
      "code": "model_config = ConfigDict(extra='allow')",
      "language": "unknown"
    },
    {
      "code": "__log_extra_fields__(data, handler)",
      "language": "unknown"
    },
    {
      "code": "__log_extra_fields__(data, handler)",
      "language": "unknown"
    },
    {
      "code": "98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"wrap\")\n@classmethod\ndef __log_extra_fields__(cls, data, handler):\n    result = handler(data)\n    if not isinstance(data, dict):\n        return result\n    field_names = cls.field_names\n    if field_names is None:\n        # Get all class field names and their potential aliases\n        field_names = set()\n        for field_name, field in cls.model_fields.items():\n            field_names.add(field_name)\n            if alias := getattr(field, \"alias\", None):\n                field_names.add(alias)\n        cls.field_names = field_names\n\n    # Compare against both field names and aliases\n    if any(k not in field_names for k in data):\n        logger.warning(\n            \"The following fields were present in the request but ignored: %s\",\n            data.keys() - field_names,\n        )\n    return result",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"wrap\")\n@classmethod\ndef __log_extra_fields__(cls, data, handler):\n    result = handler(data)\n    if not isinstance(data, dict):\n        return result\n    field_names = cls.field_names\n    if field_names is None:\n        # Get all class field names and their potential aliases\n        field_names = set()\n        for field_name, field in cls.model_fields.items():\n            field_names.add(field_name)\n            if alias := getattr(field, \"alias\", None):\n                field_names.add(alias)\n        cls.field_names = field_names\n\n    # Compare against both field names and aliases\n    if any(k not in field_names for k in data):\n        logger.warning(\n            \"The following fields were present in the request but ignored: %s\",\n            data.keys() - field_names,\n        )\n    return result",
      "language": "python"
    },
    {
      "code": "1640\n1641\n1642\n1643\n1644",
      "language": "unknown"
    },
    {
      "code": "class OutputTokensDetails(OpenAIBaseModel):\n    reasoning_tokens: int = 0\n    tool_output_tokens: int = 0\n    output_tokens_per_turn: list[int] = Field(default_factory=list)\n    tool_output_tokens_per_turn: list[int] = Field(default_factory=list)",
      "language": "typescript"
    },
    {
      "code": "class OutputTokensDetails(OpenAIBaseModel):\n    reasoning_tokens: int = 0\n    tool_output_tokens: int = 0\n    output_tokens_per_turn: list[int] = Field(default_factory=list)\n    tool_output_tokens_per_turn: list[int] = Field(default_factory=list)",
      "language": "typescript"
    },
    {
      "code": "output_tokens_per_turn: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "output_tokens_per_turn: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "reasoning_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "reasoning_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "tool_output_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "tool_output_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "tool_output_tokens_per_turn: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "tool_output_tokens_per_turn: list[int] = Field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "class PromptTokenUsageInfo(OpenAIBaseModel):\n    cached_tokens: int | None = None",
      "language": "php"
    },
    {
      "code": "class PromptTokenUsageInfo(OpenAIBaseModel):\n    cached_tokens: int | None = None",
      "language": "php"
    },
    {
      "code": "cached_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "cached_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "206\n207\n208",
      "language": "unknown"
    },
    {
      "code": "class RequestResponseMetadata(BaseModel):\n    request_id: str\n    final_usage_info: UsageInfo | None = None",
      "language": "php"
    },
    {
      "code": "class RequestResponseMetadata(BaseModel):\n    request_id: str\n    final_usage_info: UsageInfo | None = None",
      "language": "php"
    },
    {
      "code": "final_usage_info: UsageInfo | None = None",
      "language": "yaml"
    },
    {
      "code": "final_usage_info: UsageInfo | None = None",
      "language": "yaml"
    },
    {
      "code": "request_id: str",
      "language": "yaml"
    },
    {
      "code": "request_id: str",
      "language": "yaml"
    },
    {
      "code": "class ResponseCompletedEvent(OpenAIResponseCompletedEvent):\n    response: ResponsesResponse  # type: ignore[override]",
      "language": "php"
    },
    {
      "code": "class ResponseCompletedEvent(OpenAIResponseCompletedEvent):\n    response: ResponsesResponse  # type: ignore[override]",
      "language": "php"
    },
    {
      "code": "response: ResponsesResponse",
      "language": "yaml"
    },
    {
      "code": "response: ResponsesResponse",
      "language": "yaml"
    },
    {
      "code": "class ResponseCreatedEvent(OpenAIResponseCreatedEvent):\n    response: ResponsesResponse  # type: ignore[override]",
      "language": "php"
    },
    {
      "code": "class ResponseCreatedEvent(OpenAIResponseCreatedEvent):\n    response: ResponsesResponse  # type: ignore[override]",
      "language": "php"
    },
    {
      "code": "response: ResponsesResponse",
      "language": "yaml"
    },
    {
      "code": "response: ResponsesResponse",
      "language": "yaml"
    },
    {
      "code": "244\n245\n246\n247",
      "language": "unknown"
    },
    {
      "code": "class ResponseFormat(OpenAIBaseModel):\n    # type must be \"json_schema\", \"json_object\", or \"text\"\n    type: Literal[\"text\", \"json_object\", \"json_schema\"]\n    json_schema: JsonSchemaResponseFormat | None = None",
      "language": "php"
    },
    {
      "code": "class ResponseFormat(OpenAIBaseModel):\n    # type must be \"json_schema\", \"json_object\", or \"text\"\n    type: Literal[\"text\", \"json_object\", \"json_schema\"]\n    json_schema: JsonSchemaResponseFormat | None = None",
      "language": "php"
    },
    {
      "code": "json_schema: JsonSchemaResponseFormat | None = None",
      "language": "yaml"
    },
    {
      "code": "json_schema: JsonSchemaResponseFormat | None = None",
      "language": "yaml"
    },
    {
      "code": "type: Literal['text', 'json_object', 'json_schema']",
      "language": "yaml"
    },
    {
      "code": "type: Literal['text', 'json_object', 'json_schema']",
      "language": "yaml"
    },
    {
      "code": "class ResponseInProgressEvent(OpenAIResponseInProgressEvent):\n    response: ResponsesResponse  # type: ignore[override]",
      "language": "php"
    },
    {
      "code": "class ResponseInProgressEvent(OpenAIResponseInProgressEvent):\n    response: ResponsesResponse  # type: ignore[override]",
      "language": "php"
    },
    {
      "code": "response: ResponsesResponse",
      "language": "yaml"
    },
    {
      "code": "response: ResponsesResponse",
      "language": "yaml"
    },
    {
      "code": "1675\n1676\n1677\n1678\n1679\n1680\n1681",
      "language": "unknown"
    },
    {
      "code": "class ResponseRawMessageAndToken(OpenAIBaseModel):\n    \"\"\"Class to show the raw message.\n    If message / tokens diverge, tokens is the source of truth\"\"\"\n\n    message: str\n    tokens: list[int]\n    type: Literal[\"raw_message_tokens\"] = \"raw_message_tokens\"",
      "language": "php"
    },
    {
      "code": "class ResponseRawMessageAndToken(OpenAIBaseModel):\n    \"\"\"Class to show the raw message.\n    If message / tokens diverge, tokens is the source of truth\"\"\"\n\n    message: str\n    tokens: list[int]\n    type: Literal[\"raw_message_tokens\"] = \"raw_message_tokens\"",
      "language": "php"
    },
    {
      "code": "message: str",
      "language": "yaml"
    },
    {
      "code": "message: str",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "type: Literal['raw_message_tokens'] = 'raw_message_tokens'",
      "language": "yaml"
    },
    {
      "code": "type: Literal['raw_message_tokens'] = 'raw_message_tokens'",
      "language": "yaml"
    },
    {
      "code": "1823\n1824\n1825\n1826\n1827\n1828\n1829\n1830\n1831\n1832\n1833\n1834\n1835\n1836\n1837\n1838\n1839\n1840",
      "language": "unknown"
    },
    {
      "code": "class ResponseReasoningPartAddedEvent(OpenAIBaseModel):\n    content_index: int\n    \"\"\"The index of the content part that is done.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the output item that the content part was added to.\"\"\"\n\n    output_index: int\n    \"\"\"The index of the output item that the content part was added to.\"\"\"\n\n    part: ResponseReasoningTextContent\n    \"\"\"The content part that is done.\"\"\"\n\n    sequence_number: int\n    \"\"\"The sequence number of this event.\"\"\"\n\n    type: Literal[\"response.reasoning_part.added\"]\n    \"\"\"The type of the event. Always `response.reasoning_part.added`.\"\"\"",
      "language": "php"
    },
    {
      "code": "class ResponseReasoningPartAddedEvent(OpenAIBaseModel):\n    content_index: int\n    \"\"\"The index of the content part that is done.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the output item that the content part was added to.\"\"\"\n\n    output_index: int\n    \"\"\"The index of the output item that the content part was added to.\"\"\"\n\n    part: ResponseReasoningTextContent\n    \"\"\"The content part that is done.\"\"\"\n\n    sequence_number: int\n    \"\"\"The sequence number of this event.\"\"\"\n\n    type: Literal[\"response.reasoning_part.added\"]\n    \"\"\"The type of the event. Always `response.reasoning_part.added`.\"\"\"",
      "language": "php"
    },
    {
      "code": "content_index: int",
      "language": "yaml"
    },
    {
      "code": "content_index: int",
      "language": "yaml"
    },
    {
      "code": "item_id: str",
      "language": "yaml"
    },
    {
      "code": "item_id: str",
      "language": "yaml"
    },
    {
      "code": "output_index: int",
      "language": "yaml"
    },
    {
      "code": "output_index: int",
      "language": "yaml"
    },
    {
      "code": "part: Content",
      "language": "yaml"
    },
    {
      "code": "part: Content",
      "language": "yaml"
    },
    {
      "code": "sequence_number: int",
      "language": "yaml"
    },
    {
      "code": "sequence_number: int",
      "language": "yaml"
    },
    {
      "code": "type: Literal['response.reasoning_part.added']",
      "language": "yaml"
    },
    {
      "code": "type: Literal['response.reasoning_part.added']",
      "language": "yaml"
    },
    {
      "code": "1801\n1802\n1803\n1804\n1805\n1806\n1807\n1808\n1809\n1810\n1811\n1812\n1813\n1814\n1815\n1816\n1817\n1818",
      "language": "unknown"
    },
    {
      "code": "class ResponseReasoningPartDoneEvent(OpenAIBaseModel):\n    content_index: int\n    \"\"\"The index of the content part that is done.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the output item that the content part was added to.\"\"\"\n\n    output_index: int\n    \"\"\"The index of the output item that the content part was added to.\"\"\"\n\n    part: ResponseReasoningTextContent\n    \"\"\"The content part that is done.\"\"\"\n\n    sequence_number: int\n    \"\"\"The sequence number of this event.\"\"\"\n\n    type: Literal[\"response.reasoning_part.done\"]\n    \"\"\"The type of the event. Always `response.reasoning_part.done`.\"\"\"",
      "language": "php"
    },
    {
      "code": "class ResponseReasoningPartDoneEvent(OpenAIBaseModel):\n    content_index: int\n    \"\"\"The index of the content part that is done.\"\"\"\n\n    item_id: str\n    \"\"\"The ID of the output item that the content part was added to.\"\"\"\n\n    output_index: int\n    \"\"\"The index of the output item that the content part was added to.\"\"\"\n\n    part: ResponseReasoningTextContent\n    \"\"\"The content part that is done.\"\"\"\n\n    sequence_number: int\n    \"\"\"The sequence number of this event.\"\"\"\n\n    type: Literal[\"response.reasoning_part.done\"]\n    \"\"\"The type of the event. Always `response.reasoning_part.done`.\"\"\"",
      "language": "php"
    },
    {
      "code": "content_index: int",
      "language": "yaml"
    },
    {
      "code": "content_index: int",
      "language": "yaml"
    },
    {
      "code": "item_id: str",
      "language": "yaml"
    },
    {
      "code": "item_id: str",
      "language": "yaml"
    },
    {
      "code": "output_index: int",
      "language": "yaml"
    },
    {
      "code": "output_index: int",
      "language": "yaml"
    },
    {
      "code": "part: Content",
      "language": "yaml"
    },
    {
      "code": "part: Content",
      "language": "yaml"
    },
    {
      "code": "sequence_number: int",
      "language": "yaml"
    },
    {
      "code": "sequence_number: int",
      "language": "yaml"
    },
    {
      "code": "type: Literal['response.reasoning_part.done']",
      "language": "yaml"
    },
    {
      "code": "type: Literal['response.reasoning_part.done']",
      "language": "yaml"
    },
    {
      "code": "1647\n1648\n1649\n1650\n1651\n1652",
      "language": "unknown"
    },
    {
      "code": "class ResponseUsage(OpenAIBaseModel):\n    input_tokens: int\n    input_tokens_details: InputTokensDetails\n    output_tokens: int\n    output_tokens_details: OutputTokensDetails\n    total_tokens: int",
      "language": "php"
    },
    {
      "code": "class ResponseUsage(OpenAIBaseModel):\n    input_tokens: int\n    input_tokens_details: InputTokensDetails\n    output_tokens: int\n    output_tokens_details: OutputTokensDetails\n    total_tokens: int",
      "language": "php"
    },
    {
      "code": "input_tokens: int",
      "language": "yaml"
    },
    {
      "code": "input_tokens: int",
      "language": "yaml"
    },
    {
      "code": "input_tokens_details: InputTokensDetails",
      "language": "yaml"
    },
    {
      "code": "input_tokens_details: InputTokensDetails",
      "language": "yaml"
    },
    {
      "code": "output_tokens: int",
      "language": "yaml"
    },
    {
      "code": "output_tokens: int",
      "language": "yaml"
    },
    {
      "code": "output_tokens_details: OutputTokensDetails",
      "language": "yaml"
    },
    {
      "code": "output_tokens_details: OutputTokensDetails",
      "language": "yaml"
    },
    {
      "code": "total_tokens: int",
      "language": "yaml"
    },
    {
      "code": "total_tokens: int",
      "language": "yaml"
    },
    {
      "code": "330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555",
      "language": "unknown"
    },
    {
      "code": "class ResponsesRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/responses/create\n    background: bool | None = False\n    include: (\n        list[\n            Literal[\n                \"code_interpreter_call.outputs\",\n                \"computer_call_output.output.image_url\",\n                \"file_search_call.results\",\n                \"message.input_image.image_url\",\n                \"message.output_text.logprobs\",\n                \"reasoning.encrypted_content\",\n            ],\n        ]\n        | None\n    ) = None\n    input: str | list[ResponseInputOutputItem]\n    instructions: str | None = None\n    max_output_tokens: int | None = None\n    max_tool_calls: int | None = None\n    metadata: Metadata | None = None\n    model: str | None = None\n    logit_bias: dict[str, float] | None = None\n    parallel_tool_calls: bool | None = True\n    previous_response_id: str | None = None\n    prompt: ResponsePrompt | None = None\n    reasoning: Reasoning | None = None\n    service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] = \"auto\"\n    store: bool | None = True\n    stream: bool | None = False\n    temperature: float | None = None\n    text: ResponseTextConfig | None = None\n    tool_choice: ToolChoice = \"auto\"\n    tools: list[Tool] = Field(default_factory=list)\n    top_logprobs: int | None = 0\n    top_p: float | None = None\n    top_k: int | None = None\n    truncation: Literal[\"auto\", \"disabled\"] | None = \"disabled\"\n    user: str | None = None\n\n    # --8<-- [start:responses-extra-params]\n    request_id: str = Field(\n        default_factory=lambda: f\"resp_{random_uuid()}\",\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    mm_processor_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the HF processor.\"),\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"\n        ),\n    )\n    cache_salt: str | None = Field(\n        default=None,\n        description=(\n            \"If specified, the prefix cache will be salted with the provided \"\n            \"string to prevent an attacker to guess prompts in multi-user \"\n            \"environments. The salt should be random, protected from \"\n            \"access by 3rd parties, and long enough to be \"\n            \"unpredictable (e.g., 43 characters base64-encoded, corresponding \"\n            \"to 256 bit).\"\n        ),\n    )\n\n    enable_response_messages: bool = Field(\n        default=False,\n        description=(\n            \"Dictates whether or not to return messages as part of the \"\n            \"response object. Currently only supported for\"\n            \"non-background and gpt-oss only. \"\n        ),\n    )\n    # similar to input_messages / output_messages in ResponsesResponse\n    # we take in previous_input_messages (ie in harmony format)\n    # this cannot be used in conjunction with previous_response_id\n    # TODO: consider supporting non harmony messages as well\n    previous_input_messages: list[OpenAIHarmonyMessage | dict] | None = None\n    # --8<-- [end:responses-extra-params]\n\n    _DEFAULT_SAMPLING_PARAMS = {\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": 0,\n    }\n\n    def to_sampling_params(\n        self,\n        default_max_tokens: int,\n        default_sampling_params: dict | None = None,\n    ) -> SamplingParams:\n        if self.max_output_tokens is None:\n            max_tokens = default_max_tokens\n        else:\n            max_tokens = min(self.max_output_tokens, default_max_tokens)\n\n        default_sampling_params = default_sampling_params or {}\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n            )\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n            )\n        stop_token_ids = default_sampling_params.get(\"stop_token_ids\")\n\n        # Structured output\n        structured_outputs = None\n        if self.text is not None and self.text.format is not None:\n            response_format = self.text.format\n            if (\n                response_format.type == \"json_schema\"\n                and response_format.schema_ is not None\n            ):\n                structured_outputs = StructuredOutputsParams(\n                    json=response_format.schema_\n                )\n            elif response_format.type == \"json_object\":\n                raise NotImplementedError(\"json_object is not supported\")\n\n        # TODO: add more parameters\n        return SamplingParams.from_optional(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            max_tokens=max_tokens,\n            logprobs=self.top_logprobs if self.is_include_output_logprobs() else None,\n            stop_token_ids=stop_token_ids,\n            output_kind=(\n                RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY\n            ),\n            structured_outputs=structured_outputs,\n            logit_bias=self.logit_bias,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    def is_include_output_logprobs(self) -> bool:\n        \"\"\"Check if the request includes output logprobs.\"\"\"\n        if self.include is None:\n            return False\n        return (\n            isinstance(self.include, list)\n            and \"message.output_text.logprobs\" in self.include\n        )\n\n    @model_validator(mode=\"before\")\n    def validate_background(cls, data):\n        if not data.get(\"background\"):\n            return data\n        if not data.get(\"store\", True):\n            raise ValueError(\"background can only be used when `store` is true\")\n        return data\n\n    @model_validator(mode=\"before\")\n    def validate_prompt(cls, data):\n        if data.get(\"prompt\") is not None:\n            raise VLLMValidationError(\n                \"prompt template is not supported\", parameter=\"prompt\"\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    def check_cache_salt_support(cls, data):\n        if data.get(\"cache_salt\") is not None and (\n            not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n        ):\n            raise ValueError(\n                \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    def function_call_parsing(cls, data):\n        \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects.\n        This ensures Pydantic can properly resolve union types in the input field.\n        Function calls provided as dicts are converted to ResponseFunctionToolCall\n        objects before validation, while invalid structures are left for Pydantic\n        to reject with appropriate error messages.\n        \"\"\"\n\n        input_data = data.get(\"input\")\n\n        # Early return for None, strings, or bytes\n        # (strings are iterable but shouldn't be processed)\n        if input_data is None or isinstance(input_data, (str, bytes)):\n            return data\n\n        # Convert iterators (like ValidatorIterator) to list\n        if not isinstance(input_data, list):\n            try:\n                input_data = list(input_data)\n            except TypeError:\n                # Not iterable, leave as-is for Pydantic to handle\n                return data\n\n        processed_input = []\n        for item in input_data:\n            if isinstance(item, dict) and item.get(\"type\") == \"function_call\":\n                try:\n                    processed_input.append(ResponseFunctionToolCall(**item))\n                except ValidationError:\n                    # Let Pydantic handle validation for malformed function calls\n                    logger.debug(\n                        \"Failed to parse function_call to ResponseFunctionToolCall, \"\n                        \"leaving for Pydantic validation\"\n                    )\n                    processed_input.append(item)\n            else:\n                processed_input.append(item)\n\n        data[\"input\"] = processed_input\n        return data",
      "language": "python"
    },
    {
      "code": "class ResponsesRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/responses/create\n    background: bool | None = False\n    include: (\n        list[\n            Literal[\n                \"code_interpreter_call.outputs\",\n                \"computer_call_output.output.image_url\",\n                \"file_search_call.results\",\n                \"message.input_image.image_url\",\n                \"message.output_text.logprobs\",\n                \"reasoning.encrypted_content\",\n            ],\n        ]\n        | None\n    ) = None\n    input: str | list[ResponseInputOutputItem]\n    instructions: str | None = None\n    max_output_tokens: int | None = None\n    max_tool_calls: int | None = None\n    metadata: Metadata | None = None\n    model: str | None = None\n    logit_bias: dict[str, float] | None = None\n    parallel_tool_calls: bool | None = True\n    previous_response_id: str | None = None\n    prompt: ResponsePrompt | None = None\n    reasoning: Reasoning | None = None\n    service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] = \"auto\"\n    store: bool | None = True\n    stream: bool | None = False\n    temperature: float | None = None\n    text: ResponseTextConfig | None = None\n    tool_choice: ToolChoice = \"auto\"\n    tools: list[Tool] = Field(default_factory=list)\n    top_logprobs: int | None = 0\n    top_p: float | None = None\n    top_k: int | None = None\n    truncation: Literal[\"auto\", \"disabled\"] | None = \"disabled\"\n    user: str | None = None\n\n    # --8<-- [start:responses-extra-params]\n    request_id: str = Field(\n        default_factory=lambda: f\"resp_{random_uuid()}\",\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"\n        ),\n    )\n    mm_processor_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the HF processor.\"),\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"\n        ),\n    )\n    cache_salt: str | None = Field(\n        default=None,\n        description=(\n            \"If specified, the prefix cache will be salted with the provided \"\n            \"string to prevent an attacker to guess prompts in multi-user \"\n            \"environments. The salt should be random, protected from \"\n            \"access by 3rd parties, and long enough to be \"\n            \"unpredictable (e.g., 43 characters base64-encoded, corresponding \"\n            \"to 256 bit).\"\n        ),\n    )\n\n    enable_response_messages: bool = Field(\n        default=False,\n        description=(\n            \"Dictates whether or not to return messages as part of the \"\n            \"response object. Currently only supported for\"\n            \"non-background and gpt-oss only. \"\n        ),\n    )\n    # similar to input_messages / output_messages in ResponsesResponse\n    # we take in previous_input_messages (ie in harmony format)\n    # this cannot be used in conjunction with previous_response_id\n    # TODO: consider supporting non harmony messages as well\n    previous_input_messages: list[OpenAIHarmonyMessage | dict] | None = None\n    # --8<-- [end:responses-extra-params]\n\n    _DEFAULT_SAMPLING_PARAMS = {\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": 0,\n    }\n\n    def to_sampling_params(\n        self,\n        default_max_tokens: int,\n        default_sampling_params: dict | None = None,\n    ) -> SamplingParams:\n        if self.max_output_tokens is None:\n            max_tokens = default_max_tokens\n        else:\n            max_tokens = min(self.max_output_tokens, default_max_tokens)\n\n        default_sampling_params = default_sampling_params or {}\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n            )\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n            )\n        stop_token_ids = default_sampling_params.get(\"stop_token_ids\")\n\n        # Structured output\n        structured_outputs = None\n        if self.text is not None and self.text.format is not None:\n            response_format = self.text.format\n            if (\n                response_format.type == \"json_schema\"\n                and response_format.schema_ is not None\n            ):\n                structured_outputs = StructuredOutputsParams(\n                    json=response_format.schema_\n                )\n            elif response_format.type == \"json_object\":\n                raise NotImplementedError(\"json_object is not supported\")\n\n        # TODO: add more parameters\n        return SamplingParams.from_optional(\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            max_tokens=max_tokens,\n            logprobs=self.top_logprobs if self.is_include_output_logprobs() else None,\n            stop_token_ids=stop_token_ids,\n            output_kind=(\n                RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY\n            ),\n            structured_outputs=structured_outputs,\n            logit_bias=self.logit_bias,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    def is_include_output_logprobs(self) -> bool:\n        \"\"\"Check if the request includes output logprobs.\"\"\"\n        if self.include is None:\n            return False\n        return (\n            isinstance(self.include, list)\n            and \"message.output_text.logprobs\" in self.include\n        )\n\n    @model_validator(mode=\"before\")\n    def validate_background(cls, data):\n        if not data.get(\"background\"):\n            return data\n        if not data.get(\"store\", True):\n            raise ValueError(\"background can only be used when `store` is true\")\n        return data\n\n    @model_validator(mode=\"before\")\n    def validate_prompt(cls, data):\n        if data.get(\"prompt\") is not None:\n            raise VLLMValidationError(\n                \"prompt template is not supported\", parameter=\"prompt\"\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    def check_cache_salt_support(cls, data):\n        if data.get(\"cache_salt\") is not None and (\n            not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n        ):\n            raise ValueError(\n                \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n            )\n        return data\n\n    @model_validator(mode=\"before\")\n    def function_call_parsing(cls, data):\n        \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects.\n        This ensures Pydantic can properly resolve union types in the input field.\n        Function calls provided as dicts are converted to ResponseFunctionToolCall\n        objects before validation, while invalid structures are left for Pydantic\n        to reject with appropriate error messages.\n        \"\"\"\n\n        input_data = data.get(\"input\")\n\n        # Early return for None, strings, or bytes\n        # (strings are iterable but shouldn't be processed)\n        if input_data is None or isinstance(input_data, (str, bytes)):\n            return data\n\n        # Convert iterators (like ValidatorIterator) to list\n        if not isinstance(input_data, list):\n            try:\n                input_data = list(input_data)\n            except TypeError:\n                # Not iterable, leave as-is for Pydantic to handle\n                return data\n\n        processed_input = []\n        for item in input_data:\n            if isinstance(item, dict) and item.get(\"type\") == \"function_call\":\n                try:\n                    processed_input.append(ResponseFunctionToolCall(**item))\n                except ValidationError:\n                    # Let Pydantic handle validation for malformed function calls\n                    logger.debug(\n                        \"Failed to parse function_call to ResponseFunctionToolCall, \"\n                        \"leaving for Pydantic validation\"\n                    )\n                    processed_input.append(item)\n            else:\n                processed_input.append(item)\n\n        data[\"input\"] = processed_input\n        return data",
      "language": "python"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS = {\n    \"temperature\": 1.0,\n    \"top_p\": 1.0,\n    \"top_k\": 0,\n}",
      "language": "json"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS = {\n    \"temperature\": 1.0,\n    \"top_p\": 1.0,\n    \"top_k\": 0,\n}",
      "language": "json"
    },
    {
      "code": "background: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "background: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "cache_salt: str | None = Field(\n    default=None,\n    description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\",\n)",
      "language": "sql"
    },
    {
      "code": "cache_salt: str | None = Field(\n    default=None,\n    description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\",\n)",
      "language": "sql"
    },
    {
      "code": "enable_response_messages: bool = Field(\n    default=False,\n    description=\"Dictates whether or not to return messages as part of the response object. Currently only supported fornon-background and gpt-oss only. \",\n)",
      "language": "typescript"
    },
    {
      "code": "enable_response_messages: bool = Field(\n    default=False,\n    description=\"Dictates whether or not to return messages as part of the response object. Currently only supported fornon-background and gpt-oss only. \",\n)",
      "language": "typescript"
    },
    {
      "code": "include: (\n    list[\n        Literal[\n            \"code_interpreter_call.outputs\",\n            \"computer_call_output.output.image_url\",\n            \"file_search_call.results\",\n            \"message.input_image.image_url\",\n            \"message.output_text.logprobs\",\n            \"reasoning.encrypted_content\",\n        ],\n    ]\n    | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "include: (\n    list[\n        Literal[\n            \"code_interpreter_call.outputs\",\n            \"computer_call_output.output.image_url\",\n            \"file_search_call.results\",\n            \"message.input_image.image_url\",\n            \"message.output_text.logprobs\",\n            \"reasoning.encrypted_content\",\n        ],\n    ]\n    | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "input: str | list[ResponseInputOutputItem]",
      "language": "yaml"
    },
    {
      "code": "input: str | list[ResponseInputOutputItem]",
      "language": "yaml"
    },
    {
      "code": "instructions: str | None = None",
      "language": "yaml"
    },
    {
      "code": "instructions: str | None = None",
      "language": "yaml"
    },
    {
      "code": "logit_bias: dict[str, float] | None = None",
      "language": "yaml"
    },
    {
      "code": "logit_bias: dict[str, float] | None = None",
      "language": "yaml"
    },
    {
      "code": "max_output_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_output_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_tool_calls: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_tool_calls: int | None = None",
      "language": "yaml"
    },
    {
      "code": "metadata: Metadata | None = None",
      "language": "yaml"
    },
    {
      "code": "metadata: Metadata | None = None",
      "language": "yaml"
    },
    {
      "code": "mm_processor_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional kwargs to pass to the HF processor.\",\n)",
      "language": "yaml"
    },
    {
      "code": "mm_processor_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional kwargs to pass to the HF processor.\",\n)",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "parallel_tool_calls: bool | None = True",
      "language": "yaml"
    },
    {
      "code": "parallel_tool_calls: bool | None = True",
      "language": "yaml"
    },
    {
      "code": "previous_input_messages: list[Message | dict] | None = None",
      "language": "yaml"
    },
    {
      "code": "previous_input_messages: list[Message | dict] | None = None",
      "language": "yaml"
    },
    {
      "code": "previous_response_id: str | None = None",
      "language": "yaml"
    },
    {
      "code": "previous_response_id: str | None = None",
      "language": "yaml"
    },
    {
      "code": "priority: int = Field(\n    default=0,\n    description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\",\n)",
      "language": "typescript"
    },
    {
      "code": "priority: int = Field(\n    default=0,\n    description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\",\n)",
      "language": "typescript"
    },
    {
      "code": "prompt: ResponsePrompt | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt: ResponsePrompt | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning: Reasoning | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning: Reasoning | None = None",
      "language": "yaml"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=lambda: f\"resp_{random_uuid()}\",\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "request_id: str = Field(\n    default_factory=lambda: f\"resp_{random_uuid()}\",\n    description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\",\n)",
      "language": "typescript"
    },
    {
      "code": "service_tier: Literal[\n    \"auto\", \"default\", \"flex\", \"scale\", \"priority\"\n] = \"auto\"",
      "language": "yaml"
    },
    {
      "code": "service_tier: Literal[\n    \"auto\", \"default\", \"flex\", \"scale\", \"priority\"\n] = \"auto\"",
      "language": "yaml"
    },
    {
      "code": "store: bool | None = True",
      "language": "yaml"
    },
    {
      "code": "store: bool | None = True",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "temperature: float | None = None",
      "language": "yaml"
    },
    {
      "code": "temperature: float | None = None",
      "language": "yaml"
    },
    {
      "code": "text: ResponseFormatTextConfig | None = None",
      "language": "yaml"
    },
    {
      "code": "text: ResponseFormatTextConfig | None = None",
      "language": "yaml"
    },
    {
      "code": "tool_choice: ToolChoice = 'auto'",
      "language": "typescript"
    },
    {
      "code": "tool_choice: ToolChoice = 'auto'",
      "language": "typescript"
    },
    {
      "code": "tools: list[Tool] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "tools: list[Tool] = Field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "top_k: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_k: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: int | None = 0",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: int | None = 0",
      "language": "yaml"
    },
    {
      "code": "top_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "top_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "truncation: Literal['auto', 'disabled'] | None = 'disabled'",
      "language": "yaml"
    },
    {
      "code": "truncation: Literal['auto', 'disabled'] | None = 'disabled'",
      "language": "yaml"
    },
    {
      "code": "user: str | None = None",
      "language": "yaml"
    },
    {
      "code": "user: str | None = None",
      "language": "yaml"
    },
    {
      "code": "check_cache_salt_support(data)",
      "language": "unknown"
    },
    {
      "code": "check_cache_salt_support(data)",
      "language": "unknown"
    },
    {
      "code": "505\n506\n507\n508\n509\n510\n511\n512\n513",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\ndef check_cache_salt_support(cls, data):\n    if data.get(\"cache_salt\") is not None and (\n        not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n    ):\n        raise ValueError(\n            \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\ndef check_cache_salt_support(cls, data):\n    if data.get(\"cache_salt\") is not None and (\n        not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"]\n    ):\n        raise ValueError(\n            \"Parameter 'cache_salt' must be a non-empty string if provided.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "function_call_parsing(data)",
      "language": "unknown"
    },
    {
      "code": "function_call_parsing(data)",
      "language": "unknown"
    },
    {
      "code": "515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\ndef function_call_parsing(cls, data):\n    \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects.\n    This ensures Pydantic can properly resolve union types in the input field.\n    Function calls provided as dicts are converted to ResponseFunctionToolCall\n    objects before validation, while invalid structures are left for Pydantic\n    to reject with appropriate error messages.\n    \"\"\"\n\n    input_data = data.get(\"input\")\n\n    # Early return for None, strings, or bytes\n    # (strings are iterable but shouldn't be processed)\n    if input_data is None or isinstance(input_data, (str, bytes)):\n        return data\n\n    # Convert iterators (like ValidatorIterator) to list\n    if not isinstance(input_data, list):\n        try:\n            input_data = list(input_data)\n        except TypeError:\n            # Not iterable, leave as-is for Pydantic to handle\n            return data\n\n    processed_input = []\n    for item in input_data:\n        if isinstance(item, dict) and item.get(\"type\") == \"function_call\":\n            try:\n                processed_input.append(ResponseFunctionToolCall(**item))\n            except ValidationError:\n                # Let Pydantic handle validation for malformed function calls\n                logger.debug(\n                    \"Failed to parse function_call to ResponseFunctionToolCall, \"\n                    \"leaving for Pydantic validation\"\n                )\n                processed_input.append(item)\n        else:\n            processed_input.append(item)\n\n    data[\"input\"] = processed_input\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\ndef function_call_parsing(cls, data):\n    \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects.\n    This ensures Pydantic can properly resolve union types in the input field.\n    Function calls provided as dicts are converted to ResponseFunctionToolCall\n    objects before validation, while invalid structures are left for Pydantic\n    to reject with appropriate error messages.\n    \"\"\"\n\n    input_data = data.get(\"input\")\n\n    # Early return for None, strings, or bytes\n    # (strings are iterable but shouldn't be processed)\n    if input_data is None or isinstance(input_data, (str, bytes)):\n        return data\n\n    # Convert iterators (like ValidatorIterator) to list\n    if not isinstance(input_data, list):\n        try:\n            input_data = list(input_data)\n        except TypeError:\n            # Not iterable, leave as-is for Pydantic to handle\n            return data\n\n    processed_input = []\n    for item in input_data:\n        if isinstance(item, dict) and item.get(\"type\") == \"function_call\":\n            try:\n                processed_input.append(ResponseFunctionToolCall(**item))\n            except ValidationError:\n                # Let Pydantic handle validation for malformed function calls\n                logger.debug(\n                    \"Failed to parse function_call to ResponseFunctionToolCall, \"\n                    \"leaving for Pydantic validation\"\n                )\n                processed_input.append(item)\n        else:\n            processed_input.append(item)\n\n    data[\"input\"] = processed_input\n    return data",
      "language": "python"
    },
    {
      "code": "is_include_output_logprobs() -> bool",
      "language": "php"
    },
    {
      "code": "is_include_output_logprobs() -> bool",
      "language": "php"
    },
    {
      "code": "480\n481\n482\n483\n484\n485\n486\n487",
      "language": "unknown"
    },
    {
      "code": "def is_include_output_logprobs(self) -> bool:\n    \"\"\"Check if the request includes output logprobs.\"\"\"\n    if self.include is None:\n        return False\n    return (\n        isinstance(self.include, list)\n        and \"message.output_text.logprobs\" in self.include\n    )",
      "language": "python"
    },
    {
      "code": "def is_include_output_logprobs(self) -> bool:\n    \"\"\"Check if the request includes output logprobs.\"\"\"\n    if self.include is None:\n        return False\n    return (\n        isinstance(self.include, list)\n        and \"message.output_text.logprobs\" in self.include\n    )",
      "language": "python"
    },
    {
      "code": "to_sampling_params(\n    default_max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "to_sampling_params(\n    default_max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478",
      "language": "unknown"
    },
    {
      "code": "def to_sampling_params(\n    self,\n    default_max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams:\n    if self.max_output_tokens is None:\n        max_tokens = default_max_tokens\n    else:\n        max_tokens = min(self.max_output_tokens, default_max_tokens)\n\n    default_sampling_params = default_sampling_params or {}\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n    if (top_p := self.top_p) is None:\n        top_p = default_sampling_params.get(\n            \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n        )\n    if (top_k := self.top_k) is None:\n        top_k = default_sampling_params.get(\n            \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n        )\n    stop_token_ids = default_sampling_params.get(\"stop_token_ids\")\n\n    # Structured output\n    structured_outputs = None\n    if self.text is not None and self.text.format is not None:\n        response_format = self.text.format\n        if (\n            response_format.type == \"json_schema\"\n            and response_format.schema_ is not None\n        ):\n            structured_outputs = StructuredOutputsParams(\n                json=response_format.schema_\n            )\n        elif response_format.type == \"json_object\":\n            raise NotImplementedError(\"json_object is not supported\")\n\n    # TODO: add more parameters\n    return SamplingParams.from_optional(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        max_tokens=max_tokens,\n        logprobs=self.top_logprobs if self.is_include_output_logprobs() else None,\n        stop_token_ids=stop_token_ids,\n        output_kind=(\n            RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY\n        ),\n        structured_outputs=structured_outputs,\n        logit_bias=self.logit_bias,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "def to_sampling_params(\n    self,\n    default_max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams:\n    if self.max_output_tokens is None:\n        max_tokens = default_max_tokens\n    else:\n        max_tokens = min(self.max_output_tokens, default_max_tokens)\n\n    default_sampling_params = default_sampling_params or {}\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n    if (top_p := self.top_p) is None:\n        top_p = default_sampling_params.get(\n            \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n        )\n    if (top_k := self.top_k) is None:\n        top_k = default_sampling_params.get(\n            \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n        )\n    stop_token_ids = default_sampling_params.get(\"stop_token_ids\")\n\n    # Structured output\n    structured_outputs = None\n    if self.text is not None and self.text.format is not None:\n        response_format = self.text.format\n        if (\n            response_format.type == \"json_schema\"\n            and response_format.schema_ is not None\n        ):\n            structured_outputs = StructuredOutputsParams(\n                json=response_format.schema_\n            )\n        elif response_format.type == \"json_object\":\n            raise NotImplementedError(\"json_object is not supported\")\n\n    # TODO: add more parameters\n    return SamplingParams.from_optional(\n        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n        max_tokens=max_tokens,\n        logprobs=self.top_logprobs if self.is_include_output_logprobs() else None,\n        stop_token_ids=stop_token_ids,\n        output_kind=(\n            RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY\n        ),\n        structured_outputs=structured_outputs,\n        logit_bias=self.logit_bias,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "validate_background(data)",
      "language": "unknown"
    },
    {
      "code": "validate_background(data)",
      "language": "unknown"
    },
    {
      "code": "489\n490\n491\n492\n493\n494\n495",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\ndef validate_background(cls, data):\n    if not data.get(\"background\"):\n        return data\n    if not data.get(\"store\", True):\n        raise ValueError(\"background can only be used when `store` is true\")\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\ndef validate_background(cls, data):\n    if not data.get(\"background\"):\n        return data\n    if not data.get(\"store\", True):\n        raise ValueError(\"background can only be used when `store` is true\")\n    return data",
      "language": "python"
    },
    {
      "code": "validate_prompt(data)",
      "language": "unknown"
    },
    {
      "code": "validate_prompt(data)",
      "language": "unknown"
    },
    {
      "code": "497\n498\n499\n500\n501\n502\n503",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\ndef validate_prompt(cls, data):\n    if data.get(\"prompt\") is not None:\n        raise VLLMValidationError(\n            \"prompt template is not supported\", parameter=\"prompt\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\ndef validate_prompt(cls, data):\n    if data.get(\"prompt\") is not None:\n        raise VLLMValidationError(\n            \"prompt template is not supported\", parameter=\"prompt\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "1689\n1690\n1691\n1692\n1693\n1694\n1695\n1696\n1697\n1698\n1699\n1700\n1701\n1702\n1703\n1704\n1705\n1706\n1707\n1708\n1709\n1710\n1711\n1712\n1713\n1714\n1715\n1716\n1717\n1718\n1719\n1720\n1721\n1722\n1723\n1724\n1725\n1726\n1727\n1728\n1729\n1730\n1731\n1732\n1733\n1734\n1735\n1736\n1737\n1738\n1739\n1740\n1741\n1742\n1743\n1744\n1745\n1746\n1747\n1748\n1749\n1750\n1751\n1752\n1753\n1754\n1755\n1756\n1757\n1758\n1759\n1760\n1761\n1762\n1763\n1764\n1765\n1766\n1767\n1768\n1769\n1770\n1771\n1772\n1773\n1774\n1775\n1776\n1777\n1778\n1779\n1780\n1781\n1782\n1783\n1784\n1785\n1786\n1787\n1788\n1789\n1790\n1791\n1792\n1793\n1794\n1795\n1796",
      "language": "unknown"
    },
    {
      "code": "class ResponsesResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"resp_{random_uuid()}\")\n    created_at: int = Field(default_factory=lambda: int(time.time()))\n    # error: Optional[ResponseError] = None\n    incomplete_details: IncompleteDetails | None = None\n    instructions: str | None = None\n    metadata: Metadata | None = None\n    model: str\n    object: Literal[\"response\"] = \"response\"\n    output: list[ResponseOutputItem]\n    parallel_tool_calls: bool\n    temperature: float\n    tool_choice: ToolChoice\n    tools: list[Tool]\n    top_p: float\n    background: bool\n    max_output_tokens: int\n    max_tool_calls: int | None = None\n    previous_response_id: str | None = None\n    prompt: ResponsePrompt | None = None\n    reasoning: Reasoning | None = None\n    service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]\n    status: ResponseStatus\n    text: ResponseTextConfig | None = None\n    top_logprobs: int | None = None\n    truncation: Literal[\"auto\", \"disabled\"]\n    usage: ResponseUsage | None = None\n    user: str | None = None\n\n    # --8<-- [start:responses-response-extra-params]\n    # These are populated when enable_response_messages is set to True\n    # NOTE: custom serialization is needed\n    # see serialize_input_messages and serialize_output_messages\n    input_messages: ResponseInputOutputMessage | None = Field(\n        default=None,\n        description=(\n            \"If enable_response_messages, we can show raw token input to model.\"\n        ),\n    )\n    output_messages: ResponseInputOutputMessage | None = Field(\n        default=None,\n        description=(\n            \"If enable_response_messages, we can show raw token output of model.\"\n        ),\n    )\n    # --8<-- [end:responses-response-extra-params]\n\n    # NOTE: openAI harmony doesn't serialize TextContent properly,\n    # TODO: this fixes for TextContent, but need to verify for tools etc\n    # https://github.com/openai/harmony/issues/78\n    @field_serializer(\"output_messages\", when_used=\"json\")\n    def serialize_output_messages(self, msgs, _info):\n        return serialize_messages(msgs)\n\n    # NOTE: openAI harmony doesn't serialize TextContent properly, this fixes it\n    # https://github.com/openai/harmony/issues/78\n    @field_serializer(\"input_messages\", when_used=\"json\")\n    def serialize_input_messages(self, msgs, _info):\n        return serialize_messages(msgs)\n\n    @classmethod\n    def from_request(\n        cls,\n        request: ResponsesRequest,\n        sampling_params: SamplingParams,\n        model_name: str,\n        created_time: int,\n        output: list[ResponseOutputItem],\n        status: ResponseStatus,\n        usage: ResponseUsage | None = None,\n        input_messages: ResponseInputOutputMessage | None = None,\n        output_messages: ResponseInputOutputMessage | None = None,\n    ) -> \"ResponsesResponse\":\n        incomplete_details: IncompleteDetails | None = None\n        if status == \"incomplete\":\n            incomplete_details = IncompleteDetails(reason=\"max_output_tokens\")\n        # TODO: implement the other reason for incomplete_details,\n        # which is content_filter\n        # incomplete_details = IncompleteDetails(reason='content_filter')\n        return cls(\n            id=request.request_id,\n            created_at=created_time,\n            incomplete_details=incomplete_details,\n            instructions=request.instructions,\n            metadata=request.metadata,\n            model=model_name,\n            output=output,\n            input_messages=input_messages,\n            output_messages=output_messages,\n            parallel_tool_calls=request.parallel_tool_calls,\n            temperature=sampling_params.temperature,\n            tool_choice=request.tool_choice,\n            tools=request.tools,\n            top_p=sampling_params.top_p,\n            background=request.background,\n            max_output_tokens=sampling_params.max_tokens,\n            max_tool_calls=request.max_tool_calls,\n            previous_response_id=request.previous_response_id,\n            prompt=request.prompt,\n            reasoning=request.reasoning,\n            service_tier=request.service_tier,\n            status=status,\n            text=request.text,\n            top_logprobs=sampling_params.logprobs,\n            truncation=request.truncation,\n            user=request.user,\n            usage=usage,\n        )",
      "language": "python"
    },
    {
      "code": "class ResponsesResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"resp_{random_uuid()}\")\n    created_at: int = Field(default_factory=lambda: int(time.time()))\n    # error: Optional[ResponseError] = None\n    incomplete_details: IncompleteDetails | None = None\n    instructions: str | None = None\n    metadata: Metadata | None = None\n    model: str\n    object: Literal[\"response\"] = \"response\"\n    output: list[ResponseOutputItem]\n    parallel_tool_calls: bool\n    temperature: float\n    tool_choice: ToolChoice\n    tools: list[Tool]\n    top_p: float\n    background: bool\n    max_output_tokens: int\n    max_tool_calls: int | None = None\n    previous_response_id: str | None = None\n    prompt: ResponsePrompt | None = None\n    reasoning: Reasoning | None = None\n    service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"]\n    status: ResponseStatus\n    text: ResponseTextConfig | None = None\n    top_logprobs: int | None = None\n    truncation: Literal[\"auto\", \"disabled\"]\n    usage: ResponseUsage | None = None\n    user: str | None = None\n\n    # --8<-- [start:responses-response-extra-params]\n    # These are populated when enable_response_messages is set to True\n    # NOTE: custom serialization is needed\n    # see serialize_input_messages and serialize_output_messages\n    input_messages: ResponseInputOutputMessage | None = Field(\n        default=None,\n        description=(\n            \"If enable_response_messages, we can show raw token input to model.\"\n        ),\n    )\n    output_messages: ResponseInputOutputMessage | None = Field(\n        default=None,\n        description=(\n            \"If enable_response_messages, we can show raw token output of model.\"\n        ),\n    )\n    # --8<-- [end:responses-response-extra-params]\n\n    # NOTE: openAI harmony doesn't serialize TextContent properly,\n    # TODO: this fixes for TextContent, but need to verify for tools etc\n    # https://github.com/openai/harmony/issues/78\n    @field_serializer(\"output_messages\", when_used=\"json\")\n    def serialize_output_messages(self, msgs, _info):\n        return serialize_messages(msgs)\n\n    # NOTE: openAI harmony doesn't serialize TextContent properly, this fixes it\n    # https://github.com/openai/harmony/issues/78\n    @field_serializer(\"input_messages\", when_used=\"json\")\n    def serialize_input_messages(self, msgs, _info):\n        return serialize_messages(msgs)\n\n    @classmethod\n    def from_request(\n        cls,\n        request: ResponsesRequest,\n        sampling_params: SamplingParams,\n        model_name: str,\n        created_time: int,\n        output: list[ResponseOutputItem],\n        status: ResponseStatus,\n        usage: ResponseUsage | None = None,\n        input_messages: ResponseInputOutputMessage | None = None,\n        output_messages: ResponseInputOutputMessage | None = None,\n    ) -> \"ResponsesResponse\":\n        incomplete_details: IncompleteDetails | None = None\n        if status == \"incomplete\":\n            incomplete_details = IncompleteDetails(reason=\"max_output_tokens\")\n        # TODO: implement the other reason for incomplete_details,\n        # which is content_filter\n        # incomplete_details = IncompleteDetails(reason='content_filter')\n        return cls(\n            id=request.request_id,\n            created_at=created_time,\n            incomplete_details=incomplete_details,\n            instructions=request.instructions,\n            metadata=request.metadata,\n            model=model_name,\n            output=output,\n            input_messages=input_messages,\n            output_messages=output_messages,\n            parallel_tool_calls=request.parallel_tool_calls,\n            temperature=sampling_params.temperature,\n            tool_choice=request.tool_choice,\n            tools=request.tools,\n            top_p=sampling_params.top_p,\n            background=request.background,\n            max_output_tokens=sampling_params.max_tokens,\n            max_tool_calls=request.max_tool_calls,\n            previous_response_id=request.previous_response_id,\n            prompt=request.prompt,\n            reasoning=request.reasoning,\n            service_tier=request.service_tier,\n            status=status,\n            text=request.text,\n            top_logprobs=sampling_params.logprobs,\n            truncation=request.truncation,\n            user=request.user,\n            usage=usage,\n        )",
      "language": "python"
    },
    {
      "code": "background: bool",
      "language": "yaml"
    },
    {
      "code": "background: bool",
      "language": "yaml"
    },
    {
      "code": "created_at: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created_at: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"resp_{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"resp_{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "incomplete_details: IncompleteDetails | None = None",
      "language": "yaml"
    },
    {
      "code": "incomplete_details: IncompleteDetails | None = None",
      "language": "yaml"
    },
    {
      "code": "input_messages: ResponseInputOutputMessage | None = Field(\n    default=None,\n    description=\"If enable_response_messages, we can show raw token input to model.\",\n)",
      "language": "yaml"
    },
    {
      "code": "input_messages: ResponseInputOutputMessage | None = Field(\n    default=None,\n    description=\"If enable_response_messages, we can show raw token input to model.\",\n)",
      "language": "yaml"
    },
    {
      "code": "instructions: str | None = None",
      "language": "yaml"
    },
    {
      "code": "instructions: str | None = None",
      "language": "yaml"
    },
    {
      "code": "max_output_tokens: int",
      "language": "yaml"
    },
    {
      "code": "max_output_tokens: int",
      "language": "yaml"
    },
    {
      "code": "max_tool_calls: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_tool_calls: int | None = None",
      "language": "yaml"
    },
    {
      "code": "metadata: Metadata | None = None",
      "language": "yaml"
    },
    {
      "code": "metadata: Metadata | None = None",
      "language": "yaml"
    },
    {
      "code": "object: Literal['response'] = 'response'",
      "language": "yaml"
    },
    {
      "code": "object: Literal['response'] = 'response'",
      "language": "yaml"
    },
    {
      "code": "output: list[ResponseOutputItem]",
      "language": "yaml"
    },
    {
      "code": "output: list[ResponseOutputItem]",
      "language": "yaml"
    },
    {
      "code": "output_messages: ResponseInputOutputMessage | None = Field(\n    default=None,\n    description=\"If enable_response_messages, we can show raw token output of model.\",\n)",
      "language": "yaml"
    },
    {
      "code": "output_messages: ResponseInputOutputMessage | None = Field(\n    default=None,\n    description=\"If enable_response_messages, we can show raw token output of model.\",\n)",
      "language": "yaml"
    },
    {
      "code": "parallel_tool_calls: bool",
      "language": "yaml"
    },
    {
      "code": "parallel_tool_calls: bool",
      "language": "yaml"
    },
    {
      "code": "previous_response_id: str | None = None",
      "language": "yaml"
    },
    {
      "code": "previous_response_id: str | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt: ResponsePrompt | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt: ResponsePrompt | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning: Reasoning | None = None",
      "language": "yaml"
    },
    {
      "code": "reasoning: Reasoning | None = None",
      "language": "yaml"
    },
    {
      "code": "service_tier: Literal[\n    \"auto\", \"default\", \"flex\", \"scale\", \"priority\"\n]",
      "language": "yaml"
    },
    {
      "code": "service_tier: Literal[\n    \"auto\", \"default\", \"flex\", \"scale\", \"priority\"\n]",
      "language": "yaml"
    },
    {
      "code": "status: ResponseStatus",
      "language": "yaml"
    },
    {
      "code": "status: ResponseStatus",
      "language": "yaml"
    },
    {
      "code": "temperature: float",
      "language": "yaml"
    },
    {
      "code": "temperature: float",
      "language": "yaml"
    },
    {
      "code": "text: ResponseFormatTextConfig | None = None",
      "language": "yaml"
    },
    {
      "code": "text: ResponseFormatTextConfig | None = None",
      "language": "yaml"
    },
    {
      "code": "tool_choice: ToolChoice",
      "language": "yaml"
    },
    {
      "code": "tool_choice: ToolChoice",
      "language": "yaml"
    },
    {
      "code": "tools: list[Tool]",
      "language": "yaml"
    },
    {
      "code": "tools: list[Tool]",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_logprobs: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_p: float",
      "language": "yaml"
    },
    {
      "code": "top_p: float",
      "language": "yaml"
    },
    {
      "code": "truncation: Literal['auto', 'disabled']",
      "language": "yaml"
    },
    {
      "code": "truncation: Literal['auto', 'disabled']",
      "language": "yaml"
    },
    {
      "code": "usage: ResponseUsage | None = None",
      "language": "yaml"
    },
    {
      "code": "usage: ResponseUsage | None = None",
      "language": "yaml"
    },
    {
      "code": "user: str | None = None",
      "language": "yaml"
    },
    {
      "code": "user: str | None = None",
      "language": "yaml"
    },
    {
      "code": "from_request(\n    request: ResponsesRequest,\n    sampling_params: SamplingParams,\n    model_name: str,\n    created_time: int,\n    output: list[ResponseOutputItem],\n    status: ResponseStatus,\n    usage: ResponseUsage | None = None,\n    input_messages: ResponseInputOutputMessage\n    | None = None,\n    output_messages: ResponseInputOutputMessage\n    | None = None,\n) -> ResponsesResponse",
      "language": "rust"
    },
    {
      "code": "from_request(\n    request: ResponsesRequest,\n    sampling_params: SamplingParams,\n    model_name: str,\n    created_time: int,\n    output: list[ResponseOutputItem],\n    status: ResponseStatus,\n    usage: ResponseUsage | None = None,\n    input_messages: ResponseInputOutputMessage\n    | None = None,\n    output_messages: ResponseInputOutputMessage\n    | None = None,\n) -> ResponsesResponse",
      "language": "rust"
    },
    {
      "code": "1749\n1750\n1751\n1752\n1753\n1754\n1755\n1756\n1757\n1758\n1759\n1760\n1761\n1762\n1763\n1764\n1765\n1766\n1767\n1768\n1769\n1770\n1771\n1772\n1773\n1774\n1775\n1776\n1777\n1778\n1779\n1780\n1781\n1782\n1783\n1784\n1785\n1786\n1787\n1788\n1789\n1790\n1791\n1792\n1793\n1794\n1795\n1796",
      "language": "unknown"
    },
    {
      "code": "@classmethod\ndef from_request(\n    cls,\n    request: ResponsesRequest,\n    sampling_params: SamplingParams,\n    model_name: str,\n    created_time: int,\n    output: list[ResponseOutputItem],\n    status: ResponseStatus,\n    usage: ResponseUsage | None = None,\n    input_messages: ResponseInputOutputMessage | None = None,\n    output_messages: ResponseInputOutputMessage | None = None,\n) -> \"ResponsesResponse\":\n    incomplete_details: IncompleteDetails | None = None\n    if status == \"incomplete\":\n        incomplete_details = IncompleteDetails(reason=\"max_output_tokens\")\n    # TODO: implement the other reason for incomplete_details,\n    # which is content_filter\n    # incomplete_details = IncompleteDetails(reason='content_filter')\n    return cls(\n        id=request.request_id,\n        created_at=created_time,\n        incomplete_details=incomplete_details,\n        instructions=request.instructions,\n        metadata=request.metadata,\n        model=model_name,\n        output=output,\n        input_messages=input_messages,\n        output_messages=output_messages,\n        parallel_tool_calls=request.parallel_tool_calls,\n        temperature=sampling_params.temperature,\n        tool_choice=request.tool_choice,\n        tools=request.tools,\n        top_p=sampling_params.top_p,\n        background=request.background,\n        max_output_tokens=sampling_params.max_tokens,\n        max_tool_calls=request.max_tool_calls,\n        previous_response_id=request.previous_response_id,\n        prompt=request.prompt,\n        reasoning=request.reasoning,\n        service_tier=request.service_tier,\n        status=status,\n        text=request.text,\n        top_logprobs=sampling_params.logprobs,\n        truncation=request.truncation,\n        user=request.user,\n        usage=usage,\n    )",
      "language": "python"
    },
    {
      "code": "@classmethod\ndef from_request(\n    cls,\n    request: ResponsesRequest,\n    sampling_params: SamplingParams,\n    model_name: str,\n    created_time: int,\n    output: list[ResponseOutputItem],\n    status: ResponseStatus,\n    usage: ResponseUsage | None = None,\n    input_messages: ResponseInputOutputMessage | None = None,\n    output_messages: ResponseInputOutputMessage | None = None,\n) -> \"ResponsesResponse\":\n    incomplete_details: IncompleteDetails | None = None\n    if status == \"incomplete\":\n        incomplete_details = IncompleteDetails(reason=\"max_output_tokens\")\n    # TODO: implement the other reason for incomplete_details,\n    # which is content_filter\n    # incomplete_details = IncompleteDetails(reason='content_filter')\n    return cls(\n        id=request.request_id,\n        created_at=created_time,\n        incomplete_details=incomplete_details,\n        instructions=request.instructions,\n        metadata=request.metadata,\n        model=model_name,\n        output=output,\n        input_messages=input_messages,\n        output_messages=output_messages,\n        parallel_tool_calls=request.parallel_tool_calls,\n        temperature=sampling_params.temperature,\n        tool_choice=request.tool_choice,\n        tools=request.tools,\n        top_p=sampling_params.top_p,\n        background=request.background,\n        max_output_tokens=sampling_params.max_tokens,\n        max_tool_calls=request.max_tool_calls,\n        previous_response_id=request.previous_response_id,\n        prompt=request.prompt,\n        reasoning=request.reasoning,\n        service_tier=request.service_tier,\n        status=status,\n        text=request.text,\n        top_logprobs=sampling_params.logprobs,\n        truncation=request.truncation,\n        user=request.user,\n        usage=usage,\n    )",
      "language": "python"
    },
    {
      "code": "serialize_input_messages(msgs, _info)",
      "language": "unknown"
    },
    {
      "code": "serialize_input_messages(msgs, _info)",
      "language": "unknown"
    },
    {
      "code": "1745\n1746\n1747",
      "language": "unknown"
    },
    {
      "code": "@field_serializer(\"input_messages\", when_used=\"json\")\ndef serialize_input_messages(self, msgs, _info):\n    return serialize_messages(msgs)",
      "language": "python"
    },
    {
      "code": "@field_serializer(\"input_messages\", when_used=\"json\")\ndef serialize_input_messages(self, msgs, _info):\n    return serialize_messages(msgs)",
      "language": "python"
    },
    {
      "code": "serialize_output_messages(msgs, _info)",
      "language": "unknown"
    },
    {
      "code": "serialize_output_messages(msgs, _info)",
      "language": "unknown"
    },
    {
      "code": "1739\n1740\n1741",
      "language": "unknown"
    },
    {
      "code": "@field_serializer(\"output_messages\", when_used=\"json\")\ndef serialize_output_messages(self, msgs, _info):\n    return serialize_messages(msgs)",
      "language": "python"
    },
    {
      "code": "@field_serializer(\"output_messages\", when_used=\"json\")\ndef serialize_output_messages(self, msgs, _info):\n    return serialize_messages(msgs)",
      "language": "python"
    },
    {
      "code": "255\n256\n257",
      "language": "unknown"
    },
    {
      "code": "class StreamOptions(OpenAIBaseModel):\n    include_usage: bool | None = True\n    continuous_usage_stats: bool | None = False",
      "language": "php"
    },
    {
      "code": "class StreamOptions(OpenAIBaseModel):\n    include_usage: bool | None = True\n    continuous_usage_stats: bool | None = False",
      "language": "php"
    },
    {
      "code": "continuous_usage_stats: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "continuous_usage_stats: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "include_usage: bool | None = True",
      "language": "yaml"
    },
    {
      "code": "include_usage: bool | None = True",
      "language": "yaml"
    },
    {
      "code": "234\n235\n236",
      "language": "unknown"
    },
    {
      "code": "class StructuralTagResponseFormat(OpenAIBaseModel):\n    type: Literal[\"structural_tag\"]\n    format: Any",
      "language": "php"
    },
    {
      "code": "class StructuralTagResponseFormat(OpenAIBaseModel):\n    type: Literal[\"structural_tag\"]\n    format: Any",
      "language": "php"
    },
    {
      "code": "format: Any",
      "language": "yaml"
    },
    {
      "code": "format: Any",
      "language": "yaml"
    },
    {
      "code": "type: Literal['structural_tag']",
      "language": "yaml"
    },
    {
      "code": "type: Literal['structural_tag']",
      "language": "yaml"
    },
    {
      "code": "1903\n1904\n1905\n1906\n1907\n1908\n1909\n1910\n1911\n1912\n1913\n1914\n1915\n1916\n1917\n1918\n1919\n1920\n1921\n1922\n1923\n1924\n1925\n1926\n1927\n1928\n1929\n1930\n1931\n1932\n1933\n1934\n1935\n1936\n1937\n1938\n1939\n1940\n1941\n1942\n1943\n1944\n1945\n1946\n1947\n1948\n1949\n1950\n1951\n1952\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974",
      "language": "unknown"
    },
    {
      "code": "class TokenizeChatRequest(OpenAIBaseModel):\n    model: str | None = None\n    messages: list[ChatCompletionMessageParam]\n\n    add_generation_prompt: bool = Field(\n        default=True,\n        description=(\n            \"If true, the generation prompt will be added to the chat template. \"\n            \"This is a parameter used by chat template in tokenizer config of the \"\n            \"model.\"\n        ),\n    )\n    return_token_strs: bool | None = Field(\n        default=False,\n        description=(\n            \"If true, also return the token strings corresponding to the token ids.\"\n        ),\n    )\n    continue_final_message: bool = Field(\n        default=False,\n        description=(\n            \"If this is set, the chat will be formatted so that the final \"\n            \"message in the chat is open-ended, without any EOS tokens. The \"\n            \"model will continue this message rather than starting a new one. \"\n            'This allows you to \"prefill\" part of the model\\'s response for it. '\n            \"Cannot be used at the same time as `add_generation_prompt`.\"\n        ),\n    )\n    add_special_tokens: bool = Field(\n        default=False,\n        description=(\n            \"If true, special tokens (e.g. BOS) will be added to the prompt \"\n            \"on top of what is added by the chat template. \"\n            \"For most models, the chat template takes care of adding the \"\n            \"special tokens so this should be set to false (as is the \"\n            \"default).\"\n        ),\n    )\n    chat_template: str | None = Field(\n        default=None,\n        description=(\n            \"A Jinja template to use for this conversion. \"\n            \"As of transformers v4.44, default chat template is no longer \"\n            \"allowed, so you must provide a chat template if the tokenizer \"\n            \"does not define one.\"\n        ),\n    )\n    chat_template_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\n            \"Additional keyword args to pass to the template renderer. \"\n            \"Will be accessible by the chat template.\"\n        ),\n    )\n    mm_processor_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the HF processor.\"),\n    )\n    tools: list[ChatCompletionToolsParam] | None = Field(\n        default=None,\n        description=(\"A list of tools the model may call.\"),\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_generation_prompt(cls, data):\n        if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"):\n            raise ValueError(\n                \"Cannot set both `continue_final_message` and \"\n                \"`add_generation_prompt` to True.\"\n            )\n        return data",
      "language": "python"
    },
    {
      "code": "class TokenizeChatRequest(OpenAIBaseModel):\n    model: str | None = None\n    messages: list[ChatCompletionMessageParam]\n\n    add_generation_prompt: bool = Field(\n        default=True,\n        description=(\n            \"If true, the generation prompt will be added to the chat template. \"\n            \"This is a parameter used by chat template in tokenizer config of the \"\n            \"model.\"\n        ),\n    )\n    return_token_strs: bool | None = Field(\n        default=False,\n        description=(\n            \"If true, also return the token strings corresponding to the token ids.\"\n        ),\n    )\n    continue_final_message: bool = Field(\n        default=False,\n        description=(\n            \"If this is set, the chat will be formatted so that the final \"\n            \"message in the chat is open-ended, without any EOS tokens. The \"\n            \"model will continue this message rather than starting a new one. \"\n            'This allows you to \"prefill\" part of the model\\'s response for it. '\n            \"Cannot be used at the same time as `add_generation_prompt`.\"\n        ),\n    )\n    add_special_tokens: bool = Field(\n        default=False,\n        description=(\n            \"If true, special tokens (e.g. BOS) will be added to the prompt \"\n            \"on top of what is added by the chat template. \"\n            \"For most models, the chat template takes care of adding the \"\n            \"special tokens so this should be set to false (as is the \"\n            \"default).\"\n        ),\n    )\n    chat_template: str | None = Field(\n        default=None,\n        description=(\n            \"A Jinja template to use for this conversion. \"\n            \"As of transformers v4.44, default chat template is no longer \"\n            \"allowed, so you must provide a chat template if the tokenizer \"\n            \"does not define one.\"\n        ),\n    )\n    chat_template_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\n            \"Additional keyword args to pass to the template renderer. \"\n            \"Will be accessible by the chat template.\"\n        ),\n    )\n    mm_processor_kwargs: dict[str, Any] | None = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the HF processor.\"),\n    )\n    tools: list[ChatCompletionToolsParam] | None = Field(\n        default=None,\n        description=(\"A list of tools the model may call.\"),\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_generation_prompt(cls, data):\n        if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"):\n            raise ValueError(\n                \"Cannot set both `continue_final_message` and \"\n                \"`add_generation_prompt` to True.\"\n            )\n        return data",
      "language": "python"
    },
    {
      "code": "add_generation_prompt: bool = Field(\n    default=True,\n    description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\",\n)",
      "language": "typescript"
    },
    {
      "code": "add_generation_prompt: bool = Field(\n    default=True,\n    description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\",\n)",
      "language": "typescript"
    },
    {
      "code": "add_special_tokens: bool = Field(\n    default=False,\n    description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\",\n)",
      "language": "typescript"
    },
    {
      "code": "add_special_tokens: bool = Field(\n    default=False,\n    description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\",\n)",
      "language": "typescript"
    },
    {
      "code": "chat_template: str | None = Field(\n    default=None,\n    description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\",\n)",
      "language": "yaml"
    },
    {
      "code": "chat_template: str | None = Field(\n    default=None,\n    description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\",\n)",
      "language": "yaml"
    },
    {
      "code": "chat_template_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\",\n)",
      "language": "csharp"
    },
    {
      "code": "chat_template_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\",\n)",
      "language": "csharp"
    },
    {
      "code": "continue_final_message: bool = Field(\n    default=False,\n    description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.',\n)",
      "language": "typescript"
    },
    {
      "code": "continue_final_message: bool = Field(\n    default=False,\n    description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.',\n)",
      "language": "typescript"
    },
    {
      "code": "messages: list[ChatCompletionMessageParam]",
      "language": "yaml"
    },
    {
      "code": "messages: list[ChatCompletionMessageParam]",
      "language": "yaml"
    },
    {
      "code": "mm_processor_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional kwargs to pass to the HF processor.\",\n)",
      "language": "yaml"
    },
    {
      "code": "mm_processor_kwargs: dict[str, Any] | None = Field(\n    default=None,\n    description=\"Additional kwargs to pass to the HF processor.\",\n)",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "return_token_strs: bool | None = Field(\n    default=False,\n    description=\"If true, also return the token strings corresponding to the token ids.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_token_strs: bool | None = Field(\n    default=False,\n    description=\"If true, also return the token strings corresponding to the token ids.\",\n)",
      "language": "yaml"
    },
    {
      "code": "tools: list[ChatCompletionToolsParam] | None = Field(\n    default=None,\n    description=\"A list of tools the model may call.\",\n)",
      "language": "yaml"
    },
    {
      "code": "tools: list[ChatCompletionToolsParam] | None = Field(\n    default=None,\n    description=\"A list of tools the model may call.\",\n)",
      "language": "yaml"
    },
    {
      "code": "check_generation_prompt(data)",
      "language": "unknown"
    },
    {
      "code": "check_generation_prompt(data)",
      "language": "unknown"
    },
    {
      "code": "1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_generation_prompt(cls, data):\n    if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"):\n        raise ValueError(\n            \"Cannot set both `continue_final_message` and \"\n            \"`add_generation_prompt` to True.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef check_generation_prompt(cls, data):\n    if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"):\n        raise ValueError(\n            \"Cannot set both `continue_final_message` and \"\n            \"`add_generation_prompt` to True.\"\n        )\n    return data",
      "language": "python"
    },
    {
      "code": "1884\n1885\n1886\n1887\n1888\n1889\n1890\n1891\n1892\n1893\n1894\n1895\n1896\n1897\n1898\n1899\n1900",
      "language": "unknown"
    },
    {
      "code": "class TokenizeCompletionRequest(OpenAIBaseModel):\n    model: str | None = None\n    prompt: str\n\n    add_special_tokens: bool = Field(\n        default=True,\n        description=(\n            \"If true (the default), special tokens (e.g. BOS) will be added to \"\n            \"the prompt.\"\n        ),\n    )\n    return_token_strs: bool | None = Field(\n        default=False,\n        description=(\n            \"If true, also return the token strings corresponding to the token ids.\"\n        ),\n    )",
      "language": "typescript"
    },
    {
      "code": "class TokenizeCompletionRequest(OpenAIBaseModel):\n    model: str | None = None\n    prompt: str\n\n    add_special_tokens: bool = Field(\n        default=True,\n        description=(\n            \"If true (the default), special tokens (e.g. BOS) will be added to \"\n            \"the prompt.\"\n        ),\n    )\n    return_token_strs: bool | None = Field(\n        default=False,\n        description=(\n            \"If true, also return the token strings corresponding to the token ids.\"\n        ),\n    )",
      "language": "typescript"
    },
    {
      "code": "add_special_tokens: bool = Field(\n    default=True,\n    description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\",\n)",
      "language": "typescript"
    },
    {
      "code": "add_special_tokens: bool = Field(\n    default=True,\n    description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\",\n)",
      "language": "typescript"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt: str",
      "language": "yaml"
    },
    {
      "code": "prompt: str",
      "language": "yaml"
    },
    {
      "code": "return_token_strs: bool | None = Field(\n    default=False,\n    description=\"If true, also return the token strings corresponding to the token ids.\",\n)",
      "language": "yaml"
    },
    {
      "code": "return_token_strs: bool | None = Field(\n    default=False,\n    description=\"If true, also return the token strings corresponding to the token ids.\",\n)",
      "language": "yaml"
    },
    {
      "code": "1980\n1981\n1982\n1983\n1984",
      "language": "unknown"
    },
    {
      "code": "class TokenizeResponse(OpenAIBaseModel):\n    count: int\n    max_model_len: int\n    tokens: list[int]\n    token_strs: list[str] | None = None",
      "language": "php"
    },
    {
      "code": "class TokenizeResponse(OpenAIBaseModel):\n    count: int\n    max_model_len: int\n    tokens: list[int]\n    token_strs: list[str] | None = None",
      "language": "php"
    },
    {
      "code": "max_model_len: int",
      "language": "yaml"
    },
    {
      "code": "max_model_len: int",
      "language": "yaml"
    },
    {
      "code": "token_strs: list[str] | None = None",
      "language": "yaml"
    },
    {
      "code": "token_strs: list[str] | None = None",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003",
      "language": "unknown"
    },
    {
      "code": "class TokenizerInfoResponse(OpenAIBaseModel):\n    \"\"\"\n    Response containing tokenizer configuration\n    equivalent to tokenizer_config.json\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n    tokenizer_class: str",
      "language": "php"
    },
    {
      "code": "class TokenizerInfoResponse(OpenAIBaseModel):\n    \"\"\"\n    Response containing tokenizer configuration\n    equivalent to tokenizer_config.json\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n    tokenizer_class: str",
      "language": "php"
    },
    {
      "code": "model_config = ConfigDict(extra='allow')",
      "language": "unknown"
    },
    {
      "code": "model_config = ConfigDict(extra='allow')",
      "language": "unknown"
    },
    {
      "code": "tokenizer_class: str",
      "language": "yaml"
    },
    {
      "code": "tokenizer_class: str",
      "language": "yaml"
    },
    {
      "code": "1483\n1484\n1485\n1486",
      "language": "unknown"
    },
    {
      "code": "class ToolCall(OpenAIBaseModel):\n    id: str = Field(default_factory=make_tool_call_id)\n    type: Literal[\"function\"] = \"function\"\n    function: FunctionCall",
      "language": "typescript"
    },
    {
      "code": "class ToolCall(OpenAIBaseModel):\n    id: str = Field(default_factory=make_tool_call_id)\n    type: Literal[\"function\"] = \"function\"\n    function: FunctionCall",
      "language": "typescript"
    },
    {
      "code": "function: FunctionCall",
      "language": "yaml"
    },
    {
      "code": "function: FunctionCall",
      "language": "yaml"
    },
    {
      "code": "id: str = Field(default_factory=make_tool_call_id)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(default_factory=make_tool_call_id)",
      "language": "typescript"
    },
    {
      "code": "type: Literal['function'] = 'function'",
      "language": "yaml"
    },
    {
      "code": "type: Literal['function'] = 'function'",
      "language": "yaml"
    },
    {
      "code": "2020\n2021\n2022\n2023\n2024\n2025\n2026\n2027\n2028\n2029\n2030\n2031\n2032\n2033\n2034\n2035\n2036\n2037\n2038\n2039\n2040\n2041\n2042\n2043\n2044\n2045\n2046\n2047\n2048\n2049\n2050\n2051\n2052\n2053\n2054\n2055\n2056\n2057\n2058\n2059\n2060\n2061\n2062\n2063\n2064\n2065\n2066\n2067\n2068\n2069\n2070\n2071\n2072\n2073\n2074\n2075\n2076\n2077\n2078\n2079\n2080\n2081\n2082\n2083\n2084\n2085\n2086\n2087\n2088\n2089\n2090\n2091\n2092\n2093\n2094\n2095\n2096\n2097\n2098\n2099\n2100\n2101\n2102\n2103\n2104\n2105\n2106\n2107\n2108\n2109\n2110\n2111\n2112\n2113\n2114\n2115\n2116\n2117\n2118\n2119\n2120\n2121\n2122\n2123\n2124\n2125\n2126\n2127\n2128\n2129\n2130\n2131\n2132\n2133\n2134\n2135\n2136\n2137\n2138\n2139\n2140\n2141\n2142\n2143\n2144\n2145\n2146\n2147\n2148\n2149\n2150\n2151\n2152\n2153\n2154\n2155\n2156\n2157\n2158\n2159\n2160\n2161\n2162\n2163\n2164\n2165\n2166\n2167\n2168\n2169\n2170\n2171\n2172\n2173\n2174\n2175\n2176\n2177\n2178\n2179\n2180\n2181\n2182\n2183\n2184\n2185\n2186\n2187\n2188\n2189\n2190\n2191\n2192\n2193\n2194\n2195\n2196\n2197\n2198\n2199\n2200\n2201\n2202\n2203\n2204\n2205\n2206\n2207\n2208\n2209\n2210\n2211\n2212\n2213",
      "language": "unknown"
    },
    {
      "code": "class TranscriptionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/audio/createTranscription\n\n    file: UploadFile\n    \"\"\"\n    The audio file object (not file name) to transcribe, in one of these\n    formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n    \"\"\"\n\n    model: str | None = None\n    \"\"\"ID of the model to use.\n    \"\"\"\n\n    language: str | None = None\n    \"\"\"The language of the input audio.\n\n    Supplying the input language in\n    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format\n    will improve accuracy and latency.\n    \"\"\"\n\n    prompt: str = Field(default=\"\")\n    \"\"\"An optional text to guide the model's style or continue a previous audio\n    segment.\n\n    The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n    should match the audio language.\n    \"\"\"\n\n    response_format: AudioResponseFormat = Field(default=\"json\")\n    \"\"\"\n    The format of the output, in one of these options: `json`, `text`, `srt`,\n    `verbose_json`, or `vtt`.\n    \"\"\"\n\n    ## TODO (varun) : Support if set to 0, certain thresholds are met !!\n\n    timestamp_granularities: list[Literal[\"word\", \"segment\"]] = Field(\n        alias=\"timestamp_granularities[]\", default=[]\n    )\n    \"\"\"The timestamp granularities to populate for this transcription.\n\n    `response_format` must be set `verbose_json` to use timestamp granularities.\n    Either or both of these options are supported: `word`, or `segment`. Note:\n    There is no additional latency for segment timestamps, but generating word\n    timestamps incurs additional latency.\n    \"\"\"\n\n    stream: bool | None = False\n    \"\"\"When set, it will enable output to be streamed in a similar fashion\n    as the Chat Completion endpoint.\n    \"\"\"\n    # --8<-- [start:transcription-extra-params]\n    # Flattened stream option to simplify form data.\n    stream_include_usage: bool | None = False\n    stream_continuous_usage_stats: bool | None = False\n\n    vllm_xargs: dict[str, str | int | float] | None = Field(\n        default=None,\n        description=(\n            \"Additional request parameters with string or \"\n            \"numeric values, used by custom extensions.\"\n        ),\n    )\n    # --8<-- [end:transcription-extra-params]\n\n    to_language: str | None = None\n    \"\"\"The language of the output audio we transcribe to.\n\n    Please note that this is not currently used by supported models at this\n    time, but it is a placeholder for future use, matching translation api.\n    \"\"\"\n\n    # --8<-- [start:transcription-sampling-params]\n    temperature: float = Field(default=0.0)\n    \"\"\"The sampling temperature, between 0 and 1.\n\n    Higher values like 0.8 will make the output more random, while lower values\n    like 0.2 will make it more focused / deterministic. If set to 0, the model\n    will use [log probability](https://en.wikipedia.org/wiki/Log_probability)\n    to automatically increase the temperature until certain thresholds are hit.\n    \"\"\"\n\n    top_p: float | None = None\n    \"\"\"Enables nucleus (top-p) sampling, where tokens are selected from the\n    smallest possible set whose cumulative probability exceeds `p`.\n    \"\"\"\n\n    top_k: int | None = None\n    \"\"\"Limits sampling to the `k` most probable tokens at each step.\"\"\"\n\n    min_p: float | None = None\n    \"\"\"Filters out tokens with a probability lower than `min_p`, ensuring a\n    minimum likelihood threshold during sampling.\n    \"\"\"\n\n    seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    \"\"\"The seed to use for sampling.\"\"\"\n\n    frequency_penalty: float | None = 0.0\n    \"\"\"The frequency penalty to use for sampling.\"\"\"\n\n    repetition_penalty: float | None = None\n    \"\"\"The repetition penalty to use for sampling.\"\"\"\n\n    presence_penalty: float | None = 0.0\n    \"\"\"The presence penalty to use for sampling.\"\"\"\n\n    max_completion_tokens: int | None = None\n    \"\"\"The maximum number of tokens to generate.\"\"\"\n    # --8<-- [end:transcription-sampling-params]\n\n    # Default sampling parameters for transcription requests.\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"repetition_penalty\": 1.0,\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": 0,\n        \"min_p\": 0.0,\n    }\n\n    def to_sampling_params(\n        self, default_max_tokens: int, default_sampling_params: dict | None = None\n    ) -> SamplingParams:\n        max_tokens = default_max_tokens\n\n        if default_sampling_params is None:\n            default_sampling_params = {}\n\n        # Default parameters\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n            )\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n            )\n        if (min_p := self.min_p) is None:\n            min_p = default_sampling_params.get(\n                \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n            )\n\n        if (repetition_penalty := self.repetition_penalty) is None:\n            repetition_penalty = default_sampling_params.get(\n                \"repetition_penalty\",\n                self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n            )\n\n        return SamplingParams.from_optional(\n            temperature=temperature,\n            max_tokens=max_tokens,\n            seed=self.seed,\n            top_p=top_p,\n            top_k=top_k,\n            min_p=min_p,\n            frequency_penalty=self.frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            presence_penalty=self.presence_penalty,\n            output_kind=RequestOutputKind.DELTA\n            if self.stream\n            else RequestOutputKind.FINAL_ONLY,\n            extra_args=self.vllm_xargs,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_transcription_request(cls, data):\n        if isinstance(data.get(\"file\"), str):\n            raise HTTPException(\n                status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n                detail=\"Expected 'file' to be a file-like object, not 'str'.\",\n            )\n\n        stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"]\n        stream = data.get(\"stream\", False)\n        if any(bool(data.get(so, False)) for so in stream_opts) and not stream:\n            # Find which specific stream option was set\n            invalid_param = next(\n                (so for so in stream_opts if data.get(so, False)),\n                \"stream_include_usage\",\n            )\n            raise VLLMValidationError(\n                \"Stream options can only be defined when `stream=True`.\",\n                parameter=invalid_param,\n            )\n\n        return data",
      "language": "python"
    },
    {
      "code": "class TranscriptionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/audio/createTranscription\n\n    file: UploadFile\n    \"\"\"\n    The audio file object (not file name) to transcribe, in one of these\n    formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n    \"\"\"\n\n    model: str | None = None\n    \"\"\"ID of the model to use.\n    \"\"\"\n\n    language: str | None = None\n    \"\"\"The language of the input audio.\n\n    Supplying the input language in\n    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format\n    will improve accuracy and latency.\n    \"\"\"\n\n    prompt: str = Field(default=\"\")\n    \"\"\"An optional text to guide the model's style or continue a previous audio\n    segment.\n\n    The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n    should match the audio language.\n    \"\"\"\n\n    response_format: AudioResponseFormat = Field(default=\"json\")\n    \"\"\"\n    The format of the output, in one of these options: `json`, `text`, `srt`,\n    `verbose_json`, or `vtt`.\n    \"\"\"\n\n    ## TODO (varun) : Support if set to 0, certain thresholds are met !!\n\n    timestamp_granularities: list[Literal[\"word\", \"segment\"]] = Field(\n        alias=\"timestamp_granularities[]\", default=[]\n    )\n    \"\"\"The timestamp granularities to populate for this transcription.\n\n    `response_format` must be set `verbose_json` to use timestamp granularities.\n    Either or both of these options are supported: `word`, or `segment`. Note:\n    There is no additional latency for segment timestamps, but generating word\n    timestamps incurs additional latency.\n    \"\"\"\n\n    stream: bool | None = False\n    \"\"\"When set, it will enable output to be streamed in a similar fashion\n    as the Chat Completion endpoint.\n    \"\"\"\n    # --8<-- [start:transcription-extra-params]\n    # Flattened stream option to simplify form data.\n    stream_include_usage: bool | None = False\n    stream_continuous_usage_stats: bool | None = False\n\n    vllm_xargs: dict[str, str | int | float] | None = Field(\n        default=None,\n        description=(\n            \"Additional request parameters with string or \"\n            \"numeric values, used by custom extensions.\"\n        ),\n    )\n    # --8<-- [end:transcription-extra-params]\n\n    to_language: str | None = None\n    \"\"\"The language of the output audio we transcribe to.\n\n    Please note that this is not currently used by supported models at this\n    time, but it is a placeholder for future use, matching translation api.\n    \"\"\"\n\n    # --8<-- [start:transcription-sampling-params]\n    temperature: float = Field(default=0.0)\n    \"\"\"The sampling temperature, between 0 and 1.\n\n    Higher values like 0.8 will make the output more random, while lower values\n    like 0.2 will make it more focused / deterministic. If set to 0, the model\n    will use [log probability](https://en.wikipedia.org/wiki/Log_probability)\n    to automatically increase the temperature until certain thresholds are hit.\n    \"\"\"\n\n    top_p: float | None = None\n    \"\"\"Enables nucleus (top-p) sampling, where tokens are selected from the\n    smallest possible set whose cumulative probability exceeds `p`.\n    \"\"\"\n\n    top_k: int | None = None\n    \"\"\"Limits sampling to the `k` most probable tokens at each step.\"\"\"\n\n    min_p: float | None = None\n    \"\"\"Filters out tokens with a probability lower than `min_p`, ensuring a\n    minimum likelihood threshold during sampling.\n    \"\"\"\n\n    seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    \"\"\"The seed to use for sampling.\"\"\"\n\n    frequency_penalty: float | None = 0.0\n    \"\"\"The frequency penalty to use for sampling.\"\"\"\n\n    repetition_penalty: float | None = None\n    \"\"\"The repetition penalty to use for sampling.\"\"\"\n\n    presence_penalty: float | None = 0.0\n    \"\"\"The presence penalty to use for sampling.\"\"\"\n\n    max_completion_tokens: int | None = None\n    \"\"\"The maximum number of tokens to generate.\"\"\"\n    # --8<-- [end:transcription-sampling-params]\n\n    # Default sampling parameters for transcription requests.\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"repetition_penalty\": 1.0,\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": 0,\n        \"min_p\": 0.0,\n    }\n\n    def to_sampling_params(\n        self, default_max_tokens: int, default_sampling_params: dict | None = None\n    ) -> SamplingParams:\n        max_tokens = default_max_tokens\n\n        if default_sampling_params is None:\n            default_sampling_params = {}\n\n        # Default parameters\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n            )\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n            )\n        if (min_p := self.min_p) is None:\n            min_p = default_sampling_params.get(\n                \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n            )\n\n        if (repetition_penalty := self.repetition_penalty) is None:\n            repetition_penalty = default_sampling_params.get(\n                \"repetition_penalty\",\n                self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n            )\n\n        return SamplingParams.from_optional(\n            temperature=temperature,\n            max_tokens=max_tokens,\n            seed=self.seed,\n            top_p=top_p,\n            top_k=top_k,\n            min_p=min_p,\n            frequency_penalty=self.frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            presence_penalty=self.presence_penalty,\n            output_kind=RequestOutputKind.DELTA\n            if self.stream\n            else RequestOutputKind.FINAL_ONLY,\n            extra_args=self.vllm_xargs,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_transcription_request(cls, data):\n        if isinstance(data.get(\"file\"), str):\n            raise HTTPException(\n                status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n                detail=\"Expected 'file' to be a file-like object, not 'str'.\",\n            )\n\n        stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"]\n        stream = data.get(\"stream\", False)\n        if any(bool(data.get(so, False)) for so in stream_opts) and not stream:\n            # Find which specific stream option was set\n            invalid_param = next(\n                (so for so in stream_opts if data.get(so, False)),\n                \"stream_include_usage\",\n            )\n            raise VLLMValidationError(\n                \"Stream options can only be defined when `stream=True`.\",\n                parameter=invalid_param,\n            )\n\n        return data",
      "language": "python"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS: dict = {\n    \"repetition_penalty\": 1.0,\n    \"temperature\": 1.0,\n    \"top_p\": 1.0,\n    \"top_k\": 0,\n    \"min_p\": 0.0,\n}",
      "language": "json"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS: dict = {\n    \"repetition_penalty\": 1.0,\n    \"temperature\": 1.0,\n    \"top_p\": 1.0,\n    \"top_k\": 0,\n    \"min_p\": 0.0,\n}",
      "language": "json"
    },
    {
      "code": "file: UploadFile",
      "language": "yaml"
    },
    {
      "code": "file: UploadFile",
      "language": "yaml"
    },
    {
      "code": "frequency_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "frequency_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "language: str | None = None",
      "language": "yaml"
    },
    {
      "code": "language: str | None = None",
      "language": "yaml"
    },
    {
      "code": "max_completion_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_completion_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "min_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "min_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "presence_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "presence_penalty: float | None = 0.0",
      "language": "yaml"
    },
    {
      "code": "prompt: str = Field(default='')",
      "language": "typescript"
    },
    {
      "code": "prompt: str = Field(default='')",
      "language": "typescript"
    },
    {
      "code": "repetition_penalty: float | None = None",
      "language": "yaml"
    },
    {
      "code": "repetition_penalty: float | None = None",
      "language": "yaml"
    },
    {
      "code": "response_format: AudioResponseFormat = Field(default=\"json\")",
      "language": "typescript"
    },
    {
      "code": "response_format: AudioResponseFormat = Field(default=\"json\")",
      "language": "typescript"
    },
    {
      "code": "seed: int | None = Field(None, ge=min, le=max)",
      "language": "yaml"
    },
    {
      "code": "seed: int | None = Field(None, ge=min, le=max)",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_continuous_usage_stats: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_continuous_usage_stats: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_include_usage: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_include_usage: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "temperature: float = Field(default=0.0)",
      "language": "typescript"
    },
    {
      "code": "temperature: float = Field(default=0.0)",
      "language": "typescript"
    },
    {
      "code": "timestamp_granularities: list[\n    Literal[\"word\", \"segment\"]\n] = Field(alias=\"timestamp_granularities[]\", default=[])",
      "language": "yaml"
    },
    {
      "code": "timestamp_granularities: list[\n    Literal[\"word\", \"segment\"]\n] = Field(alias=\"timestamp_granularities[]\", default=[])",
      "language": "yaml"
    },
    {
      "code": "to_language: str | None = None",
      "language": "yaml"
    },
    {
      "code": "to_language: str | None = None",
      "language": "yaml"
    },
    {
      "code": "top_k: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_k: int | None = None",
      "language": "yaml"
    },
    {
      "code": "top_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "top_p: float | None = None",
      "language": "yaml"
    },
    {
      "code": "vllm_xargs: dict[str, str | int | float] | None = Field(\n    default=None,\n    description=\"Additional request parameters with string or numeric values, used by custom extensions.\",\n)",
      "language": "yaml"
    },
    {
      "code": "vllm_xargs: dict[str, str | int | float] | None = Field(\n    default=None,\n    description=\"Additional request parameters with string or numeric values, used by custom extensions.\",\n)",
      "language": "yaml"
    },
    {
      "code": "to_sampling_params(\n    default_max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "to_sampling_params(\n    default_max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "2142\n2143\n2144\n2145\n2146\n2147\n2148\n2149\n2150\n2151\n2152\n2153\n2154\n2155\n2156\n2157\n2158\n2159\n2160\n2161\n2162\n2163\n2164\n2165\n2166\n2167\n2168\n2169\n2170\n2171\n2172\n2173\n2174\n2175\n2176\n2177\n2178\n2179\n2180\n2181\n2182\n2183\n2184\n2185\n2186\n2187\n2188\n2189",
      "language": "unknown"
    },
    {
      "code": "def to_sampling_params(\n    self, default_max_tokens: int, default_sampling_params: dict | None = None\n) -> SamplingParams:\n    max_tokens = default_max_tokens\n\n    if default_sampling_params is None:\n        default_sampling_params = {}\n\n    # Default parameters\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n    if (top_p := self.top_p) is None:\n        top_p = default_sampling_params.get(\n            \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n        )\n    if (top_k := self.top_k) is None:\n        top_k = default_sampling_params.get(\n            \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n        )\n    if (min_p := self.min_p) is None:\n        min_p = default_sampling_params.get(\n            \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n        )\n\n    if (repetition_penalty := self.repetition_penalty) is None:\n        repetition_penalty = default_sampling_params.get(\n            \"repetition_penalty\",\n            self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n        )\n\n    return SamplingParams.from_optional(\n        temperature=temperature,\n        max_tokens=max_tokens,\n        seed=self.seed,\n        top_p=top_p,\n        top_k=top_k,\n        min_p=min_p,\n        frequency_penalty=self.frequency_penalty,\n        repetition_penalty=repetition_penalty,\n        presence_penalty=self.presence_penalty,\n        output_kind=RequestOutputKind.DELTA\n        if self.stream\n        else RequestOutputKind.FINAL_ONLY,\n        extra_args=self.vllm_xargs,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "def to_sampling_params(\n    self, default_max_tokens: int, default_sampling_params: dict | None = None\n) -> SamplingParams:\n    max_tokens = default_max_tokens\n\n    if default_sampling_params is None:\n        default_sampling_params = {}\n\n    # Default parameters\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n    if (top_p := self.top_p) is None:\n        top_p = default_sampling_params.get(\n            \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"]\n        )\n    if (top_k := self.top_k) is None:\n        top_k = default_sampling_params.get(\n            \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"]\n        )\n    if (min_p := self.min_p) is None:\n        min_p = default_sampling_params.get(\n            \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"]\n        )\n\n    if (repetition_penalty := self.repetition_penalty) is None:\n        repetition_penalty = default_sampling_params.get(\n            \"repetition_penalty\",\n            self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n        )\n\n    return SamplingParams.from_optional(\n        temperature=temperature,\n        max_tokens=max_tokens,\n        seed=self.seed,\n        top_p=top_p,\n        top_k=top_k,\n        min_p=min_p,\n        frequency_penalty=self.frequency_penalty,\n        repetition_penalty=repetition_penalty,\n        presence_penalty=self.presence_penalty,\n        output_kind=RequestOutputKind.DELTA\n        if self.stream\n        else RequestOutputKind.FINAL_ONLY,\n        extra_args=self.vllm_xargs,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "validate_transcription_request(data)",
      "language": "unknown"
    },
    {
      "code": "validate_transcription_request(data)",
      "language": "unknown"
    },
    {
      "code": "2191\n2192\n2193\n2194\n2195\n2196\n2197\n2198\n2199\n2200\n2201\n2202\n2203\n2204\n2205\n2206\n2207\n2208\n2209\n2210\n2211\n2212\n2213",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_transcription_request(cls, data):\n    if isinstance(data.get(\"file\"), str):\n        raise HTTPException(\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n            detail=\"Expected 'file' to be a file-like object, not 'str'.\",\n        )\n\n    stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"]\n    stream = data.get(\"stream\", False)\n    if any(bool(data.get(so, False)) for so in stream_opts) and not stream:\n        # Find which specific stream option was set\n        invalid_param = next(\n            (so for so in stream_opts if data.get(so, False)),\n            \"stream_include_usage\",\n        )\n        raise VLLMValidationError(\n            \"Stream options can only be defined when `stream=True`.\",\n            parameter=invalid_param,\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_transcription_request(cls, data):\n    if isinstance(data.get(\"file\"), str):\n        raise HTTPException(\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n            detail=\"Expected 'file' to be a file-like object, not 'str'.\",\n        )\n\n    stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"]\n    stream = data.get(\"stream\", False)\n    if any(bool(data.get(so, False)) for so in stream_opts) and not stream:\n        # Find which specific stream option was set\n        invalid_param = next(\n            (so for so in stream_opts if data.get(so, False)),\n            \"stream_include_usage\",\n        )\n        raise VLLMValidationError(\n            \"Stream options can only be defined when `stream=True`.\",\n            parameter=invalid_param,\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "2222\n2223\n2224\n2225",
      "language": "unknown"
    },
    {
      "code": "class TranscriptionResponse(OpenAIBaseModel):\n    text: str\n    \"\"\"The transcribed text.\"\"\"\n    usage: TranscriptionUsageAudio",
      "language": "php"
    },
    {
      "code": "class TranscriptionResponse(OpenAIBaseModel):\n    text: str\n    \"\"\"The transcribed text.\"\"\"\n    usage: TranscriptionUsageAudio",
      "language": "php"
    },
    {
      "code": "usage: TranscriptionUsageAudio",
      "language": "yaml"
    },
    {
      "code": "usage: TranscriptionUsageAudio",
      "language": "yaml"
    },
    {
      "code": "1619\n1620\n1621\n1622",
      "language": "unknown"
    },
    {
      "code": "class TranscriptionResponseStreamChoice(OpenAIBaseModel):\n    delta: DeltaMessage\n    finish_reason: str | None = None\n    stop_reason: int | str | None = None",
      "language": "php"
    },
    {
      "code": "class TranscriptionResponseStreamChoice(OpenAIBaseModel):\n    delta: DeltaMessage\n    finish_reason: str | None = None\n    stop_reason: int | str | None = None",
      "language": "php"
    },
    {
      "code": "delta: DeltaMessage",
      "language": "yaml"
    },
    {
      "code": "delta: DeltaMessage",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = None",
      "language": "yaml"
    },
    {
      "code": "2281\n2282\n2283\n2284\n2285\n2286\n2287\n2288\n2289\n2290\n2291\n2292\n2293\n2294\n2295",
      "language": "unknown"
    },
    {
      "code": "class TranscriptionResponseVerbose(OpenAIBaseModel):\n    duration: str\n    \"\"\"The duration of the input audio.\"\"\"\n\n    language: str\n    \"\"\"The language of the input audio.\"\"\"\n\n    text: str\n    \"\"\"The transcribed text.\"\"\"\n\n    segments: list[TranscriptionSegment] | None = None\n    \"\"\"Segments of the transcribed text and their corresponding details.\"\"\"\n\n    words: list[TranscriptionWord] | None = None\n    \"\"\"Extracted words and their corresponding timestamps.\"\"\"",
      "language": "php"
    },
    {
      "code": "class TranscriptionResponseVerbose(OpenAIBaseModel):\n    duration: str\n    \"\"\"The duration of the input audio.\"\"\"\n\n    language: str\n    \"\"\"The language of the input audio.\"\"\"\n\n    text: str\n    \"\"\"The transcribed text.\"\"\"\n\n    segments: list[TranscriptionSegment] | None = None\n    \"\"\"Segments of the transcribed text and their corresponding details.\"\"\"\n\n    words: list[TranscriptionWord] | None = None\n    \"\"\"Extracted words and their corresponding timestamps.\"\"\"",
      "language": "php"
    },
    {
      "code": "duration: str",
      "language": "yaml"
    },
    {
      "code": "duration: str",
      "language": "yaml"
    },
    {
      "code": "language: str",
      "language": "yaml"
    },
    {
      "code": "language: str",
      "language": "yaml"
    },
    {
      "code": "segments: list[TranscriptionSegment] | None = None",
      "language": "yaml"
    },
    {
      "code": "segments: list[TranscriptionSegment] | None = None",
      "language": "yaml"
    },
    {
      "code": "words: list[TranscriptionWord] | None = None",
      "language": "yaml"
    },
    {
      "code": "words: list[TranscriptionWord] | None = None",
      "language": "yaml"
    },
    {
      "code": "2239\n2240\n2241\n2242\n2243\n2244\n2245\n2246\n2247\n2248\n2249\n2250\n2251\n2252\n2253\n2254\n2255\n2256\n2257\n2258\n2259\n2260\n2261\n2262\n2263\n2264\n2265\n2266\n2267\n2268\n2269\n2270\n2271\n2272\n2273\n2274\n2275\n2276\n2277\n2278",
      "language": "unknown"
    },
    {
      "code": "class TranscriptionSegment(OpenAIBaseModel):\n    id: int\n    \"\"\"Unique identifier of the segment.\"\"\"\n\n    avg_logprob: float | None = None\n    \"\"\"Average logprob of the segment.\n\n    If the value is lower than -1, consider the logprobs failed.\n    \"\"\"\n\n    compression_ratio: float | None = None\n    \"\"\"Compression ratio of the segment.\n\n    If the value is greater than 2.4, consider the compression failed.\n    \"\"\"\n\n    end: float\n    \"\"\"End time of the segment in seconds.\"\"\"\n\n    no_speech_prob: float | None = None\n    \"\"\"Probability of no speech in the segment.\n\n    If the value is higher than 1.0 and the `avg_logprob` is below -1, consider\n    this segment silent.\n    \"\"\"\n\n    seek: int\n    \"\"\"Seek offset of the segment.\"\"\"\n\n    start: float\n    \"\"\"Start time of the segment in seconds.\"\"\"\n\n    temperature: float\n    \"\"\"Temperature parameter used for generating the segment.\"\"\"\n\n    text: str\n    \"\"\"Text content of the segment.\"\"\"\n\n    tokens: list[int]\n    \"\"\"Array of token IDs for the text content.\"\"\"",
      "language": "php"
    },
    {
      "code": "class TranscriptionSegment(OpenAIBaseModel):\n    id: int\n    \"\"\"Unique identifier of the segment.\"\"\"\n\n    avg_logprob: float | None = None\n    \"\"\"Average logprob of the segment.\n\n    If the value is lower than -1, consider the logprobs failed.\n    \"\"\"\n\n    compression_ratio: float | None = None\n    \"\"\"Compression ratio of the segment.\n\n    If the value is greater than 2.4, consider the compression failed.\n    \"\"\"\n\n    end: float\n    \"\"\"End time of the segment in seconds.\"\"\"\n\n    no_speech_prob: float | None = None\n    \"\"\"Probability of no speech in the segment.\n\n    If the value is higher than 1.0 and the `avg_logprob` is below -1, consider\n    this segment silent.\n    \"\"\"\n\n    seek: int\n    \"\"\"Seek offset of the segment.\"\"\"\n\n    start: float\n    \"\"\"Start time of the segment in seconds.\"\"\"\n\n    temperature: float\n    \"\"\"Temperature parameter used for generating the segment.\"\"\"\n\n    text: str\n    \"\"\"Text content of the segment.\"\"\"\n\n    tokens: list[int]\n    \"\"\"Array of token IDs for the text content.\"\"\"",
      "language": "php"
    },
    {
      "code": "avg_logprob: float | None = None",
      "language": "yaml"
    },
    {
      "code": "avg_logprob: float | None = None",
      "language": "yaml"
    },
    {
      "code": "compression_ratio: float | None = None",
      "language": "yaml"
    },
    {
      "code": "compression_ratio: float | None = None",
      "language": "yaml"
    },
    {
      "code": "no_speech_prob: float | None = None",
      "language": "yaml"
    },
    {
      "code": "no_speech_prob: float | None = None",
      "language": "yaml"
    },
    {
      "code": "start: float",
      "language": "yaml"
    },
    {
      "code": "start: float",
      "language": "yaml"
    },
    {
      "code": "temperature: float",
      "language": "yaml"
    },
    {
      "code": "temperature: float",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "1625\n1626\n1627\n1628\n1629\n1630\n1631",
      "language": "unknown"
    },
    {
      "code": "class TranscriptionStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"trsc-{random_uuid()}\")\n    object: Literal[\"transcription.chunk\"] = \"transcription.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[TranscriptionResponseStreamChoice]\n    usage: UsageInfo | None = Field(default=None)",
      "language": "typescript"
    },
    {
      "code": "class TranscriptionStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"trsc-{random_uuid()}\")\n    object: Literal[\"transcription.chunk\"] = \"transcription.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[TranscriptionResponseStreamChoice]\n    usage: UsageInfo | None = Field(default=None)",
      "language": "typescript"
    },
    {
      "code": "choices: list[TranscriptionResponseStreamChoice]",
      "language": "yaml"
    },
    {
      "code": "choices: list[TranscriptionResponseStreamChoice]",
      "language": "yaml"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"trsc-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"trsc-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "object: Literal[\"transcription.chunk\"] = (\n    \"transcription.chunk\"\n)",
      "language": "yaml"
    },
    {
      "code": "object: Literal[\"transcription.chunk\"] = (\n    \"transcription.chunk\"\n)",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "2217\n2218\n2219",
      "language": "unknown"
    },
    {
      "code": "class TranscriptionUsageAudio(OpenAIBaseModel):\n    type: Literal[\"duration\"] = \"duration\"\n    seconds: int",
      "language": "php"
    },
    {
      "code": "class TranscriptionUsageAudio(OpenAIBaseModel):\n    type: Literal[\"duration\"] = \"duration\"\n    seconds: int",
      "language": "php"
    },
    {
      "code": "seconds: int",
      "language": "yaml"
    },
    {
      "code": "seconds: int",
      "language": "yaml"
    },
    {
      "code": "type: Literal['duration'] = 'duration'",
      "language": "yaml"
    },
    {
      "code": "type: Literal['duration'] = 'duration'",
      "language": "yaml"
    },
    {
      "code": "2228\n2229\n2230\n2231\n2232\n2233\n2234\n2235\n2236",
      "language": "unknown"
    },
    {
      "code": "class TranscriptionWord(OpenAIBaseModel):\n    end: float\n    \"\"\"End time of the word in seconds.\"\"\"\n\n    start: float\n    \"\"\"Start time of the word in seconds.\"\"\"\n\n    word: str\n    \"\"\"The text content of the word.\"\"\"",
      "language": "php"
    },
    {
      "code": "class TranscriptionWord(OpenAIBaseModel):\n    end: float\n    \"\"\"End time of the word in seconds.\"\"\"\n\n    start: float\n    \"\"\"Start time of the word in seconds.\"\"\"\n\n    word: str\n    \"\"\"The text content of the word.\"\"\"",
      "language": "php"
    },
    {
      "code": "start: float",
      "language": "yaml"
    },
    {
      "code": "start: float",
      "language": "yaml"
    },
    {
      "code": "2318\n2319\n2320\n2321\n2322\n2323\n2324\n2325\n2326\n2327\n2328\n2329\n2330\n2331\n2332\n2333\n2334\n2335\n2336\n2337\n2338\n2339\n2340\n2341\n2342\n2343\n2344\n2345\n2346\n2347\n2348\n2349\n2350\n2351\n2352\n2353\n2354\n2355\n2356\n2357\n2358\n2359\n2360\n2361\n2362\n2363\n2364\n2365\n2366\n2367\n2368\n2369\n2370\n2371\n2372\n2373\n2374\n2375\n2376\n2377\n2378\n2379\n2380\n2381\n2382\n2383\n2384\n2385\n2386\n2387\n2388\n2389\n2390\n2391\n2392\n2393\n2394\n2395\n2396\n2397\n2398\n2399\n2400\n2401\n2402\n2403\n2404\n2405\n2406\n2407\n2408\n2409\n2410\n2411\n2412\n2413\n2414\n2415\n2416\n2417\n2418\n2419\n2420\n2421\n2422\n2423\n2424\n2425\n2426\n2427\n2428\n2429\n2430\n2431\n2432\n2433\n2434\n2435",
      "language": "unknown"
    },
    {
      "code": "class TranslationRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/audio/createTranslation\n\n    file: UploadFile\n    \"\"\"\n    The audio file object (not file name) to translate, in one of these\n    formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n    \"\"\"\n\n    model: str | None = None\n    \"\"\"ID of the model to use.\n    \"\"\"\n\n    prompt: str = Field(default=\"\")\n    \"\"\"An optional text to guide the model's style or continue a previous audio\n    segment.\n\n    The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n    should match the audio language.\n    \"\"\"\n\n    response_format: AudioResponseFormat = Field(default=\"json\")\n    \"\"\"\n    The format of the output, in one of these options: `json`, `text`, `srt`,\n    `verbose_json`, or `vtt`.\n    \"\"\"\n\n    # TODO support additional sampling parameters\n    # --8<-- [start:translation-sampling-params]\n    seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    \"\"\"The seed to use for sampling.\"\"\"\n\n    temperature: float = Field(default=0.0)\n    \"\"\"The sampling temperature, between 0 and 1.\n\n    Higher values like 0.8 will make the output more random, while lower values\n    like 0.2 will make it more focused / deterministic. If set to 0, the model\n    will use [log probability](https://en.wikipedia.org/wiki/Log_probability)\n    to automatically increase the temperature until certain thresholds are hit.\n    \"\"\"\n    # --8<-- [end:translation-sampling-params]\n\n    # --8<-- [start:translation-extra-params]\n    language: str | None = None\n    \"\"\"The language of the input audio we translate from.\n\n    Supplying the input language in\n    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format\n    will improve accuracy.\n    \"\"\"\n\n    to_language: str | None = None\n    \"\"\"The language of the input audio we translate to.\n\n    Please note that this is not supported by all models, refer to the specific\n    model documentation for more details.\n    For instance, Whisper only supports `to_language=en`.\n    \"\"\"\n\n    stream: bool | None = False\n    \"\"\"Custom field not present in the original OpenAI definition. When set,\n    it will enable output to be streamed in a similar fashion as the Chat\n    Completion endpoint.\n    \"\"\"\n    # Flattened stream option to simplify form data.\n    stream_include_usage: bool | None = False\n    stream_continuous_usage_stats: bool | None = False\n\n    max_completion_tokens: int | None = None\n    \"\"\"The maximum number of tokens to generate.\"\"\"\n    # --8<-- [end:translation-extra-params]\n\n    # Default sampling parameters for translation requests.\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"temperature\": 0,\n    }\n\n    def to_sampling_params(\n        self, default_max_tokens: int, default_sampling_params: dict | None = None\n    ) -> SamplingParams:\n        max_tokens = default_max_tokens\n\n        if default_sampling_params is None:\n            default_sampling_params = {}\n        # Default parameters\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n\n        return SamplingParams.from_optional(\n            temperature=temperature,\n            max_tokens=max_tokens,\n            seed=self.seed,\n            output_kind=RequestOutputKind.DELTA\n            if self.stream\n            else RequestOutputKind.FINAL_ONLY,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_stream_options(cls, data):\n        stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"]\n        stream = data.get(\"stream\", False)\n        if any(bool(data.get(so, False)) for so in stream_opts) and not stream:\n            # Find which specific stream option was set\n            invalid_param = next(\n                (so for so in stream_opts if data.get(so, False)),\n                \"stream_include_usage\",\n            )\n            raise VLLMValidationError(\n                \"Stream options can only be defined when `stream=True`.\",\n                parameter=invalid_param,\n            )\n\n        return data",
      "language": "python"
    },
    {
      "code": "class TranslationRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/audio/createTranslation\n\n    file: UploadFile\n    \"\"\"\n    The audio file object (not file name) to translate, in one of these\n    formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n    \"\"\"\n\n    model: str | None = None\n    \"\"\"ID of the model to use.\n    \"\"\"\n\n    prompt: str = Field(default=\"\")\n    \"\"\"An optional text to guide the model's style or continue a previous audio\n    segment.\n\n    The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting)\n    should match the audio language.\n    \"\"\"\n\n    response_format: AudioResponseFormat = Field(default=\"json\")\n    \"\"\"\n    The format of the output, in one of these options: `json`, `text`, `srt`,\n    `verbose_json`, or `vtt`.\n    \"\"\"\n\n    # TODO support additional sampling parameters\n    # --8<-- [start:translation-sampling-params]\n    seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    \"\"\"The seed to use for sampling.\"\"\"\n\n    temperature: float = Field(default=0.0)\n    \"\"\"The sampling temperature, between 0 and 1.\n\n    Higher values like 0.8 will make the output more random, while lower values\n    like 0.2 will make it more focused / deterministic. If set to 0, the model\n    will use [log probability](https://en.wikipedia.org/wiki/Log_probability)\n    to automatically increase the temperature until certain thresholds are hit.\n    \"\"\"\n    # --8<-- [end:translation-sampling-params]\n\n    # --8<-- [start:translation-extra-params]\n    language: str | None = None\n    \"\"\"The language of the input audio we translate from.\n\n    Supplying the input language in\n    [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format\n    will improve accuracy.\n    \"\"\"\n\n    to_language: str | None = None\n    \"\"\"The language of the input audio we translate to.\n\n    Please note that this is not supported by all models, refer to the specific\n    model documentation for more details.\n    For instance, Whisper only supports `to_language=en`.\n    \"\"\"\n\n    stream: bool | None = False\n    \"\"\"Custom field not present in the original OpenAI definition. When set,\n    it will enable output to be streamed in a similar fashion as the Chat\n    Completion endpoint.\n    \"\"\"\n    # Flattened stream option to simplify form data.\n    stream_include_usage: bool | None = False\n    stream_continuous_usage_stats: bool | None = False\n\n    max_completion_tokens: int | None = None\n    \"\"\"The maximum number of tokens to generate.\"\"\"\n    # --8<-- [end:translation-extra-params]\n\n    # Default sampling parameters for translation requests.\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"temperature\": 0,\n    }\n\n    def to_sampling_params(\n        self, default_max_tokens: int, default_sampling_params: dict | None = None\n    ) -> SamplingParams:\n        max_tokens = default_max_tokens\n\n        if default_sampling_params is None:\n            default_sampling_params = {}\n        # Default parameters\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n            )\n\n        return SamplingParams.from_optional(\n            temperature=temperature,\n            max_tokens=max_tokens,\n            seed=self.seed,\n            output_kind=RequestOutputKind.DELTA\n            if self.stream\n            else RequestOutputKind.FINAL_ONLY,\n            skip_clone=True,  # Created fresh per request, safe to skip clone\n        )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_stream_options(cls, data):\n        stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"]\n        stream = data.get(\"stream\", False)\n        if any(bool(data.get(so, False)) for so in stream_opts) and not stream:\n            # Find which specific stream option was set\n            invalid_param = next(\n                (so for so in stream_opts if data.get(so, False)),\n                \"stream_include_usage\",\n            )\n            raise VLLMValidationError(\n                \"Stream options can only be defined when `stream=True`.\",\n                parameter=invalid_param,\n            )\n\n        return data",
      "language": "python"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS: dict = {'temperature': 0}",
      "language": "typescript"
    },
    {
      "code": "_DEFAULT_SAMPLING_PARAMS: dict = {'temperature': 0}",
      "language": "typescript"
    },
    {
      "code": "file: UploadFile",
      "language": "yaml"
    },
    {
      "code": "file: UploadFile",
      "language": "yaml"
    },
    {
      "code": "language: str | None = None",
      "language": "yaml"
    },
    {
      "code": "language: str | None = None",
      "language": "yaml"
    },
    {
      "code": "max_completion_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "max_completion_tokens: int | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "model: str | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt: str = Field(default='')",
      "language": "typescript"
    },
    {
      "code": "prompt: str = Field(default='')",
      "language": "typescript"
    },
    {
      "code": "response_format: AudioResponseFormat = Field(default=\"json\")",
      "language": "typescript"
    },
    {
      "code": "response_format: AudioResponseFormat = Field(default=\"json\")",
      "language": "typescript"
    },
    {
      "code": "seed: int | None = Field(None, ge=min, le=max)",
      "language": "yaml"
    },
    {
      "code": "seed: int | None = Field(None, ge=min, le=max)",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_continuous_usage_stats: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_continuous_usage_stats: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_include_usage: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "stream_include_usage: bool | None = False",
      "language": "yaml"
    },
    {
      "code": "temperature: float = Field(default=0.0)",
      "language": "typescript"
    },
    {
      "code": "temperature: float = Field(default=0.0)",
      "language": "typescript"
    },
    {
      "code": "to_language: str | None = None",
      "language": "yaml"
    },
    {
      "code": "to_language: str | None = None",
      "language": "yaml"
    },
    {
      "code": "to_sampling_params(\n    default_max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "to_sampling_params(\n    default_max_tokens: int,\n    default_sampling_params: dict | None = None,\n) -> SamplingParams",
      "language": "rust"
    },
    {
      "code": "2396\n2397\n2398\n2399\n2400\n2401\n2402\n2403\n2404\n2405\n2406\n2407\n2408\n2409\n2410\n2411\n2412\n2413\n2414\n2415\n2416\n2417",
      "language": "unknown"
    },
    {
      "code": "def to_sampling_params(\n    self, default_max_tokens: int, default_sampling_params: dict | None = None\n) -> SamplingParams:\n    max_tokens = default_max_tokens\n\n    if default_sampling_params is None:\n        default_sampling_params = {}\n    # Default parameters\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n\n    return SamplingParams.from_optional(\n        temperature=temperature,\n        max_tokens=max_tokens,\n        seed=self.seed,\n        output_kind=RequestOutputKind.DELTA\n        if self.stream\n        else RequestOutputKind.FINAL_ONLY,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "def to_sampling_params(\n    self, default_max_tokens: int, default_sampling_params: dict | None = None\n) -> SamplingParams:\n    max_tokens = default_max_tokens\n\n    if default_sampling_params is None:\n        default_sampling_params = {}\n    # Default parameters\n    if (temperature := self.temperature) is None:\n        temperature = default_sampling_params.get(\n            \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"]\n        )\n\n    return SamplingParams.from_optional(\n        temperature=temperature,\n        max_tokens=max_tokens,\n        seed=self.seed,\n        output_kind=RequestOutputKind.DELTA\n        if self.stream\n        else RequestOutputKind.FINAL_ONLY,\n        skip_clone=True,  # Created fresh per request, safe to skip clone\n    )",
      "language": "python"
    },
    {
      "code": "validate_stream_options(data)",
      "language": "unknown"
    },
    {
      "code": "validate_stream_options(data)",
      "language": "unknown"
    },
    {
      "code": "2419\n2420\n2421\n2422\n2423\n2424\n2425\n2426\n2427\n2428\n2429\n2430\n2431\n2432\n2433\n2434\n2435",
      "language": "unknown"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_stream_options(cls, data):\n    stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"]\n    stream = data.get(\"stream\", False)\n    if any(bool(data.get(so, False)) for so in stream_opts) and not stream:\n        # Find which specific stream option was set\n        invalid_param = next(\n            (so for so in stream_opts if data.get(so, False)),\n            \"stream_include_usage\",\n        )\n        raise VLLMValidationError(\n            \"Stream options can only be defined when `stream=True`.\",\n            parameter=invalid_param,\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "@model_validator(mode=\"before\")\n@classmethod\ndef validate_stream_options(cls, data):\n    stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"]\n    stream = data.get(\"stream\", False)\n    if any(bool(data.get(so, False)) for so in stream_opts) and not stream:\n        # Find which specific stream option was set\n        invalid_param = next(\n            (so for so in stream_opts if data.get(so, False)),\n            \"stream_include_usage\",\n        )\n        raise VLLMValidationError(\n            \"Stream options can only be defined when `stream=True`.\",\n            parameter=invalid_param,\n        )\n\n    return data",
      "language": "python"
    },
    {
      "code": "2439\n2440\n2441",
      "language": "unknown"
    },
    {
      "code": "class TranslationResponse(OpenAIBaseModel):\n    text: str\n    \"\"\"The translated text.\"\"\"",
      "language": "php"
    },
    {
      "code": "class TranslationResponse(OpenAIBaseModel):\n    text: str\n    \"\"\"The translated text.\"\"\"",
      "language": "php"
    },
    {
      "code": "2303\n2304\n2305\n2306",
      "language": "unknown"
    },
    {
      "code": "class TranslationResponseStreamChoice(OpenAIBaseModel):\n    delta: DeltaMessage\n    finish_reason: str | None = None\n    stop_reason: int | str | None = None",
      "language": "php"
    },
    {
      "code": "class TranslationResponseStreamChoice(OpenAIBaseModel):\n    delta: DeltaMessage\n    finish_reason: str | None = None\n    stop_reason: int | str | None = None",
      "language": "php"
    },
    {
      "code": "delta: DeltaMessage",
      "language": "yaml"
    },
    {
      "code": "delta: DeltaMessage",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = None",
      "language": "yaml"
    },
    {
      "code": "stop_reason: int | str | None = None",
      "language": "yaml"
    },
    {
      "code": "2497\n2498\n2499\n2500\n2501\n2502\n2503\n2504\n2505\n2506\n2507\n2508\n2509\n2510\n2511",
      "language": "unknown"
    },
    {
      "code": "class TranslationResponseVerbose(OpenAIBaseModel):\n    duration: str\n    \"\"\"The duration of the input audio.\"\"\"\n\n    language: str\n    \"\"\"The language of the input audio.\"\"\"\n\n    text: str\n    \"\"\"The translated text.\"\"\"\n\n    segments: list[TranslationSegment] | None = None\n    \"\"\"Segments of the translated text and their corresponding details.\"\"\"\n\n    words: list[TranslationWord] | None = None\n    \"\"\"Extracted words and their corresponding timestamps.\"\"\"",
      "language": "php"
    },
    {
      "code": "class TranslationResponseVerbose(OpenAIBaseModel):\n    duration: str\n    \"\"\"The duration of the input audio.\"\"\"\n\n    language: str\n    \"\"\"The language of the input audio.\"\"\"\n\n    text: str\n    \"\"\"The translated text.\"\"\"\n\n    segments: list[TranslationSegment] | None = None\n    \"\"\"Segments of the translated text and their corresponding details.\"\"\"\n\n    words: list[TranslationWord] | None = None\n    \"\"\"Extracted words and their corresponding timestamps.\"\"\"",
      "language": "php"
    },
    {
      "code": "duration: str",
      "language": "yaml"
    },
    {
      "code": "duration: str",
      "language": "yaml"
    },
    {
      "code": "language: str",
      "language": "yaml"
    },
    {
      "code": "language: str",
      "language": "yaml"
    },
    {
      "code": "segments: list[TranslationSegment] | None = None",
      "language": "yaml"
    },
    {
      "code": "segments: list[TranslationSegment] | None = None",
      "language": "yaml"
    },
    {
      "code": "words: list[TranslationWord] | None = None",
      "language": "yaml"
    },
    {
      "code": "words: list[TranslationWord] | None = None",
      "language": "yaml"
    },
    {
      "code": "2455\n2456\n2457\n2458\n2459\n2460\n2461\n2462\n2463\n2464\n2465\n2466\n2467\n2468\n2469\n2470\n2471\n2472\n2473\n2474\n2475\n2476\n2477\n2478\n2479\n2480\n2481\n2482\n2483\n2484\n2485\n2486\n2487\n2488\n2489\n2490\n2491\n2492\n2493\n2494",
      "language": "unknown"
    },
    {
      "code": "class TranslationSegment(OpenAIBaseModel):\n    id: int\n    \"\"\"Unique identifier of the segment.\"\"\"\n\n    avg_logprob: float | None = None\n    \"\"\"Average logprob of the segment.\n\n    If the value is lower than -1, consider the logprobs failed.\n    \"\"\"\n\n    compression_ratio: float | None = None\n    \"\"\"Compression ratio of the segment.\n\n    If the value is greater than 2.4, consider the compression failed.\n    \"\"\"\n\n    end: float\n    \"\"\"End time of the segment in seconds.\"\"\"\n\n    no_speech_prob: float | None = None\n    \"\"\"Probability of no speech in the segment.\n\n    If the value is higher than 1.0 and the `avg_logprob` is below -1, consider\n    this segment silent.\n    \"\"\"\n\n    seek: int\n    \"\"\"Seek offset of the segment.\"\"\"\n\n    start: float\n    \"\"\"Start time of the segment in seconds.\"\"\"\n\n    temperature: float\n    \"\"\"Temperature parameter used for generating the segment.\"\"\"\n\n    text: str\n    \"\"\"Text content of the segment.\"\"\"\n\n    tokens: list[int]\n    \"\"\"Array of token IDs for the text content.\"\"\"",
      "language": "php"
    },
    {
      "code": "class TranslationSegment(OpenAIBaseModel):\n    id: int\n    \"\"\"Unique identifier of the segment.\"\"\"\n\n    avg_logprob: float | None = None\n    \"\"\"Average logprob of the segment.\n\n    If the value is lower than -1, consider the logprobs failed.\n    \"\"\"\n\n    compression_ratio: float | None = None\n    \"\"\"Compression ratio of the segment.\n\n    If the value is greater than 2.4, consider the compression failed.\n    \"\"\"\n\n    end: float\n    \"\"\"End time of the segment in seconds.\"\"\"\n\n    no_speech_prob: float | None = None\n    \"\"\"Probability of no speech in the segment.\n\n    If the value is higher than 1.0 and the `avg_logprob` is below -1, consider\n    this segment silent.\n    \"\"\"\n\n    seek: int\n    \"\"\"Seek offset of the segment.\"\"\"\n\n    start: float\n    \"\"\"Start time of the segment in seconds.\"\"\"\n\n    temperature: float\n    \"\"\"Temperature parameter used for generating the segment.\"\"\"\n\n    text: str\n    \"\"\"Text content of the segment.\"\"\"\n\n    tokens: list[int]\n    \"\"\"Array of token IDs for the text content.\"\"\"",
      "language": "php"
    },
    {
      "code": "avg_logprob: float | None = None",
      "language": "yaml"
    },
    {
      "code": "avg_logprob: float | None = None",
      "language": "yaml"
    },
    {
      "code": "compression_ratio: float | None = None",
      "language": "yaml"
    },
    {
      "code": "compression_ratio: float | None = None",
      "language": "yaml"
    },
    {
      "code": "no_speech_prob: float | None = None",
      "language": "yaml"
    },
    {
      "code": "no_speech_prob: float | None = None",
      "language": "yaml"
    },
    {
      "code": "start: float",
      "language": "yaml"
    },
    {
      "code": "start: float",
      "language": "yaml"
    },
    {
      "code": "temperature: float",
      "language": "yaml"
    },
    {
      "code": "temperature: float",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "tokens: list[int]",
      "language": "yaml"
    },
    {
      "code": "2309\n2310\n2311\n2312\n2313\n2314\n2315",
      "language": "unknown"
    },
    {
      "code": "class TranslationStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"trsl-{random_uuid()}\")\n    object: Literal[\"translation.chunk\"] = \"translation.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[TranslationResponseStreamChoice]\n    usage: UsageInfo | None = Field(default=None)",
      "language": "typescript"
    },
    {
      "code": "class TranslationStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"trsl-{random_uuid()}\")\n    object: Literal[\"translation.chunk\"] = \"translation.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: list[TranslationResponseStreamChoice]\n    usage: UsageInfo | None = Field(default=None)",
      "language": "typescript"
    },
    {
      "code": "choices: list[TranslationResponseStreamChoice]",
      "language": "yaml"
    },
    {
      "code": "choices: list[TranslationResponseStreamChoice]",
      "language": "yaml"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "created: int = Field(default_factory=lambda: int(time()))",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"trsl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "id: str = Field(\n    default_factory=lambda: f\"trsl-{random_uuid()}\"\n)",
      "language": "typescript"
    },
    {
      "code": "object: Literal['translation.chunk'] = 'translation.chunk'",
      "language": "yaml"
    },
    {
      "code": "object: Literal['translation.chunk'] = 'translation.chunk'",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "usage: UsageInfo | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "2444\n2445\n2446\n2447\n2448\n2449\n2450\n2451\n2452",
      "language": "unknown"
    },
    {
      "code": "class TranslationWord(OpenAIBaseModel):\n    end: float\n    \"\"\"End time of the word in seconds.\"\"\"\n\n    start: float\n    \"\"\"Start time of the word in seconds.\"\"\"\n\n    word: str\n    \"\"\"The text content of the word.\"\"\"",
      "language": "php"
    },
    {
      "code": "class TranslationWord(OpenAIBaseModel):\n    end: float\n    \"\"\"End time of the word in seconds.\"\"\"\n\n    start: float\n    \"\"\"Start time of the word in seconds.\"\"\"\n\n    word: str\n    \"\"\"The text content of the word.\"\"\"",
      "language": "php"
    },
    {
      "code": "start: float",
      "language": "yaml"
    },
    {
      "code": "start: float",
      "language": "yaml"
    },
    {
      "code": "2011\n2012\n2013",
      "language": "unknown"
    },
    {
      "code": "class UnloadLoRAAdapterRequest(BaseModel):\n    lora_name: str\n    lora_int_id: int | None = Field(default=None)",
      "language": "php"
    },
    {
      "code": "class UnloadLoRAAdapterRequest(BaseModel):\n    lora_name: str\n    lora_int_id: int | None = Field(default=None)",
      "language": "php"
    },
    {
      "code": "lora_int_id: int | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "lora_int_id: int | None = Field(default=None)",
      "language": "yaml"
    },
    {
      "code": "lora_name: str",
      "language": "yaml"
    },
    {
      "code": "lora_name: str",
      "language": "yaml"
    },
    {
      "code": "199\n200\n201\n202\n203",
      "language": "unknown"
    },
    {
      "code": "class UsageInfo(OpenAIBaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: int | None = 0\n    prompt_tokens_details: PromptTokenUsageInfo | None = None",
      "language": "typescript"
    },
    {
      "code": "class UsageInfo(OpenAIBaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: int | None = 0\n    prompt_tokens_details: PromptTokenUsageInfo | None = None",
      "language": "typescript"
    },
    {
      "code": "completion_tokens: int | None = 0",
      "language": "yaml"
    },
    {
      "code": "completion_tokens: int | None = 0",
      "language": "yaml"
    },
    {
      "code": "prompt_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "prompt_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "prompt_tokens_details: PromptTokenUsageInfo | None = None",
      "language": "yaml"
    },
    {
      "code": "prompt_tokens_details: PromptTokenUsageInfo | None = None",
      "language": "yaml"
    },
    {
      "code": "total_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "total_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161",
      "language": "unknown"
    },
    {
      "code": "class VLLMValidationError(ValueError):\n    \"\"\"vLLM-specific validation error for request validation failures.\n\n    Args:\n        message: The error message describing the validation failure.\n        parameter: Optional parameter name that failed validation.\n        value: Optional value that was rejected during validation.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        parameter: str | None = None,\n        value: Any = None,\n    ) -> None:\n        super().__init__(message)\n        self.parameter = parameter\n        self.value = value\n\n    def __str__(self):\n        base = super().__str__()\n        extras = []\n        if self.parameter is not None:\n            extras.append(f\"parameter={self.parameter}\")\n        if self.value is not None:\n            extras.append(f\"value={self.value}\")\n        return f\"{base} ({', '.join(extras)})\" if extras else base",
      "language": "python"
    },
    {
      "code": "class VLLMValidationError(ValueError):\n    \"\"\"vLLM-specific validation error for request validation failures.\n\n    Args:\n        message: The error message describing the validation failure.\n        parameter: Optional parameter name that failed validation.\n        value: Optional value that was rejected during validation.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: str,\n        *,\n        parameter: str | None = None,\n        value: Any = None,\n    ) -> None:\n        super().__init__(message)\n        self.parameter = parameter\n        self.value = value\n\n    def __str__(self):\n        base = super().__str__()\n        extras = []\n        if self.parameter is not None:\n            extras.append(f\"parameter={self.parameter}\")\n        if self.value is not None:\n            extras.append(f\"value={self.value}\")\n        return f\"{base} ({', '.join(extras)})\" if extras else base",
      "language": "python"
    },
    {
      "code": "parameter = parameter",
      "language": "unknown"
    },
    {
      "code": "parameter = parameter",
      "language": "unknown"
    },
    {
      "code": "value = value",
      "language": "unknown"
    },
    {
      "code": "value = value",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    message: str,\n    *,\n    parameter: str | None = None,\n    value: Any = None,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    message: str,\n    *,\n    parameter: str | None = None,\n    value: Any = None,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "143\n144\n145\n146\n147\n148\n149\n150\n151\n152",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    message: str,\n    *,\n    parameter: str | None = None,\n    value: Any = None,\n) -> None:\n    super().__init__(message)\n    self.parameter = parameter\n    self.value = value",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    message: str,\n    *,\n    parameter: str | None = None,\n    value: Any = None,\n) -> None:\n    super().__init__(message)\n    self.parameter = parameter\n    self.value = value",
      "language": "python"
    },
    {
      "code": "154\n155\n156\n157\n158\n159\n160\n161",
      "language": "unknown"
    },
    {
      "code": "def __str__(self):\n    base = super().__str__()\n    extras = []\n    if self.parameter is not None:\n        extras.append(f\"parameter={self.parameter}\")\n    if self.value is not None:\n        extras.append(f\"value={self.value}\")\n    return f\"{base} ({', '.join(extras)})\" if extras else base",
      "language": "python"
    },
    {
      "code": "def __str__(self):\n    base = super().__str__()\n    extras = []\n    if self.parameter is not None:\n        extras.append(f\"parameter={self.parameter}\")\n    if self.value is not None:\n        extras.append(f\"value={self.value}\")\n    return f\"{base} ({', '.join(extras)})\" if extras else base",
      "language": "python"
    },
    {
      "code": "get_logits_processors(\n    processors: LogitsProcessors | None, pattern: str | None\n) -> list[Any] | None",
      "language": "rust"
    },
    {
      "code": "get_logits_processors(\n    processors: LogitsProcessors | None, pattern: str | None\n) -> list[Any] | None",
      "language": "rust"
    },
    {
      "code": "293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324",
      "language": "unknown"
    },
    {
      "code": "def get_logits_processors(\n    processors: LogitsProcessors | None, pattern: str | None\n) -> list[Any] | None:\n    if processors and pattern:\n        logits_processors = []\n        for processor in processors:\n            qualname = processor if isinstance(processor, str) else processor.qualname\n            if not re.match(pattern, qualname):\n                raise ValueError(\n                    f\"Logits processor '{qualname}' is not allowed by this \"\n                    \"server. See --logits-processor-pattern engine argument \"\n                    \"for more information.\"\n                )\n            try:\n                logits_processor = resolve_obj_by_qualname(qualname)\n            except Exception as e:\n                raise ValueError(\n                    f\"Logits processor '{qualname}' could not be resolved: {e}\"\n                ) from e\n            if isinstance(processor, LogitsProcessorConstructor):\n                logits_processor = logits_processor(\n                    *processor.args or [], **processor.kwargs or {}\n                )\n            logits_processors.append(logits_processor)\n        return logits_processors\n    elif processors:\n        raise ValueError(\n            \"The `logits_processors` argument is not supported by this \"\n            \"server. See --logits-processor-pattern engine argument \"\n            \"for more information.\"\n        )\n    return None",
      "language": "typescript"
    },
    {
      "code": "def get_logits_processors(\n    processors: LogitsProcessors | None, pattern: str | None\n) -> list[Any] | None:\n    if processors and pattern:\n        logits_processors = []\n        for processor in processors:\n            qualname = processor if isinstance(processor, str) else processor.qualname\n            if not re.match(pattern, qualname):\n                raise ValueError(\n                    f\"Logits processor '{qualname}' is not allowed by this \"\n                    \"server. See --logits-processor-pattern engine argument \"\n                    \"for more information.\"\n                )\n            try:\n                logits_processor = resolve_obj_by_qualname(qualname)\n            except Exception as e:\n                raise ValueError(\n                    f\"Logits processor '{qualname}' could not be resolved: {e}\"\n                ) from e\n            if isinstance(processor, LogitsProcessorConstructor):\n                logits_processor = logits_processor(\n                    *processor.args or [], **processor.kwargs or {}\n                )\n            logits_processors.append(logits_processor)\n        return logits_processors\n    elif processors:\n        raise ValueError(\n            \"The `logits_processors` argument is not supported by this \"\n            \"server. See --logits-processor-pattern engine argument \"\n            \"for more information.\"\n        )\n    return None",
      "language": "typescript"
    },
    {
      "code": "serialize_message(msg)",
      "language": "unknown"
    },
    {
      "code": "serialize_message(msg)",
      "language": "unknown"
    },
    {
      "code": "1655\n1656\n1657\n1658\n1659\n1660\n1661\n1662\n1663\n1664\n1665",
      "language": "unknown"
    },
    {
      "code": "def serialize_message(msg):\n    \"\"\"\n    Serializes a single message\n    \"\"\"\n    if isinstance(msg, dict):\n        return msg\n    elif hasattr(msg, \"to_dict\"):\n        return msg.to_dict()\n    else:\n        # fallback to pyandic dump\n        return msg.model_dump_json()",
      "language": "python"
    },
    {
      "code": "def serialize_message(msg):\n    \"\"\"\n    Serializes a single message\n    \"\"\"\n    if isinstance(msg, dict):\n        return msg\n    elif hasattr(msg, \"to_dict\"):\n        return msg.to_dict()\n    else:\n        # fallback to pyandic dump\n        return msg.model_dump_json()",
      "language": "python"
    },
    {
      "code": "serialize_messages(msgs)",
      "language": "unknown"
    },
    {
      "code": "serialize_messages(msgs)",
      "language": "unknown"
    },
    {
      "code": "1668\n1669\n1670\n1671\n1672",
      "language": "unknown"
    },
    {
      "code": "def serialize_messages(msgs):\n    \"\"\"\n    Serializes multiple messages\n    \"\"\"\n    return [serialize_message(msg) for msg in msgs] if msgs else None",
      "language": "python"
    },
    {
      "code": "def serialize_messages(msgs):\n    \"\"\"\n    Serializes multiple messages\n    \"\"\"\n    return [serialize_message(msg) for msg in msgs] if msgs else None",
      "language": "python"
    }
  ],
  "patterns": [
    {
      "description": "vLLM GitHub Home User Guide User Guide Getting Started Getting Started Quickstart Installation Installation GPU CPU TPU Examples Examples Offline inference Offline inference Async LLM Streaming Audio Language Automatic Prefix Caching Basic Batch LLM Inference Chat With Tools Context Extension Data Parallel Disaggregated Prefill V1 Disaggregated Prefill Encoder Decoder Multimodal KV Load Failure Recovery Test LLM Engine Example LLM Engine Reset Kv Load Sharded State Logits Processor LoRA With Quantization Inference Metrics Mistral-Small MLPSpeculator MultiLoRA Inference Offline Inference with the OpenAI Batch file format Prefix Caching Prompt Embed Inference Qwen2.5-Omni Offline Inference Examples Qwen3 Omni Qwen 1M Reproducibility RLHF RLHF Colocate RLHF Online Quant RLHF Utils Save Sharded State Simple Profiling Skip Loading Weights In Engine Init Spec Decode Structured Outputs Torchrun Dp Example Torchrun Example Vision Language Vision Language Multi Image Online serving Online serving API Client Helm Charts Monitoring Dashboards Disaggregated Encoder Disaggregated Prefill Disaggregated Serving Disaggregated Serving P2P Nccl Xpyd Elastic Ep Gradio OpenAI Chatbot Webserver Gradio Webserver Kv Events Subscriber Multi-Node-Serving Multi Instance Data Parallel OpenAI Chat Completion Client OpenAI Chat Completion Client For Multimodal OpenAI Chat Completion Client With Tools OpenAI Chat Completion Client With Tools Required OpenAI Chat Completion Client With Tools Xlam OpenAI Chat Completion Client With Tools Xlam Streaming OpenAI Chat Completion Tool Calls With Reasoning OpenAI Chat Completion With Reasoning OpenAI Chat Completion With Reasoning Streaming OpenAI Completion Client OpenAI Responses Client OpenAI Responses Client With Mcp Tools OpenAI Responses Client With Tools OpenAI Transcription Client OpenAI Translation Client Setup OpenTelemetry POC Prometheus and Grafana Prompt Embed Inference With OpenAI Client Ray Serve Deepseek Retrieval Augmented Generation With Langchain Retrieval Augmented Generation With Llamaindex Run Cluster Sagemaker-Entrypoint Streamlit OpenAI Chatbot Webserver Structured Outputs Token Generation Client Utils Others Others LMCache Examples Logging Configuration Tensorize vLLM Model Pooling Pooling Classify Embed Plugin Pooling Score Token Classify Token Embed General General vLLM V1 Frequently Asked Questions Production Metrics Reproducibility Security Troubleshooting Usage Stats Collection Inference and Serving Inference and Serving Offline Inference OpenAI-Compatible Server Context Parallel Deployment Data Parallel Deployment Troubleshooting distributed deployments Expert Parallel Deployment Parallelism and Scaling Integrations Integrations LangChain LlamaIndex Deployment Deployment Using Docker Using Kubernetes Using Nginx Frameworks Frameworks Anyscale AnythingLLM AutoGen BentoML Cerebrium Chatbox Dify dstack Haystack Helm Hugging Face Inference Endpoints LiteLLM Lobe Chat LWS Modal Open WebUI Retrieval-Augmented Generation SkyPilot Streamlit NVIDIA Triton Integrations Integrations KAITO KServe Kthena KubeAI KubeRay Llama Stack llm-d llmaz Production stack Training Training Reinforcement Learning from Human Feedback Transformers Reinforcement Learning Configuration Configuration Conserving Memory Engine Arguments Environment Variables Model Resolution Optimization and Tuning Server Arguments TPU Models Models Supported Models Generative Models Pooling Models Extensions Extensions Loading Model weights with fastsafetensors Loading models with Run:ai Model Streamer Loading models with CoreWeave's Tensorizer Hardware Supported Models Hardware Supported Models CPU - Intel® Xeon® XPU - Intel® GPUs TPU Features Features Automatic Prefix Caching Batch Invariance Custom Arguments Custom Logits Processors Disaggregated Encoder Disaggregated Prefilling (experimental) Interleaved Thinking LoRA Adapters MooncakeConnector Usage Guide Multimodal Inputs NixlConnector Usage Guide Prompt Embedding Inputs Reasoning Outputs Sleep Mode Speculative Decoding Structured Outputs Tool Calling Quantization Quantization AutoAWQ AutoRound BitBLAS BitsAndBytes FP8 W8A8 GGUF GPTQModel FP8 INC INT4 W4A16 INT8 W8A8 NVIDIA Model Optimizer Quantized KV Cache AMD Quark TorchAO Developer Guide Developer Guide General General Deprecation Policy Dockerfile Incremental Compilation Workflow Profiling vLLM Vulnerability Management Model Implementation Model Implementation Basic Model Registering a Model Unit Testing Multi-Modal Support Speech-to-Text (Transcription/Translation) Support CI CI CI Failures Nightly Builds of vLLM Wheels Update PyTorch version on vLLM OSS CI/CD Design Documents Design Documents Plugins Plugins IO Processor Plugins LoRA Resolver Plugins Plugin System Architecture Overview CUDA Graphs Dual Batch Overlap How to debug the vLLM-torch.compile integration Fused MoE Modular Kernel Integration with Hugging Face Hybrid KV Cache Manager Logits Processors Metrics Multi-Modal Data Processing Fused MoE Kernel Features Python Multiprocessing Optimization levels P2P NCCL Connector Paged Attention Automatic Prefix Caching torch.compile integration Benchmarking Benchmarking Benchmark CLI Parameter Sweeps Performance Dashboard API Reference API Reference vllm vllm beam_search collect_env connections env_override envs forward_context logger logits_process logprobs outputs pooling_params sampling_params scalar_type scripts sequence tasks tracing version assets assets audio base image video attention attention layer selector backends backends abstract registry utils layers layers chunked_local_attention cross_attention encoder_only_attention mm_encoder_attention ops ops chunked_prefill_paged_decode common flashmla merge_attn_states paged_attn pallas_kv_cache_update prefix_prefill rocm_aiter_mla_sparse triton_decode_attention triton_merge_attn_states triton_reshape_and_cache_flash triton_unified_attention vit_attn_wrappers utils utils fa_utils kv_sharing_utils kv_transfer_utils benchmarks benchmarks datasets latency serve startup throughput lib lib endpoint_request_func ready_checker utils sweep sweep cli param_sweep plot plot_pareto serve serve_sla server sla_sweep utils compilation compilation activation_quant_fusion backends base_static_graph caching collective_fusion compiler_interface counter cuda_graph decorators fix_functionalization fusion fusion_attn fx_utils inductor_pass matcher_utils monitor noop_elimination partition_rules pass_manager piecewise_backend post_cleanup qk_norm_rope_fusion rocm_aiter_fusion sequence_parallelism torch25_custom_graph_pass vllm_inductor_pass wrapper config config attention cache compilation device ec_transfer kv_events kv_transfer load lora model multimodal observability parallel pooler profiler scheduler speculative speech_to_text structured_outputs utils vllm device_allocator device_allocator cumem distributed distributed communication_op kv_events parallel_state tpu_distributed_utils utils device_communicators device_communicators all2all all_reduce_utils base_device_communicator cpu_communicator cuda_communicator cuda_wrapper custom_all_reduce mnnvl_compat pynccl pynccl_allocator pynccl_wrapper quick_all_reduce ray_communicator shm_broadcast shm_object_storage symm_mem tpu_communicator xpu_communicator ec_transfer ec_transfer ec_transfer_state ec_connector ec_connector base example_connector factory eplb eplb async_worker eplb_state rebalance_execute policy policy abstract default kv_transfer kv_transfer kv_transfer_state kv_connector kv_connector base factory utils v1 v1 base decode_bench_connector example_connector lmcache_connector lmcache_mp_connector metrics mooncake_connector multi_connector nixl_connector offloading_connector lmcache_integration lmcache_integration multi_process_adapter utils vllm_v1_adapter p2p p2p p2p_nccl_connector p2p_nccl_engine tensor_memory_pool engine engine arg_utils async_llm_engine llm_engine protocol entrypoints entrypoints api_server chat_utils constants context launcher llm logger renderer responses_utils score_utils ssl tool tool_server utils anthropic anthropic protocol serving_messages cli cli collect_env main openai run_batch serve types benchmark benchmark base latency main serve startup sweep throughput openai openai api_server cli_args orca_metrics protocol protocol Table of contents AnyResponseFormat AnyStructuralTagResponseFormat AudioResponseFormat LogitsProcessors ResponseInputOutputItem ResponseInputOutputMessage StreamingResponsesResponse TokenizeRequest TranscriptionResponseVariant TranslationResponseVariant _LONG_INFO logger ChatCompletionLogProb bytes logprob token ChatCompletionLogProbs content ChatCompletionLogProbsContent field_names top_logprobs ChatCompletionNamedFunction name ChatCompletionNamedToolChoiceParam function type ChatCompletionRequest _DEFAULT_SAMPLING_PARAMS add_generation_prompt add_special_tokens allowed_token_ids bad_words cache_salt chat_template chat_template_kwargs continue_final_message documents echo frequency_penalty ignore_eos include_reasoning include_stop_str_in_output kv_transfer_params length_penalty logit_bias logits_processors logprobs max_completion_tokens max_tokens messages min_p min_tokens mm_processor_kwargs model n parallel_tool_calls presence_penalty priority prompt_logprobs reasoning_effort repetition_penalty request_id response_format return_token_ids return_tokens_as_token_ids seed skip_special_tokens spaces_between_special_tokens stop stop_token_ids stream stream_options structured_outputs temperature tool_choice tools top_k top_logprobs top_p truncate_prompt_tokens use_beam_search user vllm_xargs check_cache_salt_support check_generation_prompt check_logprobs check_structured_outputs_count check_tool_usage to_beam_search_params to_sampling_params validate_stream_options ChatCompletionResponse choices created id kv_transfer_params model object prompt_logprobs prompt_token_ids service_tier system_fingerprint usage ChatCompletionResponseChoice finish_reason index logprobs message stop_reason token_ids ChatCompletionResponseStreamChoice delta finish_reason index logprobs stop_reason token_ids ChatCompletionStreamResponse choices created id model object prompt_token_ids usage ChatCompletionToolsParam function type ChatMessage annotations audio content function_call reasoning reasoning_content refusal role tool_calls handle_deprecated_reasoning_content CompletionLogProbs text_offset token_logprobs tokens top_logprobs CompletionRequest _DEFAULT_SAMPLING_PARAMS add_special_tokens allowed_token_ids cache_salt echo frequency_penalty ignore_eos include_stop_str_in_output kv_transfer_params length_penalty logit_bias logits_processors logprobs max_tokens min_p min_tokens model n presence_penalty priority prompt prompt_embeds prompt_logprobs repetition_penalty request_id response_format return_token_ids return_tokens_as_token_ids seed skip_special_tokens spaces_between_special_tokens stop stop_token_ids stream stream_options structured_outputs suffix temperature top_k top_p truncate_prompt_tokens use_beam_search user vllm_xargs check_cache_salt_support check_logprobs check_structured_outputs_count to_beam_search_params to_sampling_params validate_prompt_and_prompt_embeds validate_stream_options CompletionResponse choices created id kv_transfer_params model object service_tier system_fingerprint usage CompletionResponseChoice finish_reason index logprobs prompt_logprobs prompt_token_ids stop_reason text token_ids CompletionResponseStreamChoice finish_reason index logprobs prompt_token_ids stop_reason text token_ids CompletionStreamResponse choices created id model object usage DeltaFunctionCall arguments name DeltaMessage content reasoning reasoning_content role tool_calls handle_deprecated_reasoning_content DeltaToolCall function id index type DetokenizeRequest model tokens DetokenizeResponse prompt ErrorInfo code message param type ErrorResponse error ExtractedToolCallInformation content tool_calls tools_called FunctionCall arguments name FunctionDefinition description name parameters GenerateRequest cache_salt features kv_transfer_params model priority request_id sampling_params stream stream_options token_ids GenerateResponse choices kv_transfer_params prompt_logprobs request_id GenerateResponseChoice finish_reason index logprobs token_ids InputTokensDetails cached_tokens cached_tokens_per_turn input_tokens_per_turn JsonSchemaResponseFormat description json_schema name strict LegacyStructuralTag begin end structural_tag_schema LegacyStructuralTagResponseFormat structures triggers type LoadLoRAAdapterRequest lora_name lora_path LogitsProcessorConstructor args kwargs model_config qualname ModelCard created id max_model_len object owned_by parent permission root ModelList data object ModelPermission allow_create_engine allow_fine_tuning allow_logprobs allow_sampling allow_search_indices allow_view created group id is_blocking object organization OpenAIBaseModel field_names model_config __log_extra_fields__ OutputTokensDetails output_tokens_per_turn reasoning_tokens tool_output_tokens tool_output_tokens_per_turn PromptTokenUsageInfo cached_tokens RequestResponseMetadata final_usage_info request_id ResponseCompletedEvent response ResponseCreatedEvent response ResponseFormat json_schema type ResponseInProgressEvent response ResponseRawMessageAndToken message tokens type ResponseReasoningPartAddedEvent content_index item_id output_index part sequence_number type ResponseReasoningPartDoneEvent content_index item_id output_index part sequence_number type ResponseUsage input_tokens input_tokens_details output_tokens output_tokens_details total_tokens ResponsesRequest _DEFAULT_SAMPLING_PARAMS background cache_salt enable_response_messages include input instructions logit_bias max_output_tokens max_tool_calls metadata mm_processor_kwargs model parallel_tool_calls previous_input_messages previous_response_id priority prompt reasoning request_id service_tier store stream temperature text tool_choice tools top_k top_logprobs top_p truncation user check_cache_salt_support function_call_parsing is_include_output_logprobs to_sampling_params validate_background validate_prompt ResponsesResponse background created_at id incomplete_details input_messages instructions max_output_tokens max_tool_calls metadata model object output output_messages parallel_tool_calls previous_response_id prompt reasoning service_tier status temperature text tool_choice tools top_logprobs top_p truncation usage user from_request serialize_input_messages serialize_output_messages StreamOptions continuous_usage_stats include_usage StructuralTagResponseFormat format type TokenizeChatRequest add_generation_prompt add_special_tokens chat_template chat_template_kwargs continue_final_message messages mm_processor_kwargs model return_token_strs tools check_generation_prompt TokenizeCompletionRequest add_special_tokens model prompt return_token_strs TokenizeResponse count max_model_len token_strs tokens TokenizerInfoResponse model_config tokenizer_class ToolCall function id type TranscriptionRequest _DEFAULT_SAMPLING_PARAMS file frequency_penalty language max_completion_tokens min_p model presence_penalty prompt repetition_penalty response_format seed stream stream_continuous_usage_stats stream_include_usage temperature timestamp_granularities to_language top_k top_p vllm_xargs to_sampling_params validate_transcription_request TranscriptionResponse text usage TranscriptionResponseStreamChoice delta finish_reason stop_reason TranscriptionResponseVerbose duration language segments text words TranscriptionSegment avg_logprob compression_ratio end id no_speech_prob seek start temperature text tokens TranscriptionStreamResponse choices created id model object usage TranscriptionUsageAudio seconds type TranscriptionWord end start word TranslationRequest _DEFAULT_SAMPLING_PARAMS file language max_completion_tokens model prompt response_format seed stream stream_continuous_usage_stats stream_include_usage temperature to_language to_sampling_params validate_stream_options TranslationResponse text TranslationResponseStreamChoice delta finish_reason stop_reason TranslationResponseVerbose duration language segments text words TranslationSegment avg_logprob compression_ratio end id no_speech_prob seek start temperature text tokens TranslationStreamResponse choices created id model object usage TranslationWord end start word UnloadLoRAAdapterRequest lora_int_id lora_name UsageInfo completion_tokens prompt_tokens prompt_tokens_details total_tokens VLLMValidationError parameter value __init__ __str__ get_logits_processors serialize_message serialize_messages run_batch serving_chat serving_chat_stream_harmony serving_completion serving_engine serving_models serving_responses serving_transcription speech_to_text utils parser parser harmony_utils responses_parser pooling pooling classify classify api_router protocol serving embed embed api_router conftest protocol serving pooling pooling api_router protocol serving score score api_router protocol serving sagemaker sagemaker routes serve serve cache cache api_router disagg disagg api_router protocol serving elastic_ep elastic_ep api_router middleware instrumentator instrumentator health metrics server_info lora lora api_router profile profile api_router rlhf rlhf api_router rpc rpc api_router sleep sleep api_router tokenize tokenize api_router serving inputs inputs data parse preprocess logging_utils logging_utils dump_input formatter lazy log_time lora lora lora_model lora_weights model_manager peft_helper request resolver utils worker_manager layers layers base base_linear column_parallel_linear fused_moe logits_processor replicated_linear row_parallel_linear utils vocal_parallel_embedding ops ops ipex_ops ipex_ops lora_ops torch_ops torch_ops lora_ops triton_ops triton_ops fused_moe_lora_op kernel_utils lora_expand_op lora_kernel_metadata lora_shrink_op utils xla_ops xla_ops lora_ops punica_wrapper punica_wrapper punica_base punica_cpu punica_gpu punica_selector punica_tpu punica_xpu utils model_executor model_executor custom_op parameter utils layers layers activation attention_layer_base batch_invariant conv kda layernorm lightning_attn linear logits_processor mla pooler resampler utils vocab_parallel_embedding fla fla ops ops chunk chunk_delta_h chunk_o chunk_scaled_dot_kkt cumsum fused_recurrent index kda l2norm layernorm_guard op solve_tril utils wy_fast fused_moe fused_moe all2all_utils batched_deep_gemm_moe config cpu_fused_moe cutlass_moe deep_gemm_moe deep_gemm_utils deepep_ht_prepare_finalize deepep_ll_prepare_finalize flashinfer_cutedsl_moe flashinfer_cutlass_moe flashinfer_cutlass_prepare_finalize flashinfer_trtllm_moe fused_batched_moe fused_marlin_moe fused_moe fused_moe_method_base fused_moe_modular_method gpt_oss_triton_kernels_moe layer modular_kernel moe_align_block_size moe_pallas moe_permute_unpermute moe_torch_iterative pplx_prepare_finalize prepare_finalize rocm_aiter_fused_moe routing_simulator shared_fused_moe topk_weight_and_reduce triton_deep_gemm_moe trtllm_moe unquantized_fused_moe_method utils zero_expert_fused_moe mamba mamba abstract linear_attn mamba_mixer mamba_mixer2 mamba_utils short_conv ops ops causal_conv1d layernorm_gated mamba_ssm ssd_bmm ssd_chunk_scan ssd_chunk_state ssd_combined ssd_state_passing quantization quantization auto_round awq awq_marlin awq_triton base_config bitblas bitsandbytes cpu_wna16 deepspeedfp experts_int8 fbgemm_fp8 fp8 fp_quant gguf gptq gptq_bitblas gptq_marlin gptq_marlin_24 hqq_marlin inc input_quant_fp8 ipex_quant kv_cache modelopt moe_wna16 mxfp4 petit ptpc_fp8 qutlass_utils rtn schema torchao tpu_int8 compressed_tensors compressed_tensors compressed_tensors compressed_tensors_moe triton_scaled_mm utils schemes schemes compressed_tensors_24 compressed_tensors_scheme compressed_tensors_w4a4_nvfp4 compressed_tensors_w4a8_fp8 compressed_tensors_w4a8_int compressed_tensors_w4a16_24 compressed_tensors_w4a16_nvfp4 compressed_tensors_w8a8_fp8 compressed_tensors_w8a8_int8 compressed_tensors_w8a16_fp8 compressed_tensors_wNa16 transform transform linear module utils schemes schemes linear_qutlass_nvfp4 kernels kernels mixed_precision mixed_precision allspark bitblas conch cutlass dynamic_4bit exllama MPLinearKernel machete marlin xpu scaled_mm scaled_mm aiter cpu cutlass ScaledMMLinearKernel triton xla quark quark quark quark_moe utils schemes schemes quark_ocp_mx quark_scheme quark_w8a8_fp8 quark_w8a8_int8 utils utils allspark_utils bitblas_utils flashinfer_fp4_moe flashinfer_utils fp8_utils gptq_utils int8_utils layer_utils machete_utils marlin_utils marlin_utils_fp4 marlin_utils_fp8 marlin_utils_test marlin_utils_test_24 mxfp4_utils mxfp6_utils mxfp8_utils nvfp4_emulation_utils nvfp4_moe_support ocp_mx_utils petit_utils quant_utils w8a8_utils rotary_embedding rotary_embedding base common deepseek_scaling_rope dual_chunk_rope dynamic_ntk_alpha_rope dynamic_ntk_scaling_rope ernie45_vl_rope linear_scaling_rope llama3_rope llama4_vision_rope mrope ntk_scaling_rope phi3_long_rope_scaled_rope xdrope yarn_scaling_rope model_loader model_loader base_loader bitsandbytes_loader default_loader dummy_loader gguf_loader online_quantization runai_streamer_loader sharded_state_loader tensorizer tensorizer_loader tpu utils weight_utils models models adapters afmoe aimv2 apertus arcee arctic aria audioflamingo3 aya_vision bagel baichuan bailing_moe bamba bee bert bert_with_rope blip blip2 bloom chameleon chatglm clip cohere2_vision commandr config dbrx deepencoder deepseek_eagle deepseek_mtp deepseek_ocr deepseek_v2 deepseek_vl2 dots1 dots_ocr ernie45 ernie45_moe ernie45_vl ernie45_vl_moe ernie_mtp exaone exaone4 fairseq2_llama falcon falcon_h1 flex_olmo fuyu gemma gemma2 gemma3 gemma3_mm gemma3n gemma3n_mm glm glm4 glm4_1v glm4_moe glm4_moe_mtp glm4v gpt2 gpt_bigcode gpt_j gpt_neox gpt_oss granite granite_speech granitemoe granitemoehybrid granitemoeshared gritlm grok1 h2ovl hunyuan_v1 hunyuan_vision hyperclovax_vision idefics2_vision_model idefics3 interfaces interfaces_base intern_vit internlm2 internlm2_ve interns1 interns1_vit internvl jais jais2 jamba jina_vl keye keye_vl1_5 kimi_linear kimi_vl lfm2 lfm2_moe lightonocr llama llama4 llama4_eagle llama_eagle llama_eagle3 llava llava_next llava_next_video llava_onevision longcat_flash longcat_flash_mtp mamba mamba2 medusa midashenglm mimo mimo_mtp mimo_v2_flash minicpm minicpm3 minicpm_eagle minicpmo minicpmv minimax_m2 minimax_text_01 minimax_vl_01 mistral3 mistral_large_3 mistral_large_3_eagle mixtral mllama4 mlp_speculator modernbert module_mapping molmo moonvit mpt nano_nemotron_vl nemotron nemotron_h nemotron_nas nemotron_vl nvlm_d olmo olmo2 olmoe opencua openpangu openpangu_mtp opt orion ouro ovis ovis2_5 paddleocr_vl paligemma persimmon phi phi3 phi3v phi4mm phi4mm_audio phi4mm_utils phimoe pixtral plamo2 plamo3 qwen qwen2 qwen2_5_omni_thinker qwen2_5_vl qwen2_audio qwen2_moe qwen2_rm qwen2_vl qwen3 qwen3_moe qwen3_next qwen3_next_mtp qwen3_omni_moe_thinker qwen3_vl qwen3_vl_moe qwen_vl radio registry roberta rvl seed_oss siglip siglip2navit skyworkr1v smolvlm solar stablelm starcoder2 step3_text step3_vl swin tarsier telechat2 teleflm terratorch ultravox utils vision voxtral voxtral_streaming whisper whisper_utils zamba2 transformers transformers base causal legacy moe multimodal pooling utils warmup warmup deep_gemm_warmup kernel_warmup multimodal multimodal audio base cache evs hasher image inputs parse processing profiling registry utils video platforms platforms cpu cuda interface rocm tpu xpu plugins plugins io_processors io_processors interface lora_resolvers lora_resolvers filesystem_resolver profiler profiler layerwise_profile utils wrapper ray ray lazy_utils ray_env reasoning reasoning abs_reasoning_parsers basic_parsers deepseek_r1_reasoning_parser deepseek_v3_reasoning_parser ernie45_reasoning_parser glm4_moe_reasoning_parser gptoss_reasoning_parser granite_reasoning_parser holo2_reasoning_parser hunyuan_a13b_reasoning_parser identity_reasoning_parser minimax_m2_reasoning_parser mistral_reasoning_parser olmo3_reasoning_parser qwen3_reasoning_parser seedoss_reasoning_parser step3_reasoning_parser tokenizers tokenizers deepseek_v32 deepseek_v32_encoding detokenizer_utils hf mistral protocol registry tool_parsers tool_parsers abstract_tool_parser deepseekv3_tool_parser deepseekv31_tool_parser deepseekv32_tool_parser ernie45_tool_parser functiongemma_tool_parser gigachat3_tool_parser glm4_moe_tool_parser glm47_moe_tool_parser granite_20b_fc_tool_parser granite_tool_parser hermes_tool_parser hunyuan_a13b_tool_parser internlm2_tool_parser jamba_tool_parser kimi_k2_tool_parser llama4_pythonic_tool_parser llama_tool_parser longcat_tool_parser minimax_m2_tool_parser minimax_tool_parser mistral_tool_parser olmo3_tool_parser openai_tool_parser phi4mini_tool_parser pythonic_tool_parser qwen3coder_tool_parser qwen3xml_tool_parser seed_oss_tool_parser step3_tool_parser utils xlam_tool_parser transformers_utils transformers_utils config config_parser_base dynamic_module gguf_utils processor repo_utils runai_utils s3_utils tokenizer utils chat_templates chat_templates registry configs configs afmoe arctic bagel chatglm deepseek_vl2 dotsocr eagle falcon flex_olmo hunyuan_vl jais kimi_linear kimi_vl lfm2_moe medusa midashenglm mistral mlp_speculator moonvit nemotron nemotron_h olmo3 ovis qwen3_next radio step3_vl tarsier2 ultravox speculators speculators algos base processors processors bagel deepseek_ocr deepseek_vl2 hunyuan_vl hunyuan_vl_image ovis ovis2_5 triton_utils triton_utils importing usage usage usage_lib utils utils argparse_utils async_utils cache collection_utils counter deep_gemm flashinfer func_utils gc_utils hashing import_utils jsontree math_utils mem_constants mem_utils nccl network_utils nvtx_pytorch_hooks platform_utils profiling registry serial_utils system_utils tensor_schema torch_utils v1 v1 cudagraph_dispatcher kv_cache_interface outputs request serial_utils utils attention attention backends backends cpu_attn flash_attn flashinfer flex_attention gdn_attn linear_attn mamba1_attn mamba2_attn mamba_attn pallas rocm_aiter_fa rocm_aiter_unified_attn rocm_attn short_conv_attn tree_attn triton_attn utils mla mla aiter_triton_mla common cutlass_mla flashattn_mla flashinfer_mla flashmla flashmla_sparse indexer rocm_aiter_mla rocm_aiter_mla_sparse triton_mla core core block_pool encoder_cache_manager kv_cache_coordinator kv_cache_manager kv_cache_metrics kv_cache_utils single_type_kv_cache_manager sched sched async_scheduler interface output request_queue scheduler utils engine engine async_llm coordinator core core_client detokenizer exceptions input_processor llm_engine logprobs output_processor parallel_sampling utils executor executor abstract multiproc_executor ray_distributed_executor ray_executor ray_utils uniproc_executor kv_offload kv_offload abstract arc_manager backend cpu factory lru_manager mediums spec backends backends cpu worker worker cpu_gpu worker metrics metrics loggers perf prometheus ray_wrappers reader stats pool pool metadata sample sample metadata rejection_sampler sampler logits_processor logits_processor builtin interface state ops ops bad_words logprobs penalties topk_topp_sampler tpu tpu metadata sampler spec_decode spec_decode eagle medusa metadata metrics ngram_proposer suffix_decoding utils structured_output structured_output backend_guidance backend_lm_format_enforcer backend_outlines backend_types backend_xgrammar request utils worker worker block_table cp_utils cpu_model_runner cpu_worker dp_utils ec_connector_model_runner_mixin gpu_input_batch gpu_model_runner gpu_ubatch_wrapper gpu_worker kv_connector_model_runner_mixin lora_model_runner_mixin tpu_input_batch tpu_model_runner tpu_worker ubatch_utils ubatching utils worker_base workspace xpu_model_runner xpu_worker gpu gpu async_utils attn_utils block_table cudagraph_utils dp_utils input_batch model_runner states structured_outputs metrics metrics logits sample sample gumbel logprob metadata min_p output penalties sampler spec_decode spec_decode eagle eagle_cudagraph rejection_sample CLI Reference CLI Reference vllm serve vllm chat vllm complete vllm run-batch vllm bench vllm bench vllm bench latency vllm bench serve vllm bench sweep plot vllm bench sweep plot_pareto vllm bench sweep serve vllm bench sweep serve_sla vllm bench throughput Community Community Contact Us Meetups Sponsors Governance Governance Collaboration Policy Committers Governance Process Blog Forum Slack Table of contents AnyResponseFormat AnyStructuralTagResponseFormat AudioResponseFormat LogitsProcessors ResponseInputOutputItem ResponseInputOutputMessage StreamingResponsesResponse TokenizeRequest TranscriptionResponseVariant TranslationResponseVariant _LONG_INFO logger ChatCompletionLogProb bytes logprob token ChatCompletionLogProbs content ChatCompletionLogProbsContent field_names top_logprobs ChatCompletionNamedFunction name ChatCompletionNamedToolChoiceParam function type ChatCompletionRequest _DEFAULT_SAMPLING_PARAMS add_generation_prompt add_special_tokens allowed_token_ids bad_words cache_salt chat_template chat_template_kwargs continue_final_message documents echo frequency_penalty ignore_eos include_reasoning include_stop_str_in_output kv_transfer_params length_penalty logit_bias logits_processors logprobs max_completion_tokens max_tokens messages min_p min_tokens mm_processor_kwargs model n parallel_tool_calls presence_penalty priority prompt_logprobs reasoning_effort repetition_penalty request_id response_format return_token_ids return_tokens_as_token_ids seed skip_special_tokens spaces_between_special_tokens stop stop_token_ids stream stream_options structured_outputs temperature tool_choice tools top_k top_logprobs top_p truncate_prompt_tokens use_beam_search user vllm_xargs check_cache_salt_support check_generation_prompt check_logprobs check_structured_outputs_count check_tool_usage to_beam_search_params to_sampling_params validate_stream_options ChatCompletionResponse choices created id kv_transfer_params model object prompt_logprobs prompt_token_ids service_tier system_fingerprint usage ChatCompletionResponseChoice finish_reason index logprobs message stop_reason token_ids ChatCompletionResponseStreamChoice delta finish_reason index logprobs stop_reason token_ids ChatCompletionStreamResponse choices created id model object prompt_token_ids usage ChatCompletionToolsParam function type ChatMessage annotations audio content function_call reasoning reasoning_content refusal role tool_calls handle_deprecated_reasoning_content CompletionLogProbs text_offset token_logprobs tokens top_logprobs CompletionRequest _DEFAULT_SAMPLING_PARAMS add_special_tokens allowed_token_ids cache_salt echo frequency_penalty ignore_eos include_stop_str_in_output kv_transfer_params length_penalty logit_bias logits_processors logprobs max_tokens min_p min_tokens model n presence_penalty priority prompt prompt_embeds prompt_logprobs repetition_penalty request_id response_format return_token_ids return_tokens_as_token_ids seed skip_special_tokens spaces_between_special_tokens stop stop_token_ids stream stream_options structured_outputs suffix temperature top_k top_p truncate_prompt_tokens use_beam_search user vllm_xargs check_cache_salt_support check_logprobs check_structured_outputs_count to_beam_search_params to_sampling_params validate_prompt_and_prompt_embeds validate_stream_options CompletionResponse choices created id kv_transfer_params model object service_tier system_fingerprint usage CompletionResponseChoice finish_reason index logprobs prompt_logprobs prompt_token_ids stop_reason text token_ids CompletionResponseStreamChoice finish_reason index logprobs prompt_token_ids stop_reason text token_ids CompletionStreamResponse choices created id model object usage DeltaFunctionCall arguments name DeltaMessage content reasoning reasoning_content role tool_calls handle_deprecated_reasoning_content DeltaToolCall function id index type DetokenizeRequest model tokens DetokenizeResponse prompt ErrorInfo code message param type ErrorResponse error ExtractedToolCallInformation content tool_calls tools_called FunctionCall arguments name FunctionDefinition description name parameters GenerateRequest cache_salt features kv_transfer_params model priority request_id sampling_params stream stream_options token_ids GenerateResponse choices kv_transfer_params prompt_logprobs request_id GenerateResponseChoice finish_reason index logprobs token_ids InputTokensDetails cached_tokens cached_tokens_per_turn input_tokens_per_turn JsonSchemaResponseFormat description json_schema name strict LegacyStructuralTag begin end structural_tag_schema LegacyStructuralTagResponseFormat structures triggers type LoadLoRAAdapterRequest lora_name lora_path LogitsProcessorConstructor args kwargs model_config qualname ModelCard created id max_model_len object owned_by parent permission root ModelList data object ModelPermission allow_create_engine allow_fine_tuning allow_logprobs allow_sampling allow_search_indices allow_view created group id is_blocking object organization OpenAIBaseModel field_names model_config __log_extra_fields__ OutputTokensDetails output_tokens_per_turn reasoning_tokens tool_output_tokens tool_output_tokens_per_turn PromptTokenUsageInfo cached_tokens RequestResponseMetadata final_usage_info request_id ResponseCompletedEvent response ResponseCreatedEvent response ResponseFormat json_schema type ResponseInProgressEvent response ResponseRawMessageAndToken message tokens type ResponseReasoningPartAddedEvent content_index item_id output_index part sequence_number type ResponseReasoningPartDoneEvent content_index item_id output_index part sequence_number type ResponseUsage input_tokens input_tokens_details output_tokens output_tokens_details total_tokens ResponsesRequest _DEFAULT_SAMPLING_PARAMS background cache_salt enable_response_messages include input instructions logit_bias max_output_tokens max_tool_calls metadata mm_processor_kwargs model parallel_tool_calls previous_input_messages previous_response_id priority prompt reasoning request_id service_tier store stream temperature text tool_choice tools top_k top_logprobs top_p truncation user check_cache_salt_support function_call_parsing is_include_output_logprobs to_sampling_params validate_background validate_prompt ResponsesResponse background created_at id incomplete_details input_messages instructions max_output_tokens max_tool_calls metadata model object output output_messages parallel_tool_calls previous_response_id prompt reasoning service_tier status temperature text tool_choice tools top_logprobs top_p truncation usage user from_request serialize_input_messages serialize_output_messages StreamOptions continuous_usage_stats include_usage StructuralTagResponseFormat format type TokenizeChatRequest add_generation_prompt add_special_tokens chat_template chat_template_kwargs continue_final_message messages mm_processor_kwargs model return_token_strs tools check_generation_prompt TokenizeCompletionRequest add_special_tokens model prompt return_token_strs TokenizeResponse count max_model_len token_strs tokens TokenizerInfoResponse model_config tokenizer_class ToolCall function id type TranscriptionRequest _DEFAULT_SAMPLING_PARAMS file frequency_penalty language max_completion_tokens min_p model presence_penalty prompt repetition_penalty response_format seed stream stream_continuous_usage_stats stream_include_usage temperature timestamp_granularities to_language top_k top_p vllm_xargs to_sampling_params validate_transcription_request TranscriptionResponse text usage TranscriptionResponseStreamChoice delta finish_reason stop_reason TranscriptionResponseVerbose duration language segments text words TranscriptionSegment avg_logprob compression_ratio end id no_speech_prob seek start temperature text tokens TranscriptionStreamResponse choices created id model object usage TranscriptionUsageAudio seconds type TranscriptionWord end start word TranslationRequest _DEFAULT_SAMPLING_PARAMS file language max_completion_tokens model prompt response_format seed stream stream_continuous_usage_stats stream_include_usage temperature to_language to_sampling_params validate_stream_options TranslationResponse text TranslationResponseStreamChoice delta finish_reason stop_reason TranslationResponseVerbose duration language segments text words TranslationSegment avg_logprob compression_ratio end id no_speech_prob seek start temperature text tokens TranslationStreamResponse choices created id model object usage TranslationWord end start word UnloadLoRAAdapterRequest lora_int_id lora_name UsageInfo completion_tokens prompt_tokens prompt_tokens_details total_tokens VLLMValidationError parameter value __init__ __str__ get_logits_processors serialize_message serialize_messages vllm.entrypoints.openai.protocol ¶ AnyResponseFormat module-attribute ¶ AnyResponseFormat: TypeAlias = ( ResponseFormat | StructuralTagResponseFormat | LegacyStructuralTagResponseFormat ) AnyStructuralTagResponseFormat module-attribute ¶ AnyStructuralTagResponseFormat: TypeAlias = ( LegacyStructuralTagResponseFormat | StructuralTagResponseFormat ) AudioResponseFormat module-attribute ¶ AudioResponseFormat: TypeAlias = Literal[ \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\" ] LogitsProcessors module-attribute ¶ LogitsProcessors = list[str | LogitsProcessorConstructor] ResponseInputOutputItem module-attribute ¶ ResponseInputOutputItem: TypeAlias = ( ResponseInputItemParam | ResponseOutputItem ) ResponseInputOutputMessage module-attribute ¶ ResponseInputOutputMessage: TypeAlias = ( list[ChatCompletionMessageParam] | list[ResponseRawMessageAndToken] ) StreamingResponsesResponse module-attribute ¶ StreamingResponsesResponse: TypeAlias = ( ResponseCreatedEvent | ResponseInProgressEvent | ResponseCompletedEvent | ResponseOutputItemAddedEvent | ResponseOutputItemDoneEvent | ResponseContentPartAddedEvent | ResponseContentPartDoneEvent | ResponseReasoningTextDeltaEvent | ResponseReasoningTextDoneEvent | ResponseReasoningPartAddedEvent | ResponseReasoningPartDoneEvent | ResponseCodeInterpreterCallInProgressEvent | ResponseCodeInterpreterCallCodeDeltaEvent | ResponseWebSearchCallInProgressEvent | ResponseWebSearchCallSearchingEvent | ResponseWebSearchCallCompletedEvent | ResponseCodeInterpreterCallCodeDoneEvent | ResponseCodeInterpreterCallInterpretingEvent | ResponseCodeInterpreterCallCompletedEvent | ResponseMcpCallArgumentsDeltaEvent | ResponseMcpCallArgumentsDoneEvent | ResponseMcpCallInProgressEvent | ResponseMcpCallCompletedEvent ) TokenizeRequest module-attribute ¶ TokenizeRequest: TypeAlias = ( TokenizeCompletionRequest | TokenizeChatRequest ) TranscriptionResponseVariant module-attribute ¶ TranscriptionResponseVariant: TypeAlias = ( TranscriptionResponse | TranscriptionResponseVerbose ) TranslationResponseVariant module-attribute ¶ TranslationResponseVariant: TypeAlias = ( TranslationResponse | TranslationResponseVerbose ) _LONG_INFO module-attribute ¶ _LONG_INFO = iinfo(long) logger module-attribute ¶ logger = init_logger(__name__) ChatCompletionLogProb ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1535 1536 1537 1538class ChatCompletionLogProb(OpenAIBaseModel): token: str logprob: float = -9999.0 bytes: list[int] | None = None bytes class-attribute instance-attribute ¶ bytes: list[int] | None = None logprob class-attribute instance-attribute ¶ logprob: float = -9999.0 token instance-attribute ¶ token: str ChatCompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1548 1549class ChatCompletionLogProbs(OpenAIBaseModel): content: list[ChatCompletionLogProbsContent] | None = None content class-attribute instance-attribute ¶ content: list[ChatCompletionLogProbsContent] | None = None ChatCompletionLogProbsContent ¶ Bases: ChatCompletionLogProb Source code in vllm/entrypoints/openai/protocol.py 1541 1542 1543 1544 1545class ChatCompletionLogProbsContent(ChatCompletionLogProb): # Workaround: redefine fields name cache so that it's not # shared with the super class. field_names: ClassVar[set[str] | None] = None top_logprobs: list[ChatCompletionLogProb] = Field(default_factory=list) field_names class-attribute ¶ field_names: set[str] | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[ChatCompletionLogProb] = Field( default_factory=list ) ChatCompletionNamedFunction ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 271 272class ChatCompletionNamedFunction(OpenAIBaseModel): name: str name instance-attribute ¶ name: str ChatCompletionNamedToolChoiceParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 275 276 277class ChatCompletionNamedToolChoiceParam(OpenAIBaseModel): function: ChatCompletionNamedFunction type: Literal[\"function\"] = \"function\" function instance-attribute ¶ function: ChatCompletionNamedFunction type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054class ChatCompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/chat/create messages: list[ChatCompletionMessageParam] model: str | None = None frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: bool | None = False top_logprobs: int | None = 0 max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of \" \"the max_completion_tokens field\", ) max_completion_tokens: int | None = None n: int | None = 1 presence_penalty: float | None = 0.0 response_format: AnyResponseFormat | None = None seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None temperature: float | None = None top_p: float | None = None tools: list[ChatCompletionToolsParam] | None = None tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" reasoning_effort: Literal[\"low\", \"medium\", \"high\"] | None = None include_reasoning: bool = True parallel_tool_calls: bool | None = True # NOTE this will be ignored by vLLM user: str | None = None # --8<-- [start:chat-completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None prompt_logprobs: int | None = None allowed_token_ids: list[int] | None = None bad_words: list[str] = Field(default_factory=list) # --8<-- [end:chat-completion-sampling-params] # --8<-- [start:chat-completion-extra-params] echo: bool = Field( default=False, description=( \"If true, the new message will be prepended with the last message \" \"if they belong to the same role.\" ), ) add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) documents: list[dict[str, str]] | None = Field( default=None, description=( \"A list of dicts representing documents that will be accessible to \" \"the model if it is performing RAG (retrieval-augmented generation).\" \" If the template does not support RAG, this argument will have no \" \"effect. We recommend that each document should be a dict containing \" '\"title\" and \"text\" keys.' ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float | list[str | int | float]] | None = Field( default=None, description=( \"Additional request parameters with (list of) string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:chat-completion-extra-params] # Default sampling parameters for chat completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data @model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None bad_words class-attribute instance-attribute ¶ bad_words: list[str] = Field(default_factory=list) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) documents class-attribute instance-attribute ¶ documents: list[dict[str, str]] | None = Field( default=None, description='A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.', ) echo class-attribute instance-attribute ¶ echo: bool = Field( default=False, description=\"If true, the new message will be prepended with the last message if they belong to the same role.\", ) frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_reasoning class-attribute instance-attribute ¶ include_reasoning: bool = True include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: bool | None = False max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of the max_completion_tokens field\", ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int | None = 1 parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None reasoning_effort class-attribute instance-attribute ¶ reasoning_effort: ( Literal[\"low\", \"medium\", \"high\"] | None ) = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = None return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) temperature class-attribute instance-attribute ¶ temperature: float | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: ( dict[str, str | int | float | list[str | int | float]] | None ) = Field( default=None, description=\"Additional request parameters with (list of) string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1035 1036 1037 1038 1039 1040 1041 1042 1043@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data check_tool_usage classmethod ¶ check_tool_usage(data) Source code in vllm/entrypoints/openai/protocol.py 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033@model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 883 884 885 886 887 888 889 890 891 892@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data ChatCompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580class ChatCompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion\"] = \"chat.completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[ChatCompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['chat.completion'] = 'chat.completion' prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo ChatCompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562class ChatCompletionResponseChoice(OpenAIBaseModel): index: int message: ChatMessage logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" # not part of the OpenAI spec but included in vLLM for legacy reasons stop_reason: int | str | None = None # not part of the OpenAI spec but is useful for tracing the tokens # in agent scenarios token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None message instance-attribute ¶ message: ChatMessage stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1598 1599 1600 1601 1602 1603 1604 1605class ChatCompletionResponseStreamChoice(OpenAIBaseModel): index: int delta: DeltaMessage logprobs: ChatCompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = None # not part of the OpenAI spec but for tracing the tokens token_ids: list[int] | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1608 1609 1610 1611 1612 1613 1614 1615 1616class ChatCompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) # not part of the OpenAI spec but for tracing the tokens prompt_token_ids: list[int] | None = None choices instance-attribute ¶ choices: list[ChatCompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"chat.completion.chunk\"] = ( \"chat.completion.chunk\" ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) ChatCompletionToolsParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 266 267 268class ChatCompletionToolsParam(OpenAIBaseModel): type: Literal[\"function\"] = \"function\" function: FunctionDefinition function instance-attribute ¶ function: FunctionDefinition type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532class ChatMessage(OpenAIBaseModel): role: str content: str | None = None refusal: str | None = None annotations: OpenAIAnnotation | None = None audio: OpenAIChatCompletionAudio | None = None function_call: FunctionCall | None = None tool_calls: list[ToolCall] = Field(default_factory=list) # vLLM-specific fields that are not in OpenAI spec reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self annotations class-attribute instance-attribute ¶ annotations: Annotation | None = None audio class-attribute instance-attribute ¶ audio: ChatCompletionAudio | None = None content class-attribute instance-attribute ¶ content: str | None = None function_call class-attribute instance-attribute ¶ function_call: FunctionCall | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. refusal class-attribute instance-attribute ¶ refusal: str | None = None role instance-attribute ¶ role: str tool_calls class-attribute instance-attribute ¶ tool_calls: list[ToolCall] = Field(default_factory=list) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1528 1529 1530 1531 1532@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self CompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1409 1410 1411 1412 1413class CompletionLogProbs(OpenAIBaseModel): text_offset: list[int] = Field(default_factory=list) token_logprobs: list[float | None] = Field(default_factory=list) tokens: list[str] = Field(default_factory=list) top_logprobs: list[dict[str, float] | None] = Field(default_factory=list) text_offset class-attribute instance-attribute ¶ text_offset: list[int] = Field(default_factory=list) token_logprobs class-attribute instance-attribute ¶ token_logprobs: list[float | None] = Field( default_factory=list ) tokens class-attribute instance-attribute ¶ tokens: list[str] = Field(default_factory=list) top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[dict[str, float] | None] = Field( default_factory=list ) CompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406class CompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/completions/create model: str | None = None prompt: list[int] | list[list[int]] | str | list[str] | None = None echo: bool | None = False frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: int | None = None max_tokens: int | None = 16 n: int = 1 presence_penalty: float | None = 0.0 seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None suffix: str | None = None temperature: float | None = None top_p: float | None = None user: str | None = None # --8<-- [start:completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None allowed_token_ids: list[int] | None = None prompt_logprobs: int | None = None # --8<-- [end:completion-sampling-params] # --8<-- [start:completion-extra-params] prompt_embeds: bytes | list[bytes] | None = None add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) response_format: AnyResponseFormat | None = Field( default=None, description=( \"Similar to chat completion, this parameter specifies the format \" \"of output. Only {'type': 'json_object'}, {'type': 'json_schema'}\" \", {'type': 'structural_tag'}, or {'type': 'text' } is supported.\" ), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:completion-extra-params] # Default sampling parameters for completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) echo class-attribute instance-attribute ¶ echo: bool | None = False frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = 16 min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int = 1 presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ( list[int] | list[list[int]] | str | list[str] | None ) = None prompt_embeds class-attribute instance-attribute ¶ prompt_embeds: bytes | list[bytes] | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = Field( default=None, description=\"Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'}, {'type': 'json_schema'}, {'type': 'structural_tag'}, or {'type': 'text' } is supported.\", ) return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) suffix class-attribute instance-attribute ¶ suffix: str | None = None temperature class-attribute instance-attribute ¶ temperature: float | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_prompt_and_prompt_embeds classmethod ¶ validate_prompt_and_prompt_embeds(data) Source code in vllm/entrypoints/openai/protocol.py 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395@model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data CompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447class CompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: Literal[\"text_completion\"] = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[CompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['text_completion'] = 'text_completion' service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo CompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431class CompletionResponseChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) token_ids: list[int] | None = None # For response prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None # For prompt finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466class CompletionResponseStreamChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) # not part of the OpenAI spec but for tracing the tokens # prompt tokens is put into choice to align with CompletionResponseChoice prompt_token_ids: list[int] | None = None token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1469 1470 1471 1472 1473 1474 1475class CompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: str = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[CompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: str = 'text_completion' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) DeltaFunctionCall ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1489 1490 1491class DeltaFunctionCall(BaseModel): name: str | None = None arguments: str | None = None arguments class-attribute instance-attribute ¶ arguments: str | None = None name class-attribute instance-attribute ¶ name: str | None = None DeltaMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595class DeltaMessage(OpenAIBaseModel): role: str | None = None content: str | None = None reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" tool_calls: list[DeltaToolCall] = Field(default_factory=list) @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self content class-attribute instance-attribute ¶ content: str | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. role class-attribute instance-attribute ¶ role: str | None = None tool_calls class-attribute instance-attribute ¶ tool_calls: list[DeltaToolCall] = Field( default_factory=list ) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1591 1592 1593 1594 1595@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self DeltaToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1495 1496 1497 1498 1499class DeltaToolCall(OpenAIBaseModel): id: str | None = None type: Literal[\"function\"] | None = None index: int function: DeltaFunctionCall | None = None function class-attribute instance-attribute ¶ function: DeltaFunctionCall | None = None id class-attribute instance-attribute ¶ id: str | None = None index instance-attribute ¶ index: int type class-attribute instance-attribute ¶ type: Literal['function'] | None = None DetokenizeRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1987 1988 1989class DetokenizeRequest(OpenAIBaseModel): model: str | None = None tokens: list[int] model class-attribute instance-attribute ¶ model: str | None = None tokens instance-attribute ¶ tokens: list[int] DetokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1992 1993class DetokenizeResponse(OpenAIBaseModel): prompt: str prompt instance-attribute ¶ prompt: str ErrorInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 123 124 125 126 127class ErrorInfo(OpenAIBaseModel): message: str type: str param: str | None = None code: int code instance-attribute ¶ code: int message instance-attribute ¶ message: str param class-attribute instance-attribute ¶ param: str | None = None type instance-attribute ¶ type: str ErrorResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 130 131class ErrorResponse(OpenAIBaseModel): error: ErrorInfo error instance-attribute ¶ error: ErrorInfo ExtractedToolCallInformation ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511class ExtractedToolCallInformation(BaseModel): # indicate if tools were called tools_called: bool # extracted tool calls tool_calls: list[ToolCall] # content - per OpenAI spec, content AND tool calls can be returned rarely # But some models will do this intentionally content: str | None = None content class-attribute instance-attribute ¶ content: str | None = None tool_calls instance-attribute ¶ tool_calls: list[ToolCall] tools_called instance-attribute ¶ tools_called: bool FunctionCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1478 1479 1480class FunctionCall(OpenAIBaseModel): name: str arguments: str arguments instance-attribute ¶ arguments: str name instance-attribute ¶ name: str FunctionDefinition ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 260 261 262 263class FunctionDefinition(OpenAIBaseModel): name: str description: str | None = None parameters: dict[str, Any] | None = None description class-attribute instance-attribute ¶ description: str | None = None name instance-attribute ¶ name: str parameters class-attribute instance-attribute ¶ parameters: dict[str, Any] | None = None GenerateRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564class GenerateRequest(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) token_ids: list[int] \"\"\"The token ids to generate text from.\"\"\" # features: MultiModalFeatureSpec # TODO (NickLucche): implement once Renderer work is completed features: str | None = None \"\"\"The processed MM inputs for the model.\"\"\" sampling_params: SamplingParams \"\"\"The sampling parameters for the model.\"\"\" model: str | None = None stream: bool | None = False stream_options: StreamOptions | None = None cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) features class-attribute instance-attribute ¶ features: str | None = None The processed MM inputs for the model. kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) model class-attribute instance-attribute ¶ model: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) sampling_params instance-attribute ¶ sampling_params: SamplingParams The sampling parameters for the model. stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None token_ids instance-attribute ¶ token_ids: list[int] The token ids to generate text from. GenerateResponse ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591class GenerateResponse(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) choices: list[GenerateResponseChoice] prompt_logprobs: list[dict[int, Logprob] | None] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) choices instance-attribute ¶ choices: list[GenerateResponseChoice] kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) GenerateResponseChoice ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2567 2568 2569 2570 2571 2572class GenerateResponseChoice(BaseModel): index: int logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None InputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1634 1635 1636 1637class InputTokensDetails(OpenAIBaseModel): cached_tokens: int input_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens instance-attribute ¶ cached_tokens: int cached_tokens_per_turn class-attribute instance-attribute ¶ cached_tokens_per_turn: list[int] = Field( default_factory=list ) input_tokens_per_turn class-attribute instance-attribute ¶ input_tokens_per_turn: list[int] = Field( default_factory=list ) JsonSchemaResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 211 212 213 214 215 216 217class JsonSchemaResponseFormat(OpenAIBaseModel): name: str description: str | None = None # schema is the field in openai but that causes conflicts with pydantic so # instead use json_schema with an alias json_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") strict: bool | None = None description class-attribute instance-attribute ¶ description: str | None = None json_schema class-attribute instance-attribute ¶ json_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) name instance-attribute ¶ name: str strict class-attribute instance-attribute ¶ strict: bool | None = None LegacyStructuralTag ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 220 221 222 223 224 225class LegacyStructuralTag(OpenAIBaseModel): begin: str # schema is the field, but that causes conflicts with pydantic so # instead use structural_tag_schema with an alias structural_tag_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") end: str begin instance-attribute ¶ begin: str end instance-attribute ¶ end: str structural_tag_schema class-attribute instance-attribute ¶ structural_tag_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) LegacyStructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 228 229 230 231class LegacyStructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] structures: list[LegacyStructuralTag] triggers: list[str] structures instance-attribute ¶ structures: list[LegacyStructuralTag] triggers instance-attribute ¶ triggers: list[str] type instance-attribute ¶ type: Literal['structural_tag'] LoadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2006 2007 2008class LoadLoRAAdapterRequest(BaseModel): lora_name: str lora_path: str lora_name instance-attribute ¶ lora_name: str lora_path instance-attribute ¶ lora_path: str LogitsProcessorConstructor ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 282 283 284 285 286 287class LogitsProcessorConstructor(BaseModel): qualname: str args: list[Any] | None = None kwargs: dict[str, Any] | None = None model_config = ConfigDict(extra=\"forbid\") args class-attribute instance-attribute ¶ args: list[Any] | None = None kwargs class-attribute instance-attribute ¶ kwargs: dict[str, Any] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='forbid') qualname instance-attribute ¶ qualname: str ModelCard ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 179 180 181 182 183 184 185 186 187class ModelCard(OpenAIBaseModel): id: str object: str = \"model\" created: int = Field(default_factory=lambda: int(time.time())) owned_by: str = \"vllm\" root: str | None = None parent: str | None = None max_model_len: int | None = None permission: list[ModelPermission] = Field(default_factory=list) created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id instance-attribute ¶ id: str max_model_len class-attribute instance-attribute ¶ max_model_len: int | None = None object class-attribute instance-attribute ¶ object: str = 'model' owned_by class-attribute instance-attribute ¶ owned_by: str = 'vllm' parent class-attribute instance-attribute ¶ parent: str | None = None permission class-attribute instance-attribute ¶ permission: list[ModelPermission] = Field( default_factory=list ) root class-attribute instance-attribute ¶ root: str | None = None ModelList ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 190 191 192class ModelList(OpenAIBaseModel): object: str = \"list\" data: list[ModelCard] = Field(default_factory=list) data class-attribute instance-attribute ¶ data: list[ModelCard] = Field(default_factory=list) object class-attribute instance-attribute ¶ object: str = 'list' ModelPermission ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 164 165 166 167 168 169 170 171 172 173 174 175 176class ModelPermission(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\") object: str = \"model_permission\" created: int = Field(default_factory=lambda: int(time.time())) allow_create_engine: bool = False allow_sampling: bool = True allow_logprobs: bool = True allow_search_indices: bool = False allow_view: bool = True allow_fine_tuning: bool = False organization: str = \"*\" group: str | None = None is_blocking: bool = False allow_create_engine class-attribute instance-attribute ¶ allow_create_engine: bool = False allow_fine_tuning class-attribute instance-attribute ¶ allow_fine_tuning: bool = False allow_logprobs class-attribute instance-attribute ¶ allow_logprobs: bool = True allow_sampling class-attribute instance-attribute ¶ allow_sampling: bool = True allow_search_indices class-attribute instance-attribute ¶ allow_search_indices: bool = False allow_view class-attribute instance-attribute ¶ allow_view: bool = True created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) group class-attribute instance-attribute ¶ group: str | None = None id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"modelperm-{random_uuid()}\" ) is_blocking class-attribute instance-attribute ¶ is_blocking: bool = False object class-attribute instance-attribute ¶ object: str = 'model_permission' organization class-attribute instance-attribute ¶ organization: str = '*' OpenAIBaseModel ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120class OpenAIBaseModel(BaseModel): # OpenAI API does allow extra fields model_config = ConfigDict(extra=\"allow\") # Cache class field names field_names: ClassVar[set[str] | None] = None @model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result field_names class-attribute ¶ field_names: set[str] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') __log_extra_fields__ classmethod ¶ __log_extra_fields__(data, handler) Source code in vllm/entrypoints/openai/protocol.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120@model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result OutputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1640 1641 1642 1643 1644class OutputTokensDetails(OpenAIBaseModel): reasoning_tokens: int = 0 tool_output_tokens: int = 0 output_tokens_per_turn: list[int] = Field(default_factory=list) tool_output_tokens_per_turn: list[int] = Field(default_factory=list) output_tokens_per_turn class-attribute instance-attribute ¶ output_tokens_per_turn: list[int] = Field( default_factory=list ) reasoning_tokens class-attribute instance-attribute ¶ reasoning_tokens: int = 0 tool_output_tokens class-attribute instance-attribute ¶ tool_output_tokens: int = 0 tool_output_tokens_per_turn class-attribute instance-attribute ¶ tool_output_tokens_per_turn: list[int] = Field( default_factory=list ) PromptTokenUsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 195 196class PromptTokenUsageInfo(OpenAIBaseModel): cached_tokens: int | None = None cached_tokens class-attribute instance-attribute ¶ cached_tokens: int | None = None RequestResponseMetadata ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 206 207 208class RequestResponseMetadata(BaseModel): request_id: str final_usage_info: UsageInfo | None = None final_usage_info class-attribute instance-attribute ¶ final_usage_info: UsageInfo | None = None request_id instance-attribute ¶ request_id: str ResponseCompletedEvent ¶ Bases: ResponseCompletedEvent Source code in vllm/entrypoints/openai/protocol.py 1845 1846class ResponseCompletedEvent(OpenAIResponseCompletedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseCreatedEvent ¶ Bases: ResponseCreatedEvent Source code in vllm/entrypoints/openai/protocol.py 1849 1850class ResponseCreatedEvent(OpenAIResponseCreatedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 244 245 246 247class ResponseFormat(OpenAIBaseModel): # type must be \"json_schema\", \"json_object\", or \"text\" type: Literal[\"text\", \"json_object\", \"json_schema\"] json_schema: JsonSchemaResponseFormat | None = None json_schema class-attribute instance-attribute ¶ json_schema: JsonSchemaResponseFormat | None = None type instance-attribute ¶ type: Literal['text', 'json_object', 'json_schema'] ResponseInProgressEvent ¶ Bases: ResponseInProgressEvent Source code in vllm/entrypoints/openai/protocol.py 1853 1854class ResponseInProgressEvent(OpenAIResponseInProgressEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseRawMessageAndToken ¶ Bases: OpenAIBaseModel Class to show the raw message. If message / tokens diverge, tokens is the source of truth Source code in vllm/entrypoints/openai/protocol.py 1675 1676 1677 1678 1679 1680 1681class ResponseRawMessageAndToken(OpenAIBaseModel): \"\"\"Class to show the raw message. If message / tokens diverge, tokens is the source of truth\"\"\" message: str tokens: list[int] type: Literal[\"raw_message_tokens\"] = \"raw_message_tokens\" message instance-attribute ¶ message: str tokens instance-attribute ¶ tokens: list[int] type class-attribute instance-attribute ¶ type: Literal['raw_message_tokens'] = 'raw_message_tokens' ResponseReasoningPartAddedEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840class ResponseReasoningPartAddedEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.added\"] \"\"\"The type of the event. Always `response.reasoning_part.added`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.added'] The type of the event. Always response.reasoning_part.added. ResponseReasoningPartDoneEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818class ResponseReasoningPartDoneEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.done\"] \"\"\"The type of the event. Always `response.reasoning_part.done`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.done'] The type of the event. Always response.reasoning_part.done. ResponseUsage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1647 1648 1649 1650 1651 1652class ResponseUsage(OpenAIBaseModel): input_tokens: int input_tokens_details: InputTokensDetails output_tokens: int output_tokens_details: OutputTokensDetails total_tokens: int input_tokens instance-attribute ¶ input_tokens: int input_tokens_details instance-attribute ¶ input_tokens_details: InputTokensDetails output_tokens instance-attribute ¶ output_tokens: int output_tokens_details instance-attribute ¶ output_tokens_details: OutputTokensDetails total_tokens instance-attribute ¶ total_tokens: int ResponsesRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555class ResponsesRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/responses/create background: bool | None = False include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input: str | list[ResponseInputOutputItem] instructions: str | None = None max_output_tokens: int | None = None max_tool_calls: int | None = None metadata: Metadata | None = None model: str | None = None logit_bias: dict[str, float] | None = None parallel_tool_calls: bool | None = True previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] = \"auto\" store: bool | None = True stream: bool | None = False temperature: float | None = None text: ResponseTextConfig | None = None tool_choice: ToolChoice = \"auto\" tools: list[Tool] = Field(default_factory=list) top_logprobs: int | None = 0 top_p: float | None = None top_k: int | None = None truncation: Literal[\"auto\", \"disabled\"] | None = \"disabled\" user: str | None = None # --8<-- [start:responses-extra-params] request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) enable_response_messages: bool = Field( default=False, description=( \"Dictates whether or not to return messages as part of the \" \"response object. Currently only supported for\" \"non-background and gpt-oss only. \" ), ) # similar to input_messages / output_messages in ResponsesResponse # we take in previous_input_messages (ie in harmony format) # this cannot be used in conjunction with previous_response_id # TODO: consider supporting non harmony messages as well previous_input_messages: list[OpenAIHarmonyMessage | dict] | None = None # --8<-- [end:responses-extra-params] _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) @model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data @model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data @model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data @model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } background class-attribute instance-attribute ¶ background: bool | None = False cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) enable_response_messages class-attribute instance-attribute ¶ enable_response_messages: bool = Field( default=False, description=\"Dictates whether or not to return messages as part of the response object. Currently only supported fornon-background and gpt-oss only. \", ) include class-attribute instance-attribute ¶ include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input instance-attribute ¶ input: str | list[ResponseInputOutputItem] instructions class-attribute instance-attribute ¶ instructions: str | None = None logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None max_output_tokens class-attribute instance-attribute ¶ max_output_tokens: int | None = None max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True previous_input_messages class-attribute instance-attribute ¶ previous_input_messages: list[Message | dict] | None = None previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) service_tier class-attribute instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] = \"auto\" store class-attribute instance-attribute ¶ store: bool | None = True stream class-attribute instance-attribute ¶ stream: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float | None = None text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ToolChoice = 'auto' tools class-attribute instance-attribute ¶ tools: list[Tool] = Field(default_factory=list) top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncation class-attribute instance-attribute ¶ truncation: Literal['auto', 'disabled'] | None = 'disabled' user class-attribute instance-attribute ¶ user: str | None = None check_cache_salt_support ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 505 506 507 508 509 510 511 512 513@model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data function_call_parsing ¶ function_call_parsing(data) Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. Source code in vllm/entrypoints/openai/protocol.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555@model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data is_include_output_logprobs ¶ is_include_output_logprobs() -> bool Check if the request includes output logprobs. Source code in vllm/entrypoints/openai/protocol.py 480 481 482 483 484 485 486 487def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_background ¶ validate_background(data) Source code in vllm/entrypoints/openai/protocol.py 489 490 491 492 493 494 495@model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data validate_prompt ¶ validate_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 497 498 499 500 501 502 503@model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data ResponsesResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796class ResponsesResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"resp_{random_uuid()}\") created_at: int = Field(default_factory=lambda: int(time.time())) # error: Optional[ResponseError] = None incomplete_details: IncompleteDetails | None = None instructions: str | None = None metadata: Metadata | None = None model: str object: Literal[\"response\"] = \"response\" output: list[ResponseOutputItem] parallel_tool_calls: bool temperature: float tool_choice: ToolChoice tools: list[Tool] top_p: float background: bool max_output_tokens: int max_tool_calls: int | None = None previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] status: ResponseStatus text: ResponseTextConfig | None = None top_logprobs: int | None = None truncation: Literal[\"auto\", \"disabled\"] usage: ResponseUsage | None = None user: str | None = None # --8<-- [start:responses-response-extra-params] # These are populated when enable_response_messages is set to True # NOTE: custom serialization is needed # see serialize_input_messages and serialize_output_messages input_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token input to model.\" ), ) output_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token output of model.\" ), ) # --8<-- [end:responses-response-extra-params] # NOTE: openAI harmony doesn't serialize TextContent properly, # TODO: this fixes for TextContent, but need to verify for tools etc # https://github.com/openai/harmony/issues/78 @field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) # NOTE: openAI harmony doesn't serialize TextContent properly, this fixes it # https://github.com/openai/harmony/issues/78 @field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) @classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) background instance-attribute ¶ background: bool created_at class-attribute instance-attribute ¶ created_at: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\" ) incomplete_details class-attribute instance-attribute ¶ incomplete_details: IncompleteDetails | None = None input_messages class-attribute instance-attribute ¶ input_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token input to model.\", ) instructions class-attribute instance-attribute ¶ instructions: str | None = None max_output_tokens instance-attribute ¶ max_output_tokens: int max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['response'] = 'response' output instance-attribute ¶ output: list[ResponseOutputItem] output_messages class-attribute instance-attribute ¶ output_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token output of model.\", ) parallel_tool_calls instance-attribute ¶ parallel_tool_calls: bool previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None service_tier instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] status instance-attribute ¶ status: ResponseStatus temperature instance-attribute ¶ temperature: float text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice instance-attribute ¶ tool_choice: ToolChoice tools instance-attribute ¶ tools: list[Tool] top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = None top_p instance-attribute ¶ top_p: float truncation instance-attribute ¶ truncation: Literal['auto', 'disabled'] usage class-attribute instance-attribute ¶ usage: ResponseUsage | None = None user class-attribute instance-attribute ¶ user: str | None = None from_request classmethod ¶ from_request( request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> ResponsesResponse Source code in vllm/entrypoints/openai/protocol.py 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796@classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) serialize_input_messages ¶ serialize_input_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1745 1746 1747@field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) serialize_output_messages ¶ serialize_output_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1739 1740 1741@field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) StreamOptions ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 255 256 257class StreamOptions(OpenAIBaseModel): include_usage: bool | None = True continuous_usage_stats: bool | None = False continuous_usage_stats class-attribute instance-attribute ¶ continuous_usage_stats: bool | None = False include_usage class-attribute instance-attribute ¶ include_usage: bool | None = True StructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 234 235 236class StructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] format: Any format instance-attribute ¶ format: Any type instance-attribute ¶ type: Literal['structural_tag'] TokenizeChatRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974class TokenizeChatRequest(OpenAIBaseModel): model: str | None = None messages: list[ChatCompletionMessageParam] add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=(\"A list of tools the model may call.\"), ) @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=\"A list of tools the model may call.\", ) check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1966 1967 1968 1969 1970 1971 1972 1973 1974@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data TokenizeCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900class TokenizeCompletionRequest(OpenAIBaseModel): model: str | None = None prompt: str add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) model class-attribute instance-attribute ¶ model: str | None = None prompt instance-attribute ¶ prompt: str return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) TokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1980 1981 1982 1983 1984class TokenizeResponse(OpenAIBaseModel): count: int max_model_len: int tokens: list[int] token_strs: list[str] | None = None count instance-attribute ¶ count: int max_model_len instance-attribute ¶ max_model_len: int token_strs class-attribute instance-attribute ¶ token_strs: list[str] | None = None tokens instance-attribute ¶ tokens: list[int] TokenizerInfoResponse ¶ Bases: OpenAIBaseModel Response containing tokenizer configuration equivalent to tokenizer_config.json Source code in vllm/entrypoints/openai/protocol.py 1996 1997 1998 1999 2000 2001 2002 2003class TokenizerInfoResponse(OpenAIBaseModel): \"\"\" Response containing tokenizer configuration equivalent to tokenizer_config.json \"\"\" model_config = ConfigDict(extra=\"allow\") tokenizer_class: str model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') tokenizer_class instance-attribute ¶ tokenizer_class: str ToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1483 1484 1485 1486class ToolCall(OpenAIBaseModel): id: str = Field(default_factory=make_tool_call_id) type: Literal[\"function\"] = \"function\" function: FunctionCall function instance-attribute ¶ function: FunctionCall id class-attribute instance-attribute ¶ id: str = Field(default_factory=make_tool_call_id) type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' TranscriptionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213class TranscriptionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranscription file: UploadFile \"\"\" The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" language: str | None = None \"\"\"The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" ## TODO (varun) : Support if set to 0, certain thresholds are met !! timestamp_granularities: list[Literal[\"word\", \"segment\"]] = Field( alias=\"timestamp_granularities[]\", default=[] ) \"\"\"The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. \"\"\" stream: bool | None = False \"\"\"When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # --8<-- [start:transcription-extra-params] # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:transcription-extra-params] to_language: str | None = None \"\"\"The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. \"\"\" # --8<-- [start:transcription-sampling-params] temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" top_p: float | None = None \"\"\"Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds `p`. \"\"\" top_k: int | None = None \"\"\"Limits sampling to the `k` most probable tokens at each step.\"\"\" min_p: float | None = None \"\"\"Filters out tokens with a probability lower than `min_p`, ensuring a minimum likelihood threshold during sampling. \"\"\" seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" frequency_penalty: float | None = 0.0 \"\"\"The frequency penalty to use for sampling.\"\"\" repetition_penalty: float | None = None \"\"\"The repetition penalty to use for sampling.\"\"\" presence_penalty: float | None = 0.0 \"\"\"The presence penalty to use for sampling.\"\"\" max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:transcription-sampling-params] # Default sampling parameters for transcription requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } file instance-attribute ¶ file: UploadFile The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 The frequency penalty to use for sampling. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. min_p class-attribute instance-attribute ¶ min_p: float | None = None Filters out tokens with a probability lower than min_p, ensuring a minimum likelihood threshold during sampling. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 The presence penalty to use for sampling. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None The repetition penalty to use for sampling. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. timestamp_granularities class-attribute instance-attribute ¶ timestamp_granularities: list[ Literal[\"word\", \"segment\"] ] = Field(alias=\"timestamp_granularities[]\", default=[]) The timestamp granularities to populate for this transcription. response_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. top_k class-attribute instance-attribute ¶ top_k: int | None = None Limits sampling to the k most probable tokens at each step. top_p class-attribute instance-attribute ¶ top_p: float | None = None Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds p. vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_transcription_request classmethod ¶ validate_transcription_request(data) Source code in vllm/entrypoints/openai/protocol.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213@model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranscriptionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2222 2223 2224 2225class TranscriptionResponse(OpenAIBaseModel): text: str \"\"\"The transcribed text.\"\"\" usage: TranscriptionUsageAudio text instance-attribute ¶ text: str The transcribed text. usage instance-attribute ¶ usage: TranscriptionUsageAudio TranscriptionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1619 1620 1621 1622class TranscriptionResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranscriptionResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295class TranscriptionResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The transcribed text.\"\"\" segments: list[TranscriptionSegment] | None = None \"\"\"Segments of the transcribed text and their corresponding details.\"\"\" words: list[TranscriptionWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranscriptionSegment] | None = None Segments of the transcribed text and their corresponding details. text instance-attribute ¶ text: str The transcribed text. words class-attribute instance-attribute ¶ words: list[TranscriptionWord] | None = None Extracted words and their corresponding timestamps. TranscriptionSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278class TranscriptionSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranscriptionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1625 1626 1627 1628 1629 1630 1631class TranscriptionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsc-{random_uuid()}\") object: Literal[\"transcription.chunk\"] = \"transcription.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranscriptionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranscriptionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsc-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"transcription.chunk\"] = ( \"transcription.chunk\" ) usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranscriptionUsageAudio ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2217 2218 2219class TranscriptionUsageAudio(OpenAIBaseModel): type: Literal[\"duration\"] = \"duration\" seconds: int seconds instance-attribute ¶ seconds: int type class-attribute instance-attribute ¶ type: Literal['duration'] = 'duration' TranscriptionWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2228 2229 2230 2231 2232 2233 2234 2235 2236class TranscriptionWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. TranslationRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435class TranslationRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranslation file: UploadFile \"\"\" The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" # TODO support additional sampling parameters # --8<-- [start:translation-sampling-params] seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" # --8<-- [end:translation-sampling-params] # --8<-- [start:translation-extra-params] language: str | None = None \"\"\"The language of the input audio we translate from. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy. \"\"\" to_language: str | None = None \"\"\"The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports `to_language=en`. \"\"\" stream: bool | None = False \"\"\"Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:translation-extra-params] # Default sampling parameters for translation requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"temperature\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = {'temperature': 0} file instance-attribute ¶ file: UploadFile The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio we translate from. Supplying the input language in ISO-639-1 format will improve accuracy. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports to_language=en. to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranslationResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2439 2440 2441class TranslationResponse(OpenAIBaseModel): text: str \"\"\"The translated text.\"\"\" text instance-attribute ¶ text: str The translated text. TranslationResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2303 2304 2305 2306class TranslationResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranslationResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511class TranslationResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The translated text.\"\"\" segments: list[TranslationSegment] | None = None \"\"\"Segments of the translated text and their corresponding details.\"\"\" words: list[TranslationWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranslationSegment] | None = None Segments of the translated text and their corresponding details. text instance-attribute ¶ text: str The translated text. words class-attribute instance-attribute ¶ words: list[TranslationWord] | None = None Extracted words and their corresponding timestamps. TranslationSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494class TranslationSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranslationStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2309 2310 2311 2312 2313 2314 2315class TranslationStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsl-{random_uuid()}\") object: Literal[\"translation.chunk\"] = \"translation.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranslationResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranslationResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['translation.chunk'] = 'translation.chunk' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranslationWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2444 2445 2446 2447 2448 2449 2450 2451 2452class TranslationWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. UnloadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2011 2012 2013class UnloadLoRAAdapterRequest(BaseModel): lora_name: str lora_int_id: int | None = Field(default=None) lora_int_id class-attribute instance-attribute ¶ lora_int_id: int | None = Field(default=None) lora_name instance-attribute ¶ lora_name: str UsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 199 200 201 202 203class UsageInfo(OpenAIBaseModel): prompt_tokens: int = 0 total_tokens: int = 0 completion_tokens: int | None = 0 prompt_tokens_details: PromptTokenUsageInfo | None = None completion_tokens class-attribute instance-attribute ¶ completion_tokens: int | None = 0 prompt_tokens class-attribute instance-attribute ¶ prompt_tokens: int = 0 prompt_tokens_details class-attribute instance-attribute ¶ prompt_tokens_details: PromptTokenUsageInfo | None = None total_tokens class-attribute instance-attribute ¶ total_tokens: int = 0 VLLMValidationError ¶ Bases: ValueError vLLM-specific validation error for request validation failures. Parameters: Name Type Description Default message str The error message describing the validation failure. required parameter str | None Optional parameter name that failed validation. None value Any Optional value that was rejected during validation. None Source code in vllm/entrypoints/openai/protocol.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161class VLLMValidationError(ValueError): \"\"\"vLLM-specific validation error for request validation failures. Args: message: The error message describing the validation failure. parameter: Optional parameter name that failed validation. value: Optional value that was rejected during validation. \"\"\" def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base parameter instance-attribute ¶ parameter = parameter value instance-attribute ¶ value = value __init__ ¶ __init__( message: str, *, parameter: str | None = None, value: Any = None, ) -> None Source code in vllm/entrypoints/openai/protocol.py 143 144 145 146 147 148 149 150 151 152def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value __str__ ¶ __str__() Source code in vllm/entrypoints/openai/protocol.py 154 155 156 157 158 159 160 161def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base get_logits_processors ¶ get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None Source code in vllm/entrypoints/openai/protocol.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324def get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None: if processors and pattern: logits_processors = [] for processor in processors: qualname = processor if isinstance(processor, str) else processor.qualname if not re.match(pattern, qualname): raise ValueError( f\"Logits processor '{qualname}' is not allowed by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) try: logits_processor = resolve_obj_by_qualname(qualname) except Exception as e: raise ValueError( f\"Logits processor '{qualname}' could not be resolved: {e}\" ) from e if isinstance(processor, LogitsProcessorConstructor): logits_processor = logits_processor( *processor.args or [], **processor.kwargs or {} ) logits_processors.append(logits_processor) return logits_processors elif processors: raise ValueError( \"The `logits_processors` argument is not supported by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) return None serialize_message ¶ serialize_message(msg) Serializes a single message Source code in vllm/entrypoints/openai/protocol.py 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665def serialize_message(msg): \"\"\" Serializes a single message \"\"\" if isinstance(msg, dict): return msg elif hasattr(msg, \"to_dict\"): return msg.to_dict() else: # fallback to pyandic dump return msg.model_dump_json() serialize_messages ¶ serialize_messages(msgs) Serializes multiple messages Source code in vllm/entrypoints/openai/protocol.py 1668 1669 1670 1671 1672def serialize_messages(msgs): \"\"\" Serializes multiple messages \"\"\" return [serialize_message(msg) for msg in msgs] if msgs else None December 25, 2025",
      "code": ""
    },
    {
      "description": "vllm.entrypoints.openai.protocol ¶ AnyResponseFormat module-attribute ¶ AnyResponseFormat: TypeAlias = ( ResponseFormat | StructuralTagResponseFormat | LegacyStructuralTagResponseFormat ) AnyStructuralTagResponseFormat module-attribute ¶ AnyStructuralTagResponseFormat: TypeAlias = ( LegacyStructuralTagResponseFormat | StructuralTagResponseFormat ) AudioResponseFormat module-attribute ¶ AudioResponseFormat: TypeAlias = Literal[ \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\" ] LogitsProcessors module-attribute ¶ LogitsProcessors = list[str | LogitsProcessorConstructor] ResponseInputOutputItem module-attribute ¶ ResponseInputOutputItem: TypeAlias = ( ResponseInputItemParam | ResponseOutputItem ) ResponseInputOutputMessage module-attribute ¶ ResponseInputOutputMessage: TypeAlias = ( list[ChatCompletionMessageParam] | list[ResponseRawMessageAndToken] ) StreamingResponsesResponse module-attribute ¶ StreamingResponsesResponse: TypeAlias = ( ResponseCreatedEvent | ResponseInProgressEvent | ResponseCompletedEvent | ResponseOutputItemAddedEvent | ResponseOutputItemDoneEvent | ResponseContentPartAddedEvent | ResponseContentPartDoneEvent | ResponseReasoningTextDeltaEvent | ResponseReasoningTextDoneEvent | ResponseReasoningPartAddedEvent | ResponseReasoningPartDoneEvent | ResponseCodeInterpreterCallInProgressEvent | ResponseCodeInterpreterCallCodeDeltaEvent | ResponseWebSearchCallInProgressEvent | ResponseWebSearchCallSearchingEvent | ResponseWebSearchCallCompletedEvent | ResponseCodeInterpreterCallCodeDoneEvent | ResponseCodeInterpreterCallInterpretingEvent | ResponseCodeInterpreterCallCompletedEvent | ResponseMcpCallArgumentsDeltaEvent | ResponseMcpCallArgumentsDoneEvent | ResponseMcpCallInProgressEvent | ResponseMcpCallCompletedEvent ) TokenizeRequest module-attribute ¶ TokenizeRequest: TypeAlias = ( TokenizeCompletionRequest | TokenizeChatRequest ) TranscriptionResponseVariant module-attribute ¶ TranscriptionResponseVariant: TypeAlias = ( TranscriptionResponse | TranscriptionResponseVerbose ) TranslationResponseVariant module-attribute ¶ TranslationResponseVariant: TypeAlias = ( TranslationResponse | TranslationResponseVerbose ) _LONG_INFO module-attribute ¶ _LONG_INFO = iinfo(long) logger module-attribute ¶ logger = init_logger(__name__) ChatCompletionLogProb ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1535 1536 1537 1538class ChatCompletionLogProb(OpenAIBaseModel): token: str logprob: float = -9999.0 bytes: list[int] | None = None bytes class-attribute instance-attribute ¶ bytes: list[int] | None = None logprob class-attribute instance-attribute ¶ logprob: float = -9999.0 token instance-attribute ¶ token: str ChatCompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1548 1549class ChatCompletionLogProbs(OpenAIBaseModel): content: list[ChatCompletionLogProbsContent] | None = None content class-attribute instance-attribute ¶ content: list[ChatCompletionLogProbsContent] | None = None ChatCompletionLogProbsContent ¶ Bases: ChatCompletionLogProb Source code in vllm/entrypoints/openai/protocol.py 1541 1542 1543 1544 1545class ChatCompletionLogProbsContent(ChatCompletionLogProb): # Workaround: redefine fields name cache so that it's not # shared with the super class. field_names: ClassVar[set[str] | None] = None top_logprobs: list[ChatCompletionLogProb] = Field(default_factory=list) field_names class-attribute ¶ field_names: set[str] | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[ChatCompletionLogProb] = Field( default_factory=list ) ChatCompletionNamedFunction ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 271 272class ChatCompletionNamedFunction(OpenAIBaseModel): name: str name instance-attribute ¶ name: str ChatCompletionNamedToolChoiceParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 275 276 277class ChatCompletionNamedToolChoiceParam(OpenAIBaseModel): function: ChatCompletionNamedFunction type: Literal[\"function\"] = \"function\" function instance-attribute ¶ function: ChatCompletionNamedFunction type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054class ChatCompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/chat/create messages: list[ChatCompletionMessageParam] model: str | None = None frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: bool | None = False top_logprobs: int | None = 0 max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of \" \"the max_completion_tokens field\", ) max_completion_tokens: int | None = None n: int | None = 1 presence_penalty: float | None = 0.0 response_format: AnyResponseFormat | None = None seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None temperature: float | None = None top_p: float | None = None tools: list[ChatCompletionToolsParam] | None = None tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" reasoning_effort: Literal[\"low\", \"medium\", \"high\"] | None = None include_reasoning: bool = True parallel_tool_calls: bool | None = True # NOTE this will be ignored by vLLM user: str | None = None # --8<-- [start:chat-completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None prompt_logprobs: int | None = None allowed_token_ids: list[int] | None = None bad_words: list[str] = Field(default_factory=list) # --8<-- [end:chat-completion-sampling-params] # --8<-- [start:chat-completion-extra-params] echo: bool = Field( default=False, description=( \"If true, the new message will be prepended with the last message \" \"if they belong to the same role.\" ), ) add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) documents: list[dict[str, str]] | None = Field( default=None, description=( \"A list of dicts representing documents that will be accessible to \" \"the model if it is performing RAG (retrieval-augmented generation).\" \" If the template does not support RAG, this argument will have no \" \"effect. We recommend that each document should be a dict containing \" '\"title\" and \"text\" keys.' ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float | list[str | int | float]] | None = Field( default=None, description=( \"Additional request parameters with (list of) string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:chat-completion-extra-params] # Default sampling parameters for chat completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data @model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None bad_words class-attribute instance-attribute ¶ bad_words: list[str] = Field(default_factory=list) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) documents class-attribute instance-attribute ¶ documents: list[dict[str, str]] | None = Field( default=None, description='A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.', ) echo class-attribute instance-attribute ¶ echo: bool = Field( default=False, description=\"If true, the new message will be prepended with the last message if they belong to the same role.\", ) frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_reasoning class-attribute instance-attribute ¶ include_reasoning: bool = True include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: bool | None = False max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of the max_completion_tokens field\", ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int | None = 1 parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None reasoning_effort class-attribute instance-attribute ¶ reasoning_effort: ( Literal[\"low\", \"medium\", \"high\"] | None ) = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = None return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) temperature class-attribute instance-attribute ¶ temperature: float | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: ( dict[str, str | int | float | list[str | int | float]] | None ) = Field( default=None, description=\"Additional request parameters with (list of) string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1035 1036 1037 1038 1039 1040 1041 1042 1043@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data check_tool_usage classmethod ¶ check_tool_usage(data) Source code in vllm/entrypoints/openai/protocol.py 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033@model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 883 884 885 886 887 888 889 890 891 892@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data ChatCompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580class ChatCompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion\"] = \"chat.completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[ChatCompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['chat.completion'] = 'chat.completion' prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo ChatCompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562class ChatCompletionResponseChoice(OpenAIBaseModel): index: int message: ChatMessage logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" # not part of the OpenAI spec but included in vLLM for legacy reasons stop_reason: int | str | None = None # not part of the OpenAI spec but is useful for tracing the tokens # in agent scenarios token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None message instance-attribute ¶ message: ChatMessage stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1598 1599 1600 1601 1602 1603 1604 1605class ChatCompletionResponseStreamChoice(OpenAIBaseModel): index: int delta: DeltaMessage logprobs: ChatCompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = None # not part of the OpenAI spec but for tracing the tokens token_ids: list[int] | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1608 1609 1610 1611 1612 1613 1614 1615 1616class ChatCompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) # not part of the OpenAI spec but for tracing the tokens prompt_token_ids: list[int] | None = None choices instance-attribute ¶ choices: list[ChatCompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"chat.completion.chunk\"] = ( \"chat.completion.chunk\" ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) ChatCompletionToolsParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 266 267 268class ChatCompletionToolsParam(OpenAIBaseModel): type: Literal[\"function\"] = \"function\" function: FunctionDefinition function instance-attribute ¶ function: FunctionDefinition type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532class ChatMessage(OpenAIBaseModel): role: str content: str | None = None refusal: str | None = None annotations: OpenAIAnnotation | None = None audio: OpenAIChatCompletionAudio | None = None function_call: FunctionCall | None = None tool_calls: list[ToolCall] = Field(default_factory=list) # vLLM-specific fields that are not in OpenAI spec reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self annotations class-attribute instance-attribute ¶ annotations: Annotation | None = None audio class-attribute instance-attribute ¶ audio: ChatCompletionAudio | None = None content class-attribute instance-attribute ¶ content: str | None = None function_call class-attribute instance-attribute ¶ function_call: FunctionCall | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. refusal class-attribute instance-attribute ¶ refusal: str | None = None role instance-attribute ¶ role: str tool_calls class-attribute instance-attribute ¶ tool_calls: list[ToolCall] = Field(default_factory=list) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1528 1529 1530 1531 1532@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self CompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1409 1410 1411 1412 1413class CompletionLogProbs(OpenAIBaseModel): text_offset: list[int] = Field(default_factory=list) token_logprobs: list[float | None] = Field(default_factory=list) tokens: list[str] = Field(default_factory=list) top_logprobs: list[dict[str, float] | None] = Field(default_factory=list) text_offset class-attribute instance-attribute ¶ text_offset: list[int] = Field(default_factory=list) token_logprobs class-attribute instance-attribute ¶ token_logprobs: list[float | None] = Field( default_factory=list ) tokens class-attribute instance-attribute ¶ tokens: list[str] = Field(default_factory=list) top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[dict[str, float] | None] = Field( default_factory=list ) CompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406class CompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/completions/create model: str | None = None prompt: list[int] | list[list[int]] | str | list[str] | None = None echo: bool | None = False frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: int | None = None max_tokens: int | None = 16 n: int = 1 presence_penalty: float | None = 0.0 seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None suffix: str | None = None temperature: float | None = None top_p: float | None = None user: str | None = None # --8<-- [start:completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None allowed_token_ids: list[int] | None = None prompt_logprobs: int | None = None # --8<-- [end:completion-sampling-params] # --8<-- [start:completion-extra-params] prompt_embeds: bytes | list[bytes] | None = None add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) response_format: AnyResponseFormat | None = Field( default=None, description=( \"Similar to chat completion, this parameter specifies the format \" \"of output. Only {'type': 'json_object'}, {'type': 'json_schema'}\" \", {'type': 'structural_tag'}, or {'type': 'text' } is supported.\" ), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:completion-extra-params] # Default sampling parameters for completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) echo class-attribute instance-attribute ¶ echo: bool | None = False frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = 16 min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int = 1 presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ( list[int] | list[list[int]] | str | list[str] | None ) = None prompt_embeds class-attribute instance-attribute ¶ prompt_embeds: bytes | list[bytes] | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = Field( default=None, description=\"Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'}, {'type': 'json_schema'}, {'type': 'structural_tag'}, or {'type': 'text' } is supported.\", ) return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) suffix class-attribute instance-attribute ¶ suffix: str | None = None temperature class-attribute instance-attribute ¶ temperature: float | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_prompt_and_prompt_embeds classmethod ¶ validate_prompt_and_prompt_embeds(data) Source code in vllm/entrypoints/openai/protocol.py 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395@model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data CompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447class CompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: Literal[\"text_completion\"] = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[CompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['text_completion'] = 'text_completion' service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo CompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431class CompletionResponseChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) token_ids: list[int] | None = None # For response prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None # For prompt finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466class CompletionResponseStreamChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) # not part of the OpenAI spec but for tracing the tokens # prompt tokens is put into choice to align with CompletionResponseChoice prompt_token_ids: list[int] | None = None token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1469 1470 1471 1472 1473 1474 1475class CompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: str = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[CompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: str = 'text_completion' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) DeltaFunctionCall ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1489 1490 1491class DeltaFunctionCall(BaseModel): name: str | None = None arguments: str | None = None arguments class-attribute instance-attribute ¶ arguments: str | None = None name class-attribute instance-attribute ¶ name: str | None = None DeltaMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595class DeltaMessage(OpenAIBaseModel): role: str | None = None content: str | None = None reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" tool_calls: list[DeltaToolCall] = Field(default_factory=list) @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self content class-attribute instance-attribute ¶ content: str | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. role class-attribute instance-attribute ¶ role: str | None = None tool_calls class-attribute instance-attribute ¶ tool_calls: list[DeltaToolCall] = Field( default_factory=list ) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1591 1592 1593 1594 1595@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self DeltaToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1495 1496 1497 1498 1499class DeltaToolCall(OpenAIBaseModel): id: str | None = None type: Literal[\"function\"] | None = None index: int function: DeltaFunctionCall | None = None function class-attribute instance-attribute ¶ function: DeltaFunctionCall | None = None id class-attribute instance-attribute ¶ id: str | None = None index instance-attribute ¶ index: int type class-attribute instance-attribute ¶ type: Literal['function'] | None = None DetokenizeRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1987 1988 1989class DetokenizeRequest(OpenAIBaseModel): model: str | None = None tokens: list[int] model class-attribute instance-attribute ¶ model: str | None = None tokens instance-attribute ¶ tokens: list[int] DetokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1992 1993class DetokenizeResponse(OpenAIBaseModel): prompt: str prompt instance-attribute ¶ prompt: str ErrorInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 123 124 125 126 127class ErrorInfo(OpenAIBaseModel): message: str type: str param: str | None = None code: int code instance-attribute ¶ code: int message instance-attribute ¶ message: str param class-attribute instance-attribute ¶ param: str | None = None type instance-attribute ¶ type: str ErrorResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 130 131class ErrorResponse(OpenAIBaseModel): error: ErrorInfo error instance-attribute ¶ error: ErrorInfo ExtractedToolCallInformation ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511class ExtractedToolCallInformation(BaseModel): # indicate if tools were called tools_called: bool # extracted tool calls tool_calls: list[ToolCall] # content - per OpenAI spec, content AND tool calls can be returned rarely # But some models will do this intentionally content: str | None = None content class-attribute instance-attribute ¶ content: str | None = None tool_calls instance-attribute ¶ tool_calls: list[ToolCall] tools_called instance-attribute ¶ tools_called: bool FunctionCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1478 1479 1480class FunctionCall(OpenAIBaseModel): name: str arguments: str arguments instance-attribute ¶ arguments: str name instance-attribute ¶ name: str FunctionDefinition ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 260 261 262 263class FunctionDefinition(OpenAIBaseModel): name: str description: str | None = None parameters: dict[str, Any] | None = None description class-attribute instance-attribute ¶ description: str | None = None name instance-attribute ¶ name: str parameters class-attribute instance-attribute ¶ parameters: dict[str, Any] | None = None GenerateRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564class GenerateRequest(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) token_ids: list[int] \"\"\"The token ids to generate text from.\"\"\" # features: MultiModalFeatureSpec # TODO (NickLucche): implement once Renderer work is completed features: str | None = None \"\"\"The processed MM inputs for the model.\"\"\" sampling_params: SamplingParams \"\"\"The sampling parameters for the model.\"\"\" model: str | None = None stream: bool | None = False stream_options: StreamOptions | None = None cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) features class-attribute instance-attribute ¶ features: str | None = None The processed MM inputs for the model. kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) model class-attribute instance-attribute ¶ model: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) sampling_params instance-attribute ¶ sampling_params: SamplingParams The sampling parameters for the model. stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None token_ids instance-attribute ¶ token_ids: list[int] The token ids to generate text from. GenerateResponse ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591class GenerateResponse(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) choices: list[GenerateResponseChoice] prompt_logprobs: list[dict[int, Logprob] | None] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) choices instance-attribute ¶ choices: list[GenerateResponseChoice] kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) GenerateResponseChoice ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2567 2568 2569 2570 2571 2572class GenerateResponseChoice(BaseModel): index: int logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None InputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1634 1635 1636 1637class InputTokensDetails(OpenAIBaseModel): cached_tokens: int input_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens instance-attribute ¶ cached_tokens: int cached_tokens_per_turn class-attribute instance-attribute ¶ cached_tokens_per_turn: list[int] = Field( default_factory=list ) input_tokens_per_turn class-attribute instance-attribute ¶ input_tokens_per_turn: list[int] = Field( default_factory=list ) JsonSchemaResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 211 212 213 214 215 216 217class JsonSchemaResponseFormat(OpenAIBaseModel): name: str description: str | None = None # schema is the field in openai but that causes conflicts with pydantic so # instead use json_schema with an alias json_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") strict: bool | None = None description class-attribute instance-attribute ¶ description: str | None = None json_schema class-attribute instance-attribute ¶ json_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) name instance-attribute ¶ name: str strict class-attribute instance-attribute ¶ strict: bool | None = None LegacyStructuralTag ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 220 221 222 223 224 225class LegacyStructuralTag(OpenAIBaseModel): begin: str # schema is the field, but that causes conflicts with pydantic so # instead use structural_tag_schema with an alias structural_tag_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") end: str begin instance-attribute ¶ begin: str end instance-attribute ¶ end: str structural_tag_schema class-attribute instance-attribute ¶ structural_tag_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) LegacyStructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 228 229 230 231class LegacyStructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] structures: list[LegacyStructuralTag] triggers: list[str] structures instance-attribute ¶ structures: list[LegacyStructuralTag] triggers instance-attribute ¶ triggers: list[str] type instance-attribute ¶ type: Literal['structural_tag'] LoadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2006 2007 2008class LoadLoRAAdapterRequest(BaseModel): lora_name: str lora_path: str lora_name instance-attribute ¶ lora_name: str lora_path instance-attribute ¶ lora_path: str LogitsProcessorConstructor ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 282 283 284 285 286 287class LogitsProcessorConstructor(BaseModel): qualname: str args: list[Any] | None = None kwargs: dict[str, Any] | None = None model_config = ConfigDict(extra=\"forbid\") args class-attribute instance-attribute ¶ args: list[Any] | None = None kwargs class-attribute instance-attribute ¶ kwargs: dict[str, Any] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='forbid') qualname instance-attribute ¶ qualname: str ModelCard ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 179 180 181 182 183 184 185 186 187class ModelCard(OpenAIBaseModel): id: str object: str = \"model\" created: int = Field(default_factory=lambda: int(time.time())) owned_by: str = \"vllm\" root: str | None = None parent: str | None = None max_model_len: int | None = None permission: list[ModelPermission] = Field(default_factory=list) created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id instance-attribute ¶ id: str max_model_len class-attribute instance-attribute ¶ max_model_len: int | None = None object class-attribute instance-attribute ¶ object: str = 'model' owned_by class-attribute instance-attribute ¶ owned_by: str = 'vllm' parent class-attribute instance-attribute ¶ parent: str | None = None permission class-attribute instance-attribute ¶ permission: list[ModelPermission] = Field( default_factory=list ) root class-attribute instance-attribute ¶ root: str | None = None ModelList ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 190 191 192class ModelList(OpenAIBaseModel): object: str = \"list\" data: list[ModelCard] = Field(default_factory=list) data class-attribute instance-attribute ¶ data: list[ModelCard] = Field(default_factory=list) object class-attribute instance-attribute ¶ object: str = 'list' ModelPermission ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 164 165 166 167 168 169 170 171 172 173 174 175 176class ModelPermission(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\") object: str = \"model_permission\" created: int = Field(default_factory=lambda: int(time.time())) allow_create_engine: bool = False allow_sampling: bool = True allow_logprobs: bool = True allow_search_indices: bool = False allow_view: bool = True allow_fine_tuning: bool = False organization: str = \"*\" group: str | None = None is_blocking: bool = False allow_create_engine class-attribute instance-attribute ¶ allow_create_engine: bool = False allow_fine_tuning class-attribute instance-attribute ¶ allow_fine_tuning: bool = False allow_logprobs class-attribute instance-attribute ¶ allow_logprobs: bool = True allow_sampling class-attribute instance-attribute ¶ allow_sampling: bool = True allow_search_indices class-attribute instance-attribute ¶ allow_search_indices: bool = False allow_view class-attribute instance-attribute ¶ allow_view: bool = True created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) group class-attribute instance-attribute ¶ group: str | None = None id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"modelperm-{random_uuid()}\" ) is_blocking class-attribute instance-attribute ¶ is_blocking: bool = False object class-attribute instance-attribute ¶ object: str = 'model_permission' organization class-attribute instance-attribute ¶ organization: str = '*' OpenAIBaseModel ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120class OpenAIBaseModel(BaseModel): # OpenAI API does allow extra fields model_config = ConfigDict(extra=\"allow\") # Cache class field names field_names: ClassVar[set[str] | None] = None @model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result field_names class-attribute ¶ field_names: set[str] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') __log_extra_fields__ classmethod ¶ __log_extra_fields__(data, handler) Source code in vllm/entrypoints/openai/protocol.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120@model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result OutputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1640 1641 1642 1643 1644class OutputTokensDetails(OpenAIBaseModel): reasoning_tokens: int = 0 tool_output_tokens: int = 0 output_tokens_per_turn: list[int] = Field(default_factory=list) tool_output_tokens_per_turn: list[int] = Field(default_factory=list) output_tokens_per_turn class-attribute instance-attribute ¶ output_tokens_per_turn: list[int] = Field( default_factory=list ) reasoning_tokens class-attribute instance-attribute ¶ reasoning_tokens: int = 0 tool_output_tokens class-attribute instance-attribute ¶ tool_output_tokens: int = 0 tool_output_tokens_per_turn class-attribute instance-attribute ¶ tool_output_tokens_per_turn: list[int] = Field( default_factory=list ) PromptTokenUsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 195 196class PromptTokenUsageInfo(OpenAIBaseModel): cached_tokens: int | None = None cached_tokens class-attribute instance-attribute ¶ cached_tokens: int | None = None RequestResponseMetadata ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 206 207 208class RequestResponseMetadata(BaseModel): request_id: str final_usage_info: UsageInfo | None = None final_usage_info class-attribute instance-attribute ¶ final_usage_info: UsageInfo | None = None request_id instance-attribute ¶ request_id: str ResponseCompletedEvent ¶ Bases: ResponseCompletedEvent Source code in vllm/entrypoints/openai/protocol.py 1845 1846class ResponseCompletedEvent(OpenAIResponseCompletedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseCreatedEvent ¶ Bases: ResponseCreatedEvent Source code in vllm/entrypoints/openai/protocol.py 1849 1850class ResponseCreatedEvent(OpenAIResponseCreatedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 244 245 246 247class ResponseFormat(OpenAIBaseModel): # type must be \"json_schema\", \"json_object\", or \"text\" type: Literal[\"text\", \"json_object\", \"json_schema\"] json_schema: JsonSchemaResponseFormat | None = None json_schema class-attribute instance-attribute ¶ json_schema: JsonSchemaResponseFormat | None = None type instance-attribute ¶ type: Literal['text', 'json_object', 'json_schema'] ResponseInProgressEvent ¶ Bases: ResponseInProgressEvent Source code in vllm/entrypoints/openai/protocol.py 1853 1854class ResponseInProgressEvent(OpenAIResponseInProgressEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseRawMessageAndToken ¶ Bases: OpenAIBaseModel Class to show the raw message. If message / tokens diverge, tokens is the source of truth Source code in vllm/entrypoints/openai/protocol.py 1675 1676 1677 1678 1679 1680 1681class ResponseRawMessageAndToken(OpenAIBaseModel): \"\"\"Class to show the raw message. If message / tokens diverge, tokens is the source of truth\"\"\" message: str tokens: list[int] type: Literal[\"raw_message_tokens\"] = \"raw_message_tokens\" message instance-attribute ¶ message: str tokens instance-attribute ¶ tokens: list[int] type class-attribute instance-attribute ¶ type: Literal['raw_message_tokens'] = 'raw_message_tokens' ResponseReasoningPartAddedEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840class ResponseReasoningPartAddedEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.added\"] \"\"\"The type of the event. Always `response.reasoning_part.added`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.added'] The type of the event. Always response.reasoning_part.added. ResponseReasoningPartDoneEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818class ResponseReasoningPartDoneEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.done\"] \"\"\"The type of the event. Always `response.reasoning_part.done`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.done'] The type of the event. Always response.reasoning_part.done. ResponseUsage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1647 1648 1649 1650 1651 1652class ResponseUsage(OpenAIBaseModel): input_tokens: int input_tokens_details: InputTokensDetails output_tokens: int output_tokens_details: OutputTokensDetails total_tokens: int input_tokens instance-attribute ¶ input_tokens: int input_tokens_details instance-attribute ¶ input_tokens_details: InputTokensDetails output_tokens instance-attribute ¶ output_tokens: int output_tokens_details instance-attribute ¶ output_tokens_details: OutputTokensDetails total_tokens instance-attribute ¶ total_tokens: int ResponsesRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555class ResponsesRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/responses/create background: bool | None = False include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input: str | list[ResponseInputOutputItem] instructions: str | None = None max_output_tokens: int | None = None max_tool_calls: int | None = None metadata: Metadata | None = None model: str | None = None logit_bias: dict[str, float] | None = None parallel_tool_calls: bool | None = True previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] = \"auto\" store: bool | None = True stream: bool | None = False temperature: float | None = None text: ResponseTextConfig | None = None tool_choice: ToolChoice = \"auto\" tools: list[Tool] = Field(default_factory=list) top_logprobs: int | None = 0 top_p: float | None = None top_k: int | None = None truncation: Literal[\"auto\", \"disabled\"] | None = \"disabled\" user: str | None = None # --8<-- [start:responses-extra-params] request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) enable_response_messages: bool = Field( default=False, description=( \"Dictates whether or not to return messages as part of the \" \"response object. Currently only supported for\" \"non-background and gpt-oss only. \" ), ) # similar to input_messages / output_messages in ResponsesResponse # we take in previous_input_messages (ie in harmony format) # this cannot be used in conjunction with previous_response_id # TODO: consider supporting non harmony messages as well previous_input_messages: list[OpenAIHarmonyMessage | dict] | None = None # --8<-- [end:responses-extra-params] _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) @model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data @model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data @model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data @model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } background class-attribute instance-attribute ¶ background: bool | None = False cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) enable_response_messages class-attribute instance-attribute ¶ enable_response_messages: bool = Field( default=False, description=\"Dictates whether or not to return messages as part of the response object. Currently only supported fornon-background and gpt-oss only. \", ) include class-attribute instance-attribute ¶ include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input instance-attribute ¶ input: str | list[ResponseInputOutputItem] instructions class-attribute instance-attribute ¶ instructions: str | None = None logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None max_output_tokens class-attribute instance-attribute ¶ max_output_tokens: int | None = None max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True previous_input_messages class-attribute instance-attribute ¶ previous_input_messages: list[Message | dict] | None = None previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) service_tier class-attribute instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] = \"auto\" store class-attribute instance-attribute ¶ store: bool | None = True stream class-attribute instance-attribute ¶ stream: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float | None = None text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ToolChoice = 'auto' tools class-attribute instance-attribute ¶ tools: list[Tool] = Field(default_factory=list) top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncation class-attribute instance-attribute ¶ truncation: Literal['auto', 'disabled'] | None = 'disabled' user class-attribute instance-attribute ¶ user: str | None = None check_cache_salt_support ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 505 506 507 508 509 510 511 512 513@model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data function_call_parsing ¶ function_call_parsing(data) Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. Source code in vllm/entrypoints/openai/protocol.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555@model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data is_include_output_logprobs ¶ is_include_output_logprobs() -> bool Check if the request includes output logprobs. Source code in vllm/entrypoints/openai/protocol.py 480 481 482 483 484 485 486 487def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_background ¶ validate_background(data) Source code in vllm/entrypoints/openai/protocol.py 489 490 491 492 493 494 495@model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data validate_prompt ¶ validate_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 497 498 499 500 501 502 503@model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data ResponsesResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796class ResponsesResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"resp_{random_uuid()}\") created_at: int = Field(default_factory=lambda: int(time.time())) # error: Optional[ResponseError] = None incomplete_details: IncompleteDetails | None = None instructions: str | None = None metadata: Metadata | None = None model: str object: Literal[\"response\"] = \"response\" output: list[ResponseOutputItem] parallel_tool_calls: bool temperature: float tool_choice: ToolChoice tools: list[Tool] top_p: float background: bool max_output_tokens: int max_tool_calls: int | None = None previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] status: ResponseStatus text: ResponseTextConfig | None = None top_logprobs: int | None = None truncation: Literal[\"auto\", \"disabled\"] usage: ResponseUsage | None = None user: str | None = None # --8<-- [start:responses-response-extra-params] # These are populated when enable_response_messages is set to True # NOTE: custom serialization is needed # see serialize_input_messages and serialize_output_messages input_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token input to model.\" ), ) output_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token output of model.\" ), ) # --8<-- [end:responses-response-extra-params] # NOTE: openAI harmony doesn't serialize TextContent properly, # TODO: this fixes for TextContent, but need to verify for tools etc # https://github.com/openai/harmony/issues/78 @field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) # NOTE: openAI harmony doesn't serialize TextContent properly, this fixes it # https://github.com/openai/harmony/issues/78 @field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) @classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) background instance-attribute ¶ background: bool created_at class-attribute instance-attribute ¶ created_at: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\" ) incomplete_details class-attribute instance-attribute ¶ incomplete_details: IncompleteDetails | None = None input_messages class-attribute instance-attribute ¶ input_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token input to model.\", ) instructions class-attribute instance-attribute ¶ instructions: str | None = None max_output_tokens instance-attribute ¶ max_output_tokens: int max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['response'] = 'response' output instance-attribute ¶ output: list[ResponseOutputItem] output_messages class-attribute instance-attribute ¶ output_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token output of model.\", ) parallel_tool_calls instance-attribute ¶ parallel_tool_calls: bool previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None service_tier instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] status instance-attribute ¶ status: ResponseStatus temperature instance-attribute ¶ temperature: float text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice instance-attribute ¶ tool_choice: ToolChoice tools instance-attribute ¶ tools: list[Tool] top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = None top_p instance-attribute ¶ top_p: float truncation instance-attribute ¶ truncation: Literal['auto', 'disabled'] usage class-attribute instance-attribute ¶ usage: ResponseUsage | None = None user class-attribute instance-attribute ¶ user: str | None = None from_request classmethod ¶ from_request( request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> ResponsesResponse Source code in vllm/entrypoints/openai/protocol.py 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796@classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) serialize_input_messages ¶ serialize_input_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1745 1746 1747@field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) serialize_output_messages ¶ serialize_output_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1739 1740 1741@field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) StreamOptions ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 255 256 257class StreamOptions(OpenAIBaseModel): include_usage: bool | None = True continuous_usage_stats: bool | None = False continuous_usage_stats class-attribute instance-attribute ¶ continuous_usage_stats: bool | None = False include_usage class-attribute instance-attribute ¶ include_usage: bool | None = True StructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 234 235 236class StructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] format: Any format instance-attribute ¶ format: Any type instance-attribute ¶ type: Literal['structural_tag'] TokenizeChatRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974class TokenizeChatRequest(OpenAIBaseModel): model: str | None = None messages: list[ChatCompletionMessageParam] add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=(\"A list of tools the model may call.\"), ) @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=\"A list of tools the model may call.\", ) check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1966 1967 1968 1969 1970 1971 1972 1973 1974@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data TokenizeCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900class TokenizeCompletionRequest(OpenAIBaseModel): model: str | None = None prompt: str add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) model class-attribute instance-attribute ¶ model: str | None = None prompt instance-attribute ¶ prompt: str return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) TokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1980 1981 1982 1983 1984class TokenizeResponse(OpenAIBaseModel): count: int max_model_len: int tokens: list[int] token_strs: list[str] | None = None count instance-attribute ¶ count: int max_model_len instance-attribute ¶ max_model_len: int token_strs class-attribute instance-attribute ¶ token_strs: list[str] | None = None tokens instance-attribute ¶ tokens: list[int] TokenizerInfoResponse ¶ Bases: OpenAIBaseModel Response containing tokenizer configuration equivalent to tokenizer_config.json Source code in vllm/entrypoints/openai/protocol.py 1996 1997 1998 1999 2000 2001 2002 2003class TokenizerInfoResponse(OpenAIBaseModel): \"\"\" Response containing tokenizer configuration equivalent to tokenizer_config.json \"\"\" model_config = ConfigDict(extra=\"allow\") tokenizer_class: str model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') tokenizer_class instance-attribute ¶ tokenizer_class: str ToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1483 1484 1485 1486class ToolCall(OpenAIBaseModel): id: str = Field(default_factory=make_tool_call_id) type: Literal[\"function\"] = \"function\" function: FunctionCall function instance-attribute ¶ function: FunctionCall id class-attribute instance-attribute ¶ id: str = Field(default_factory=make_tool_call_id) type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' TranscriptionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213class TranscriptionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranscription file: UploadFile \"\"\" The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" language: str | None = None \"\"\"The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" ## TODO (varun) : Support if set to 0, certain thresholds are met !! timestamp_granularities: list[Literal[\"word\", \"segment\"]] = Field( alias=\"timestamp_granularities[]\", default=[] ) \"\"\"The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. \"\"\" stream: bool | None = False \"\"\"When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # --8<-- [start:transcription-extra-params] # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:transcription-extra-params] to_language: str | None = None \"\"\"The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. \"\"\" # --8<-- [start:transcription-sampling-params] temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" top_p: float | None = None \"\"\"Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds `p`. \"\"\" top_k: int | None = None \"\"\"Limits sampling to the `k` most probable tokens at each step.\"\"\" min_p: float | None = None \"\"\"Filters out tokens with a probability lower than `min_p`, ensuring a minimum likelihood threshold during sampling. \"\"\" seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" frequency_penalty: float | None = 0.0 \"\"\"The frequency penalty to use for sampling.\"\"\" repetition_penalty: float | None = None \"\"\"The repetition penalty to use for sampling.\"\"\" presence_penalty: float | None = 0.0 \"\"\"The presence penalty to use for sampling.\"\"\" max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:transcription-sampling-params] # Default sampling parameters for transcription requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } file instance-attribute ¶ file: UploadFile The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 The frequency penalty to use for sampling. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. min_p class-attribute instance-attribute ¶ min_p: float | None = None Filters out tokens with a probability lower than min_p, ensuring a minimum likelihood threshold during sampling. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 The presence penalty to use for sampling. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None The repetition penalty to use for sampling. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. timestamp_granularities class-attribute instance-attribute ¶ timestamp_granularities: list[ Literal[\"word\", \"segment\"] ] = Field(alias=\"timestamp_granularities[]\", default=[]) The timestamp granularities to populate for this transcription. response_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. top_k class-attribute instance-attribute ¶ top_k: int | None = None Limits sampling to the k most probable tokens at each step. top_p class-attribute instance-attribute ¶ top_p: float | None = None Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds p. vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_transcription_request classmethod ¶ validate_transcription_request(data) Source code in vllm/entrypoints/openai/protocol.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213@model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranscriptionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2222 2223 2224 2225class TranscriptionResponse(OpenAIBaseModel): text: str \"\"\"The transcribed text.\"\"\" usage: TranscriptionUsageAudio text instance-attribute ¶ text: str The transcribed text. usage instance-attribute ¶ usage: TranscriptionUsageAudio TranscriptionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1619 1620 1621 1622class TranscriptionResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranscriptionResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295class TranscriptionResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The transcribed text.\"\"\" segments: list[TranscriptionSegment] | None = None \"\"\"Segments of the transcribed text and their corresponding details.\"\"\" words: list[TranscriptionWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranscriptionSegment] | None = None Segments of the transcribed text and their corresponding details. text instance-attribute ¶ text: str The transcribed text. words class-attribute instance-attribute ¶ words: list[TranscriptionWord] | None = None Extracted words and their corresponding timestamps. TranscriptionSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278class TranscriptionSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranscriptionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1625 1626 1627 1628 1629 1630 1631class TranscriptionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsc-{random_uuid()}\") object: Literal[\"transcription.chunk\"] = \"transcription.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranscriptionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranscriptionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsc-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"transcription.chunk\"] = ( \"transcription.chunk\" ) usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranscriptionUsageAudio ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2217 2218 2219class TranscriptionUsageAudio(OpenAIBaseModel): type: Literal[\"duration\"] = \"duration\" seconds: int seconds instance-attribute ¶ seconds: int type class-attribute instance-attribute ¶ type: Literal['duration'] = 'duration' TranscriptionWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2228 2229 2230 2231 2232 2233 2234 2235 2236class TranscriptionWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. TranslationRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435class TranslationRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranslation file: UploadFile \"\"\" The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" # TODO support additional sampling parameters # --8<-- [start:translation-sampling-params] seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" # --8<-- [end:translation-sampling-params] # --8<-- [start:translation-extra-params] language: str | None = None \"\"\"The language of the input audio we translate from. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy. \"\"\" to_language: str | None = None \"\"\"The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports `to_language=en`. \"\"\" stream: bool | None = False \"\"\"Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:translation-extra-params] # Default sampling parameters for translation requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"temperature\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = {'temperature': 0} file instance-attribute ¶ file: UploadFile The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio we translate from. Supplying the input language in ISO-639-1 format will improve accuracy. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports to_language=en. to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranslationResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2439 2440 2441class TranslationResponse(OpenAIBaseModel): text: str \"\"\"The translated text.\"\"\" text instance-attribute ¶ text: str The translated text. TranslationResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2303 2304 2305 2306class TranslationResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranslationResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511class TranslationResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The translated text.\"\"\" segments: list[TranslationSegment] | None = None \"\"\"Segments of the translated text and their corresponding details.\"\"\" words: list[TranslationWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranslationSegment] | None = None Segments of the translated text and their corresponding details. text instance-attribute ¶ text: str The translated text. words class-attribute instance-attribute ¶ words: list[TranslationWord] | None = None Extracted words and their corresponding timestamps. TranslationSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494class TranslationSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranslationStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2309 2310 2311 2312 2313 2314 2315class TranslationStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsl-{random_uuid()}\") object: Literal[\"translation.chunk\"] = \"translation.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranslationResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranslationResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['translation.chunk'] = 'translation.chunk' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranslationWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2444 2445 2446 2447 2448 2449 2450 2451 2452class TranslationWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. UnloadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2011 2012 2013class UnloadLoRAAdapterRequest(BaseModel): lora_name: str lora_int_id: int | None = Field(default=None) lora_int_id class-attribute instance-attribute ¶ lora_int_id: int | None = Field(default=None) lora_name instance-attribute ¶ lora_name: str UsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 199 200 201 202 203class UsageInfo(OpenAIBaseModel): prompt_tokens: int = 0 total_tokens: int = 0 completion_tokens: int | None = 0 prompt_tokens_details: PromptTokenUsageInfo | None = None completion_tokens class-attribute instance-attribute ¶ completion_tokens: int | None = 0 prompt_tokens class-attribute instance-attribute ¶ prompt_tokens: int = 0 prompt_tokens_details class-attribute instance-attribute ¶ prompt_tokens_details: PromptTokenUsageInfo | None = None total_tokens class-attribute instance-attribute ¶ total_tokens: int = 0 VLLMValidationError ¶ Bases: ValueError vLLM-specific validation error for request validation failures. Parameters: Name Type Description Default message str The error message describing the validation failure. required parameter str | None Optional parameter name that failed validation. None value Any Optional value that was rejected during validation. None Source code in vllm/entrypoints/openai/protocol.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161class VLLMValidationError(ValueError): \"\"\"vLLM-specific validation error for request validation failures. Args: message: The error message describing the validation failure. parameter: Optional parameter name that failed validation. value: Optional value that was rejected during validation. \"\"\" def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base parameter instance-attribute ¶ parameter = parameter value instance-attribute ¶ value = value __init__ ¶ __init__( message: str, *, parameter: str | None = None, value: Any = None, ) -> None Source code in vllm/entrypoints/openai/protocol.py 143 144 145 146 147 148 149 150 151 152def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value __str__ ¶ __str__() Source code in vllm/entrypoints/openai/protocol.py 154 155 156 157 158 159 160 161def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base get_logits_processors ¶ get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None Source code in vllm/entrypoints/openai/protocol.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324def get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None: if processors and pattern: logits_processors = [] for processor in processors: qualname = processor if isinstance(processor, str) else processor.qualname if not re.match(pattern, qualname): raise ValueError( f\"Logits processor '{qualname}' is not allowed by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) try: logits_processor = resolve_obj_by_qualname(qualname) except Exception as e: raise ValueError( f\"Logits processor '{qualname}' could not be resolved: {e}\" ) from e if isinstance(processor, LogitsProcessorConstructor): logits_processor = logits_processor( *processor.args or [], **processor.kwargs or {} ) logits_processors.append(logits_processor) return logits_processors elif processors: raise ValueError( \"The `logits_processors` argument is not supported by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) return None serialize_message ¶ serialize_message(msg) Serializes a single message Source code in vllm/entrypoints/openai/protocol.py 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665def serialize_message(msg): \"\"\" Serializes a single message \"\"\" if isinstance(msg, dict): return msg elif hasattr(msg, \"to_dict\"): return msg.to_dict() else: # fallback to pyandic dump return msg.model_dump_json() serialize_messages ¶ serialize_messages(msgs) Serializes multiple messages Source code in vllm/entrypoints/openai/protocol.py 1668 1669 1670 1671 1672def serialize_messages(msgs): \"\"\" Serializes multiple messages \"\"\" return [serialize_message(msg) for msg in msgs] if msgs else None December 25, 2025",
      "code": ""
    },
    {
      "description": "vllm.entrypoints.openai.protocol ¶ AnyResponseFormat module-attribute ¶ AnyResponseFormat: TypeAlias = ( ResponseFormat | StructuralTagResponseFormat | LegacyStructuralTagResponseFormat ) AnyStructuralTagResponseFormat module-attribute ¶ AnyStructuralTagResponseFormat: TypeAlias = ( LegacyStructuralTagResponseFormat | StructuralTagResponseFormat ) AudioResponseFormat module-attribute ¶ AudioResponseFormat: TypeAlias = Literal[ \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\" ] LogitsProcessors module-attribute ¶ LogitsProcessors = list[str | LogitsProcessorConstructor] ResponseInputOutputItem module-attribute ¶ ResponseInputOutputItem: TypeAlias = ( ResponseInputItemParam | ResponseOutputItem ) ResponseInputOutputMessage module-attribute ¶ ResponseInputOutputMessage: TypeAlias = ( list[ChatCompletionMessageParam] | list[ResponseRawMessageAndToken] ) StreamingResponsesResponse module-attribute ¶ StreamingResponsesResponse: TypeAlias = ( ResponseCreatedEvent | ResponseInProgressEvent | ResponseCompletedEvent | ResponseOutputItemAddedEvent | ResponseOutputItemDoneEvent | ResponseContentPartAddedEvent | ResponseContentPartDoneEvent | ResponseReasoningTextDeltaEvent | ResponseReasoningTextDoneEvent | ResponseReasoningPartAddedEvent | ResponseReasoningPartDoneEvent | ResponseCodeInterpreterCallInProgressEvent | ResponseCodeInterpreterCallCodeDeltaEvent | ResponseWebSearchCallInProgressEvent | ResponseWebSearchCallSearchingEvent | ResponseWebSearchCallCompletedEvent | ResponseCodeInterpreterCallCodeDoneEvent | ResponseCodeInterpreterCallInterpretingEvent | ResponseCodeInterpreterCallCompletedEvent | ResponseMcpCallArgumentsDeltaEvent | ResponseMcpCallArgumentsDoneEvent | ResponseMcpCallInProgressEvent | ResponseMcpCallCompletedEvent ) TokenizeRequest module-attribute ¶ TokenizeRequest: TypeAlias = ( TokenizeCompletionRequest | TokenizeChatRequest ) TranscriptionResponseVariant module-attribute ¶ TranscriptionResponseVariant: TypeAlias = ( TranscriptionResponse | TranscriptionResponseVerbose ) TranslationResponseVariant module-attribute ¶ TranslationResponseVariant: TypeAlias = ( TranslationResponse | TranslationResponseVerbose ) _LONG_INFO module-attribute ¶ _LONG_INFO = iinfo(long) logger module-attribute ¶ logger = init_logger(__name__) ChatCompletionLogProb ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1535 1536 1537 1538class ChatCompletionLogProb(OpenAIBaseModel): token: str logprob: float = -9999.0 bytes: list[int] | None = None bytes class-attribute instance-attribute ¶ bytes: list[int] | None = None logprob class-attribute instance-attribute ¶ logprob: float = -9999.0 token instance-attribute ¶ token: str ChatCompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1548 1549class ChatCompletionLogProbs(OpenAIBaseModel): content: list[ChatCompletionLogProbsContent] | None = None content class-attribute instance-attribute ¶ content: list[ChatCompletionLogProbsContent] | None = None ChatCompletionLogProbsContent ¶ Bases: ChatCompletionLogProb Source code in vllm/entrypoints/openai/protocol.py 1541 1542 1543 1544 1545class ChatCompletionLogProbsContent(ChatCompletionLogProb): # Workaround: redefine fields name cache so that it's not # shared with the super class. field_names: ClassVar[set[str] | None] = None top_logprobs: list[ChatCompletionLogProb] = Field(default_factory=list) field_names class-attribute ¶ field_names: set[str] | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[ChatCompletionLogProb] = Field( default_factory=list ) ChatCompletionNamedFunction ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 271 272class ChatCompletionNamedFunction(OpenAIBaseModel): name: str name instance-attribute ¶ name: str ChatCompletionNamedToolChoiceParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 275 276 277class ChatCompletionNamedToolChoiceParam(OpenAIBaseModel): function: ChatCompletionNamedFunction type: Literal[\"function\"] = \"function\" function instance-attribute ¶ function: ChatCompletionNamedFunction type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054class ChatCompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/chat/create messages: list[ChatCompletionMessageParam] model: str | None = None frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: bool | None = False top_logprobs: int | None = 0 max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of \" \"the max_completion_tokens field\", ) max_completion_tokens: int | None = None n: int | None = 1 presence_penalty: float | None = 0.0 response_format: AnyResponseFormat | None = None seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None temperature: float | None = None top_p: float | None = None tools: list[ChatCompletionToolsParam] | None = None tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" reasoning_effort: Literal[\"low\", \"medium\", \"high\"] | None = None include_reasoning: bool = True parallel_tool_calls: bool | None = True # NOTE this will be ignored by vLLM user: str | None = None # --8<-- [start:chat-completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None prompt_logprobs: int | None = None allowed_token_ids: list[int] | None = None bad_words: list[str] = Field(default_factory=list) # --8<-- [end:chat-completion-sampling-params] # --8<-- [start:chat-completion-extra-params] echo: bool = Field( default=False, description=( \"If true, the new message will be prepended with the last message \" \"if they belong to the same role.\" ), ) add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) documents: list[dict[str, str]] | None = Field( default=None, description=( \"A list of dicts representing documents that will be accessible to \" \"the model if it is performing RAG (retrieval-augmented generation).\" \" If the template does not support RAG, this argument will have no \" \"effect. We recommend that each document should be a dict containing \" '\"title\" and \"text\" keys.' ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float | list[str | int | float]] | None = Field( default=None, description=( \"Additional request parameters with (list of) string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:chat-completion-extra-params] # Default sampling parameters for chat completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data @model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None bad_words class-attribute instance-attribute ¶ bad_words: list[str] = Field(default_factory=list) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) documents class-attribute instance-attribute ¶ documents: list[dict[str, str]] | None = Field( default=None, description='A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.', ) echo class-attribute instance-attribute ¶ echo: bool = Field( default=False, description=\"If true, the new message will be prepended with the last message if they belong to the same role.\", ) frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_reasoning class-attribute instance-attribute ¶ include_reasoning: bool = True include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: bool | None = False max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of the max_completion_tokens field\", ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int | None = 1 parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None reasoning_effort class-attribute instance-attribute ¶ reasoning_effort: ( Literal[\"low\", \"medium\", \"high\"] | None ) = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = None return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) temperature class-attribute instance-attribute ¶ temperature: float | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: ( dict[str, str | int | float | list[str | int | float]] | None ) = Field( default=None, description=\"Additional request parameters with (list of) string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1035 1036 1037 1038 1039 1040 1041 1042 1043@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data check_tool_usage classmethod ¶ check_tool_usage(data) Source code in vllm/entrypoints/openai/protocol.py 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033@model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 883 884 885 886 887 888 889 890 891 892@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data ChatCompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580class ChatCompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion\"] = \"chat.completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[ChatCompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['chat.completion'] = 'chat.completion' prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo ChatCompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562class ChatCompletionResponseChoice(OpenAIBaseModel): index: int message: ChatMessage logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" # not part of the OpenAI spec but included in vLLM for legacy reasons stop_reason: int | str | None = None # not part of the OpenAI spec but is useful for tracing the tokens # in agent scenarios token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None message instance-attribute ¶ message: ChatMessage stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1598 1599 1600 1601 1602 1603 1604 1605class ChatCompletionResponseStreamChoice(OpenAIBaseModel): index: int delta: DeltaMessage logprobs: ChatCompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = None # not part of the OpenAI spec but for tracing the tokens token_ids: list[int] | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1608 1609 1610 1611 1612 1613 1614 1615 1616class ChatCompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) # not part of the OpenAI spec but for tracing the tokens prompt_token_ids: list[int] | None = None choices instance-attribute ¶ choices: list[ChatCompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"chat.completion.chunk\"] = ( \"chat.completion.chunk\" ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) ChatCompletionToolsParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 266 267 268class ChatCompletionToolsParam(OpenAIBaseModel): type: Literal[\"function\"] = \"function\" function: FunctionDefinition function instance-attribute ¶ function: FunctionDefinition type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532class ChatMessage(OpenAIBaseModel): role: str content: str | None = None refusal: str | None = None annotations: OpenAIAnnotation | None = None audio: OpenAIChatCompletionAudio | None = None function_call: FunctionCall | None = None tool_calls: list[ToolCall] = Field(default_factory=list) # vLLM-specific fields that are not in OpenAI spec reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self annotations class-attribute instance-attribute ¶ annotations: Annotation | None = None audio class-attribute instance-attribute ¶ audio: ChatCompletionAudio | None = None content class-attribute instance-attribute ¶ content: str | None = None function_call class-attribute instance-attribute ¶ function_call: FunctionCall | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. refusal class-attribute instance-attribute ¶ refusal: str | None = None role instance-attribute ¶ role: str tool_calls class-attribute instance-attribute ¶ tool_calls: list[ToolCall] = Field(default_factory=list) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1528 1529 1530 1531 1532@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self CompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1409 1410 1411 1412 1413class CompletionLogProbs(OpenAIBaseModel): text_offset: list[int] = Field(default_factory=list) token_logprobs: list[float | None] = Field(default_factory=list) tokens: list[str] = Field(default_factory=list) top_logprobs: list[dict[str, float] | None] = Field(default_factory=list) text_offset class-attribute instance-attribute ¶ text_offset: list[int] = Field(default_factory=list) token_logprobs class-attribute instance-attribute ¶ token_logprobs: list[float | None] = Field( default_factory=list ) tokens class-attribute instance-attribute ¶ tokens: list[str] = Field(default_factory=list) top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[dict[str, float] | None] = Field( default_factory=list ) CompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406class CompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/completions/create model: str | None = None prompt: list[int] | list[list[int]] | str | list[str] | None = None echo: bool | None = False frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: int | None = None max_tokens: int | None = 16 n: int = 1 presence_penalty: float | None = 0.0 seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None suffix: str | None = None temperature: float | None = None top_p: float | None = None user: str | None = None # --8<-- [start:completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None allowed_token_ids: list[int] | None = None prompt_logprobs: int | None = None # --8<-- [end:completion-sampling-params] # --8<-- [start:completion-extra-params] prompt_embeds: bytes | list[bytes] | None = None add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) response_format: AnyResponseFormat | None = Field( default=None, description=( \"Similar to chat completion, this parameter specifies the format \" \"of output. Only {'type': 'json_object'}, {'type': 'json_schema'}\" \", {'type': 'structural_tag'}, or {'type': 'text' } is supported.\" ), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:completion-extra-params] # Default sampling parameters for completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) echo class-attribute instance-attribute ¶ echo: bool | None = False frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = 16 min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int = 1 presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ( list[int] | list[list[int]] | str | list[str] | None ) = None prompt_embeds class-attribute instance-attribute ¶ prompt_embeds: bytes | list[bytes] | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = Field( default=None, description=\"Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'}, {'type': 'json_schema'}, {'type': 'structural_tag'}, or {'type': 'text' } is supported.\", ) return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) suffix class-attribute instance-attribute ¶ suffix: str | None = None temperature class-attribute instance-attribute ¶ temperature: float | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_prompt_and_prompt_embeds classmethod ¶ validate_prompt_and_prompt_embeds(data) Source code in vllm/entrypoints/openai/protocol.py 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395@model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data CompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447class CompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: Literal[\"text_completion\"] = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[CompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['text_completion'] = 'text_completion' service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo CompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431class CompletionResponseChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) token_ids: list[int] | None = None # For response prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None # For prompt finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466class CompletionResponseStreamChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) # not part of the OpenAI spec but for tracing the tokens # prompt tokens is put into choice to align with CompletionResponseChoice prompt_token_ids: list[int] | None = None token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1469 1470 1471 1472 1473 1474 1475class CompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: str = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[CompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: str = 'text_completion' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) DeltaFunctionCall ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1489 1490 1491class DeltaFunctionCall(BaseModel): name: str | None = None arguments: str | None = None arguments class-attribute instance-attribute ¶ arguments: str | None = None name class-attribute instance-attribute ¶ name: str | None = None DeltaMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595class DeltaMessage(OpenAIBaseModel): role: str | None = None content: str | None = None reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" tool_calls: list[DeltaToolCall] = Field(default_factory=list) @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self content class-attribute instance-attribute ¶ content: str | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. role class-attribute instance-attribute ¶ role: str | None = None tool_calls class-attribute instance-attribute ¶ tool_calls: list[DeltaToolCall] = Field( default_factory=list ) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1591 1592 1593 1594 1595@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self DeltaToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1495 1496 1497 1498 1499class DeltaToolCall(OpenAIBaseModel): id: str | None = None type: Literal[\"function\"] | None = None index: int function: DeltaFunctionCall | None = None function class-attribute instance-attribute ¶ function: DeltaFunctionCall | None = None id class-attribute instance-attribute ¶ id: str | None = None index instance-attribute ¶ index: int type class-attribute instance-attribute ¶ type: Literal['function'] | None = None DetokenizeRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1987 1988 1989class DetokenizeRequest(OpenAIBaseModel): model: str | None = None tokens: list[int] model class-attribute instance-attribute ¶ model: str | None = None tokens instance-attribute ¶ tokens: list[int] DetokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1992 1993class DetokenizeResponse(OpenAIBaseModel): prompt: str prompt instance-attribute ¶ prompt: str ErrorInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 123 124 125 126 127class ErrorInfo(OpenAIBaseModel): message: str type: str param: str | None = None code: int code instance-attribute ¶ code: int message instance-attribute ¶ message: str param class-attribute instance-attribute ¶ param: str | None = None type instance-attribute ¶ type: str ErrorResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 130 131class ErrorResponse(OpenAIBaseModel): error: ErrorInfo error instance-attribute ¶ error: ErrorInfo ExtractedToolCallInformation ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511class ExtractedToolCallInformation(BaseModel): # indicate if tools were called tools_called: bool # extracted tool calls tool_calls: list[ToolCall] # content - per OpenAI spec, content AND tool calls can be returned rarely # But some models will do this intentionally content: str | None = None content class-attribute instance-attribute ¶ content: str | None = None tool_calls instance-attribute ¶ tool_calls: list[ToolCall] tools_called instance-attribute ¶ tools_called: bool FunctionCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1478 1479 1480class FunctionCall(OpenAIBaseModel): name: str arguments: str arguments instance-attribute ¶ arguments: str name instance-attribute ¶ name: str FunctionDefinition ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 260 261 262 263class FunctionDefinition(OpenAIBaseModel): name: str description: str | None = None parameters: dict[str, Any] | None = None description class-attribute instance-attribute ¶ description: str | None = None name instance-attribute ¶ name: str parameters class-attribute instance-attribute ¶ parameters: dict[str, Any] | None = None GenerateRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564class GenerateRequest(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) token_ids: list[int] \"\"\"The token ids to generate text from.\"\"\" # features: MultiModalFeatureSpec # TODO (NickLucche): implement once Renderer work is completed features: str | None = None \"\"\"The processed MM inputs for the model.\"\"\" sampling_params: SamplingParams \"\"\"The sampling parameters for the model.\"\"\" model: str | None = None stream: bool | None = False stream_options: StreamOptions | None = None cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) features class-attribute instance-attribute ¶ features: str | None = None The processed MM inputs for the model. kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) model class-attribute instance-attribute ¶ model: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) sampling_params instance-attribute ¶ sampling_params: SamplingParams The sampling parameters for the model. stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None token_ids instance-attribute ¶ token_ids: list[int] The token ids to generate text from. GenerateResponse ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591class GenerateResponse(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) choices: list[GenerateResponseChoice] prompt_logprobs: list[dict[int, Logprob] | None] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) choices instance-attribute ¶ choices: list[GenerateResponseChoice] kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) GenerateResponseChoice ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2567 2568 2569 2570 2571 2572class GenerateResponseChoice(BaseModel): index: int logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None InputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1634 1635 1636 1637class InputTokensDetails(OpenAIBaseModel): cached_tokens: int input_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens instance-attribute ¶ cached_tokens: int cached_tokens_per_turn class-attribute instance-attribute ¶ cached_tokens_per_turn: list[int] = Field( default_factory=list ) input_tokens_per_turn class-attribute instance-attribute ¶ input_tokens_per_turn: list[int] = Field( default_factory=list ) JsonSchemaResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 211 212 213 214 215 216 217class JsonSchemaResponseFormat(OpenAIBaseModel): name: str description: str | None = None # schema is the field in openai but that causes conflicts with pydantic so # instead use json_schema with an alias json_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") strict: bool | None = None description class-attribute instance-attribute ¶ description: str | None = None json_schema class-attribute instance-attribute ¶ json_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) name instance-attribute ¶ name: str strict class-attribute instance-attribute ¶ strict: bool | None = None LegacyStructuralTag ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 220 221 222 223 224 225class LegacyStructuralTag(OpenAIBaseModel): begin: str # schema is the field, but that causes conflicts with pydantic so # instead use structural_tag_schema with an alias structural_tag_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") end: str begin instance-attribute ¶ begin: str end instance-attribute ¶ end: str structural_tag_schema class-attribute instance-attribute ¶ structural_tag_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) LegacyStructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 228 229 230 231class LegacyStructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] structures: list[LegacyStructuralTag] triggers: list[str] structures instance-attribute ¶ structures: list[LegacyStructuralTag] triggers instance-attribute ¶ triggers: list[str] type instance-attribute ¶ type: Literal['structural_tag'] LoadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2006 2007 2008class LoadLoRAAdapterRequest(BaseModel): lora_name: str lora_path: str lora_name instance-attribute ¶ lora_name: str lora_path instance-attribute ¶ lora_path: str LogitsProcessorConstructor ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 282 283 284 285 286 287class LogitsProcessorConstructor(BaseModel): qualname: str args: list[Any] | None = None kwargs: dict[str, Any] | None = None model_config = ConfigDict(extra=\"forbid\") args class-attribute instance-attribute ¶ args: list[Any] | None = None kwargs class-attribute instance-attribute ¶ kwargs: dict[str, Any] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='forbid') qualname instance-attribute ¶ qualname: str ModelCard ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 179 180 181 182 183 184 185 186 187class ModelCard(OpenAIBaseModel): id: str object: str = \"model\" created: int = Field(default_factory=lambda: int(time.time())) owned_by: str = \"vllm\" root: str | None = None parent: str | None = None max_model_len: int | None = None permission: list[ModelPermission] = Field(default_factory=list) created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id instance-attribute ¶ id: str max_model_len class-attribute instance-attribute ¶ max_model_len: int | None = None object class-attribute instance-attribute ¶ object: str = 'model' owned_by class-attribute instance-attribute ¶ owned_by: str = 'vllm' parent class-attribute instance-attribute ¶ parent: str | None = None permission class-attribute instance-attribute ¶ permission: list[ModelPermission] = Field( default_factory=list ) root class-attribute instance-attribute ¶ root: str | None = None ModelList ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 190 191 192class ModelList(OpenAIBaseModel): object: str = \"list\" data: list[ModelCard] = Field(default_factory=list) data class-attribute instance-attribute ¶ data: list[ModelCard] = Field(default_factory=list) object class-attribute instance-attribute ¶ object: str = 'list' ModelPermission ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 164 165 166 167 168 169 170 171 172 173 174 175 176class ModelPermission(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\") object: str = \"model_permission\" created: int = Field(default_factory=lambda: int(time.time())) allow_create_engine: bool = False allow_sampling: bool = True allow_logprobs: bool = True allow_search_indices: bool = False allow_view: bool = True allow_fine_tuning: bool = False organization: str = \"*\" group: str | None = None is_blocking: bool = False allow_create_engine class-attribute instance-attribute ¶ allow_create_engine: bool = False allow_fine_tuning class-attribute instance-attribute ¶ allow_fine_tuning: bool = False allow_logprobs class-attribute instance-attribute ¶ allow_logprobs: bool = True allow_sampling class-attribute instance-attribute ¶ allow_sampling: bool = True allow_search_indices class-attribute instance-attribute ¶ allow_search_indices: bool = False allow_view class-attribute instance-attribute ¶ allow_view: bool = True created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) group class-attribute instance-attribute ¶ group: str | None = None id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"modelperm-{random_uuid()}\" ) is_blocking class-attribute instance-attribute ¶ is_blocking: bool = False object class-attribute instance-attribute ¶ object: str = 'model_permission' organization class-attribute instance-attribute ¶ organization: str = '*' OpenAIBaseModel ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120class OpenAIBaseModel(BaseModel): # OpenAI API does allow extra fields model_config = ConfigDict(extra=\"allow\") # Cache class field names field_names: ClassVar[set[str] | None] = None @model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result field_names class-attribute ¶ field_names: set[str] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') __log_extra_fields__ classmethod ¶ __log_extra_fields__(data, handler) Source code in vllm/entrypoints/openai/protocol.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120@model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result OutputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1640 1641 1642 1643 1644class OutputTokensDetails(OpenAIBaseModel): reasoning_tokens: int = 0 tool_output_tokens: int = 0 output_tokens_per_turn: list[int] = Field(default_factory=list) tool_output_tokens_per_turn: list[int] = Field(default_factory=list) output_tokens_per_turn class-attribute instance-attribute ¶ output_tokens_per_turn: list[int] = Field( default_factory=list ) reasoning_tokens class-attribute instance-attribute ¶ reasoning_tokens: int = 0 tool_output_tokens class-attribute instance-attribute ¶ tool_output_tokens: int = 0 tool_output_tokens_per_turn class-attribute instance-attribute ¶ tool_output_tokens_per_turn: list[int] = Field( default_factory=list ) PromptTokenUsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 195 196class PromptTokenUsageInfo(OpenAIBaseModel): cached_tokens: int | None = None cached_tokens class-attribute instance-attribute ¶ cached_tokens: int | None = None RequestResponseMetadata ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 206 207 208class RequestResponseMetadata(BaseModel): request_id: str final_usage_info: UsageInfo | None = None final_usage_info class-attribute instance-attribute ¶ final_usage_info: UsageInfo | None = None request_id instance-attribute ¶ request_id: str ResponseCompletedEvent ¶ Bases: ResponseCompletedEvent Source code in vllm/entrypoints/openai/protocol.py 1845 1846class ResponseCompletedEvent(OpenAIResponseCompletedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseCreatedEvent ¶ Bases: ResponseCreatedEvent Source code in vllm/entrypoints/openai/protocol.py 1849 1850class ResponseCreatedEvent(OpenAIResponseCreatedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 244 245 246 247class ResponseFormat(OpenAIBaseModel): # type must be \"json_schema\", \"json_object\", or \"text\" type: Literal[\"text\", \"json_object\", \"json_schema\"] json_schema: JsonSchemaResponseFormat | None = None json_schema class-attribute instance-attribute ¶ json_schema: JsonSchemaResponseFormat | None = None type instance-attribute ¶ type: Literal['text', 'json_object', 'json_schema'] ResponseInProgressEvent ¶ Bases: ResponseInProgressEvent Source code in vllm/entrypoints/openai/protocol.py 1853 1854class ResponseInProgressEvent(OpenAIResponseInProgressEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseRawMessageAndToken ¶ Bases: OpenAIBaseModel Class to show the raw message. If message / tokens diverge, tokens is the source of truth Source code in vllm/entrypoints/openai/protocol.py 1675 1676 1677 1678 1679 1680 1681class ResponseRawMessageAndToken(OpenAIBaseModel): \"\"\"Class to show the raw message. If message / tokens diverge, tokens is the source of truth\"\"\" message: str tokens: list[int] type: Literal[\"raw_message_tokens\"] = \"raw_message_tokens\" message instance-attribute ¶ message: str tokens instance-attribute ¶ tokens: list[int] type class-attribute instance-attribute ¶ type: Literal['raw_message_tokens'] = 'raw_message_tokens' ResponseReasoningPartAddedEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840class ResponseReasoningPartAddedEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.added\"] \"\"\"The type of the event. Always `response.reasoning_part.added`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.added'] The type of the event. Always response.reasoning_part.added. ResponseReasoningPartDoneEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818class ResponseReasoningPartDoneEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.done\"] \"\"\"The type of the event. Always `response.reasoning_part.done`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.done'] The type of the event. Always response.reasoning_part.done. ResponseUsage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1647 1648 1649 1650 1651 1652class ResponseUsage(OpenAIBaseModel): input_tokens: int input_tokens_details: InputTokensDetails output_tokens: int output_tokens_details: OutputTokensDetails total_tokens: int input_tokens instance-attribute ¶ input_tokens: int input_tokens_details instance-attribute ¶ input_tokens_details: InputTokensDetails output_tokens instance-attribute ¶ output_tokens: int output_tokens_details instance-attribute ¶ output_tokens_details: OutputTokensDetails total_tokens instance-attribute ¶ total_tokens: int ResponsesRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555class ResponsesRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/responses/create background: bool | None = False include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input: str | list[ResponseInputOutputItem] instructions: str | None = None max_output_tokens: int | None = None max_tool_calls: int | None = None metadata: Metadata | None = None model: str | None = None logit_bias: dict[str, float] | None = None parallel_tool_calls: bool | None = True previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] = \"auto\" store: bool | None = True stream: bool | None = False temperature: float | None = None text: ResponseTextConfig | None = None tool_choice: ToolChoice = \"auto\" tools: list[Tool] = Field(default_factory=list) top_logprobs: int | None = 0 top_p: float | None = None top_k: int | None = None truncation: Literal[\"auto\", \"disabled\"] | None = \"disabled\" user: str | None = None # --8<-- [start:responses-extra-params] request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) enable_response_messages: bool = Field( default=False, description=( \"Dictates whether or not to return messages as part of the \" \"response object. Currently only supported for\" \"non-background and gpt-oss only. \" ), ) # similar to input_messages / output_messages in ResponsesResponse # we take in previous_input_messages (ie in harmony format) # this cannot be used in conjunction with previous_response_id # TODO: consider supporting non harmony messages as well previous_input_messages: list[OpenAIHarmonyMessage | dict] | None = None # --8<-- [end:responses-extra-params] _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) @model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data @model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data @model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data @model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } background class-attribute instance-attribute ¶ background: bool | None = False cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) enable_response_messages class-attribute instance-attribute ¶ enable_response_messages: bool = Field( default=False, description=\"Dictates whether or not to return messages as part of the response object. Currently only supported fornon-background and gpt-oss only. \", ) include class-attribute instance-attribute ¶ include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input instance-attribute ¶ input: str | list[ResponseInputOutputItem] instructions class-attribute instance-attribute ¶ instructions: str | None = None logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None max_output_tokens class-attribute instance-attribute ¶ max_output_tokens: int | None = None max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True previous_input_messages class-attribute instance-attribute ¶ previous_input_messages: list[Message | dict] | None = None previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) service_tier class-attribute instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] = \"auto\" store class-attribute instance-attribute ¶ store: bool | None = True stream class-attribute instance-attribute ¶ stream: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float | None = None text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ToolChoice = 'auto' tools class-attribute instance-attribute ¶ tools: list[Tool] = Field(default_factory=list) top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncation class-attribute instance-attribute ¶ truncation: Literal['auto', 'disabled'] | None = 'disabled' user class-attribute instance-attribute ¶ user: str | None = None check_cache_salt_support ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 505 506 507 508 509 510 511 512 513@model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data function_call_parsing ¶ function_call_parsing(data) Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. Source code in vllm/entrypoints/openai/protocol.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555@model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data is_include_output_logprobs ¶ is_include_output_logprobs() -> bool Check if the request includes output logprobs. Source code in vllm/entrypoints/openai/protocol.py 480 481 482 483 484 485 486 487def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_background ¶ validate_background(data) Source code in vllm/entrypoints/openai/protocol.py 489 490 491 492 493 494 495@model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data validate_prompt ¶ validate_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 497 498 499 500 501 502 503@model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data ResponsesResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796class ResponsesResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"resp_{random_uuid()}\") created_at: int = Field(default_factory=lambda: int(time.time())) # error: Optional[ResponseError] = None incomplete_details: IncompleteDetails | None = None instructions: str | None = None metadata: Metadata | None = None model: str object: Literal[\"response\"] = \"response\" output: list[ResponseOutputItem] parallel_tool_calls: bool temperature: float tool_choice: ToolChoice tools: list[Tool] top_p: float background: bool max_output_tokens: int max_tool_calls: int | None = None previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] status: ResponseStatus text: ResponseTextConfig | None = None top_logprobs: int | None = None truncation: Literal[\"auto\", \"disabled\"] usage: ResponseUsage | None = None user: str | None = None # --8<-- [start:responses-response-extra-params] # These are populated when enable_response_messages is set to True # NOTE: custom serialization is needed # see serialize_input_messages and serialize_output_messages input_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token input to model.\" ), ) output_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token output of model.\" ), ) # --8<-- [end:responses-response-extra-params] # NOTE: openAI harmony doesn't serialize TextContent properly, # TODO: this fixes for TextContent, but need to verify for tools etc # https://github.com/openai/harmony/issues/78 @field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) # NOTE: openAI harmony doesn't serialize TextContent properly, this fixes it # https://github.com/openai/harmony/issues/78 @field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) @classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) background instance-attribute ¶ background: bool created_at class-attribute instance-attribute ¶ created_at: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\" ) incomplete_details class-attribute instance-attribute ¶ incomplete_details: IncompleteDetails | None = None input_messages class-attribute instance-attribute ¶ input_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token input to model.\", ) instructions class-attribute instance-attribute ¶ instructions: str | None = None max_output_tokens instance-attribute ¶ max_output_tokens: int max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['response'] = 'response' output instance-attribute ¶ output: list[ResponseOutputItem] output_messages class-attribute instance-attribute ¶ output_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token output of model.\", ) parallel_tool_calls instance-attribute ¶ parallel_tool_calls: bool previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None service_tier instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] status instance-attribute ¶ status: ResponseStatus temperature instance-attribute ¶ temperature: float text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice instance-attribute ¶ tool_choice: ToolChoice tools instance-attribute ¶ tools: list[Tool] top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = None top_p instance-attribute ¶ top_p: float truncation instance-attribute ¶ truncation: Literal['auto', 'disabled'] usage class-attribute instance-attribute ¶ usage: ResponseUsage | None = None user class-attribute instance-attribute ¶ user: str | None = None from_request classmethod ¶ from_request( request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> ResponsesResponse Source code in vllm/entrypoints/openai/protocol.py 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796@classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) serialize_input_messages ¶ serialize_input_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1745 1746 1747@field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) serialize_output_messages ¶ serialize_output_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1739 1740 1741@field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) StreamOptions ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 255 256 257class StreamOptions(OpenAIBaseModel): include_usage: bool | None = True continuous_usage_stats: bool | None = False continuous_usage_stats class-attribute instance-attribute ¶ continuous_usage_stats: bool | None = False include_usage class-attribute instance-attribute ¶ include_usage: bool | None = True StructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 234 235 236class StructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] format: Any format instance-attribute ¶ format: Any type instance-attribute ¶ type: Literal['structural_tag'] TokenizeChatRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974class TokenizeChatRequest(OpenAIBaseModel): model: str | None = None messages: list[ChatCompletionMessageParam] add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=(\"A list of tools the model may call.\"), ) @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=\"A list of tools the model may call.\", ) check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1966 1967 1968 1969 1970 1971 1972 1973 1974@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data TokenizeCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900class TokenizeCompletionRequest(OpenAIBaseModel): model: str | None = None prompt: str add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) model class-attribute instance-attribute ¶ model: str | None = None prompt instance-attribute ¶ prompt: str return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) TokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1980 1981 1982 1983 1984class TokenizeResponse(OpenAIBaseModel): count: int max_model_len: int tokens: list[int] token_strs: list[str] | None = None count instance-attribute ¶ count: int max_model_len instance-attribute ¶ max_model_len: int token_strs class-attribute instance-attribute ¶ token_strs: list[str] | None = None tokens instance-attribute ¶ tokens: list[int] TokenizerInfoResponse ¶ Bases: OpenAIBaseModel Response containing tokenizer configuration equivalent to tokenizer_config.json Source code in vllm/entrypoints/openai/protocol.py 1996 1997 1998 1999 2000 2001 2002 2003class TokenizerInfoResponse(OpenAIBaseModel): \"\"\" Response containing tokenizer configuration equivalent to tokenizer_config.json \"\"\" model_config = ConfigDict(extra=\"allow\") tokenizer_class: str model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') tokenizer_class instance-attribute ¶ tokenizer_class: str ToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1483 1484 1485 1486class ToolCall(OpenAIBaseModel): id: str = Field(default_factory=make_tool_call_id) type: Literal[\"function\"] = \"function\" function: FunctionCall function instance-attribute ¶ function: FunctionCall id class-attribute instance-attribute ¶ id: str = Field(default_factory=make_tool_call_id) type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' TranscriptionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213class TranscriptionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranscription file: UploadFile \"\"\" The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" language: str | None = None \"\"\"The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" ## TODO (varun) : Support if set to 0, certain thresholds are met !! timestamp_granularities: list[Literal[\"word\", \"segment\"]] = Field( alias=\"timestamp_granularities[]\", default=[] ) \"\"\"The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. \"\"\" stream: bool | None = False \"\"\"When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # --8<-- [start:transcription-extra-params] # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:transcription-extra-params] to_language: str | None = None \"\"\"The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. \"\"\" # --8<-- [start:transcription-sampling-params] temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" top_p: float | None = None \"\"\"Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds `p`. \"\"\" top_k: int | None = None \"\"\"Limits sampling to the `k` most probable tokens at each step.\"\"\" min_p: float | None = None \"\"\"Filters out tokens with a probability lower than `min_p`, ensuring a minimum likelihood threshold during sampling. \"\"\" seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" frequency_penalty: float | None = 0.0 \"\"\"The frequency penalty to use for sampling.\"\"\" repetition_penalty: float | None = None \"\"\"The repetition penalty to use for sampling.\"\"\" presence_penalty: float | None = 0.0 \"\"\"The presence penalty to use for sampling.\"\"\" max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:transcription-sampling-params] # Default sampling parameters for transcription requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } file instance-attribute ¶ file: UploadFile The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 The frequency penalty to use for sampling. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. min_p class-attribute instance-attribute ¶ min_p: float | None = None Filters out tokens with a probability lower than min_p, ensuring a minimum likelihood threshold during sampling. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 The presence penalty to use for sampling. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None The repetition penalty to use for sampling. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. timestamp_granularities class-attribute instance-attribute ¶ timestamp_granularities: list[ Literal[\"word\", \"segment\"] ] = Field(alias=\"timestamp_granularities[]\", default=[]) The timestamp granularities to populate for this transcription. response_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. top_k class-attribute instance-attribute ¶ top_k: int | None = None Limits sampling to the k most probable tokens at each step. top_p class-attribute instance-attribute ¶ top_p: float | None = None Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds p. vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_transcription_request classmethod ¶ validate_transcription_request(data) Source code in vllm/entrypoints/openai/protocol.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213@model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranscriptionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2222 2223 2224 2225class TranscriptionResponse(OpenAIBaseModel): text: str \"\"\"The transcribed text.\"\"\" usage: TranscriptionUsageAudio text instance-attribute ¶ text: str The transcribed text. usage instance-attribute ¶ usage: TranscriptionUsageAudio TranscriptionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1619 1620 1621 1622class TranscriptionResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranscriptionResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295class TranscriptionResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The transcribed text.\"\"\" segments: list[TranscriptionSegment] | None = None \"\"\"Segments of the transcribed text and their corresponding details.\"\"\" words: list[TranscriptionWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranscriptionSegment] | None = None Segments of the transcribed text and their corresponding details. text instance-attribute ¶ text: str The transcribed text. words class-attribute instance-attribute ¶ words: list[TranscriptionWord] | None = None Extracted words and their corresponding timestamps. TranscriptionSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278class TranscriptionSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranscriptionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1625 1626 1627 1628 1629 1630 1631class TranscriptionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsc-{random_uuid()}\") object: Literal[\"transcription.chunk\"] = \"transcription.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranscriptionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranscriptionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsc-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"transcription.chunk\"] = ( \"transcription.chunk\" ) usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranscriptionUsageAudio ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2217 2218 2219class TranscriptionUsageAudio(OpenAIBaseModel): type: Literal[\"duration\"] = \"duration\" seconds: int seconds instance-attribute ¶ seconds: int type class-attribute instance-attribute ¶ type: Literal['duration'] = 'duration' TranscriptionWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2228 2229 2230 2231 2232 2233 2234 2235 2236class TranscriptionWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. TranslationRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435class TranslationRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranslation file: UploadFile \"\"\" The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" # TODO support additional sampling parameters # --8<-- [start:translation-sampling-params] seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" # --8<-- [end:translation-sampling-params] # --8<-- [start:translation-extra-params] language: str | None = None \"\"\"The language of the input audio we translate from. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy. \"\"\" to_language: str | None = None \"\"\"The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports `to_language=en`. \"\"\" stream: bool | None = False \"\"\"Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:translation-extra-params] # Default sampling parameters for translation requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"temperature\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = {'temperature': 0} file instance-attribute ¶ file: UploadFile The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio we translate from. Supplying the input language in ISO-639-1 format will improve accuracy. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports to_language=en. to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranslationResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2439 2440 2441class TranslationResponse(OpenAIBaseModel): text: str \"\"\"The translated text.\"\"\" text instance-attribute ¶ text: str The translated text. TranslationResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2303 2304 2305 2306class TranslationResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranslationResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511class TranslationResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The translated text.\"\"\" segments: list[TranslationSegment] | None = None \"\"\"Segments of the translated text and their corresponding details.\"\"\" words: list[TranslationWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranslationSegment] | None = None Segments of the translated text and their corresponding details. text instance-attribute ¶ text: str The translated text. words class-attribute instance-attribute ¶ words: list[TranslationWord] | None = None Extracted words and their corresponding timestamps. TranslationSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494class TranslationSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranslationStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2309 2310 2311 2312 2313 2314 2315class TranslationStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsl-{random_uuid()}\") object: Literal[\"translation.chunk\"] = \"translation.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranslationResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranslationResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['translation.chunk'] = 'translation.chunk' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranslationWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2444 2445 2446 2447 2448 2449 2450 2451 2452class TranslationWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. UnloadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2011 2012 2013class UnloadLoRAAdapterRequest(BaseModel): lora_name: str lora_int_id: int | None = Field(default=None) lora_int_id class-attribute instance-attribute ¶ lora_int_id: int | None = Field(default=None) lora_name instance-attribute ¶ lora_name: str UsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 199 200 201 202 203class UsageInfo(OpenAIBaseModel): prompt_tokens: int = 0 total_tokens: int = 0 completion_tokens: int | None = 0 prompt_tokens_details: PromptTokenUsageInfo | None = None completion_tokens class-attribute instance-attribute ¶ completion_tokens: int | None = 0 prompt_tokens class-attribute instance-attribute ¶ prompt_tokens: int = 0 prompt_tokens_details class-attribute instance-attribute ¶ prompt_tokens_details: PromptTokenUsageInfo | None = None total_tokens class-attribute instance-attribute ¶ total_tokens: int = 0 VLLMValidationError ¶ Bases: ValueError vLLM-specific validation error for request validation failures. Parameters: Name Type Description Default message str The error message describing the validation failure. required parameter str | None Optional parameter name that failed validation. None value Any Optional value that was rejected during validation. None Source code in vllm/entrypoints/openai/protocol.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161class VLLMValidationError(ValueError): \"\"\"vLLM-specific validation error for request validation failures. Args: message: The error message describing the validation failure. parameter: Optional parameter name that failed validation. value: Optional value that was rejected during validation. \"\"\" def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base parameter instance-attribute ¶ parameter = parameter value instance-attribute ¶ value = value __init__ ¶ __init__( message: str, *, parameter: str | None = None, value: Any = None, ) -> None Source code in vllm/entrypoints/openai/protocol.py 143 144 145 146 147 148 149 150 151 152def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value __str__ ¶ __str__() Source code in vllm/entrypoints/openai/protocol.py 154 155 156 157 158 159 160 161def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base get_logits_processors ¶ get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None Source code in vllm/entrypoints/openai/protocol.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324def get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None: if processors and pattern: logits_processors = [] for processor in processors: qualname = processor if isinstance(processor, str) else processor.qualname if not re.match(pattern, qualname): raise ValueError( f\"Logits processor '{qualname}' is not allowed by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) try: logits_processor = resolve_obj_by_qualname(qualname) except Exception as e: raise ValueError( f\"Logits processor '{qualname}' could not be resolved: {e}\" ) from e if isinstance(processor, LogitsProcessorConstructor): logits_processor = logits_processor( *processor.args or [], **processor.kwargs or {} ) logits_processors.append(logits_processor) return logits_processors elif processors: raise ValueError( \"The `logits_processors` argument is not supported by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) return None serialize_message ¶ serialize_message(msg) Serializes a single message Source code in vllm/entrypoints/openai/protocol.py 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665def serialize_message(msg): \"\"\" Serializes a single message \"\"\" if isinstance(msg, dict): return msg elif hasattr(msg, \"to_dict\"): return msg.to_dict() else: # fallback to pyandic dump return msg.model_dump_json() serialize_messages ¶ serialize_messages(msgs) Serializes multiple messages Source code in vllm/entrypoints/openai/protocol.py 1668 1669 1670 1671 1672def serialize_messages(msgs): \"\"\" Serializes multiple messages \"\"\" return [serialize_message(msg) for msg in msgs] if msgs else None",
      "code": ""
    },
    {
      "description": "AnyResponseFormat module-attribute ¶ AnyResponseFormat: TypeAlias = ( ResponseFormat | StructuralTagResponseFormat | LegacyStructuralTagResponseFormat ) AnyStructuralTagResponseFormat module-attribute ¶ AnyStructuralTagResponseFormat: TypeAlias = ( LegacyStructuralTagResponseFormat | StructuralTagResponseFormat ) AudioResponseFormat module-attribute ¶ AudioResponseFormat: TypeAlias = Literal[ \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\" ] LogitsProcessors module-attribute ¶ LogitsProcessors = list[str | LogitsProcessorConstructor] ResponseInputOutputItem module-attribute ¶ ResponseInputOutputItem: TypeAlias = ( ResponseInputItemParam | ResponseOutputItem ) ResponseInputOutputMessage module-attribute ¶ ResponseInputOutputMessage: TypeAlias = ( list[ChatCompletionMessageParam] | list[ResponseRawMessageAndToken] ) StreamingResponsesResponse module-attribute ¶ StreamingResponsesResponse: TypeAlias = ( ResponseCreatedEvent | ResponseInProgressEvent | ResponseCompletedEvent | ResponseOutputItemAddedEvent | ResponseOutputItemDoneEvent | ResponseContentPartAddedEvent | ResponseContentPartDoneEvent | ResponseReasoningTextDeltaEvent | ResponseReasoningTextDoneEvent | ResponseReasoningPartAddedEvent | ResponseReasoningPartDoneEvent | ResponseCodeInterpreterCallInProgressEvent | ResponseCodeInterpreterCallCodeDeltaEvent | ResponseWebSearchCallInProgressEvent | ResponseWebSearchCallSearchingEvent | ResponseWebSearchCallCompletedEvent | ResponseCodeInterpreterCallCodeDoneEvent | ResponseCodeInterpreterCallInterpretingEvent | ResponseCodeInterpreterCallCompletedEvent | ResponseMcpCallArgumentsDeltaEvent | ResponseMcpCallArgumentsDoneEvent | ResponseMcpCallInProgressEvent | ResponseMcpCallCompletedEvent ) TokenizeRequest module-attribute ¶ TokenizeRequest: TypeAlias = ( TokenizeCompletionRequest | TokenizeChatRequest ) TranscriptionResponseVariant module-attribute ¶ TranscriptionResponseVariant: TypeAlias = ( TranscriptionResponse | TranscriptionResponseVerbose ) TranslationResponseVariant module-attribute ¶ TranslationResponseVariant: TypeAlias = ( TranslationResponse | TranslationResponseVerbose ) _LONG_INFO module-attribute ¶ _LONG_INFO = iinfo(long) logger module-attribute ¶ logger = init_logger(__name__) ChatCompletionLogProb ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1535 1536 1537 1538class ChatCompletionLogProb(OpenAIBaseModel): token: str logprob: float = -9999.0 bytes: list[int] | None = None bytes class-attribute instance-attribute ¶ bytes: list[int] | None = None logprob class-attribute instance-attribute ¶ logprob: float = -9999.0 token instance-attribute ¶ token: str ChatCompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1548 1549class ChatCompletionLogProbs(OpenAIBaseModel): content: list[ChatCompletionLogProbsContent] | None = None content class-attribute instance-attribute ¶ content: list[ChatCompletionLogProbsContent] | None = None ChatCompletionLogProbsContent ¶ Bases: ChatCompletionLogProb Source code in vllm/entrypoints/openai/protocol.py 1541 1542 1543 1544 1545class ChatCompletionLogProbsContent(ChatCompletionLogProb): # Workaround: redefine fields name cache so that it's not # shared with the super class. field_names: ClassVar[set[str] | None] = None top_logprobs: list[ChatCompletionLogProb] = Field(default_factory=list) field_names class-attribute ¶ field_names: set[str] | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[ChatCompletionLogProb] = Field( default_factory=list ) ChatCompletionNamedFunction ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 271 272class ChatCompletionNamedFunction(OpenAIBaseModel): name: str name instance-attribute ¶ name: str ChatCompletionNamedToolChoiceParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 275 276 277class ChatCompletionNamedToolChoiceParam(OpenAIBaseModel): function: ChatCompletionNamedFunction type: Literal[\"function\"] = \"function\" function instance-attribute ¶ function: ChatCompletionNamedFunction type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054class ChatCompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/chat/create messages: list[ChatCompletionMessageParam] model: str | None = None frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: bool | None = False top_logprobs: int | None = 0 max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of \" \"the max_completion_tokens field\", ) max_completion_tokens: int | None = None n: int | None = 1 presence_penalty: float | None = 0.0 response_format: AnyResponseFormat | None = None seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None temperature: float | None = None top_p: float | None = None tools: list[ChatCompletionToolsParam] | None = None tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" reasoning_effort: Literal[\"low\", \"medium\", \"high\"] | None = None include_reasoning: bool = True parallel_tool_calls: bool | None = True # NOTE this will be ignored by vLLM user: str | None = None # --8<-- [start:chat-completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None prompt_logprobs: int | None = None allowed_token_ids: list[int] | None = None bad_words: list[str] = Field(default_factory=list) # --8<-- [end:chat-completion-sampling-params] # --8<-- [start:chat-completion-extra-params] echo: bool = Field( default=False, description=( \"If true, the new message will be prepended with the last message \" \"if they belong to the same role.\" ), ) add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) documents: list[dict[str, str]] | None = Field( default=None, description=( \"A list of dicts representing documents that will be accessible to \" \"the model if it is performing RAG (retrieval-augmented generation).\" \" If the template does not support RAG, this argument will have no \" \"effect. We recommend that each document should be a dict containing \" '\"title\" and \"text\" keys.' ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float | list[str | int | float]] | None = Field( default=None, description=( \"Additional request parameters with (list of) string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:chat-completion-extra-params] # Default sampling parameters for chat completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data @model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None bad_words class-attribute instance-attribute ¶ bad_words: list[str] = Field(default_factory=list) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) documents class-attribute instance-attribute ¶ documents: list[dict[str, str]] | None = Field( default=None, description='A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.', ) echo class-attribute instance-attribute ¶ echo: bool = Field( default=False, description=\"If true, the new message will be prepended with the last message if they belong to the same role.\", ) frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_reasoning class-attribute instance-attribute ¶ include_reasoning: bool = True include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: bool | None = False max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of the max_completion_tokens field\", ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int | None = 1 parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None reasoning_effort class-attribute instance-attribute ¶ reasoning_effort: ( Literal[\"low\", \"medium\", \"high\"] | None ) = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = None return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) temperature class-attribute instance-attribute ¶ temperature: float | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: ( dict[str, str | int | float | list[str | int | float]] | None ) = Field( default=None, description=\"Additional request parameters with (list of) string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1035 1036 1037 1038 1039 1040 1041 1042 1043@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data check_tool_usage classmethod ¶ check_tool_usage(data) Source code in vllm/entrypoints/openai/protocol.py 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033@model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 883 884 885 886 887 888 889 890 891 892@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data ChatCompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580class ChatCompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion\"] = \"chat.completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[ChatCompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['chat.completion'] = 'chat.completion' prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo ChatCompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562class ChatCompletionResponseChoice(OpenAIBaseModel): index: int message: ChatMessage logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" # not part of the OpenAI spec but included in vLLM for legacy reasons stop_reason: int | str | None = None # not part of the OpenAI spec but is useful for tracing the tokens # in agent scenarios token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None message instance-attribute ¶ message: ChatMessage stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1598 1599 1600 1601 1602 1603 1604 1605class ChatCompletionResponseStreamChoice(OpenAIBaseModel): index: int delta: DeltaMessage logprobs: ChatCompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = None # not part of the OpenAI spec but for tracing the tokens token_ids: list[int] | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1608 1609 1610 1611 1612 1613 1614 1615 1616class ChatCompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) # not part of the OpenAI spec but for tracing the tokens prompt_token_ids: list[int] | None = None choices instance-attribute ¶ choices: list[ChatCompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"chat.completion.chunk\"] = ( \"chat.completion.chunk\" ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) ChatCompletionToolsParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 266 267 268class ChatCompletionToolsParam(OpenAIBaseModel): type: Literal[\"function\"] = \"function\" function: FunctionDefinition function instance-attribute ¶ function: FunctionDefinition type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532class ChatMessage(OpenAIBaseModel): role: str content: str | None = None refusal: str | None = None annotations: OpenAIAnnotation | None = None audio: OpenAIChatCompletionAudio | None = None function_call: FunctionCall | None = None tool_calls: list[ToolCall] = Field(default_factory=list) # vLLM-specific fields that are not in OpenAI spec reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self annotations class-attribute instance-attribute ¶ annotations: Annotation | None = None audio class-attribute instance-attribute ¶ audio: ChatCompletionAudio | None = None content class-attribute instance-attribute ¶ content: str | None = None function_call class-attribute instance-attribute ¶ function_call: FunctionCall | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. refusal class-attribute instance-attribute ¶ refusal: str | None = None role instance-attribute ¶ role: str tool_calls class-attribute instance-attribute ¶ tool_calls: list[ToolCall] = Field(default_factory=list) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1528 1529 1530 1531 1532@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self CompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1409 1410 1411 1412 1413class CompletionLogProbs(OpenAIBaseModel): text_offset: list[int] = Field(default_factory=list) token_logprobs: list[float | None] = Field(default_factory=list) tokens: list[str] = Field(default_factory=list) top_logprobs: list[dict[str, float] | None] = Field(default_factory=list) text_offset class-attribute instance-attribute ¶ text_offset: list[int] = Field(default_factory=list) token_logprobs class-attribute instance-attribute ¶ token_logprobs: list[float | None] = Field( default_factory=list ) tokens class-attribute instance-attribute ¶ tokens: list[str] = Field(default_factory=list) top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[dict[str, float] | None] = Field( default_factory=list ) CompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406class CompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/completions/create model: str | None = None prompt: list[int] | list[list[int]] | str | list[str] | None = None echo: bool | None = False frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: int | None = None max_tokens: int | None = 16 n: int = 1 presence_penalty: float | None = 0.0 seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None suffix: str | None = None temperature: float | None = None top_p: float | None = None user: str | None = None # --8<-- [start:completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None allowed_token_ids: list[int] | None = None prompt_logprobs: int | None = None # --8<-- [end:completion-sampling-params] # --8<-- [start:completion-extra-params] prompt_embeds: bytes | list[bytes] | None = None add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) response_format: AnyResponseFormat | None = Field( default=None, description=( \"Similar to chat completion, this parameter specifies the format \" \"of output. Only {'type': 'json_object'}, {'type': 'json_schema'}\" \", {'type': 'structural_tag'}, or {'type': 'text' } is supported.\" ), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:completion-extra-params] # Default sampling parameters for completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) echo class-attribute instance-attribute ¶ echo: bool | None = False frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = 16 min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int = 1 presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ( list[int] | list[list[int]] | str | list[str] | None ) = None prompt_embeds class-attribute instance-attribute ¶ prompt_embeds: bytes | list[bytes] | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = Field( default=None, description=\"Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'}, {'type': 'json_schema'}, {'type': 'structural_tag'}, or {'type': 'text' } is supported.\", ) return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) suffix class-attribute instance-attribute ¶ suffix: str | None = None temperature class-attribute instance-attribute ¶ temperature: float | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_prompt_and_prompt_embeds classmethod ¶ validate_prompt_and_prompt_embeds(data) Source code in vllm/entrypoints/openai/protocol.py 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395@model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data CompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447class CompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: Literal[\"text_completion\"] = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[CompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['text_completion'] = 'text_completion' service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo CompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431class CompletionResponseChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) token_ids: list[int] | None = None # For response prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None # For prompt finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466class CompletionResponseStreamChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) # not part of the OpenAI spec but for tracing the tokens # prompt tokens is put into choice to align with CompletionResponseChoice prompt_token_ids: list[int] | None = None token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1469 1470 1471 1472 1473 1474 1475class CompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: str = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[CompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: str = 'text_completion' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) DeltaFunctionCall ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1489 1490 1491class DeltaFunctionCall(BaseModel): name: str | None = None arguments: str | None = None arguments class-attribute instance-attribute ¶ arguments: str | None = None name class-attribute instance-attribute ¶ name: str | None = None DeltaMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595class DeltaMessage(OpenAIBaseModel): role: str | None = None content: str | None = None reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" tool_calls: list[DeltaToolCall] = Field(default_factory=list) @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self content class-attribute instance-attribute ¶ content: str | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. role class-attribute instance-attribute ¶ role: str | None = None tool_calls class-attribute instance-attribute ¶ tool_calls: list[DeltaToolCall] = Field( default_factory=list ) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1591 1592 1593 1594 1595@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self DeltaToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1495 1496 1497 1498 1499class DeltaToolCall(OpenAIBaseModel): id: str | None = None type: Literal[\"function\"] | None = None index: int function: DeltaFunctionCall | None = None function class-attribute instance-attribute ¶ function: DeltaFunctionCall | None = None id class-attribute instance-attribute ¶ id: str | None = None index instance-attribute ¶ index: int type class-attribute instance-attribute ¶ type: Literal['function'] | None = None DetokenizeRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1987 1988 1989class DetokenizeRequest(OpenAIBaseModel): model: str | None = None tokens: list[int] model class-attribute instance-attribute ¶ model: str | None = None tokens instance-attribute ¶ tokens: list[int] DetokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1992 1993class DetokenizeResponse(OpenAIBaseModel): prompt: str prompt instance-attribute ¶ prompt: str ErrorInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 123 124 125 126 127class ErrorInfo(OpenAIBaseModel): message: str type: str param: str | None = None code: int code instance-attribute ¶ code: int message instance-attribute ¶ message: str param class-attribute instance-attribute ¶ param: str | None = None type instance-attribute ¶ type: str ErrorResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 130 131class ErrorResponse(OpenAIBaseModel): error: ErrorInfo error instance-attribute ¶ error: ErrorInfo ExtractedToolCallInformation ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511class ExtractedToolCallInformation(BaseModel): # indicate if tools were called tools_called: bool # extracted tool calls tool_calls: list[ToolCall] # content - per OpenAI spec, content AND tool calls can be returned rarely # But some models will do this intentionally content: str | None = None content class-attribute instance-attribute ¶ content: str | None = None tool_calls instance-attribute ¶ tool_calls: list[ToolCall] tools_called instance-attribute ¶ tools_called: bool FunctionCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1478 1479 1480class FunctionCall(OpenAIBaseModel): name: str arguments: str arguments instance-attribute ¶ arguments: str name instance-attribute ¶ name: str FunctionDefinition ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 260 261 262 263class FunctionDefinition(OpenAIBaseModel): name: str description: str | None = None parameters: dict[str, Any] | None = None description class-attribute instance-attribute ¶ description: str | None = None name instance-attribute ¶ name: str parameters class-attribute instance-attribute ¶ parameters: dict[str, Any] | None = None GenerateRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564class GenerateRequest(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) token_ids: list[int] \"\"\"The token ids to generate text from.\"\"\" # features: MultiModalFeatureSpec # TODO (NickLucche): implement once Renderer work is completed features: str | None = None \"\"\"The processed MM inputs for the model.\"\"\" sampling_params: SamplingParams \"\"\"The sampling parameters for the model.\"\"\" model: str | None = None stream: bool | None = False stream_options: StreamOptions | None = None cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) features class-attribute instance-attribute ¶ features: str | None = None The processed MM inputs for the model. kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) model class-attribute instance-attribute ¶ model: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) sampling_params instance-attribute ¶ sampling_params: SamplingParams The sampling parameters for the model. stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None token_ids instance-attribute ¶ token_ids: list[int] The token ids to generate text from. GenerateResponse ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591class GenerateResponse(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) choices: list[GenerateResponseChoice] prompt_logprobs: list[dict[int, Logprob] | None] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) choices instance-attribute ¶ choices: list[GenerateResponseChoice] kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) GenerateResponseChoice ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2567 2568 2569 2570 2571 2572class GenerateResponseChoice(BaseModel): index: int logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None InputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1634 1635 1636 1637class InputTokensDetails(OpenAIBaseModel): cached_tokens: int input_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens instance-attribute ¶ cached_tokens: int cached_tokens_per_turn class-attribute instance-attribute ¶ cached_tokens_per_turn: list[int] = Field( default_factory=list ) input_tokens_per_turn class-attribute instance-attribute ¶ input_tokens_per_turn: list[int] = Field( default_factory=list ) JsonSchemaResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 211 212 213 214 215 216 217class JsonSchemaResponseFormat(OpenAIBaseModel): name: str description: str | None = None # schema is the field in openai but that causes conflicts with pydantic so # instead use json_schema with an alias json_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") strict: bool | None = None description class-attribute instance-attribute ¶ description: str | None = None json_schema class-attribute instance-attribute ¶ json_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) name instance-attribute ¶ name: str strict class-attribute instance-attribute ¶ strict: bool | None = None LegacyStructuralTag ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 220 221 222 223 224 225class LegacyStructuralTag(OpenAIBaseModel): begin: str # schema is the field, but that causes conflicts with pydantic so # instead use structural_tag_schema with an alias structural_tag_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") end: str begin instance-attribute ¶ begin: str end instance-attribute ¶ end: str structural_tag_schema class-attribute instance-attribute ¶ structural_tag_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) LegacyStructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 228 229 230 231class LegacyStructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] structures: list[LegacyStructuralTag] triggers: list[str] structures instance-attribute ¶ structures: list[LegacyStructuralTag] triggers instance-attribute ¶ triggers: list[str] type instance-attribute ¶ type: Literal['structural_tag'] LoadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2006 2007 2008class LoadLoRAAdapterRequest(BaseModel): lora_name: str lora_path: str lora_name instance-attribute ¶ lora_name: str lora_path instance-attribute ¶ lora_path: str LogitsProcessorConstructor ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 282 283 284 285 286 287class LogitsProcessorConstructor(BaseModel): qualname: str args: list[Any] | None = None kwargs: dict[str, Any] | None = None model_config = ConfigDict(extra=\"forbid\") args class-attribute instance-attribute ¶ args: list[Any] | None = None kwargs class-attribute instance-attribute ¶ kwargs: dict[str, Any] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='forbid') qualname instance-attribute ¶ qualname: str ModelCard ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 179 180 181 182 183 184 185 186 187class ModelCard(OpenAIBaseModel): id: str object: str = \"model\" created: int = Field(default_factory=lambda: int(time.time())) owned_by: str = \"vllm\" root: str | None = None parent: str | None = None max_model_len: int | None = None permission: list[ModelPermission] = Field(default_factory=list) created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id instance-attribute ¶ id: str max_model_len class-attribute instance-attribute ¶ max_model_len: int | None = None object class-attribute instance-attribute ¶ object: str = 'model' owned_by class-attribute instance-attribute ¶ owned_by: str = 'vllm' parent class-attribute instance-attribute ¶ parent: str | None = None permission class-attribute instance-attribute ¶ permission: list[ModelPermission] = Field( default_factory=list ) root class-attribute instance-attribute ¶ root: str | None = None ModelList ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 190 191 192class ModelList(OpenAIBaseModel): object: str = \"list\" data: list[ModelCard] = Field(default_factory=list) data class-attribute instance-attribute ¶ data: list[ModelCard] = Field(default_factory=list) object class-attribute instance-attribute ¶ object: str = 'list' ModelPermission ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 164 165 166 167 168 169 170 171 172 173 174 175 176class ModelPermission(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\") object: str = \"model_permission\" created: int = Field(default_factory=lambda: int(time.time())) allow_create_engine: bool = False allow_sampling: bool = True allow_logprobs: bool = True allow_search_indices: bool = False allow_view: bool = True allow_fine_tuning: bool = False organization: str = \"*\" group: str | None = None is_blocking: bool = False allow_create_engine class-attribute instance-attribute ¶ allow_create_engine: bool = False allow_fine_tuning class-attribute instance-attribute ¶ allow_fine_tuning: bool = False allow_logprobs class-attribute instance-attribute ¶ allow_logprobs: bool = True allow_sampling class-attribute instance-attribute ¶ allow_sampling: bool = True allow_search_indices class-attribute instance-attribute ¶ allow_search_indices: bool = False allow_view class-attribute instance-attribute ¶ allow_view: bool = True created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) group class-attribute instance-attribute ¶ group: str | None = None id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"modelperm-{random_uuid()}\" ) is_blocking class-attribute instance-attribute ¶ is_blocking: bool = False object class-attribute instance-attribute ¶ object: str = 'model_permission' organization class-attribute instance-attribute ¶ organization: str = '*' OpenAIBaseModel ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120class OpenAIBaseModel(BaseModel): # OpenAI API does allow extra fields model_config = ConfigDict(extra=\"allow\") # Cache class field names field_names: ClassVar[set[str] | None] = None @model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result field_names class-attribute ¶ field_names: set[str] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') __log_extra_fields__ classmethod ¶ __log_extra_fields__(data, handler) Source code in vllm/entrypoints/openai/protocol.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120@model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result OutputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1640 1641 1642 1643 1644class OutputTokensDetails(OpenAIBaseModel): reasoning_tokens: int = 0 tool_output_tokens: int = 0 output_tokens_per_turn: list[int] = Field(default_factory=list) tool_output_tokens_per_turn: list[int] = Field(default_factory=list) output_tokens_per_turn class-attribute instance-attribute ¶ output_tokens_per_turn: list[int] = Field( default_factory=list ) reasoning_tokens class-attribute instance-attribute ¶ reasoning_tokens: int = 0 tool_output_tokens class-attribute instance-attribute ¶ tool_output_tokens: int = 0 tool_output_tokens_per_turn class-attribute instance-attribute ¶ tool_output_tokens_per_turn: list[int] = Field( default_factory=list ) PromptTokenUsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 195 196class PromptTokenUsageInfo(OpenAIBaseModel): cached_tokens: int | None = None cached_tokens class-attribute instance-attribute ¶ cached_tokens: int | None = None RequestResponseMetadata ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 206 207 208class RequestResponseMetadata(BaseModel): request_id: str final_usage_info: UsageInfo | None = None final_usage_info class-attribute instance-attribute ¶ final_usage_info: UsageInfo | None = None request_id instance-attribute ¶ request_id: str ResponseCompletedEvent ¶ Bases: ResponseCompletedEvent Source code in vllm/entrypoints/openai/protocol.py 1845 1846class ResponseCompletedEvent(OpenAIResponseCompletedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseCreatedEvent ¶ Bases: ResponseCreatedEvent Source code in vllm/entrypoints/openai/protocol.py 1849 1850class ResponseCreatedEvent(OpenAIResponseCreatedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 244 245 246 247class ResponseFormat(OpenAIBaseModel): # type must be \"json_schema\", \"json_object\", or \"text\" type: Literal[\"text\", \"json_object\", \"json_schema\"] json_schema: JsonSchemaResponseFormat | None = None json_schema class-attribute instance-attribute ¶ json_schema: JsonSchemaResponseFormat | None = None type instance-attribute ¶ type: Literal['text', 'json_object', 'json_schema'] ResponseInProgressEvent ¶ Bases: ResponseInProgressEvent Source code in vllm/entrypoints/openai/protocol.py 1853 1854class ResponseInProgressEvent(OpenAIResponseInProgressEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseRawMessageAndToken ¶ Bases: OpenAIBaseModel Class to show the raw message. If message / tokens diverge, tokens is the source of truth Source code in vllm/entrypoints/openai/protocol.py 1675 1676 1677 1678 1679 1680 1681class ResponseRawMessageAndToken(OpenAIBaseModel): \"\"\"Class to show the raw message. If message / tokens diverge, tokens is the source of truth\"\"\" message: str tokens: list[int] type: Literal[\"raw_message_tokens\"] = \"raw_message_tokens\" message instance-attribute ¶ message: str tokens instance-attribute ¶ tokens: list[int] type class-attribute instance-attribute ¶ type: Literal['raw_message_tokens'] = 'raw_message_tokens' ResponseReasoningPartAddedEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840class ResponseReasoningPartAddedEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.added\"] \"\"\"The type of the event. Always `response.reasoning_part.added`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.added'] The type of the event. Always response.reasoning_part.added. ResponseReasoningPartDoneEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818class ResponseReasoningPartDoneEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.done\"] \"\"\"The type of the event. Always `response.reasoning_part.done`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.done'] The type of the event. Always response.reasoning_part.done. ResponseUsage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1647 1648 1649 1650 1651 1652class ResponseUsage(OpenAIBaseModel): input_tokens: int input_tokens_details: InputTokensDetails output_tokens: int output_tokens_details: OutputTokensDetails total_tokens: int input_tokens instance-attribute ¶ input_tokens: int input_tokens_details instance-attribute ¶ input_tokens_details: InputTokensDetails output_tokens instance-attribute ¶ output_tokens: int output_tokens_details instance-attribute ¶ output_tokens_details: OutputTokensDetails total_tokens instance-attribute ¶ total_tokens: int ResponsesRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555class ResponsesRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/responses/create background: bool | None = False include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input: str | list[ResponseInputOutputItem] instructions: str | None = None max_output_tokens: int | None = None max_tool_calls: int | None = None metadata: Metadata | None = None model: str | None = None logit_bias: dict[str, float] | None = None parallel_tool_calls: bool | None = True previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] = \"auto\" store: bool | None = True stream: bool | None = False temperature: float | None = None text: ResponseTextConfig | None = None tool_choice: ToolChoice = \"auto\" tools: list[Tool] = Field(default_factory=list) top_logprobs: int | None = 0 top_p: float | None = None top_k: int | None = None truncation: Literal[\"auto\", \"disabled\"] | None = \"disabled\" user: str | None = None # --8<-- [start:responses-extra-params] request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) enable_response_messages: bool = Field( default=False, description=( \"Dictates whether or not to return messages as part of the \" \"response object. Currently only supported for\" \"non-background and gpt-oss only. \" ), ) # similar to input_messages / output_messages in ResponsesResponse # we take in previous_input_messages (ie in harmony format) # this cannot be used in conjunction with previous_response_id # TODO: consider supporting non harmony messages as well previous_input_messages: list[OpenAIHarmonyMessage | dict] | None = None # --8<-- [end:responses-extra-params] _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) @model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data @model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data @model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data @model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } background class-attribute instance-attribute ¶ background: bool | None = False cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) enable_response_messages class-attribute instance-attribute ¶ enable_response_messages: bool = Field( default=False, description=\"Dictates whether or not to return messages as part of the response object. Currently only supported fornon-background and gpt-oss only. \", ) include class-attribute instance-attribute ¶ include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input instance-attribute ¶ input: str | list[ResponseInputOutputItem] instructions class-attribute instance-attribute ¶ instructions: str | None = None logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None max_output_tokens class-attribute instance-attribute ¶ max_output_tokens: int | None = None max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True previous_input_messages class-attribute instance-attribute ¶ previous_input_messages: list[Message | dict] | None = None previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) service_tier class-attribute instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] = \"auto\" store class-attribute instance-attribute ¶ store: bool | None = True stream class-attribute instance-attribute ¶ stream: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float | None = None text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ToolChoice = 'auto' tools class-attribute instance-attribute ¶ tools: list[Tool] = Field(default_factory=list) top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncation class-attribute instance-attribute ¶ truncation: Literal['auto', 'disabled'] | None = 'disabled' user class-attribute instance-attribute ¶ user: str | None = None check_cache_salt_support ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 505 506 507 508 509 510 511 512 513@model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data function_call_parsing ¶ function_call_parsing(data) Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. Source code in vllm/entrypoints/openai/protocol.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555@model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data is_include_output_logprobs ¶ is_include_output_logprobs() -> bool Check if the request includes output logprobs. Source code in vllm/entrypoints/openai/protocol.py 480 481 482 483 484 485 486 487def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_background ¶ validate_background(data) Source code in vllm/entrypoints/openai/protocol.py 489 490 491 492 493 494 495@model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data validate_prompt ¶ validate_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 497 498 499 500 501 502 503@model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data ResponsesResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796class ResponsesResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"resp_{random_uuid()}\") created_at: int = Field(default_factory=lambda: int(time.time())) # error: Optional[ResponseError] = None incomplete_details: IncompleteDetails | None = None instructions: str | None = None metadata: Metadata | None = None model: str object: Literal[\"response\"] = \"response\" output: list[ResponseOutputItem] parallel_tool_calls: bool temperature: float tool_choice: ToolChoice tools: list[Tool] top_p: float background: bool max_output_tokens: int max_tool_calls: int | None = None previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] status: ResponseStatus text: ResponseTextConfig | None = None top_logprobs: int | None = None truncation: Literal[\"auto\", \"disabled\"] usage: ResponseUsage | None = None user: str | None = None # --8<-- [start:responses-response-extra-params] # These are populated when enable_response_messages is set to True # NOTE: custom serialization is needed # see serialize_input_messages and serialize_output_messages input_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token input to model.\" ), ) output_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token output of model.\" ), ) # --8<-- [end:responses-response-extra-params] # NOTE: openAI harmony doesn't serialize TextContent properly, # TODO: this fixes for TextContent, but need to verify for tools etc # https://github.com/openai/harmony/issues/78 @field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) # NOTE: openAI harmony doesn't serialize TextContent properly, this fixes it # https://github.com/openai/harmony/issues/78 @field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) @classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) background instance-attribute ¶ background: bool created_at class-attribute instance-attribute ¶ created_at: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\" ) incomplete_details class-attribute instance-attribute ¶ incomplete_details: IncompleteDetails | None = None input_messages class-attribute instance-attribute ¶ input_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token input to model.\", ) instructions class-attribute instance-attribute ¶ instructions: str | None = None max_output_tokens instance-attribute ¶ max_output_tokens: int max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['response'] = 'response' output instance-attribute ¶ output: list[ResponseOutputItem] output_messages class-attribute instance-attribute ¶ output_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token output of model.\", ) parallel_tool_calls instance-attribute ¶ parallel_tool_calls: bool previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None service_tier instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] status instance-attribute ¶ status: ResponseStatus temperature instance-attribute ¶ temperature: float text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice instance-attribute ¶ tool_choice: ToolChoice tools instance-attribute ¶ tools: list[Tool] top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = None top_p instance-attribute ¶ top_p: float truncation instance-attribute ¶ truncation: Literal['auto', 'disabled'] usage class-attribute instance-attribute ¶ usage: ResponseUsage | None = None user class-attribute instance-attribute ¶ user: str | None = None from_request classmethod ¶ from_request( request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> ResponsesResponse Source code in vllm/entrypoints/openai/protocol.py 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796@classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) serialize_input_messages ¶ serialize_input_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1745 1746 1747@field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) serialize_output_messages ¶ serialize_output_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1739 1740 1741@field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) StreamOptions ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 255 256 257class StreamOptions(OpenAIBaseModel): include_usage: bool | None = True continuous_usage_stats: bool | None = False continuous_usage_stats class-attribute instance-attribute ¶ continuous_usage_stats: bool | None = False include_usage class-attribute instance-attribute ¶ include_usage: bool | None = True StructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 234 235 236class StructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] format: Any format instance-attribute ¶ format: Any type instance-attribute ¶ type: Literal['structural_tag'] TokenizeChatRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974class TokenizeChatRequest(OpenAIBaseModel): model: str | None = None messages: list[ChatCompletionMessageParam] add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=(\"A list of tools the model may call.\"), ) @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=\"A list of tools the model may call.\", ) check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1966 1967 1968 1969 1970 1971 1972 1973 1974@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data TokenizeCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900class TokenizeCompletionRequest(OpenAIBaseModel): model: str | None = None prompt: str add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) model class-attribute instance-attribute ¶ model: str | None = None prompt instance-attribute ¶ prompt: str return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) TokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1980 1981 1982 1983 1984class TokenizeResponse(OpenAIBaseModel): count: int max_model_len: int tokens: list[int] token_strs: list[str] | None = None count instance-attribute ¶ count: int max_model_len instance-attribute ¶ max_model_len: int token_strs class-attribute instance-attribute ¶ token_strs: list[str] | None = None tokens instance-attribute ¶ tokens: list[int] TokenizerInfoResponse ¶ Bases: OpenAIBaseModel Response containing tokenizer configuration equivalent to tokenizer_config.json Source code in vllm/entrypoints/openai/protocol.py 1996 1997 1998 1999 2000 2001 2002 2003class TokenizerInfoResponse(OpenAIBaseModel): \"\"\" Response containing tokenizer configuration equivalent to tokenizer_config.json \"\"\" model_config = ConfigDict(extra=\"allow\") tokenizer_class: str model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') tokenizer_class instance-attribute ¶ tokenizer_class: str ToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1483 1484 1485 1486class ToolCall(OpenAIBaseModel): id: str = Field(default_factory=make_tool_call_id) type: Literal[\"function\"] = \"function\" function: FunctionCall function instance-attribute ¶ function: FunctionCall id class-attribute instance-attribute ¶ id: str = Field(default_factory=make_tool_call_id) type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' TranscriptionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213class TranscriptionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranscription file: UploadFile \"\"\" The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" language: str | None = None \"\"\"The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" ## TODO (varun) : Support if set to 0, certain thresholds are met !! timestamp_granularities: list[Literal[\"word\", \"segment\"]] = Field( alias=\"timestamp_granularities[]\", default=[] ) \"\"\"The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. \"\"\" stream: bool | None = False \"\"\"When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # --8<-- [start:transcription-extra-params] # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:transcription-extra-params] to_language: str | None = None \"\"\"The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. \"\"\" # --8<-- [start:transcription-sampling-params] temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" top_p: float | None = None \"\"\"Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds `p`. \"\"\" top_k: int | None = None \"\"\"Limits sampling to the `k` most probable tokens at each step.\"\"\" min_p: float | None = None \"\"\"Filters out tokens with a probability lower than `min_p`, ensuring a minimum likelihood threshold during sampling. \"\"\" seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" frequency_penalty: float | None = 0.0 \"\"\"The frequency penalty to use for sampling.\"\"\" repetition_penalty: float | None = None \"\"\"The repetition penalty to use for sampling.\"\"\" presence_penalty: float | None = 0.0 \"\"\"The presence penalty to use for sampling.\"\"\" max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:transcription-sampling-params] # Default sampling parameters for transcription requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } file instance-attribute ¶ file: UploadFile The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 The frequency penalty to use for sampling. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. min_p class-attribute instance-attribute ¶ min_p: float | None = None Filters out tokens with a probability lower than min_p, ensuring a minimum likelihood threshold during sampling. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 The presence penalty to use for sampling. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None The repetition penalty to use for sampling. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. timestamp_granularities class-attribute instance-attribute ¶ timestamp_granularities: list[ Literal[\"word\", \"segment\"] ] = Field(alias=\"timestamp_granularities[]\", default=[]) The timestamp granularities to populate for this transcription. response_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. top_k class-attribute instance-attribute ¶ top_k: int | None = None Limits sampling to the k most probable tokens at each step. top_p class-attribute instance-attribute ¶ top_p: float | None = None Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds p. vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_transcription_request classmethod ¶ validate_transcription_request(data) Source code in vllm/entrypoints/openai/protocol.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213@model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranscriptionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2222 2223 2224 2225class TranscriptionResponse(OpenAIBaseModel): text: str \"\"\"The transcribed text.\"\"\" usage: TranscriptionUsageAudio text instance-attribute ¶ text: str The transcribed text. usage instance-attribute ¶ usage: TranscriptionUsageAudio TranscriptionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1619 1620 1621 1622class TranscriptionResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranscriptionResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295class TranscriptionResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The transcribed text.\"\"\" segments: list[TranscriptionSegment] | None = None \"\"\"Segments of the transcribed text and their corresponding details.\"\"\" words: list[TranscriptionWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranscriptionSegment] | None = None Segments of the transcribed text and their corresponding details. text instance-attribute ¶ text: str The transcribed text. words class-attribute instance-attribute ¶ words: list[TranscriptionWord] | None = None Extracted words and their corresponding timestamps. TranscriptionSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278class TranscriptionSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranscriptionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1625 1626 1627 1628 1629 1630 1631class TranscriptionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsc-{random_uuid()}\") object: Literal[\"transcription.chunk\"] = \"transcription.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranscriptionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranscriptionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsc-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"transcription.chunk\"] = ( \"transcription.chunk\" ) usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranscriptionUsageAudio ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2217 2218 2219class TranscriptionUsageAudio(OpenAIBaseModel): type: Literal[\"duration\"] = \"duration\" seconds: int seconds instance-attribute ¶ seconds: int type class-attribute instance-attribute ¶ type: Literal['duration'] = 'duration' TranscriptionWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2228 2229 2230 2231 2232 2233 2234 2235 2236class TranscriptionWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. TranslationRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435class TranslationRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranslation file: UploadFile \"\"\" The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" # TODO support additional sampling parameters # --8<-- [start:translation-sampling-params] seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" # --8<-- [end:translation-sampling-params] # --8<-- [start:translation-extra-params] language: str | None = None \"\"\"The language of the input audio we translate from. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy. \"\"\" to_language: str | None = None \"\"\"The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports `to_language=en`. \"\"\" stream: bool | None = False \"\"\"Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:translation-extra-params] # Default sampling parameters for translation requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"temperature\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = {'temperature': 0} file instance-attribute ¶ file: UploadFile The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio we translate from. Supplying the input language in ISO-639-1 format will improve accuracy. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports to_language=en. to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranslationResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2439 2440 2441class TranslationResponse(OpenAIBaseModel): text: str \"\"\"The translated text.\"\"\" text instance-attribute ¶ text: str The translated text. TranslationResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2303 2304 2305 2306class TranslationResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranslationResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511class TranslationResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The translated text.\"\"\" segments: list[TranslationSegment] | None = None \"\"\"Segments of the translated text and their corresponding details.\"\"\" words: list[TranslationWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranslationSegment] | None = None Segments of the translated text and their corresponding details. text instance-attribute ¶ text: str The translated text. words class-attribute instance-attribute ¶ words: list[TranslationWord] | None = None Extracted words and their corresponding timestamps. TranslationSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494class TranslationSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranslationStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2309 2310 2311 2312 2313 2314 2315class TranslationStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsl-{random_uuid()}\") object: Literal[\"translation.chunk\"] = \"translation.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranslationResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranslationResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['translation.chunk'] = 'translation.chunk' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranslationWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2444 2445 2446 2447 2448 2449 2450 2451 2452class TranslationWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. UnloadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2011 2012 2013class UnloadLoRAAdapterRequest(BaseModel): lora_name: str lora_int_id: int | None = Field(default=None) lora_int_id class-attribute instance-attribute ¶ lora_int_id: int | None = Field(default=None) lora_name instance-attribute ¶ lora_name: str UsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 199 200 201 202 203class UsageInfo(OpenAIBaseModel): prompt_tokens: int = 0 total_tokens: int = 0 completion_tokens: int | None = 0 prompt_tokens_details: PromptTokenUsageInfo | None = None completion_tokens class-attribute instance-attribute ¶ completion_tokens: int | None = 0 prompt_tokens class-attribute instance-attribute ¶ prompt_tokens: int = 0 prompt_tokens_details class-attribute instance-attribute ¶ prompt_tokens_details: PromptTokenUsageInfo | None = None total_tokens class-attribute instance-attribute ¶ total_tokens: int = 0 VLLMValidationError ¶ Bases: ValueError vLLM-specific validation error for request validation failures. Parameters: Name Type Description Default message str The error message describing the validation failure. required parameter str | None Optional parameter name that failed validation. None value Any Optional value that was rejected during validation. None Source code in vllm/entrypoints/openai/protocol.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161class VLLMValidationError(ValueError): \"\"\"vLLM-specific validation error for request validation failures. Args: message: The error message describing the validation failure. parameter: Optional parameter name that failed validation. value: Optional value that was rejected during validation. \"\"\" def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base parameter instance-attribute ¶ parameter = parameter value instance-attribute ¶ value = value __init__ ¶ __init__( message: str, *, parameter: str | None = None, value: Any = None, ) -> None Source code in vllm/entrypoints/openai/protocol.py 143 144 145 146 147 148 149 150 151 152def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value __str__ ¶ __str__() Source code in vllm/entrypoints/openai/protocol.py 154 155 156 157 158 159 160 161def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base get_logits_processors ¶ get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None Source code in vllm/entrypoints/openai/protocol.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324def get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None: if processors and pattern: logits_processors = [] for processor in processors: qualname = processor if isinstance(processor, str) else processor.qualname if not re.match(pattern, qualname): raise ValueError( f\"Logits processor '{qualname}' is not allowed by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) try: logits_processor = resolve_obj_by_qualname(qualname) except Exception as e: raise ValueError( f\"Logits processor '{qualname}' could not be resolved: {e}\" ) from e if isinstance(processor, LogitsProcessorConstructor): logits_processor = logits_processor( *processor.args or [], **processor.kwargs or {} ) logits_processors.append(logits_processor) return logits_processors elif processors: raise ValueError( \"The `logits_processors` argument is not supported by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) return None serialize_message ¶ serialize_message(msg) Serializes a single message Source code in vllm/entrypoints/openai/protocol.py 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665def serialize_message(msg): \"\"\" Serializes a single message \"\"\" if isinstance(msg, dict): return msg elif hasattr(msg, \"to_dict\"): return msg.to_dict() else: # fallback to pyandic dump return msg.model_dump_json() serialize_messages ¶ serialize_messages(msgs) Serializes multiple messages Source code in vllm/entrypoints/openai/protocol.py 1668 1669 1670 1671 1672def serialize_messages(msgs): \"\"\" Serializes multiple messages \"\"\" return [serialize_message(msg) for msg in msgs] if msgs else None",
      "code": ""
    },
    {
      "description": "AnyResponseFormat module-attribute ¶ AnyResponseFormat: TypeAlias = ( ResponseFormat | StructuralTagResponseFormat | LegacyStructuralTagResponseFormat ) AnyStructuralTagResponseFormat module-attribute ¶ AnyStructuralTagResponseFormat: TypeAlias = ( LegacyStructuralTagResponseFormat | StructuralTagResponseFormat ) AudioResponseFormat module-attribute ¶ AudioResponseFormat: TypeAlias = Literal[ \"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\" ] LogitsProcessors module-attribute ¶ LogitsProcessors = list[str | LogitsProcessorConstructor] ResponseInputOutputItem module-attribute ¶ ResponseInputOutputItem: TypeAlias = ( ResponseInputItemParam | ResponseOutputItem ) ResponseInputOutputMessage module-attribute ¶ ResponseInputOutputMessage: TypeAlias = ( list[ChatCompletionMessageParam] | list[ResponseRawMessageAndToken] ) StreamingResponsesResponse module-attribute ¶ StreamingResponsesResponse: TypeAlias = ( ResponseCreatedEvent | ResponseInProgressEvent | ResponseCompletedEvent | ResponseOutputItemAddedEvent | ResponseOutputItemDoneEvent | ResponseContentPartAddedEvent | ResponseContentPartDoneEvent | ResponseReasoningTextDeltaEvent | ResponseReasoningTextDoneEvent | ResponseReasoningPartAddedEvent | ResponseReasoningPartDoneEvent | ResponseCodeInterpreterCallInProgressEvent | ResponseCodeInterpreterCallCodeDeltaEvent | ResponseWebSearchCallInProgressEvent | ResponseWebSearchCallSearchingEvent | ResponseWebSearchCallCompletedEvent | ResponseCodeInterpreterCallCodeDoneEvent | ResponseCodeInterpreterCallInterpretingEvent | ResponseCodeInterpreterCallCompletedEvent | ResponseMcpCallArgumentsDeltaEvent | ResponseMcpCallArgumentsDoneEvent | ResponseMcpCallInProgressEvent | ResponseMcpCallCompletedEvent ) TokenizeRequest module-attribute ¶ TokenizeRequest: TypeAlias = ( TokenizeCompletionRequest | TokenizeChatRequest ) TranscriptionResponseVariant module-attribute ¶ TranscriptionResponseVariant: TypeAlias = ( TranscriptionResponse | TranscriptionResponseVerbose ) TranslationResponseVariant module-attribute ¶ TranslationResponseVariant: TypeAlias = ( TranslationResponse | TranslationResponseVerbose ) _LONG_INFO module-attribute ¶ _LONG_INFO = iinfo(long) logger module-attribute ¶ logger = init_logger(__name__) ChatCompletionLogProb ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1535 1536 1537 1538class ChatCompletionLogProb(OpenAIBaseModel): token: str logprob: float = -9999.0 bytes: list[int] | None = None bytes class-attribute instance-attribute ¶ bytes: list[int] | None = None logprob class-attribute instance-attribute ¶ logprob: float = -9999.0 token instance-attribute ¶ token: str ChatCompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1548 1549class ChatCompletionLogProbs(OpenAIBaseModel): content: list[ChatCompletionLogProbsContent] | None = None content class-attribute instance-attribute ¶ content: list[ChatCompletionLogProbsContent] | None = None ChatCompletionLogProbsContent ¶ Bases: ChatCompletionLogProb Source code in vllm/entrypoints/openai/protocol.py 1541 1542 1543 1544 1545class ChatCompletionLogProbsContent(ChatCompletionLogProb): # Workaround: redefine fields name cache so that it's not # shared with the super class. field_names: ClassVar[set[str] | None] = None top_logprobs: list[ChatCompletionLogProb] = Field(default_factory=list) field_names class-attribute ¶ field_names: set[str] | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[ChatCompletionLogProb] = Field( default_factory=list ) ChatCompletionNamedFunction ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 271 272class ChatCompletionNamedFunction(OpenAIBaseModel): name: str name instance-attribute ¶ name: str ChatCompletionNamedToolChoiceParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 275 276 277class ChatCompletionNamedToolChoiceParam(OpenAIBaseModel): function: ChatCompletionNamedFunction type: Literal[\"function\"] = \"function\" function instance-attribute ¶ function: ChatCompletionNamedFunction type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054class ChatCompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/chat/create messages: list[ChatCompletionMessageParam] model: str | None = None frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: bool | None = False top_logprobs: int | None = 0 max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of \" \"the max_completion_tokens field\", ) max_completion_tokens: int | None = None n: int | None = 1 presence_penalty: float | None = 0.0 response_format: AnyResponseFormat | None = None seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None temperature: float | None = None top_p: float | None = None tools: list[ChatCompletionToolsParam] | None = None tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" reasoning_effort: Literal[\"low\", \"medium\", \"high\"] | None = None include_reasoning: bool = True parallel_tool_calls: bool | None = True # NOTE this will be ignored by vLLM user: str | None = None # --8<-- [start:chat-completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None prompt_logprobs: int | None = None allowed_token_ids: list[int] | None = None bad_words: list[str] = Field(default_factory=list) # --8<-- [end:chat-completion-sampling-params] # --8<-- [start:chat-completion-extra-params] echo: bool = Field( default=False, description=( \"If true, the new message will be prepended with the last message \" \"if they belong to the same role.\" ), ) add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) documents: list[dict[str, str]] | None = Field( default=None, description=( \"A list of dicts representing documents that will be accessible to \" \"the model if it is performing RAG (retrieval-augmented generation).\" \" If the template does not support RAG, this argument will have no \" \"effect. We recommend that each document should be a dict containing \" '\"title\" and \"text\" keys.' ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float | list[str | int | float]] | None = Field( default=None, description=( \"Additional request parameters with (list of) string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:chat-completion-extra-params] # Default sampling parameters for chat completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data @model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None bad_words class-attribute instance-attribute ¶ bad_words: list[str] = Field(default_factory=list) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) documents class-attribute instance-attribute ¶ documents: list[dict[str, str]] | None = Field( default=None, description='A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.', ) echo class-attribute instance-attribute ¶ echo: bool = Field( default=False, description=\"If true, the new message will be prepended with the last message if they belong to the same role.\", ) frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_reasoning class-attribute instance-attribute ¶ include_reasoning: bool = True include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: bool | None = False max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = Field( default=None, deprecated=\"max_tokens is deprecated in favor of the max_completion_tokens field\", ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int | None = 1 parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None reasoning_effort class-attribute instance-attribute ¶ reasoning_effort: ( Literal[\"low\", \"medium\", \"high\"] | None ) = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = None return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) temperature class-attribute instance-attribute ¶ temperature: float | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ( Literal[\"none\"] | Literal[\"auto\"] | Literal[\"required\"] | ChatCompletionNamedToolChoiceParam | None ) = \"none\" tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: ( dict[str, str | int | float | list[str | int | float]] | None ) = Field( default=None, description=\"Additional request parameters with (list of) string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1035 1036 1037 1038 1039 1040 1041 1042 1043@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (top_logprobs := data.get(\"top_logprobs\")) is not None: if top_logprobs < 0 and top_logprobs != -1: raise VLLMValidationError( \"`top_logprobs` must be a positive value or -1.\", parameter=\"top_logprobs\", value=top_logprobs, ) if (top_logprobs == -1 or top_logprobs > 0) and not data.get(\"logprobs\"): raise VLLMValidationError( \"when using `top_logprobs`, `logprobs` must be set to true.\", parameter=\"top_logprobs\", ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if isinstance(data, ValueError): raise data if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) # you can only use one kind of constraints for structured outputs if count > 1: raise ValueError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\" ) # you can only either use structured outputs or tools, not both if count > 1 and data.get(\"tool_choice\", \"none\") not in ( \"none\", \"auto\", \"required\", ): raise ValueError( \"You can only either use constraints for structured outputs \" \"or tools, not both.\" ) return data check_tool_usage classmethod ¶ check_tool_usage(data) Source code in vllm/entrypoints/openai/protocol.py 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033@model_validator(mode=\"before\") @classmethod def check_tool_usage(cls, data): # if \"tool_choice\" is not specified but tools are provided, # default to \"auto\" tool_choice if \"tool_choice\" not in data and data.get(\"tools\"): data[\"tool_choice\"] = \"auto\" # if \"tool_choice\" is \"none\" -- no validation is needed for tools if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\": return data # if \"tool_choice\" is specified -- validation if \"tool_choice\" in data and data[\"tool_choice\"] is not None: # ensure that if \"tool choice\" is specified, tools are present if \"tools\" not in data or data[\"tools\"] is None: raise ValueError(\"When using `tool_choice`, `tools` must be set.\") # make sure that tool choice is either a named tool # OR that it's set to \"auto\" or \"required\" if data[\"tool_choice\"] not in [\"auto\", \"required\"] and not isinstance( data[\"tool_choice\"], dict ): raise ValueError( f\"Invalid value for `tool_choice`: {data['tool_choice']}! \" 'Only named tools, \"none\", \"auto\" or \"required\" ' \"are supported.\" ) # if tool_choice is \"required\" but the \"tools\" list is empty, # override the data to behave like \"none\" to align with # OpenAI’s behavior. if ( data[\"tool_choice\"] == \"required\" and isinstance(data[\"tools\"], list) and len(data[\"tools\"]) == 0 ): data[\"tool_choice\"] = \"none\" del data[\"tools\"] return data # ensure that if \"tool_choice\" is specified as an object, # it matches a valid tool correct_usage_message = ( 'Correct usage: `{\"type\": \"function\",' ' \"function\": {\"name\": \"my_function\"}}`' ) if isinstance(data[\"tool_choice\"], dict): valid_tool = False function = data[\"tool_choice\"].get(\"function\") if not isinstance(function, dict): raise ValueError( f\"Invalid value for `function`: `{function}` in \" f\"`tool_choice`! {correct_usage_message}\" ) if \"name\" not in function: raise ValueError( f\"Expected field `name` in `function` in \" f\"`tool_choice`! {correct_usage_message}\" ) function_name = function[\"name\"] if not isinstance(function_name, str) or len(function_name) == 0: raise ValueError( f\"Invalid `name` in `function`: `{function_name}`\" f\" in `tool_choice`! {correct_usage_message}\" ) for tool in data[\"tools\"]: if tool[\"function\"][\"name\"] == function_name: valid_tool = True break if not valid_tool: raise ValueError( \"The tool specified in `tool_choice` does not match any\" \" of the specified `tools`\" ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict ) -> BeamSearchParams: n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict, ) -> SamplingParams: # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.top_logprobs response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.top_logprobs if self.logprobs else None, prompt_logprobs=prompt_logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens, min_tokens=self.min_tokens, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), include_stop_str_in_output=self.include_stop_str_in_output, truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, bad_words=self.bad_words, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 883 884 885 886 887 888 889 890 891 892@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data ChatCompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580class ChatCompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion\"] = \"chat.completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[ChatCompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['chat.completion'] = 'chat.completion' prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo ChatCompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562class ChatCompletionResponseChoice(OpenAIBaseModel): index: int message: ChatMessage logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" # not part of the OpenAI spec but included in vLLM for legacy reasons stop_reason: int | str | None = None # not part of the OpenAI spec but is useful for tracing the tokens # in agent scenarios token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None message instance-attribute ¶ message: ChatMessage stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1598 1599 1600 1601 1602 1603 1604 1605class ChatCompletionResponseStreamChoice(OpenAIBaseModel): index: int delta: DeltaMessage logprobs: ChatCompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = None # not part of the OpenAI spec but for tracing the tokens token_ids: list[int] | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None ChatCompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1608 1609 1610 1611 1612 1613 1614 1615 1616class ChatCompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\") object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[ChatCompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) # not part of the OpenAI spec but for tracing the tokens prompt_token_ids: list[int] | None = None choices instance-attribute ¶ choices: list[ChatCompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"chatcmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"chat.completion.chunk\"] = ( \"chat.completion.chunk\" ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) ChatCompletionToolsParam ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 266 267 268class ChatCompletionToolsParam(OpenAIBaseModel): type: Literal[\"function\"] = \"function\" function: FunctionDefinition function instance-attribute ¶ function: FunctionDefinition type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' ChatMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532class ChatMessage(OpenAIBaseModel): role: str content: str | None = None refusal: str | None = None annotations: OpenAIAnnotation | None = None audio: OpenAIChatCompletionAudio | None = None function_call: FunctionCall | None = None tool_calls: list[ToolCall] = Field(default_factory=list) # vLLM-specific fields that are not in OpenAI spec reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self annotations class-attribute instance-attribute ¶ annotations: Annotation | None = None audio class-attribute instance-attribute ¶ audio: ChatCompletionAudio | None = None content class-attribute instance-attribute ¶ content: str | None = None function_call class-attribute instance-attribute ¶ function_call: FunctionCall | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. refusal class-attribute instance-attribute ¶ refusal: str | None = None role instance-attribute ¶ role: str tool_calls class-attribute instance-attribute ¶ tool_calls: list[ToolCall] = Field(default_factory=list) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1528 1529 1530 1531 1532@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self CompletionLogProbs ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1409 1410 1411 1412 1413class CompletionLogProbs(OpenAIBaseModel): text_offset: list[int] = Field(default_factory=list) token_logprobs: list[float | None] = Field(default_factory=list) tokens: list[str] = Field(default_factory=list) top_logprobs: list[dict[str, float] | None] = Field(default_factory=list) text_offset class-attribute instance-attribute ¶ text_offset: list[int] = Field(default_factory=list) token_logprobs class-attribute instance-attribute ¶ token_logprobs: list[float | None] = Field( default_factory=list ) tokens class-attribute instance-attribute ¶ tokens: list[str] = Field(default_factory=list) top_logprobs class-attribute instance-attribute ¶ top_logprobs: list[dict[str, float] | None] = Field( default_factory=list ) CompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406class CompletionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/completions/create model: str | None = None prompt: list[int] | list[list[int]] | str | list[str] | None = None echo: bool | None = False frequency_penalty: float | None = 0.0 logit_bias: dict[str, float] | None = None logprobs: int | None = None max_tokens: int | None = 16 n: int = 1 presence_penalty: float | None = 0.0 seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) stop: str | list[str] | None = [] stream: bool | None = False stream_options: StreamOptions | None = None suffix: str | None = None temperature: float | None = None top_p: float | None = None user: str | None = None # --8<-- [start:completion-sampling-params] use_beam_search: bool = False top_k: int | None = None min_p: float | None = None repetition_penalty: float | None = None length_penalty: float = 1.0 stop_token_ids: list[int] | None = [] include_stop_str_in_output: bool = False ignore_eos: bool = False min_tokens: int = 0 skip_special_tokens: bool = True spaces_between_special_tokens: bool = True truncate_prompt_tokens: Annotated[int, Field(ge=-1)] | None = None allowed_token_ids: list[int] | None = None prompt_logprobs: int | None = None # --8<-- [end:completion-sampling-params] # --8<-- [start:completion-extra-params] prompt_embeds: bytes | list[bytes] | None = None add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) response_format: AnyResponseFormat | None = Field( default=None, description=( \"Similar to chat completion, this parameter specifies the format \" \"of output. Only {'type': 'json_object'}, {'type': 'json_schema'}\" \", {'type': 'structural_tag'}, or {'type': 'text' } is supported.\" ), ) structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) logits_processors: LogitsProcessors | None = Field( default=None, description=( \"A list of either qualified names of logits processors, or \" \"constructor objects, to apply when sampling. A constructor is \" \"a JSON object with a required 'qualname' field specifying the \" \"qualified name of the processor class/factory, and optional \" \"'args' and 'kwargs' fields containing positional and keyword \" \"arguments. For example: {'qualname': \" \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \" \"{'param': 'value'}}.\" ), ) return_tokens_as_token_ids: bool | None = Field( default=None, description=( \"If specified with 'logprobs', tokens are represented \" \" as strings of the form 'token_id:{token_id}' so that tokens \" \"that are not JSON-encodable can be identified.\" ), ) return_token_ids: bool | None = Field( default=None, description=( \"If specified, the result will include token IDs alongside the \" \"generated text. In streaming mode, prompt_token_ids is included \" \"only in the first chunk, and token_ids contains the delta tokens \" \"for each chunk. This is useful for debugging or when you \" \"need to map generated text back to input tokens.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:completion-extra-params] # Default sampling parameters for completion requests _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data @model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data @model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data @model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) allowed_token_ids class-attribute instance-attribute ¶ allowed_token_ids: list[int] | None = None cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) echo class-attribute instance-attribute ¶ echo: bool | None = False frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 ignore_eos class-attribute instance-attribute ¶ ignore_eos: bool = False include_stop_str_in_output class-attribute instance-attribute ¶ include_stop_str_in_output: bool = False kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) length_penalty class-attribute instance-attribute ¶ length_penalty: float = 1.0 logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None logits_processors class-attribute instance-attribute ¶ logits_processors: LogitsProcessors | None = Field( default=None, description=\"A list of either qualified names of logits processors, or constructor objects, to apply when sampling. A constructor is a JSON object with a required 'qualname' field specifying the qualified name of the processor class/factory, and optional 'args' and 'kwargs' fields containing positional and keyword arguments. For example: {'qualname': 'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': {'param': 'value'}}.\", ) logprobs class-attribute instance-attribute ¶ logprobs: int | None = None max_tokens class-attribute instance-attribute ¶ max_tokens: int | None = 16 min_p class-attribute instance-attribute ¶ min_p: float | None = None min_tokens class-attribute instance-attribute ¶ min_tokens: int = 0 model class-attribute instance-attribute ¶ model: str | None = None n class-attribute instance-attribute ¶ n: int = 1 presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ( list[int] | list[list[int]] | str | list[str] | None ) = None prompt_embeds class-attribute instance-attribute ¶ prompt_embeds: bytes | list[bytes] | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: int | None = None repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) response_format class-attribute instance-attribute ¶ response_format: AnyResponseFormat | None = Field( default=None, description=\"Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'}, {'type': 'json_schema'}, {'type': 'structural_tag'}, or {'type': 'text' } is supported.\", ) return_token_ids class-attribute instance-attribute ¶ return_token_ids: bool | None = Field( default=None, description=\"If specified, the result will include token IDs alongside the generated text. In streaming mode, prompt_token_ids is included only in the first chunk, and token_ids contains the delta tokens for each chunk. This is useful for debugging or when you need to map generated text back to input tokens.\", ) return_tokens_as_token_ids class-attribute instance-attribute ¶ return_tokens_as_token_ids: bool | None = Field( default=None, description=\"If specified with 'logprobs', tokens are represented as strings of the form 'token_id:{token_id}' so that tokens that are not JSON-encodable can be identified.\", ) seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) skip_special_tokens class-attribute instance-attribute ¶ skip_special_tokens: bool = True spaces_between_special_tokens class-attribute instance-attribute ¶ spaces_between_special_tokens: bool = True stop class-attribute instance-attribute ¶ stop: str | list[str] | None = [] stop_token_ids class-attribute instance-attribute ¶ stop_token_ids: list[int] | None = [] stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None structured_outputs class-attribute instance-attribute ¶ structured_outputs: StructuredOutputsParams | None = Field( default=None, description=\"Additional kwargs for structured outputs\", ) suffix class-attribute instance-attribute ¶ suffix: str | None = None temperature class-attribute instance-attribute ¶ temperature: float | None = None top_k class-attribute instance-attribute ¶ top_k: int | None = None top_p class-attribute instance-attribute ¶ top_p: float | None = None truncate_prompt_tokens class-attribute instance-attribute ¶ truncate_prompt_tokens: ( Annotated[int, Field(ge=-1)] | None ) = None use_beam_search class-attribute instance-attribute ¶ use_beam_search: bool = False user class-attribute instance-attribute ¶ user: str | None = None vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) check_cache_salt_support classmethod ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406@model_validator(mode=\"before\") @classmethod def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data check_logprobs classmethod ¶ check_logprobs(data) Source code in vllm/entrypoints/openai/protocol.py 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366@model_validator(mode=\"before\") @classmethod def check_logprobs(cls, data): if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None: if data.get(\"stream\") and (prompt_logprobs > 0 or prompt_logprobs == -1): raise VLLMValidationError( \"`prompt_logprobs` are not available when `stream=True`.\", parameter=\"prompt_logprobs\", ) if prompt_logprobs < 0 and prompt_logprobs != -1: raise VLLMValidationError( \"`prompt_logprobs` must be a positive value or -1.\", parameter=\"prompt_logprobs\", value=prompt_logprobs, ) if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0: raise VLLMValidationError( \"`logprobs` must be a positive value.\", parameter=\"logprobs\", value=logprobs, ) return data check_structured_outputs_count classmethod ¶ check_structured_outputs_count(data) Source code in vllm/entrypoints/openai/protocol.py 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341@model_validator(mode=\"before\") @classmethod def check_structured_outputs_count(cls, data): if data.get(\"structured_outputs\", None) is None: return data structured_outputs_kwargs = data[\"structured_outputs\"] count = sum( structured_outputs_kwargs.get(k) is not None for k in (\"json\", \"regex\", \"choice\") ) if count > 1: raise VLLMValidationError( \"You can only use one kind of constraints for structured \" \"outputs ('json', 'regex' or 'choice').\", parameter=\"structured_outputs\", ) return data to_beam_search_params ¶ to_beam_search_params( max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams Source code in vllm/entrypoints/openai/protocol.py 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220def to_beam_search_params( self, max_tokens: int, default_sampling_params: dict | None = None, ) -> BeamSearchParams: if default_sampling_params is None: default_sampling_params = {} n = self.n if self.n is not None else 1 if (temperature := self.temperature) is None: temperature = default_sampling_params.get(\"temperature\", 1.0) return BeamSearchParams( beam_width=n, max_tokens=max_tokens, ignore_eos=self.ignore_eos, temperature=temperature, length_penalty=self.length_penalty, include_stop_str_in_output=self.include_stop_str_in_output, ) to_sampling_params ¶ to_sampling_params( max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322def to_sampling_params( self, max_tokens: int, logits_processor_pattern: str | None, default_sampling_params: dict | None = None, ) -> SamplingParams: if default_sampling_params is None: default_sampling_params = {} # Default parameters if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) prompt_logprobs = self.prompt_logprobs if prompt_logprobs is None and self.echo: prompt_logprobs = self.logprobs echo_without_generation = self.echo and self.max_tokens == 0 response_format = self.response_format if response_format is not None: # If structured outputs wasn't already enabled, # we must enable it for these features to work if self.structured_outputs is None: self.structured_outputs = StructuredOutputsParams() # Set structured output params for response format if response_format.type == \"json_object\": self.structured_outputs.json_object = True elif response_format.type == \"json_schema\": json_schema = response_format.json_schema assert json_schema is not None self.structured_outputs.json = json_schema.json_schema elif response_format.type == \"structural_tag\": structural_tag = response_format assert structural_tag is not None and isinstance( structural_tag, ( LegacyStructuralTagResponseFormat, StructuralTagResponseFormat, ), ) s_tag_obj = structural_tag.model_dump(by_alias=True) self.structured_outputs.structural_tag = json.dumps(s_tag_obj) extra_args: dict[str, Any] = self.vllm_xargs if self.vllm_xargs else {} if self.kv_transfer_params: # Pass in kv_transfer_params via extra_args extra_args[\"kv_transfer_params\"] = self.kv_transfer_params return SamplingParams.from_optional( n=self.n, presence_penalty=self.presence_penalty, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, seed=self.seed, stop=self.stop, stop_token_ids=self.stop_token_ids, logprobs=self.logprobs, ignore_eos=self.ignore_eos, max_tokens=max_tokens if not echo_without_generation else 1, min_tokens=self.min_tokens, prompt_logprobs=prompt_logprobs, skip_special_tokens=self.skip_special_tokens, spaces_between_special_tokens=self.spaces_between_special_tokens, include_stop_str_in_output=self.include_stop_str_in_output, logits_processors=get_logits_processors( self.logits_processors, logits_processor_pattern ), truncate_prompt_tokens=self.truncate_prompt_tokens, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, structured_outputs=self.structured_outputs, logit_bias=self.logit_bias, allowed_token_ids=self.allowed_token_ids, extra_args=extra_args or None, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_prompt_and_prompt_embeds classmethod ¶ validate_prompt_and_prompt_embeds(data) Source code in vllm/entrypoints/openai/protocol.py 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395@model_validator(mode=\"before\") @classmethod def validate_prompt_and_prompt_embeds(cls, data): prompt = data.get(\"prompt\") prompt_embeds = data.get(\"prompt_embeds\") prompt_is_empty = prompt is None or (isinstance(prompt, str) and prompt == \"\") embeds_is_empty = prompt_embeds is None or ( isinstance(prompt_embeds, list) and len(prompt_embeds) == 0 ) if prompt_is_empty and embeds_is_empty: raise ValueError( \"Either prompt or prompt_embeds must be provided and non-empty.\" ) return data validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): if data.get(\"stream_options\") and not data.get(\"stream\"): raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=\"stream_options\", ) return data CompletionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447class CompletionResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: Literal[\"text_completion\"] = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseChoice] service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None = None system_fingerprint: str | None = None usage: UsageInfo # vLLM-specific fields that are not in OpenAI spec kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) choices instance-attribute ¶ choices: list[CompletionResponseChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters.\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['text_completion'] = 'text_completion' service_tier class-attribute instance-attribute ¶ service_tier: ( Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] | None ) = None system_fingerprint class-attribute instance-attribute ¶ system_fingerprint: str | None = None usage instance-attribute ¶ usage: UsageInfo CompletionResponseChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431class CompletionResponseChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) token_ids: list[int] | None = None # For response prompt_logprobs: list[dict[int, Logprob] | None] | None = None prompt_token_ids: list[int] | None = None # For prompt finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466class CompletionResponseStreamChoice(OpenAIBaseModel): index: int text: str logprobs: CompletionLogProbs | None = None finish_reason: str | None = None stop_reason: int | str | None = Field( default=None, description=( \"The stop string or token id that caused the completion \" \"to stop, None if the completion finished for some other reason \" \"including encountering the EOS token\" ), ) # not part of the OpenAI spec but for tracing the tokens # prompt tokens is put into choice to align with CompletionResponseChoice prompt_token_ids: list[int] | None = None token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: CompletionLogProbs | None = None prompt_token_ids class-attribute instance-attribute ¶ prompt_token_ids: list[int] | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = Field( default=None, description=\"The stop string or token id that caused the completion to stop, None if the completion finished for some other reason including encountering the EOS token\", ) text instance-attribute ¶ text: str token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None CompletionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1469 1470 1471 1472 1473 1474 1475class CompletionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\") object: str = \"text_completion\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[CompletionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[CompletionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"cmpl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: str = 'text_completion' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) DeltaFunctionCall ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1489 1490 1491class DeltaFunctionCall(BaseModel): name: str | None = None arguments: str | None = None arguments class-attribute instance-attribute ¶ arguments: str | None = None name class-attribute instance-attribute ¶ name: str | None = None DeltaMessage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595class DeltaMessage(OpenAIBaseModel): role: str | None = None content: str | None = None reasoning: str | None = None reasoning_content: str | None = None \"\"\"Deprecated: use `reasoning` instead.\"\"\" tool_calls: list[DeltaToolCall] = Field(default_factory=list) @model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self content class-attribute instance-attribute ¶ content: str | None = None reasoning class-attribute instance-attribute ¶ reasoning: str | None = None reasoning_content class-attribute instance-attribute ¶ reasoning_content: str | None = None Deprecated: use reasoning instead. role class-attribute instance-attribute ¶ role: str | None = None tool_calls class-attribute instance-attribute ¶ tool_calls: list[DeltaToolCall] = Field( default_factory=list ) handle_deprecated_reasoning_content ¶ handle_deprecated_reasoning_content() Copy reasoning to reasoning_content for backward compatibility. Source code in vllm/entrypoints/openai/protocol.py 1591 1592 1593 1594 1595@model_validator(mode=\"after\") def handle_deprecated_reasoning_content(self): \"\"\"Copy reasoning to reasoning_content for backward compatibility.\"\"\" self.reasoning_content = self.reasoning return self DeltaToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1495 1496 1497 1498 1499class DeltaToolCall(OpenAIBaseModel): id: str | None = None type: Literal[\"function\"] | None = None index: int function: DeltaFunctionCall | None = None function class-attribute instance-attribute ¶ function: DeltaFunctionCall | None = None id class-attribute instance-attribute ¶ id: str | None = None index instance-attribute ¶ index: int type class-attribute instance-attribute ¶ type: Literal['function'] | None = None DetokenizeRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1987 1988 1989class DetokenizeRequest(OpenAIBaseModel): model: str | None = None tokens: list[int] model class-attribute instance-attribute ¶ model: str | None = None tokens instance-attribute ¶ tokens: list[int] DetokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1992 1993class DetokenizeResponse(OpenAIBaseModel): prompt: str prompt instance-attribute ¶ prompt: str ErrorInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 123 124 125 126 127class ErrorInfo(OpenAIBaseModel): message: str type: str param: str | None = None code: int code instance-attribute ¶ code: int message instance-attribute ¶ message: str param class-attribute instance-attribute ¶ param: str | None = None type instance-attribute ¶ type: str ErrorResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 130 131class ErrorResponse(OpenAIBaseModel): error: ErrorInfo error instance-attribute ¶ error: ErrorInfo ExtractedToolCallInformation ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511class ExtractedToolCallInformation(BaseModel): # indicate if tools were called tools_called: bool # extracted tool calls tool_calls: list[ToolCall] # content - per OpenAI spec, content AND tool calls can be returned rarely # But some models will do this intentionally content: str | None = None content class-attribute instance-attribute ¶ content: str | None = None tool_calls instance-attribute ¶ tool_calls: list[ToolCall] tools_called instance-attribute ¶ tools_called: bool FunctionCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1478 1479 1480class FunctionCall(OpenAIBaseModel): name: str arguments: str arguments instance-attribute ¶ arguments: str name instance-attribute ¶ name: str FunctionDefinition ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 260 261 262 263class FunctionDefinition(OpenAIBaseModel): name: str description: str | None = None parameters: dict[str, Any] | None = None description class-attribute instance-attribute ¶ description: str | None = None name instance-attribute ¶ name: str parameters class-attribute instance-attribute ¶ parameters: dict[str, Any] | None = None GenerateRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564class GenerateRequest(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) token_ids: list[int] \"\"\"The token ids to generate text from.\"\"\" # features: MultiModalFeatureSpec # TODO (NickLucche): implement once Renderer work is completed features: str | None = None \"\"\"The processed MM inputs for the model.\"\"\" sampling_params: SamplingParams \"\"\"The sampling parameters for the model.\"\"\" model: str | None = None stream: bool | None = False stream_options: StreamOptions | None = None cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) features class-attribute instance-attribute ¶ features: str | None = None The processed MM inputs for the model. kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) model class-attribute instance-attribute ¶ model: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) sampling_params instance-attribute ¶ sampling_params: SamplingParams The sampling parameters for the model. stream class-attribute instance-attribute ¶ stream: bool | None = False stream_options class-attribute instance-attribute ¶ stream_options: StreamOptions | None = None token_ids instance-attribute ¶ token_ids: list[int] The token ids to generate text from. GenerateResponse ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591class GenerateResponse(BaseModel): request_id: str = Field( default_factory=random_uuid, description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) choices: list[GenerateResponseChoice] prompt_logprobs: list[dict[int, Logprob] | None] | None = None kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) choices instance-attribute ¶ choices: list[GenerateResponseChoice] kv_transfer_params class-attribute instance-attribute ¶ kv_transfer_params: dict[str, Any] | None = Field( default=None, description=\"KVTransfer parameters used for disaggregated serving.\", ) prompt_logprobs class-attribute instance-attribute ¶ prompt_logprobs: list[dict[int, Logprob] | None] | None = ( None ) request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=random_uuid, description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) GenerateResponseChoice ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2567 2568 2569 2570 2571 2572class GenerateResponseChoice(BaseModel): index: int logprobs: ChatCompletionLogProbs | None = None # per OpenAI spec this is the default finish_reason: str | None = \"stop\" token_ids: list[int] | None = None finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = 'stop' index instance-attribute ¶ index: int logprobs class-attribute instance-attribute ¶ logprobs: ChatCompletionLogProbs | None = None token_ids class-attribute instance-attribute ¶ token_ids: list[int] | None = None InputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1634 1635 1636 1637class InputTokensDetails(OpenAIBaseModel): cached_tokens: int input_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens_per_turn: list[int] = Field(default_factory=list) cached_tokens instance-attribute ¶ cached_tokens: int cached_tokens_per_turn class-attribute instance-attribute ¶ cached_tokens_per_turn: list[int] = Field( default_factory=list ) input_tokens_per_turn class-attribute instance-attribute ¶ input_tokens_per_turn: list[int] = Field( default_factory=list ) JsonSchemaResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 211 212 213 214 215 216 217class JsonSchemaResponseFormat(OpenAIBaseModel): name: str description: str | None = None # schema is the field in openai but that causes conflicts with pydantic so # instead use json_schema with an alias json_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") strict: bool | None = None description class-attribute instance-attribute ¶ description: str | None = None json_schema class-attribute instance-attribute ¶ json_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) name instance-attribute ¶ name: str strict class-attribute instance-attribute ¶ strict: bool | None = None LegacyStructuralTag ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 220 221 222 223 224 225class LegacyStructuralTag(OpenAIBaseModel): begin: str # schema is the field, but that causes conflicts with pydantic so # instead use structural_tag_schema with an alias structural_tag_schema: dict[str, Any] | None = Field(default=None, alias=\"schema\") end: str begin instance-attribute ¶ begin: str end instance-attribute ¶ end: str structural_tag_schema class-attribute instance-attribute ¶ structural_tag_schema: dict[str, Any] | None = Field( default=None, alias=\"schema\" ) LegacyStructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 228 229 230 231class LegacyStructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] structures: list[LegacyStructuralTag] triggers: list[str] structures instance-attribute ¶ structures: list[LegacyStructuralTag] triggers instance-attribute ¶ triggers: list[str] type instance-attribute ¶ type: Literal['structural_tag'] LoadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2006 2007 2008class LoadLoRAAdapterRequest(BaseModel): lora_name: str lora_path: str lora_name instance-attribute ¶ lora_name: str lora_path instance-attribute ¶ lora_path: str LogitsProcessorConstructor ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 282 283 284 285 286 287class LogitsProcessorConstructor(BaseModel): qualname: str args: list[Any] | None = None kwargs: dict[str, Any] | None = None model_config = ConfigDict(extra=\"forbid\") args class-attribute instance-attribute ¶ args: list[Any] | None = None kwargs class-attribute instance-attribute ¶ kwargs: dict[str, Any] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='forbid') qualname instance-attribute ¶ qualname: str ModelCard ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 179 180 181 182 183 184 185 186 187class ModelCard(OpenAIBaseModel): id: str object: str = \"model\" created: int = Field(default_factory=lambda: int(time.time())) owned_by: str = \"vllm\" root: str | None = None parent: str | None = None max_model_len: int | None = None permission: list[ModelPermission] = Field(default_factory=list) created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id instance-attribute ¶ id: str max_model_len class-attribute instance-attribute ¶ max_model_len: int | None = None object class-attribute instance-attribute ¶ object: str = 'model' owned_by class-attribute instance-attribute ¶ owned_by: str = 'vllm' parent class-attribute instance-attribute ¶ parent: str | None = None permission class-attribute instance-attribute ¶ permission: list[ModelPermission] = Field( default_factory=list ) root class-attribute instance-attribute ¶ root: str | None = None ModelList ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 190 191 192class ModelList(OpenAIBaseModel): object: str = \"list\" data: list[ModelCard] = Field(default_factory=list) data class-attribute instance-attribute ¶ data: list[ModelCard] = Field(default_factory=list) object class-attribute instance-attribute ¶ object: str = 'list' ModelPermission ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 164 165 166 167 168 169 170 171 172 173 174 175 176class ModelPermission(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\") object: str = \"model_permission\" created: int = Field(default_factory=lambda: int(time.time())) allow_create_engine: bool = False allow_sampling: bool = True allow_logprobs: bool = True allow_search_indices: bool = False allow_view: bool = True allow_fine_tuning: bool = False organization: str = \"*\" group: str | None = None is_blocking: bool = False allow_create_engine class-attribute instance-attribute ¶ allow_create_engine: bool = False allow_fine_tuning class-attribute instance-attribute ¶ allow_fine_tuning: bool = False allow_logprobs class-attribute instance-attribute ¶ allow_logprobs: bool = True allow_sampling class-attribute instance-attribute ¶ allow_sampling: bool = True allow_search_indices class-attribute instance-attribute ¶ allow_search_indices: bool = False allow_view class-attribute instance-attribute ¶ allow_view: bool = True created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) group class-attribute instance-attribute ¶ group: str | None = None id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"modelperm-{random_uuid()}\" ) is_blocking class-attribute instance-attribute ¶ is_blocking: bool = False object class-attribute instance-attribute ¶ object: str = 'model_permission' organization class-attribute instance-attribute ¶ organization: str = '*' OpenAIBaseModel ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120class OpenAIBaseModel(BaseModel): # OpenAI API does allow extra fields model_config = ConfigDict(extra=\"allow\") # Cache class field names field_names: ClassVar[set[str] | None] = None @model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result field_names class-attribute ¶ field_names: set[str] | None = None model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') __log_extra_fields__ classmethod ¶ __log_extra_fields__(data, handler) Source code in vllm/entrypoints/openai/protocol.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120@model_validator(mode=\"wrap\") @classmethod def __log_extra_fields__(cls, data, handler): result = handler(data) if not isinstance(data, dict): return result field_names = cls.field_names if field_names is None: # Get all class field names and their potential aliases field_names = set() for field_name, field in cls.model_fields.items(): field_names.add(field_name) if alias := getattr(field, \"alias\", None): field_names.add(alias) cls.field_names = field_names # Compare against both field names and aliases if any(k not in field_names for k in data): logger.warning( \"The following fields were present in the request but ignored: %s\", data.keys() - field_names, ) return result OutputTokensDetails ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1640 1641 1642 1643 1644class OutputTokensDetails(OpenAIBaseModel): reasoning_tokens: int = 0 tool_output_tokens: int = 0 output_tokens_per_turn: list[int] = Field(default_factory=list) tool_output_tokens_per_turn: list[int] = Field(default_factory=list) output_tokens_per_turn class-attribute instance-attribute ¶ output_tokens_per_turn: list[int] = Field( default_factory=list ) reasoning_tokens class-attribute instance-attribute ¶ reasoning_tokens: int = 0 tool_output_tokens class-attribute instance-attribute ¶ tool_output_tokens: int = 0 tool_output_tokens_per_turn class-attribute instance-attribute ¶ tool_output_tokens_per_turn: list[int] = Field( default_factory=list ) PromptTokenUsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 195 196class PromptTokenUsageInfo(OpenAIBaseModel): cached_tokens: int | None = None cached_tokens class-attribute instance-attribute ¶ cached_tokens: int | None = None RequestResponseMetadata ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 206 207 208class RequestResponseMetadata(BaseModel): request_id: str final_usage_info: UsageInfo | None = None final_usage_info class-attribute instance-attribute ¶ final_usage_info: UsageInfo | None = None request_id instance-attribute ¶ request_id: str ResponseCompletedEvent ¶ Bases: ResponseCompletedEvent Source code in vllm/entrypoints/openai/protocol.py 1845 1846class ResponseCompletedEvent(OpenAIResponseCompletedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseCreatedEvent ¶ Bases: ResponseCreatedEvent Source code in vllm/entrypoints/openai/protocol.py 1849 1850class ResponseCreatedEvent(OpenAIResponseCreatedEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 244 245 246 247class ResponseFormat(OpenAIBaseModel): # type must be \"json_schema\", \"json_object\", or \"text\" type: Literal[\"text\", \"json_object\", \"json_schema\"] json_schema: JsonSchemaResponseFormat | None = None json_schema class-attribute instance-attribute ¶ json_schema: JsonSchemaResponseFormat | None = None type instance-attribute ¶ type: Literal['text', 'json_object', 'json_schema'] ResponseInProgressEvent ¶ Bases: ResponseInProgressEvent Source code in vllm/entrypoints/openai/protocol.py 1853 1854class ResponseInProgressEvent(OpenAIResponseInProgressEvent): response: ResponsesResponse # type: ignore[override] response instance-attribute ¶ response: ResponsesResponse ResponseRawMessageAndToken ¶ Bases: OpenAIBaseModel Class to show the raw message. If message / tokens diverge, tokens is the source of truth Source code in vllm/entrypoints/openai/protocol.py 1675 1676 1677 1678 1679 1680 1681class ResponseRawMessageAndToken(OpenAIBaseModel): \"\"\"Class to show the raw message. If message / tokens diverge, tokens is the source of truth\"\"\" message: str tokens: list[int] type: Literal[\"raw_message_tokens\"] = \"raw_message_tokens\" message instance-attribute ¶ message: str tokens instance-attribute ¶ tokens: list[int] type class-attribute instance-attribute ¶ type: Literal['raw_message_tokens'] = 'raw_message_tokens' ResponseReasoningPartAddedEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840class ResponseReasoningPartAddedEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.added\"] \"\"\"The type of the event. Always `response.reasoning_part.added`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.added'] The type of the event. Always response.reasoning_part.added. ResponseReasoningPartDoneEvent ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818class ResponseReasoningPartDoneEvent(OpenAIBaseModel): content_index: int \"\"\"The index of the content part that is done.\"\"\" item_id: str \"\"\"The ID of the output item that the content part was added to.\"\"\" output_index: int \"\"\"The index of the output item that the content part was added to.\"\"\" part: ResponseReasoningTextContent \"\"\"The content part that is done.\"\"\" sequence_number: int \"\"\"The sequence number of this event.\"\"\" type: Literal[\"response.reasoning_part.done\"] \"\"\"The type of the event. Always `response.reasoning_part.done`.\"\"\" content_index instance-attribute ¶ content_index: int The index of the content part that is done. item_id instance-attribute ¶ item_id: str The ID of the output item that the content part was added to. output_index instance-attribute ¶ output_index: int The index of the output item that the content part was added to. part instance-attribute ¶ part: Content The content part that is done. sequence_number instance-attribute ¶ sequence_number: int The sequence number of this event. type instance-attribute ¶ type: Literal['response.reasoning_part.done'] The type of the event. Always response.reasoning_part.done. ResponseUsage ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1647 1648 1649 1650 1651 1652class ResponseUsage(OpenAIBaseModel): input_tokens: int input_tokens_details: InputTokensDetails output_tokens: int output_tokens_details: OutputTokensDetails total_tokens: int input_tokens instance-attribute ¶ input_tokens: int input_tokens_details instance-attribute ¶ input_tokens_details: InputTokensDetails output_tokens instance-attribute ¶ output_tokens: int output_tokens_details instance-attribute ¶ output_tokens_details: OutputTokensDetails total_tokens instance-attribute ¶ total_tokens: int ResponsesRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555class ResponsesRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/responses/create background: bool | None = False include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input: str | list[ResponseInputOutputItem] instructions: str | None = None max_output_tokens: int | None = None max_tool_calls: int | None = None metadata: Metadata | None = None model: str | None = None logit_bias: dict[str, float] | None = None parallel_tool_calls: bool | None = True previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] = \"auto\" store: bool | None = True stream: bool | None = False temperature: float | None = None text: ResponseTextConfig | None = None tool_choice: ToolChoice = \"auto\" tools: list[Tool] = Field(default_factory=list) top_logprobs: int | None = 0 top_p: float | None = None top_k: int | None = None truncation: Literal[\"auto\", \"disabled\"] | None = \"disabled\" user: str | None = None # --8<-- [start:responses-extra-params] request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=( \"The request_id related to this request. If the caller does \" \"not set it, a random_uuid will be generated. This id is used \" \"through out the inference process and return in response.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) priority: int = Field( default=0, description=( \"The priority of the request (lower means earlier handling; \" \"default: 0). Any priority other than 0 will raise an error \" \"if the served model does not use priority scheduling.\" ), ) cache_salt: str | None = Field( default=None, description=( \"If specified, the prefix cache will be salted with the provided \" \"string to prevent an attacker to guess prompts in multi-user \" \"environments. The salt should be random, protected from \" \"access by 3rd parties, and long enough to be \" \"unpredictable (e.g., 43 characters base64-encoded, corresponding \" \"to 256 bit).\" ), ) enable_response_messages: bool = Field( default=False, description=( \"Dictates whether or not to return messages as part of the \" \"response object. Currently only supported for\" \"non-background and gpt-oss only. \" ), ) # similar to input_messages / output_messages in ResponsesResponse # we take in previous_input_messages (ie in harmony format) # this cannot be used in conjunction with previous_response_id # TODO: consider supporting non harmony messages as well previous_input_messages: list[OpenAIHarmonyMessage | dict] | None = None # --8<-- [end:responses-extra-params] _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) @model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data @model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data @model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data @model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS = { \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, } background class-attribute instance-attribute ¶ background: bool | None = False cache_salt class-attribute instance-attribute ¶ cache_salt: str | None = Field( default=None, description=\"If specified, the prefix cache will be salted with the provided string to prevent an attacker to guess prompts in multi-user environments. The salt should be random, protected from access by 3rd parties, and long enough to be unpredictable (e.g., 43 characters base64-encoded, corresponding to 256 bit).\", ) enable_response_messages class-attribute instance-attribute ¶ enable_response_messages: bool = Field( default=False, description=\"Dictates whether or not to return messages as part of the response object. Currently only supported fornon-background and gpt-oss only. \", ) include class-attribute instance-attribute ¶ include: ( list[ Literal[ \"code_interpreter_call.outputs\", \"computer_call_output.output.image_url\", \"file_search_call.results\", \"message.input_image.image_url\", \"message.output_text.logprobs\", \"reasoning.encrypted_content\", ], ] | None ) = None input instance-attribute ¶ input: str | list[ResponseInputOutputItem] instructions class-attribute instance-attribute ¶ instructions: str | None = None logit_bias class-attribute instance-attribute ¶ logit_bias: dict[str, float] | None = None max_output_tokens class-attribute instance-attribute ¶ max_output_tokens: int | None = None max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None parallel_tool_calls class-attribute instance-attribute ¶ parallel_tool_calls: bool | None = True previous_input_messages class-attribute instance-attribute ¶ previous_input_messages: list[Message | dict] | None = None previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None priority class-attribute instance-attribute ¶ priority: int = Field( default=0, description=\"The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.\", ) prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None request_id class-attribute instance-attribute ¶ request_id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\", description=\"The request_id related to this request. If the caller does not set it, a random_uuid will be generated. This id is used through out the inference process and return in response.\", ) service_tier class-attribute instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] = \"auto\" store class-attribute instance-attribute ¶ store: bool | None = True stream class-attribute instance-attribute ¶ stream: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float | None = None text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice class-attribute instance-attribute ¶ tool_choice: ToolChoice = 'auto' tools class-attribute instance-attribute ¶ tools: list[Tool] = Field(default_factory=list) top_k class-attribute instance-attribute ¶ top_k: int | None = None top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = 0 top_p class-attribute instance-attribute ¶ top_p: float | None = None truncation class-attribute instance-attribute ¶ truncation: Literal['auto', 'disabled'] | None = 'disabled' user class-attribute instance-attribute ¶ user: str | None = None check_cache_salt_support ¶ check_cache_salt_support(data) Source code in vllm/entrypoints/openai/protocol.py 505 506 507 508 509 510 511 512 513@model_validator(mode=\"before\") def check_cache_salt_support(cls, data): if data.get(\"cache_salt\") is not None and ( not isinstance(data[\"cache_salt\"], str) or not data[\"cache_salt\"] ): raise ValueError( \"Parameter 'cache_salt' must be a non-empty string if provided.\" ) return data function_call_parsing ¶ function_call_parsing(data) Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. Source code in vllm/entrypoints/openai/protocol.py 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555@model_validator(mode=\"before\") def function_call_parsing(cls, data): \"\"\"Parse function_call dictionaries into ResponseFunctionToolCall objects. This ensures Pydantic can properly resolve union types in the input field. Function calls provided as dicts are converted to ResponseFunctionToolCall objects before validation, while invalid structures are left for Pydantic to reject with appropriate error messages. \"\"\" input_data = data.get(\"input\") # Early return for None, strings, or bytes # (strings are iterable but shouldn't be processed) if input_data is None or isinstance(input_data, (str, bytes)): return data # Convert iterators (like ValidatorIterator) to list if not isinstance(input_data, list): try: input_data = list(input_data) except TypeError: # Not iterable, leave as-is for Pydantic to handle return data processed_input = [] for item in input_data: if isinstance(item, dict) and item.get(\"type\") == \"function_call\": try: processed_input.append(ResponseFunctionToolCall(**item)) except ValidationError: # Let Pydantic handle validation for malformed function calls logger.debug( \"Failed to parse function_call to ResponseFunctionToolCall, \" \"leaving for Pydantic validation\" ) processed_input.append(item) else: processed_input.append(item) data[\"input\"] = processed_input return data is_include_output_logprobs ¶ is_include_output_logprobs() -> bool Check if the request includes output logprobs. Source code in vllm/entrypoints/openai/protocol.py 480 481 482 483 484 485 486 487def is_include_output_logprobs(self) -> bool: \"\"\"Check if the request includes output logprobs.\"\"\" if self.include is None: return False return ( isinstance(self.include, list) and \"message.output_text.logprobs\" in self.include ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams: if self.max_output_tokens is None: max_tokens = default_max_tokens else: max_tokens = min(self.max_output_tokens, default_max_tokens) default_sampling_params = default_sampling_params or {} if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) stop_token_ids = default_sampling_params.get(\"stop_token_ids\") # Structured output structured_outputs = None if self.text is not None and self.text.format is not None: response_format = self.text.format if ( response_format.type == \"json_schema\" and response_format.schema_ is not None ): structured_outputs = StructuredOutputsParams( json=response_format.schema_ ) elif response_format.type == \"json_object\": raise NotImplementedError(\"json_object is not supported\") # TODO: add more parameters return SamplingParams.from_optional( temperature=temperature, top_p=top_p, top_k=top_k, max_tokens=max_tokens, logprobs=self.top_logprobs if self.is_include_output_logprobs() else None, stop_token_ids=stop_token_ids, output_kind=( RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY ), structured_outputs=structured_outputs, logit_bias=self.logit_bias, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_background ¶ validate_background(data) Source code in vllm/entrypoints/openai/protocol.py 489 490 491 492 493 494 495@model_validator(mode=\"before\") def validate_background(cls, data): if not data.get(\"background\"): return data if not data.get(\"store\", True): raise ValueError(\"background can only be used when `store` is true\") return data validate_prompt ¶ validate_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 497 498 499 500 501 502 503@model_validator(mode=\"before\") def validate_prompt(cls, data): if data.get(\"prompt\") is not None: raise VLLMValidationError( \"prompt template is not supported\", parameter=\"prompt\" ) return data ResponsesResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796class ResponsesResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"resp_{random_uuid()}\") created_at: int = Field(default_factory=lambda: int(time.time())) # error: Optional[ResponseError] = None incomplete_details: IncompleteDetails | None = None instructions: str | None = None metadata: Metadata | None = None model: str object: Literal[\"response\"] = \"response\" output: list[ResponseOutputItem] parallel_tool_calls: bool temperature: float tool_choice: ToolChoice tools: list[Tool] top_p: float background: bool max_output_tokens: int max_tool_calls: int | None = None previous_response_id: str | None = None prompt: ResponsePrompt | None = None reasoning: Reasoning | None = None service_tier: Literal[\"auto\", \"default\", \"flex\", \"scale\", \"priority\"] status: ResponseStatus text: ResponseTextConfig | None = None top_logprobs: int | None = None truncation: Literal[\"auto\", \"disabled\"] usage: ResponseUsage | None = None user: str | None = None # --8<-- [start:responses-response-extra-params] # These are populated when enable_response_messages is set to True # NOTE: custom serialization is needed # see serialize_input_messages and serialize_output_messages input_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token input to model.\" ), ) output_messages: ResponseInputOutputMessage | None = Field( default=None, description=( \"If enable_response_messages, we can show raw token output of model.\" ), ) # --8<-- [end:responses-response-extra-params] # NOTE: openAI harmony doesn't serialize TextContent properly, # TODO: this fixes for TextContent, but need to verify for tools etc # https://github.com/openai/harmony/issues/78 @field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) # NOTE: openAI harmony doesn't serialize TextContent properly, this fixes it # https://github.com/openai/harmony/issues/78 @field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) @classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) background instance-attribute ¶ background: bool created_at class-attribute instance-attribute ¶ created_at: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"resp_{random_uuid()}\" ) incomplete_details class-attribute instance-attribute ¶ incomplete_details: IncompleteDetails | None = None input_messages class-attribute instance-attribute ¶ input_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token input to model.\", ) instructions class-attribute instance-attribute ¶ instructions: str | None = None max_output_tokens instance-attribute ¶ max_output_tokens: int max_tool_calls class-attribute instance-attribute ¶ max_tool_calls: int | None = None metadata class-attribute instance-attribute ¶ metadata: Metadata | None = None model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['response'] = 'response' output instance-attribute ¶ output: list[ResponseOutputItem] output_messages class-attribute instance-attribute ¶ output_messages: ResponseInputOutputMessage | None = Field( default=None, description=\"If enable_response_messages, we can show raw token output of model.\", ) parallel_tool_calls instance-attribute ¶ parallel_tool_calls: bool previous_response_id class-attribute instance-attribute ¶ previous_response_id: str | None = None prompt class-attribute instance-attribute ¶ prompt: ResponsePrompt | None = None reasoning class-attribute instance-attribute ¶ reasoning: Reasoning | None = None service_tier instance-attribute ¶ service_tier: Literal[ \"auto\", \"default\", \"flex\", \"scale\", \"priority\" ] status instance-attribute ¶ status: ResponseStatus temperature instance-attribute ¶ temperature: float text class-attribute instance-attribute ¶ text: ResponseFormatTextConfig | None = None tool_choice instance-attribute ¶ tool_choice: ToolChoice tools instance-attribute ¶ tools: list[Tool] top_logprobs class-attribute instance-attribute ¶ top_logprobs: int | None = None top_p instance-attribute ¶ top_p: float truncation instance-attribute ¶ truncation: Literal['auto', 'disabled'] usage class-attribute instance-attribute ¶ usage: ResponseUsage | None = None user class-attribute instance-attribute ¶ user: str | None = None from_request classmethod ¶ from_request( request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> ResponsesResponse Source code in vllm/entrypoints/openai/protocol.py 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796@classmethod def from_request( cls, request: ResponsesRequest, sampling_params: SamplingParams, model_name: str, created_time: int, output: list[ResponseOutputItem], status: ResponseStatus, usage: ResponseUsage | None = None, input_messages: ResponseInputOutputMessage | None = None, output_messages: ResponseInputOutputMessage | None = None, ) -> \"ResponsesResponse\": incomplete_details: IncompleteDetails | None = None if status == \"incomplete\": incomplete_details = IncompleteDetails(reason=\"max_output_tokens\") # TODO: implement the other reason for incomplete_details, # which is content_filter # incomplete_details = IncompleteDetails(reason='content_filter') return cls( id=request.request_id, created_at=created_time, incomplete_details=incomplete_details, instructions=request.instructions, metadata=request.metadata, model=model_name, output=output, input_messages=input_messages, output_messages=output_messages, parallel_tool_calls=request.parallel_tool_calls, temperature=sampling_params.temperature, tool_choice=request.tool_choice, tools=request.tools, top_p=sampling_params.top_p, background=request.background, max_output_tokens=sampling_params.max_tokens, max_tool_calls=request.max_tool_calls, previous_response_id=request.previous_response_id, prompt=request.prompt, reasoning=request.reasoning, service_tier=request.service_tier, status=status, text=request.text, top_logprobs=sampling_params.logprobs, truncation=request.truncation, user=request.user, usage=usage, ) serialize_input_messages ¶ serialize_input_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1745 1746 1747@field_serializer(\"input_messages\", when_used=\"json\") def serialize_input_messages(self, msgs, _info): return serialize_messages(msgs) serialize_output_messages ¶ serialize_output_messages(msgs, _info) Source code in vllm/entrypoints/openai/protocol.py 1739 1740 1741@field_serializer(\"output_messages\", when_used=\"json\") def serialize_output_messages(self, msgs, _info): return serialize_messages(msgs) StreamOptions ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 255 256 257class StreamOptions(OpenAIBaseModel): include_usage: bool | None = True continuous_usage_stats: bool | None = False continuous_usage_stats class-attribute instance-attribute ¶ continuous_usage_stats: bool | None = False include_usage class-attribute instance-attribute ¶ include_usage: bool | None = True StructuralTagResponseFormat ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 234 235 236class StructuralTagResponseFormat(OpenAIBaseModel): type: Literal[\"structural_tag\"] format: Any format instance-attribute ¶ format: Any type instance-attribute ¶ type: Literal['structural_tag'] TokenizeChatRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974class TokenizeChatRequest(OpenAIBaseModel): model: str | None = None messages: list[ChatCompletionMessageParam] add_generation_prompt: bool = Field( default=True, description=( \"If true, the generation prompt will be added to the chat template. \" \"This is a parameter used by chat template in tokenizer config of the \" \"model.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) continue_final_message: bool = Field( default=False, description=( \"If this is set, the chat will be formatted so that the final \" \"message in the chat is open-ended, without any EOS tokens. The \" \"model will continue this message rather than starting a new one. \" 'This allows you to \"prefill\" part of the model\\'s response for it. ' \"Cannot be used at the same time as `add_generation_prompt`.\" ), ) add_special_tokens: bool = Field( default=False, description=( \"If true, special tokens (e.g. BOS) will be added to the prompt \" \"on top of what is added by the chat template. \" \"For most models, the chat template takes care of adding the \" \"special tokens so this should be set to false (as is the \" \"default).\" ), ) chat_template: str | None = Field( default=None, description=( \"A Jinja template to use for this conversion. \" \"As of transformers v4.44, default chat template is no longer \" \"allowed, so you must provide a chat template if the tokenizer \" \"does not define one.\" ), ) chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=( \"Additional keyword args to pass to the template renderer. \" \"Will be accessible by the chat template.\" ), ) mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=(\"Additional kwargs to pass to the HF processor.\"), ) tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=(\"A list of tools the model may call.\"), ) @model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data add_generation_prompt class-attribute instance-attribute ¶ add_generation_prompt: bool = Field( default=True, description=\"If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.\", ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=False, description=\"If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).\", ) chat_template class-attribute instance-attribute ¶ chat_template: str | None = Field( default=None, description=\"A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.\", ) chat_template_kwargs class-attribute instance-attribute ¶ chat_template_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional keyword args to pass to the template renderer. Will be accessible by the chat template.\", ) continue_final_message class-attribute instance-attribute ¶ continue_final_message: bool = Field( default=False, description='If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model\\'s response for it. Cannot be used at the same time as `add_generation_prompt`.', ) messages instance-attribute ¶ messages: list[ChatCompletionMessageParam] mm_processor_kwargs class-attribute instance-attribute ¶ mm_processor_kwargs: dict[str, Any] | None = Field( default=None, description=\"Additional kwargs to pass to the HF processor.\", ) model class-attribute instance-attribute ¶ model: str | None = None return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) tools class-attribute instance-attribute ¶ tools: list[ChatCompletionToolsParam] | None = Field( default=None, description=\"A list of tools the model may call.\", ) check_generation_prompt classmethod ¶ check_generation_prompt(data) Source code in vllm/entrypoints/openai/protocol.py 1966 1967 1968 1969 1970 1971 1972 1973 1974@model_validator(mode=\"before\") @classmethod def check_generation_prompt(cls, data): if data.get(\"continue_final_message\") and data.get(\"add_generation_prompt\"): raise ValueError( \"Cannot set both `continue_final_message` and \" \"`add_generation_prompt` to True.\" ) return data TokenizeCompletionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900class TokenizeCompletionRequest(OpenAIBaseModel): model: str | None = None prompt: str add_special_tokens: bool = Field( default=True, description=( \"If true (the default), special tokens (e.g. BOS) will be added to \" \"the prompt.\" ), ) return_token_strs: bool | None = Field( default=False, description=( \"If true, also return the token strings corresponding to the token ids.\" ), ) add_special_tokens class-attribute instance-attribute ¶ add_special_tokens: bool = Field( default=True, description=\"If true (the default), special tokens (e.g. BOS) will be added to the prompt.\", ) model class-attribute instance-attribute ¶ model: str | None = None prompt instance-attribute ¶ prompt: str return_token_strs class-attribute instance-attribute ¶ return_token_strs: bool | None = Field( default=False, description=\"If true, also return the token strings corresponding to the token ids.\", ) TokenizeResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1980 1981 1982 1983 1984class TokenizeResponse(OpenAIBaseModel): count: int max_model_len: int tokens: list[int] token_strs: list[str] | None = None count instance-attribute ¶ count: int max_model_len instance-attribute ¶ max_model_len: int token_strs class-attribute instance-attribute ¶ token_strs: list[str] | None = None tokens instance-attribute ¶ tokens: list[int] TokenizerInfoResponse ¶ Bases: OpenAIBaseModel Response containing tokenizer configuration equivalent to tokenizer_config.json Source code in vllm/entrypoints/openai/protocol.py 1996 1997 1998 1999 2000 2001 2002 2003class TokenizerInfoResponse(OpenAIBaseModel): \"\"\" Response containing tokenizer configuration equivalent to tokenizer_config.json \"\"\" model_config = ConfigDict(extra=\"allow\") tokenizer_class: str model_config class-attribute instance-attribute ¶ model_config = ConfigDict(extra='allow') tokenizer_class instance-attribute ¶ tokenizer_class: str ToolCall ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1483 1484 1485 1486class ToolCall(OpenAIBaseModel): id: str = Field(default_factory=make_tool_call_id) type: Literal[\"function\"] = \"function\" function: FunctionCall function instance-attribute ¶ function: FunctionCall id class-attribute instance-attribute ¶ id: str = Field(default_factory=make_tool_call_id) type class-attribute instance-attribute ¶ type: Literal['function'] = 'function' TranscriptionRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213class TranscriptionRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranscription file: UploadFile \"\"\" The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" language: str | None = None \"\"\"The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" ## TODO (varun) : Support if set to 0, certain thresholds are met !! timestamp_granularities: list[Literal[\"word\", \"segment\"]] = Field( alias=\"timestamp_granularities[]\", default=[] ) \"\"\"The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. \"\"\" stream: bool | None = False \"\"\"When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # --8<-- [start:transcription-extra-params] # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=( \"Additional request parameters with string or \" \"numeric values, used by custom extensions.\" ), ) # --8<-- [end:transcription-extra-params] to_language: str | None = None \"\"\"The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. \"\"\" # --8<-- [start:transcription-sampling-params] temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" top_p: float | None = None \"\"\"Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds `p`. \"\"\" top_k: int | None = None \"\"\"Limits sampling to the `k` most probable tokens at each step.\"\"\" min_p: float | None = None \"\"\"Filters out tokens with a probability lower than `min_p`, ensuring a minimum likelihood threshold during sampling. \"\"\" seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" frequency_penalty: float | None = 0.0 \"\"\"The frequency penalty to use for sampling.\"\"\" repetition_penalty: float | None = None \"\"\"The repetition penalty to use for sampling.\"\"\" presence_penalty: float | None = 0.0 \"\"\"The presence penalty to use for sampling.\"\"\" max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:transcription-sampling-params] # Default sampling parameters for transcription requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = { \"repetition_penalty\": 1.0, \"temperature\": 1.0, \"top_p\": 1.0, \"top_k\": 0, \"min_p\": 0.0, } file instance-attribute ¶ file: UploadFile The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. frequency_penalty class-attribute instance-attribute ¶ frequency_penalty: float | None = 0.0 The frequency penalty to use for sampling. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio. Supplying the input language in ISO-639-1 format will improve accuracy and latency. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. min_p class-attribute instance-attribute ¶ min_p: float | None = None Filters out tokens with a probability lower than min_p, ensuring a minimum likelihood threshold during sampling. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. presence_penalty class-attribute instance-attribute ¶ presence_penalty: float | None = 0.0 The presence penalty to use for sampling. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. repetition_penalty class-attribute instance-attribute ¶ repetition_penalty: float | None = None The repetition penalty to use for sampling. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. timestamp_granularities class-attribute instance-attribute ¶ timestamp_granularities: list[ Literal[\"word\", \"segment\"] ] = Field(alias=\"timestamp_granularities[]\", default=[]) The timestamp granularities to populate for this transcription. response_format must be set verbose_json to use timestamp granularities. Either or both of these options are supported: word, or segment. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the output audio we transcribe to. Please note that this is not currently used by supported models at this time, but it is a placeholder for future use, matching translation api. top_k class-attribute instance-attribute ¶ top_k: int | None = None Limits sampling to the k most probable tokens at each step. top_p class-attribute instance-attribute ¶ top_p: float | None = None Enables nucleus (top-p) sampling, where tokens are selected from the smallest possible set whose cumulative probability exceeds p. vllm_xargs class-attribute instance-attribute ¶ vllm_xargs: dict[str, str | int | float] | None = Field( default=None, description=\"Additional request parameters with string or numeric values, used by custom extensions.\", ) to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) if (top_p := self.top_p) is None: top_p = default_sampling_params.get( \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"] ) if (top_k := self.top_k) is None: top_k = default_sampling_params.get( \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"] ) if (min_p := self.min_p) is None: min_p = default_sampling_params.get( \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"] ) if (repetition_penalty := self.repetition_penalty) is None: repetition_penalty = default_sampling_params.get( \"repetition_penalty\", self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"], ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, top_p=top_p, top_k=top_k, min_p=min_p, frequency_penalty=self.frequency_penalty, repetition_penalty=repetition_penalty, presence_penalty=self.presence_penalty, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, extra_args=self.vllm_xargs, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_transcription_request classmethod ¶ validate_transcription_request(data) Source code in vllm/entrypoints/openai/protocol.py 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213@model_validator(mode=\"before\") @classmethod def validate_transcription_request(cls, data): if isinstance(data.get(\"file\"), str): raise HTTPException( status_code=HTTPStatus.UNPROCESSABLE_ENTITY, detail=\"Expected 'file' to be a file-like object, not 'str'.\", ) stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranscriptionResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2222 2223 2224 2225class TranscriptionResponse(OpenAIBaseModel): text: str \"\"\"The transcribed text.\"\"\" usage: TranscriptionUsageAudio text instance-attribute ¶ text: str The transcribed text. usage instance-attribute ¶ usage: TranscriptionUsageAudio TranscriptionResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1619 1620 1621 1622class TranscriptionResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranscriptionResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295class TranscriptionResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The transcribed text.\"\"\" segments: list[TranscriptionSegment] | None = None \"\"\"Segments of the transcribed text and their corresponding details.\"\"\" words: list[TranscriptionWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranscriptionSegment] | None = None Segments of the transcribed text and their corresponding details. text instance-attribute ¶ text: str The transcribed text. words class-attribute instance-attribute ¶ words: list[TranscriptionWord] | None = None Extracted words and their corresponding timestamps. TranscriptionSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278class TranscriptionSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranscriptionStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 1625 1626 1627 1628 1629 1630 1631class TranscriptionStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsc-{random_uuid()}\") object: Literal[\"transcription.chunk\"] = \"transcription.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranscriptionResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranscriptionResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsc-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal[\"transcription.chunk\"] = ( \"transcription.chunk\" ) usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranscriptionUsageAudio ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2217 2218 2219class TranscriptionUsageAudio(OpenAIBaseModel): type: Literal[\"duration\"] = \"duration\" seconds: int seconds instance-attribute ¶ seconds: int type class-attribute instance-attribute ¶ type: Literal['duration'] = 'duration' TranscriptionWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2228 2229 2230 2231 2232 2233 2234 2235 2236class TranscriptionWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. TranslationRequest ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435class TranslationRequest(OpenAIBaseModel): # Ordered by official OpenAI API documentation # https://platform.openai.com/docs/api-reference/audio/createTranslation file: UploadFile \"\"\" The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. \"\"\" model: str | None = None \"\"\"ID of the model to use. \"\"\" prompt: str = Field(default=\"\") \"\"\"An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. \"\"\" response_format: AudioResponseFormat = Field(default=\"json\") \"\"\" The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. \"\"\" # TODO support additional sampling parameters # --8<-- [start:translation-sampling-params] seed: int | None = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max) \"\"\"The seed to use for sampling.\"\"\" temperature: float = Field(default=0.0) \"\"\"The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. \"\"\" # --8<-- [end:translation-sampling-params] # --8<-- [start:translation-extra-params] language: str | None = None \"\"\"The language of the input audio we translate from. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy. \"\"\" to_language: str | None = None \"\"\"The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports `to_language=en`. \"\"\" stream: bool | None = False \"\"\"Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. \"\"\" # Flattened stream option to simplify form data. stream_include_usage: bool | None = False stream_continuous_usage_stats: bool | None = False max_completion_tokens: int | None = None \"\"\"The maximum number of tokens to generate.\"\"\" # --8<-- [end:translation-extra-params] # Default sampling parameters for translation requests. _DEFAULT_SAMPLING_PARAMS: dict = { \"temperature\": 0, } def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) @model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data _DEFAULT_SAMPLING_PARAMS class-attribute instance-attribute ¶ _DEFAULT_SAMPLING_PARAMS: dict = {'temperature': 0} file instance-attribute ¶ file: UploadFile The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. language class-attribute instance-attribute ¶ language: str | None = None The language of the input audio we translate from. Supplying the input language in ISO-639-1 format will improve accuracy. max_completion_tokens class-attribute instance-attribute ¶ max_completion_tokens: int | None = None The maximum number of tokens to generate. model class-attribute instance-attribute ¶ model: str | None = None ID of the model to use. prompt class-attribute instance-attribute ¶ prompt: str = Field(default='') An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language. response_format class-attribute instance-attribute ¶ response_format: AudioResponseFormat = Field(default=\"json\") The format of the output, in one of these options: json, text, srt, verbose_json, or vtt. seed class-attribute instance-attribute ¶ seed: int | None = Field(None, ge=min, le=max) The seed to use for sampling. stream class-attribute instance-attribute ¶ stream: bool | None = False Custom field not present in the original OpenAI definition. When set, it will enable output to be streamed in a similar fashion as the Chat Completion endpoint. stream_continuous_usage_stats class-attribute instance-attribute ¶ stream_continuous_usage_stats: bool | None = False stream_include_usage class-attribute instance-attribute ¶ stream_include_usage: bool | None = False temperature class-attribute instance-attribute ¶ temperature: float = Field(default=0.0) The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused / deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit. to_language class-attribute instance-attribute ¶ to_language: str | None = None The language of the input audio we translate to. Please note that this is not supported by all models, refer to the specific model documentation for more details. For instance, Whisper only supports to_language=en. to_sampling_params ¶ to_sampling_params( default_max_tokens: int, default_sampling_params: dict | None = None, ) -> SamplingParams Source code in vllm/entrypoints/openai/protocol.py 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417def to_sampling_params( self, default_max_tokens: int, default_sampling_params: dict | None = None ) -> SamplingParams: max_tokens = default_max_tokens if default_sampling_params is None: default_sampling_params = {} # Default parameters if (temperature := self.temperature) is None: temperature = default_sampling_params.get( \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"] ) return SamplingParams.from_optional( temperature=temperature, max_tokens=max_tokens, seed=self.seed, output_kind=RequestOutputKind.DELTA if self.stream else RequestOutputKind.FINAL_ONLY, skip_clone=True, # Created fresh per request, safe to skip clone ) validate_stream_options classmethod ¶ validate_stream_options(data) Source code in vllm/entrypoints/openai/protocol.py 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435@model_validator(mode=\"before\") @classmethod def validate_stream_options(cls, data): stream_opts = [\"stream_include_usage\", \"stream_continuous_usage_stats\"] stream = data.get(\"stream\", False) if any(bool(data.get(so, False)) for so in stream_opts) and not stream: # Find which specific stream option was set invalid_param = next( (so for so in stream_opts if data.get(so, False)), \"stream_include_usage\", ) raise VLLMValidationError( \"Stream options can only be defined when `stream=True`.\", parameter=invalid_param, ) return data TranslationResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2439 2440 2441class TranslationResponse(OpenAIBaseModel): text: str \"\"\"The translated text.\"\"\" text instance-attribute ¶ text: str The translated text. TranslationResponseStreamChoice ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2303 2304 2305 2306class TranslationResponseStreamChoice(OpenAIBaseModel): delta: DeltaMessage finish_reason: str | None = None stop_reason: int | str | None = None delta instance-attribute ¶ delta: DeltaMessage finish_reason class-attribute instance-attribute ¶ finish_reason: str | None = None stop_reason class-attribute instance-attribute ¶ stop_reason: int | str | None = None TranslationResponseVerbose ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511class TranslationResponseVerbose(OpenAIBaseModel): duration: str \"\"\"The duration of the input audio.\"\"\" language: str \"\"\"The language of the input audio.\"\"\" text: str \"\"\"The translated text.\"\"\" segments: list[TranslationSegment] | None = None \"\"\"Segments of the translated text and their corresponding details.\"\"\" words: list[TranslationWord] | None = None \"\"\"Extracted words and their corresponding timestamps.\"\"\" duration instance-attribute ¶ duration: str The duration of the input audio. language instance-attribute ¶ language: str The language of the input audio. segments class-attribute instance-attribute ¶ segments: list[TranslationSegment] | None = None Segments of the translated text and their corresponding details. text instance-attribute ¶ text: str The translated text. words class-attribute instance-attribute ¶ words: list[TranslationWord] | None = None Extracted words and their corresponding timestamps. TranslationSegment ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494class TranslationSegment(OpenAIBaseModel): id: int \"\"\"Unique identifier of the segment.\"\"\" avg_logprob: float | None = None \"\"\"Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. \"\"\" compression_ratio: float | None = None \"\"\"Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. \"\"\" end: float \"\"\"End time of the segment in seconds.\"\"\" no_speech_prob: float | None = None \"\"\"Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent. \"\"\" seek: int \"\"\"Seek offset of the segment.\"\"\" start: float \"\"\"Start time of the segment in seconds.\"\"\" temperature: float \"\"\"Temperature parameter used for generating the segment.\"\"\" text: str \"\"\"Text content of the segment.\"\"\" tokens: list[int] \"\"\"Array of token IDs for the text content.\"\"\" avg_logprob class-attribute instance-attribute ¶ avg_logprob: float | None = None Average logprob of the segment. If the value is lower than -1, consider the logprobs failed. compression_ratio class-attribute instance-attribute ¶ compression_ratio: float | None = None Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed. end instance-attribute ¶ end: float End time of the segment in seconds. id instance-attribute ¶ id: int Unique identifier of the segment. no_speech_prob class-attribute instance-attribute ¶ no_speech_prob: float | None = None Probability of no speech in the segment. If the value is higher than 1.0 and the avg_logprob is below -1, consider this segment silent. seek instance-attribute ¶ seek: int Seek offset of the segment. start instance-attribute ¶ start: float Start time of the segment in seconds. temperature instance-attribute ¶ temperature: float Temperature parameter used for generating the segment. text instance-attribute ¶ text: str Text content of the segment. tokens instance-attribute ¶ tokens: list[int] Array of token IDs for the text content. TranslationStreamResponse ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2309 2310 2311 2312 2313 2314 2315class TranslationStreamResponse(OpenAIBaseModel): id: str = Field(default_factory=lambda: f\"trsl-{random_uuid()}\") object: Literal[\"translation.chunk\"] = \"translation.chunk\" created: int = Field(default_factory=lambda: int(time.time())) model: str choices: list[TranslationResponseStreamChoice] usage: UsageInfo | None = Field(default=None) choices instance-attribute ¶ choices: list[TranslationResponseStreamChoice] created class-attribute instance-attribute ¶ created: int = Field(default_factory=lambda: int(time())) id class-attribute instance-attribute ¶ id: str = Field( default_factory=lambda: f\"trsl-{random_uuid()}\" ) model instance-attribute ¶ model: str object class-attribute instance-attribute ¶ object: Literal['translation.chunk'] = 'translation.chunk' usage class-attribute instance-attribute ¶ usage: UsageInfo | None = Field(default=None) TranslationWord ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 2444 2445 2446 2447 2448 2449 2450 2451 2452class TranslationWord(OpenAIBaseModel): end: float \"\"\"End time of the word in seconds.\"\"\" start: float \"\"\"Start time of the word in seconds.\"\"\" word: str \"\"\"The text content of the word.\"\"\" end instance-attribute ¶ end: float End time of the word in seconds. start instance-attribute ¶ start: float Start time of the word in seconds. word instance-attribute ¶ word: str The text content of the word. UnloadLoRAAdapterRequest ¶ Bases: BaseModel Source code in vllm/entrypoints/openai/protocol.py 2011 2012 2013class UnloadLoRAAdapterRequest(BaseModel): lora_name: str lora_int_id: int | None = Field(default=None) lora_int_id class-attribute instance-attribute ¶ lora_int_id: int | None = Field(default=None) lora_name instance-attribute ¶ lora_name: str UsageInfo ¶ Bases: OpenAIBaseModel Source code in vllm/entrypoints/openai/protocol.py 199 200 201 202 203class UsageInfo(OpenAIBaseModel): prompt_tokens: int = 0 total_tokens: int = 0 completion_tokens: int | None = 0 prompt_tokens_details: PromptTokenUsageInfo | None = None completion_tokens class-attribute instance-attribute ¶ completion_tokens: int | None = 0 prompt_tokens class-attribute instance-attribute ¶ prompt_tokens: int = 0 prompt_tokens_details class-attribute instance-attribute ¶ prompt_tokens_details: PromptTokenUsageInfo | None = None total_tokens class-attribute instance-attribute ¶ total_tokens: int = 0 VLLMValidationError ¶ Bases: ValueError vLLM-specific validation error for request validation failures. Parameters: Name Type Description Default message str The error message describing the validation failure. required parameter str | None Optional parameter name that failed validation. None value Any Optional value that was rejected during validation. None Source code in vllm/entrypoints/openai/protocol.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161class VLLMValidationError(ValueError): \"\"\"vLLM-specific validation error for request validation failures. Args: message: The error message describing the validation failure. parameter: Optional parameter name that failed validation. value: Optional value that was rejected during validation. \"\"\" def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base parameter instance-attribute ¶ parameter = parameter value instance-attribute ¶ value = value __init__ ¶ __init__( message: str, *, parameter: str | None = None, value: Any = None, ) -> None Source code in vllm/entrypoints/openai/protocol.py 143 144 145 146 147 148 149 150 151 152def __init__( self, message: str, *, parameter: str | None = None, value: Any = None, ) -> None: super().__init__(message) self.parameter = parameter self.value = value __str__ ¶ __str__() Source code in vllm/entrypoints/openai/protocol.py 154 155 156 157 158 159 160 161def __str__(self): base = super().__str__() extras = [] if self.parameter is not None: extras.append(f\"parameter={self.parameter}\") if self.value is not None: extras.append(f\"value={self.value}\") return f\"{base} ({', '.join(extras)})\" if extras else base get_logits_processors ¶ get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None Source code in vllm/entrypoints/openai/protocol.py 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324def get_logits_processors( processors: LogitsProcessors | None, pattern: str | None ) -> list[Any] | None: if processors and pattern: logits_processors = [] for processor in processors: qualname = processor if isinstance(processor, str) else processor.qualname if not re.match(pattern, qualname): raise ValueError( f\"Logits processor '{qualname}' is not allowed by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) try: logits_processor = resolve_obj_by_qualname(qualname) except Exception as e: raise ValueError( f\"Logits processor '{qualname}' could not be resolved: {e}\" ) from e if isinstance(processor, LogitsProcessorConstructor): logits_processor = logits_processor( *processor.args or [], **processor.kwargs or {} ) logits_processors.append(logits_processor) return logits_processors elif processors: raise ValueError( \"The `logits_processors` argument is not supported by this \" \"server. See --logits-processor-pattern engine argument \" \"for more information.\" ) return None serialize_message ¶ serialize_message(msg) Serializes a single message Source code in vllm/entrypoints/openai/protocol.py 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665def serialize_message(msg): \"\"\" Serializes a single message \"\"\" if isinstance(msg, dict): return msg elif hasattr(msg, \"to_dict\"): return msg.to_dict() else: # fallback to pyandic dump return msg.model_dump_json() serialize_messages ¶ serialize_messages(msgs) Serializes multiple messages Source code in vllm/entrypoints/openai/protocol.py 1668 1669 1670 1671 1672def serialize_messages(msgs): \"\"\" Serializes multiple messages \"\"\" return [serialize_message(msg) for msg in msgs] if msgs else None",
      "code": ""
    }
  ],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}