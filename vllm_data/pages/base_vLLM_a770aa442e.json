{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
  "title": "base - vLLM",
  "content": "KVConnectorBase_V1 Class for Distributed KV Cache & Hidden State communication in vLLM v1\n\nScheduler-side: runs in the scheduler, binds metadata, which is used by the worker-side to load/save KV cache. get_num_new_matched_tokens() - get number of new tokens that exist in the remote KV cache. Might be called multiple times for a given request and should be side-effect free. update_state_after_alloc() - update KVConnector state after temporary buffer alloc by the CacheManager. update_connector_output() - update KVConnector state after output is received from worker-side connectors. request_finished() - called once when a request is finished, with the computed kv cache blocks for the request. Returns whether KV cache should be freed now or if the connector now assumes responsibility for freeing the the blocks asynchronously. Also optionally returns KV transfer params. take_events() - returns new KV events that were collected by the connector since the last call.\n\nWorker-side: runs in each worker, loads/saves KV cache to/from the Connector based on the metadata. start_load_kv() - starts loading all KVs (maybe async) wait_for_layer_load() - blocks until layer i load is done\n\nBase class for KV connectors.\n\nIndicates whether this connector prefers KV blocks that hold KV data for all layers (for speeding up KV data transfers). Defaults to False.\n\nGet the connector metadata.\n\nThis function should only be called inside the connector.\n\nthe connector metadata.\n\nSet the connector metadata from the scheduler.\n\nThis function should be called by the model runner every time before the model execution. The metadata will be used for runtime KV cache loading and saving.\n\nthe connector metadata.\n\nBuild the connector metadata for this step.\n\nThis function should NOT modify fields in the scheduler_output. Also, calling this function will reset the state of the connector.\n\nthe scheduler output object.\n\nKVConnectorStats resolution method. This method allows dynamically registered connectors to return their own KVConnectorStats object, which can implement custom aggregation logic on the data dict.\n\nCreate a KVConnectorPromMetrics subclass which should register per-connector Prometheus metrics and implement observe() to expose connector transfer stats via Prometheus.\n\nClear the connector metadata.\n\nThis function should be called by the model runner every time after the model execution.\n\nGet the set of block IDs that failed to load.\n\nSet of block IDs that encountered load errors.\n\nEmpty set if no load errors occurred.\n\nNotifies worker-side connector ids of requests that have finished generating tokens on the worker. The scheduler process (via the Executors) will use this output to track which workers are done.\n\nids of requests that have finished asynchronous transfer\n\n(requests that previously returned True from request_finished()),\n\ntuple of (sending/saving ids, recving/loading ids).\n\nThe finished saves/sends req ids must belong to a set provided in a\n\ncall to this method (this call or a prior one).\n\nGet the count of requests expected to complete send/receive operations via this connector. This method is used to initialize the KVOutputAggregator, overwriting the default world_size.\n\nexpected sending or receiving completion count.\n\nGet the KVConnector handshake metadata for this connector. This metadata is used for out-of-band connector handshake between P/D workers.\n\nthe handshake metadata.\n\nNone if no handshake metadata is available.\n\nGet the KV connector kv cache events collected during the last interval. This function should be called by the model runner every time after the model execution and before cleanup.\n\nGet the KV connector stats collected during the last interval.\n\nGet number of new tokens that can be loaded from the external KV cache beyond the num_computed_tokens.\n\nthe number of locally computed tokens for this request\n\nA tuple with the following elements: - An optional number of tokens that can be loaded from the external KV cache beyond what is already computed. If None, it means that the connector needs more time to determine the number of matched tokens, and the scheduler should query for this request again later. - True if external KV cache tokens will be loaded asynchronously (between scheduler steps). Must be 'False' if the first element is 0.\n\nThe connector should only consider the largest prefix of prompt- tokens for which KV cache is actually available at the time of the call. If the cache cannot be loaded for some tokens (e.g., due to connectivity issues or eviction), those tokens must not be taken into account.\n\nGet the required KV cache layout for this connector. Args: vllm_config (VllmConfig): the vllm config.\n\nthe required KV cache layout. e.g. HND, or NHD.\n\nNone if the connector does not require a specific layout.\n\nCheck whether the connector metadata is currently set.\n\nTrue if connector metadata exists, False otherwise.\n\nInitialize with a single KV cache tensor used by all layers. The first dimension should be num_layers. This function will only be called for models with uniform layers, and only if the prefers_cross_layer_blocks is set to True. Only one of the functions {register_kv_caches, register_cross_layers_kv_cache} will be called.\n\na cross-layers kv cache tensor\n\nThe attention backend that corresponds to all layers\n\nInitialize with the KV caches. Useful for pre-registering the KV Caches in the KVConnector (e.g. for NIXL).\n\ndictionary of layer names, kv cache\n\nCalled exactly once when a request has finished, before its blocks are freed.\n\nThe connector may assumes responsibility for freeing the blocks asynchronously by returning True.\n\nTrue if the request is being saved/sent asynchronously and blocks\n\nshould not be freed until the request_id is returned from\n\nOptional KVTransferParams to be included in the request outputs\n\nreturned by the engine.\n\nReset the connector's internal cache.\n\nTrue if the cache was successfully reset, False otherwise.\n\nStart saving a layer of KV cache from vLLM's paged buffer to the connector. This is called from within attention layer to enable async copying during execution.\n\nthe name of the layer.\n\nthe paged KV buffer of the current layer in vLLM.\n\nthe attention metadata.\n\nadditional arguments for the save operation.\n\nSet the xPU-specific ops for copying KV between host and device. Needed when host buffer is used for kv transfer (e.g., in NixlConnector)\n\nSet the KV connector handshake metadata for this connector.\n\nthe handshake metadata to set.\n\nShutdown the connector. This is called when the worker process is shutting down to ensure that all the async operations are completed and the connector is cleaned up properly.\n\nStart loading the KV cache from the connector to vLLM's paged KV buffer. This is called from the forward context before the forward pass to enable async loading during model execution.\n\nadditional arguments for the load operation\n\nThe number of elements in kv_caches and layer_names should be the same.\n\nTake the KV cache events from the connector.\n\nNew KV cache events since the last call.\n\nUpdate KVConnector state from worker-side connectors output.\n\nthe worker-side connectors output.\n\nUpdate KVConnector state after block allocation.\n\nIf get_num_new_matched_tokens previously returned True for a request, this function may be called twice for that same request - first when blocks are allocated for the connector tokens to be asynchronously loaded into, and second when any additional blocks are allocated, after the load/transfer is complete.\n\nthe blocks allocated for the request.\n\nthe number of tokens that will be loaded from the external KV cache.\n\nBlock until the KV for a specific layer is loaded into vLLM's paged buffer. This is called from within attention layer to ensure async copying from start_load_kv is complete.\n\nThis interface will be useful for layer-by-layer pipelining.\n\nthe name of that layer\n\nBlock until all the save operations is done. This is called as the forward context exits to ensure that the async saving from save_kv_layer is complete before finishing the forward.\n\nThis prevents overwrites of paged KV buffer before saving done.\n\nMetadata used for out of band connector handshake between P/D workers. This needs to serializeable.\n\nAbstract Metadata used to communicate between the Scheduler KVConnector and Worker KVConnector.\n\nThe class that indicates the corresponding connector supports hybrid memory allocator (HMA). This is required to use the connector together with hybrid memory allocator.\n\nCalled exactly once when a request has finished for all kv cache groups, before its blocks are freed for each group.\n\nNOTE(Kuntai): This function is only supported by connectors that support HMA.\n\nThe connector may assumes responsibility for freeing the blocks asynchronously by returning True.\n\nTrue if the request is being saved/sent asynchronously and blocks\n\nshould not be freed until the request_id is returned from\n\nOptional KVTransferParams to be included in the request outputs\n\nreturned by the engine.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.distributed.kv_transfer.kv_connector.v1.base ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base"
    },
    {
      "level": "h2",
      "text": "CopyBlocksOp module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.CopyBlocksOp"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.logger"
    },
    {
      "level": "h2",
      "text": "KVConnectorBase_V1 ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1"
    },
    {
      "level": "h3",
      "text": "_connector_metadata instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1._connector_metadata"
    },
    {
      "level": "h3",
      "text": "_kv_cache_config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1._kv_cache_config"
    },
    {
      "level": "h3",
      "text": "_kv_transfer_config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1._kv_transfer_config"
    },
    {
      "level": "h3",
      "text": "_role instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1._role"
    },
    {
      "level": "h3",
      "text": "_vllm_config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1._vllm_config"
    },
    {
      "level": "h3",
      "text": "prefer_cross_layer_blocks class-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.prefer_cross_layer_blocks"
    },
    {
      "level": "h3",
      "text": "role property ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.role"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.__init__"
    },
    {
      "level": "h3",
      "text": "_get_connector_metadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1._get_connector_metadata"
    },
    {
      "level": "h3",
      "text": "bind_connector_metadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.bind_connector_metadata"
    },
    {
      "level": "h3",
      "text": "build_connector_meta abstractmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.build_connector_meta"
    },
    {
      "level": "h3",
      "text": "build_kv_connector_stats classmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.build_kv_connector_stats"
    },
    {
      "level": "h3",
      "text": "build_prom_metrics classmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.build_prom_metrics"
    },
    {
      "level": "h3",
      "text": "clear_connector_metadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.clear_connector_metadata"
    },
    {
      "level": "h3",
      "text": "get_block_ids_with_load_errors ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.get_block_ids_with_load_errors"
    },
    {
      "level": "h3",
      "text": "get_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.get_finished"
    },
    {
      "level": "h3",
      "text": "get_finished_count ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.get_finished_count"
    },
    {
      "level": "h3",
      "text": "get_handshake_metadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.get_handshake_metadata"
    },
    {
      "level": "h3",
      "text": "get_kv_connector_kv_cache_events ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.get_kv_connector_kv_cache_events"
    },
    {
      "level": "h3",
      "text": "get_kv_connector_stats ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.get_kv_connector_stats"
    },
    {
      "level": "h3",
      "text": "get_num_new_matched_tokens abstractmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.get_num_new_matched_tokens"
    },
    {
      "level": "h3",
      "text": "get_required_kvcache_layout classmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.get_required_kvcache_layout"
    },
    {
      "level": "h3",
      "text": "has_connector_metadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.has_connector_metadata"
    },
    {
      "level": "h3",
      "text": "register_cross_layers_kv_cache ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.register_cross_layers_kv_cache"
    },
    {
      "level": "h3",
      "text": "register_kv_caches ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.register_kv_caches"
    },
    {
      "level": "h3",
      "text": "request_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.request_finished"
    },
    {
      "level": "h3",
      "text": "reset_cache ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.reset_cache"
    },
    {
      "level": "h3",
      "text": "save_kv_layer abstractmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.save_kv_layer"
    },
    {
      "level": "h3",
      "text": "set_host_xfer_buffer_ops ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.set_host_xfer_buffer_ops"
    },
    {
      "level": "h3",
      "text": "set_xfer_handshake_metadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.set_xfer_handshake_metadata"
    },
    {
      "level": "h3",
      "text": "shutdown ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.shutdown"
    },
    {
      "level": "h3",
      "text": "start_load_kv abstractmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.start_load_kv"
    },
    {
      "level": "h3",
      "text": "take_events ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.take_events"
    },
    {
      "level": "h3",
      "text": "update_connector_output ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.update_connector_output"
    },
    {
      "level": "h3",
      "text": "update_state_after_alloc abstractmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.update_state_after_alloc"
    },
    {
      "level": "h3",
      "text": "wait_for_layer_load abstractmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.wait_for_layer_load"
    },
    {
      "level": "h3",
      "text": "wait_for_save abstractmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorBase_V1.wait_for_save"
    },
    {
      "level": "h2",
      "text": "KVConnectorHandshakeMetadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorHandshakeMetadata"
    },
    {
      "level": "h2",
      "text": "KVConnectorMetadata ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorMetadata"
    },
    {
      "level": "h2",
      "text": "KVConnectorRole ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorRole"
    },
    {
      "level": "h3",
      "text": "SCHEDULER class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorRole.SCHEDULER"
    },
    {
      "level": "h3",
      "text": "WORKER class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.KVConnectorRole.WORKER"
    },
    {
      "level": "h2",
      "text": "SupportsHMA ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.SupportsHMA"
    },
    {
      "level": "h3",
      "text": "request_finished_all_groups abstractmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.SupportsHMA.request_finished_all_groups"
    },
    {
      "level": "h2",
      "text": "supports_hma ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.base.supports_hma"
    }
  ],
  "code_samples": [
    {
      "code": "save_kv_layer() - starts saving KV for layer i (maybe async)\nwait_for_save() - blocks until all saves are done\n\nget_finished() - called with ids of finished requests, returns\n    ids of requests that have completed async sending/recving.",
      "language": "unknown"
    },
    {
      "code": "save_kv_layer() - starts saving KV for layer i (maybe async)\nwait_for_save() - blocks until all saves are done\n\nget_finished() - called with ids of finished requests, returns\n    ids of requests that have completed async sending/recving.",
      "language": "unknown"
    },
    {
      "code": "CopyBlocksOp = Callable[\n    [\n        dict[str, Tensor],\n        dict[str, Tensor],\n        list[int],\n        list[int],\n        Literal[\"h2d\", \"d2h\"],\n    ],\n    None,\n]",
      "language": "json"
    },
    {
      "code": "CopyBlocksOp = Callable[\n    [\n        dict[str, Tensor],\n        dict[str, Tensor],\n        list[int],\n        list[int],\n        Literal[\"h2d\", \"d2h\"],\n    ],\n    None,\n]",
      "language": "json"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597",
      "language": "unknown"
    },
    {
      "code": "class KVConnectorBase_V1(ABC):\n    \"\"\"\n    Base class for KV connectors.\n\n    Attributes:\n        prefer_cross_layer_blocks (bool): Indicates whether this connector\n            prefers KV blocks that hold KV data for all layers (for speeding\n            up KV data transfers).\n            Defaults to False.\n    \"\"\"\n\n    prefer_cross_layer_blocks: ClassVar[bool] = False\n\n    def __init__(\n        self,\n        vllm_config: \"VllmConfig\",\n        role: KVConnectorRole,\n        kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n    ):\n        logger.warning(\n            \"Initializing KVConnectorBase_V1. This API is experimental and \"\n            \"subject to change in the future as we iterate the design.\"\n        )\n        self._connector_metadata: KVConnectorMetadata | None = None\n        self._vllm_config = vllm_config\n        if vllm_config.kv_transfer_config is not None:\n            self._kv_transfer_config = vllm_config.kv_transfer_config\n        else:\n            raise ValueError(\"kv_transfer_config must be set for KVConnectorBase_V1\")\n        self._kv_cache_config = kv_cache_config\n        if self._kv_cache_config is None:\n            logger.warning(\n                \"KVConnectorBase_V1 initialized without kv_cache_config. \"\n                \"This is deprecated - please update your connector to accept \"\n                \"kv_cache_config as the third constructor argument and pass it \"\n                \"to super().__init__().\"\n            )\n        self._role = role\n\n    @property\n    def role(self) -> KVConnectorRole:\n        return self._role\n\n    # ==============================\n    # Worker-side methods\n    # ==============================\n\n    def bind_connector_metadata(self, connector_metadata: KVConnectorMetadata) -> None:\n        \"\"\"Set the connector metadata from the scheduler.\n\n        This function should be called by the model runner every time\n        before the model execution. The metadata will be used for runtime\n        KV cache loading and saving.\n\n        Args:\n            connector_metadata (dict): the connector metadata.\n        \"\"\"\n        self._connector_metadata = connector_metadata\n\n    def clear_connector_metadata(self) -> None:\n        \"\"\"Clear the connector metadata.\n\n        This function should be called by the model runner every time\n        after the model execution.\n        \"\"\"\n        self._connector_metadata = None\n\n    def _get_connector_metadata(self) -> KVConnectorMetadata:\n        \"\"\"Get the connector metadata.\n\n        This function should only be called inside the connector.\n\n        Returns:\n            ConnectorMetadata: the connector metadata.\n        \"\"\"\n        # Should only be called while set to valid metadata.\n        assert self._connector_metadata is not None\n        return self._connector_metadata\n\n    def has_connector_metadata(self) -> bool:\n        \"\"\"Check whether the connector metadata is currently set.\n\n        Returns:\n            bool: True if connector metadata exists, False otherwise.\n        \"\"\"\n        return self._connector_metadata is not None\n\n    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n        \"\"\"\n        Initialize with the KV caches. Useful for pre-registering the\n        KV Caches in the KVConnector (e.g. for NIXL).\n\n        Args:\n            kv_caches: dictionary of layer names, kv cache\n        \"\"\"\n        return\n\n    def register_cross_layers_kv_cache(\n        self, kv_cache: torch.Tensor, attn_backend: type[\"AttentionBackend\"]\n    ):\n        \"\"\"\n        Initialize with a single KV cache tensor used by all layers.\n        The first dimension should be num_layers.\n        This function will only be called for models with uniform layers,\n        and only if the prefers_cross_layer_blocks is set to True.\n        Only one of the functions\n        {register_kv_caches, register_cross_layers_kv_cache} will be called.\n\n        Args:\n            kv_cache: a cross-layers kv cache tensor\n            attn_backend: The attention backend that corresponds to all layers\n        \"\"\"\n        return\n\n    def set_host_xfer_buffer_ops(self, copy_operation: CopyBlocksOp):\n        \"\"\"\n        Set the xPU-specific ops for copying KV between host and device.\n        Needed when host buffer is used for kv transfer (e.g., in NixlConnector)\n        \"\"\"\n        return\n\n    @abstractmethod\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n        \"\"\"\n        Start loading the KV cache from the connector to vLLM's paged\n        KV buffer. This is called from the forward context before the\n        forward pass to enable async loading during model execution.\n\n        Args:\n            forward_context (ForwardContext): the forward context.\n            **kwargs: additional arguments for the load operation\n\n        Note:\n            The number of elements in kv_caches and layer_names should be\n            the same.\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"\n        Block until the KV for a specific layer is loaded into vLLM's\n        paged buffer. This is called from within attention layer to ensure\n        async copying from start_load_kv is complete.\n\n        This interface will be useful for layer-by-layer pipelining.\n\n        Args:\n            layer_name: the name of that layer\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: \"AttentionMetadata\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        Start saving a layer of KV cache from vLLM's paged buffer\n        to the connector. This is called from within attention layer to\n        enable async copying during execution.\n\n        Args:\n            layer_name (str): the name of the layer.\n            kv_layer (torch.Tensor): the paged KV buffer of the current\n                layer in vLLM.\n            attn_metadata (AttentionMetadata): the attention metadata.\n            **kwargs: additional arguments for the save operation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def wait_for_save(self):\n        \"\"\"\n        Block until all the save operations is done. This is called\n        as the forward context exits to ensure that the async saving\n        from save_kv_layer is complete before finishing the forward.\n\n        This prevents overwrites of paged KV buffer before saving done.\n        \"\"\"\n        pass\n\n    def get_finished(\n        self, finished_req_ids: set[str]\n    ) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"\n        Notifies worker-side connector ids of requests that have\n        finished generating tokens on the worker.\n        The scheduler process (via the Executors) will use this output\n        to track which workers are done.\n\n        Returns:\n            ids of requests that have finished asynchronous transfer\n            (requests that previously returned True from request_finished()),\n            tuple of (sending/saving ids, recving/loading ids).\n            The finished saves/sends req ids must belong to a set provided in a\n            call to this method (this call or a prior one).\n        \"\"\"\n        return None, None\n\n    def get_block_ids_with_load_errors(self) -> set[int]:\n        \"\"\"\n        Get the set of block IDs that failed to load.\n\n        Returns:\n            Set of block IDs that encountered load errors.\n            Empty set if no load errors occurred.\n\n        Notes:\n            - Applies to both sync- and async-loading requests.\n            - Async loading: failed blocks may be reported in any forward pass\n              up to and including the pass where the request ID is returned by\n              `get_finished()`. Even if failures occur, the request must still\n              be reported via `get_finished()`, and the failed block IDs must\n              appear here no later than that same pass.\n            - Sync loading: failed blocks should be reported in the forward\n              pass in which they are detected.\n        \"\"\"\n        return set()\n\n    def shutdown(self):\n        \"\"\"\n        Shutdown the connector. This is called when the worker process\n        is shutting down to ensure that all the async operations are\n        completed and the connector is cleaned up properly.\n        \"\"\"\n        return None\n\n    def get_kv_connector_stats(self) -> Optional[\"KVConnectorStats\"]:\n        \"\"\"\n        Get the KV connector stats collected during the last interval.\n        \"\"\"\n        return None\n\n    def get_kv_connector_kv_cache_events(self) -> Optional[\"KVConnectorKVEvents\"]:\n        \"\"\"\n        Get the KV connector kv cache events collected during the last interval.\n        This function should be called by the model runner every time after the\n        model execution and before cleanup.\n        \"\"\"\n        return None\n\n    def get_handshake_metadata(self) -> KVConnectorHandshakeMetadata | None:\n        \"\"\"\n        Get the KVConnector handshake metadata for this connector.\n        This metadata is used for out-of-band connector handshake\n        between P/D workers.\n\n        Returns:\n            KVConnectorHandshakeMetadata: the handshake metadata.\n            None if no handshake metadata is available.\n        \"\"\"\n        return None\n\n    # ==============================\n    # Scheduler-side methods\n    # ==============================\n\n    @abstractmethod\n    def get_num_new_matched_tokens(\n        self,\n        request: \"Request\",\n        num_computed_tokens: int,\n    ) -> tuple[int | None, bool]:\n        \"\"\"\n        Get number of new tokens that can be loaded from the\n        external KV cache beyond the num_computed_tokens.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n\n        Returns:\n            A tuple with the following elements:\n                - An optional number of tokens that can be loaded from the\n                  external KV cache beyond what is already computed.\n                  If None, it means that the connector needs more time to\n                  determine the number of matched tokens, and the scheduler\n                  should query for this request again later.\n                - `True` if external KV cache tokens will be loaded\n                  asynchronously (between scheduler steps). Must be\n                  'False' if the first element is 0.\n\n        Notes:\n            The connector should only consider the largest prefix of prompt-\n            tokens for which KV cache is actually available at the time of the\n            call. If the cache cannot be loaded for some tokens (e.g., due to\n            connectivity issues or eviction), those tokens must not be taken\n            into account.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        \"\"\"\n        Update KVConnector state after block allocation.\n\n        If get_num_new_matched_tokens previously returned True for a\n        request, this function may be called twice for that same request -\n        first when blocks are allocated for the connector tokens to be\n        asynchronously loaded into, and second when any additional blocks\n        are allocated, after the load/transfer is complete.\n\n        Args:\n            request (Request): the request object.\n            blocks (KVCacheBlocks): the blocks allocated for the request.\n            num_external_tokens (int): the number of tokens that will be\n                loaded from the external KV cache.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def build_connector_meta(\n        self, scheduler_output: SchedulerOutput\n    ) -> KVConnectorMetadata:\n        \"\"\"\n        Build the connector metadata for this step.\n\n        This function should NOT modify fields in the scheduler_output.\n        Also, calling this function will reset the state of the connector.\n\n        Args:\n            scheduler_output (SchedulerOutput): the scheduler output object.\n        \"\"\"\n        pass\n\n    def update_connector_output(self, connector_output: KVConnectorOutput):\n        \"\"\"\n        Update KVConnector state from worker-side connectors output.\n\n        Args:\n            connector_output (KVConnectorOutput): the worker-side\n                connectors output.\n        \"\"\"\n        return\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Called exactly once when a request has finished, before its blocks are\n        freed.\n\n        The connector may assumes responsibility for freeing the blocks\n        asynchronously by returning True.\n\n        Returns:\n            True if the request is being saved/sent asynchronously and blocks\n            should not be freed until the request_id is returned from\n            get_finished().\n            Optional KVTransferParams to be included in the request outputs\n            returned by the engine.\n        \"\"\"\n        return False, None\n\n    def take_events(self) -> Iterable[\"KVCacheEvent\"]:\n        \"\"\"\n        Take the KV cache events from the connector.\n\n        Yields:\n            New KV cache events since the last call.\n        \"\"\"\n        return ()\n\n    @classmethod\n    def get_required_kvcache_layout(cls, vllm_config: \"VllmConfig\") -> str | None:\n        \"\"\"\n        Get the required KV cache layout for this connector.\n        Args:\n            vllm_config (VllmConfig): the vllm config.\n\n        Returns:\n            str: the required KV cache layout. e.g. HND, or NHD.\n            None if the connector does not require a specific layout.\n        \"\"\"\n\n        if cls is KVConnectorBase_V1:\n            raise TypeError(\n                \"get_required_kvcache_layout should not be called \"\n                \"on the abstract base class\"\n            )\n        return None\n\n    def get_finished_count(self) -> int | None:\n        \"\"\"\n        Get the count of requests expected to complete send/receive operations\n        via this connector. This method is used to initialize the\n        KVOutputAggregator, overwriting the default world_size.\n\n        Returns:\n            int: expected sending or receiving completion count.\n        \"\"\"\n\n        return None\n\n    @classmethod\n    def build_kv_connector_stats(\n        cls, data: dict[str, Any] | None = None\n    ) -> Optional[\"KVConnectorStats\"]:\n        \"\"\"\n        KVConnectorStats resolution method. This method allows dynamically\n        registered connectors to return their own KVConnectorStats object,\n        which can implement custom aggregation logic on the data dict.\n        \"\"\"\n        return None\n\n    def set_xfer_handshake_metadata(\n        self, metadata: dict[int, KVConnectorHandshakeMetadata]\n    ) -> None:\n        \"\"\"\n        Set the KV connector handshake metadata for this connector.\n\n        Args:\n            metadata (KVConnectorHandshakeMetadata): the handshake metadata to set.\n        \"\"\"\n        return None\n\n    @classmethod\n    def build_prom_metrics(\n        cls,\n        vllm_config: \"VllmConfig\",\n        metric_types: dict[type[\"PromMetric\"], type[\"PromMetricT\"]],\n        labelnames: list[str],\n        per_engine_labelvalues: dict[int, list[object]],\n    ) -> Optional[\"KVConnectorPromMetrics\"]:\n        \"\"\"\n        Create a KVConnectorPromMetrics subclass which should register\n        per-connector Prometheus metrics and implement observe() to\n        expose connector transfer stats via Prometheus.\n        \"\"\"\n        return None\n\n    def reset_cache(self) -> bool | None:\n        \"\"\"\n        Reset the connector's internal cache.\n\n        Returns:\n            bool: True if the cache was successfully reset, False otherwise.\n        \"\"\"\n        logger.debug(\n            \"Connector cache reset requested, but %s does not implement reset_cache().\",\n            type(self).__name__,\n        )\n\n        return None",
      "language": "python"
    },
    {
      "code": "class KVConnectorBase_V1(ABC):\n    \"\"\"\n    Base class for KV connectors.\n\n    Attributes:\n        prefer_cross_layer_blocks (bool): Indicates whether this connector\n            prefers KV blocks that hold KV data for all layers (for speeding\n            up KV data transfers).\n            Defaults to False.\n    \"\"\"\n\n    prefer_cross_layer_blocks: ClassVar[bool] = False\n\n    def __init__(\n        self,\n        vllm_config: \"VllmConfig\",\n        role: KVConnectorRole,\n        kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n    ):\n        logger.warning(\n            \"Initializing KVConnectorBase_V1. This API is experimental and \"\n            \"subject to change in the future as we iterate the design.\"\n        )\n        self._connector_metadata: KVConnectorMetadata | None = None\n        self._vllm_config = vllm_config\n        if vllm_config.kv_transfer_config is not None:\n            self._kv_transfer_config = vllm_config.kv_transfer_config\n        else:\n            raise ValueError(\"kv_transfer_config must be set for KVConnectorBase_V1\")\n        self._kv_cache_config = kv_cache_config\n        if self._kv_cache_config is None:\n            logger.warning(\n                \"KVConnectorBase_V1 initialized without kv_cache_config. \"\n                \"This is deprecated - please update your connector to accept \"\n                \"kv_cache_config as the third constructor argument and pass it \"\n                \"to super().__init__().\"\n            )\n        self._role = role\n\n    @property\n    def role(self) -> KVConnectorRole:\n        return self._role\n\n    # ==============================\n    # Worker-side methods\n    # ==============================\n\n    def bind_connector_metadata(self, connector_metadata: KVConnectorMetadata) -> None:\n        \"\"\"Set the connector metadata from the scheduler.\n\n        This function should be called by the model runner every time\n        before the model execution. The metadata will be used for runtime\n        KV cache loading and saving.\n\n        Args:\n            connector_metadata (dict): the connector metadata.\n        \"\"\"\n        self._connector_metadata = connector_metadata\n\n    def clear_connector_metadata(self) -> None:\n        \"\"\"Clear the connector metadata.\n\n        This function should be called by the model runner every time\n        after the model execution.\n        \"\"\"\n        self._connector_metadata = None\n\n    def _get_connector_metadata(self) -> KVConnectorMetadata:\n        \"\"\"Get the connector metadata.\n\n        This function should only be called inside the connector.\n\n        Returns:\n            ConnectorMetadata: the connector metadata.\n        \"\"\"\n        # Should only be called while set to valid metadata.\n        assert self._connector_metadata is not None\n        return self._connector_metadata\n\n    def has_connector_metadata(self) -> bool:\n        \"\"\"Check whether the connector metadata is currently set.\n\n        Returns:\n            bool: True if connector metadata exists, False otherwise.\n        \"\"\"\n        return self._connector_metadata is not None\n\n    def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n        \"\"\"\n        Initialize with the KV caches. Useful for pre-registering the\n        KV Caches in the KVConnector (e.g. for NIXL).\n\n        Args:\n            kv_caches: dictionary of layer names, kv cache\n        \"\"\"\n        return\n\n    def register_cross_layers_kv_cache(\n        self, kv_cache: torch.Tensor, attn_backend: type[\"AttentionBackend\"]\n    ):\n        \"\"\"\n        Initialize with a single KV cache tensor used by all layers.\n        The first dimension should be num_layers.\n        This function will only be called for models with uniform layers,\n        and only if the prefers_cross_layer_blocks is set to True.\n        Only one of the functions\n        {register_kv_caches, register_cross_layers_kv_cache} will be called.\n\n        Args:\n            kv_cache: a cross-layers kv cache tensor\n            attn_backend: The attention backend that corresponds to all layers\n        \"\"\"\n        return\n\n    def set_host_xfer_buffer_ops(self, copy_operation: CopyBlocksOp):\n        \"\"\"\n        Set the xPU-specific ops for copying KV between host and device.\n        Needed when host buffer is used for kv transfer (e.g., in NixlConnector)\n        \"\"\"\n        return\n\n    @abstractmethod\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n        \"\"\"\n        Start loading the KV cache from the connector to vLLM's paged\n        KV buffer. This is called from the forward context before the\n        forward pass to enable async loading during model execution.\n\n        Args:\n            forward_context (ForwardContext): the forward context.\n            **kwargs: additional arguments for the load operation\n\n        Note:\n            The number of elements in kv_caches and layer_names should be\n            the same.\n\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"\n        Block until the KV for a specific layer is loaded into vLLM's\n        paged buffer. This is called from within attention layer to ensure\n        async copying from start_load_kv is complete.\n\n        This interface will be useful for layer-by-layer pipelining.\n\n        Args:\n            layer_name: the name of that layer\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: \"AttentionMetadata\",\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"\n        Start saving a layer of KV cache from vLLM's paged buffer\n        to the connector. This is called from within attention layer to\n        enable async copying during execution.\n\n        Args:\n            layer_name (str): the name of the layer.\n            kv_layer (torch.Tensor): the paged KV buffer of the current\n                layer in vLLM.\n            attn_metadata (AttentionMetadata): the attention metadata.\n            **kwargs: additional arguments for the save operation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def wait_for_save(self):\n        \"\"\"\n        Block until all the save operations is done. This is called\n        as the forward context exits to ensure that the async saving\n        from save_kv_layer is complete before finishing the forward.\n\n        This prevents overwrites of paged KV buffer before saving done.\n        \"\"\"\n        pass\n\n    def get_finished(\n        self, finished_req_ids: set[str]\n    ) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"\n        Notifies worker-side connector ids of requests that have\n        finished generating tokens on the worker.\n        The scheduler process (via the Executors) will use this output\n        to track which workers are done.\n\n        Returns:\n            ids of requests that have finished asynchronous transfer\n            (requests that previously returned True from request_finished()),\n            tuple of (sending/saving ids, recving/loading ids).\n            The finished saves/sends req ids must belong to a set provided in a\n            call to this method (this call or a prior one).\n        \"\"\"\n        return None, None\n\n    def get_block_ids_with_load_errors(self) -> set[int]:\n        \"\"\"\n        Get the set of block IDs that failed to load.\n\n        Returns:\n            Set of block IDs that encountered load errors.\n            Empty set if no load errors occurred.\n\n        Notes:\n            - Applies to both sync- and async-loading requests.\n            - Async loading: failed blocks may be reported in any forward pass\n              up to and including the pass where the request ID is returned by\n              `get_finished()`. Even if failures occur, the request must still\n              be reported via `get_finished()`, and the failed block IDs must\n              appear here no later than that same pass.\n            - Sync loading: failed blocks should be reported in the forward\n              pass in which they are detected.\n        \"\"\"\n        return set()\n\n    def shutdown(self):\n        \"\"\"\n        Shutdown the connector. This is called when the worker process\n        is shutting down to ensure that all the async operations are\n        completed and the connector is cleaned up properly.\n        \"\"\"\n        return None\n\n    def get_kv_connector_stats(self) -> Optional[\"KVConnectorStats\"]:\n        \"\"\"\n        Get the KV connector stats collected during the last interval.\n        \"\"\"\n        return None\n\n    def get_kv_connector_kv_cache_events(self) -> Optional[\"KVConnectorKVEvents\"]:\n        \"\"\"\n        Get the KV connector kv cache events collected during the last interval.\n        This function should be called by the model runner every time after the\n        model execution and before cleanup.\n        \"\"\"\n        return None\n\n    def get_handshake_metadata(self) -> KVConnectorHandshakeMetadata | None:\n        \"\"\"\n        Get the KVConnector handshake metadata for this connector.\n        This metadata is used for out-of-band connector handshake\n        between P/D workers.\n\n        Returns:\n            KVConnectorHandshakeMetadata: the handshake metadata.\n            None if no handshake metadata is available.\n        \"\"\"\n        return None\n\n    # ==============================\n    # Scheduler-side methods\n    # ==============================\n\n    @abstractmethod\n    def get_num_new_matched_tokens(\n        self,\n        request: \"Request\",\n        num_computed_tokens: int,\n    ) -> tuple[int | None, bool]:\n        \"\"\"\n        Get number of new tokens that can be loaded from the\n        external KV cache beyond the num_computed_tokens.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n\n        Returns:\n            A tuple with the following elements:\n                - An optional number of tokens that can be loaded from the\n                  external KV cache beyond what is already computed.\n                  If None, it means that the connector needs more time to\n                  determine the number of matched tokens, and the scheduler\n                  should query for this request again later.\n                - `True` if external KV cache tokens will be loaded\n                  asynchronously (between scheduler steps). Must be\n                  'False' if the first element is 0.\n\n        Notes:\n            The connector should only consider the largest prefix of prompt-\n            tokens for which KV cache is actually available at the time of the\n            call. If the cache cannot be loaded for some tokens (e.g., due to\n            connectivity issues or eviction), those tokens must not be taken\n            into account.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        \"\"\"\n        Update KVConnector state after block allocation.\n\n        If get_num_new_matched_tokens previously returned True for a\n        request, this function may be called twice for that same request -\n        first when blocks are allocated for the connector tokens to be\n        asynchronously loaded into, and second when any additional blocks\n        are allocated, after the load/transfer is complete.\n\n        Args:\n            request (Request): the request object.\n            blocks (KVCacheBlocks): the blocks allocated for the request.\n            num_external_tokens (int): the number of tokens that will be\n                loaded from the external KV cache.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def build_connector_meta(\n        self, scheduler_output: SchedulerOutput\n    ) -> KVConnectorMetadata:\n        \"\"\"\n        Build the connector metadata for this step.\n\n        This function should NOT modify fields in the scheduler_output.\n        Also, calling this function will reset the state of the connector.\n\n        Args:\n            scheduler_output (SchedulerOutput): the scheduler output object.\n        \"\"\"\n        pass\n\n    def update_connector_output(self, connector_output: KVConnectorOutput):\n        \"\"\"\n        Update KVConnector state from worker-side connectors output.\n\n        Args:\n            connector_output (KVConnectorOutput): the worker-side\n                connectors output.\n        \"\"\"\n        return\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Called exactly once when a request has finished, before its blocks are\n        freed.\n\n        The connector may assumes responsibility for freeing the blocks\n        asynchronously by returning True.\n\n        Returns:\n            True if the request is being saved/sent asynchronously and blocks\n            should not be freed until the request_id is returned from\n            get_finished().\n            Optional KVTransferParams to be included in the request outputs\n            returned by the engine.\n        \"\"\"\n        return False, None\n\n    def take_events(self) -> Iterable[\"KVCacheEvent\"]:\n        \"\"\"\n        Take the KV cache events from the connector.\n\n        Yields:\n            New KV cache events since the last call.\n        \"\"\"\n        return ()\n\n    @classmethod\n    def get_required_kvcache_layout(cls, vllm_config: \"VllmConfig\") -> str | None:\n        \"\"\"\n        Get the required KV cache layout for this connector.\n        Args:\n            vllm_config (VllmConfig): the vllm config.\n\n        Returns:\n            str: the required KV cache layout. e.g. HND, or NHD.\n            None if the connector does not require a specific layout.\n        \"\"\"\n\n        if cls is KVConnectorBase_V1:\n            raise TypeError(\n                \"get_required_kvcache_layout should not be called \"\n                \"on the abstract base class\"\n            )\n        return None\n\n    def get_finished_count(self) -> int | None:\n        \"\"\"\n        Get the count of requests expected to complete send/receive operations\n        via this connector. This method is used to initialize the\n        KVOutputAggregator, overwriting the default world_size.\n\n        Returns:\n            int: expected sending or receiving completion count.\n        \"\"\"\n\n        return None\n\n    @classmethod\n    def build_kv_connector_stats(\n        cls, data: dict[str, Any] | None = None\n    ) -> Optional[\"KVConnectorStats\"]:\n        \"\"\"\n        KVConnectorStats resolution method. This method allows dynamically\n        registered connectors to return their own KVConnectorStats object,\n        which can implement custom aggregation logic on the data dict.\n        \"\"\"\n        return None\n\n    def set_xfer_handshake_metadata(\n        self, metadata: dict[int, KVConnectorHandshakeMetadata]\n    ) -> None:\n        \"\"\"\n        Set the KV connector handshake metadata for this connector.\n\n        Args:\n            metadata (KVConnectorHandshakeMetadata): the handshake metadata to set.\n        \"\"\"\n        return None\n\n    @classmethod\n    def build_prom_metrics(\n        cls,\n        vllm_config: \"VllmConfig\",\n        metric_types: dict[type[\"PromMetric\"], type[\"PromMetricT\"]],\n        labelnames: list[str],\n        per_engine_labelvalues: dict[int, list[object]],\n    ) -> Optional[\"KVConnectorPromMetrics\"]:\n        \"\"\"\n        Create a KVConnectorPromMetrics subclass which should register\n        per-connector Prometheus metrics and implement observe() to\n        expose connector transfer stats via Prometheus.\n        \"\"\"\n        return None\n\n    def reset_cache(self) -> bool | None:\n        \"\"\"\n        Reset the connector's internal cache.\n\n        Returns:\n            bool: True if the cache was successfully reset, False otherwise.\n        \"\"\"\n        logger.debug(\n            \"Connector cache reset requested, but %s does not implement reset_cache().\",\n            type(self).__name__,\n        )\n\n        return None",
      "language": "python"
    },
    {
      "code": "_connector_metadata: KVConnectorMetadata | None = None",
      "language": "yaml"
    },
    {
      "code": "_connector_metadata: KVConnectorMetadata | None = None",
      "language": "yaml"
    },
    {
      "code": "_kv_cache_config = kv_cache_config",
      "language": "unknown"
    },
    {
      "code": "_kv_cache_config = kv_cache_config",
      "language": "unknown"
    },
    {
      "code": "_kv_transfer_config = kv_transfer_config",
      "language": "unknown"
    },
    {
      "code": "_kv_transfer_config = kv_transfer_config",
      "language": "unknown"
    },
    {
      "code": "_role = role",
      "language": "unknown"
    },
    {
      "code": "_role = role",
      "language": "unknown"
    },
    {
      "code": "_vllm_config = vllm_config",
      "language": "unknown"
    },
    {
      "code": "_vllm_config = vllm_config",
      "language": "unknown"
    },
    {
      "code": "prefer_cross_layer_blocks: bool = False",
      "language": "typescript"
    },
    {
      "code": "prefer_cross_layer_blocks: bool = False",
      "language": "typescript"
    },
    {
      "code": "role: KVConnectorRole",
      "language": "yaml"
    },
    {
      "code": "role: KVConnectorRole",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n)",
      "language": "python"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n)",
      "language": "python"
    },
    {
      "code": "157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: \"VllmConfig\",\n    role: KVConnectorRole,\n    kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n):\n    logger.warning(\n        \"Initializing KVConnectorBase_V1. This API is experimental and \"\n        \"subject to change in the future as we iterate the design.\"\n    )\n    self._connector_metadata: KVConnectorMetadata | None = None\n    self._vllm_config = vllm_config\n    if vllm_config.kv_transfer_config is not None:\n        self._kv_transfer_config = vllm_config.kv_transfer_config\n    else:\n        raise ValueError(\"kv_transfer_config must be set for KVConnectorBase_V1\")\n    self._kv_cache_config = kv_cache_config\n    if self._kv_cache_config is None:\n        logger.warning(\n            \"KVConnectorBase_V1 initialized without kv_cache_config. \"\n            \"This is deprecated - please update your connector to accept \"\n            \"kv_cache_config as the third constructor argument and pass it \"\n            \"to super().__init__().\"\n        )\n    self._role = role",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: \"VllmConfig\",\n    role: KVConnectorRole,\n    kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n):\n    logger.warning(\n        \"Initializing KVConnectorBase_V1. This API is experimental and \"\n        \"subject to change in the future as we iterate the design.\"\n    )\n    self._connector_metadata: KVConnectorMetadata | None = None\n    self._vllm_config = vllm_config\n    if vllm_config.kv_transfer_config is not None:\n        self._kv_transfer_config = vllm_config.kv_transfer_config\n    else:\n        raise ValueError(\"kv_transfer_config must be set for KVConnectorBase_V1\")\n    self._kv_cache_config = kv_cache_config\n    if self._kv_cache_config is None:\n        logger.warning(\n            \"KVConnectorBase_V1 initialized without kv_cache_config. \"\n            \"This is deprecated - please update your connector to accept \"\n            \"kv_cache_config as the third constructor argument and pass it \"\n            \"to super().__init__().\"\n        )\n    self._role = role",
      "language": "python"
    },
    {
      "code": "_get_connector_metadata() -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "_get_connector_metadata() -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221",
      "language": "unknown"
    },
    {
      "code": "def _get_connector_metadata(self) -> KVConnectorMetadata:\n    \"\"\"Get the connector metadata.\n\n    This function should only be called inside the connector.\n\n    Returns:\n        ConnectorMetadata: the connector metadata.\n    \"\"\"\n    # Should only be called while set to valid metadata.\n    assert self._connector_metadata is not None\n    return self._connector_metadata",
      "language": "python"
    },
    {
      "code": "def _get_connector_metadata(self) -> KVConnectorMetadata:\n    \"\"\"Get the connector metadata.\n\n    This function should only be called inside the connector.\n\n    Returns:\n        ConnectorMetadata: the connector metadata.\n    \"\"\"\n    # Should only be called while set to valid metadata.\n    assert self._connector_metadata is not None\n    return self._connector_metadata",
      "language": "python"
    },
    {
      "code": "bind_connector_metadata(\n    connector_metadata: KVConnectorMetadata,\n) -> None",
      "language": "rust"
    },
    {
      "code": "bind_connector_metadata(\n    connector_metadata: KVConnectorMetadata,\n) -> None",
      "language": "rust"
    },
    {
      "code": "191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201",
      "language": "unknown"
    },
    {
      "code": "def bind_connector_metadata(self, connector_metadata: KVConnectorMetadata) -> None:\n    \"\"\"Set the connector metadata from the scheduler.\n\n    This function should be called by the model runner every time\n    before the model execution. The metadata will be used for runtime\n    KV cache loading and saving.\n\n    Args:\n        connector_metadata (dict): the connector metadata.\n    \"\"\"\n    self._connector_metadata = connector_metadata",
      "language": "python"
    },
    {
      "code": "def bind_connector_metadata(self, connector_metadata: KVConnectorMetadata) -> None:\n    \"\"\"Set the connector metadata from the scheduler.\n\n    This function should be called by the model runner every time\n    before the model execution. The metadata will be used for runtime\n    KV cache loading and saving.\n\n    Args:\n        connector_metadata (dict): the connector metadata.\n    \"\"\"\n    self._connector_metadata = connector_metadata",
      "language": "python"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef build_connector_meta(\n    self, scheduler_output: SchedulerOutput\n) -> KVConnectorMetadata:\n    \"\"\"\n    Build the connector metadata for this step.\n\n    This function should NOT modify fields in the scheduler_output.\n    Also, calling this function will reset the state of the connector.\n\n    Args:\n        scheduler_output (SchedulerOutput): the scheduler output object.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef build_connector_meta(\n    self, scheduler_output: SchedulerOutput\n) -> KVConnectorMetadata:\n    \"\"\"\n    Build the connector metadata for this step.\n\n    This function should NOT modify fields in the scheduler_output.\n    Also, calling this function will reset the state of the connector.\n\n    Args:\n        scheduler_output (SchedulerOutput): the scheduler output object.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "build_kv_connector_stats(\n    data: dict[str, Any] | None = None,\n) -> Optional[KVConnectorStats]",
      "language": "rust"
    },
    {
      "code": "build_kv_connector_stats(\n    data: dict[str, Any] | None = None,\n) -> Optional[KVConnectorStats]",
      "language": "rust"
    },
    {
      "code": "548\n549\n550\n551\n552\n553\n554\n555\n556\n557",
      "language": "unknown"
    },
    {
      "code": "@classmethod\ndef build_kv_connector_stats(\n    cls, data: dict[str, Any] | None = None\n) -> Optional[\"KVConnectorStats\"]:\n    \"\"\"\n    KVConnectorStats resolution method. This method allows dynamically\n    registered connectors to return their own KVConnectorStats object,\n    which can implement custom aggregation logic on the data dict.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "@classmethod\ndef build_kv_connector_stats(\n    cls, data: dict[str, Any] | None = None\n) -> Optional[\"KVConnectorStats\"]:\n    \"\"\"\n    KVConnectorStats resolution method. This method allows dynamically\n    registered connectors to return their own KVConnectorStats object,\n    which can implement custom aggregation logic on the data dict.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "build_prom_metrics(\n    vllm_config: VllmConfig,\n    metric_types: dict[type[PromMetric], type[PromMetricT]],\n    labelnames: list[str],\n    per_engine_labelvalues: dict[int, list[object]],\n) -> Optional[KVConnectorPromMetrics]",
      "language": "php"
    },
    {
      "code": "build_prom_metrics(\n    vllm_config: VllmConfig,\n    metric_types: dict[type[PromMetric], type[PromMetricT]],\n    labelnames: list[str],\n    per_engine_labelvalues: dict[int, list[object]],\n) -> Optional[KVConnectorPromMetrics]",
      "language": "php"
    },
    {
      "code": "570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583",
      "language": "unknown"
    },
    {
      "code": "@classmethod\ndef build_prom_metrics(\n    cls,\n    vllm_config: \"VllmConfig\",\n    metric_types: dict[type[\"PromMetric\"], type[\"PromMetricT\"]],\n    labelnames: list[str],\n    per_engine_labelvalues: dict[int, list[object]],\n) -> Optional[\"KVConnectorPromMetrics\"]:\n    \"\"\"\n    Create a KVConnectorPromMetrics subclass which should register\n    per-connector Prometheus metrics and implement observe() to\n    expose connector transfer stats via Prometheus.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "@classmethod\ndef build_prom_metrics(\n    cls,\n    vllm_config: \"VllmConfig\",\n    metric_types: dict[type[\"PromMetric\"], type[\"PromMetricT\"]],\n    labelnames: list[str],\n    per_engine_labelvalues: dict[int, list[object]],\n) -> Optional[\"KVConnectorPromMetrics\"]:\n    \"\"\"\n    Create a KVConnectorPromMetrics subclass which should register\n    per-connector Prometheus metrics and implement observe() to\n    expose connector transfer stats via Prometheus.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "clear_connector_metadata() -> None",
      "language": "rust"
    },
    {
      "code": "clear_connector_metadata() -> None",
      "language": "rust"
    },
    {
      "code": "203\n204\n205\n206\n207\n208\n209",
      "language": "unknown"
    },
    {
      "code": "def clear_connector_metadata(self) -> None:\n    \"\"\"Clear the connector metadata.\n\n    This function should be called by the model runner every time\n    after the model execution.\n    \"\"\"\n    self._connector_metadata = None",
      "language": "python"
    },
    {
      "code": "def clear_connector_metadata(self) -> None:\n    \"\"\"Clear the connector metadata.\n\n    This function should be called by the model runner every time\n    after the model execution.\n    \"\"\"\n    self._connector_metadata = None",
      "language": "python"
    },
    {
      "code": "get_block_ids_with_load_errors() -> set[int]",
      "language": "php"
    },
    {
      "code": "get_block_ids_with_load_errors() -> set[int]",
      "language": "php"
    },
    {
      "code": "348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366",
      "language": "unknown"
    },
    {
      "code": "def get_block_ids_with_load_errors(self) -> set[int]:\n    \"\"\"\n    Get the set of block IDs that failed to load.\n\n    Returns:\n        Set of block IDs that encountered load errors.\n        Empty set if no load errors occurred.\n\n    Notes:\n        - Applies to both sync- and async-loading requests.\n        - Async loading: failed blocks may be reported in any forward pass\n          up to and including the pass where the request ID is returned by\n          `get_finished()`. Even if failures occur, the request must still\n          be reported via `get_finished()`, and the failed block IDs must\n          appear here no later than that same pass.\n        - Sync loading: failed blocks should be reported in the forward\n          pass in which they are detected.\n    \"\"\"\n    return set()",
      "language": "python"
    },
    {
      "code": "def get_block_ids_with_load_errors(self) -> set[int]:\n    \"\"\"\n    Get the set of block IDs that failed to load.\n\n    Returns:\n        Set of block IDs that encountered load errors.\n        Empty set if no load errors occurred.\n\n    Notes:\n        - Applies to both sync- and async-loading requests.\n        - Async loading: failed blocks may be reported in any forward pass\n          up to and including the pass where the request ID is returned by\n          `get_finished()`. Even if failures occur, the request must still\n          be reported via `get_finished()`, and the failed block IDs must\n          appear here no later than that same pass.\n        - Sync loading: failed blocks should be reported in the forward\n          pass in which they are detected.\n    \"\"\"\n    return set()",
      "language": "python"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str],\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str],\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346",
      "language": "unknown"
    },
    {
      "code": "def get_finished(\n    self, finished_req_ids: set[str]\n) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"\n    Notifies worker-side connector ids of requests that have\n    finished generating tokens on the worker.\n    The scheduler process (via the Executors) will use this output\n    to track which workers are done.\n\n    Returns:\n        ids of requests that have finished asynchronous transfer\n        (requests that previously returned True from request_finished()),\n        tuple of (sending/saving ids, recving/loading ids).\n        The finished saves/sends req ids must belong to a set provided in a\n        call to this method (this call or a prior one).\n    \"\"\"\n    return None, None",
      "language": "python"
    },
    {
      "code": "def get_finished(\n    self, finished_req_ids: set[str]\n) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"\n    Notifies worker-side connector ids of requests that have\n    finished generating tokens on the worker.\n    The scheduler process (via the Executors) will use this output\n    to track which workers are done.\n\n    Returns:\n        ids of requests that have finished asynchronous transfer\n        (requests that previously returned True from request_finished()),\n        tuple of (sending/saving ids, recving/loading ids).\n        The finished saves/sends req ids must belong to a set provided in a\n        call to this method (this call or a prior one).\n    \"\"\"\n    return None, None",
      "language": "python"
    },
    {
      "code": "get_finished_count() -> int | None",
      "language": "rust"
    },
    {
      "code": "get_finished_count() -> int | None",
      "language": "rust"
    },
    {
      "code": "536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546",
      "language": "unknown"
    },
    {
      "code": "def get_finished_count(self) -> int | None:\n    \"\"\"\n    Get the count of requests expected to complete send/receive operations\n    via this connector. This method is used to initialize the\n    KVOutputAggregator, overwriting the default world_size.\n\n    Returns:\n        int: expected sending or receiving completion count.\n    \"\"\"\n\n    return None",
      "language": "python"
    },
    {
      "code": "def get_finished_count(self) -> int | None:\n    \"\"\"\n    Get the count of requests expected to complete send/receive operations\n    via this connector. This method is used to initialize the\n    KVOutputAggregator, overwriting the default world_size.\n\n    Returns:\n        int: expected sending or receiving completion count.\n    \"\"\"\n\n    return None",
      "language": "python"
    },
    {
      "code": "get_handshake_metadata() -> (\n    KVConnectorHandshakeMetadata | None\n)",
      "language": "rust"
    },
    {
      "code": "get_handshake_metadata() -> (\n    KVConnectorHandshakeMetadata | None\n)",
      "language": "rust"
    },
    {
      "code": "390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400",
      "language": "unknown"
    },
    {
      "code": "def get_handshake_metadata(self) -> KVConnectorHandshakeMetadata | None:\n    \"\"\"\n    Get the KVConnector handshake metadata for this connector.\n    This metadata is used for out-of-band connector handshake\n    between P/D workers.\n\n    Returns:\n        KVConnectorHandshakeMetadata: the handshake metadata.\n        None if no handshake metadata is available.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "def get_handshake_metadata(self) -> KVConnectorHandshakeMetadata | None:\n    \"\"\"\n    Get the KVConnector handshake metadata for this connector.\n    This metadata is used for out-of-band connector handshake\n    between P/D workers.\n\n    Returns:\n        KVConnectorHandshakeMetadata: the handshake metadata.\n        None if no handshake metadata is available.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "get_kv_connector_kv_cache_events() -> Optional[\n    KVConnectorKVEvents\n]",
      "language": "php"
    },
    {
      "code": "get_kv_connector_kv_cache_events() -> Optional[\n    KVConnectorKVEvents\n]",
      "language": "php"
    },
    {
      "code": "382\n383\n384\n385\n386\n387\n388",
      "language": "unknown"
    },
    {
      "code": "def get_kv_connector_kv_cache_events(self) -> Optional[\"KVConnectorKVEvents\"]:\n    \"\"\"\n    Get the KV connector kv cache events collected during the last interval.\n    This function should be called by the model runner every time after the\n    model execution and before cleanup.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "def get_kv_connector_kv_cache_events(self) -> Optional[\"KVConnectorKVEvents\"]:\n    \"\"\"\n    Get the KV connector kv cache events collected during the last interval.\n    This function should be called by the model runner every time after the\n    model execution and before cleanup.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "get_kv_connector_stats() -> Optional[KVConnectorStats]",
      "language": "php"
    },
    {
      "code": "get_kv_connector_stats() -> Optional[KVConnectorStats]",
      "language": "php"
    },
    {
      "code": "376\n377\n378\n379\n380",
      "language": "unknown"
    },
    {
      "code": "def get_kv_connector_stats(self) -> Optional[\"KVConnectorStats\"]:\n    \"\"\"\n    Get the KV connector stats collected during the last interval.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "def get_kv_connector_stats(self) -> Optional[\"KVConnectorStats\"]:\n    \"\"\"\n    Get the KV connector stats collected during the last interval.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int | None, bool]",
      "language": "rust"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int | None, bool]",
      "language": "rust"
    },
    {
      "code": "406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef get_num_new_matched_tokens(\n    self,\n    request: \"Request\",\n    num_computed_tokens: int,\n) -> tuple[int | None, bool]:\n    \"\"\"\n    Get number of new tokens that can be loaded from the\n    external KV cache beyond the num_computed_tokens.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n\n    Returns:\n        A tuple with the following elements:\n            - An optional number of tokens that can be loaded from the\n              external KV cache beyond what is already computed.\n              If None, it means that the connector needs more time to\n              determine the number of matched tokens, and the scheduler\n              should query for this request again later.\n            - `True` if external KV cache tokens will be loaded\n              asynchronously (between scheduler steps). Must be\n              'False' if the first element is 0.\n\n    Notes:\n        The connector should only consider the largest prefix of prompt-\n        tokens for which KV cache is actually available at the time of the\n        call. If the cache cannot be loaded for some tokens (e.g., due to\n        connectivity issues or eviction), those tokens must not be taken\n        into account.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef get_num_new_matched_tokens(\n    self,\n    request: \"Request\",\n    num_computed_tokens: int,\n) -> tuple[int | None, bool]:\n    \"\"\"\n    Get number of new tokens that can be loaded from the\n    external KV cache beyond the num_computed_tokens.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n\n    Returns:\n        A tuple with the following elements:\n            - An optional number of tokens that can be loaded from the\n              external KV cache beyond what is already computed.\n              If None, it means that the connector needs more time to\n              determine the number of matched tokens, and the scheduler\n              should query for this request again later.\n            - `True` if external KV cache tokens will be loaded\n              asynchronously (between scheduler steps). Must be\n              'False' if the first element is 0.\n\n    Notes:\n        The connector should only consider the largest prefix of prompt-\n        tokens for which KV cache is actually available at the time of the\n        call. If the cache cannot be loaded for some tokens (e.g., due to\n        connectivity issues or eviction), those tokens must not be taken\n        into account.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "get_required_kvcache_layout(\n    vllm_config: VllmConfig,\n) -> str | None",
      "language": "rust"
    },
    {
      "code": "get_required_kvcache_layout(\n    vllm_config: VllmConfig,\n) -> str | None",
      "language": "rust"
    },
    {
      "code": "517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534",
      "language": "unknown"
    },
    {
      "code": "@classmethod\ndef get_required_kvcache_layout(cls, vllm_config: \"VllmConfig\") -> str | None:\n    \"\"\"\n    Get the required KV cache layout for this connector.\n    Args:\n        vllm_config (VllmConfig): the vllm config.\n\n    Returns:\n        str: the required KV cache layout. e.g. HND, or NHD.\n        None if the connector does not require a specific layout.\n    \"\"\"\n\n    if cls is KVConnectorBase_V1:\n        raise TypeError(\n            \"get_required_kvcache_layout should not be called \"\n            \"on the abstract base class\"\n        )\n    return None",
      "language": "python"
    },
    {
      "code": "@classmethod\ndef get_required_kvcache_layout(cls, vllm_config: \"VllmConfig\") -> str | None:\n    \"\"\"\n    Get the required KV cache layout for this connector.\n    Args:\n        vllm_config (VllmConfig): the vllm config.\n\n    Returns:\n        str: the required KV cache layout. e.g. HND, or NHD.\n        None if the connector does not require a specific layout.\n    \"\"\"\n\n    if cls is KVConnectorBase_V1:\n        raise TypeError(\n            \"get_required_kvcache_layout should not be called \"\n            \"on the abstract base class\"\n        )\n    return None",
      "language": "python"
    },
    {
      "code": "has_connector_metadata() -> bool",
      "language": "php"
    },
    {
      "code": "has_connector_metadata() -> bool",
      "language": "php"
    },
    {
      "code": "223\n224\n225\n226\n227\n228\n229",
      "language": "unknown"
    },
    {
      "code": "def has_connector_metadata(self) -> bool:\n    \"\"\"Check whether the connector metadata is currently set.\n\n    Returns:\n        bool: True if connector metadata exists, False otherwise.\n    \"\"\"\n    return self._connector_metadata is not None",
      "language": "python"
    },
    {
      "code": "def has_connector_metadata(self) -> bool:\n    \"\"\"Check whether the connector metadata is currently set.\n\n    Returns:\n        bool: True if connector metadata exists, False otherwise.\n    \"\"\"\n    return self._connector_metadata is not None",
      "language": "python"
    },
    {
      "code": "register_cross_layers_kv_cache(\n    kv_cache: Tensor, attn_backend: type[AttentionBackend]\n)",
      "language": "yaml"
    },
    {
      "code": "register_cross_layers_kv_cache(\n    kv_cache: Tensor, attn_backend: type[AttentionBackend]\n)",
      "language": "yaml"
    },
    {
      "code": "241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256",
      "language": "unknown"
    },
    {
      "code": "def register_cross_layers_kv_cache(\n    self, kv_cache: torch.Tensor, attn_backend: type[\"AttentionBackend\"]\n):\n    \"\"\"\n    Initialize with a single KV cache tensor used by all layers.\n    The first dimension should be num_layers.\n    This function will only be called for models with uniform layers,\n    and only if the prefers_cross_layer_blocks is set to True.\n    Only one of the functions\n    {register_kv_caches, register_cross_layers_kv_cache} will be called.\n\n    Args:\n        kv_cache: a cross-layers kv cache tensor\n        attn_backend: The attention backend that corresponds to all layers\n    \"\"\"\n    return",
      "language": "json"
    },
    {
      "code": "def register_cross_layers_kv_cache(\n    self, kv_cache: torch.Tensor, attn_backend: type[\"AttentionBackend\"]\n):\n    \"\"\"\n    Initialize with a single KV cache tensor used by all layers.\n    The first dimension should be num_layers.\n    This function will only be called for models with uniform layers,\n    and only if the prefers_cross_layer_blocks is set to True.\n    Only one of the functions\n    {register_kv_caches, register_cross_layers_kv_cache} will be called.\n\n    Args:\n        kv_cache: a cross-layers kv cache tensor\n        attn_backend: The attention backend that corresponds to all layers\n    \"\"\"\n    return",
      "language": "json"
    },
    {
      "code": "register_kv_caches(kv_caches: dict[str, Tensor])",
      "language": "unknown"
    },
    {
      "code": "register_kv_caches(kv_caches: dict[str, Tensor])",
      "language": "unknown"
    },
    {
      "code": "231\n232\n233\n234\n235\n236\n237\n238\n239",
      "language": "unknown"
    },
    {
      "code": "def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n    \"\"\"\n    Initialize with the KV caches. Useful for pre-registering the\n    KV Caches in the KVConnector (e.g. for NIXL).\n\n    Args:\n        kv_caches: dictionary of layer names, kv cache\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "def register_kv_caches(self, kv_caches: dict[str, torch.Tensor]):\n    \"\"\"\n    Initialize with the KV caches. Useful for pre-registering the\n    KV Caches in the KVConnector (e.g. for NIXL).\n\n    Args:\n        kv_caches: dictionary of layer names, kv cache\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506",
      "language": "unknown"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Called exactly once when a request has finished, before its blocks are\n    freed.\n\n    The connector may assumes responsibility for freeing the blocks\n    asynchronously by returning True.\n\n    Returns:\n        True if the request is being saved/sent asynchronously and blocks\n        should not be freed until the request_id is returned from\n        get_finished().\n        Optional KVTransferParams to be included in the request outputs\n        returned by the engine.\n    \"\"\"\n    return False, None",
      "language": "python"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Called exactly once when a request has finished, before its blocks are\n    freed.\n\n    The connector may assumes responsibility for freeing the blocks\n    asynchronously by returning True.\n\n    Returns:\n        True if the request is being saved/sent asynchronously and blocks\n        should not be freed until the request_id is returned from\n        get_finished().\n        Optional KVTransferParams to be included in the request outputs\n        returned by the engine.\n    \"\"\"\n    return False, None",
      "language": "python"
    },
    {
      "code": "reset_cache() -> bool | None",
      "language": "rust"
    },
    {
      "code": "reset_cache() -> bool | None",
      "language": "rust"
    },
    {
      "code": "585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597",
      "language": "unknown"
    },
    {
      "code": "def reset_cache(self) -> bool | None:\n    \"\"\"\n    Reset the connector's internal cache.\n\n    Returns:\n        bool: True if the cache was successfully reset, False otherwise.\n    \"\"\"\n    logger.debug(\n        \"Connector cache reset requested, but %s does not implement reset_cache().\",\n        type(self).__name__,\n    )\n\n    return None",
      "language": "python"
    },
    {
      "code": "def reset_cache(self) -> bool | None:\n    \"\"\"\n    Reset the connector's internal cache.\n\n    Returns:\n        bool: True if the cache was successfully reset, False otherwise.\n    \"\"\"\n    logger.debug(\n        \"Connector cache reset requested, but %s does not implement reset_cache().\",\n        type(self).__name__,\n    )\n\n    return None",
      "language": "python"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None",
      "language": "rust"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None",
      "language": "rust"
    },
    {
      "code": "297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: \"AttentionMetadata\",\n    **kwargs: Any,\n) -> None:\n    \"\"\"\n    Start saving a layer of KV cache from vLLM's paged buffer\n    to the connector. This is called from within attention layer to\n    enable async copying during execution.\n\n    Args:\n        layer_name (str): the name of the layer.\n        kv_layer (torch.Tensor): the paged KV buffer of the current\n            layer in vLLM.\n        attn_metadata (AttentionMetadata): the attention metadata.\n        **kwargs: additional arguments for the save operation.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: \"AttentionMetadata\",\n    **kwargs: Any,\n) -> None:\n    \"\"\"\n    Start saving a layer of KV cache from vLLM's paged buffer\n    to the connector. This is called from within attention layer to\n    enable async copying during execution.\n\n    Args:\n        layer_name (str): the name of the layer.\n        kv_layer (torch.Tensor): the paged KV buffer of the current\n            layer in vLLM.\n        attn_metadata (AttentionMetadata): the attention metadata.\n        **kwargs: additional arguments for the save operation.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "set_host_xfer_buffer_ops(copy_operation: CopyBlocksOp)",
      "language": "unknown"
    },
    {
      "code": "set_host_xfer_buffer_ops(copy_operation: CopyBlocksOp)",
      "language": "unknown"
    },
    {
      "code": "258\n259\n260\n261\n262\n263",
      "language": "unknown"
    },
    {
      "code": "def set_host_xfer_buffer_ops(self, copy_operation: CopyBlocksOp):\n    \"\"\"\n    Set the xPU-specific ops for copying KV between host and device.\n    Needed when host buffer is used for kv transfer (e.g., in NixlConnector)\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "def set_host_xfer_buffer_ops(self, copy_operation: CopyBlocksOp):\n    \"\"\"\n    Set the xPU-specific ops for copying KV between host and device.\n    Needed when host buffer is used for kv transfer (e.g., in NixlConnector)\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "set_xfer_handshake_metadata(\n    metadata: dict[int, KVConnectorHandshakeMetadata],\n) -> None",
      "language": "rust"
    },
    {
      "code": "set_xfer_handshake_metadata(\n    metadata: dict[int, KVConnectorHandshakeMetadata],\n) -> None",
      "language": "rust"
    },
    {
      "code": "559\n560\n561\n562\n563\n564\n565\n566\n567\n568",
      "language": "unknown"
    },
    {
      "code": "def set_xfer_handshake_metadata(\n    self, metadata: dict[int, KVConnectorHandshakeMetadata]\n) -> None:\n    \"\"\"\n    Set the KV connector handshake metadata for this connector.\n\n    Args:\n        metadata (KVConnectorHandshakeMetadata): the handshake metadata to set.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "def set_xfer_handshake_metadata(\n    self, metadata: dict[int, KVConnectorHandshakeMetadata]\n) -> None:\n    \"\"\"\n    Set the KV connector handshake metadata for this connector.\n\n    Args:\n        metadata (KVConnectorHandshakeMetadata): the handshake metadata to set.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "368\n369\n370\n371\n372\n373\n374",
      "language": "unknown"
    },
    {
      "code": "def shutdown(self):\n    \"\"\"\n    Shutdown the connector. This is called when the worker process\n    is shutting down to ensure that all the async operations are\n    completed and the connector is cleaned up properly.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "def shutdown(self):\n    \"\"\"\n    Shutdown the connector. This is called when the worker process\n    is shutting down to ensure that all the async operations are\n    completed and the connector is cleaned up properly.\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs: Any\n) -> None",
      "language": "rust"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs: Any\n) -> None",
      "language": "rust"
    },
    {
      "code": "265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n    \"\"\"\n    Start loading the KV cache from the connector to vLLM's paged\n    KV buffer. This is called from the forward context before the\n    forward pass to enable async loading during model execution.\n\n    Args:\n        forward_context (ForwardContext): the forward context.\n        **kwargs: additional arguments for the load operation\n\n    Note:\n        The number of elements in kv_caches and layer_names should be\n        the same.\n\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n    \"\"\"\n    Start loading the KV cache from the connector to vLLM's paged\n    KV buffer. This is called from the forward context before the\n    forward pass to enable async loading during model execution.\n\n    Args:\n        forward_context (ForwardContext): the forward context.\n        **kwargs: additional arguments for the load operation\n\n    Note:\n        The number of elements in kv_caches and layer_names should be\n        the same.\n\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "take_events() -> Iterable[KVCacheEvent]",
      "language": "php"
    },
    {
      "code": "take_events() -> Iterable[KVCacheEvent]",
      "language": "php"
    },
    {
      "code": "508\n509\n510\n511\n512\n513\n514\n515",
      "language": "unknown"
    },
    {
      "code": "def take_events(self) -> Iterable[\"KVCacheEvent\"]:\n    \"\"\"\n    Take the KV cache events from the connector.\n\n    Yields:\n        New KV cache events since the last call.\n    \"\"\"\n    return ()",
      "language": "python"
    },
    {
      "code": "def take_events(self) -> Iterable[\"KVCacheEvent\"]:\n    \"\"\"\n    Take the KV cache events from the connector.\n\n    Yields:\n        New KV cache events since the last call.\n    \"\"\"\n    return ()",
      "language": "python"
    },
    {
      "code": "update_connector_output(\n    connector_output: KVConnectorOutput,\n)",
      "language": "yaml"
    },
    {
      "code": "update_connector_output(\n    connector_output: KVConnectorOutput,\n)",
      "language": "yaml"
    },
    {
      "code": "477\n478\n479\n480\n481\n482\n483\n484\n485",
      "language": "unknown"
    },
    {
      "code": "def update_connector_output(self, connector_output: KVConnectorOutput):\n    \"\"\"\n    Update KVConnector state from worker-side connectors output.\n\n    Args:\n        connector_output (KVConnectorOutput): the worker-side\n            connectors output.\n    \"\"\"\n    return",
      "language": "sql"
    },
    {
      "code": "def update_connector_output(self, connector_output: KVConnectorOutput):\n    \"\"\"\n    Update KVConnector state from worker-side connectors output.\n\n    Args:\n        connector_output (KVConnectorOutput): the worker-side\n            connectors output.\n    \"\"\"\n    return",
      "language": "sql"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    \"\"\"\n    Update KVConnector state after block allocation.\n\n    If get_num_new_matched_tokens previously returned True for a\n    request, this function may be called twice for that same request -\n    first when blocks are allocated for the connector tokens to be\n    asynchronously loaded into, and second when any additional blocks\n    are allocated, after the load/transfer is complete.\n\n    Args:\n        request (Request): the request object.\n        blocks (KVCacheBlocks): the blocks allocated for the request.\n        num_external_tokens (int): the number of tokens that will be\n            loaded from the external KV cache.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    \"\"\"\n    Update KVConnector state after block allocation.\n\n    If get_num_new_matched_tokens previously returned True for a\n    request, this function may be called twice for that same request -\n    first when blocks are allocated for the connector tokens to be\n    asynchronously loaded into, and second when any additional blocks\n    are allocated, after the load/transfer is complete.\n\n    Args:\n        request (Request): the request object.\n        blocks (KVCacheBlocks): the blocks allocated for the request.\n        num_external_tokens (int): the number of tokens that will be\n            loaded from the external KV cache.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"\n    Block until the KV for a specific layer is loaded into vLLM's\n    paged buffer. This is called from within attention layer to ensure\n    async copying from start_load_kv is complete.\n\n    This interface will be useful for layer-by-layer pipelining.\n\n    Args:\n        layer_name: the name of that layer\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"\n    Block until the KV for a specific layer is loaded into vLLM's\n    paged buffer. This is called from within attention layer to ensure\n    async copying from start_load_kv is complete.\n\n    This interface will be useful for layer-by-layer pipelining.\n\n    Args:\n        layer_name: the name of that layer\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "319\n320\n321\n322\n323\n324\n325\n326\n327\n328",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef wait_for_save(self):\n    \"\"\"\n    Block until all the save operations is done. This is called\n    as the forward context exits to ensure that the async saving\n    from save_kv_layer is complete before finishing the forward.\n\n    This prevents overwrites of paged KV buffer before saving done.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef wait_for_save(self):\n    \"\"\"\n    Block until all the save operations is done. This is called\n    as the forward context exits to ensure that the async saving\n    from save_kv_layer is complete before finishing the forward.\n\n    This prevents overwrites of paged KV buffer before saving done.\n    \"\"\"\n    pass",
      "language": "python"
    },
    {
      "code": "126\n127\n128\n129\n130\n131\n132",
      "language": "unknown"
    },
    {
      "code": "class KVConnectorHandshakeMetadata(ABC):  # noqa: B024\n    \"\"\"\n    Metadata used for out of band connector handshake between\n    P/D workers. This needs to serializeable.\n    \"\"\"\n\n    pass",
      "language": "php"
    },
    {
      "code": "class KVConnectorHandshakeMetadata(ABC):  # noqa: B024\n    \"\"\"\n    Metadata used for out of band connector handshake between\n    P/D workers. This needs to serializeable.\n    \"\"\"\n\n    pass",
      "language": "php"
    },
    {
      "code": "135\n136\n137\n138\n139\n140\n141",
      "language": "unknown"
    },
    {
      "code": "class KVConnectorMetadata(ABC):  # noqa: B024\n    \"\"\"\n    Abstract Metadata used to communicate between the\n    Scheduler KVConnector and Worker KVConnector.\n    \"\"\"\n\n    pass",
      "language": "php"
    },
    {
      "code": "class KVConnectorMetadata(ABC):  # noqa: B024\n    \"\"\"\n    Abstract Metadata used to communicate between the\n    Scheduler KVConnector and Worker KVConnector.\n    \"\"\"\n\n    pass",
      "language": "php"
    },
    {
      "code": "118\n119\n120\n121\n122\n123",
      "language": "unknown"
    },
    {
      "code": "class KVConnectorRole(enum.Enum):\n    # Connector running in the scheduler process\n    SCHEDULER = 0\n\n    # Connector running in the worker process\n    WORKER = 1",
      "language": "php"
    },
    {
      "code": "class KVConnectorRole(enum.Enum):\n    # Connector running in the scheduler process\n    SCHEDULER = 0\n\n    # Connector running in the worker process\n    WORKER = 1",
      "language": "php"
    },
    {
      "code": "SCHEDULER = 0",
      "language": "unknown"
    },
    {
      "code": "SCHEDULER = 0",
      "language": "unknown"
    },
    {
      "code": "79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108",
      "language": "unknown"
    },
    {
      "code": "class SupportsHMA(ABC):\n    \"\"\"\n    The class that indicates the corresponding connector supports hybrid memory\n    allocator (HMA).\n    This is required to use the connector together with hybrid memory allocator.\n    \"\"\"\n\n    @abstractmethod\n    def request_finished_all_groups(\n        self,\n        request: \"Request\",\n        block_ids: tuple[list[int], ...],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Called exactly once when a request has finished for all kv cache groups,\n        before its blocks are freed for each group.\n\n        NOTE(Kuntai): This function is only supported by connectors that support HMA.\n\n        The connector may assumes responsibility for freeing the blocks\n        asynchronously by returning True.\n\n        Returns:\n            True if the request is being saved/sent asynchronously and blocks\n            should not be freed until the request_id is returned from\n            get_finished().\n            Optional KVTransferParams to be included in the request outputs\n            returned by the engine.\n        \"\"\"\n        raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "class SupportsHMA(ABC):\n    \"\"\"\n    The class that indicates the corresponding connector supports hybrid memory\n    allocator (HMA).\n    This is required to use the connector together with hybrid memory allocator.\n    \"\"\"\n\n    @abstractmethod\n    def request_finished_all_groups(\n        self,\n        request: \"Request\",\n        block_ids: tuple[list[int], ...],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Called exactly once when a request has finished for all kv cache groups,\n        before its blocks are freed for each group.\n\n        NOTE(Kuntai): This function is only supported by connectors that support HMA.\n\n        The connector may assumes responsibility for freeing the blocks\n        asynchronously by returning True.\n\n        Returns:\n            True if the request is being saved/sent asynchronously and blocks\n            should not be freed until the request_id is returned from\n            get_finished().\n            Optional KVTransferParams to be included in the request outputs\n            returned by the engine.\n        \"\"\"\n        raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "request_finished_all_groups(\n    request: Request, block_ids: tuple[list[int], ...]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "request_finished_all_groups(\n    request: Request, block_ids: tuple[list[int], ...]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef request_finished_all_groups(\n    self,\n    request: \"Request\",\n    block_ids: tuple[list[int], ...],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Called exactly once when a request has finished for all kv cache groups,\n    before its blocks are freed for each group.\n\n    NOTE(Kuntai): This function is only supported by connectors that support HMA.\n\n    The connector may assumes responsibility for freeing the blocks\n    asynchronously by returning True.\n\n    Returns:\n        True if the request is being saved/sent asynchronously and blocks\n        should not be freed until the request_id is returned from\n        get_finished().\n        Optional KVTransferParams to be included in the request outputs\n        returned by the engine.\n    \"\"\"\n    raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef request_finished_all_groups(\n    self,\n    request: \"Request\",\n    block_ids: tuple[list[int], ...],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Called exactly once when a request has finished for all kv cache groups,\n    before its blocks are freed for each group.\n\n    NOTE(Kuntai): This function is only supported by connectors that support HMA.\n\n    The connector may assumes responsibility for freeing the blocks\n    asynchronously by returning True.\n\n    Returns:\n        True if the request is being saved/sent asynchronously and blocks\n        should not be freed until the request_id is returned from\n        get_finished().\n        Optional KVTransferParams to be included in the request outputs\n        returned by the engine.\n    \"\"\"\n    raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "supports_hma(connector: Any) -> bool",
      "language": "php"
    },
    {
      "code": "supports_hma(connector: Any) -> bool",
      "language": "php"
    },
    {
      "code": "111\n112\n113\n114\n115",
      "language": "unknown"
    },
    {
      "code": "def supports_hma(connector: Any) -> bool:\n    if isinstance(connector, type):\n        return issubclass(connector, SupportsHMA)\n    else:\n        return isinstance(connector, SupportsHMA)",
      "language": "python"
    },
    {
      "code": "def supports_hma(connector: Any) -> bool:\n    if isinstance(connector, type):\n        return issubclass(connector, SupportsHMA)\n    else:\n        return isinstance(connector, SupportsHMA)",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}