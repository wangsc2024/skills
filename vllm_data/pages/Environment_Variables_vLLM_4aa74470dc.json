{
  "url": "https://docs.vllm.ai/en/latest/configuration/env_vars/",
  "title": "Environment Variables - vLLM",
  "content": "vLLM uses the following environment variables to configure the system:\n\nPlease note that VLLM_PORT and VLLM_HOST_IP set the port and ip for vLLM's internal usage. It is not the port and ip for the API server. If you use --host $VLLM_HOST_IP and --port $VLLM_PORT to start the API server, it will not work.\n\nAll environment variables used by vLLM are prefixed with VLLM_. Special care should be taken for Kubernetes users: please do not name the service as vllm, otherwise environment variables set by Kubernetes might conflict with vLLM's environment variables, because Kubernetes sets environment variables for each service with the capitalized service name as the prefix.",
  "headings": [
    {
      "level": "h1",
      "text": "Environment VariablesÂ¶",
      "id": "environment-variables"
    }
  ],
  "code_samples": [
    {
      "code": "logger = logging.getLogger(__name__)\n\nenvironment_variables: dict[str, Callable[[], Any]] = {\n    # ================== Installation Time Env Vars ==================\n    # Target device of vLLM, supporting [cuda (by default),\n    # rocm, cpu]\n    \"VLLM_TARGET_DEVICE\": lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\").lower(),\n    # Main CUDA version of vLLM. This follows PyTorch but can be overridden.\n    \"VLLM_MAIN_CUDA_VERSION\": lambda: os.getenv(\"VLLM_MAIN_CUDA_VERSION\", \"\").lower()\n    or \"12.9\",\n    # Controls PyTorch float32 matmul precision mode within vLLM workers.\n    # Accepted values:\n    #   - \"ieee\" (default): force full IEEE FP32 matmul precision.\n    #   - \"tf32\": enable TensorFloat32-based fast matmul.\n    \"VLLM_FLOAT32_MATMUL_PRECISION\": env_with_choices(\n        \"VLLM_FLOAT32_MATMUL_PRECISION\",\n        \"ieee\",\n        [\"ieee\", \"tf32\"],\n        case_sensitive=False,\n    ),\n    # Maximum number of compilation jobs to run in parallel.\n    # By default this is the number of CPUs\n    \"MAX_JOBS\": lambda: os.getenv(\"MAX_JOBS\", None),\n    # Number of threads to use for nvcc\n    # By default this is 1.\n    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.\n    \"NVCC_THREADS\": lambda: os.getenv(\"NVCC_THREADS\", None),\n    # If set, vllm will use precompiled binaries (*.so)\n    \"VLLM_USE_PRECOMPILED\": lambda: os.environ.get(\"VLLM_USE_PRECOMPILED\", \"\")\n    .strip()\n    .lower()\n    in (\"1\", \"true\")\n    or bool(os.environ.get(\"VLLM_PRECOMPILED_WHEEL_LOCATION\")),\n    # If set, skip adding +precompiled suffix to version string\n    \"VLLM_SKIP_PRECOMPILED_VERSION_SUFFIX\": lambda: bool(\n        int(os.environ.get(\"VLLM_SKIP_PRECOMPILED_VERSION_SUFFIX\", \"0\"))\n    ),\n    # Used to mark that setup.py is running in a Docker build context,\n    # in order to force the use of precompiled binaries.\n    \"VLLM_DOCKER_BUILD_CONTEXT\": lambda: os.environ.get(\"VLLM_DOCKER_BUILD_CONTEXT\", \"\")\n    .strip()\n    .lower()\n    in (\"1\", \"true\"),\n    # CMake build type\n    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"\n    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"\n    \"CMAKE_BUILD_TYPE\": env_with_choices(\n        \"CMAKE_BUILD_TYPE\", None, [\"Debug\", \"Release\", \"RelWithDebInfo\"]\n    ),\n    # If set, vllm will print verbose logs during installation\n    \"VERBOSE\": lambda: bool(int(os.getenv(\"VERBOSE\", \"0\"))),\n    # Root directory for vLLM configuration files\n    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set\n    # Note that this not only affects how vllm finds its configuration files\n    # during runtime, but also affects how vllm installs its configuration\n    # files during **installation**.\n    \"VLLM_CONFIG_ROOT\": lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CONFIG_ROOT\",\n            os.path.join(get_default_config_root(), \"vllm\"),\n        )\n    ),\n    # ================== Runtime Env Vars ==================\n    # Root directory for vLLM cache files\n    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set\n    \"VLLM_CACHE_ROOT\": lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CACHE_ROOT\",\n            os.path.join(get_default_cache_root(), \"vllm\"),\n        )\n    ),\n    # used in distributed environment to determine the ip address\n    # of the current node, when the node has multiple network interfaces.\n    # If you are using multi-node inference, you should set this differently\n    # on each node.\n    \"VLLM_HOST_IP\": lambda: os.getenv(\"VLLM_HOST_IP\", \"\"),\n    # used in distributed environment to manually set the communication port\n    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the\n    # VLLM_PORT will be used as the first port, and the rest will be generated\n    # by incrementing the VLLM_PORT value.\n    \"VLLM_PORT\": get_vllm_port,\n    # path used for ipc when the frontend api server is running in\n    # multi-processing mode to communicate with the backend engine process.\n    \"VLLM_RPC_BASE_PATH\": lambda: os.getenv(\n        \"VLLM_RPC_BASE_PATH\", tempfile.gettempdir()\n    ),\n    # If true, will load models from ModelScope instead of Hugging Face Hub.\n    # note that the value is true or false, not numbers\n    \"VLLM_USE_MODELSCOPE\": lambda: os.environ.get(\n        \"VLLM_USE_MODELSCOPE\", \"False\"\n    ).lower()\n    == \"true\",\n    # Interval in seconds to log a warning message when the ring buffer is full\n    \"VLLM_RINGBUFFER_WARNING_INTERVAL\": lambda: int(\n        os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")\n    ),\n    # path to cudatoolkit home directory, under which should be bin, include,\n    # and lib directories.\n    \"CUDA_HOME\": lambda: os.environ.get(\"CUDA_HOME\", None),\n    # Path to the NCCL library file. It is needed because nccl>=2.19 brought\n    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234\n    \"VLLM_NCCL_SO_PATH\": lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),\n    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl\n    # library file in the locations specified by `LD_LIBRARY_PATH`\n    \"LD_LIBRARY_PATH\": lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),\n    # flag to control the chunk size (in MB) for sleeping memory allocations under ROCm\n    \"VLLM_ROCM_SLEEP_MEM_CHUNK_SIZE\": lambda: int(\n        os.environ.get(\"VLLM_ROCM_SLEEP_MEM_CHUNK_SIZE\", \"256\")\n    ),\n    # Use separate prefill and decode kernels for V1 attention instead of\n    # the unified triton kernel.\n    \"VLLM_V1_USE_PREFILL_DECODE_ATTENTION\": lambda: (\n        os.getenv(\"VLLM_V1_USE_PREFILL_DECODE_ATTENTION\", \"False\").lower()\n        in (\"true\", \"1\")\n    ),\n    # Force vllm to use a specific flash-attention version (2 or 3), only valid\n    # when using the flash-attention backend.\n    \"VLLM_FLASH_ATTN_VERSION\": lambda: maybe_convert_int(\n        os.environ.get(\"VLLM_FLASH_ATTN_VERSION\", None)\n    ),\n    # Feature flag to enable/disable Inductor standalone compile.\n    # In torch <= 2.7 we ignore this flag; in torch >= 2.9 this is\n    # enabled by default.\n    \"VLLM_USE_STANDALONE_COMPILE\": lambda: os.environ.get(\n        \"VLLM_USE_STANDALONE_COMPILE\", \"1\"\n    )\n    == \"1\",\n    # Debug pattern matching inside custom passes.\n    # Should be set to the fx.Node name (e.g. 'getitem_34' or 'scaled_mm_3').\n    \"VLLM_PATTERN_MATCH_DEBUG\": lambda: os.environ.get(\n        \"VLLM_PATTERN_MATCH_DEBUG\", None\n    ),\n    # Dump fx graphs to the given directory.\n    # It will override CompilationConfig.debug_dump_path if set.\n    \"VLLM_DEBUG_DUMP_PATH\": lambda: os.environ.get(\"VLLM_DEBUG_DUMP_PATH\", None),\n    # Feature flag to enable/disable AOT compilation. This will ensure\n    # compilation is done in warmup phase and the compilation will be\n    # reused in subsequent calls.\n    \"VLLM_USE_AOT_COMPILE\": use_aot_compile,\n    # Feature flag to enable/disable bytecode in\n    # TorchCompileWithNoGuardsWrapper.\n    \"VLLM_USE_BYTECODE_HOOK\": lambda: bool(\n        int(os.environ.get(\"VLLM_USE_BYTECODE_HOOK\", \"1\"))\n    ),\n    # Force vllm to always load AOT compiled models from disk. Failure\n    # to load will result in a hard error when this is enabled.\n    # Will be ignored when VLLM_USE_AOT_COMPILE is disabled.\n    \"VLLM_FORCE_AOT_LOAD\": lambda: os.environ.get(\"VLLM_FORCE_AOT_LOAD\", \"0\") == \"1\",\n    # local rank of the process in the distributed setting, used to determine\n    # the GPU device id\n    \"LOCAL_RANK\": lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),\n    # used to control the visible devices in the distributed setting\n    \"CUDA_VISIBLE_DEVICES\": lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n    # timeout for each iteration in the engine\n    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\": lambda: int(\n        os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")\n    ),\n    # Timeout in seconds for waiting for engine cores to become ready\n    # during startup. Default is 600 seconds (10 minutes).\n    \"VLLM_ENGINE_READY_TIMEOUT_S\": lambda: int(\n        os.environ.get(\"VLLM_ENGINE_READY_TIMEOUT_S\", \"600\")\n    ),\n    # API key for vLLM API server\n    \"VLLM_API_KEY\": lambda: os.environ.get(\"VLLM_API_KEY\", None),\n    # Whether to log responses from API Server for debugging\n    \"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\": lambda: os.environ.get(\n        \"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\", \"False\"\n    ).lower()\n    == \"true\",\n    # S3 access information, used for tensorizer to load model from S3\n    \"S3_ACCESS_KEY_ID\": lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n    \"S3_SECRET_ACCESS_KEY\": lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n    \"S3_ENDPOINT_URL\": lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),\n    # Usage stats collection\n    \"VLLM_USAGE_STATS_SERVER\": lambda: os.environ.get(\n        \"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"\n    ),\n    \"VLLM_NO_USAGE_STATS\": lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",\n    \"VLLM_DISABLE_FLASHINFER_PREFILL\": lambda: os.environ.get(\n        \"VLLM_DISABLE_FLASHINFER_PREFILL\", \"0\"\n    )\n    == \"1\",\n    \"VLLM_DO_NOT_TRACK\": lambda: (\n        os.environ.get(\"VLLM_DO_NOT_TRACK\", None)\n        or os.environ.get(\"DO_NOT_TRACK\", None)\n        or \"0\"\n    )\n    == \"1\",\n    \"VLLM_USAGE_SOURCE\": lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),\n    # Logging configuration\n    # If set to 0, vllm will not configure logging\n    # If set to 1, vllm will configure logging using the default configuration\n    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH\n    \"VLLM_CONFIGURE_LOGGING\": lambda: bool(\n        int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\"))\n    ),\n    \"VLLM_LOGGING_CONFIG_PATH\": lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),\n    # this is used for configuring the default logging level\n    \"VLLM_LOGGING_LEVEL\": lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\").upper(),\n    # this is used for configuring the default logging stream\n    \"VLLM_LOGGING_STREAM\": lambda: os.getenv(\"VLLM_LOGGING_STREAM\", \"ext://sys.stdout\"),\n    # if set, VLLM_LOGGING_PREFIX will be prepended to all log messages\n    \"VLLM_LOGGING_PREFIX\": lambda: os.getenv(\"VLLM_LOGGING_PREFIX\", \"\"),\n    # Controls colored logging output. Options: \"auto\" (default, colors when terminal),\n    # \"1\" (always use colors), \"0\" (never use colors)\n    \"VLLM_LOGGING_COLOR\": lambda: os.getenv(\"VLLM_LOGGING_COLOR\", \"auto\"),\n    # Standard unix flag for disabling ANSI color codes\n    \"NO_COLOR\": lambda: os.getenv(\"NO_COLOR\", \"0\") != \"0\",\n    # If set, vllm will log stats at this interval in seconds\n    # If not set, vllm will log stats every 10 seconds.\n    \"VLLM_LOG_STATS_INTERVAL\": lambda: val\n    if (val := float(os.getenv(\"VLLM_LOG_STATS_INTERVAL\", \"10.\"))) > 0.0\n    else 10.0,\n    # Trace function calls\n    # If set to 1, vllm will trace function calls\n    # Useful for debugging\n    \"VLLM_TRACE_FUNCTION\": lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),\n    # Backend for attention computation\n    # Example options:\n    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n    # - \"FLASH_ATTN\": use FlashAttention\n    # - \"FLASHINFER\": use flashinfer\n    # - \"FLASHMLA\": use FlashMLA\n    # - \"FLASH_ATTN_MLA\": use FlashAttention for MLA\n    # - \"FLASHINFER_MLA\": use FlashInfer for MLA\n    # - \"CUTLASS_MLA\": use CUTLASS for MLA\n    # All possible options loaded dynamically from AttentionBackendEnum\n    \"VLLM_ATTENTION_BACKEND\": env_with_choices(\n        \"VLLM_ATTENTION_BACKEND\",\n        None,\n        lambda: list(\n            __import__(\n                \"vllm.attention.backends.registry\", fromlist=[\"AttentionBackendEnum\"]\n            ).AttentionBackendEnum.__members__.keys()\n        ),\n    ),\n    # If set, vllm will use flashinfer sampler\n    \"VLLM_USE_FLASHINFER_SAMPLER\": lambda: bool(\n        int(os.environ[\"VLLM_USE_FLASHINFER_SAMPLER\"])\n    )\n    if \"VLLM_USE_FLASHINFER_SAMPLER\" in os.environ\n    else None,\n    # Pipeline stage partition strategy\n    \"VLLM_PP_LAYER_PARTITION\": lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),\n    # (CPU backend only) CPU key-value cache space.\n    # default is None and will be set as 4 GB\n    \"VLLM_CPU_KVCACHE_SPACE\": lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\"))\n    if \"VLLM_CPU_KVCACHE_SPACE\" in os.environ\n    else None,\n    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",\n    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.\n    \"VLLM_CPU_OMP_THREADS_BIND\": lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"auto\"),\n    # (CPU backend only) CPU cores not used by OMP threads .\n    # Those CPU cores will not be used by OMP threads of a rank.\n    \"VLLM_CPU_NUM_OF_RESERVED_CPU\": lambda: int(\n        os.getenv(\"VLLM_CPU_NUM_OF_RESERVED_CPU\", \"0\")\n    )\n    if \"VLLM_CPU_NUM_OF_RESERVED_CPU\" in os.environ\n    else None,\n    # (CPU backend only) whether to use SGL kernels, optimized for small batch.\n    \"VLLM_CPU_SGL_KERNEL\": lambda: bool(int(os.getenv(\"VLLM_CPU_SGL_KERNEL\", \"0\"))),\n    # If the env var is set, Ray Compiled Graph uses the specified\n    # channel type to communicate between workers belonging to\n    # different pipeline-parallel stages.\n    # Available options:\n    # - \"auto\": use the default channel type\n    # - \"nccl\": use NCCL for communication\n    # - \"shm\": use shared memory and gRPC for communication\n    \"VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE\": env_with_choices(\n        \"VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE\", \"auto\", [\"auto\", \"nccl\", \"shm\"]\n    ),\n    # If the env var is set, it enables GPU communication overlap\n    # (experimental feature) in Ray's Compiled Graph.\n    \"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\", \"0\"))\n    ),\n    # If the env var is set, it uses a Ray Communicator wrapping\n    # vLLM's pipeline parallelism communicator to interact with Ray's\n    # Compiled Graph. Otherwise, it uses Ray's NCCL communicator.\n    \"VLLM_USE_RAY_WRAPPED_PP_COMM\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_RAY_WRAPPED_PP_COMM\", \"1\"))\n    ),\n    # Use dedicated multiprocess context for workers.\n    # Both spawn and fork work\n    \"VLLM_WORKER_MULTIPROC_METHOD\": env_with_choices(\n        \"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\", [\"spawn\", \"fork\"]\n    ),\n    # Path to the cache for storing downloaded assets\n    \"VLLM_ASSETS_CACHE\": lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_ASSETS_CACHE\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),\n        )\n    ),\n    # If the env var is set, we will clean model file in\n    # this path $VLLM_ASSETS_CACHE/model_streamer/$model_name\n    \"VLLM_ASSETS_CACHE_MODEL_CLEAN\": lambda: bool(\n        int(os.getenv(\"VLLM_ASSETS_CACHE_MODEL_CLEAN\", \"0\"))\n    ),\n    # Timeout for fetching images when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_IMAGE_FETCH_TIMEOUT\": lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),\n    # Timeout for fetching videos when serving multimodal models\n    # Default is 30 seconds\n    \"VLLM_VIDEO_FETCH_TIMEOUT\": lambda: int(\n        os.getenv(\"VLLM_VIDEO_FETCH_TIMEOUT\", \"30\")\n    ),\n    # Timeout for fetching audio when serving multimodal models\n    # Default is 10 seconds\n    \"VLLM_AUDIO_FETCH_TIMEOUT\": lambda: int(\n        os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"10\")\n    ),\n    # Whether to allow HTTP redirects when fetching from media URLs.\n    # Default to True\n    \"VLLM_MEDIA_URL_ALLOW_REDIRECTS\": lambda: bool(\n        int(os.getenv(\"VLLM_MEDIA_URL_ALLOW_REDIRECTS\", \"1\"))\n    ),\n    # Max number of workers for the thread pool handling\n    # media bytes loading. Set to 1 to disable parallel processing.\n    # Default is 8\n    \"VLLM_MEDIA_LOADING_THREAD_COUNT\": lambda: int(\n        os.getenv(\"VLLM_MEDIA_LOADING_THREAD_COUNT\", \"8\")\n    ),\n    # Maximum filesize in MB for a single audio file when processing\n    # speech-to-text requests. Files larger than this will be rejected.\n    # Default is 25 MB\n    \"VLLM_MAX_AUDIO_CLIP_FILESIZE_MB\": lambda: int(\n        os.getenv(\"VLLM_MAX_AUDIO_CLIP_FILESIZE_MB\", \"25\")\n    ),\n    # Backend for Video IO\n    # - \"opencv\": Default backend that uses OpenCV stream buffered backend.\n    #\n    # Custom backend implementations can be registered\n    # via `@VIDEO_LOADER_REGISTRY.register(\"my_custom_video_loader\")` and\n    # imported at runtime.\n    # If a non-existing backend is used, an AssertionError will be thrown.\n    \"VLLM_VIDEO_LOADER_BACKEND\": lambda: os.getenv(\n        \"VLLM_VIDEO_LOADER_BACKEND\", \"opencv\"\n    ),\n    # Media connector implementation.\n    # - \"http\": Default connector that supports fetching media via HTTP.\n    #\n    # Custom implementations can be registered\n    # via `@MEDIA_CONNECTOR_REGISTRY.register(\"my_custom_media_connector\")` and\n    # imported at runtime.\n    # If a non-existing backend is used, an AssertionError will be thrown.\n    \"VLLM_MEDIA_CONNECTOR\": lambda: os.getenv(\"VLLM_MEDIA_CONNECTOR\", \"http\"),\n    # Path to the XLA persistent cache directory.\n    # Only used for XLA devices such as TPUs.\n    \"VLLM_XLA_CACHE_PATH\": lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_XLA_CACHE_PATH\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),\n        )\n    ),\n    # If set, assert on XLA recompilation after each execution step.\n    \"VLLM_XLA_CHECK_RECOMPILATION\": lambda: bool(\n        int(os.getenv(\"VLLM_XLA_CHECK_RECOMPILATION\", \"0\"))\n    ),\n    # Enable SPMD mode for TPU backend.\n    \"VLLM_XLA_USE_SPMD\": lambda: bool(int(os.getenv(\"VLLM_XLA_USE_SPMD\", \"0\"))),\n    \"VLLM_FUSED_MOE_CHUNK_SIZE\": lambda: int(\n        os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", str(16 * 1024))\n    ),\n    # Control whether to use fused MoE activation chunking. Current chunking\n    # logic is incompatible with torch.compile and causes IMA. See issue\n    # https://github.com/vllm-project/vllm/issues/19631.\n    \"VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING\", \"1\"))\n    ),\n    # If set, the OpenAI API server will stay alive even after the underlying\n    # AsyncLLMEngine errors and stops serving requests\n    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\": lambda: bool(\n        int(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", \"0\"))\n    ),\n    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows\n    # the user to specify a max sequence length greater than\n    # the max length derived from the model's config.json.\n    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.\n    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": lambda: (\n        os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower()\n        in (\"1\", \"true\")\n    ),\n    # If set, forces FP8 Marlin to be used for FP8 quantization regardless\n    # of the hardware support for FP8 compute.\n    \"VLLM_TEST_FORCE_FP8_MARLIN\": lambda: (\n        os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower()\n        in (\"1\", \"true\")\n    ),\n    \"VLLM_TEST_FORCE_LOAD_FORMAT\": lambda: os.getenv(\n        \"VLLM_TEST_FORCE_LOAD_FORMAT\", \"dummy\"\n    ),\n    # Time in ms for the zmq client to wait for a response from the backend\n    # server for simple data operations\n    \"VLLM_RPC_TIMEOUT\": lambda: int(os.getenv(\"VLLM_RPC_TIMEOUT\", \"10000\")),\n    # Timeout in seconds for keeping HTTP connections alive in API server\n    \"VLLM_HTTP_TIMEOUT_KEEP_ALIVE\": lambda: int(\n        os.environ.get(\"VLLM_HTTP_TIMEOUT_KEEP_ALIVE\", \"5\")\n    ),\n    # a list of plugin names to load, separated by commas.\n    # if this is not set, it means all plugins will be loaded\n    # if this is set to an empty string, no plugins will be loaded\n    \"VLLM_PLUGINS\": lambda: None\n    if \"VLLM_PLUGINS\" not in os.environ\n    else os.environ[\"VLLM_PLUGINS\"].split(\",\"),\n    # a local directory to look in for unrecognized LoRA adapters.\n    # only works if plugins are enabled and\n    # VLLM_ALLOW_RUNTIME_LORA_UPDATING is enabled.\n    \"VLLM_LORA_RESOLVER_CACHE_DIR\": lambda: os.getenv(\n        \"VLLM_LORA_RESOLVER_CACHE_DIR\", None\n    ),\n    # Enables torch CUDA profiling if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_CUDA_PROFILE\": lambda: os.getenv(\"VLLM_TORCH_CUDA_PROFILE\"),\n    # Enables torch profiler if set.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_DIR\": lambda: os.getenv(\"VLLM_TORCH_PROFILER_DIR\"),\n    # Enable torch profiler to record shapes if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_RECORD_SHAPES\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_RECORD_SHAPES\")\n    ),\n    # Enable torch profiler to profile memory if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY\")\n    ),\n    # Enable torch profiler to profile stack if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_WITH_STACK\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_WITH_STACK\")\n    ),\n    # Enable torch profiler to profile flops if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_WITH_FLOPS\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_WITH_FLOPS\")\n    ),\n    # Disable torch profiling of the AsyncLLMEngine process if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_DISABLE_ASYNC_LLM\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_DISABLE_ASYNC_LLM\")\n    ),\n    # Delay number of iterations before starting profiling when using\n    # the torch/torch CUDA profiler. If set to 0, will start profiling immediately.\n    # Deprecated, see profiler_config.\n    \"VLLM_PROFILER_DELAY_ITERS\": lambda: (os.getenv(\"VLLM_PROFILER_DELAY_ITERS\")),\n    # Maximum number of iterations to profile when using the torch/torch CUDA profiler.\n    # If set to 0, will not limit the number of iterations.\n    \"VLLM_PROFILER_MAX_ITERS\": lambda: os.getenv(\"VLLM_PROFILER_MAX_ITERS\"),\n    # Control whether torch profiler gzip-compresses profiling files.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_USE_GZIP\": lambda: os.getenv(\"VLLM_TORCH_PROFILER_USE_GZIP\"),\n    # Control whether torch profiler dumps the self_cuda_time_total table.\n    # Set to 0 to disable dumping the table.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_DUMP_CUDA_TIME_TOTAL\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_DUMP_CUDA_TIME_TOTAL\")\n    ),\n    # If set, vLLM will use Triton implementations of AWQ.\n    \"VLLM_USE_TRITON_AWQ\": lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),\n    # If set, allow loading or unloading lora adapters in runtime,\n    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\": lambda: (\n        os.environ.get(\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\", \"0\").strip().lower()\n        in (\"1\", \"true\")\n    ),\n    # We assume drivers can report p2p status correctly.\n    # If the program hangs when using custom allreduce,\n    # potantially caused by a bug in the driver (535 series),\n    # if might be helpful to set VLLM_SKIP_P2P_CHECK=0\n    # so that vLLM can verify if p2p is actually working.\n    # See https://github.com/vllm-project/vllm/blob/a9b15c606fea67a072416ea0ea115261a2756058/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L101-L108 for details. # noqa\n    \"VLLM_SKIP_P2P_CHECK\": lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"1\") == \"1\",\n    # List of quantization kernels that should be disabled, used for testing\n    # and performance comparisons. Currently only affects MPLinearKernel\n    # selection\n    # (kernels: MacheteLinearKernel, MarlinLinearKernel, ExllamaLinearKernel)\n    \"VLLM_DISABLED_KERNELS\": lambda: []\n    if \"VLLM_DISABLED_KERNELS\" not in os.environ\n    else os.environ[\"VLLM_DISABLED_KERNELS\"].split(\",\"),\n    # Disable pynccl (using torch.distributed instead)\n    \"VLLM_DISABLE_PYNCCL\": lambda: (\n        os.getenv(\"VLLM_DISABLE_PYNCCL\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # Disable aiter ops unless specifically enabled.\n    # Acts as a parent switch to enable the rest of the other operations.\n    \"VLLM_ROCM_USE_AITER\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter paged attention.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_PAGED_ATTN\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_PAGED_ATTN\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # use aiter linear op if aiter ops are enabled\n    # The following list of related ops\n    # - scaled_mm (per-tensor / rowwise)\n    \"VLLM_ROCM_USE_AITER_LINEAR\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_LINEAR\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter moe ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_MOE\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_MOE\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # use aiter rms norm op if aiter ops are enabled.\n    \"VLLM_ROCM_USE_AITER_RMSNORM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_RMSNORM\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter mla ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_MLA\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_MLA\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter mha ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_MHA\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_MHA\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter fp4 gemm asm.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_FP4_ASM_GEMM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_FP4_ASM_GEMM\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter rope.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_TRITON_ROPE\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_TRITON_ROPE\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter triton fp8 bmm kernel\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_FP8BMM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_FP8BMM\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Use AITER triton unified attention for V1 attention\n    \"VLLM_ROCM_USE_AITER_UNIFIED_ATTENTION\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_UNIFIED_ATTENTION\", \"False\").lower()\n        in (\"true\", \"1\")\n    ),\n    # Whether to use aiter fusion shared experts ops.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS\", \"False\").lower()\n        in (\"true\", \"1\")\n    ),\n    # Whether to use aiter triton kernels for gemm ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_TRITON_GEMM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_TRITON_GEMM\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # use rocm skinny gemms\n    \"VLLM_ROCM_USE_SKINNY_GEMM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_SKINNY_GEMM\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Pad the fp8 weights to 256 bytes for ROCm\n    \"VLLM_ROCM_FP8_PADDING\": lambda: bool(int(os.getenv(\"VLLM_ROCM_FP8_PADDING\", \"1\"))),\n    # Pad the weights for the moe kernel\n    \"VLLM_ROCM_MOE_PADDING\": lambda: bool(int(os.getenv(\"VLLM_ROCM_MOE_PADDING\", \"1\"))),\n    # custom paged attention kernel for MI3* cards\n    \"VLLM_ROCM_CUSTOM_PAGED_ATTN\": lambda: (\n        os.getenv(\"VLLM_ROCM_CUSTOM_PAGED_ATTN\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Custom quick allreduce kernel for MI3* cards\n    # Choice of quantization level: FP, INT8, INT6, INT4 or NONE\n    # Recommended for large models to get allreduce\n    \"VLLM_ROCM_QUICK_REDUCE_QUANTIZATION\": env_with_choices(\n        \"VLLM_ROCM_QUICK_REDUCE_QUANTIZATION\",\n        \"NONE\",\n        [\"FP\", \"INT8\", \"INT6\", \"INT4\", \"NONE\"],\n    ),\n    # Custom quick allreduce kernel for MI3* cards\n    # Due to the lack of the bfloat16 asm instruction, bfloat16\n    # kernels are slower than fp16,\n    # If environment variable is set to 1, the input is converted to fp16\n    \"VLLM_ROCM_QUICK_REDUCE_CAST_BF16_TO_FP16\": lambda: (\n        os.getenv(\"VLLM_ROCM_QUICK_REDUCE_CAST_BF16_TO_FP16\", \"True\").lower()\n        in (\"true\", \"1\")\n    ),\n    # Custom quick allreduce kernel for MI3* cards.\n    # Controls the maximum allowed number of data bytes(MB) for custom quick\n    # allreduce communication.\n    # Default: 2048 MB.\n    # Data exceeding this size will use either custom allreduce or RCCL\n    # communication.\n    \"VLLM_ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB\": lambda: maybe_convert_int(\n        os.environ.get(\"VLLM_ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB\", None)\n    ),\n    # Divisor for dynamic query scale factor calculation for FP8 KV Cache\n    \"Q_SCALE_CONSTANT\": lambda: int(os.getenv(\"Q_SCALE_CONSTANT\", \"200\")),\n    # Divisor for dynamic key scale factor calculation for FP8 KV Cache\n    \"K_SCALE_CONSTANT\": lambda: int(os.getenv(\"K_SCALE_CONSTANT\", \"200\")),\n    # Divisor for dynamic value scale factor calculation for FP8 KV Cache\n    \"V_SCALE_CONSTANT\": lambda: int(os.getenv(\"V_SCALE_CONSTANT\", \"100\")),\n    # If set, enable multiprocessing in LLM for the V1 code path.\n    \"VLLM_ENABLE_V1_MULTIPROCESSING\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_V1_MULTIPROCESSING\", \"1\"))\n    ),\n    \"VLLM_LOG_BATCHSIZE_INTERVAL\": lambda: float(\n        os.getenv(\"VLLM_LOG_BATCHSIZE_INTERVAL\", \"-1\")\n    ),\n    \"VLLM_DISABLE_COMPILE_CACHE\": disable_compile_cache,\n    # If set, vllm will run in development mode, which will enable\n    # some additional endpoints for developing and debugging,\n    # e.g. `/reset_prefix_cache`\n    \"VLLM_SERVER_DEV_MODE\": lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),\n    # Controls the maximum number of requests to handle in a\n    # single asyncio task when processing per-token outputs in the\n    # V1 AsyncLLM interface. It is applicable when handling a high\n    # concurrency of streaming requests.\n    # Setting this too high can result in a higher variance of\n    # inter-message latencies. Setting it too low can negatively impact\n    # TTFT and overall throughput.\n    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\": lambda: int(\n        os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")\n    ),\n    # If set, vLLM will disable the MLA attention optimizations.\n    \"VLLM_MLA_DISABLE\": lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE\", \"0\"))),\n    # If set, vLLM will pick up the provided Flash Attention MLA\n    # max number splits for cuda graph decode\n    \"VLLM_FLASH_ATTN_MAX_NUM_SPLITS_FOR_CUDA_GRAPH\": lambda: int(\n        os.getenv(\"VLLM_FLASH_ATTN_MAX_NUM_SPLITS_FOR_CUDA_GRAPH\", \"32\")\n    ),\n    # Number of GPUs per worker in Ray, if it is set to be a fraction,\n    # it allows ray to schedule multiple actors on a single GPU,\n    # so that users can colocate other actors on the same GPUs as vLLM.\n    \"VLLM_RAY_PER_WORKER_GPUS\": lambda: float(\n        os.getenv(\"VLLM_RAY_PER_WORKER_GPUS\", \"1.0\")\n    ),\n    # Bundle indices for Ray, if it is set, it can control precisely\n    # which indices are used for the Ray bundle, for every worker.\n    # Format: comma-separated list of integers, e.g. \"0,1,2,3\"\n    \"VLLM_RAY_BUNDLE_INDICES\": lambda: os.getenv(\"VLLM_RAY_BUNDLE_INDICES\", \"\"),\n    # In some system, find_loaded_library() may not work. So we allow users to\n    # specify the path through environment variable VLLM_CUDART_SO_PATH.\n    \"VLLM_CUDART_SO_PATH\": lambda: os.getenv(\"VLLM_CUDART_SO_PATH\", None),\n    # Rank of the process in the data parallel setting\n    \"VLLM_DP_RANK\": lambda: int(os.getenv(\"VLLM_DP_RANK\", \"0\")),\n    # Rank of the process in the data parallel setting.\n    # Defaults to VLLM_DP_RANK when not set.\n    \"VLLM_DP_RANK_LOCAL\": lambda: int(\n        os.getenv(\"VLLM_DP_RANK_LOCAL\", sys.modules[__name__].VLLM_DP_RANK)\n    ),\n    # World size of the data parallel setting\n    \"VLLM_DP_SIZE\": lambda: int(os.getenv(\"VLLM_DP_SIZE\", \"1\")),\n    # IP address of the master node in the data parallel setting\n    \"VLLM_DP_MASTER_IP\": lambda: os.getenv(\"VLLM_DP_MASTER_IP\", \"127.0.0.1\"),\n    # Port of the master node in the data parallel setting\n    \"VLLM_DP_MASTER_PORT\": lambda: int(os.getenv(\"VLLM_DP_MASTER_PORT\", \"0\")),\n    # In the context of executing MoE models with Data-Parallel, Expert-Parallel\n    # and Batched All-to-All dispatch/combine kernels, VLLM_MOE_DP_CHUNK_SIZE\n    # dictates the quantum of tokens that can be dispatched from a DP\n    # rank. All DP ranks process the activations in VLLM_MOE_DP_CHUNK_SIZE\n    # units.\n    \"VLLM_MOE_DP_CHUNK_SIZE\": lambda: int(os.getenv(\"VLLM_MOE_DP_CHUNK_SIZE\", \"256\")),\n    \"VLLM_ENABLE_MOE_DP_CHUNK\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_MOE_DP_CHUNK\", \"1\"))\n    ),\n    # Randomize inputs during dummy runs when using Data Parallel\n    \"VLLM_RANDOMIZE_DP_DUMMY_INPUTS\": lambda: os.environ.get(\n        \"VLLM_RANDOMIZE_DP_DUMMY_INPUTS\", \"0\"\n    )\n    == \"1\",\n    # Strategy to pack the data parallel ranks for Ray.\n    # Available options:\n    # - \"fill\":\n    #   for DP master node, allocate exactly data-parallel-size-local DP ranks,\n    #   for non-master nodes, allocate as many DP ranks as can fit;\n    # - \"strict\":\n    #   allocate exactly data-parallel-size-local DP ranks to each picked node;\n    # - \"span\":\n    #   Should be used only when a single DP rank requires multiple nodes.\n    #   allocate one DP rank over as many nodes as required for set world_size;\n    # This environment variable is ignored if data-parallel-backend is not Ray.\n    \"VLLM_RAY_DP_PACK_STRATEGY\": lambda: os.getenv(\n        \"VLLM_RAY_DP_PACK_STRATEGY\", \"strict\"\n    ),\n    # Whether to use S3 path for model loading in CI via RunAI Streamer\n    \"VLLM_CI_USE_S3\": lambda: os.environ.get(\"VLLM_CI_USE_S3\", \"0\") == \"1\",\n    # Use model_redirect to redirect the model name to a local folder.\n    # `model_redirect` can be a json file mapping the model between\n    # repo_id and local folder:\n    # {\"meta-llama/Llama-3.2-1B\": \"/tmp/Llama-3.2-1B\"}\n    # or a space separated values table file:\n    # meta-llama/Llama-3.2-1B   /tmp/Llama-3.2-1B\n    \"VLLM_MODEL_REDIRECT_PATH\": lambda: os.environ.get(\n        \"VLLM_MODEL_REDIRECT_PATH\", None\n    ),\n    # Whether to use atomicAdd reduce in gptq/awq marlin kernel.\n    \"VLLM_MARLIN_USE_ATOMIC_ADD\": lambda: os.environ.get(\n        \"VLLM_MARLIN_USE_ATOMIC_ADD\", \"0\"\n    )\n    == \"1\",\n    # Whether to use marlin kernel in mxfp4 quantization method\n    \"VLLM_MXFP4_USE_MARLIN\": lambda: maybe_convert_bool(\n        os.environ.get(\"VLLM_MXFP4_USE_MARLIN\", None)\n    ),\n    # The activation dtype for marlin kernel\n    \"VLLM_MARLIN_INPUT_DTYPE\": env_with_choices(\n        \"VLLM_MARLIN_INPUT_DTYPE\", None, [\"int8\", \"fp8\"]\n    ),\n    # Whether to use DeepEPLL kernels for NVFP4 quantization and dispatch method\n    # only supported on Blackwell GPUs and with\n    # https://github.com/deepseek-ai/DeepEP/pull/341\n    \"VLLM_DEEPEPLL_NVFP4_DISPATCH\": lambda: bool(\n        int(os.getenv(\"VLLM_DEEPEPLL_NVFP4_DISPATCH\", \"0\"))\n    ),\n    # Whether to turn on the outlines cache for V1\n    # This cache is unbounded and on disk, so it's not safe to use in\n    # an environment with potentially malicious users.\n    \"VLLM_V1_USE_OUTLINES_CACHE\": lambda: os.environ.get(\n        \"VLLM_V1_USE_OUTLINES_CACHE\", \"0\"\n    )\n    == \"1\",\n    # Gap between padding buckets for the forward pass. So we have\n    # 8, we will run forward pass with [16, 24, 32, ...].\n    \"VLLM_TPU_BUCKET_PADDING_GAP\": lambda: int(\n        os.environ[\"VLLM_TPU_BUCKET_PADDING_GAP\"]\n    )\n    if \"VLLM_TPU_BUCKET_PADDING_GAP\" in os.environ\n    else 0,\n    \"VLLM_TPU_MOST_MODEL_LEN\": lambda: maybe_convert_int(\n        os.environ.get(\"VLLM_TPU_MOST_MODEL_LEN\", None)\n    ),\n    # Whether using Pathways\n    \"VLLM_TPU_USING_PATHWAYS\": lambda: bool(\n        \"proxy\" in os.getenv(\"JAX_PLATFORMS\", \"\").lower()\n    ),\n    # Allow use of DeepGemm kernels for fused moe ops.\n    \"VLLM_USE_DEEP_GEMM\": lambda: bool(int(os.getenv(\"VLLM_USE_DEEP_GEMM\", \"1\"))),\n    # Allow use of DeepGemm specifically for MoE fused ops (overrides only MoE).\n    \"VLLM_MOE_USE_DEEP_GEMM\": lambda: bool(\n        int(os.getenv(\"VLLM_MOE_USE_DEEP_GEMM\", \"1\"))\n    ),\n    # Whether to use E8M0 scaling when DeepGEMM is used on Blackwell GPUs.\n    \"VLLM_USE_DEEP_GEMM_E8M0\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_DEEP_GEMM_E8M0\", \"1\"))\n    ),\n    # DeepGemm JITs the kernels on-demand. The warmup attempts to make DeepGemm\n    # JIT all the required kernels before model execution so there is no\n    # JIT'ing in the hot-path. However, this warmup increases the engine\n    # startup time by a couple of minutes.\n    # Available options:\n    #  - \"skip\"  : Skip warmup.\n    #  - \"full\"  : Warmup deepgemm by running all possible gemm shapes the\n    #   engine could encounter.\n    #  - \"relax\" : Select gemm shapes to run based on some heuristics. The\n    #   heuristic aims to have the same effect as running all possible gemm\n    #   shapes, but provides no guarantees.\n    \"VLLM_DEEP_GEMM_WARMUP\": env_with_choices(\n        \"VLLM_DEEP_GEMM_WARMUP\",\n        \"relax\",\n        [\n            \"skip\",\n            \"full\",\n            \"relax\",\n        ],\n    ),\n    # Whether to use fused grouped_topk used for MoE expert selection.\n    \"VLLM_USE_FUSED_MOE_GROUPED_TOPK\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FUSED_MOE_GROUPED_TOPK\", \"1\"))\n    ),\n    # Allow use of FlashInfer MoE kernels for fused moe ops.\n    \"VLLM_USE_FLASHINFER_MOE_FP16\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP16\", \"0\"))\n    ),\n    # Allow use of FlashInfer MoE kernels for fused moe ops.\n    \"VLLM_USE_FLASHINFER_MOE_FP8\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP8\", \"0\"))\n    ),\n    # Allow use of FlashInfer CUTLASS kernels for fused moe ops.\n    \"VLLM_USE_FLASHINFER_MOE_FP4\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP4\", \"0\"))\n    ),\n    # If set to 1, use the FlashInfer\n    # MXFP8 (activation) x MXFP4 (weight) MoE backend.\n    \"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8\", \"0\"))\n    ),\n    # If set to 1, use the FlashInfer CUTLASS backend for\n    # MXFP8 (activation) x MXFP4 (weight) MoE.\n    # This is separate from the TRTLLMGEN path controlled by\n    # VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8.\n    \"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8_CUTLASS\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8_CUTLASS\", \"0\"))\n    ),\n    # If set to 1, use the FlashInfer\n    # BF16 (activation) x MXFP4 (weight) MoE backend.\n    \"VLLM_USE_FLASHINFER_MOE_MXFP4_BF16\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_MXFP4_BF16\", \"0\"))\n    ),\n    # Control the cache sized used by the xgrammar compiler. The default\n    # of 512 MB should be enough for roughly 1000 JSON schemas.\n    # It can be changed with this variable if needed for some reason.\n    \"VLLM_XGRAMMAR_CACHE_MB\": lambda: int(os.getenv(\"VLLM_XGRAMMAR_CACHE_MB\", \"512\")),\n    # Control the threshold for msgspec to use 'zero copy' for\n    # serialization/deserialization of tensors. Tensors below\n    # this limit will be encoded into the msgpack buffer, and\n    # tensors above will instead be sent via a separate message.\n    # While the sending side still actually copies the tensor\n    # in all cases, on the receiving side, tensors above this\n    # limit will actually be zero-copy decoded.\n    \"VLLM_MSGPACK_ZERO_COPY_THRESHOLD\": lambda: int(\n        os.getenv(\"VLLM_MSGPACK_ZERO_COPY_THRESHOLD\", \"256\")\n    ),\n    # If set, allow insecure serialization using pickle.\n    # This is useful for environments where it is deemed safe to use the\n    # insecure method and it is needed for some reason.\n    \"VLLM_ALLOW_INSECURE_SERIALIZATION\": lambda: bool(\n        int(os.getenv(\"VLLM_ALLOW_INSECURE_SERIALIZATION\", \"0\"))\n    ),\n    # IP address used for NIXL handshake between remote agents.\n    \"VLLM_NIXL_SIDE_CHANNEL_HOST\": lambda: os.getenv(\n        \"VLLM_NIXL_SIDE_CHANNEL_HOST\", \"localhost\"\n    ),\n    # Port used for NIXL handshake between remote agents.\n    \"VLLM_NIXL_SIDE_CHANNEL_PORT\": lambda: int(\n        os.getenv(\"VLLM_NIXL_SIDE_CHANNEL_PORT\", \"5600\")\n    ),\n    # Port used for Mooncake handshake between remote agents.\n    \"VLLM_MOONCAKE_BOOTSTRAP_PORT\": lambda: int(\n        os.getenv(\"VLLM_MOONCAKE_BOOTSTRAP_PORT\", \"8998\")\n    ),\n    # [DEPRECATED - will be removed in v0.15.0] all2all backend for vllm's\n    # expert parallel communication. Use --all2all-backend CLI argument instead.\n    # Available options:\n    # - \"naive\": naive all2all implementation using broadcasts\n    # - \"allgather_reducescatter\": all2all implementation based on allgather and\n    #  reducescatter\n    # - \"pplx\": use pplx kernels\n    # - \"deepep_high_throughput\", use deepep high-throughput kernels\n    # - \"deepep_low_latency\", use deepep low-latency kernels\n    # - \"flashinfer_all2allv\", use flashinfer alltoallv kernels for mnnvl\n    \"VLLM_ALL2ALL_BACKEND\": env_with_choices(\n        \"VLLM_ALL2ALL_BACKEND\",\n        None,\n        [\n            \"naive\",\n            \"pplx\",\n            \"deepep_high_throughput\",\n            \"deepep_low_latency\",\n            \"allgather_reducescatter\",\n            \"flashinfer_all2allv\",\n        ],\n    ),\n    # Flashinfer MoE backend for vLLM's fused Mixture-of-Experts support.\n    # Both require compute capability 10.0 or above.\n    # Available options:\n    # - \"throughput\":  [default]\n    #     Uses CUTLASS kernels optimized for high-throughput batch inference.\n    # - \"latency\":\n    #     Uses TensorRT-LLM kernels optimized for low-latency inference.\n    \"VLLM_FLASHINFER_MOE_BACKEND\": env_with_choices(\n        \"VLLM_FLASHINFER_MOE_BACKEND\",\n        \"latency\",\n        [\"throughput\", \"latency\", \"masked_gemm\"],\n    ),\n    # Control the workspace buffer size for the FlashInfer backend.\n    \"VLLM_FLASHINFER_WORKSPACE_BUFFER_SIZE\": lambda: int(\n        os.getenv(\"VLLM_FLASHINFER_WORKSPACE_BUFFER_SIZE\", str(394 * 1024 * 1024))\n    ),\n    # Control the maximum number of tokens per expert supported by the\n    # NVFP4 MoE CUTLASS Kernel. This value is used to create a buffer for\n    # the blockscale tensor of activations NVFP4 Quantization.\n    # This is used to prevent the kernel from running out of memory.\n    \"VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE\": lambda: int(\n        os.getenv(\"VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE\", \"163840\")\n    ),\n    # Specifies the thresholds of the communicated tensor sizes under which\n    # vllm should use flashinfer fused allreduce. The variable should be a\n    # JSON with the following format:\n    #     { <world size>: <max size in mb> }\n    # Unspecified world sizes will fall back to\n    #     { 2: 64, 4: 1, <everything else>: 0.5 }\n    \"VLLM_FLASHINFER_ALLREDUCE_FUSION_THRESHOLDS_MB\": lambda: json.loads(\n        os.getenv(\"VLLM_FLASHINFER_ALLREDUCE_FUSION_THRESHOLDS_MB\", \"{}\")\n    ),\n    # MoE routing strategy selector.\n    # See `RoutingSimulator.get_available_strategies()` # for available\n    # strategies.\n    # Custom routing strategies can be registered by\n    # RoutingSimulator.register_strategy()\n    # Note: custom strategies may not produce correct model outputs\n    \"VLLM_MOE_ROUTING_SIMULATION_STRATEGY\": lambda: os.environ.get(\n        \"VLLM_MOE_ROUTING_SIMULATION_STRATEGY\", \"\"\n    ).lower(),\n    # Regex timeout for use by the vLLM tool parsing plugins.\n    \"VLLM_TOOL_PARSE_REGEX_TIMEOUT_SECONDS\": lambda: int(\n        os.getenv(\"VLLM_TOOL_PARSE_REGEX_TIMEOUT_SECONDS\", \"1\")\n    ),\n    # Reduce CPU usage when vLLM is idle. Enabling this will incur small\n    # latency penalty when a request eventually comes.\n    \"VLLM_SLEEP_WHEN_IDLE\": lambda: bool(int(os.getenv(\"VLLM_SLEEP_WHEN_IDLE\", \"0\"))),\n    # Control the max chunk bytes (in MB) for the rpc message queue.\n    # Object larger than this threshold will be broadcast to worker\n    # processes via zmq.\n    \"VLLM_MQ_MAX_CHUNK_BYTES_MB\": lambda: int(\n        os.getenv(\"VLLM_MQ_MAX_CHUNK_BYTES_MB\", \"16\")\n    ),\n    # Timeout in seconds for execute_model RPC calls in multiprocessing\n    # executor (only applies when TP > 1).\n    \"VLLM_EXECUTE_MODEL_TIMEOUT_SECONDS\": lambda: int(\n        os.getenv(\"VLLM_EXECUTE_MODEL_TIMEOUT_SECONDS\", \"300\")\n    ),\n    # KV Cache layout used throughout vllm.\n    # Some common values are:\n    # - NHD\n    # - HND\n    # Where N=num_blocks, H=num_heads and D=head_size. The default value will\n    # leave the layout choice to the backend. Mind that backends may only\n    # implement and support a subset of all possible layouts.\n    \"VLLM_KV_CACHE_LAYOUT\": env_with_choices(\n        \"VLLM_KV_CACHE_LAYOUT\", None, [\"NHD\", \"HND\"]\n    ),\n    # Enable checking whether the generated logits contain NaNs,\n    # indicating corrupted output. Useful for debugging low level bugs\n    # or bad hardware but it may add compute overhead.\n    \"VLLM_COMPUTE_NANS_IN_LOGITS\": lambda: bool(\n        int(os.getenv(\"VLLM_COMPUTE_NANS_IN_LOGITS\", \"0\"))\n    ),\n    # Controls whether or not emulations are used for NVFP4\n    # generations on machines < 100 for compressed-tensors\n    # models\n    \"VLLM_USE_NVFP4_CT_EMULATIONS\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_NVFP4_CT_EMULATIONS\", \"0\"))\n    ),\n    # Time (in seconds) after which the KV cache on the producer side is\n    # automatically cleared if no READ notification is received from the\n    # consumer. This is only applicable when using NixlConnector in a\n    # disaggregated decode-prefill setup.\n    \"VLLM_NIXL_ABORT_REQUEST_TIMEOUT\": lambda: int(\n        os.getenv(\"VLLM_NIXL_ABORT_REQUEST_TIMEOUT\", \"480\")\n    ),\n    # Timeout (in seconds) for MooncakeConnector in PD disaggregated setup.\n    \"VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT\": lambda: int(\n        os.getenv(\"VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT\", \"480\")\n    ),\n    # Controls whether or not to use cudnn prefill\n    \"VLLM_USE_CUDNN_PREFILL\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_CUDNN_PREFILL\", \"0\"))\n    ),\n    # Controls whether to use TRT-LLM ragged DeepSeek prefill\n    \"VLLM_USE_TRTLLM_RAGGED_DEEPSEEK_PREFILL\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_TRTLLM_RAGGED_DEEPSEEK_PREFILL\", \"0\"))\n    ),\n    # If set to 1/True, use the TRTLLM attention backend in flashinfer.\n    # If set to 0/False, use the default attention backend in flashinfer.\n    # If not set, auto-detect the attention backend in flashinfer.\n    \"VLLM_USE_TRTLLM_ATTENTION\": lambda: (\n        None\n        if \"VLLM_USE_TRTLLM_ATTENTION\" not in os.environ\n        else os.environ[\"VLLM_USE_TRTLLM_ATTENTION\"].lower() in (\"1\", \"true\")\n    ),\n    # If set to 1, when we use fp8 kv, we do not quantize Q to fp8\n    \"VLLM_FLASHINFER_DISABLE_Q_QUANTIZATION\": lambda: bool(\n        int(os.getenv(\"VLLM_FLASHINFER_DISABLE_Q_QUANTIZATION\", \"0\"))\n    ),\n    # If set, it means we pre-downloaded cubin files and flashinfer will\n    # read the cubin files directly.\n    \"VLLM_HAS_FLASHINFER_CUBIN\": lambda: bool(\n        int(os.getenv(\"VLLM_HAS_FLASHINFER_CUBIN\", \"0\"))\n    ),\n    # Supported options:\n    # - \"flashinfer-cudnn\": use flashinfer cudnn GEMM backend\n    # - \"flashinfer-trtllm\": use flashinfer trtllm GEMM backend\n    # - \"flashinfer-cutlass\": use flashinfer cutlass GEMM backend\n    # - <none>: automatically pick an available backend\n    \"VLLM_NVFP4_GEMM_BACKEND\": env_with_choices(\n        \"VLLM_NVFP4_GEMM_BACKEND\",\n        None,\n        [\"flashinfer-cudnn\", \"flashinfer-trtllm\", \"flashinfer-cutlass\", \"cutlass\"],\n    ),\n    # Controls garbage collection during CUDA graph capture.\n    # If set to 0 (default), enables GC freezing to speed up capture time.\n    # If set to 1, allows GC to run during capture.\n    \"VLLM_ENABLE_CUDAGRAPH_GC\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_CUDAGRAPH_GC\", \"0\"))\n    ),\n    # Used to force set up loopback IP\n    \"VLLM_LOOPBACK_IP\": lambda: os.getenv(\"VLLM_LOOPBACK_IP\", \"\"),\n    # Used to set the process name prefix for vLLM processes.\n    # This is useful for debugging and monitoring purposes.\n    # The default value is \"VLLM\".\n    \"VLLM_PROCESS_NAME_PREFIX\": lambda: os.getenv(\"VLLM_PROCESS_NAME_PREFIX\", \"VLLM\"),\n    # Allow chunked local attention with hybrid kv cache manager.\n    # Currently using the Hybrid KV cache manager with chunked local attention\n    # in the Llama4 models (the only models currently using chunked local attn)\n    # causes a latency regression. For this reason, we disable it by default.\n    # This flag is used to allow users to enable it if they want to (to save on\n    # kv-cache memory usage and enable longer contexts)\n    # TODO(lucas): Remove this flag once latency regression is resolved.\n    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\": lambda: bool(\n        int(os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"1\"))\n    ),\n    # Enables support for the \"store\" option in the OpenAI Responses API.\n    # When set to 1, vLLM's OpenAI server will retain the input and output\n    # messages for those requests in memory. By default, this is disabled (0),\n    # and the \"store\" option is ignored.\n    # NOTE/WARNING:\n    # 1. Messages are kept in memory only (not persisted to disk) and will be\n    #    lost when the vLLM server shuts down.\n    # 2. Enabling this option will cause a memory leak, as stored messages are\n    #    never removed from memory until the server terminates.\n    \"VLLM_ENABLE_RESPONSES_API_STORE\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_RESPONSES_API_STORE\", \"0\"))\n    ),\n    # If set, use the fp8 mfma in rocm paged attention.\n    \"VLLM_ROCM_FP8_MFMA_PAGE_ATTN\": lambda: bool(\n        int(os.getenv(\"VLLM_ROCM_FP8_MFMA_PAGE_ATTN\", \"0\"))\n    ),\n    # Whether to use pytorch symmetric memory for allreduce\n    \"VLLM_ALLREDUCE_USE_SYMM_MEM\": lambda: bool(\n        int(os.getenv(\"VLLM_ALLREDUCE_USE_SYMM_MEM\", \"1\"))\n    ),\n    # Experimental: use this to enable MCP tool calling for non harmony models\n    \"VLLM_USE_EXPERIMENTAL_PARSER_CONTEXT\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_EXPERIMENTAL_PARSER_CONTEXT\", \"0\"))\n    ),\n    # Allows vllm to find tuned config under customized folder\n    \"VLLM_TUNED_CONFIG_FOLDER\": lambda: os.getenv(\"VLLM_TUNED_CONFIG_FOLDER\", None),\n    # Valid values are container,code_interpreter,web_search_preview\n    # ex VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS=container,code_interpreter\n    # If the server_label of your mcp tool is not in this list it will\n    # be completely ignored.\n    \"VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS\": env_set_with_choices(\n        \"VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS\",\n        default=[],\n        choices=[\"container\", \"code_interpreter\", \"web_search_preview\"],\n    ),\n    # Allows harmony instructions to be injected on system messages\n    \"VLLM_GPT_OSS_HARMONY_SYSTEM_INSTRUCTIONS\": lambda: bool(\n        int(os.getenv(\"VLLM_GPT_OSS_HARMONY_SYSTEM_INSTRUCTIONS\", \"0\"))\n    ),\n    # Enable automatic retry when tool call JSON parsing fails\n    # If enabled, returns an error message to the model to retry\n    # If disabled (default), raises an exception and fails the request\n    \"VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY\": lambda: bool(\n        int(os.getenv(\"VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY\", \"0\"))\n    ),\n    # Add optional custom scopes for profiling, disable to avoid overheads\n    \"VLLM_CUSTOM_SCOPES_FOR_PROFILING\": lambda: bool(\n        int(os.getenv(\"VLLM_CUSTOM_SCOPES_FOR_PROFILING\", \"0\"))\n    ),\n    # Add optional nvtx scopes for profiling, disable to avoid overheads\n    \"VLLM_NVTX_SCOPES_FOR_PROFILING\": lambda: bool(\n        int(os.getenv(\"VLLM_NVTX_SCOPES_FOR_PROFILING\", \"0\"))\n    ),\n    # Represent block hashes in KV cache events as 64-bit integers instead of\n    # raw bytes. Defaults to True for backward compatibility.\n    \"VLLM_KV_EVENTS_USE_INT_BLOCK_HASHES\": lambda: bool(\n        int(os.getenv(\"VLLM_KV_EVENTS_USE_INT_BLOCK_HASHES\", \"1\"))\n    ),\n    # Name of the shared memory buffer used for object storage.\n    # Only effective when mm_config.mm_processor_cache_type == \"shm\".\n    \"VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME\": lambda: os.getenv(\n        \"VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME\", \"VLLM_OBJECT_STORAGE_SHM_BUFFER\"\n    ),\n    # The size in MB of the buffers (NVL and RDMA) used by DeepEP\n    \"VLLM_DEEPEP_BUFFER_SIZE_MB\": lambda: int(\n        os.getenv(\"VLLM_DEEPEP_BUFFER_SIZE_MB\", \"1024\")\n    ),\n    # Force DeepEP to use intranode kernel for inter-node communication in\n    # high throughput mode. This is useful archive higher prefill throuhgput\n    # on system supports multi-node nvlink (e.g GB200).\n    \"VLLM_DEEPEP_HIGH_THROUGHPUT_FORCE_INTRA_NODE\": lambda: bool(\n        int(os.getenv(\"VLLM_DEEPEP_HIGH_THROUGHPUT_FORCE_INTRA_NODE\", \"0\"))\n    ),\n    # Allow DeepEP to use MNNVL (multi-node nvlink) for internode_ll kernel,\n    # turn this for better latency on GB200 like system\n    \"VLLM_DEEPEP_LOW_LATENCY_USE_MNNVL\": lambda: bool(\n        int(os.getenv(\"VLLM_DEEPEP_LOW_LATENCY_USE_MNNVL\", \"0\"))\n    ),\n    # The number of SMs to allocate for communication kernels when running DBO\n    # the rest of the SMs on the device will be allocated to compute\n    \"VLLM_DBO_COMM_SMS\": lambda: int(os.getenv(\"VLLM_DBO_COMM_SMS\", \"20\")),\n    # Enable max_autotune & coordinate_descent_tuning in inductor_config\n    # to compile static shapes passed from compile_sizes in compilation_config\n    # If set to 1, enable max_autotune; By default, this is enabled (1)\n    \"VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE\", \"1\"))\n    ),\n    # If set to 1, enable coordinate_descent_tuning;\n    # By default, this is enabled (1)\n    \"VLLM_ENABLE_INDUCTOR_COORDINATE_DESCENT_TUNING\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_INDUCTOR_COORDINATE_DESCENT_TUNING\", \"1\"))\n    ),\n    # Flag to enable NCCL symmetric memory allocation and registration\n    \"VLLM_USE_NCCL_SYMM_MEM\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_NCCL_SYMM_MEM\", \"0\"))\n    ),\n    # NCCL header path\n    \"VLLM_NCCL_INCLUDE_PATH\": lambda: os.environ.get(\"VLLM_NCCL_INCLUDE_PATH\", None),\n    # Flag to enable FBGemm kernels on model execution\n    \"VLLM_USE_FBGEMM\": lambda: bool(int(os.getenv(\"VLLM_USE_FBGEMM\", \"0\"))),\n    # GC debug config\n    # - VLLM_GC_DEBUG=0: disable GC debugger\n    # - VLLM_GC_DEBUG=1: enable GC debugger with gc.collect elpased times\n    # - VLLM_GC_DEBUG='{\"top_objects\":5}': enable GC debugger with\n    #                                      top 5 collected objects\n    \"VLLM_GC_DEBUG\": lambda: os.getenv(\"VLLM_GC_DEBUG\", \"\"),\n    # Debug workspace allocations.\n    # logging of workspace resize operations.\n    \"VLLM_DEBUG_WORKSPACE\": lambda: bool(int(os.getenv(\"VLLM_DEBUG_WORKSPACE\", \"0\"))),\n    # Disables parallel execution of shared_experts via separate cuda stream\n    \"VLLM_DISABLE_SHARED_EXPERTS_STREAM\": lambda: bool(\n        int(os.getenv(\"VLLM_DISABLE_SHARED_EXPERTS_STREAM\", \"0\"))\n    ),\n    # Limits when we run shared_experts in a separate stream.\n    # We found out that for large batch sizes, the separate stream\n    # execution is not beneficial (most likely because of the input clone)\n    # TODO(alexm-redhat): Tune to be more dynamic based on GPU type\n    \"VLLM_SHARED_EXPERTS_STREAM_TOKEN_THRESHOLD\": lambda: int(\n        int(os.getenv(\"VLLM_SHARED_EXPERTS_STREAM_TOKEN_THRESHOLD\", 256))\n    ),\n    # Format for saving torch.compile cache artifacts\n    # - \"binary\": saves as binary file\n    #     Safe for multiple vllm serve processes accessing the same torch compile cache.\n    # - \"unpacked\": saves as directory structure (for inspection/debugging)\n    #     NOT multiprocess safe - race conditions may occur with multiple processes.\n    #     Allows viewing and setting breakpoints in Inductor's code output files.\n    \"VLLM_COMPILE_CACHE_SAVE_FORMAT\": env_with_choices(\n        \"VLLM_COMPILE_CACHE_SAVE_FORMAT\", \"binary\", [\"binary\", \"unpacked\"]\n    ),\n    # Flag to enable v2 model runner.\n    \"VLLM_USE_V2_MODEL_RUNNER\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_V2_MODEL_RUNNER\", \"0\"))\n    ),\n    # Debug logging for --enable-mfu-metrics\n    \"VLLM_DEBUG_MFU_METRICS\": lambda: bool(\n        int(os.getenv(\"VLLM_DEBUG_MFU_METRICS\", \"0\"))\n    ),\n}",
      "language": "sql"
    },
    {
      "code": "logger = logging.getLogger(__name__)\n\nenvironment_variables: dict[str, Callable[[], Any]] = {\n    # ================== Installation Time Env Vars ==================\n    # Target device of vLLM, supporting [cuda (by default),\n    # rocm, cpu]\n    \"VLLM_TARGET_DEVICE\": lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\").lower(),\n    # Main CUDA version of vLLM. This follows PyTorch but can be overridden.\n    \"VLLM_MAIN_CUDA_VERSION\": lambda: os.getenv(\"VLLM_MAIN_CUDA_VERSION\", \"\").lower()\n    or \"12.9\",\n    # Controls PyTorch float32 matmul precision mode within vLLM workers.\n    # Accepted values:\n    #   - \"ieee\" (default): force full IEEE FP32 matmul precision.\n    #   - \"tf32\": enable TensorFloat32-based fast matmul.\n    \"VLLM_FLOAT32_MATMUL_PRECISION\": env_with_choices(\n        \"VLLM_FLOAT32_MATMUL_PRECISION\",\n        \"ieee\",\n        [\"ieee\", \"tf32\"],\n        case_sensitive=False,\n    ),\n    # Maximum number of compilation jobs to run in parallel.\n    # By default this is the number of CPUs\n    \"MAX_JOBS\": lambda: os.getenv(\"MAX_JOBS\", None),\n    # Number of threads to use for nvcc\n    # By default this is 1.\n    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.\n    \"NVCC_THREADS\": lambda: os.getenv(\"NVCC_THREADS\", None),\n    # If set, vllm will use precompiled binaries (*.so)\n    \"VLLM_USE_PRECOMPILED\": lambda: os.environ.get(\"VLLM_USE_PRECOMPILED\", \"\")\n    .strip()\n    .lower()\n    in (\"1\", \"true\")\n    or bool(os.environ.get(\"VLLM_PRECOMPILED_WHEEL_LOCATION\")),\n    # If set, skip adding +precompiled suffix to version string\n    \"VLLM_SKIP_PRECOMPILED_VERSION_SUFFIX\": lambda: bool(\n        int(os.environ.get(\"VLLM_SKIP_PRECOMPILED_VERSION_SUFFIX\", \"0\"))\n    ),\n    # Used to mark that setup.py is running in a Docker build context,\n    # in order to force the use of precompiled binaries.\n    \"VLLM_DOCKER_BUILD_CONTEXT\": lambda: os.environ.get(\"VLLM_DOCKER_BUILD_CONTEXT\", \"\")\n    .strip()\n    .lower()\n    in (\"1\", \"true\"),\n    # CMake build type\n    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"\n    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"\n    \"CMAKE_BUILD_TYPE\": env_with_choices(\n        \"CMAKE_BUILD_TYPE\", None, [\"Debug\", \"Release\", \"RelWithDebInfo\"]\n    ),\n    # If set, vllm will print verbose logs during installation\n    \"VERBOSE\": lambda: bool(int(os.getenv(\"VERBOSE\", \"0\"))),\n    # Root directory for vLLM configuration files\n    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set\n    # Note that this not only affects how vllm finds its configuration files\n    # during runtime, but also affects how vllm installs its configuration\n    # files during **installation**.\n    \"VLLM_CONFIG_ROOT\": lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CONFIG_ROOT\",\n            os.path.join(get_default_config_root(), \"vllm\"),\n        )\n    ),\n    # ================== Runtime Env Vars ==================\n    # Root directory for vLLM cache files\n    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set\n    \"VLLM_CACHE_ROOT\": lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CACHE_ROOT\",\n            os.path.join(get_default_cache_root(), \"vllm\"),\n        )\n    ),\n    # used in distributed environment to determine the ip address\n    # of the current node, when the node has multiple network interfaces.\n    # If you are using multi-node inference, you should set this differently\n    # on each node.\n    \"VLLM_HOST_IP\": lambda: os.getenv(\"VLLM_HOST_IP\", \"\"),\n    # used in distributed environment to manually set the communication port\n    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the\n    # VLLM_PORT will be used as the first port, and the rest will be generated\n    # by incrementing the VLLM_PORT value.\n    \"VLLM_PORT\": get_vllm_port,\n    # path used for ipc when the frontend api server is running in\n    # multi-processing mode to communicate with the backend engine process.\n    \"VLLM_RPC_BASE_PATH\": lambda: os.getenv(\n        \"VLLM_RPC_BASE_PATH\", tempfile.gettempdir()\n    ),\n    # If true, will load models from ModelScope instead of Hugging Face Hub.\n    # note that the value is true or false, not numbers\n    \"VLLM_USE_MODELSCOPE\": lambda: os.environ.get(\n        \"VLLM_USE_MODELSCOPE\", \"False\"\n    ).lower()\n    == \"true\",\n    # Interval in seconds to log a warning message when the ring buffer is full\n    \"VLLM_RINGBUFFER_WARNING_INTERVAL\": lambda: int(\n        os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")\n    ),\n    # path to cudatoolkit home directory, under which should be bin, include,\n    # and lib directories.\n    \"CUDA_HOME\": lambda: os.environ.get(\"CUDA_HOME\", None),\n    # Path to the NCCL library file. It is needed because nccl>=2.19 brought\n    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234\n    \"VLLM_NCCL_SO_PATH\": lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),\n    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl\n    # library file in the locations specified by `LD_LIBRARY_PATH`\n    \"LD_LIBRARY_PATH\": lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),\n    # flag to control the chunk size (in MB) for sleeping memory allocations under ROCm\n    \"VLLM_ROCM_SLEEP_MEM_CHUNK_SIZE\": lambda: int(\n        os.environ.get(\"VLLM_ROCM_SLEEP_MEM_CHUNK_SIZE\", \"256\")\n    ),\n    # Use separate prefill and decode kernels for V1 attention instead of\n    # the unified triton kernel.\n    \"VLLM_V1_USE_PREFILL_DECODE_ATTENTION\": lambda: (\n        os.getenv(\"VLLM_V1_USE_PREFILL_DECODE_ATTENTION\", \"False\").lower()\n        in (\"true\", \"1\")\n    ),\n    # Force vllm to use a specific flash-attention version (2 or 3), only valid\n    # when using the flash-attention backend.\n    \"VLLM_FLASH_ATTN_VERSION\": lambda: maybe_convert_int(\n        os.environ.get(\"VLLM_FLASH_ATTN_VERSION\", None)\n    ),\n    # Feature flag to enable/disable Inductor standalone compile.\n    # In torch <= 2.7 we ignore this flag; in torch >= 2.9 this is\n    # enabled by default.\n    \"VLLM_USE_STANDALONE_COMPILE\": lambda: os.environ.get(\n        \"VLLM_USE_STANDALONE_COMPILE\", \"1\"\n    )\n    == \"1\",\n    # Debug pattern matching inside custom passes.\n    # Should be set to the fx.Node name (e.g. 'getitem_34' or 'scaled_mm_3').\n    \"VLLM_PATTERN_MATCH_DEBUG\": lambda: os.environ.get(\n        \"VLLM_PATTERN_MATCH_DEBUG\", None\n    ),\n    # Dump fx graphs to the given directory.\n    # It will override CompilationConfig.debug_dump_path if set.\n    \"VLLM_DEBUG_DUMP_PATH\": lambda: os.environ.get(\"VLLM_DEBUG_DUMP_PATH\", None),\n    # Feature flag to enable/disable AOT compilation. This will ensure\n    # compilation is done in warmup phase and the compilation will be\n    # reused in subsequent calls.\n    \"VLLM_USE_AOT_COMPILE\": use_aot_compile,\n    # Feature flag to enable/disable bytecode in\n    # TorchCompileWithNoGuardsWrapper.\n    \"VLLM_USE_BYTECODE_HOOK\": lambda: bool(\n        int(os.environ.get(\"VLLM_USE_BYTECODE_HOOK\", \"1\"))\n    ),\n    # Force vllm to always load AOT compiled models from disk. Failure\n    # to load will result in a hard error when this is enabled.\n    # Will be ignored when VLLM_USE_AOT_COMPILE is disabled.\n    \"VLLM_FORCE_AOT_LOAD\": lambda: os.environ.get(\"VLLM_FORCE_AOT_LOAD\", \"0\") == \"1\",\n    # local rank of the process in the distributed setting, used to determine\n    # the GPU device id\n    \"LOCAL_RANK\": lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),\n    # used to control the visible devices in the distributed setting\n    \"CUDA_VISIBLE_DEVICES\": lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n    # timeout for each iteration in the engine\n    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\": lambda: int(\n        os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")\n    ),\n    # Timeout in seconds for waiting for engine cores to become ready\n    # during startup. Default is 600 seconds (10 minutes).\n    \"VLLM_ENGINE_READY_TIMEOUT_S\": lambda: int(\n        os.environ.get(\"VLLM_ENGINE_READY_TIMEOUT_S\", \"600\")\n    ),\n    # API key for vLLM API server\n    \"VLLM_API_KEY\": lambda: os.environ.get(\"VLLM_API_KEY\", None),\n    # Whether to log responses from API Server for debugging\n    \"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\": lambda: os.environ.get(\n        \"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\", \"False\"\n    ).lower()\n    == \"true\",\n    # S3 access information, used for tensorizer to load model from S3\n    \"S3_ACCESS_KEY_ID\": lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n    \"S3_SECRET_ACCESS_KEY\": lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n    \"S3_ENDPOINT_URL\": lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),\n    # Usage stats collection\n    \"VLLM_USAGE_STATS_SERVER\": lambda: os.environ.get(\n        \"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"\n    ),\n    \"VLLM_NO_USAGE_STATS\": lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",\n    \"VLLM_DISABLE_FLASHINFER_PREFILL\": lambda: os.environ.get(\n        \"VLLM_DISABLE_FLASHINFER_PREFILL\", \"0\"\n    )\n    == \"1\",\n    \"VLLM_DO_NOT_TRACK\": lambda: (\n        os.environ.get(\"VLLM_DO_NOT_TRACK\", None)\n        or os.environ.get(\"DO_NOT_TRACK\", None)\n        or \"0\"\n    )\n    == \"1\",\n    \"VLLM_USAGE_SOURCE\": lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),\n    # Logging configuration\n    # If set to 0, vllm will not configure logging\n    # If set to 1, vllm will configure logging using the default configuration\n    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH\n    \"VLLM_CONFIGURE_LOGGING\": lambda: bool(\n        int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\"))\n    ),\n    \"VLLM_LOGGING_CONFIG_PATH\": lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),\n    # this is used for configuring the default logging level\n    \"VLLM_LOGGING_LEVEL\": lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\").upper(),\n    # this is used for configuring the default logging stream\n    \"VLLM_LOGGING_STREAM\": lambda: os.getenv(\"VLLM_LOGGING_STREAM\", \"ext://sys.stdout\"),\n    # if set, VLLM_LOGGING_PREFIX will be prepended to all log messages\n    \"VLLM_LOGGING_PREFIX\": lambda: os.getenv(\"VLLM_LOGGING_PREFIX\", \"\"),\n    # Controls colored logging output. Options: \"auto\" (default, colors when terminal),\n    # \"1\" (always use colors), \"0\" (never use colors)\n    \"VLLM_LOGGING_COLOR\": lambda: os.getenv(\"VLLM_LOGGING_COLOR\", \"auto\"),\n    # Standard unix flag for disabling ANSI color codes\n    \"NO_COLOR\": lambda: os.getenv(\"NO_COLOR\", \"0\") != \"0\",\n    # If set, vllm will log stats at this interval in seconds\n    # If not set, vllm will log stats every 10 seconds.\n    \"VLLM_LOG_STATS_INTERVAL\": lambda: val\n    if (val := float(os.getenv(\"VLLM_LOG_STATS_INTERVAL\", \"10.\"))) > 0.0\n    else 10.0,\n    # Trace function calls\n    # If set to 1, vllm will trace function calls\n    # Useful for debugging\n    \"VLLM_TRACE_FUNCTION\": lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),\n    # Backend for attention computation\n    # Example options:\n    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n    # - \"FLASH_ATTN\": use FlashAttention\n    # - \"FLASHINFER\": use flashinfer\n    # - \"FLASHMLA\": use FlashMLA\n    # - \"FLASH_ATTN_MLA\": use FlashAttention for MLA\n    # - \"FLASHINFER_MLA\": use FlashInfer for MLA\n    # - \"CUTLASS_MLA\": use CUTLASS for MLA\n    # All possible options loaded dynamically from AttentionBackendEnum\n    \"VLLM_ATTENTION_BACKEND\": env_with_choices(\n        \"VLLM_ATTENTION_BACKEND\",\n        None,\n        lambda: list(\n            __import__(\n                \"vllm.attention.backends.registry\", fromlist=[\"AttentionBackendEnum\"]\n            ).AttentionBackendEnum.__members__.keys()\n        ),\n    ),\n    # If set, vllm will use flashinfer sampler\n    \"VLLM_USE_FLASHINFER_SAMPLER\": lambda: bool(\n        int(os.environ[\"VLLM_USE_FLASHINFER_SAMPLER\"])\n    )\n    if \"VLLM_USE_FLASHINFER_SAMPLER\" in os.environ\n    else None,\n    # Pipeline stage partition strategy\n    \"VLLM_PP_LAYER_PARTITION\": lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),\n    # (CPU backend only) CPU key-value cache space.\n    # default is None and will be set as 4 GB\n    \"VLLM_CPU_KVCACHE_SPACE\": lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\"))\n    if \"VLLM_CPU_KVCACHE_SPACE\" in os.environ\n    else None,\n    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",\n    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.\n    \"VLLM_CPU_OMP_THREADS_BIND\": lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"auto\"),\n    # (CPU backend only) CPU cores not used by OMP threads .\n    # Those CPU cores will not be used by OMP threads of a rank.\n    \"VLLM_CPU_NUM_OF_RESERVED_CPU\": lambda: int(\n        os.getenv(\"VLLM_CPU_NUM_OF_RESERVED_CPU\", \"0\")\n    )\n    if \"VLLM_CPU_NUM_OF_RESERVED_CPU\" in os.environ\n    else None,\n    # (CPU backend only) whether to use SGL kernels, optimized for small batch.\n    \"VLLM_CPU_SGL_KERNEL\": lambda: bool(int(os.getenv(\"VLLM_CPU_SGL_KERNEL\", \"0\"))),\n    # If the env var is set, Ray Compiled Graph uses the specified\n    # channel type to communicate between workers belonging to\n    # different pipeline-parallel stages.\n    # Available options:\n    # - \"auto\": use the default channel type\n    # - \"nccl\": use NCCL for communication\n    # - \"shm\": use shared memory and gRPC for communication\n    \"VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE\": env_with_choices(\n        \"VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE\", \"auto\", [\"auto\", \"nccl\", \"shm\"]\n    ),\n    # If the env var is set, it enables GPU communication overlap\n    # (experimental feature) in Ray's Compiled Graph.\n    \"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\", \"0\"))\n    ),\n    # If the env var is set, it uses a Ray Communicator wrapping\n    # vLLM's pipeline parallelism communicator to interact with Ray's\n    # Compiled Graph. Otherwise, it uses Ray's NCCL communicator.\n    \"VLLM_USE_RAY_WRAPPED_PP_COMM\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_RAY_WRAPPED_PP_COMM\", \"1\"))\n    ),\n    # Use dedicated multiprocess context for workers.\n    # Both spawn and fork work\n    \"VLLM_WORKER_MULTIPROC_METHOD\": env_with_choices(\n        \"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\", [\"spawn\", \"fork\"]\n    ),\n    # Path to the cache for storing downloaded assets\n    \"VLLM_ASSETS_CACHE\": lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_ASSETS_CACHE\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),\n        )\n    ),\n    # If the env var is set, we will clean model file in\n    # this path $VLLM_ASSETS_CACHE/model_streamer/$model_name\n    \"VLLM_ASSETS_CACHE_MODEL_CLEAN\": lambda: bool(\n        int(os.getenv(\"VLLM_ASSETS_CACHE_MODEL_CLEAN\", \"0\"))\n    ),\n    # Timeout for fetching images when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_IMAGE_FETCH_TIMEOUT\": lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),\n    # Timeout for fetching videos when serving multimodal models\n    # Default is 30 seconds\n    \"VLLM_VIDEO_FETCH_TIMEOUT\": lambda: int(\n        os.getenv(\"VLLM_VIDEO_FETCH_TIMEOUT\", \"30\")\n    ),\n    # Timeout for fetching audio when serving multimodal models\n    # Default is 10 seconds\n    \"VLLM_AUDIO_FETCH_TIMEOUT\": lambda: int(\n        os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"10\")\n    ),\n    # Whether to allow HTTP redirects when fetching from media URLs.\n    # Default to True\n    \"VLLM_MEDIA_URL_ALLOW_REDIRECTS\": lambda: bool(\n        int(os.getenv(\"VLLM_MEDIA_URL_ALLOW_REDIRECTS\", \"1\"))\n    ),\n    # Max number of workers for the thread pool handling\n    # media bytes loading. Set to 1 to disable parallel processing.\n    # Default is 8\n    \"VLLM_MEDIA_LOADING_THREAD_COUNT\": lambda: int(\n        os.getenv(\"VLLM_MEDIA_LOADING_THREAD_COUNT\", \"8\")\n    ),\n    # Maximum filesize in MB for a single audio file when processing\n    # speech-to-text requests. Files larger than this will be rejected.\n    # Default is 25 MB\n    \"VLLM_MAX_AUDIO_CLIP_FILESIZE_MB\": lambda: int(\n        os.getenv(\"VLLM_MAX_AUDIO_CLIP_FILESIZE_MB\", \"25\")\n    ),\n    # Backend for Video IO\n    # - \"opencv\": Default backend that uses OpenCV stream buffered backend.\n    #\n    # Custom backend implementations can be registered\n    # via `@VIDEO_LOADER_REGISTRY.register(\"my_custom_video_loader\")` and\n    # imported at runtime.\n    # If a non-existing backend is used, an AssertionError will be thrown.\n    \"VLLM_VIDEO_LOADER_BACKEND\": lambda: os.getenv(\n        \"VLLM_VIDEO_LOADER_BACKEND\", \"opencv\"\n    ),\n    # Media connector implementation.\n    # - \"http\": Default connector that supports fetching media via HTTP.\n    #\n    # Custom implementations can be registered\n    # via `@MEDIA_CONNECTOR_REGISTRY.register(\"my_custom_media_connector\")` and\n    # imported at runtime.\n    # If a non-existing backend is used, an AssertionError will be thrown.\n    \"VLLM_MEDIA_CONNECTOR\": lambda: os.getenv(\"VLLM_MEDIA_CONNECTOR\", \"http\"),\n    # Path to the XLA persistent cache directory.\n    # Only used for XLA devices such as TPUs.\n    \"VLLM_XLA_CACHE_PATH\": lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_XLA_CACHE_PATH\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),\n        )\n    ),\n    # If set, assert on XLA recompilation after each execution step.\n    \"VLLM_XLA_CHECK_RECOMPILATION\": lambda: bool(\n        int(os.getenv(\"VLLM_XLA_CHECK_RECOMPILATION\", \"0\"))\n    ),\n    # Enable SPMD mode for TPU backend.\n    \"VLLM_XLA_USE_SPMD\": lambda: bool(int(os.getenv(\"VLLM_XLA_USE_SPMD\", \"0\"))),\n    \"VLLM_FUSED_MOE_CHUNK_SIZE\": lambda: int(\n        os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", str(16 * 1024))\n    ),\n    # Control whether to use fused MoE activation chunking. Current chunking\n    # logic is incompatible with torch.compile and causes IMA. See issue\n    # https://github.com/vllm-project/vllm/issues/19631.\n    \"VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_FUSED_MOE_ACTIVATION_CHUNKING\", \"1\"))\n    ),\n    # If set, the OpenAI API server will stay alive even after the underlying\n    # AsyncLLMEngine errors and stops serving requests\n    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\": lambda: bool(\n        int(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", \"0\"))\n    ),\n    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows\n    # the user to specify a max sequence length greater than\n    # the max length derived from the model's config.json.\n    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.\n    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\": lambda: (\n        os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower()\n        in (\"1\", \"true\")\n    ),\n    # If set, forces FP8 Marlin to be used for FP8 quantization regardless\n    # of the hardware support for FP8 compute.\n    \"VLLM_TEST_FORCE_FP8_MARLIN\": lambda: (\n        os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower()\n        in (\"1\", \"true\")\n    ),\n    \"VLLM_TEST_FORCE_LOAD_FORMAT\": lambda: os.getenv(\n        \"VLLM_TEST_FORCE_LOAD_FORMAT\", \"dummy\"\n    ),\n    # Time in ms for the zmq client to wait for a response from the backend\n    # server for simple data operations\n    \"VLLM_RPC_TIMEOUT\": lambda: int(os.getenv(\"VLLM_RPC_TIMEOUT\", \"10000\")),\n    # Timeout in seconds for keeping HTTP connections alive in API server\n    \"VLLM_HTTP_TIMEOUT_KEEP_ALIVE\": lambda: int(\n        os.environ.get(\"VLLM_HTTP_TIMEOUT_KEEP_ALIVE\", \"5\")\n    ),\n    # a list of plugin names to load, separated by commas.\n    # if this is not set, it means all plugins will be loaded\n    # if this is set to an empty string, no plugins will be loaded\n    \"VLLM_PLUGINS\": lambda: None\n    if \"VLLM_PLUGINS\" not in os.environ\n    else os.environ[\"VLLM_PLUGINS\"].split(\",\"),\n    # a local directory to look in for unrecognized LoRA adapters.\n    # only works if plugins are enabled and\n    # VLLM_ALLOW_RUNTIME_LORA_UPDATING is enabled.\n    \"VLLM_LORA_RESOLVER_CACHE_DIR\": lambda: os.getenv(\n        \"VLLM_LORA_RESOLVER_CACHE_DIR\", None\n    ),\n    # Enables torch CUDA profiling if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_CUDA_PROFILE\": lambda: os.getenv(\"VLLM_TORCH_CUDA_PROFILE\"),\n    # Enables torch profiler if set.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_DIR\": lambda: os.getenv(\"VLLM_TORCH_PROFILER_DIR\"),\n    # Enable torch profiler to record shapes if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_RECORD_SHAPES\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_RECORD_SHAPES\")\n    ),\n    # Enable torch profiler to profile memory if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_WITH_PROFILE_MEMORY\")\n    ),\n    # Enable torch profiler to profile stack if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_WITH_STACK\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_WITH_STACK\")\n    ),\n    # Enable torch profiler to profile flops if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_WITH_FLOPS\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_WITH_FLOPS\")\n    ),\n    # Disable torch profiling of the AsyncLLMEngine process if set to 1.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_DISABLE_ASYNC_LLM\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_DISABLE_ASYNC_LLM\")\n    ),\n    # Delay number of iterations before starting profiling when using\n    # the torch/torch CUDA profiler. If set to 0, will start profiling immediately.\n    # Deprecated, see profiler_config.\n    \"VLLM_PROFILER_DELAY_ITERS\": lambda: (os.getenv(\"VLLM_PROFILER_DELAY_ITERS\")),\n    # Maximum number of iterations to profile when using the torch/torch CUDA profiler.\n    # If set to 0, will not limit the number of iterations.\n    \"VLLM_PROFILER_MAX_ITERS\": lambda: os.getenv(\"VLLM_PROFILER_MAX_ITERS\"),\n    # Control whether torch profiler gzip-compresses profiling files.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_USE_GZIP\": lambda: os.getenv(\"VLLM_TORCH_PROFILER_USE_GZIP\"),\n    # Control whether torch profiler dumps the self_cuda_time_total table.\n    # Set to 0 to disable dumping the table.\n    # Deprecated, see profiler_config.\n    \"VLLM_TORCH_PROFILER_DUMP_CUDA_TIME_TOTAL\": lambda: (\n        os.getenv(\"VLLM_TORCH_PROFILER_DUMP_CUDA_TIME_TOTAL\")\n    ),\n    # If set, vLLM will use Triton implementations of AWQ.\n    \"VLLM_USE_TRITON_AWQ\": lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),\n    # If set, allow loading or unloading lora adapters in runtime,\n    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\": lambda: (\n        os.environ.get(\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\", \"0\").strip().lower()\n        in (\"1\", \"true\")\n    ),\n    # We assume drivers can report p2p status correctly.\n    # If the program hangs when using custom allreduce,\n    # potantially caused by a bug in the driver (535 series),\n    # if might be helpful to set VLLM_SKIP_P2P_CHECK=0\n    # so that vLLM can verify if p2p is actually working.\n    # See https://github.com/vllm-project/vllm/blob/a9b15c606fea67a072416ea0ea115261a2756058/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L101-L108 for details. # noqa\n    \"VLLM_SKIP_P2P_CHECK\": lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"1\") == \"1\",\n    # List of quantization kernels that should be disabled, used for testing\n    # and performance comparisons. Currently only affects MPLinearKernel\n    # selection\n    # (kernels: MacheteLinearKernel, MarlinLinearKernel, ExllamaLinearKernel)\n    \"VLLM_DISABLED_KERNELS\": lambda: []\n    if \"VLLM_DISABLED_KERNELS\" not in os.environ\n    else os.environ[\"VLLM_DISABLED_KERNELS\"].split(\",\"),\n    # Disable pynccl (using torch.distributed instead)\n    \"VLLM_DISABLE_PYNCCL\": lambda: (\n        os.getenv(\"VLLM_DISABLE_PYNCCL\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # Disable aiter ops unless specifically enabled.\n    # Acts as a parent switch to enable the rest of the other operations.\n    \"VLLM_ROCM_USE_AITER\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter paged attention.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_PAGED_ATTN\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_PAGED_ATTN\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # use aiter linear op if aiter ops are enabled\n    # The following list of related ops\n    # - scaled_mm (per-tensor / rowwise)\n    \"VLLM_ROCM_USE_AITER_LINEAR\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_LINEAR\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter moe ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_MOE\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_MOE\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # use aiter rms norm op if aiter ops are enabled.\n    \"VLLM_ROCM_USE_AITER_RMSNORM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_RMSNORM\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter mla ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_MLA\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_MLA\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter mha ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_MHA\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_MHA\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter fp4 gemm asm.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_FP4_ASM_GEMM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_FP4_ASM_GEMM\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter rope.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_TRITON_ROPE\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_TRITON_ROPE\", \"False\").lower() in (\"true\", \"1\")\n    ),\n    # Whether to use aiter triton fp8 bmm kernel\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_FP8BMM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_FP8BMM\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Use AITER triton unified attention for V1 attention\n    \"VLLM_ROCM_USE_AITER_UNIFIED_ATTENTION\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_UNIFIED_ATTENTION\", \"False\").lower()\n        in (\"true\", \"1\")\n    ),\n    # Whether to use aiter fusion shared experts ops.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS\", \"False\").lower()\n        in (\"true\", \"1\")\n    ),\n    # Whether to use aiter triton kernels for gemm ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_TRITON_GEMM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_AITER_TRITON_GEMM\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # use rocm skinny gemms\n    \"VLLM_ROCM_USE_SKINNY_GEMM\": lambda: (\n        os.getenv(\"VLLM_ROCM_USE_SKINNY_GEMM\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Pad the fp8 weights to 256 bytes for ROCm\n    \"VLLM_ROCM_FP8_PADDING\": lambda: bool(int(os.getenv(\"VLLM_ROCM_FP8_PADDING\", \"1\"))),\n    # Pad the weights for the moe kernel\n    \"VLLM_ROCM_MOE_PADDING\": lambda: bool(int(os.getenv(\"VLLM_ROCM_MOE_PADDING\", \"1\"))),\n    # custom paged attention kernel for MI3* cards\n    \"VLLM_ROCM_CUSTOM_PAGED_ATTN\": lambda: (\n        os.getenv(\"VLLM_ROCM_CUSTOM_PAGED_ATTN\", \"True\").lower() in (\"true\", \"1\")\n    ),\n    # Custom quick allreduce kernel for MI3* cards\n    # Choice of quantization level: FP, INT8, INT6, INT4 or NONE\n    # Recommended for large models to get allreduce\n    \"VLLM_ROCM_QUICK_REDUCE_QUANTIZATION\": env_with_choices(\n        \"VLLM_ROCM_QUICK_REDUCE_QUANTIZATION\",\n        \"NONE\",\n        [\"FP\", \"INT8\", \"INT6\", \"INT4\", \"NONE\"],\n    ),\n    # Custom quick allreduce kernel for MI3* cards\n    # Due to the lack of the bfloat16 asm instruction, bfloat16\n    # kernels are slower than fp16,\n    # If environment variable is set to 1, the input is converted to fp16\n    \"VLLM_ROCM_QUICK_REDUCE_CAST_BF16_TO_FP16\": lambda: (\n        os.getenv(\"VLLM_ROCM_QUICK_REDUCE_CAST_BF16_TO_FP16\", \"True\").lower()\n        in (\"true\", \"1\")\n    ),\n    # Custom quick allreduce kernel for MI3* cards.\n    # Controls the maximum allowed number of data bytes(MB) for custom quick\n    # allreduce communication.\n    # Default: 2048 MB.\n    # Data exceeding this size will use either custom allreduce or RCCL\n    # communication.\n    \"VLLM_ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB\": lambda: maybe_convert_int(\n        os.environ.get(\"VLLM_ROCM_QUICK_REDUCE_MAX_SIZE_BYTES_MB\", None)\n    ),\n    # Divisor for dynamic query scale factor calculation for FP8 KV Cache\n    \"Q_SCALE_CONSTANT\": lambda: int(os.getenv(\"Q_SCALE_CONSTANT\", \"200\")),\n    # Divisor for dynamic key scale factor calculation for FP8 KV Cache\n    \"K_SCALE_CONSTANT\": lambda: int(os.getenv(\"K_SCALE_CONSTANT\", \"200\")),\n    # Divisor for dynamic value scale factor calculation for FP8 KV Cache\n    \"V_SCALE_CONSTANT\": lambda: int(os.getenv(\"V_SCALE_CONSTANT\", \"100\")),\n    # If set, enable multiprocessing in LLM for the V1 code path.\n    \"VLLM_ENABLE_V1_MULTIPROCESSING\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_V1_MULTIPROCESSING\", \"1\"))\n    ),\n    \"VLLM_LOG_BATCHSIZE_INTERVAL\": lambda: float(\n        os.getenv(\"VLLM_LOG_BATCHSIZE_INTERVAL\", \"-1\")\n    ),\n    \"VLLM_DISABLE_COMPILE_CACHE\": disable_compile_cache,\n    # If set, vllm will run in development mode, which will enable\n    # some additional endpoints for developing and debugging,\n    # e.g. `/reset_prefix_cache`\n    \"VLLM_SERVER_DEV_MODE\": lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),\n    # Controls the maximum number of requests to handle in a\n    # single asyncio task when processing per-token outputs in the\n    # V1 AsyncLLM interface. It is applicable when handling a high\n    # concurrency of streaming requests.\n    # Setting this too high can result in a higher variance of\n    # inter-message latencies. Setting it too low can negatively impact\n    # TTFT and overall throughput.\n    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\": lambda: int(\n        os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")\n    ),\n    # If set, vLLM will disable the MLA attention optimizations.\n    \"VLLM_MLA_DISABLE\": lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE\", \"0\"))),\n    # If set, vLLM will pick up the provided Flash Attention MLA\n    # max number splits for cuda graph decode\n    \"VLLM_FLASH_ATTN_MAX_NUM_SPLITS_FOR_CUDA_GRAPH\": lambda: int(\n        os.getenv(\"VLLM_FLASH_ATTN_MAX_NUM_SPLITS_FOR_CUDA_GRAPH\", \"32\")\n    ),\n    # Number of GPUs per worker in Ray, if it is set to be a fraction,\n    # it allows ray to schedule multiple actors on a single GPU,\n    # so that users can colocate other actors on the same GPUs as vLLM.\n    \"VLLM_RAY_PER_WORKER_GPUS\": lambda: float(\n        os.getenv(\"VLLM_RAY_PER_WORKER_GPUS\", \"1.0\")\n    ),\n    # Bundle indices for Ray, if it is set, it can control precisely\n    # which indices are used for the Ray bundle, for every worker.\n    # Format: comma-separated list of integers, e.g. \"0,1,2,3\"\n    \"VLLM_RAY_BUNDLE_INDICES\": lambda: os.getenv(\"VLLM_RAY_BUNDLE_INDICES\", \"\"),\n    # In some system, find_loaded_library() may not work. So we allow users to\n    # specify the path through environment variable VLLM_CUDART_SO_PATH.\n    \"VLLM_CUDART_SO_PATH\": lambda: os.getenv(\"VLLM_CUDART_SO_PATH\", None),\n    # Rank of the process in the data parallel setting\n    \"VLLM_DP_RANK\": lambda: int(os.getenv(\"VLLM_DP_RANK\", \"0\")),\n    # Rank of the process in the data parallel setting.\n    # Defaults to VLLM_DP_RANK when not set.\n    \"VLLM_DP_RANK_LOCAL\": lambda: int(\n        os.getenv(\"VLLM_DP_RANK_LOCAL\", sys.modules[__name__].VLLM_DP_RANK)\n    ),\n    # World size of the data parallel setting\n    \"VLLM_DP_SIZE\": lambda: int(os.getenv(\"VLLM_DP_SIZE\", \"1\")),\n    # IP address of the master node in the data parallel setting\n    \"VLLM_DP_MASTER_IP\": lambda: os.getenv(\"VLLM_DP_MASTER_IP\", \"127.0.0.1\"),\n    # Port of the master node in the data parallel setting\n    \"VLLM_DP_MASTER_PORT\": lambda: int(os.getenv(\"VLLM_DP_MASTER_PORT\", \"0\")),\n    # In the context of executing MoE models with Data-Parallel, Expert-Parallel\n    # and Batched All-to-All dispatch/combine kernels, VLLM_MOE_DP_CHUNK_SIZE\n    # dictates the quantum of tokens that can be dispatched from a DP\n    # rank. All DP ranks process the activations in VLLM_MOE_DP_CHUNK_SIZE\n    # units.\n    \"VLLM_MOE_DP_CHUNK_SIZE\": lambda: int(os.getenv(\"VLLM_MOE_DP_CHUNK_SIZE\", \"256\")),\n    \"VLLM_ENABLE_MOE_DP_CHUNK\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_MOE_DP_CHUNK\", \"1\"))\n    ),\n    # Randomize inputs during dummy runs when using Data Parallel\n    \"VLLM_RANDOMIZE_DP_DUMMY_INPUTS\": lambda: os.environ.get(\n        \"VLLM_RANDOMIZE_DP_DUMMY_INPUTS\", \"0\"\n    )\n    == \"1\",\n    # Strategy to pack the data parallel ranks for Ray.\n    # Available options:\n    # - \"fill\":\n    #   for DP master node, allocate exactly data-parallel-size-local DP ranks,\n    #   for non-master nodes, allocate as many DP ranks as can fit;\n    # - \"strict\":\n    #   allocate exactly data-parallel-size-local DP ranks to each picked node;\n    # - \"span\":\n    #   Should be used only when a single DP rank requires multiple nodes.\n    #   allocate one DP rank over as many nodes as required for set world_size;\n    # This environment variable is ignored if data-parallel-backend is not Ray.\n    \"VLLM_RAY_DP_PACK_STRATEGY\": lambda: os.getenv(\n        \"VLLM_RAY_DP_PACK_STRATEGY\", \"strict\"\n    ),\n    # Whether to use S3 path for model loading in CI via RunAI Streamer\n    \"VLLM_CI_USE_S3\": lambda: os.environ.get(\"VLLM_CI_USE_S3\", \"0\") == \"1\",\n    # Use model_redirect to redirect the model name to a local folder.\n    # `model_redirect` can be a json file mapping the model between\n    # repo_id and local folder:\n    # {\"meta-llama/Llama-3.2-1B\": \"/tmp/Llama-3.2-1B\"}\n    # or a space separated values table file:\n    # meta-llama/Llama-3.2-1B   /tmp/Llama-3.2-1B\n    \"VLLM_MODEL_REDIRECT_PATH\": lambda: os.environ.get(\n        \"VLLM_MODEL_REDIRECT_PATH\", None\n    ),\n    # Whether to use atomicAdd reduce in gptq/awq marlin kernel.\n    \"VLLM_MARLIN_USE_ATOMIC_ADD\": lambda: os.environ.get(\n        \"VLLM_MARLIN_USE_ATOMIC_ADD\", \"0\"\n    )\n    == \"1\",\n    # Whether to use marlin kernel in mxfp4 quantization method\n    \"VLLM_MXFP4_USE_MARLIN\": lambda: maybe_convert_bool(\n        os.environ.get(\"VLLM_MXFP4_USE_MARLIN\", None)\n    ),\n    # The activation dtype for marlin kernel\n    \"VLLM_MARLIN_INPUT_DTYPE\": env_with_choices(\n        \"VLLM_MARLIN_INPUT_DTYPE\", None, [\"int8\", \"fp8\"]\n    ),\n    # Whether to use DeepEPLL kernels for NVFP4 quantization and dispatch method\n    # only supported on Blackwell GPUs and with\n    # https://github.com/deepseek-ai/DeepEP/pull/341\n    \"VLLM_DEEPEPLL_NVFP4_DISPATCH\": lambda: bool(\n        int(os.getenv(\"VLLM_DEEPEPLL_NVFP4_DISPATCH\", \"0\"))\n    ),\n    # Whether to turn on the outlines cache for V1\n    # This cache is unbounded and on disk, so it's not safe to use in\n    # an environment with potentially malicious users.\n    \"VLLM_V1_USE_OUTLINES_CACHE\": lambda: os.environ.get(\n        \"VLLM_V1_USE_OUTLINES_CACHE\", \"0\"\n    )\n    == \"1\",\n    # Gap between padding buckets for the forward pass. So we have\n    # 8, we will run forward pass with [16, 24, 32, ...].\n    \"VLLM_TPU_BUCKET_PADDING_GAP\": lambda: int(\n        os.environ[\"VLLM_TPU_BUCKET_PADDING_GAP\"]\n    )\n    if \"VLLM_TPU_BUCKET_PADDING_GAP\" in os.environ\n    else 0,\n    \"VLLM_TPU_MOST_MODEL_LEN\": lambda: maybe_convert_int(\n        os.environ.get(\"VLLM_TPU_MOST_MODEL_LEN\", None)\n    ),\n    # Whether using Pathways\n    \"VLLM_TPU_USING_PATHWAYS\": lambda: bool(\n        \"proxy\" in os.getenv(\"JAX_PLATFORMS\", \"\").lower()\n    ),\n    # Allow use of DeepGemm kernels for fused moe ops.\n    \"VLLM_USE_DEEP_GEMM\": lambda: bool(int(os.getenv(\"VLLM_USE_DEEP_GEMM\", \"1\"))),\n    # Allow use of DeepGemm specifically for MoE fused ops (overrides only MoE).\n    \"VLLM_MOE_USE_DEEP_GEMM\": lambda: bool(\n        int(os.getenv(\"VLLM_MOE_USE_DEEP_GEMM\", \"1\"))\n    ),\n    # Whether to use E8M0 scaling when DeepGEMM is used on Blackwell GPUs.\n    \"VLLM_USE_DEEP_GEMM_E8M0\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_DEEP_GEMM_E8M0\", \"1\"))\n    ),\n    # DeepGemm JITs the kernels on-demand. The warmup attempts to make DeepGemm\n    # JIT all the required kernels before model execution so there is no\n    # JIT'ing in the hot-path. However, this warmup increases the engine\n    # startup time by a couple of minutes.\n    # Available options:\n    #  - \"skip\"  : Skip warmup.\n    #  - \"full\"  : Warmup deepgemm by running all possible gemm shapes the\n    #   engine could encounter.\n    #  - \"relax\" : Select gemm shapes to run based on some heuristics. The\n    #   heuristic aims to have the same effect as running all possible gemm\n    #   shapes, but provides no guarantees.\n    \"VLLM_DEEP_GEMM_WARMUP\": env_with_choices(\n        \"VLLM_DEEP_GEMM_WARMUP\",\n        \"relax\",\n        [\n            \"skip\",\n            \"full\",\n            \"relax\",\n        ],\n    ),\n    # Whether to use fused grouped_topk used for MoE expert selection.\n    \"VLLM_USE_FUSED_MOE_GROUPED_TOPK\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FUSED_MOE_GROUPED_TOPK\", \"1\"))\n    ),\n    # Allow use of FlashInfer MoE kernels for fused moe ops.\n    \"VLLM_USE_FLASHINFER_MOE_FP16\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP16\", \"0\"))\n    ),\n    # Allow use of FlashInfer MoE kernels for fused moe ops.\n    \"VLLM_USE_FLASHINFER_MOE_FP8\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP8\", \"0\"))\n    ),\n    # Allow use of FlashInfer CUTLASS kernels for fused moe ops.\n    \"VLLM_USE_FLASHINFER_MOE_FP4\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP4\", \"0\"))\n    ),\n    # If set to 1, use the FlashInfer\n    # MXFP8 (activation) x MXFP4 (weight) MoE backend.\n    \"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8\", \"0\"))\n    ),\n    # If set to 1, use the FlashInfer CUTLASS backend for\n    # MXFP8 (activation) x MXFP4 (weight) MoE.\n    # This is separate from the TRTLLMGEN path controlled by\n    # VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8.\n    \"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8_CUTLASS\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8_CUTLASS\", \"0\"))\n    ),\n    # If set to 1, use the FlashInfer\n    # BF16 (activation) x MXFP4 (weight) MoE backend.\n    \"VLLM_USE_FLASHINFER_MOE_MXFP4_BF16\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_MXFP4_BF16\", \"0\"))\n    ),\n    # Control the cache sized used by the xgrammar compiler. The default\n    # of 512 MB should be enough for roughly 1000 JSON schemas.\n    # It can be changed with this variable if needed for some reason.\n    \"VLLM_XGRAMMAR_CACHE_MB\": lambda: int(os.getenv(\"VLLM_XGRAMMAR_CACHE_MB\", \"512\")),\n    # Control the threshold for msgspec to use 'zero copy' for\n    # serialization/deserialization of tensors. Tensors below\n    # this limit will be encoded into the msgpack buffer, and\n    # tensors above will instead be sent via a separate message.\n    # While the sending side still actually copies the tensor\n    # in all cases, on the receiving side, tensors above this\n    # limit will actually be zero-copy decoded.\n    \"VLLM_MSGPACK_ZERO_COPY_THRESHOLD\": lambda: int(\n        os.getenv(\"VLLM_MSGPACK_ZERO_COPY_THRESHOLD\", \"256\")\n    ),\n    # If set, allow insecure serialization using pickle.\n    # This is useful for environments where it is deemed safe to use the\n    # insecure method and it is needed for some reason.\n    \"VLLM_ALLOW_INSECURE_SERIALIZATION\": lambda: bool(\n        int(os.getenv(\"VLLM_ALLOW_INSECURE_SERIALIZATION\", \"0\"))\n    ),\n    # IP address used for NIXL handshake between remote agents.\n    \"VLLM_NIXL_SIDE_CHANNEL_HOST\": lambda: os.getenv(\n        \"VLLM_NIXL_SIDE_CHANNEL_HOST\", \"localhost\"\n    ),\n    # Port used for NIXL handshake between remote agents.\n    \"VLLM_NIXL_SIDE_CHANNEL_PORT\": lambda: int(\n        os.getenv(\"VLLM_NIXL_SIDE_CHANNEL_PORT\", \"5600\")\n    ),\n    # Port used for Mooncake handshake between remote agents.\n    \"VLLM_MOONCAKE_BOOTSTRAP_PORT\": lambda: int(\n        os.getenv(\"VLLM_MOONCAKE_BOOTSTRAP_PORT\", \"8998\")\n    ),\n    # [DEPRECATED - will be removed in v0.15.0] all2all backend for vllm's\n    # expert parallel communication. Use --all2all-backend CLI argument instead.\n    # Available options:\n    # - \"naive\": naive all2all implementation using broadcasts\n    # - \"allgather_reducescatter\": all2all implementation based on allgather and\n    #  reducescatter\n    # - \"pplx\": use pplx kernels\n    # - \"deepep_high_throughput\", use deepep high-throughput kernels\n    # - \"deepep_low_latency\", use deepep low-latency kernels\n    # - \"flashinfer_all2allv\", use flashinfer alltoallv kernels for mnnvl\n    \"VLLM_ALL2ALL_BACKEND\": env_with_choices(\n        \"VLLM_ALL2ALL_BACKEND\",\n        None,\n        [\n            \"naive\",\n            \"pplx\",\n            \"deepep_high_throughput\",\n            \"deepep_low_latency\",\n            \"allgather_reducescatter\",\n            \"flashinfer_all2allv\",\n        ],\n    ),\n    # Flashinfer MoE backend for vLLM's fused Mixture-of-Experts support.\n    # Both require compute capability 10.0 or above.\n    # Available options:\n    # - \"throughput\":  [default]\n    #     Uses CUTLASS kernels optimized for high-throughput batch inference.\n    # - \"latency\":\n    #     Uses TensorRT-LLM kernels optimized for low-latency inference.\n    \"VLLM_FLASHINFER_MOE_BACKEND\": env_with_choices(\n        \"VLLM_FLASHINFER_MOE_BACKEND\",\n        \"latency\",\n        [\"throughput\", \"latency\", \"masked_gemm\"],\n    ),\n    # Control the workspace buffer size for the FlashInfer backend.\n    \"VLLM_FLASHINFER_WORKSPACE_BUFFER_SIZE\": lambda: int(\n        os.getenv(\"VLLM_FLASHINFER_WORKSPACE_BUFFER_SIZE\", str(394 * 1024 * 1024))\n    ),\n    # Control the maximum number of tokens per expert supported by the\n    # NVFP4 MoE CUTLASS Kernel. This value is used to create a buffer for\n    # the blockscale tensor of activations NVFP4 Quantization.\n    # This is used to prevent the kernel from running out of memory.\n    \"VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE\": lambda: int(\n        os.getenv(\"VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE\", \"163840\")\n    ),\n    # Specifies the thresholds of the communicated tensor sizes under which\n    # vllm should use flashinfer fused allreduce. The variable should be a\n    # JSON with the following format:\n    #     { <world size>: <max size in mb> }\n    # Unspecified world sizes will fall back to\n    #     { 2: 64, 4: 1, <everything else>: 0.5 }\n    \"VLLM_FLASHINFER_ALLREDUCE_FUSION_THRESHOLDS_MB\": lambda: json.loads(\n        os.getenv(\"VLLM_FLASHINFER_ALLREDUCE_FUSION_THRESHOLDS_MB\", \"{}\")\n    ),\n    # MoE routing strategy selector.\n    # See `RoutingSimulator.get_available_strategies()` # for available\n    # strategies.\n    # Custom routing strategies can be registered by\n    # RoutingSimulator.register_strategy()\n    # Note: custom strategies may not produce correct model outputs\n    \"VLLM_MOE_ROUTING_SIMULATION_STRATEGY\": lambda: os.environ.get(\n        \"VLLM_MOE_ROUTING_SIMULATION_STRATEGY\", \"\"\n    ).lower(),\n    # Regex timeout for use by the vLLM tool parsing plugins.\n    \"VLLM_TOOL_PARSE_REGEX_TIMEOUT_SECONDS\": lambda: int(\n        os.getenv(\"VLLM_TOOL_PARSE_REGEX_TIMEOUT_SECONDS\", \"1\")\n    ),\n    # Reduce CPU usage when vLLM is idle. Enabling this will incur small\n    # latency penalty when a request eventually comes.\n    \"VLLM_SLEEP_WHEN_IDLE\": lambda: bool(int(os.getenv(\"VLLM_SLEEP_WHEN_IDLE\", \"0\"))),\n    # Control the max chunk bytes (in MB) for the rpc message queue.\n    # Object larger than this threshold will be broadcast to worker\n    # processes via zmq.\n    \"VLLM_MQ_MAX_CHUNK_BYTES_MB\": lambda: int(\n        os.getenv(\"VLLM_MQ_MAX_CHUNK_BYTES_MB\", \"16\")\n    ),\n    # Timeout in seconds for execute_model RPC calls in multiprocessing\n    # executor (only applies when TP > 1).\n    \"VLLM_EXECUTE_MODEL_TIMEOUT_SECONDS\": lambda: int(\n        os.getenv(\"VLLM_EXECUTE_MODEL_TIMEOUT_SECONDS\", \"300\")\n    ),\n    # KV Cache layout used throughout vllm.\n    # Some common values are:\n    # - NHD\n    # - HND\n    # Where N=num_blocks, H=num_heads and D=head_size. The default value will\n    # leave the layout choice to the backend. Mind that backends may only\n    # implement and support a subset of all possible layouts.\n    \"VLLM_KV_CACHE_LAYOUT\": env_with_choices(\n        \"VLLM_KV_CACHE_LAYOUT\", None, [\"NHD\", \"HND\"]\n    ),\n    # Enable checking whether the generated logits contain NaNs,\n    # indicating corrupted output. Useful for debugging low level bugs\n    # or bad hardware but it may add compute overhead.\n    \"VLLM_COMPUTE_NANS_IN_LOGITS\": lambda: bool(\n        int(os.getenv(\"VLLM_COMPUTE_NANS_IN_LOGITS\", \"0\"))\n    ),\n    # Controls whether or not emulations are used for NVFP4\n    # generations on machines < 100 for compressed-tensors\n    # models\n    \"VLLM_USE_NVFP4_CT_EMULATIONS\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_NVFP4_CT_EMULATIONS\", \"0\"))\n    ),\n    # Time (in seconds) after which the KV cache on the producer side is\n    # automatically cleared if no READ notification is received from the\n    # consumer. This is only applicable when using NixlConnector in a\n    # disaggregated decode-prefill setup.\n    \"VLLM_NIXL_ABORT_REQUEST_TIMEOUT\": lambda: int(\n        os.getenv(\"VLLM_NIXL_ABORT_REQUEST_TIMEOUT\", \"480\")\n    ),\n    # Timeout (in seconds) for MooncakeConnector in PD disaggregated setup.\n    \"VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT\": lambda: int(\n        os.getenv(\"VLLM_MOONCAKE_ABORT_REQUEST_TIMEOUT\", \"480\")\n    ),\n    # Controls whether or not to use cudnn prefill\n    \"VLLM_USE_CUDNN_PREFILL\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_CUDNN_PREFILL\", \"0\"))\n    ),\n    # Controls whether to use TRT-LLM ragged DeepSeek prefill\n    \"VLLM_USE_TRTLLM_RAGGED_DEEPSEEK_PREFILL\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_TRTLLM_RAGGED_DEEPSEEK_PREFILL\", \"0\"))\n    ),\n    # If set to 1/True, use the TRTLLM attention backend in flashinfer.\n    # If set to 0/False, use the default attention backend in flashinfer.\n    # If not set, auto-detect the attention backend in flashinfer.\n    \"VLLM_USE_TRTLLM_ATTENTION\": lambda: (\n        None\n        if \"VLLM_USE_TRTLLM_ATTENTION\" not in os.environ\n        else os.environ[\"VLLM_USE_TRTLLM_ATTENTION\"].lower() in (\"1\", \"true\")\n    ),\n    # If set to 1, when we use fp8 kv, we do not quantize Q to fp8\n    \"VLLM_FLASHINFER_DISABLE_Q_QUANTIZATION\": lambda: bool(\n        int(os.getenv(\"VLLM_FLASHINFER_DISABLE_Q_QUANTIZATION\", \"0\"))\n    ),\n    # If set, it means we pre-downloaded cubin files and flashinfer will\n    # read the cubin files directly.\n    \"VLLM_HAS_FLASHINFER_CUBIN\": lambda: bool(\n        int(os.getenv(\"VLLM_HAS_FLASHINFER_CUBIN\", \"0\"))\n    ),\n    # Supported options:\n    # - \"flashinfer-cudnn\": use flashinfer cudnn GEMM backend\n    # - \"flashinfer-trtllm\": use flashinfer trtllm GEMM backend\n    # - \"flashinfer-cutlass\": use flashinfer cutlass GEMM backend\n    # - <none>: automatically pick an available backend\n    \"VLLM_NVFP4_GEMM_BACKEND\": env_with_choices(\n        \"VLLM_NVFP4_GEMM_BACKEND\",\n        None,\n        [\"flashinfer-cudnn\", \"flashinfer-trtllm\", \"flashinfer-cutlass\", \"cutlass\"],\n    ),\n    # Controls garbage collection during CUDA graph capture.\n    # If set to 0 (default), enables GC freezing to speed up capture time.\n    # If set to 1, allows GC to run during capture.\n    \"VLLM_ENABLE_CUDAGRAPH_GC\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_CUDAGRAPH_GC\", \"0\"))\n    ),\n    # Used to force set up loopback IP\n    \"VLLM_LOOPBACK_IP\": lambda: os.getenv(\"VLLM_LOOPBACK_IP\", \"\"),\n    # Used to set the process name prefix for vLLM processes.\n    # This is useful for debugging and monitoring purposes.\n    # The default value is \"VLLM\".\n    \"VLLM_PROCESS_NAME_PREFIX\": lambda: os.getenv(\"VLLM_PROCESS_NAME_PREFIX\", \"VLLM\"),\n    # Allow chunked local attention with hybrid kv cache manager.\n    # Currently using the Hybrid KV cache manager with chunked local attention\n    # in the Llama4 models (the only models currently using chunked local attn)\n    # causes a latency regression. For this reason, we disable it by default.\n    # This flag is used to allow users to enable it if they want to (to save on\n    # kv-cache memory usage and enable longer contexts)\n    # TODO(lucas): Remove this flag once latency regression is resolved.\n    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\": lambda: bool(\n        int(os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"1\"))\n    ),\n    # Enables support for the \"store\" option in the OpenAI Responses API.\n    # When set to 1, vLLM's OpenAI server will retain the input and output\n    # messages for those requests in memory. By default, this is disabled (0),\n    # and the \"store\" option is ignored.\n    # NOTE/WARNING:\n    # 1. Messages are kept in memory only (not persisted to disk) and will be\n    #    lost when the vLLM server shuts down.\n    # 2. Enabling this option will cause a memory leak, as stored messages are\n    #    never removed from memory until the server terminates.\n    \"VLLM_ENABLE_RESPONSES_API_STORE\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_RESPONSES_API_STORE\", \"0\"))\n    ),\n    # If set, use the fp8 mfma in rocm paged attention.\n    \"VLLM_ROCM_FP8_MFMA_PAGE_ATTN\": lambda: bool(\n        int(os.getenv(\"VLLM_ROCM_FP8_MFMA_PAGE_ATTN\", \"0\"))\n    ),\n    # Whether to use pytorch symmetric memory for allreduce\n    \"VLLM_ALLREDUCE_USE_SYMM_MEM\": lambda: bool(\n        int(os.getenv(\"VLLM_ALLREDUCE_USE_SYMM_MEM\", \"1\"))\n    ),\n    # Experimental: use this to enable MCP tool calling for non harmony models\n    \"VLLM_USE_EXPERIMENTAL_PARSER_CONTEXT\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_EXPERIMENTAL_PARSER_CONTEXT\", \"0\"))\n    ),\n    # Allows vllm to find tuned config under customized folder\n    \"VLLM_TUNED_CONFIG_FOLDER\": lambda: os.getenv(\"VLLM_TUNED_CONFIG_FOLDER\", None),\n    # Valid values are container,code_interpreter,web_search_preview\n    # ex VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS=container,code_interpreter\n    # If the server_label of your mcp tool is not in this list it will\n    # be completely ignored.\n    \"VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS\": env_set_with_choices(\n        \"VLLM_GPT_OSS_SYSTEM_TOOL_MCP_LABELS\",\n        default=[],\n        choices=[\"container\", \"code_interpreter\", \"web_search_preview\"],\n    ),\n    # Allows harmony instructions to be injected on system messages\n    \"VLLM_GPT_OSS_HARMONY_SYSTEM_INSTRUCTIONS\": lambda: bool(\n        int(os.getenv(\"VLLM_GPT_OSS_HARMONY_SYSTEM_INSTRUCTIONS\", \"0\"))\n    ),\n    # Enable automatic retry when tool call JSON parsing fails\n    # If enabled, returns an error message to the model to retry\n    # If disabled (default), raises an exception and fails the request\n    \"VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY\": lambda: bool(\n        int(os.getenv(\"VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY\", \"0\"))\n    ),\n    # Add optional custom scopes for profiling, disable to avoid overheads\n    \"VLLM_CUSTOM_SCOPES_FOR_PROFILING\": lambda: bool(\n        int(os.getenv(\"VLLM_CUSTOM_SCOPES_FOR_PROFILING\", \"0\"))\n    ),\n    # Add optional nvtx scopes for profiling, disable to avoid overheads\n    \"VLLM_NVTX_SCOPES_FOR_PROFILING\": lambda: bool(\n        int(os.getenv(\"VLLM_NVTX_SCOPES_FOR_PROFILING\", \"0\"))\n    ),\n    # Represent block hashes in KV cache events as 64-bit integers instead of\n    # raw bytes. Defaults to True for backward compatibility.\n    \"VLLM_KV_EVENTS_USE_INT_BLOCK_HASHES\": lambda: bool(\n        int(os.getenv(\"VLLM_KV_EVENTS_USE_INT_BLOCK_HASHES\", \"1\"))\n    ),\n    # Name of the shared memory buffer used for object storage.\n    # Only effective when mm_config.mm_processor_cache_type == \"shm\".\n    \"VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME\": lambda: os.getenv(\n        \"VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME\", \"VLLM_OBJECT_STORAGE_SHM_BUFFER\"\n    ),\n    # The size in MB of the buffers (NVL and RDMA) used by DeepEP\n    \"VLLM_DEEPEP_BUFFER_SIZE_MB\": lambda: int(\n        os.getenv(\"VLLM_DEEPEP_BUFFER_SIZE_MB\", \"1024\")\n    ),\n    # Force DeepEP to use intranode kernel for inter-node communication in\n    # high throughput mode. This is useful archive higher prefill throuhgput\n    # on system supports multi-node nvlink (e.g GB200).\n    \"VLLM_DEEPEP_HIGH_THROUGHPUT_FORCE_INTRA_NODE\": lambda: bool(\n        int(os.getenv(\"VLLM_DEEPEP_HIGH_THROUGHPUT_FORCE_INTRA_NODE\", \"0\"))\n    ),\n    # Allow DeepEP to use MNNVL (multi-node nvlink) for internode_ll kernel,\n    # turn this for better latency on GB200 like system\n    \"VLLM_DEEPEP_LOW_LATENCY_USE_MNNVL\": lambda: bool(\n        int(os.getenv(\"VLLM_DEEPEP_LOW_LATENCY_USE_MNNVL\", \"0\"))\n    ),\n    # The number of SMs to allocate for communication kernels when running DBO\n    # the rest of the SMs on the device will be allocated to compute\n    \"VLLM_DBO_COMM_SMS\": lambda: int(os.getenv(\"VLLM_DBO_COMM_SMS\", \"20\")),\n    # Enable max_autotune & coordinate_descent_tuning in inductor_config\n    # to compile static shapes passed from compile_sizes in compilation_config\n    # If set to 1, enable max_autotune; By default, this is enabled (1)\n    \"VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE\", \"1\"))\n    ),\n    # If set to 1, enable coordinate_descent_tuning;\n    # By default, this is enabled (1)\n    \"VLLM_ENABLE_INDUCTOR_COORDINATE_DESCENT_TUNING\": lambda: bool(\n        int(os.getenv(\"VLLM_ENABLE_INDUCTOR_COORDINATE_DESCENT_TUNING\", \"1\"))\n    ),\n    # Flag to enable NCCL symmetric memory allocation and registration\n    \"VLLM_USE_NCCL_SYMM_MEM\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_NCCL_SYMM_MEM\", \"0\"))\n    ),\n    # NCCL header path\n    \"VLLM_NCCL_INCLUDE_PATH\": lambda: os.environ.get(\"VLLM_NCCL_INCLUDE_PATH\", None),\n    # Flag to enable FBGemm kernels on model execution\n    \"VLLM_USE_FBGEMM\": lambda: bool(int(os.getenv(\"VLLM_USE_FBGEMM\", \"0\"))),\n    # GC debug config\n    # - VLLM_GC_DEBUG=0: disable GC debugger\n    # - VLLM_GC_DEBUG=1: enable GC debugger with gc.collect elpased times\n    # - VLLM_GC_DEBUG='{\"top_objects\":5}': enable GC debugger with\n    #                                      top 5 collected objects\n    \"VLLM_GC_DEBUG\": lambda: os.getenv(\"VLLM_GC_DEBUG\", \"\"),\n    # Debug workspace allocations.\n    # logging of workspace resize operations.\n    \"VLLM_DEBUG_WORKSPACE\": lambda: bool(int(os.getenv(\"VLLM_DEBUG_WORKSPACE\", \"0\"))),\n    # Disables parallel execution of shared_experts via separate cuda stream\n    \"VLLM_DISABLE_SHARED_EXPERTS_STREAM\": lambda: bool(\n        int(os.getenv(\"VLLM_DISABLE_SHARED_EXPERTS_STREAM\", \"0\"))\n    ),\n    # Limits when we run shared_experts in a separate stream.\n    # We found out that for large batch sizes, the separate stream\n    # execution is not beneficial (most likely because of the input clone)\n    # TODO(alexm-redhat): Tune to be more dynamic based on GPU type\n    \"VLLM_SHARED_EXPERTS_STREAM_TOKEN_THRESHOLD\": lambda: int(\n        int(os.getenv(\"VLLM_SHARED_EXPERTS_STREAM_TOKEN_THRESHOLD\", 256))\n    ),\n    # Format for saving torch.compile cache artifacts\n    # - \"binary\": saves as binary file\n    #     Safe for multiple vllm serve processes accessing the same torch compile cache.\n    # - \"unpacked\": saves as directory structure (for inspection/debugging)\n    #     NOT multiprocess safe - race conditions may occur with multiple processes.\n    #     Allows viewing and setting breakpoints in Inductor's code output files.\n    \"VLLM_COMPILE_CACHE_SAVE_FORMAT\": env_with_choices(\n        \"VLLM_COMPILE_CACHE_SAVE_FORMAT\", \"binary\", [\"binary\", \"unpacked\"]\n    ),\n    # Flag to enable v2 model runner.\n    \"VLLM_USE_V2_MODEL_RUNNER\": lambda: bool(\n        int(os.getenv(\"VLLM_USE_V2_MODEL_RUNNER\", \"0\"))\n    ),\n    # Debug logging for --enable-mfu-metrics\n    \"VLLM_DEBUG_MFU_METRICS\": lambda: bool(\n        int(os.getenv(\"VLLM_DEBUG_MFU_METRICS\", \"0\"))\n    ),\n}",
      "language": "sql"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}