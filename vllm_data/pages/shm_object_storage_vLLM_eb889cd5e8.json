{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
  "title": "shm_object_storage - vLLM",
  "content": "Deserialize bytes back to an object.\n\nSerialize an object to bytes.\n\nA single-writer, multiple-reader object storage system built on top of a shared memory ring buffer. Provides key-value storage with automatic memory management and cross-process serialization support.\n\nThis storage system follows a FIFO (First-In-First-Out) eviction policy where the oldest objects are automatically freed when memory runs low. Memory is reclaimed based on reader reference counting - objects are only freed when all readers have finished accessing them.\n\nArchitecture: - Single writer process can put(key, value) objects - Multiple reader processes can get(address, monotonic_id) objects - Built on SingleWriterShmRingBuffer for efficient shared memory management - Thread-safe operations with reader synchronization via locks\n\nKey Features: - FIFO Eviction: Oldest objects are evicted first when memory is full - Reference Counting: Objects are only freed when no readers are accessing them - Duplicate Key Handling: Existing keys are not overwritten, just re-referenced - Customized Serialization: By default uses Msgpack for efficient serialization of Python objects, but can be extended for custom types - Cross-Process Safety: Uses shared memory with proper synchronization - Automatic Cleanup: Garbage collection happens transparently during allocation\n\nMemory Layout per Object: [4-byte reference_count][metadata_size][serialized_object_data]\n\nThread Safety: - Writer operations (put, clear) are single-threaded by design - Reader operations (get) are thread-safe with lock-based reference counting - Memory reclamation is handled exclusively by the writer process\n\nInitialize the object storage.\n\nMaximum size for a single object in bytes.\n\nNumber of reader processes that can access the storage.\n\nThe shared memory ring buffer for storing objects.\n\nSerializer/deserializer for objects.\n\nOptional lock for synchronizing reader access.\n\nRaises: ValueError: If reader_lock is None for readers.\n\nClear the object storage.\n\nDefault is_free function that checks if the first 4 bytes are zero. This indicates that the buffer is free.\n\nFree unused buffers in the ring buffer.\n\nGet the cached object by key if it exists.\n\nGet handle for sharing across processes.\n\nSet the in-use flag for the reader.\n\nSet the in-use flag for the writer.\n\nCheck if the object with the given key is cached.\n\nStore a key-value pair in the object storage. Attempts to free max_object_size bytes using FIFO order when the ring buffer runs out of space during a put() operation.\n\nString key to identify the object\n\nAny serializable Python object\n\nIf there's not enough space in the buffer\n\nIf the serialized object is too large\n\nIf the key already exists in the storage\n\nTouch an existing cached item to update its eviction status.\n\nFor writers (ShmObjectStoreSenderCache): Increment writer_flag For readers (ShmObjectStoreReceiverCache): Increment reader_count\n\nString key of the object to touch\n\nAddress of the object (only for readers)\n\nMonotonic ID of the object (only for readers)\n\nA single-writer, multiple-reader ring buffer implementation using shared memory. This class provides a thread-safe ring buffer where one process can write data while multiple processes/threads can read from it.\n\nArchitecture: - Uses shared memory for cross-process communication - Maintains metadata for each allocated buffer chunk in the writer process - Supports custom \"is_free_fn\" functions to determine when buffers can be reused - Each buffer chunk contains: [4-byte id][4-byte size][actual_data]\n\nKey Concepts: - monotonic_id_start/end: Track the range of active buffer IDs - data_buffer_start/end: Track the physical memory range in use - Automatic wraparound when reaching buffer end - Lazy garbage collection based on is_free_fn checks\n\nExample Usage Scenarios:\n\nScenario 1: Simple Linear Allocation\n\nScenario 2: Memory Reclamation\n\nScenario 3: Wraparound Allocation (continuing from Scenario 2)\n\nScenario 4: Error Handling - Out of Space\n\nThread Safety: - Single writer: Only one process/thread should write (allocate_buf) - Multiple readers: Multiple processes/threads can read (access_buf) - Reader synchronization handled by is_free_fn callback - Writer handles garbage collection (free_buf) based on reader feedback\n\nMemory Layout per Buffer Chunk: [4-byte monotonic_id][4-byte chunk_size][actual_data...] ^metadata_start ^data_start\n\nThe monotonic_id ensures data integrity - readers can verify they're accessing the correct data even after buffer wraparound or reuse.\n\nAllocate a buffer MD_SIZE + size bytes in the shared memory. Memory layout: [4-byte monotonic_id][4-byte size][buffer data...]\n\nConvert bytes back to an integer.\n\nClear the ring buffer.\n\nFree a buffer of the given size. This is a no-op in shared memory, but we need to keep track of the metadata.\n\nIf freed memory spreads across the end and start of the ring buffer, the actual freed memory will be in two segments. In this case there still might not be a contiguous space of nbytes available.\n\nThe size of the buffer to free. If None, frees the maximum size of the ring buffer.\n\nConvert an integer to bytes.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.distributed.device_communicators.shm_object_storage ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.logger"
    },
    {
      "level": "h2",
      "text": "MsgpackSerde ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.MsgpackSerde"
    },
    {
      "level": "h3",
      "text": "_mm_kwargs_item_cls instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.MsgpackSerde._mm_kwargs_item_cls"
    },
    {
      "level": "h3",
      "text": "encoder instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.MsgpackSerde.encoder"
    },
    {
      "level": "h3",
      "text": "mm_decoder instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.MsgpackSerde.mm_decoder"
    },
    {
      "level": "h3",
      "text": "tensor_decoder instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.MsgpackSerde.tensor_decoder"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.MsgpackSerde.__init__"
    },
    {
      "level": "h3",
      "text": "deserialize ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.MsgpackSerde.deserialize"
    },
    {
      "level": "h3",
      "text": "serialize ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.MsgpackSerde.serialize"
    },
    {
      "level": "h2",
      "text": "ObjectSerde ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ObjectSerde"
    },
    {
      "level": "h3",
      "text": "deserialize abstractmethod ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ObjectSerde.deserialize"
    },
    {
      "level": "h3",
      "text": "serialize abstractmethod ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ObjectSerde.serialize"
    },
    {
      "level": "h2",
      "text": "ShmObjectStorageHandle dataclass ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ShmObjectStorageHandle"
    },
    {
      "level": "h3",
      "text": "max_object_size instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ShmObjectStorageHandle.max_object_size"
    },
    {
      "level": "h3",
      "text": "n_readers instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ShmObjectStorageHandle.n_readers"
    },
    {
      "level": "h3",
      "text": "reader_lock instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ShmObjectStorageHandle.reader_lock"
    },
    {
      "level": "h3",
      "text": "ring_buffer_handle instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ShmObjectStorageHandle.ring_buffer_handle"
    },
    {
      "level": "h3",
      "text": "serde_class instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ShmObjectStorageHandle.serde_class"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.ShmObjectStorageHandle.__init__"
    },
    {
      "level": "h2",
      "text": "SingleWriterShmObjectStorage ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage"
    },
    {
      "level": "h3",
      "text": "_reader_lock instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage._reader_lock"
    },
    {
      "level": "h3",
      "text": "flag_bytes instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.flag_bytes"
    },
    {
      "level": "h3",
      "text": "id_index instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.id_index"
    },
    {
      "level": "h3",
      "text": "is_writer instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.is_writer"
    },
    {
      "level": "h3",
      "text": "key_index instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.key_index"
    },
    {
      "level": "h3",
      "text": "max_object_size instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.max_object_size"
    },
    {
      "level": "h3",
      "text": "n_readers instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.n_readers"
    },
    {
      "level": "h3",
      "text": "ring_buffer instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.ring_buffer"
    },
    {
      "level": "h3",
      "text": "ser_de instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.ser_de"
    },
    {
      "level": "h3",
      "text": "serde_class instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.serde_class"
    },
    {
      "level": "h3",
      "text": "writer_flag instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.writer_flag"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.__init__"
    },
    {
      "level": "h3",
      "text": "clear ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.clear"
    },
    {
      "level": "h3",
      "text": "copy_to_buffer ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.copy_to_buffer"
    },
    {
      "level": "h3",
      "text": "create_from_handle staticmethod ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.create_from_handle"
    },
    {
      "level": "h3",
      "text": "default_is_free_check ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.default_is_free_check"
    },
    {
      "level": "h3",
      "text": "free_unused ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.free_unused"
    },
    {
      "level": "h3",
      "text": "get ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.get"
    },
    {
      "level": "h3",
      "text": "get_cached ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.get_cached"
    },
    {
      "level": "h3",
      "text": "handle ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.handle"
    },
    {
      "level": "h3",
      "text": "increment_reader_flag ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.increment_reader_flag"
    },
    {
      "level": "h3",
      "text": "increment_writer_flag ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.increment_writer_flag"
    },
    {
      "level": "h3",
      "text": "is_cached ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.is_cached"
    },
    {
      "level": "h3",
      "text": "put ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.put"
    },
    {
      "level": "h3",
      "text": "touch ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmObjectStorage.touch"
    },
    {
      "level": "h2",
      "text": "SingleWriterShmRingBuffer ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer"
    },
    {
      "level": "h3",
      "text": "ID_MAX instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.ID_MAX"
    },
    {
      "level": "h3",
      "text": "ID_NBYTES instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.ID_NBYTES"
    },
    {
      "level": "h3",
      "text": "MD_SIZE instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.MD_SIZE"
    },
    {
      "level": "h3",
      "text": "SIZE_NBYTES instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.SIZE_NBYTES"
    },
    {
      "level": "h3",
      "text": "data_buffer_end instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.data_buffer_end"
    },
    {
      "level": "h3",
      "text": "data_buffer_size instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.data_buffer_size"
    },
    {
      "level": "h3",
      "text": "data_buffer_start instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.data_buffer_start"
    },
    {
      "level": "h3",
      "text": "is_writer instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.is_writer"
    },
    {
      "level": "h3",
      "text": "metadata instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.metadata"
    },
    {
      "level": "h3",
      "text": "monotonic_id_end instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.monotonic_id_end"
    },
    {
      "level": "h3",
      "text": "monotonic_id_start instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.monotonic_id_start"
    },
    {
      "level": "h3",
      "text": "shared_memory instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.shared_memory"
    },
    {
      "level": "h3",
      "text": "__del__ ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.__del__"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.__init__"
    },
    {
      "level": "h3",
      "text": "access_buf ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.access_buf"
    },
    {
      "level": "h3",
      "text": "allocate_buf ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.allocate_buf"
    },
    {
      "level": "h3",
      "text": "byte2int ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.byte2int"
    },
    {
      "level": "h3",
      "text": "clear ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.clear"
    },
    {
      "level": "h3",
      "text": "free_buf ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.free_buf"
    },
    {
      "level": "h3",
      "text": "handle ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.handle"
    },
    {
      "level": "h3",
      "text": "int2byte ¶",
      "id": "vllm.distributed.device_communicators.shm_object_storage.SingleWriterShmRingBuffer.int2byte"
    }
  ],
  "code_samples": [
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394",
      "language": "unknown"
    },
    {
      "code": "class MsgpackSerde(ObjectSerde):\n    def __init__(self):\n        # Delayed import to avoid circular dependency\n        from vllm.multimodal.inputs import MultiModalKwargsItem\n        from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder\n\n        self.encoder = MsgpackEncoder()\n        self.tensor_decoder = MsgpackDecoder(torch.Tensor, share_mem=False)\n        self.mm_decoder = MsgpackDecoder(MultiModalKwargsItem, share_mem=False)\n        self._mm_kwargs_item_cls = MultiModalKwargsItem\n\n    def serialize(self, value: Any) -> tuple[bytes | list[bytes], int, bytes, int]:\n        len_arr = None\n        if isinstance(value, (torch.Tensor, self._mm_kwargs_item_cls)):\n            type_name = type(value).__name__\n            value = self.encoder.encode(value)\n            len_arr = [len(s) for s in value]\n            nbytes = sum(len_arr)\n        else:\n            value = pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)\n            type_name = type(value).__name__\n            nbytes = len(value)\n\n        object_metadata = (type_name, nbytes, len_arr)\n        serialized_metadata = pickle.dumps(\n            object_metadata, protocol=pickle.HIGHEST_PROTOCOL\n        )\n        return value, nbytes, serialized_metadata, len(serialized_metadata)\n\n    def deserialize(self, data_view: memoryview) -> Any:\n        # pickle.loads do not read past the end of a pickled object\n        # within a large buffer, so we can skip storing the metadata size\n        type_name, nbytes, len_arr = pickle.loads(data_view)\n        serialized_data = data_view[-nbytes:]\n\n        if type_name == torch.Tensor.__name__:\n            obj = []\n            start_idx = 0\n            for length in len_arr:\n                item_bytes = serialized_data[start_idx : start_idx + length]\n                obj.append(item_bytes)\n                start_idx += length\n            obj = self.tensor_decoder.decode(obj)\n        elif type_name == self._mm_kwargs_item_cls.__name__:\n            obj = []\n            start_idx = 0\n            for length in len_arr:\n                item_bytes = serialized_data[start_idx : start_idx + length]\n                obj.append(item_bytes)\n                start_idx += length\n            obj = self.mm_decoder.decode(obj)\n        elif type_name == bytes.__name__:\n            obj = pickle.loads(serialized_data)\n        else:\n            raise ValueError(f\"Unsupported object type '{type_name}' in metadata\")\n\n        return obj",
      "language": "python"
    },
    {
      "code": "class MsgpackSerde(ObjectSerde):\n    def __init__(self):\n        # Delayed import to avoid circular dependency\n        from vllm.multimodal.inputs import MultiModalKwargsItem\n        from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder\n\n        self.encoder = MsgpackEncoder()\n        self.tensor_decoder = MsgpackDecoder(torch.Tensor, share_mem=False)\n        self.mm_decoder = MsgpackDecoder(MultiModalKwargsItem, share_mem=False)\n        self._mm_kwargs_item_cls = MultiModalKwargsItem\n\n    def serialize(self, value: Any) -> tuple[bytes | list[bytes], int, bytes, int]:\n        len_arr = None\n        if isinstance(value, (torch.Tensor, self._mm_kwargs_item_cls)):\n            type_name = type(value).__name__\n            value = self.encoder.encode(value)\n            len_arr = [len(s) for s in value]\n            nbytes = sum(len_arr)\n        else:\n            value = pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)\n            type_name = type(value).__name__\n            nbytes = len(value)\n\n        object_metadata = (type_name, nbytes, len_arr)\n        serialized_metadata = pickle.dumps(\n            object_metadata, protocol=pickle.HIGHEST_PROTOCOL\n        )\n        return value, nbytes, serialized_metadata, len(serialized_metadata)\n\n    def deserialize(self, data_view: memoryview) -> Any:\n        # pickle.loads do not read past the end of a pickled object\n        # within a large buffer, so we can skip storing the metadata size\n        type_name, nbytes, len_arr = pickle.loads(data_view)\n        serialized_data = data_view[-nbytes:]\n\n        if type_name == torch.Tensor.__name__:\n            obj = []\n            start_idx = 0\n            for length in len_arr:\n                item_bytes = serialized_data[start_idx : start_idx + length]\n                obj.append(item_bytes)\n                start_idx += length\n            obj = self.tensor_decoder.decode(obj)\n        elif type_name == self._mm_kwargs_item_cls.__name__:\n            obj = []\n            start_idx = 0\n            for length in len_arr:\n                item_bytes = serialized_data[start_idx : start_idx + length]\n                obj.append(item_bytes)\n                start_idx += length\n            obj = self.mm_decoder.decode(obj)\n        elif type_name == bytes.__name__:\n            obj = pickle.loads(serialized_data)\n        else:\n            raise ValueError(f\"Unsupported object type '{type_name}' in metadata\")\n\n        return obj",
      "language": "python"
    },
    {
      "code": "_mm_kwargs_item_cls = MultiModalKwargsItem",
      "language": "unknown"
    },
    {
      "code": "_mm_kwargs_item_cls = MultiModalKwargsItem",
      "language": "unknown"
    },
    {
      "code": "encoder = MsgpackEncoder()",
      "language": "unknown"
    },
    {
      "code": "encoder = MsgpackEncoder()",
      "language": "unknown"
    },
    {
      "code": "mm_decoder = MsgpackDecoder(\n    MultiModalKwargsItem, share_mem=False\n)",
      "language": "unknown"
    },
    {
      "code": "mm_decoder = MsgpackDecoder(\n    MultiModalKwargsItem, share_mem=False\n)",
      "language": "unknown"
    },
    {
      "code": "tensor_decoder = MsgpackDecoder(Tensor, share_mem=False)",
      "language": "unknown"
    },
    {
      "code": "tensor_decoder = MsgpackDecoder(Tensor, share_mem=False)",
      "language": "unknown"
    },
    {
      "code": "339\n340\n341\n342\n343\n344\n345\n346\n347",
      "language": "unknown"
    },
    {
      "code": "def __init__(self):\n    # Delayed import to avoid circular dependency\n    from vllm.multimodal.inputs import MultiModalKwargsItem\n    from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder\n\n    self.encoder = MsgpackEncoder()\n    self.tensor_decoder = MsgpackDecoder(torch.Tensor, share_mem=False)\n    self.mm_decoder = MsgpackDecoder(MultiModalKwargsItem, share_mem=False)\n    self._mm_kwargs_item_cls = MultiModalKwargsItem",
      "language": "python"
    },
    {
      "code": "def __init__(self):\n    # Delayed import to avoid circular dependency\n    from vllm.multimodal.inputs import MultiModalKwargsItem\n    from vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder\n\n    self.encoder = MsgpackEncoder()\n    self.tensor_decoder = MsgpackDecoder(torch.Tensor, share_mem=False)\n    self.mm_decoder = MsgpackDecoder(MultiModalKwargsItem, share_mem=False)\n    self._mm_kwargs_item_cls = MultiModalKwargsItem",
      "language": "python"
    },
    {
      "code": "deserialize(data_view: memoryview) -> Any",
      "language": "php"
    },
    {
      "code": "deserialize(data_view: memoryview) -> Any",
      "language": "php"
    },
    {
      "code": "367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394",
      "language": "unknown"
    },
    {
      "code": "def deserialize(self, data_view: memoryview) -> Any:\n    # pickle.loads do not read past the end of a pickled object\n    # within a large buffer, so we can skip storing the metadata size\n    type_name, nbytes, len_arr = pickle.loads(data_view)\n    serialized_data = data_view[-nbytes:]\n\n    if type_name == torch.Tensor.__name__:\n        obj = []\n        start_idx = 0\n        for length in len_arr:\n            item_bytes = serialized_data[start_idx : start_idx + length]\n            obj.append(item_bytes)\n            start_idx += length\n        obj = self.tensor_decoder.decode(obj)\n    elif type_name == self._mm_kwargs_item_cls.__name__:\n        obj = []\n        start_idx = 0\n        for length in len_arr:\n            item_bytes = serialized_data[start_idx : start_idx + length]\n            obj.append(item_bytes)\n            start_idx += length\n        obj = self.mm_decoder.decode(obj)\n    elif type_name == bytes.__name__:\n        obj = pickle.loads(serialized_data)\n    else:\n        raise ValueError(f\"Unsupported object type '{type_name}' in metadata\")\n\n    return obj",
      "language": "python"
    },
    {
      "code": "def deserialize(self, data_view: memoryview) -> Any:\n    # pickle.loads do not read past the end of a pickled object\n    # within a large buffer, so we can skip storing the metadata size\n    type_name, nbytes, len_arr = pickle.loads(data_view)\n    serialized_data = data_view[-nbytes:]\n\n    if type_name == torch.Tensor.__name__:\n        obj = []\n        start_idx = 0\n        for length in len_arr:\n            item_bytes = serialized_data[start_idx : start_idx + length]\n            obj.append(item_bytes)\n            start_idx += length\n        obj = self.tensor_decoder.decode(obj)\n    elif type_name == self._mm_kwargs_item_cls.__name__:\n        obj = []\n        start_idx = 0\n        for length in len_arr:\n            item_bytes = serialized_data[start_idx : start_idx + length]\n            obj.append(item_bytes)\n            start_idx += length\n        obj = self.mm_decoder.decode(obj)\n    elif type_name == bytes.__name__:\n        obj = pickle.loads(serialized_data)\n    else:\n        raise ValueError(f\"Unsupported object type '{type_name}' in metadata\")\n\n    return obj",
      "language": "python"
    },
    {
      "code": "serialize(\n    value: Any,\n) -> tuple[bytes | list[bytes], int, bytes, int]",
      "language": "php"
    },
    {
      "code": "serialize(\n    value: Any,\n) -> tuple[bytes | list[bytes], int, bytes, int]",
      "language": "php"
    },
    {
      "code": "349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365",
      "language": "unknown"
    },
    {
      "code": "def serialize(self, value: Any) -> tuple[bytes | list[bytes], int, bytes, int]:\n    len_arr = None\n    if isinstance(value, (torch.Tensor, self._mm_kwargs_item_cls)):\n        type_name = type(value).__name__\n        value = self.encoder.encode(value)\n        len_arr = [len(s) for s in value]\n        nbytes = sum(len_arr)\n    else:\n        value = pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)\n        type_name = type(value).__name__\n        nbytes = len(value)\n\n    object_metadata = (type_name, nbytes, len_arr)\n    serialized_metadata = pickle.dumps(\n        object_metadata, protocol=pickle.HIGHEST_PROTOCOL\n    )\n    return value, nbytes, serialized_metadata, len(serialized_metadata)",
      "language": "python"
    },
    {
      "code": "def serialize(self, value: Any) -> tuple[bytes | list[bytes], int, bytes, int]:\n    len_arr = None\n    if isinstance(value, (torch.Tensor, self._mm_kwargs_item_cls)):\n        type_name = type(value).__name__\n        value = self.encoder.encode(value)\n        len_arr = [len(s) for s in value]\n        nbytes = sum(len_arr)\n    else:\n        value = pickle.dumps(value, protocol=pickle.HIGHEST_PROTOCOL)\n        type_name = type(value).__name__\n        nbytes = len(value)\n\n    object_metadata = (type_name, nbytes, len_arr)\n    serialized_metadata = pickle.dumps(\n        object_metadata, protocol=pickle.HIGHEST_PROTOCOL\n    )\n    return value, nbytes, serialized_metadata, len(serialized_metadata)",
      "language": "python"
    },
    {
      "code": "326\n327\n328\n329\n330\n331\n332\n333\n334\n335",
      "language": "unknown"
    },
    {
      "code": "class ObjectSerde(ABC):\n    @abstractmethod\n    def serialize(self, value: Any) -> tuple[Any, int, bytes, int]:\n        \"\"\"Serialize an object to bytes.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def deserialize(self, data: memoryview) -> Any:\n        \"\"\"Deserialize bytes back to an object.\"\"\"\n        raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "class ObjectSerde(ABC):\n    @abstractmethod\n    def serialize(self, value: Any) -> tuple[Any, int, bytes, int]:\n        \"\"\"Serialize an object to bytes.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def deserialize(self, data: memoryview) -> Any:\n        \"\"\"Deserialize bytes back to an object.\"\"\"\n        raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "deserialize(data: memoryview) -> Any",
      "language": "php"
    },
    {
      "code": "deserialize(data: memoryview) -> Any",
      "language": "php"
    },
    {
      "code": "332\n333\n334\n335",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef deserialize(self, data: memoryview) -> Any:\n    \"\"\"Deserialize bytes back to an object.\"\"\"\n    raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef deserialize(self, data: memoryview) -> Any:\n    \"\"\"Deserialize bytes back to an object.\"\"\"\n    raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "serialize(value: Any) -> tuple[Any, int, bytes, int]",
      "language": "php"
    },
    {
      "code": "serialize(value: Any) -> tuple[Any, int, bytes, int]",
      "language": "php"
    },
    {
      "code": "327\n328\n329\n330",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef serialize(self, value: Any) -> tuple[Any, int, bytes, int]:\n    \"\"\"Serialize an object to bytes.\"\"\"\n    raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef serialize(self, value: Any) -> tuple[Any, int, bytes, int]:\n    \"\"\"Serialize an object to bytes.\"\"\"\n    raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "397\n398\n399\n400\n401\n402\n403",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass ShmObjectStorageHandle:\n    max_object_size: int\n    n_readers: int\n    ring_buffer_handle: tuple[int, str]\n    serde_class: type[ObjectSerde]\n    reader_lock: LockType | None",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass ShmObjectStorageHandle:\n    max_object_size: int\n    n_readers: int\n    ring_buffer_handle: tuple[int, str]\n    serde_class: type[ObjectSerde]\n    reader_lock: LockType | None",
      "language": "python"
    },
    {
      "code": "max_object_size: int",
      "language": "yaml"
    },
    {
      "code": "max_object_size: int",
      "language": "yaml"
    },
    {
      "code": "n_readers: int",
      "language": "yaml"
    },
    {
      "code": "n_readers: int",
      "language": "yaml"
    },
    {
      "code": "reader_lock: Lock | None",
      "language": "yaml"
    },
    {
      "code": "reader_lock: Lock | None",
      "language": "yaml"
    },
    {
      "code": "ring_buffer_handle: tuple[int, str]",
      "language": "yaml"
    },
    {
      "code": "ring_buffer_handle: tuple[int, str]",
      "language": "yaml"
    },
    {
      "code": "serde_class: type[ObjectSerde]",
      "language": "yaml"
    },
    {
      "code": "serde_class: type[ObjectSerde]",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    max_object_size: int,\n    n_readers: int,\n    ring_buffer_handle: tuple[int, str],\n    serde_class: type[ObjectSerde],\n    reader_lock: Lock | None,\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    max_object_size: int,\n    n_readers: int,\n    ring_buffer_handle: tuple[int, str],\n    serde_class: type[ObjectSerde],\n    reader_lock: Lock | None,\n) -> None",
      "language": "python"
    },
    {
      "code": "406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697",
      "language": "unknown"
    },
    {
      "code": "class SingleWriterShmObjectStorage:\n    \"\"\"\n    A single-writer, multiple-reader object storage system built on top of a\n    shared memory ring buffer. Provides key-value storage with automatic memory\n    management and cross-process serialization support.\n\n    This storage system follows a FIFO (First-In-First-Out) eviction policy\n    where the oldest objects are automatically freed when memory runs low.\n    Memory is reclaimed based on reader reference counting - objects are only\n    freed when all readers have finished accessing them.\n\n    Architecture:\n    - Single writer process can put(key, value) objects\n    - Multiple reader processes can get(address, monotonic_id) objects\n    - Built on SingleWriterShmRingBuffer for efficient shared memory management\n    - Thread-safe operations with reader synchronization via locks\n\n    Key Features:\n    - FIFO Eviction: Oldest objects are evicted first when memory is full\n    - Reference Counting: Objects are only freed when no readers are\n      accessing them\n    - Duplicate Key Handling: Existing keys are not overwritten, just\n      re-referenced\n    - Customized Serialization: By default uses Msgpack for efficient\n      serialization of Python objects, but can be extended for custom types\n    - Cross-Process Safety: Uses shared memory with proper synchronization\n    - Automatic Cleanup: Garbage collection happens transparently during\n      allocation\n\n    Memory Layout per Object:\n    `[4-byte reference_count][metadata_size][serialized_object_data]`\n\n    Thread Safety:\n    - Writer operations (put, clear) are single-threaded by design\n    - Reader operations (get) are thread-safe with lock-based reference\n      counting\n    - Memory reclamation is handled exclusively by the writer process\n    \"\"\"\n\n    def __init__(\n        self,\n        max_object_size: int,\n        n_readers: int,\n        ring_buffer: SingleWriterShmRingBuffer,\n        serde_class: type[ObjectSerde] = MsgpackSerde,\n        reader_lock: LockType | None = None,\n    ):\n        \"\"\"\n        Initialize the object storage.\n\n        Args:\n            max_object_size: Maximum size for a single object in bytes.\n            n_readers: Number of reader processes that can access the storage.\n            ring_buffer: The shared memory ring buffer for storing objects.\n            serde_class: Serializer/deserializer for objects.\n            reader_lock: Optional lock for synchronizing reader access.\n        Raises:\n            ValueError: If reader_lock is None for readers.\n        \"\"\"\n\n        self.max_object_size = max_object_size\n        self.n_readers = n_readers\n        self.serde_class = serde_class\n        self.ser_de = serde_class()\n        self.ring_buffer = ring_buffer\n        self.is_writer = self.ring_buffer.is_writer\n\n        self.flag_bytes = 4  # for in-use flag\n\n        if self.is_writer:\n            # Key-value mapping: key -> (address, monotonic_id)\n            self.key_index: dict[str, tuple[int, int]] = {}\n            # Reverse mapping: monotonic_id -> key\n            self.id_index: dict[int, str] = {}\n            # Writer flag to track in-use status: monotonic_id -> count\n            self.writer_flag: dict[int, int] = {}\n        else:\n            if reader_lock is None:\n                raise ValueError(\"Lock must be provided for readers.\")\n\n        self._reader_lock = reader_lock\n\n    def clear(self) -> None:\n        \"\"\"Clear the object storage.\"\"\"\n        if self.is_writer:\n            self.ring_buffer.clear()\n            self.key_index.clear()\n            self.id_index.clear()\n            self.writer_flag.clear()\n            logger.debug(\"Object storage cleared and reinitialized.\")\n\n    def copy_to_buffer(\n        self,\n        data: bytes | list[bytes],\n        data_bytes: int,\n        metadata: bytes,\n        md_bytes: int,\n        data_view: memoryview,\n    ) -> None:\n        data_view[self.flag_bytes : self.flag_bytes + md_bytes] = metadata\n        if isinstance(data, bytes):\n            data_view[-data_bytes:] = data\n        elif isinstance(data, list):\n            start_idx = self.flag_bytes + md_bytes\n            for item_bytes in data:\n                item_size = len(item_bytes)\n                data_view[start_idx : start_idx + item_size] = item_bytes\n                start_idx += item_size\n        else:\n            raise ValueError(f\"Unsupported data type for serialization: {type(data)}\")\n\n    def increment_writer_flag(self, id: int) -> None:\n        \"\"\"Set the in-use flag for the writer.\"\"\"\n        self.writer_flag[id] = self.writer_flag.get(id, 0) + 1\n\n    def increment_reader_flag(self, data_view: memoryview) -> None:\n        \"\"\"Set the in-use flag for the reader.\"\"\"\n        # >0 for in-use flag\n        reader_count = self.ring_buffer.byte2int(data_view)\n        data_view[:] = self.ring_buffer.int2byte(reader_count + 1)\n\n    def free_unused(self) -> None:\n        \"\"\"Free unused buffers in the ring buffer.\"\"\"\n        # try to free up 2*max_object_size bytes of space in the ring buffer,\n        # since the buffer might be fragmented\n        freed_ids = self.ring_buffer.free_buf(\n            self.default_is_free_check, 2 * self.max_object_size\n        )\n        # update the metadata after freeing up space\n        for freed_id in freed_ids:\n            key_to_free = self.id_index[freed_id]\n            del self.key_index[key_to_free]\n            del self.id_index[freed_id]\n            del self.writer_flag[freed_id]\n\n    def is_cached(self, key: str) -> bool:\n        \"\"\"\n        Check if the object with the given key is cached.\n        \"\"\"\n        return key in self.key_index\n\n    def get_cached(self, key: str) -> tuple[int, int]:\n        \"\"\"\n        Get the cached object by key if it exists.\n        \"\"\"\n        address, monotonic_id = self.key_index[key]\n        self.increment_writer_flag(monotonic_id)\n        return address, monotonic_id\n\n    def put(self, key: str, value: Any) -> tuple[int, int]:\n        \"\"\"\n        Store a key-value pair in the object storage.\n        Attempts to free max_object_size bytes using FIFO order\n        when the ring buffer runs out of space during a put() operation.\n\n        Args:\n            key: String key to identify the object\n            value: Any serializable Python object\n\n        Raises:\n            MemoryError: If there's not enough space in the buffer\n            ValueError: If the serialized object is too large\n            ValueError: If the key already exists in the storage\n        \"\"\"\n        if key in self.key_index:\n            raise ValueError(f\"Key '{key}' already exists in the storage.\")\n\n        object_data, data_bytes, object_metadata, md_bytes = self.ser_de.serialize(\n            value\n        )\n        buffer_size = self.flag_bytes + data_bytes + md_bytes\n        # Sanity checks\n        if buffer_size > self.max_object_size:\n            raise ValueError(\n                f\"Serialized object size ({buffer_size} bytes) exceeds \"\n                f\"max object size ({self.max_object_size} bytes)\"\n            )\n\n        # Allocate new buffer\n        try:\n            address, monotonic_id = self.ring_buffer.allocate_buf(buffer_size)\n        except MemoryError:\n            self.free_unused()\n            # try again after freeing up space\n            address, monotonic_id = self.ring_buffer.allocate_buf(buffer_size)\n\n        # Write data to buffer\n        with self.ring_buffer.access_buf(address) as (data_view, metadata):\n            data_view[: self.flag_bytes] = self.ring_buffer.int2byte(0)\n            self.copy_to_buffer(\n                object_data, data_bytes, object_metadata, md_bytes, data_view\n            )\n        self.increment_writer_flag(monotonic_id)\n\n        # Update key index\n        self.key_index[key] = (address, monotonic_id)\n        self.id_index[monotonic_id] = key\n        return address, monotonic_id\n\n    def get(self, address: int, monotonic_id: int) -> Any:\n        # Read data from buffer\n        with self.ring_buffer.access_buf(address) as (data_view, buf_metadata):\n            # check id from metadata\n            if buf_metadata[0] != monotonic_id:\n                raise ValueError(\n                    f\"Data for address:id '{address}:{monotonic_id}'\"\n                    \" has been modified or is invalid.\"\n                )\n\n            obj = self.ser_de.deserialize(data_view[self.flag_bytes :])\n\n            # decrease the in-use flag for reader reads\n            if self._reader_lock is not None:\n                with self._reader_lock:\n                    self.increment_reader_flag(data_view[: self.flag_bytes])\n            else:\n                # if self._reader_lock is None, it means we are the writer\n                # in this case, we do not need to decrease the reader count\n                assert self.is_writer\n\n        return obj\n\n    def touch(\n        self,\n        key: str,\n        address: int = 0,\n        monotonic_id: int = 0,\n    ) -> None:\n        \"\"\"\n        Touch an existing cached item to update its eviction status.\n\n        For writers (ShmObjectStoreSenderCache): Increment writer_flag\n        For readers (ShmObjectStoreReceiverCache): Increment reader_count\n\n        Args:\n            key: String key of the object to touch\n            address: Address of the object (only for readers)\n            monotonic_id: Monotonic ID of the object (only for readers)\n\n        \"\"\"\n        if self._reader_lock is None:\n            if key not in self.key_index:\n                return None\n            address, monotonic_id = self.key_index[key]\n            # Writer side: increment writer_flag to raise eviction threshold\n            self.increment_writer_flag(monotonic_id)\n        else:\n            with (\n                self._reader_lock,\n                self.ring_buffer.access_buf(address) as (data_view, _),\n            ):\n                reader_count = self.ring_buffer.byte2int(data_view[: self.flag_bytes])\n\n                # NOTE(Long):\n                # Avoid increasing flag on newly added item (sync with sender)\n                # Since when a new item is added\n                # pre-touch has no effect on writer side\n                if reader_count >= self.n_readers:\n                    self.increment_reader_flag(data_view[: self.flag_bytes])\n\n    def handle(self):\n        \"\"\"Get handle for sharing across processes.\"\"\"\n        return ShmObjectStorageHandle(\n            max_object_size=self.max_object_size,\n            n_readers=self.n_readers,\n            ring_buffer_handle=self.ring_buffer.handle(),\n            serde_class=self.serde_class,\n            reader_lock=self._reader_lock,\n        )\n\n    @staticmethod\n    def create_from_handle(\n        handle: ShmObjectStorageHandle,\n    ) -> \"SingleWriterShmObjectStorage\":\n        logger.debug(\"Creating storage from handle: %s\", handle)\n        ring_buffer = SingleWriterShmRingBuffer(*handle.ring_buffer_handle)\n        return SingleWriterShmObjectStorage(\n            max_object_size=handle.max_object_size,\n            n_readers=handle.n_readers,\n            ring_buffer=ring_buffer,\n            serde_class=handle.serde_class,\n            reader_lock=handle.reader_lock,\n        )\n\n    def default_is_free_check(self, id: int, buf: memoryview) -> bool:\n        \"\"\"\n        Default is_free function that checks if the first 4 bytes are zero.\n        This indicates that the buffer is free.\n        \"\"\"\n        reader_count = int.from_bytes(buf[0:4], \"little\", signed=True)\n        writer_count = self.writer_flag[id]\n        return reader_count >= writer_count * self.n_readers",
      "language": "python"
    },
    {
      "code": "class SingleWriterShmObjectStorage:\n    \"\"\"\n    A single-writer, multiple-reader object storage system built on top of a\n    shared memory ring buffer. Provides key-value storage with automatic memory\n    management and cross-process serialization support.\n\n    This storage system follows a FIFO (First-In-First-Out) eviction policy\n    where the oldest objects are automatically freed when memory runs low.\n    Memory is reclaimed based on reader reference counting - objects are only\n    freed when all readers have finished accessing them.\n\n    Architecture:\n    - Single writer process can put(key, value) objects\n    - Multiple reader processes can get(address, monotonic_id) objects\n    - Built on SingleWriterShmRingBuffer for efficient shared memory management\n    - Thread-safe operations with reader synchronization via locks\n\n    Key Features:\n    - FIFO Eviction: Oldest objects are evicted first when memory is full\n    - Reference Counting: Objects are only freed when no readers are\n      accessing them\n    - Duplicate Key Handling: Existing keys are not overwritten, just\n      re-referenced\n    - Customized Serialization: By default uses Msgpack for efficient\n      serialization of Python objects, but can be extended for custom types\n    - Cross-Process Safety: Uses shared memory with proper synchronization\n    - Automatic Cleanup: Garbage collection happens transparently during\n      allocation\n\n    Memory Layout per Object:\n    `[4-byte reference_count][metadata_size][serialized_object_data]`\n\n    Thread Safety:\n    - Writer operations (put, clear) are single-threaded by design\n    - Reader operations (get) are thread-safe with lock-based reference\n      counting\n    - Memory reclamation is handled exclusively by the writer process\n    \"\"\"\n\n    def __init__(\n        self,\n        max_object_size: int,\n        n_readers: int,\n        ring_buffer: SingleWriterShmRingBuffer,\n        serde_class: type[ObjectSerde] = MsgpackSerde,\n        reader_lock: LockType | None = None,\n    ):\n        \"\"\"\n        Initialize the object storage.\n\n        Args:\n            max_object_size: Maximum size for a single object in bytes.\n            n_readers: Number of reader processes that can access the storage.\n            ring_buffer: The shared memory ring buffer for storing objects.\n            serde_class: Serializer/deserializer for objects.\n            reader_lock: Optional lock for synchronizing reader access.\n        Raises:\n            ValueError: If reader_lock is None for readers.\n        \"\"\"\n\n        self.max_object_size = max_object_size\n        self.n_readers = n_readers\n        self.serde_class = serde_class\n        self.ser_de = serde_class()\n        self.ring_buffer = ring_buffer\n        self.is_writer = self.ring_buffer.is_writer\n\n        self.flag_bytes = 4  # for in-use flag\n\n        if self.is_writer:\n            # Key-value mapping: key -> (address, monotonic_id)\n            self.key_index: dict[str, tuple[int, int]] = {}\n            # Reverse mapping: monotonic_id -> key\n            self.id_index: dict[int, str] = {}\n            # Writer flag to track in-use status: monotonic_id -> count\n            self.writer_flag: dict[int, int] = {}\n        else:\n            if reader_lock is None:\n                raise ValueError(\"Lock must be provided for readers.\")\n\n        self._reader_lock = reader_lock\n\n    def clear(self) -> None:\n        \"\"\"Clear the object storage.\"\"\"\n        if self.is_writer:\n            self.ring_buffer.clear()\n            self.key_index.clear()\n            self.id_index.clear()\n            self.writer_flag.clear()\n            logger.debug(\"Object storage cleared and reinitialized.\")\n\n    def copy_to_buffer(\n        self,\n        data: bytes | list[bytes],\n        data_bytes: int,\n        metadata: bytes,\n        md_bytes: int,\n        data_view: memoryview,\n    ) -> None:\n        data_view[self.flag_bytes : self.flag_bytes + md_bytes] = metadata\n        if isinstance(data, bytes):\n            data_view[-data_bytes:] = data\n        elif isinstance(data, list):\n            start_idx = self.flag_bytes + md_bytes\n            for item_bytes in data:\n                item_size = len(item_bytes)\n                data_view[start_idx : start_idx + item_size] = item_bytes\n                start_idx += item_size\n        else:\n            raise ValueError(f\"Unsupported data type for serialization: {type(data)}\")\n\n    def increment_writer_flag(self, id: int) -> None:\n        \"\"\"Set the in-use flag for the writer.\"\"\"\n        self.writer_flag[id] = self.writer_flag.get(id, 0) + 1\n\n    def increment_reader_flag(self, data_view: memoryview) -> None:\n        \"\"\"Set the in-use flag for the reader.\"\"\"\n        # >0 for in-use flag\n        reader_count = self.ring_buffer.byte2int(data_view)\n        data_view[:] = self.ring_buffer.int2byte(reader_count + 1)\n\n    def free_unused(self) -> None:\n        \"\"\"Free unused buffers in the ring buffer.\"\"\"\n        # try to free up 2*max_object_size bytes of space in the ring buffer,\n        # since the buffer might be fragmented\n        freed_ids = self.ring_buffer.free_buf(\n            self.default_is_free_check, 2 * self.max_object_size\n        )\n        # update the metadata after freeing up space\n        for freed_id in freed_ids:\n            key_to_free = self.id_index[freed_id]\n            del self.key_index[key_to_free]\n            del self.id_index[freed_id]\n            del self.writer_flag[freed_id]\n\n    def is_cached(self, key: str) -> bool:\n        \"\"\"\n        Check if the object with the given key is cached.\n        \"\"\"\n        return key in self.key_index\n\n    def get_cached(self, key: str) -> tuple[int, int]:\n        \"\"\"\n        Get the cached object by key if it exists.\n        \"\"\"\n        address, monotonic_id = self.key_index[key]\n        self.increment_writer_flag(monotonic_id)\n        return address, monotonic_id\n\n    def put(self, key: str, value: Any) -> tuple[int, int]:\n        \"\"\"\n        Store a key-value pair in the object storage.\n        Attempts to free max_object_size bytes using FIFO order\n        when the ring buffer runs out of space during a put() operation.\n\n        Args:\n            key: String key to identify the object\n            value: Any serializable Python object\n\n        Raises:\n            MemoryError: If there's not enough space in the buffer\n            ValueError: If the serialized object is too large\n            ValueError: If the key already exists in the storage\n        \"\"\"\n        if key in self.key_index:\n            raise ValueError(f\"Key '{key}' already exists in the storage.\")\n\n        object_data, data_bytes, object_metadata, md_bytes = self.ser_de.serialize(\n            value\n        )\n        buffer_size = self.flag_bytes + data_bytes + md_bytes\n        # Sanity checks\n        if buffer_size > self.max_object_size:\n            raise ValueError(\n                f\"Serialized object size ({buffer_size} bytes) exceeds \"\n                f\"max object size ({self.max_object_size} bytes)\"\n            )\n\n        # Allocate new buffer\n        try:\n            address, monotonic_id = self.ring_buffer.allocate_buf(buffer_size)\n        except MemoryError:\n            self.free_unused()\n            # try again after freeing up space\n            address, monotonic_id = self.ring_buffer.allocate_buf(buffer_size)\n\n        # Write data to buffer\n        with self.ring_buffer.access_buf(address) as (data_view, metadata):\n            data_view[: self.flag_bytes] = self.ring_buffer.int2byte(0)\n            self.copy_to_buffer(\n                object_data, data_bytes, object_metadata, md_bytes, data_view\n            )\n        self.increment_writer_flag(monotonic_id)\n\n        # Update key index\n        self.key_index[key] = (address, monotonic_id)\n        self.id_index[monotonic_id] = key\n        return address, monotonic_id\n\n    def get(self, address: int, monotonic_id: int) -> Any:\n        # Read data from buffer\n        with self.ring_buffer.access_buf(address) as (data_view, buf_metadata):\n            # check id from metadata\n            if buf_metadata[0] != monotonic_id:\n                raise ValueError(\n                    f\"Data for address:id '{address}:{monotonic_id}'\"\n                    \" has been modified or is invalid.\"\n                )\n\n            obj = self.ser_de.deserialize(data_view[self.flag_bytes :])\n\n            # decrease the in-use flag for reader reads\n            if self._reader_lock is not None:\n                with self._reader_lock:\n                    self.increment_reader_flag(data_view[: self.flag_bytes])\n            else:\n                # if self._reader_lock is None, it means we are the writer\n                # in this case, we do not need to decrease the reader count\n                assert self.is_writer\n\n        return obj\n\n    def touch(\n        self,\n        key: str,\n        address: int = 0,\n        monotonic_id: int = 0,\n    ) -> None:\n        \"\"\"\n        Touch an existing cached item to update its eviction status.\n\n        For writers (ShmObjectStoreSenderCache): Increment writer_flag\n        For readers (ShmObjectStoreReceiverCache): Increment reader_count\n\n        Args:\n            key: String key of the object to touch\n            address: Address of the object (only for readers)\n            monotonic_id: Monotonic ID of the object (only for readers)\n\n        \"\"\"\n        if self._reader_lock is None:\n            if key not in self.key_index:\n                return None\n            address, monotonic_id = self.key_index[key]\n            # Writer side: increment writer_flag to raise eviction threshold\n            self.increment_writer_flag(monotonic_id)\n        else:\n            with (\n                self._reader_lock,\n                self.ring_buffer.access_buf(address) as (data_view, _),\n            ):\n                reader_count = self.ring_buffer.byte2int(data_view[: self.flag_bytes])\n\n                # NOTE(Long):\n                # Avoid increasing flag on newly added item (sync with sender)\n                # Since when a new item is added\n                # pre-touch has no effect on writer side\n                if reader_count >= self.n_readers:\n                    self.increment_reader_flag(data_view[: self.flag_bytes])\n\n    def handle(self):\n        \"\"\"Get handle for sharing across processes.\"\"\"\n        return ShmObjectStorageHandle(\n            max_object_size=self.max_object_size,\n            n_readers=self.n_readers,\n            ring_buffer_handle=self.ring_buffer.handle(),\n            serde_class=self.serde_class,\n            reader_lock=self._reader_lock,\n        )\n\n    @staticmethod\n    def create_from_handle(\n        handle: ShmObjectStorageHandle,\n    ) -> \"SingleWriterShmObjectStorage\":\n        logger.debug(\"Creating storage from handle: %s\", handle)\n        ring_buffer = SingleWriterShmRingBuffer(*handle.ring_buffer_handle)\n        return SingleWriterShmObjectStorage(\n            max_object_size=handle.max_object_size,\n            n_readers=handle.n_readers,\n            ring_buffer=ring_buffer,\n            serde_class=handle.serde_class,\n            reader_lock=handle.reader_lock,\n        )\n\n    def default_is_free_check(self, id: int, buf: memoryview) -> bool:\n        \"\"\"\n        Default is_free function that checks if the first 4 bytes are zero.\n        This indicates that the buffer is free.\n        \"\"\"\n        reader_count = int.from_bytes(buf[0:4], \"little\", signed=True)\n        writer_count = self.writer_flag[id]\n        return reader_count >= writer_count * self.n_readers",
      "language": "python"
    },
    {
      "code": "_reader_lock = reader_lock",
      "language": "unknown"
    },
    {
      "code": "_reader_lock = reader_lock",
      "language": "unknown"
    },
    {
      "code": "flag_bytes = 4",
      "language": "unknown"
    },
    {
      "code": "flag_bytes = 4",
      "language": "unknown"
    },
    {
      "code": "id_index: dict[int, str] = {}",
      "language": "yaml"
    },
    {
      "code": "id_index: dict[int, str] = {}",
      "language": "yaml"
    },
    {
      "code": "is_writer = is_writer",
      "language": "unknown"
    },
    {
      "code": "is_writer = is_writer",
      "language": "unknown"
    },
    {
      "code": "key_index: dict[str, tuple[int, int]] = {}",
      "language": "yaml"
    },
    {
      "code": "key_index: dict[str, tuple[int, int]] = {}",
      "language": "yaml"
    },
    {
      "code": "max_object_size = max_object_size",
      "language": "unknown"
    },
    {
      "code": "max_object_size = max_object_size",
      "language": "unknown"
    },
    {
      "code": "n_readers = n_readers",
      "language": "unknown"
    },
    {
      "code": "n_readers = n_readers",
      "language": "unknown"
    },
    {
      "code": "ring_buffer = ring_buffer",
      "language": "unknown"
    },
    {
      "code": "ring_buffer = ring_buffer",
      "language": "unknown"
    },
    {
      "code": "ser_de = serde_class()",
      "language": "unknown"
    },
    {
      "code": "ser_de = serde_class()",
      "language": "unknown"
    },
    {
      "code": "serde_class = serde_class",
      "language": "unknown"
    },
    {
      "code": "serde_class = serde_class",
      "language": "unknown"
    },
    {
      "code": "writer_flag: dict[int, int] = {}",
      "language": "yaml"
    },
    {
      "code": "writer_flag: dict[int, int] = {}",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    max_object_size: int,\n    n_readers: int,\n    ring_buffer: SingleWriterShmRingBuffer,\n    serde_class: type[ObjectSerde] = MsgpackSerde,\n    reader_lock: Lock | None = None,\n)",
      "language": "python"
    },
    {
      "code": "__init__(\n    max_object_size: int,\n    n_readers: int,\n    ring_buffer: SingleWriterShmRingBuffer,\n    serde_class: type[ObjectSerde] = MsgpackSerde,\n    reader_lock: Lock | None = None,\n)",
      "language": "python"
    },
    {
      "code": "445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    max_object_size: int,\n    n_readers: int,\n    ring_buffer: SingleWriterShmRingBuffer,\n    serde_class: type[ObjectSerde] = MsgpackSerde,\n    reader_lock: LockType | None = None,\n):\n    \"\"\"\n    Initialize the object storage.\n\n    Args:\n        max_object_size: Maximum size for a single object in bytes.\n        n_readers: Number of reader processes that can access the storage.\n        ring_buffer: The shared memory ring buffer for storing objects.\n        serde_class: Serializer/deserializer for objects.\n        reader_lock: Optional lock for synchronizing reader access.\n    Raises:\n        ValueError: If reader_lock is None for readers.\n    \"\"\"\n\n    self.max_object_size = max_object_size\n    self.n_readers = n_readers\n    self.serde_class = serde_class\n    self.ser_de = serde_class()\n    self.ring_buffer = ring_buffer\n    self.is_writer = self.ring_buffer.is_writer\n\n    self.flag_bytes = 4  # for in-use flag\n\n    if self.is_writer:\n        # Key-value mapping: key -> (address, monotonic_id)\n        self.key_index: dict[str, tuple[int, int]] = {}\n        # Reverse mapping: monotonic_id -> key\n        self.id_index: dict[int, str] = {}\n        # Writer flag to track in-use status: monotonic_id -> count\n        self.writer_flag: dict[int, int] = {}\n    else:\n        if reader_lock is None:\n            raise ValueError(\"Lock must be provided for readers.\")\n\n    self._reader_lock = reader_lock",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    max_object_size: int,\n    n_readers: int,\n    ring_buffer: SingleWriterShmRingBuffer,\n    serde_class: type[ObjectSerde] = MsgpackSerde,\n    reader_lock: LockType | None = None,\n):\n    \"\"\"\n    Initialize the object storage.\n\n    Args:\n        max_object_size: Maximum size for a single object in bytes.\n        n_readers: Number of reader processes that can access the storage.\n        ring_buffer: The shared memory ring buffer for storing objects.\n        serde_class: Serializer/deserializer for objects.\n        reader_lock: Optional lock for synchronizing reader access.\n    Raises:\n        ValueError: If reader_lock is None for readers.\n    \"\"\"\n\n    self.max_object_size = max_object_size\n    self.n_readers = n_readers\n    self.serde_class = serde_class\n    self.ser_de = serde_class()\n    self.ring_buffer = ring_buffer\n    self.is_writer = self.ring_buffer.is_writer\n\n    self.flag_bytes = 4  # for in-use flag\n\n    if self.is_writer:\n        # Key-value mapping: key -> (address, monotonic_id)\n        self.key_index: dict[str, tuple[int, int]] = {}\n        # Reverse mapping: monotonic_id -> key\n        self.id_index: dict[int, str] = {}\n        # Writer flag to track in-use status: monotonic_id -> count\n        self.writer_flag: dict[int, int] = {}\n    else:\n        if reader_lock is None:\n            raise ValueError(\"Lock must be provided for readers.\")\n\n    self._reader_lock = reader_lock",
      "language": "python"
    },
    {
      "code": "clear() -> None",
      "language": "rust"
    },
    {
      "code": "clear() -> None",
      "language": "rust"
    },
    {
      "code": "488\n489\n490\n491\n492\n493\n494\n495",
      "language": "unknown"
    },
    {
      "code": "def clear(self) -> None:\n    \"\"\"Clear the object storage.\"\"\"\n    if self.is_writer:\n        self.ring_buffer.clear()\n        self.key_index.clear()\n        self.id_index.clear()\n        self.writer_flag.clear()\n        logger.debug(\"Object storage cleared and reinitialized.\")",
      "language": "python"
    },
    {
      "code": "def clear(self) -> None:\n    \"\"\"Clear the object storage.\"\"\"\n    if self.is_writer:\n        self.ring_buffer.clear()\n        self.key_index.clear()\n        self.id_index.clear()\n        self.writer_flag.clear()\n        logger.debug(\"Object storage cleared and reinitialized.\")",
      "language": "python"
    },
    {
      "code": "copy_to_buffer(\n    data: bytes | list[bytes],\n    data_bytes: int,\n    metadata: bytes,\n    md_bytes: int,\n    data_view: memoryview,\n) -> None",
      "language": "rust"
    },
    {
      "code": "copy_to_buffer(\n    data: bytes | list[bytes],\n    data_bytes: int,\n    metadata: bytes,\n    md_bytes: int,\n    data_view: memoryview,\n) -> None",
      "language": "rust"
    },
    {
      "code": "497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515",
      "language": "unknown"
    },
    {
      "code": "def copy_to_buffer(\n    self,\n    data: bytes | list[bytes],\n    data_bytes: int,\n    metadata: bytes,\n    md_bytes: int,\n    data_view: memoryview,\n) -> None:\n    data_view[self.flag_bytes : self.flag_bytes + md_bytes] = metadata\n    if isinstance(data, bytes):\n        data_view[-data_bytes:] = data\n    elif isinstance(data, list):\n        start_idx = self.flag_bytes + md_bytes\n        for item_bytes in data:\n            item_size = len(item_bytes)\n            data_view[start_idx : start_idx + item_size] = item_bytes\n            start_idx += item_size\n    else:\n        raise ValueError(f\"Unsupported data type for serialization: {type(data)}\")",
      "language": "python"
    },
    {
      "code": "def copy_to_buffer(\n    self,\n    data: bytes | list[bytes],\n    data_bytes: int,\n    metadata: bytes,\n    md_bytes: int,\n    data_view: memoryview,\n) -> None:\n    data_view[self.flag_bytes : self.flag_bytes + md_bytes] = metadata\n    if isinstance(data, bytes):\n        data_view[-data_bytes:] = data\n    elif isinstance(data, list):\n        start_idx = self.flag_bytes + md_bytes\n        for item_bytes in data:\n            item_size = len(item_bytes)\n            data_view[start_idx : start_idx + item_size] = item_bytes\n            start_idx += item_size\n    else:\n        raise ValueError(f\"Unsupported data type for serialization: {type(data)}\")",
      "language": "python"
    },
    {
      "code": "create_from_handle(\n    handle: ShmObjectStorageHandle,\n) -> SingleWriterShmObjectStorage",
      "language": "php"
    },
    {
      "code": "create_from_handle(\n    handle: ShmObjectStorageHandle,\n) -> SingleWriterShmObjectStorage",
      "language": "php"
    },
    {
      "code": "676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef create_from_handle(\n    handle: ShmObjectStorageHandle,\n) -> \"SingleWriterShmObjectStorage\":\n    logger.debug(\"Creating storage from handle: %s\", handle)\n    ring_buffer = SingleWriterShmRingBuffer(*handle.ring_buffer_handle)\n    return SingleWriterShmObjectStorage(\n        max_object_size=handle.max_object_size,\n        n_readers=handle.n_readers,\n        ring_buffer=ring_buffer,\n        serde_class=handle.serde_class,\n        reader_lock=handle.reader_lock,\n    )",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef create_from_handle(\n    handle: ShmObjectStorageHandle,\n) -> \"SingleWriterShmObjectStorage\":\n    logger.debug(\"Creating storage from handle: %s\", handle)\n    ring_buffer = SingleWriterShmRingBuffer(*handle.ring_buffer_handle)\n    return SingleWriterShmObjectStorage(\n        max_object_size=handle.max_object_size,\n        n_readers=handle.n_readers,\n        ring_buffer=ring_buffer,\n        serde_class=handle.serde_class,\n        reader_lock=handle.reader_lock,\n    )",
      "language": "python"
    },
    {
      "code": "default_is_free_check(id: int, buf: memoryview) -> bool",
      "language": "php"
    },
    {
      "code": "default_is_free_check(id: int, buf: memoryview) -> bool",
      "language": "php"
    },
    {
      "code": "690\n691\n692\n693\n694\n695\n696\n697",
      "language": "unknown"
    },
    {
      "code": "def default_is_free_check(self, id: int, buf: memoryview) -> bool:\n    \"\"\"\n    Default is_free function that checks if the first 4 bytes are zero.\n    This indicates that the buffer is free.\n    \"\"\"\n    reader_count = int.from_bytes(buf[0:4], \"little\", signed=True)\n    writer_count = self.writer_flag[id]\n    return reader_count >= writer_count * self.n_readers",
      "language": "python"
    },
    {
      "code": "def default_is_free_check(self, id: int, buf: memoryview) -> bool:\n    \"\"\"\n    Default is_free function that checks if the first 4 bytes are zero.\n    This indicates that the buffer is free.\n    \"\"\"\n    reader_count = int.from_bytes(buf[0:4], \"little\", signed=True)\n    writer_count = self.writer_flag[id]\n    return reader_count >= writer_count * self.n_readers",
      "language": "python"
    },
    {
      "code": "free_unused() -> None",
      "language": "rust"
    },
    {
      "code": "free_unused() -> None",
      "language": "rust"
    },
    {
      "code": "527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539",
      "language": "unknown"
    },
    {
      "code": "def free_unused(self) -> None:\n    \"\"\"Free unused buffers in the ring buffer.\"\"\"\n    # try to free up 2*max_object_size bytes of space in the ring buffer,\n    # since the buffer might be fragmented\n    freed_ids = self.ring_buffer.free_buf(\n        self.default_is_free_check, 2 * self.max_object_size\n    )\n    # update the metadata after freeing up space\n    for freed_id in freed_ids:\n        key_to_free = self.id_index[freed_id]\n        del self.key_index[key_to_free]\n        del self.id_index[freed_id]\n        del self.writer_flag[freed_id]",
      "language": "python"
    },
    {
      "code": "def free_unused(self) -> None:\n    \"\"\"Free unused buffers in the ring buffer.\"\"\"\n    # try to free up 2*max_object_size bytes of space in the ring buffer,\n    # since the buffer might be fragmented\n    freed_ids = self.ring_buffer.free_buf(\n        self.default_is_free_check, 2 * self.max_object_size\n    )\n    # update the metadata after freeing up space\n    for freed_id in freed_ids:\n        key_to_free = self.id_index[freed_id]\n        del self.key_index[key_to_free]\n        del self.id_index[freed_id]\n        del self.writer_flag[freed_id]",
      "language": "python"
    },
    {
      "code": "get(address: int, monotonic_id: int) -> Any",
      "language": "php"
    },
    {
      "code": "get(address: int, monotonic_id: int) -> Any",
      "language": "php"
    },
    {
      "code": "605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626",
      "language": "unknown"
    },
    {
      "code": "def get(self, address: int, monotonic_id: int) -> Any:\n    # Read data from buffer\n    with self.ring_buffer.access_buf(address) as (data_view, buf_metadata):\n        # check id from metadata\n        if buf_metadata[0] != monotonic_id:\n            raise ValueError(\n                f\"Data for address:id '{address}:{monotonic_id}'\"\n                \" has been modified or is invalid.\"\n            )\n\n        obj = self.ser_de.deserialize(data_view[self.flag_bytes :])\n\n        # decrease the in-use flag for reader reads\n        if self._reader_lock is not None:\n            with self._reader_lock:\n                self.increment_reader_flag(data_view[: self.flag_bytes])\n        else:\n            # if self._reader_lock is None, it means we are the writer\n            # in this case, we do not need to decrease the reader count\n            assert self.is_writer\n\n    return obj",
      "language": "python"
    },
    {
      "code": "def get(self, address: int, monotonic_id: int) -> Any:\n    # Read data from buffer\n    with self.ring_buffer.access_buf(address) as (data_view, buf_metadata):\n        # check id from metadata\n        if buf_metadata[0] != monotonic_id:\n            raise ValueError(\n                f\"Data for address:id '{address}:{monotonic_id}'\"\n                \" has been modified or is invalid.\"\n            )\n\n        obj = self.ser_de.deserialize(data_view[self.flag_bytes :])\n\n        # decrease the in-use flag for reader reads\n        if self._reader_lock is not None:\n            with self._reader_lock:\n                self.increment_reader_flag(data_view[: self.flag_bytes])\n        else:\n            # if self._reader_lock is None, it means we are the writer\n            # in this case, we do not need to decrease the reader count\n            assert self.is_writer\n\n    return obj",
      "language": "python"
    },
    {
      "code": "get_cached(key: str) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "get_cached(key: str) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "547\n548\n549\n550\n551\n552\n553",
      "language": "unknown"
    },
    {
      "code": "def get_cached(self, key: str) -> tuple[int, int]:\n    \"\"\"\n    Get the cached object by key if it exists.\n    \"\"\"\n    address, monotonic_id = self.key_index[key]\n    self.increment_writer_flag(monotonic_id)\n    return address, monotonic_id",
      "language": "python"
    },
    {
      "code": "def get_cached(self, key: str) -> tuple[int, int]:\n    \"\"\"\n    Get the cached object by key if it exists.\n    \"\"\"\n    address, monotonic_id = self.key_index[key]\n    self.increment_writer_flag(monotonic_id)\n    return address, monotonic_id",
      "language": "python"
    },
    {
      "code": "666\n667\n668\n669\n670\n671\n672\n673\n674",
      "language": "unknown"
    },
    {
      "code": "def handle(self):\n    \"\"\"Get handle for sharing across processes.\"\"\"\n    return ShmObjectStorageHandle(\n        max_object_size=self.max_object_size,\n        n_readers=self.n_readers,\n        ring_buffer_handle=self.ring_buffer.handle(),\n        serde_class=self.serde_class,\n        reader_lock=self._reader_lock,\n    )",
      "language": "python"
    },
    {
      "code": "def handle(self):\n    \"\"\"Get handle for sharing across processes.\"\"\"\n    return ShmObjectStorageHandle(\n        max_object_size=self.max_object_size,\n        n_readers=self.n_readers,\n        ring_buffer_handle=self.ring_buffer.handle(),\n        serde_class=self.serde_class,\n        reader_lock=self._reader_lock,\n    )",
      "language": "python"
    },
    {
      "code": "increment_reader_flag(data_view: memoryview) -> None",
      "language": "rust"
    },
    {
      "code": "increment_reader_flag(data_view: memoryview) -> None",
      "language": "rust"
    },
    {
      "code": "521\n522\n523\n524\n525",
      "language": "unknown"
    },
    {
      "code": "def increment_reader_flag(self, data_view: memoryview) -> None:\n    \"\"\"Set the in-use flag for the reader.\"\"\"\n    # >0 for in-use flag\n    reader_count = self.ring_buffer.byte2int(data_view)\n    data_view[:] = self.ring_buffer.int2byte(reader_count + 1)",
      "language": "python"
    },
    {
      "code": "def increment_reader_flag(self, data_view: memoryview) -> None:\n    \"\"\"Set the in-use flag for the reader.\"\"\"\n    # >0 for in-use flag\n    reader_count = self.ring_buffer.byte2int(data_view)\n    data_view[:] = self.ring_buffer.int2byte(reader_count + 1)",
      "language": "python"
    },
    {
      "code": "increment_writer_flag(id: int) -> None",
      "language": "rust"
    },
    {
      "code": "increment_writer_flag(id: int) -> None",
      "language": "rust"
    },
    {
      "code": "517\n518\n519",
      "language": "unknown"
    },
    {
      "code": "def increment_writer_flag(self, id: int) -> None:\n    \"\"\"Set the in-use flag for the writer.\"\"\"\n    self.writer_flag[id] = self.writer_flag.get(id, 0) + 1",
      "language": "python"
    },
    {
      "code": "def increment_writer_flag(self, id: int) -> None:\n    \"\"\"Set the in-use flag for the writer.\"\"\"\n    self.writer_flag[id] = self.writer_flag.get(id, 0) + 1",
      "language": "python"
    },
    {
      "code": "is_cached(key: str) -> bool",
      "language": "php"
    },
    {
      "code": "is_cached(key: str) -> bool",
      "language": "php"
    },
    {
      "code": "541\n542\n543\n544\n545",
      "language": "unknown"
    },
    {
      "code": "def is_cached(self, key: str) -> bool:\n    \"\"\"\n    Check if the object with the given key is cached.\n    \"\"\"\n    return key in self.key_index",
      "language": "python"
    },
    {
      "code": "def is_cached(self, key: str) -> bool:\n    \"\"\"\n    Check if the object with the given key is cached.\n    \"\"\"\n    return key in self.key_index",
      "language": "python"
    },
    {
      "code": "put(key: str, value: Any) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "put(key: str, value: Any) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603",
      "language": "unknown"
    },
    {
      "code": "def put(self, key: str, value: Any) -> tuple[int, int]:\n    \"\"\"\n    Store a key-value pair in the object storage.\n    Attempts to free max_object_size bytes using FIFO order\n    when the ring buffer runs out of space during a put() operation.\n\n    Args:\n        key: String key to identify the object\n        value: Any serializable Python object\n\n    Raises:\n        MemoryError: If there's not enough space in the buffer\n        ValueError: If the serialized object is too large\n        ValueError: If the key already exists in the storage\n    \"\"\"\n    if key in self.key_index:\n        raise ValueError(f\"Key '{key}' already exists in the storage.\")\n\n    object_data, data_bytes, object_metadata, md_bytes = self.ser_de.serialize(\n        value\n    )\n    buffer_size = self.flag_bytes + data_bytes + md_bytes\n    # Sanity checks\n    if buffer_size > self.max_object_size:\n        raise ValueError(\n            f\"Serialized object size ({buffer_size} bytes) exceeds \"\n            f\"max object size ({self.max_object_size} bytes)\"\n        )\n\n    # Allocate new buffer\n    try:\n        address, monotonic_id = self.ring_buffer.allocate_buf(buffer_size)\n    except MemoryError:\n        self.free_unused()\n        # try again after freeing up space\n        address, monotonic_id = self.ring_buffer.allocate_buf(buffer_size)\n\n    # Write data to buffer\n    with self.ring_buffer.access_buf(address) as (data_view, metadata):\n        data_view[: self.flag_bytes] = self.ring_buffer.int2byte(0)\n        self.copy_to_buffer(\n            object_data, data_bytes, object_metadata, md_bytes, data_view\n        )\n    self.increment_writer_flag(monotonic_id)\n\n    # Update key index\n    self.key_index[key] = (address, monotonic_id)\n    self.id_index[monotonic_id] = key\n    return address, monotonic_id",
      "language": "python"
    },
    {
      "code": "def put(self, key: str, value: Any) -> tuple[int, int]:\n    \"\"\"\n    Store a key-value pair in the object storage.\n    Attempts to free max_object_size bytes using FIFO order\n    when the ring buffer runs out of space during a put() operation.\n\n    Args:\n        key: String key to identify the object\n        value: Any serializable Python object\n\n    Raises:\n        MemoryError: If there's not enough space in the buffer\n        ValueError: If the serialized object is too large\n        ValueError: If the key already exists in the storage\n    \"\"\"\n    if key in self.key_index:\n        raise ValueError(f\"Key '{key}' already exists in the storage.\")\n\n    object_data, data_bytes, object_metadata, md_bytes = self.ser_de.serialize(\n        value\n    )\n    buffer_size = self.flag_bytes + data_bytes + md_bytes\n    # Sanity checks\n    if buffer_size > self.max_object_size:\n        raise ValueError(\n            f\"Serialized object size ({buffer_size} bytes) exceeds \"\n            f\"max object size ({self.max_object_size} bytes)\"\n        )\n\n    # Allocate new buffer\n    try:\n        address, monotonic_id = self.ring_buffer.allocate_buf(buffer_size)\n    except MemoryError:\n        self.free_unused()\n        # try again after freeing up space\n        address, monotonic_id = self.ring_buffer.allocate_buf(buffer_size)\n\n    # Write data to buffer\n    with self.ring_buffer.access_buf(address) as (data_view, metadata):\n        data_view[: self.flag_bytes] = self.ring_buffer.int2byte(0)\n        self.copy_to_buffer(\n            object_data, data_bytes, object_metadata, md_bytes, data_view\n        )\n    self.increment_writer_flag(monotonic_id)\n\n    # Update key index\n    self.key_index[key] = (address, monotonic_id)\n    self.id_index[monotonic_id] = key\n    return address, monotonic_id",
      "language": "python"
    },
    {
      "code": "touch(\n    key: str, address: int = 0, monotonic_id: int = 0\n) -> None",
      "language": "typescript"
    },
    {
      "code": "touch(\n    key: str, address: int = 0, monotonic_id: int = 0\n) -> None",
      "language": "typescript"
    },
    {
      "code": "628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664",
      "language": "unknown"
    },
    {
      "code": "def touch(\n    self,\n    key: str,\n    address: int = 0,\n    monotonic_id: int = 0,\n) -> None:\n    \"\"\"\n    Touch an existing cached item to update its eviction status.\n\n    For writers (ShmObjectStoreSenderCache): Increment writer_flag\n    For readers (ShmObjectStoreReceiverCache): Increment reader_count\n\n    Args:\n        key: String key of the object to touch\n        address: Address of the object (only for readers)\n        monotonic_id: Monotonic ID of the object (only for readers)\n\n    \"\"\"\n    if self._reader_lock is None:\n        if key not in self.key_index:\n            return None\n        address, monotonic_id = self.key_index[key]\n        # Writer side: increment writer_flag to raise eviction threshold\n        self.increment_writer_flag(monotonic_id)\n    else:\n        with (\n            self._reader_lock,\n            self.ring_buffer.access_buf(address) as (data_view, _),\n        ):\n            reader_count = self.ring_buffer.byte2int(data_view[: self.flag_bytes])\n\n            # NOTE(Long):\n            # Avoid increasing flag on newly added item (sync with sender)\n            # Since when a new item is added\n            # pre-touch has no effect on writer side\n            if reader_count >= self.n_readers:\n                self.increment_reader_flag(data_view[: self.flag_bytes])",
      "language": "python"
    },
    {
      "code": "def touch(\n    self,\n    key: str,\n    address: int = 0,\n    monotonic_id: int = 0,\n) -> None:\n    \"\"\"\n    Touch an existing cached item to update its eviction status.\n\n    For writers (ShmObjectStoreSenderCache): Increment writer_flag\n    For readers (ShmObjectStoreReceiverCache): Increment reader_count\n\n    Args:\n        key: String key of the object to touch\n        address: Address of the object (only for readers)\n        monotonic_id: Monotonic ID of the object (only for readers)\n\n    \"\"\"\n    if self._reader_lock is None:\n        if key not in self.key_index:\n            return None\n        address, monotonic_id = self.key_index[key]\n        # Writer side: increment writer_flag to raise eviction threshold\n        self.increment_writer_flag(monotonic_id)\n    else:\n        with (\n            self._reader_lock,\n            self.ring_buffer.access_buf(address) as (data_view, _),\n        ):\n            reader_count = self.ring_buffer.byte2int(data_view[: self.flag_bytes])\n\n            # NOTE(Long):\n            # Avoid increasing flag on newly added item (sync with sender)\n            # Since when a new item is added\n            # pre-touch has no effect on writer side\n            if reader_count >= self.n_readers:\n                self.increment_reader_flag(data_view[: self.flag_bytes])",
      "language": "python"
    },
    {
      "code": "Buffer size: 100 bytes\nInitial state: [................................................. ]\n               ^start=end(0)\n\nAfter allocating 20 bytes (id=0):\n[id:0|size:20|data........][...................................]\n^start(0)                  ^end(28)\n\nAfter allocating 30 bytes (id=1):\n[id:0|size:20|data........][id:1|size:30|data..............][..]\n^start(0)                                                   ^end(66)",
      "language": "json"
    },
    {
      "code": "Buffer size: 100 bytes\nInitial state: [................................................. ]\n               ^start=end(0)\n\nAfter allocating 20 bytes (id=0):\n[id:0|size:20|data........][...................................]\n^start(0)                  ^end(28)\n\nAfter allocating 30 bytes (id=1):\n[id:0|size:20|data........][id:1|size:30|data..............][..]\n^start(0)                                                   ^end(66)",
      "language": "json"
    },
    {
      "code": "Before freeing (both buffers still in use):\n[id:0|size:20|data........][id:1|size:30|data..............][..]\n^start(0)                                                   ^end(66)\n\nAfter id:0 is marked free by readers:\n[FREED.................... ][id:1|size:30|data..............][..]\n                            ^start(28)                       ^end(66)\n\nAfter both are freed:\n[FREED..............................................][..]\n                                                     ^start=end(66)",
      "language": "json"
    },
    {
      "code": "Before freeing (both buffers still in use):\n[id:0|size:20|data........][id:1|size:30|data..............][..]\n^start(0)                                                   ^end(66)\n\nAfter id:0 is marked free by readers:\n[FREED.................... ][id:1|size:30|data..............][..]\n                            ^start(28)                       ^end(66)\n\nAfter both are freed:\n[FREED..............................................][..]\n                                                     ^start=end(66)",
      "language": "json"
    },
    {
      "code": "Starting from after memory reclamation in Scenario 2:\n[FREED..............................................][..]\n                                                     ^start=end(66)\n\nAllocate 40 bytes (id=2) - only 34 bytes available at end, so wraparound:\n[id:2|size:40|data........................][FREED.............][..]\n                                          ^end(148)            ^start(66)",
      "language": "json"
    },
    {
      "code": "Starting from after memory reclamation in Scenario 2:\n[FREED..............................................][..]\n                                                     ^start=end(66)\n\nAllocate 40 bytes (id=2) - only 34 bytes available at end, so wraparound:\n[id:2|size:40|data........................][FREED.............][..]\n                                          ^end(148)            ^start(66)",
      "language": "json"
    },
    {
      "code": "Starting from after wraparound allocation in Scenario 3:\n[id:2|size:40|data........................][FREED.............][..]\n                                          ^end(148)            ^start(66)\n\nTrying to allocate 20 more bytes:\noccupied_size_new = end + size - start = 148 + 28 - 66 > buffer_size(100)\n-> Raises MemoryError: \"Not enough space in the data buffer\"",
      "language": "json"
    },
    {
      "code": "Starting from after wraparound allocation in Scenario 3:\n[id:2|size:40|data........................][FREED.............][..]\n                                          ^end(148)            ^start(66)\n\nTrying to allocate 20 more bytes:\noccupied_size_new = end + size - start = 148 + 28 - 66 > buffer_size(100)\n-> Raises MemoryError: \"Not enough space in the data buffer\"",
      "language": "json"
    },
    {
      "code": "22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323",
      "language": "unknown"
    },
    {
      "code": "class SingleWriterShmRingBuffer:\n    \"\"\"\n    A single-writer, multiple-reader ring buffer implementation using shared\n    memory. This class provides a thread-safe ring buffer where one process\n    can write data while multiple processes/threads can read from it.\n\n    Architecture:\n    - Uses shared memory for cross-process communication\n    - Maintains metadata for each allocated buffer chunk in the writer process\n    - Supports custom \"is_free_fn\" functions to determine when buffers can be\n      reused\n    - Each buffer chunk contains: `[4-byte id][4-byte size][actual_data]`\n\n    Key Concepts:\n    - monotonic_id_start/end: Track the range of active buffer IDs\n    - data_buffer_start/end: Track the physical memory range in use\n    - Automatic wraparound when reaching buffer end\n    - Lazy garbage collection based on is_free_fn checks\n\n    Example Usage Scenarios:\n\n    Scenario 1: Simple Linear Allocation\n    ```\n    Buffer size: 100 bytes\n    Initial state: [................................................. ]\n                   ^start=end(0)\n\n    After allocating 20 bytes (id=0):\n    [id:0|size:20|data........][...................................]\n    ^start(0)                  ^end(28)\n\n    After allocating 30 bytes (id=1):\n    [id:0|size:20|data........][id:1|size:30|data..............][..]\n    ^start(0)                                                   ^end(66)\n    ```\n\n    Scenario 2: Memory Reclamation\n    ```\n    Before freeing (both buffers still in use):\n    [id:0|size:20|data........][id:1|size:30|data..............][..]\n    ^start(0)                                                   ^end(66)\n\n    After id:0 is marked free by readers:\n    [FREED.................... ][id:1|size:30|data..............][..]\n                                ^start(28)                       ^end(66)\n\n    After both are freed:\n    [FREED..............................................][..]\n                                                         ^start=end(66)\n    ```\n\n    Scenario 3: Wraparound Allocation (continuing from Scenario 2)\n    ```\n    Starting from after memory reclamation in Scenario 2:\n    [FREED..............................................][..]\n                                                         ^start=end(66)\n\n    Allocate 40 bytes (id=2) - only 34 bytes available at end, so wraparound:\n    [id:2|size:40|data........................][FREED.............][..]\n                                              ^end(148)            ^start(66)\n    ```\n\n    Scenario 4: Error Handling - Out of Space\n    ```\n    Starting from after wraparound allocation in Scenario 3:\n    [id:2|size:40|data........................][FREED.............][..]\n                                              ^end(148)            ^start(66)\n\n    Trying to allocate 20 more bytes:\n    occupied_size_new = end + size - start = 148 + 28 - 66 > buffer_size(100)\n    -> Raises MemoryError: \"Not enough space in the data buffer\"\n    ```\n\n    Thread Safety:\n    - Single writer: Only one process/thread should write (allocate_buf)\n    - Multiple readers: Multiple processes/threads can read (access_buf)\n    - Reader synchronization handled by is_free_fn callback\n    - Writer handles garbage collection (free_buf) based on reader feedback\n\n    Memory Layout per Buffer Chunk:\n    `[4-byte monotonic_id][4-byte chunk_size][actual_data...]`\n    ^metadata_start                         ^data_start\n\n    The monotonic_id ensures data integrity - readers can verify they're\n    accessing the correct data even after buffer wraparound or reuse.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_buffer_size: int,\n        name: str | None = None,\n        create: bool = False,\n    ):\n        self.data_buffer_size = data_buffer_size\n        self.is_writer = create\n\n        self.ID_NBYTES = 4\n        self.ID_MAX = 2**31  # exclusive, so 2**31 - 1 is the max value\n        self.SIZE_NBYTES = 4\n        # 4 bytes for id, 4 bytes for buffer size\n        self.MD_SIZE = self.ID_NBYTES + self.SIZE_NBYTES\n        self.monotonic_id_end = 0\n        self.monotonic_id_start = 0\n        self.data_buffer_start = 0\n        self.data_buffer_end = 0\n\n        if create:\n            # we are creating a buffer\n            self.metadata: dict[int, int] = {}  # monotonic_id -> start address\n            self.shared_memory = shared_memory.SharedMemory(\n                create=True, size=self.data_buffer_size, name=name\n            )\n        else:\n            # we are opening an existing buffer\n            # fix to https://stackoverflow.com/q/62748654/9191338\n            # Python incorrectly tracks shared memory even if it is not\n            # created by the process. The following patch is a workaround.\n            with patch(\n                \"multiprocessing.resource_tracker.register\",\n                lambda *args, **kwargs: None,\n            ):\n                self.shared_memory = shared_memory.SharedMemory(name=name)\n                # See https://docs.python.org/3/library/multiprocessing.shared_memory.html # noqa\n                # Some platforms allocate memory based on page size,\n                # so the shared memory block size may be larger or equal\n                # to the requested size. The size parameter is ignored\n                # when attaching to an existing block.\n                assert self.shared_memory.size >= self.data_buffer_size\n\n        logger.debug(\n            \"Shared memory created/opened with name: %s, size: %d\",\n            self.shared_memory.name,\n            self.data_buffer_size,\n        )\n\n    def handle(self):\n        return (\n            self.data_buffer_size,\n            self.shared_memory.name,\n        )\n\n    def clear(self) -> None:\n        \"\"\"Clear the ring buffer.\"\"\"\n        assert self.is_writer, \"Only the writer can clear the buffer.\"\n        self.metadata.clear()\n        self.monotonic_id_end = 0\n        self.monotonic_id_start = 0\n        self.data_buffer_start = 0\n        self.data_buffer_end = 0\n\n    def __del__(self):\n        if hasattr(self, \"shared_memory\"):\n            self.shared_memory.close()\n            if self.is_writer:\n                self.shared_memory.unlink()\n\n    def int2byte(self, integer: int) -> bytes:\n        \"\"\"Convert an integer to bytes.\"\"\"\n        return integer.to_bytes(self.ID_NBYTES, \"little\", signed=True)\n\n    def byte2int(self, byte_data: bytes) -> int:\n        \"\"\"Convert bytes back to an integer.\"\"\"\n        return int.from_bytes(byte_data, \"little\", signed=True)\n\n    def allocate_buf(self, size: int) -> tuple[int, int]:\n        \"\"\"\n        Allocate a buffer `MD_SIZE` + `size` bytes in the shared memory.\n        Memory layout:\n        `[4-byte monotonic_id][4-byte size][buffer data...]`\n        \"\"\"\n        assert self.is_writer, \"Only the writer can allocate buffers.\"\n        assert size > 0, \"Size must be greater than 0\"\n        size += self.MD_SIZE  # add metadata size to the buffer size\n        # reset to beginning if the buffer does have enough contiguous space\n        buffer_end_reset = self.data_buffer_end % self.data_buffer_size\n        if buffer_end_reset + size > self.data_buffer_size:\n            buffer_end_reset = (\n                self.data_buffer_end // self.data_buffer_size + 1\n            ) * self.data_buffer_size\n        else:  # no reset needed\n            buffer_end_reset = self.data_buffer_end\n\n        # check if we have enough space in the data buffer\n        # i.e. if the new end (self.data_buffer_end + size)\n        # exceeds the start of the data buffer\n        occupied_size_new = buffer_end_reset + size - self.data_buffer_start\n        if occupied_size_new > self.data_buffer_size:\n            raise MemoryError(\n                \"Not enough space in the data buffer, \"\n                \"try calling free_buf() to free up space\"\n            )\n        self.data_buffer_end = buffer_end_reset\n\n        # first 4 bytes as the monotonic id\n        buf_idx = self.data_buffer_end % self.data_buffer_size\n        self.shared_memory.buf[buf_idx : buf_idx + self.ID_NBYTES] = self.int2byte(\n            self.monotonic_id_end\n        )\n        # next 4 bytes as the size of the data buffer\n        self.shared_memory.buf[buf_idx + self.ID_NBYTES : buf_idx + self.MD_SIZE] = (\n            self.int2byte(size)\n        )\n\n        # record metadata\n        self.metadata[self.monotonic_id_end % self.ID_MAX] = self.data_buffer_end\n        # update buffer and monotonic id indices\n        current_buffer_end = self.data_buffer_end\n        current_id_end = self.monotonic_id_end\n        self.data_buffer_end += size\n        self.monotonic_id_end = (self.monotonic_id_end + 1) % self.ID_MAX\n        return current_buffer_end, current_id_end\n\n    @contextmanager\n    def access_buf(self, address: int):\n        buf_idx = address % self.data_buffer_size\n\n        # read metadata\n        metadata_buff = self.shared_memory.buf[buf_idx : buf_idx + self.MD_SIZE]\n        id = self.byte2int(metadata_buff[: self.ID_NBYTES])\n        size = self.byte2int(metadata_buff[self.ID_NBYTES : self.MD_SIZE])\n\n        # yield the data buffer and metadata\n        data_buff = self.shared_memory.buf[buf_idx + self.MD_SIZE : buf_idx + size]\n        with (\n            memoryview(data_buff) as data_view,\n        ):\n            yield data_view, (id, size)\n\n    def free_buf(\n        self,\n        is_free_fn: Callable[[int, memoryview], bool],\n        nbytes: int | None = None,\n    ) -> Iterable[int]:\n        \"\"\"\n        Free a buffer of the given size. This is a no-op in shared memory,\n        but we need to keep track of the metadata.\n\n        If freed memory spreads across the end and start of the ring buffer,\n        the actual freed memory will be in two segments. In this case there\n        still might not be a contiguous space of `nbytes` available.\n\n        Args:\n            nbytes (int, optional): The size of the buffer to free. If None,\n                frees the maximum size of the ring buffer.\n        \"\"\"\n\n        assert self.is_writer, \"Only the writer can free buffers.\"\n        logger.debug(\n            \"Freeing up space in the ring buffer, \"\n            \"monotonic_id_start: %d, monotonic_id_end: %d\",\n            self.monotonic_id_start,\n            self.monotonic_id_end,\n        )\n        monotonic_id_before = self.monotonic_id_start\n        # if nbytes is None, free up the maximum size of the ring buffer\n        if nbytes is None:\n            nbytes = self.data_buffer_size\n        freed_bytes = 0\n        while self.monotonic_id_start in self.metadata and freed_bytes < nbytes:\n            address = self.metadata[self.monotonic_id_start]\n            with self.access_buf(address) as (data_buff, metadata):\n                if is_free_fn(self.monotonic_id_start, data_buff):\n                    # check passed, we can free the buffer\n                    del self.metadata[self.monotonic_id_start]\n                    self.monotonic_id_start = (\n                        self.monotonic_id_start + 1\n                    ) % self.ID_MAX\n                    if self.monotonic_id_start in self.metadata:\n                        # pointing to the start addr of next allocation\n                        self.data_buffer_start += (\n                            self.metadata[self.monotonic_id_start]\n                            - self.data_buffer_start\n                        ) % self.data_buffer_size\n                    else:\n                        # no remaining allocation, reset to zero\n                        self.data_buffer_start = self.data_buffer_end = 0\n                    freed_bytes += metadata[1]\n                else:\n                    # there are still readers, we cannot free the buffer\n                    break\n\n        logger.debug(\n            \"Freed %d bytes from the ring buffer, \"\n            \"monotonic_id_start: %d, monotonic_id_end: %d\",\n            freed_bytes,\n            self.monotonic_id_start,\n            self.monotonic_id_end,\n        )\n\n        # buffer wrap around\n        if self.data_buffer_start >= self.data_buffer_size:\n            self.data_buffer_start -= self.data_buffer_size\n            self.data_buffer_end -= self.data_buffer_size\n\n        monotonic_id_after = self.monotonic_id_start\n        # id wrap around\n        if monotonic_id_after >= monotonic_id_before:\n            return range(monotonic_id_before, monotonic_id_after)\n        else:\n            return chain(\n                range(monotonic_id_before, self.ID_MAX), range(0, monotonic_id_after)\n            )",
      "language": "python"
    },
    {
      "code": "class SingleWriterShmRingBuffer:\n    \"\"\"\n    A single-writer, multiple-reader ring buffer implementation using shared\n    memory. This class provides a thread-safe ring buffer where one process\n    can write data while multiple processes/threads can read from it.\n\n    Architecture:\n    - Uses shared memory for cross-process communication\n    - Maintains metadata for each allocated buffer chunk in the writer process\n    - Supports custom \"is_free_fn\" functions to determine when buffers can be\n      reused\n    - Each buffer chunk contains: `[4-byte id][4-byte size][actual_data]`\n\n    Key Concepts:\n    - monotonic_id_start/end: Track the range of active buffer IDs\n    - data_buffer_start/end: Track the physical memory range in use\n    - Automatic wraparound when reaching buffer end\n    - Lazy garbage collection based on is_free_fn checks\n\n    Example Usage Scenarios:\n\n    Scenario 1: Simple Linear Allocation\n    ```\n    Buffer size: 100 bytes\n    Initial state: [................................................. ]\n                   ^start=end(0)\n\n    After allocating 20 bytes (id=0):\n    [id:0|size:20|data........][...................................]\n    ^start(0)                  ^end(28)\n\n    After allocating 30 bytes (id=1):\n    [id:0|size:20|data........][id:1|size:30|data..............][..]\n    ^start(0)                                                   ^end(66)\n    ```\n\n    Scenario 2: Memory Reclamation\n    ```\n    Before freeing (both buffers still in use):\n    [id:0|size:20|data........][id:1|size:30|data..............][..]\n    ^start(0)                                                   ^end(66)\n\n    After id:0 is marked free by readers:\n    [FREED.................... ][id:1|size:30|data..............][..]\n                                ^start(28)                       ^end(66)\n\n    After both are freed:\n    [FREED..............................................][..]\n                                                         ^start=end(66)\n    ```\n\n    Scenario 3: Wraparound Allocation (continuing from Scenario 2)\n    ```\n    Starting from after memory reclamation in Scenario 2:\n    [FREED..............................................][..]\n                                                         ^start=end(66)\n\n    Allocate 40 bytes (id=2) - only 34 bytes available at end, so wraparound:\n    [id:2|size:40|data........................][FREED.............][..]\n                                              ^end(148)            ^start(66)\n    ```\n\n    Scenario 4: Error Handling - Out of Space\n    ```\n    Starting from after wraparound allocation in Scenario 3:\n    [id:2|size:40|data........................][FREED.............][..]\n                                              ^end(148)            ^start(66)\n\n    Trying to allocate 20 more bytes:\n    occupied_size_new = end + size - start = 148 + 28 - 66 > buffer_size(100)\n    -> Raises MemoryError: \"Not enough space in the data buffer\"\n    ```\n\n    Thread Safety:\n    - Single writer: Only one process/thread should write (allocate_buf)\n    - Multiple readers: Multiple processes/threads can read (access_buf)\n    - Reader synchronization handled by is_free_fn callback\n    - Writer handles garbage collection (free_buf) based on reader feedback\n\n    Memory Layout per Buffer Chunk:\n    `[4-byte monotonic_id][4-byte chunk_size][actual_data...]`\n    ^metadata_start                         ^data_start\n\n    The monotonic_id ensures data integrity - readers can verify they're\n    accessing the correct data even after buffer wraparound or reuse.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_buffer_size: int,\n        name: str | None = None,\n        create: bool = False,\n    ):\n        self.data_buffer_size = data_buffer_size\n        self.is_writer = create\n\n        self.ID_NBYTES = 4\n        self.ID_MAX = 2**31  # exclusive, so 2**31 - 1 is the max value\n        self.SIZE_NBYTES = 4\n        # 4 bytes for id, 4 bytes for buffer size\n        self.MD_SIZE = self.ID_NBYTES + self.SIZE_NBYTES\n        self.monotonic_id_end = 0\n        self.monotonic_id_start = 0\n        self.data_buffer_start = 0\n        self.data_buffer_end = 0\n\n        if create:\n            # we are creating a buffer\n            self.metadata: dict[int, int] = {}  # monotonic_id -> start address\n            self.shared_memory = shared_memory.SharedMemory(\n                create=True, size=self.data_buffer_size, name=name\n            )\n        else:\n            # we are opening an existing buffer\n            # fix to https://stackoverflow.com/q/62748654/9191338\n            # Python incorrectly tracks shared memory even if it is not\n            # created by the process. The following patch is a workaround.\n            with patch(\n                \"multiprocessing.resource_tracker.register\",\n                lambda *args, **kwargs: None,\n            ):\n                self.shared_memory = shared_memory.SharedMemory(name=name)\n                # See https://docs.python.org/3/library/multiprocessing.shared_memory.html # noqa\n                # Some platforms allocate memory based on page size,\n                # so the shared memory block size may be larger or equal\n                # to the requested size. The size parameter is ignored\n                # when attaching to an existing block.\n                assert self.shared_memory.size >= self.data_buffer_size\n\n        logger.debug(\n            \"Shared memory created/opened with name: %s, size: %d\",\n            self.shared_memory.name,\n            self.data_buffer_size,\n        )\n\n    def handle(self):\n        return (\n            self.data_buffer_size,\n            self.shared_memory.name,\n        )\n\n    def clear(self) -> None:\n        \"\"\"Clear the ring buffer.\"\"\"\n        assert self.is_writer, \"Only the writer can clear the buffer.\"\n        self.metadata.clear()\n        self.monotonic_id_end = 0\n        self.monotonic_id_start = 0\n        self.data_buffer_start = 0\n        self.data_buffer_end = 0\n\n    def __del__(self):\n        if hasattr(self, \"shared_memory\"):\n            self.shared_memory.close()\n            if self.is_writer:\n                self.shared_memory.unlink()\n\n    def int2byte(self, integer: int) -> bytes:\n        \"\"\"Convert an integer to bytes.\"\"\"\n        return integer.to_bytes(self.ID_NBYTES, \"little\", signed=True)\n\n    def byte2int(self, byte_data: bytes) -> int:\n        \"\"\"Convert bytes back to an integer.\"\"\"\n        return int.from_bytes(byte_data, \"little\", signed=True)\n\n    def allocate_buf(self, size: int) -> tuple[int, int]:\n        \"\"\"\n        Allocate a buffer `MD_SIZE` + `size` bytes in the shared memory.\n        Memory layout:\n        `[4-byte monotonic_id][4-byte size][buffer data...]`\n        \"\"\"\n        assert self.is_writer, \"Only the writer can allocate buffers.\"\n        assert size > 0, \"Size must be greater than 0\"\n        size += self.MD_SIZE  # add metadata size to the buffer size\n        # reset to beginning if the buffer does have enough contiguous space\n        buffer_end_reset = self.data_buffer_end % self.data_buffer_size\n        if buffer_end_reset + size > self.data_buffer_size:\n            buffer_end_reset = (\n                self.data_buffer_end // self.data_buffer_size + 1\n            ) * self.data_buffer_size\n        else:  # no reset needed\n            buffer_end_reset = self.data_buffer_end\n\n        # check if we have enough space in the data buffer\n        # i.e. if the new end (self.data_buffer_end + size)\n        # exceeds the start of the data buffer\n        occupied_size_new = buffer_end_reset + size - self.data_buffer_start\n        if occupied_size_new > self.data_buffer_size:\n            raise MemoryError(\n                \"Not enough space in the data buffer, \"\n                \"try calling free_buf() to free up space\"\n            )\n        self.data_buffer_end = buffer_end_reset\n\n        # first 4 bytes as the monotonic id\n        buf_idx = self.data_buffer_end % self.data_buffer_size\n        self.shared_memory.buf[buf_idx : buf_idx + self.ID_NBYTES] = self.int2byte(\n            self.monotonic_id_end\n        )\n        # next 4 bytes as the size of the data buffer\n        self.shared_memory.buf[buf_idx + self.ID_NBYTES : buf_idx + self.MD_SIZE] = (\n            self.int2byte(size)\n        )\n\n        # record metadata\n        self.metadata[self.monotonic_id_end % self.ID_MAX] = self.data_buffer_end\n        # update buffer and monotonic id indices\n        current_buffer_end = self.data_buffer_end\n        current_id_end = self.monotonic_id_end\n        self.data_buffer_end += size\n        self.monotonic_id_end = (self.monotonic_id_end + 1) % self.ID_MAX\n        return current_buffer_end, current_id_end\n\n    @contextmanager\n    def access_buf(self, address: int):\n        buf_idx = address % self.data_buffer_size\n\n        # read metadata\n        metadata_buff = self.shared_memory.buf[buf_idx : buf_idx + self.MD_SIZE]\n        id = self.byte2int(metadata_buff[: self.ID_NBYTES])\n        size = self.byte2int(metadata_buff[self.ID_NBYTES : self.MD_SIZE])\n\n        # yield the data buffer and metadata\n        data_buff = self.shared_memory.buf[buf_idx + self.MD_SIZE : buf_idx + size]\n        with (\n            memoryview(data_buff) as data_view,\n        ):\n            yield data_view, (id, size)\n\n    def free_buf(\n        self,\n        is_free_fn: Callable[[int, memoryview], bool],\n        nbytes: int | None = None,\n    ) -> Iterable[int]:\n        \"\"\"\n        Free a buffer of the given size. This is a no-op in shared memory,\n        but we need to keep track of the metadata.\n\n        If freed memory spreads across the end and start of the ring buffer,\n        the actual freed memory will be in two segments. In this case there\n        still might not be a contiguous space of `nbytes` available.\n\n        Args:\n            nbytes (int, optional): The size of the buffer to free. If None,\n                frees the maximum size of the ring buffer.\n        \"\"\"\n\n        assert self.is_writer, \"Only the writer can free buffers.\"\n        logger.debug(\n            \"Freeing up space in the ring buffer, \"\n            \"monotonic_id_start: %d, monotonic_id_end: %d\",\n            self.monotonic_id_start,\n            self.monotonic_id_end,\n        )\n        monotonic_id_before = self.monotonic_id_start\n        # if nbytes is None, free up the maximum size of the ring buffer\n        if nbytes is None:\n            nbytes = self.data_buffer_size\n        freed_bytes = 0\n        while self.monotonic_id_start in self.metadata and freed_bytes < nbytes:\n            address = self.metadata[self.monotonic_id_start]\n            with self.access_buf(address) as (data_buff, metadata):\n                if is_free_fn(self.monotonic_id_start, data_buff):\n                    # check passed, we can free the buffer\n                    del self.metadata[self.monotonic_id_start]\n                    self.monotonic_id_start = (\n                        self.monotonic_id_start + 1\n                    ) % self.ID_MAX\n                    if self.monotonic_id_start in self.metadata:\n                        # pointing to the start addr of next allocation\n                        self.data_buffer_start += (\n                            self.metadata[self.monotonic_id_start]\n                            - self.data_buffer_start\n                        ) % self.data_buffer_size\n                    else:\n                        # no remaining allocation, reset to zero\n                        self.data_buffer_start = self.data_buffer_end = 0\n                    freed_bytes += metadata[1]\n                else:\n                    # there are still readers, we cannot free the buffer\n                    break\n\n        logger.debug(\n            \"Freed %d bytes from the ring buffer, \"\n            \"monotonic_id_start: %d, monotonic_id_end: %d\",\n            freed_bytes,\n            self.monotonic_id_start,\n            self.monotonic_id_end,\n        )\n\n        # buffer wrap around\n        if self.data_buffer_start >= self.data_buffer_size:\n            self.data_buffer_start -= self.data_buffer_size\n            self.data_buffer_end -= self.data_buffer_size\n\n        monotonic_id_after = self.monotonic_id_start\n        # id wrap around\n        if monotonic_id_after >= monotonic_id_before:\n            return range(monotonic_id_before, monotonic_id_after)\n        else:\n            return chain(\n                range(monotonic_id_before, self.ID_MAX), range(0, monotonic_id_after)\n            )",
      "language": "python"
    },
    {
      "code": "ID_MAX = 2 ** 31",
      "language": "unknown"
    },
    {
      "code": "ID_MAX = 2 ** 31",
      "language": "unknown"
    },
    {
      "code": "ID_NBYTES = 4",
      "language": "unknown"
    },
    {
      "code": "ID_NBYTES = 4",
      "language": "unknown"
    },
    {
      "code": "MD_SIZE = ID_NBYTES + SIZE_NBYTES",
      "language": "unknown"
    },
    {
      "code": "MD_SIZE = ID_NBYTES + SIZE_NBYTES",
      "language": "unknown"
    },
    {
      "code": "SIZE_NBYTES = 4",
      "language": "unknown"
    },
    {
      "code": "SIZE_NBYTES = 4",
      "language": "unknown"
    },
    {
      "code": "data_buffer_end = 0",
      "language": "unknown"
    },
    {
      "code": "data_buffer_end = 0",
      "language": "unknown"
    },
    {
      "code": "data_buffer_size = data_buffer_size",
      "language": "unknown"
    },
    {
      "code": "data_buffer_size = data_buffer_size",
      "language": "unknown"
    },
    {
      "code": "data_buffer_start = 0",
      "language": "unknown"
    },
    {
      "code": "data_buffer_start = 0",
      "language": "unknown"
    },
    {
      "code": "is_writer = create",
      "language": "unknown"
    },
    {
      "code": "is_writer = create",
      "language": "unknown"
    },
    {
      "code": "metadata: dict[int, int] = {}",
      "language": "yaml"
    },
    {
      "code": "metadata: dict[int, int] = {}",
      "language": "yaml"
    },
    {
      "code": "monotonic_id_end = 0",
      "language": "unknown"
    },
    {
      "code": "monotonic_id_end = 0",
      "language": "unknown"
    },
    {
      "code": "monotonic_id_start = 0",
      "language": "unknown"
    },
    {
      "code": "monotonic_id_start = 0",
      "language": "unknown"
    },
    {
      "code": "shared_memory = SharedMemory(name=name)",
      "language": "unknown"
    },
    {
      "code": "shared_memory = SharedMemory(name=name)",
      "language": "unknown"
    },
    {
      "code": "172\n173\n174\n175\n176",
      "language": "unknown"
    },
    {
      "code": "def __del__(self):\n    if hasattr(self, \"shared_memory\"):\n        self.shared_memory.close()\n        if self.is_writer:\n            self.shared_memory.unlink()",
      "language": "python"
    },
    {
      "code": "def __del__(self):\n    if hasattr(self, \"shared_memory\"):\n        self.shared_memory.close()\n        if self.is_writer:\n            self.shared_memory.unlink()",
      "language": "python"
    },
    {
      "code": "__init__(\n    data_buffer_size: int,\n    name: str | None = None,\n    create: bool = False,\n)",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    data_buffer_size: int,\n    name: str | None = None,\n    create: bool = False,\n)",
      "language": "typescript"
    },
    {
      "code": "109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    data_buffer_size: int,\n    name: str | None = None,\n    create: bool = False,\n):\n    self.data_buffer_size = data_buffer_size\n    self.is_writer = create\n\n    self.ID_NBYTES = 4\n    self.ID_MAX = 2**31  # exclusive, so 2**31 - 1 is the max value\n    self.SIZE_NBYTES = 4\n    # 4 bytes for id, 4 bytes for buffer size\n    self.MD_SIZE = self.ID_NBYTES + self.SIZE_NBYTES\n    self.monotonic_id_end = 0\n    self.monotonic_id_start = 0\n    self.data_buffer_start = 0\n    self.data_buffer_end = 0\n\n    if create:\n        # we are creating a buffer\n        self.metadata: dict[int, int] = {}  # monotonic_id -> start address\n        self.shared_memory = shared_memory.SharedMemory(\n            create=True, size=self.data_buffer_size, name=name\n        )\n    else:\n        # we are opening an existing buffer\n        # fix to https://stackoverflow.com/q/62748654/9191338\n        # Python incorrectly tracks shared memory even if it is not\n        # created by the process. The following patch is a workaround.\n        with patch(\n            \"multiprocessing.resource_tracker.register\",\n            lambda *args, **kwargs: None,\n        ):\n            self.shared_memory = shared_memory.SharedMemory(name=name)\n            # See https://docs.python.org/3/library/multiprocessing.shared_memory.html # noqa\n            # Some platforms allocate memory based on page size,\n            # so the shared memory block size may be larger or equal\n            # to the requested size. The size parameter is ignored\n            # when attaching to an existing block.\n            assert self.shared_memory.size >= self.data_buffer_size\n\n    logger.debug(\n        \"Shared memory created/opened with name: %s, size: %d\",\n        self.shared_memory.name,\n        self.data_buffer_size,\n    )",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    data_buffer_size: int,\n    name: str | None = None,\n    create: bool = False,\n):\n    self.data_buffer_size = data_buffer_size\n    self.is_writer = create\n\n    self.ID_NBYTES = 4\n    self.ID_MAX = 2**31  # exclusive, so 2**31 - 1 is the max value\n    self.SIZE_NBYTES = 4\n    # 4 bytes for id, 4 bytes for buffer size\n    self.MD_SIZE = self.ID_NBYTES + self.SIZE_NBYTES\n    self.monotonic_id_end = 0\n    self.monotonic_id_start = 0\n    self.data_buffer_start = 0\n    self.data_buffer_end = 0\n\n    if create:\n        # we are creating a buffer\n        self.metadata: dict[int, int] = {}  # monotonic_id -> start address\n        self.shared_memory = shared_memory.SharedMemory(\n            create=True, size=self.data_buffer_size, name=name\n        )\n    else:\n        # we are opening an existing buffer\n        # fix to https://stackoverflow.com/q/62748654/9191338\n        # Python incorrectly tracks shared memory even if it is not\n        # created by the process. The following patch is a workaround.\n        with patch(\n            \"multiprocessing.resource_tracker.register\",\n            lambda *args, **kwargs: None,\n        ):\n            self.shared_memory = shared_memory.SharedMemory(name=name)\n            # See https://docs.python.org/3/library/multiprocessing.shared_memory.html # noqa\n            # Some platforms allocate memory based on page size,\n            # so the shared memory block size may be larger or equal\n            # to the requested size. The size parameter is ignored\n            # when attaching to an existing block.\n            assert self.shared_memory.size >= self.data_buffer_size\n\n    logger.debug(\n        \"Shared memory created/opened with name: %s, size: %d\",\n        self.shared_memory.name,\n        self.data_buffer_size,\n    )",
      "language": "python"
    },
    {
      "code": "access_buf(address: int)",
      "language": "unknown"
    },
    {
      "code": "access_buf(address: int)",
      "language": "unknown"
    },
    {
      "code": "234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248",
      "language": "unknown"
    },
    {
      "code": "@contextmanager\ndef access_buf(self, address: int):\n    buf_idx = address % self.data_buffer_size\n\n    # read metadata\n    metadata_buff = self.shared_memory.buf[buf_idx : buf_idx + self.MD_SIZE]\n    id = self.byte2int(metadata_buff[: self.ID_NBYTES])\n    size = self.byte2int(metadata_buff[self.ID_NBYTES : self.MD_SIZE])\n\n    # yield the data buffer and metadata\n    data_buff = self.shared_memory.buf[buf_idx + self.MD_SIZE : buf_idx + size]\n    with (\n        memoryview(data_buff) as data_view,\n    ):\n        yield data_view, (id, size)",
      "language": "python"
    },
    {
      "code": "@contextmanager\ndef access_buf(self, address: int):\n    buf_idx = address % self.data_buffer_size\n\n    # read metadata\n    metadata_buff = self.shared_memory.buf[buf_idx : buf_idx + self.MD_SIZE]\n    id = self.byte2int(metadata_buff[: self.ID_NBYTES])\n    size = self.byte2int(metadata_buff[self.ID_NBYTES : self.MD_SIZE])\n\n    # yield the data buffer and metadata\n    data_buff = self.shared_memory.buf[buf_idx + self.MD_SIZE : buf_idx + size]\n    with (\n        memoryview(data_buff) as data_view,\n    ):\n        yield data_view, (id, size)",
      "language": "python"
    },
    {
      "code": "allocate_buf(size: int) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "allocate_buf(size: int) -> tuple[int, int]",
      "language": "php"
    },
    {
      "code": "186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232",
      "language": "unknown"
    },
    {
      "code": "def allocate_buf(self, size: int) -> tuple[int, int]:\n    \"\"\"\n    Allocate a buffer `MD_SIZE` + `size` bytes in the shared memory.\n    Memory layout:\n    `[4-byte monotonic_id][4-byte size][buffer data...]`\n    \"\"\"\n    assert self.is_writer, \"Only the writer can allocate buffers.\"\n    assert size > 0, \"Size must be greater than 0\"\n    size += self.MD_SIZE  # add metadata size to the buffer size\n    # reset to beginning if the buffer does have enough contiguous space\n    buffer_end_reset = self.data_buffer_end % self.data_buffer_size\n    if buffer_end_reset + size > self.data_buffer_size:\n        buffer_end_reset = (\n            self.data_buffer_end // self.data_buffer_size + 1\n        ) * self.data_buffer_size\n    else:  # no reset needed\n        buffer_end_reset = self.data_buffer_end\n\n    # check if we have enough space in the data buffer\n    # i.e. if the new end (self.data_buffer_end + size)\n    # exceeds the start of the data buffer\n    occupied_size_new = buffer_end_reset + size - self.data_buffer_start\n    if occupied_size_new > self.data_buffer_size:\n        raise MemoryError(\n            \"Not enough space in the data buffer, \"\n            \"try calling free_buf() to free up space\"\n        )\n    self.data_buffer_end = buffer_end_reset\n\n    # first 4 bytes as the monotonic id\n    buf_idx = self.data_buffer_end % self.data_buffer_size\n    self.shared_memory.buf[buf_idx : buf_idx + self.ID_NBYTES] = self.int2byte(\n        self.monotonic_id_end\n    )\n    # next 4 bytes as the size of the data buffer\n    self.shared_memory.buf[buf_idx + self.ID_NBYTES : buf_idx + self.MD_SIZE] = (\n        self.int2byte(size)\n    )\n\n    # record metadata\n    self.metadata[self.monotonic_id_end % self.ID_MAX] = self.data_buffer_end\n    # update buffer and monotonic id indices\n    current_buffer_end = self.data_buffer_end\n    current_id_end = self.monotonic_id_end\n    self.data_buffer_end += size\n    self.monotonic_id_end = (self.monotonic_id_end + 1) % self.ID_MAX\n    return current_buffer_end, current_id_end",
      "language": "python"
    },
    {
      "code": "def allocate_buf(self, size: int) -> tuple[int, int]:\n    \"\"\"\n    Allocate a buffer `MD_SIZE` + `size` bytes in the shared memory.\n    Memory layout:\n    `[4-byte monotonic_id][4-byte size][buffer data...]`\n    \"\"\"\n    assert self.is_writer, \"Only the writer can allocate buffers.\"\n    assert size > 0, \"Size must be greater than 0\"\n    size += self.MD_SIZE  # add metadata size to the buffer size\n    # reset to beginning if the buffer does have enough contiguous space\n    buffer_end_reset = self.data_buffer_end % self.data_buffer_size\n    if buffer_end_reset + size > self.data_buffer_size:\n        buffer_end_reset = (\n            self.data_buffer_end // self.data_buffer_size + 1\n        ) * self.data_buffer_size\n    else:  # no reset needed\n        buffer_end_reset = self.data_buffer_end\n\n    # check if we have enough space in the data buffer\n    # i.e. if the new end (self.data_buffer_end + size)\n    # exceeds the start of the data buffer\n    occupied_size_new = buffer_end_reset + size - self.data_buffer_start\n    if occupied_size_new > self.data_buffer_size:\n        raise MemoryError(\n            \"Not enough space in the data buffer, \"\n            \"try calling free_buf() to free up space\"\n        )\n    self.data_buffer_end = buffer_end_reset\n\n    # first 4 bytes as the monotonic id\n    buf_idx = self.data_buffer_end % self.data_buffer_size\n    self.shared_memory.buf[buf_idx : buf_idx + self.ID_NBYTES] = self.int2byte(\n        self.monotonic_id_end\n    )\n    # next 4 bytes as the size of the data buffer\n    self.shared_memory.buf[buf_idx + self.ID_NBYTES : buf_idx + self.MD_SIZE] = (\n        self.int2byte(size)\n    )\n\n    # record metadata\n    self.metadata[self.monotonic_id_end % self.ID_MAX] = self.data_buffer_end\n    # update buffer and monotonic id indices\n    current_buffer_end = self.data_buffer_end\n    current_id_end = self.monotonic_id_end\n    self.data_buffer_end += size\n    self.monotonic_id_end = (self.monotonic_id_end + 1) % self.ID_MAX\n    return current_buffer_end, current_id_end",
      "language": "python"
    },
    {
      "code": "byte2int(byte_data: bytes) -> int",
      "language": "php"
    },
    {
      "code": "byte2int(byte_data: bytes) -> int",
      "language": "php"
    },
    {
      "code": "182\n183\n184",
      "language": "unknown"
    },
    {
      "code": "def byte2int(self, byte_data: bytes) -> int:\n    \"\"\"Convert bytes back to an integer.\"\"\"\n    return int.from_bytes(byte_data, \"little\", signed=True)",
      "language": "python"
    },
    {
      "code": "def byte2int(self, byte_data: bytes) -> int:\n    \"\"\"Convert bytes back to an integer.\"\"\"\n    return int.from_bytes(byte_data, \"little\", signed=True)",
      "language": "python"
    },
    {
      "code": "clear() -> None",
      "language": "rust"
    },
    {
      "code": "clear() -> None",
      "language": "rust"
    },
    {
      "code": "163\n164\n165\n166\n167\n168\n169\n170",
      "language": "unknown"
    },
    {
      "code": "def clear(self) -> None:\n    \"\"\"Clear the ring buffer.\"\"\"\n    assert self.is_writer, \"Only the writer can clear the buffer.\"\n    self.metadata.clear()\n    self.monotonic_id_end = 0\n    self.monotonic_id_start = 0\n    self.data_buffer_start = 0\n    self.data_buffer_end = 0",
      "language": "python"
    },
    {
      "code": "def clear(self) -> None:\n    \"\"\"Clear the ring buffer.\"\"\"\n    assert self.is_writer, \"Only the writer can clear the buffer.\"\n    self.metadata.clear()\n    self.monotonic_id_end = 0\n    self.monotonic_id_start = 0\n    self.data_buffer_start = 0\n    self.data_buffer_end = 0",
      "language": "python"
    },
    {
      "code": "free_buf(\n    is_free_fn: Callable[[int, memoryview], bool],\n    nbytes: int | None = None,\n) -> Iterable[int]",
      "language": "rust"
    },
    {
      "code": "free_buf(\n    is_free_fn: Callable[[int, memoryview], bool],\n    nbytes: int | None = None,\n) -> Iterable[int]",
      "language": "rust"
    },
    {
      "code": "250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323",
      "language": "unknown"
    },
    {
      "code": "def free_buf(\n    self,\n    is_free_fn: Callable[[int, memoryview], bool],\n    nbytes: int | None = None,\n) -> Iterable[int]:\n    \"\"\"\n    Free a buffer of the given size. This is a no-op in shared memory,\n    but we need to keep track of the metadata.\n\n    If freed memory spreads across the end and start of the ring buffer,\n    the actual freed memory will be in two segments. In this case there\n    still might not be a contiguous space of `nbytes` available.\n\n    Args:\n        nbytes (int, optional): The size of the buffer to free. If None,\n            frees the maximum size of the ring buffer.\n    \"\"\"\n\n    assert self.is_writer, \"Only the writer can free buffers.\"\n    logger.debug(\n        \"Freeing up space in the ring buffer, \"\n        \"monotonic_id_start: %d, monotonic_id_end: %d\",\n        self.monotonic_id_start,\n        self.monotonic_id_end,\n    )\n    monotonic_id_before = self.monotonic_id_start\n    # if nbytes is None, free up the maximum size of the ring buffer\n    if nbytes is None:\n        nbytes = self.data_buffer_size\n    freed_bytes = 0\n    while self.monotonic_id_start in self.metadata and freed_bytes < nbytes:\n        address = self.metadata[self.monotonic_id_start]\n        with self.access_buf(address) as (data_buff, metadata):\n            if is_free_fn(self.monotonic_id_start, data_buff):\n                # check passed, we can free the buffer\n                del self.metadata[self.monotonic_id_start]\n                self.monotonic_id_start = (\n                    self.monotonic_id_start + 1\n                ) % self.ID_MAX\n                if self.monotonic_id_start in self.metadata:\n                    # pointing to the start addr of next allocation\n                    self.data_buffer_start += (\n                        self.metadata[self.monotonic_id_start]\n                        - self.data_buffer_start\n                    ) % self.data_buffer_size\n                else:\n                    # no remaining allocation, reset to zero\n                    self.data_buffer_start = self.data_buffer_end = 0\n                freed_bytes += metadata[1]\n            else:\n                # there are still readers, we cannot free the buffer\n                break\n\n    logger.debug(\n        \"Freed %d bytes from the ring buffer, \"\n        \"monotonic_id_start: %d, monotonic_id_end: %d\",\n        freed_bytes,\n        self.monotonic_id_start,\n        self.monotonic_id_end,\n    )\n\n    # buffer wrap around\n    if self.data_buffer_start >= self.data_buffer_size:\n        self.data_buffer_start -= self.data_buffer_size\n        self.data_buffer_end -= self.data_buffer_size\n\n    monotonic_id_after = self.monotonic_id_start\n    # id wrap around\n    if monotonic_id_after >= monotonic_id_before:\n        return range(monotonic_id_before, monotonic_id_after)\n    else:\n        return chain(\n            range(monotonic_id_before, self.ID_MAX), range(0, monotonic_id_after)\n        )",
      "language": "python"
    },
    {
      "code": "def free_buf(\n    self,\n    is_free_fn: Callable[[int, memoryview], bool],\n    nbytes: int | None = None,\n) -> Iterable[int]:\n    \"\"\"\n    Free a buffer of the given size. This is a no-op in shared memory,\n    but we need to keep track of the metadata.\n\n    If freed memory spreads across the end and start of the ring buffer,\n    the actual freed memory will be in two segments. In this case there\n    still might not be a contiguous space of `nbytes` available.\n\n    Args:\n        nbytes (int, optional): The size of the buffer to free. If None,\n            frees the maximum size of the ring buffer.\n    \"\"\"\n\n    assert self.is_writer, \"Only the writer can free buffers.\"\n    logger.debug(\n        \"Freeing up space in the ring buffer, \"\n        \"monotonic_id_start: %d, monotonic_id_end: %d\",\n        self.monotonic_id_start,\n        self.monotonic_id_end,\n    )\n    monotonic_id_before = self.monotonic_id_start\n    # if nbytes is None, free up the maximum size of the ring buffer\n    if nbytes is None:\n        nbytes = self.data_buffer_size\n    freed_bytes = 0\n    while self.monotonic_id_start in self.metadata and freed_bytes < nbytes:\n        address = self.metadata[self.monotonic_id_start]\n        with self.access_buf(address) as (data_buff, metadata):\n            if is_free_fn(self.monotonic_id_start, data_buff):\n                # check passed, we can free the buffer\n                del self.metadata[self.monotonic_id_start]\n                self.monotonic_id_start = (\n                    self.monotonic_id_start + 1\n                ) % self.ID_MAX\n                if self.monotonic_id_start in self.metadata:\n                    # pointing to the start addr of next allocation\n                    self.data_buffer_start += (\n                        self.metadata[self.monotonic_id_start]\n                        - self.data_buffer_start\n                    ) % self.data_buffer_size\n                else:\n                    # no remaining allocation, reset to zero\n                    self.data_buffer_start = self.data_buffer_end = 0\n                freed_bytes += metadata[1]\n            else:\n                # there are still readers, we cannot free the buffer\n                break\n\n    logger.debug(\n        \"Freed %d bytes from the ring buffer, \"\n        \"monotonic_id_start: %d, monotonic_id_end: %d\",\n        freed_bytes,\n        self.monotonic_id_start,\n        self.monotonic_id_end,\n    )\n\n    # buffer wrap around\n    if self.data_buffer_start >= self.data_buffer_size:\n        self.data_buffer_start -= self.data_buffer_size\n        self.data_buffer_end -= self.data_buffer_size\n\n    monotonic_id_after = self.monotonic_id_start\n    # id wrap around\n    if monotonic_id_after >= monotonic_id_before:\n        return range(monotonic_id_before, monotonic_id_after)\n    else:\n        return chain(\n            range(monotonic_id_before, self.ID_MAX), range(0, monotonic_id_after)\n        )",
      "language": "python"
    },
    {
      "code": "157\n158\n159\n160\n161",
      "language": "unknown"
    },
    {
      "code": "def handle(self):\n    return (\n        self.data_buffer_size,\n        self.shared_memory.name,\n    )",
      "language": "python"
    },
    {
      "code": "def handle(self):\n    return (\n        self.data_buffer_size,\n        self.shared_memory.name,\n    )",
      "language": "python"
    },
    {
      "code": "int2byte(integer: int) -> bytes",
      "language": "php"
    },
    {
      "code": "int2byte(integer: int) -> bytes",
      "language": "php"
    },
    {
      "code": "178\n179\n180",
      "language": "unknown"
    },
    {
      "code": "def int2byte(self, integer: int) -> bytes:\n    \"\"\"Convert an integer to bytes.\"\"\"\n    return integer.to_bytes(self.ID_NBYTES, \"little\", signed=True)",
      "language": "python"
    },
    {
      "code": "def int2byte(self, integer: int) -> bytes:\n    \"\"\"Convert an integer to bytes.\"\"\"\n    return integer.to_bytes(self.ID_NBYTES, \"little\", signed=True)",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}