{
  "url": "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
  "title": "Vision Language - vLLM",
  "content": "Source https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/vision_language.py.",
  "headings": [
    {
      "level": "h1",
      "text": "Vision Language¶",
      "id": "vision-language"
    }
  ],
  "code_samples": [
    {
      "code": "# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\"\"\"\nThis example shows how to use vLLM for running offline inference with\nthe correct prompt format on vision language models for text generation.\n\nFor most models, the prompt format should follow corresponding examples\non HuggingFace model repository.\n\"\"\"\n\nimport os\nimport random\nfrom contextlib import contextmanager\nfrom dataclasses import asdict\nfrom typing import NamedTuple\n\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoTokenizer\n\nfrom vllm import LLM, EngineArgs, SamplingParams\nfrom vllm.assets.image import ImageAsset\nfrom vllm.assets.video import VideoAsset\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal.image import convert_image_mode\nfrom vllm.utils.argparse_utils import FlexibleArgumentParser\n\n\nclass ModelRequestData(NamedTuple):\n    engine_args: EngineArgs\n    prompts: list[str]\n    stop_token_ids: list[int] | None = None\n    lora_requests: list[LoRARequest] | None = None\n    sampling_params: list[SamplingParams] | None = None\n\n\n# NOTE: The default `max_num_seqs` and `max_model_len` may result in OOM on\n# lower-end GPUs.\n# Unless specified, these settings have been tested to work on a single L4.\n\n\n# Aria\ndef run_aria(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"rhymes-ai/Aria\"\n\n    # NOTE: Need L40 (or equivalent) to avoid OOM\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        dtype=\"bfloat16\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        (\n            f\"<|im_start|>user\\n<fim_prefix><|img|><fim_suffix>{question}\"\n            \"<|im_end|>\\n<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    stop_token_ids = [93532, 93653, 944, 93421, 1019, 93653, 93519]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# Aya Vision\ndef run_aya_vision(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"CohereLabs/aya-vision-8b\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=2048,\n        max_num_seqs=2,\n        mm_processor_kwargs={\"crop_to_patches\": True},\n        limit_mm_per_prompt={modality: 1},\n    )\n    prompts = [\n        f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|><image>{question}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        for question in questions\n    ]\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Bee-8B\ndef run_bee(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"Open-Bee/Bee-8B-RL\"\n\n    prompts = [\n        (\n            f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<image>\\n{question}<|im_end|>\"\n            f\"<|im_start|>assistant\\n<think>\\n\"\n        )\n        for question in questions\n    ]\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=16384,\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_bagel(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"ByteDance-Seed/BAGEL-7B-MoT\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        (\n            f\"<|im_start|>user\\n<|image_pad|>\\n{question}<|im_end|>\\n\"\n            f\"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# BLIP-2\ndef run_blip2(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    # BLIP-2 prompt format is inaccurate on HuggingFace model repository.\n    # See https://huggingface.co/Salesforce/blip2-opt-2.7b/discussions/15#64ff02f3f8cf9e4f5b038262 #noqa\n    prompts = [f\"Question: {question} Answer:\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"Salesforce/blip2-opt-2.7b\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Chameleon\ndef run_chameleon(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"{question}<image>\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"facebook/chameleon-7b\",\n        max_model_len=4096,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_command_a_vision(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"CohereLabs/command-a-vision-07-2025\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=32768,\n        tensor_parallel_size=4,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|><|IMG_PATCH|>{question}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Deepseek-VL2\ndef run_deepseek_vl2(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"deepseek-ai/deepseek-vl2-tiny\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        hf_overrides={\"architectures\": [\"DeepseekVLV2ForCausalLM\"]},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        f\"<|User|>: <image>\\n{question}\\n\\n<|Assistant|>:\" for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_deepseek_ocr(questions: list[str], modality: str) -> ModelRequestData:\n    from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\n\n    assert modality == \"image\"\n\n    model_name = \"deepseek-ai/DeepSeek-OCR\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        limit_mm_per_prompt={modality: 1},\n        logits_processors=[NGramPerReqLogitsProcessor],\n    )\n\n    # deepseek-ocr use plain prompt template\n    prompts = [f\"<image>\\n{question}\" for question in questions]\n\n    # The following sampling params config is taken from\n    # the official Deepseek-OCR inference example.\n    # (IMPORTANT) Use the custom logits processor and avoid skipping\n    # special tokens for this model for the optimal OCR performance.\n    sampling_params = [\n        SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                # whitelist: <td>, </td>\n                whitelist_token_ids={128821, 128822},\n            ),\n            skip_special_tokens=False,\n        )\n        for _ in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        sampling_params=sampling_params,\n    )\n\n\n# Dots-OCR\ndef run_dots_ocr(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"<|img|><|imgpad|><|endofimg|>{question}\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"rednote-hilab/dots.ocr\",\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Ernie4.5-VL\ndef run_ernie45_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"baidu/ERNIE-4.5-VL-28B-A3B-PT\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n    )\n\n    if modality == \"image\":\n        placeholder = \"Picture 1:<|IMAGE_START|><|image@placeholder|><|IMAGE_END|>\"\n    elif modality == \"video\":\n        placeholder = \"Video 1:<|VIDEO_START|><|video@placeholder|><|VIDEO_END|>\"\n\n    prompts = [\n        (\n            f\"<|begin_of_sentence|>User: {question}{placeholder}\\n\"\n            \"Assistant: <think></think>\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Fuyu\ndef run_fuyu(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"{question}\\n\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"adept/fuyu-8b\",\n        max_model_len=2048,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Gemma 3\ndef run_gemma3(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"google/gemma-3-4b-it\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=2048,\n        max_num_seqs=2,\n        mm_processor_kwargs={\"do_pan_and_scan\": True},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        (\n            \"<bos><start_of_turn>user\\n\"\n            f\"<start_of_image>{question}<end_of_turn>\\n\"\n            \"<start_of_turn>model\\n\"\n        )\n        for question in questions\n    ]\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Gemma3N\ndef run_gemma3n(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"google/gemma-3n-E2B-it\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=2048,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n    )\n\n    prompts = [\n        (\n            \"<start_of_turn>user\\n\"\n            f\"<image_soft_token>{question}<end_of_turn>\\n\"\n            \"<start_of_turn>model\\n\"\n        )\n        for question in questions\n    ]\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# GLM-4v\ndef run_glm4v(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"zai-org/glm-4v-9b\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=2048,\n        max_num_seqs=2,\n        trust_remote_code=True,\n        enforce_eager=True,\n        hf_overrides={\"architectures\": [\"GLM4VForCausalLM\"]},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        (\n            \"<|user|>\\n<|begin_of_image|><|endoftext|><|end_of_image|>\"\n            f\"{question}<|assistant|>\"\n        )\n        for question in questions\n    ]\n\n    stop_token_ids = [151329, 151336, 151338]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# GLM-4.1V\ndef run_glm4_1v(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"zai-org/GLM-4.1V-9B-Thinking\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        mm_processor_kwargs={\n            \"size\": {\"shortest_edge\": 12544, \"longest_edge\": 47040000},\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|begin_of_image|><|image|><|end_of_image|>\"\n    elif modality == \"video\":\n        placeholder = \"<|begin_of_video|><|video|><|end_of_video|>\"\n\n    prompts = [\n        (\n            \"[gMASK]<sop><|system|>\\nYou are a helpful assistant.<|user|>\\n\"\n            f\"{placeholder}\"\n            f\"{question}<|assistant|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# GLM-4.5V\ndef run_glm4_5v(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"zai-org/GLM-4.5V\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        mm_processor_kwargs={\n            \"size\": {\"shortest_edge\": 12544, \"longest_edge\": 47040000},\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n        tensor_parallel_size=4,\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|begin_of_image|><|image|><|end_of_image|>\"\n    elif modality == \"video\":\n        placeholder = \"<|begin_of_video|><|video|><|end_of_video|>\"\n\n    prompts = [\n        (\n            \"[gMASK]<sop><|system|>\\nYou are a helpful assistant.<|user|>\\n\"\n            f\"{placeholder}\"\n            f\"{question}<|assistant|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# GLM-4.5V-FP8\ndef run_glm4_5v_fp8(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"zai-org/GLM-4.5V-FP8\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        mm_processor_kwargs={\n            \"size\": {\"shortest_edge\": 12544, \"longest_edge\": 47040000},\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n        tensor_parallel_size=4,\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|begin_of_image|><|image|><|end_of_image|>\"\n    elif modality == \"video\":\n        placeholder = \"<|begin_of_video|><|video|><|end_of_video|>\"\n\n    prompts = [\n        (\n            \"[gMASK]<sop><|system|>\\nYou are a helpful assistant.<|user|>\\n\"\n            f\"{placeholder}\"\n            f\"{question}<|assistant|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# H2OVL-Mississippi\ndef run_h2ovl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"h2oai/h2ovl-mississippi-800m\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    # Stop tokens for H2OVL-Mississippi\n    # https://huggingface.co/h2oai/h2ovl-mississippi-800m\n    stop_token_ids = [tokenizer.eos_token_id]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# HunyuanOCR\ndef run_hunyuan_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"tencent/HunyuanOCR\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    placeholder = \"<｜hy_place▁holder▁no▁100｜><｜hy_place▁holder▁no▁102｜><｜hy_place▁holder▁no▁101｜>\"  # noqa: E501\n    prompts = [\n        f\"<｜hy_begin▁of▁sentence｜>{placeholder}{question}<｜hy_User｜>\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=None,\n    )\n\n\n# naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B\ndef run_hyperclovax_seed_vision(\n    questions: list[str], modality: str\n) -> ModelRequestData:\n    model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192 if modality == \"image\" else 16384,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    messages = list()\n    for question in questions:\n        if modality == \"image\":\n            \"\"\"\n            ocr: List the words in the image in raster order.\n                Even if the word order feels unnatural for reading,\n                the model will handle it as long as it follows raster order.\n                e.g. \"Naver, CLOVA, bigshane\"\n            lens_keywords: List the entity names in the image.\n                e.g. \"iPhone\"\n            lens_local_keywords: List the entity names with quads in the image.\n                e.g. \"[0.07, 0.21, 0.92, 0.90] iPhone\"\n            \"\"\"\n            messages.append(\n                [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"image\",\n                                \"ocr\": \"\",\n                                \"lens_keywords\": \"\",\n                                \"lens_local_keywords\": \"\",\n                            },\n                            {\n                                \"type\": \"text\",\n                                \"text\": question,\n                            },\n                        ],\n                    }\n                ]\n            )\n        elif modality == \"video\":\n            messages.append(\n                [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"video\",\n                            },\n                            {\n                                \"type\": \"text\",\n                                \"text\": question,\n                            },\n                        ],\n                    }\n                ]\n            )\n        else:\n            raise ValueError(f\"Unsupported modality: {modality}\")\n\n    prompts = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=None,\n    )\n\n\n# Idefics3-8B-Llama3\ndef run_idefics3(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"HuggingFaceM4/Idefics3-8B-Llama3\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        max_num_seqs=2,\n        enforce_eager=True,\n        # if you are running out of memory, you can reduce the \"longest_edge\".\n        # see: https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3#model-optimizations\n        mm_processor_kwargs={\n            \"size\": {\"longest_edge\": 3 * 364},\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n    prompts = [\n        (f\"<|begin_of_text|>User:<image>{question}<end_of_utterance>\\nAssistant:\")\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Intern-S1\ndef run_interns1(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"internlm/Intern-S1-mini\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n    )\n\n    if modality == \"image\":\n        placeholder = \"<IMG_CONTEXT>\"\n    elif modality == \"video\":\n        placeholder = \"<video>\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"{placeholder}\\n{question}\"}]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# InternVL\ndef run_internvl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"OpenGVLab/InternVL3-2B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<image>\"\n    elif modality == \"video\":\n        placeholder = \"<video>\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"{placeholder}\\n{question}\"}]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    # Stop tokens for InternVL\n    # models variants may have different stop tokens\n    # please refer to the model card for the correct \"stop words\":\n    # https://huggingface.co/OpenGVLab/InternVL2-2B/blob/main/conversation.py\n    stop_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|end|>\"]\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n    stop_token_ids = [token_id for token_id in stop_token_ids if token_id is not None]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# Keye-VL\ndef run_keye_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Kwai-Keye/Keye-VL-8B-Preview\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        trust_remote_code=True,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Keye-VL-1.5\ndef run_keye_vl1_5(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Kwai-Keye/Keye-VL-1.5-8B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        trust_remote_code=True,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Kimi-VL\ndef run_kimi_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [\n        \"<|im_user|>user<|im_middle|><|media_start|>image<|media_content|>\"\n        f\"<|media_pad|><|media_end|>{question}<|im_end|>\"\n        \"<|im_assistant|>assistant<|im_middle|>\"\n        for question in questions\n    ]\n\n    engine_args = EngineArgs(\n        model=\"moonshotai/Kimi-VL-A3B-Instruct\",\n        trust_remote_code=True,\n        max_model_len=4096,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# LightOnOCR\ndef run_lightonocr(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [\n        \"<|im_start|>system<|im_end|>\\n<|im_start|>user\\n<|image_pad|><|im_end|>\\n<|im_start|>assistant\\n\"\n        for _ in questions\n    ]\n\n    engine_args = EngineArgs(\n        model=\"lightonai/LightOnOCR-1B\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_llama4(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        max_num_seqs=4,\n        tensor_parallel_size=8,\n        gpu_memory_utilization=0.4,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    messages = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": f\"{question}\"}],\n            }\n        ]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, add_generation_prompt=True, tokenize=False\n    )\n    stop_token_ids = None\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# LLaVA-1.5\ndef run_llava(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"USER: <image>\\n{question}\\nASSISTANT:\" for question in questions]\n\n    engine_args = EngineArgs(\n        model=\"llava-hf/llava-1.5-7b-hf\",\n        max_model_len=4096,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# LLaVA-1.6/LLaVA-NeXT\ndef run_llava_next(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"[INST] <image>\\n{question} [/INST]\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# LlaVA-NeXT-Video\n# Currently only support for video input\ndef run_llava_next_video(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"video\"\n\n    prompts = [f\"USER: <video>\\n{question} ASSISTANT:\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n        max_model_len=8192,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# LLaVA-OneVision\ndef run_llava_onevision(questions: list[str], modality: str) -> ModelRequestData:\n    if modality == \"video\":\n        prompts = [\n            f\"<|im_start|>user <video>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n            for question in questions\n        ]\n\n    elif modality == \"image\":\n        prompts = [\n            f\"<|im_start|>user <image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n            for question in questions\n        ]\n\n    engine_args = EngineArgs(\n        model=\"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n        max_model_len=16384,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Mantis\ndef run_mantis(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    llama3_template = \"<|start_header_id|>user<|end_header_id|>\\n\\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"  # noqa: E501\n    prompts = [llama3_template.format(f\"{question}\\n<image>\") for question in questions]\n\n    engine_args = EngineArgs(\n        model=\"TIGER-Lab/Mantis-8B-siglip-llama3\",\n        max_model_len=4096,\n        hf_overrides={\"architectures\": [\"MantisForConditionalGeneration\"]},\n        limit_mm_per_prompt={modality: 1},\n    )\n    stop_token_ids = [128009]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# MiniCPM-V\ndef run_minicpmv_base(questions: list[str], modality: str, model_name):\n    assert modality in [\"image\", \"video\"]\n    # If you want to use `MiniCPM-o-2_6` with audio inputs, check `audio_language.py` # noqa\n\n    # 2.0\n    # The official repo doesn't work yet, so we need to use a fork for now\n    # For more details, please see: See: https://github.com/vllm-project/vllm/pull/4087#issuecomment-2250397630 # noqa\n    # model_name = \"HwwwH/MiniCPM-V-2\"\n\n    # 2.5\n    # model_name = \"openbmb/MiniCPM-Llama3-V-2_5\"\n\n    # 2.6\n    # model_name = \"openbmb/MiniCPM-V-2_6\"\n    # o2.6\n\n    # modality supports\n    # 2.0: image\n    # 2.5: image\n    # 2.6: image, video\n    # o2.6: image, video, audio\n    # model_name = \"openbmb/MiniCPM-o-2_6\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        trust_remote_code=True,\n        limit_mm_per_prompt={modality: 1},\n    )\n    # NOTE The stop_token_ids are different for various versions of MiniCPM-V\n    # 2.0\n    # stop_token_ids = [tokenizer.eos_id]\n\n    # 2.5\n    # stop_token_ids = [tokenizer.eos_id, tokenizer.eot_id]\n\n    # 2.6 / o2.6\n    stop_tokens = [\"<|im_end|>\", \"<|endoftext|>\"]\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n\n    modality_placeholder = {\n        \"image\": \"(<image>./</image>)\",\n        \"video\": \"(<video>./</video>)\",\n    }\n\n    prompts = [\n        tokenizer.apply_chat_template(\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"{modality_placeholder[modality]}\\n{question}\",\n                }\n            ],\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\ndef run_minicpmo(questions: list[str], modality: str) -> ModelRequestData:\n    return run_minicpmv_base(questions, modality, \"openbmb/MiniCPM-o-2_6\")\n\n\ndef run_minicpmv(questions: list[str], modality: str) -> ModelRequestData:\n    return run_minicpmv_base(questions, modality, \"openbmb/MiniCPM-V-2_6\")\n\n\ndef run_minimax_vl_01(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"MiniMaxAI/MiniMax-VL-01\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n        tensor_parallel_size=8,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    messages = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question}],\n            }\n        ]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, add_generation_prompt=True, tokenize=False\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Mistral-3 HF-format\ndef run_mistral3(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\n    # NOTE: Need L40 (or equivalent) to avoid OOM\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        max_num_seqs=2,\n        tensor_parallel_size=2,\n        limit_mm_per_prompt={modality: 1},\n        ignore_patterns=[\"consolidated.safetensors\"],\n    )\n\n    prompts = [f\"<s>[INST]{question}\\n[IMG][/INST]\" for question in questions]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Molmo\ndef run_molmo(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"allenai/Molmo-7B-D-0924\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        dtype=\"bfloat16\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        f\"<|im_start|>user <image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Nemontron_VL\ndef run_nemotron_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    assert modality == \"image\"\n    placeholder = \"<image>\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"{placeholder}\\n{question}\"}]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    # Stop tokens for InternVL\n    # models variants may have different stop tokens\n    # please refer to the model card for the correct \"stop words\":\n    # https://huggingface.co/OpenGVLab/InternVL2-2B/blob/main/conversation.py\n    stop_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|end|>\"]\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n    stop_token_ids = [token_id for token_id in stop_token_ids if token_id is not None]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# NVLM-D\ndef run_nvlm_d(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"nvidia/NVLM-D-72B\"\n\n    # Adjust this as necessary to fit in GPU\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=4096,\n        tensor_parallel_size=4,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Ovis\ndef run_ovis(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"AIDC-AI/Ovis2-1B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        trust_remote_code=True,\n        dtype=\"half\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Ovis2_5\ndef run_ovis2_5(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"AIDC-AI/Ovis2.5-2B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        trust_remote_code=True,\n        dtype=\"half\",\n        limit_mm_per_prompt={modality: 1},\n    )\n    if modality == \"image\":\n        placeholder = \"<image>\"\n    elif modality == \"video\":\n        placeholder = \"<video>\"\n\n    prompts = [\n        f\"<|im_start|>user\\n\\n{placeholder}\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# PaddleOCR-VL\ndef run_paddleocr_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"PaddlePaddle/PaddleOCR-VL\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n    )\n\n    placeholder = \"<|IMAGE_START|><|IMAGE_PLACEHOLDER|><|IMAGE_END|>\"\n    prompts = [\n        (f\"<|begin_of_sentence|>User: {question}{placeholder}\\nAssistant: \")\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# PaliGemma\ndef run_paligemma(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    # PaliGemma has special prompt format for VQA\n    prompts = [\"caption en\" for _ in questions]\n    engine_args = EngineArgs(\n        model=\"google/paligemma-3b-mix-224\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# PaliGemma 2\ndef run_paligemma2(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    # PaliGemma 2 has special prompt format for VQA\n    prompts = [\"caption en\" for _ in questions]\n    engine_args = EngineArgs(\n        model=\"google/paligemma2-3b-ft-docci-448\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Phi-3-Vision\ndef run_phi3v(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [\n        f\"<|user|>\\n<|image_1|>\\n{question}<|end|>\\n<|assistant|>\\n\"\n        for question in questions\n    ]\n\n    # num_crops is an override kwarg to the multimodal image processor;\n    # For some models, e.g., Phi-3.5-vision-instruct, it is recommended\n    # to use 16 for single frame scenarios, and 4 for multi-frame.\n    #\n    # Generally speaking, a larger value for num_crops results in more\n    # tokens per image instance, because it may scale the image more in\n    # the image preprocessing. Some references in the model docs and the\n    # formula for image tokens after the preprocessing\n    # transform can be found below.\n    #\n    # https://huggingface.co/microsoft/Phi-3.5-vision-instruct#loading-the-model-locally\n    # https://huggingface.co/microsoft/Phi-3.5-vision-instruct/blob/main/processing_phi3_v.py#L194\n    engine_args = EngineArgs(\n        model=\"microsoft/Phi-3.5-vision-instruct\",\n        trust_remote_code=True,\n        max_model_len=4096,\n        max_num_seqs=2,\n        # Note - mm_processor_kwargs can also be passed to generate/chat calls\n        mm_processor_kwargs={\"num_crops\": 16},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Phi-4-multimodal-instruct\ndef run_phi4mm(questions: list[str], modality: str) -> ModelRequestData:\n    \"\"\"\n    Phi-4-multimodal-instruct supports both image and audio inputs. Here, we\n    show how to process image inputs.\n    \"\"\"\n    assert modality == \"image\"\n    model_path = snapshot_download(\"microsoft/Phi-4-multimodal-instruct\")\n    # Since the vision-lora and speech-lora co-exist with the base model,\n    # we have to manually specify the path of the lora weights.\n    vision_lora_path = os.path.join(model_path, \"vision-lora\")\n    prompts = [\n        f\"<|user|><|image_1|>{question}<|end|><|assistant|>\" for question in questions\n    ]\n    engine_args = EngineArgs(\n        model=model_path,\n        trust_remote_code=True,\n        max_model_len=5120,\n        max_num_seqs=2,\n        max_num_batched_tokens=12800,\n        enable_lora=True,\n        max_lora_rank=320,\n        # Note - mm_processor_kwargs can also be passed to generate/chat calls\n        mm_processor_kwargs={\"dynamic_hd\": 16},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        lora_requests=[LoRARequest(\"vision\", 1, vision_lora_path)],\n    )\n\n\n# HF format Phi-4-multimodal-instruct\ndef run_phi4_multimodal(questions: list[str], modality: str) -> ModelRequestData:\n    \"\"\"\n    Phi-4-multimodal-instruct supports both image and audio inputs. Here, we\n    show how to process image inputs.\n    \"\"\"\n    assert modality == \"image\"\n    model_path = snapshot_download(\n        \"microsoft/Phi-4-multimodal-instruct\", revision=\"refs/pr/70\"\n    )\n    # Since the vision-lora and speech-lora co-exist with the base model,\n    # we have to manually specify the path of the lora weights.\n    vision_lora_path = os.path.join(model_path, \"vision-lora\")\n    prompts = [\n        f\"<|user|><|image|>{question}<|end|><|assistant|>\" for question in questions\n    ]\n    engine_args = EngineArgs(\n        model=model_path,\n        max_model_len=5120,\n        max_num_seqs=2,\n        max_num_batched_tokens=12800,\n        enable_lora=True,\n        max_lora_rank=320,\n        # Note - mm_processor_kwargs can also be passed to generate/chat calls\n        mm_processor_kwargs={\"dynamic_hd\": 16},\n        limit_mm_per_prompt={\"image\": 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        lora_requests=[LoRARequest(\"vision\", 1, vision_lora_path)],\n    )\n\n\n# Pixtral HF-format\ndef run_pixtral_hf(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"mistral-community/pixtral-12b\"\n\n    # NOTE: Need L40 (or equivalent) to avoid OOM\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=6144,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [f\"<s>[INST]{question}\\n[IMG][/INST]\" for question in questions]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen-VL\ndef run_qwen_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    engine_args = EngineArgs(\n        model=\"Qwen/Qwen-VL\",\n        trust_remote_code=True,\n        max_model_len=1024,\n        max_num_seqs=2,\n        hf_overrides={\"architectures\": [\"QwenVLForConditionalGeneration\"]},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [f\"{question}Picture 1: <img></img>\\n\" for question in questions]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen2-VL\ndef run_qwen2_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        # Note - mm_processor_kwargs can also be passed to generate/chat calls\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen2.5-VL\ndef run_qwen2_5_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen2.5-Omni\ndef run_qwen2_5_omni(questions: list[str], modality: str):\n    model_name = \"Qwen/Qwen2.5-Omni-7B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|IMAGE|>\"\n    elif modality == \"video\":\n        placeholder = \"<|VIDEO|>\"\n\n    default_system = (\n        \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba \"\n        \"Group, capable of perceiving auditory and visual inputs, as well as \"\n        \"generating text and speech.\"\n    )\n\n    prompts = [\n        (\n            f\"<|im_start|>system\\n{default_system}<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_bos|>{placeholder}<|vision_eos|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen3-VL-Dense\ndef run_qwen3_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Qwen/Qwen3-VL-4B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen3-VL-MOE\ndef run_qwen3_vl_moe(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Qwen/Qwen3-VL-30B-A3B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# R-4B\ndef run_r_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"YannQi/R-4B\"\n\n    prompts = [\n        f\"<|im_start|>user <image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n        for question in questions\n    ]\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=16384,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# SkyworkR1V\ndef run_skyworkr1v(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"Skywork/Skywork-R1V-38B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=4096,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    # Stop tokens for SkyworkR1V\n    # https://huggingface.co/Skywork/Skywork-R1V-38B/blob/main/conversation.py\n    stop_tokens = [\"<｜end▁of▁sentence｜>\", \"<|endoftext|>\"]\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# SmolVLM2-2.2B-Instruct\ndef run_smolvlm(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        max_num_seqs=2,\n        enforce_eager=True,\n        mm_processor_kwargs={\n            \"max_image_size\": {\"longest_edge\": 384},\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n    prompts = [\n        (f\"<|im_start|>User:<image>{question}<end_of_utterance>\\nAssistant:\")\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Step3\ndef run_step3(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"stepfun-ai/step3-fp8\"\n\n    # NOTE: Below are verified configurations for step3-fp8\n    # on 8xH100 GPUs.\n    engine_args = EngineArgs(\n        model=model_name,\n        max_num_batched_tokens=4096,\n        gpu_memory_utilization=0.85,\n        tensor_parallel_size=8,\n        limit_mm_per_prompt={modality: 1},\n        reasoning_parser=\"step3\",\n    )\n\n    prompts = [\n        \"<｜begin▁of▁sentence｜> You are a helpful assistant. <|BOT|>user\\n \"\n        f\"<im_patch>{question} <|EOT|><|BOT|>assistant\\n<think>\\n\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# omni-research/Tarsier-7b\ndef run_tarsier(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"omni-research/Tarsier-7b\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=4096,\n        limit_mm_per_prompt={modality: 1},\n    )\n    prompts = [(f\"USER: <image>\\n{question} ASSISTANT:\") for question in questions]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_tarsier2(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"omni-research/Tarsier2-Recap-7b\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        hf_overrides={\n            \"architectures\": [\"Tarsier2ForConditionalGeneration\"],\n            \"model_type\": \"tarsier2\",\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\nmodel_example_map = {\n    \"aria\": run_aria,\n    \"aya_vision\": run_aya_vision,\n    \"bagel\": run_bagel,\n    \"bee\": run_bee,\n    \"blip-2\": run_blip2,\n    \"chameleon\": run_chameleon,\n    \"command_a_vision\": run_command_a_vision,\n    \"deepseek_vl_v2\": run_deepseek_vl2,\n    \"deepseek_ocr\": run_deepseek_ocr,\n    \"dots_ocr\": run_dots_ocr,\n    \"ernie45_vl\": run_ernie45_vl,\n    \"fuyu\": run_fuyu,\n    \"gemma3\": run_gemma3,\n    \"gemma3n\": run_gemma3n,\n    \"glm4v\": run_glm4v,\n    \"glm4_1v\": run_glm4_1v,\n    \"glm4_5v\": run_glm4_5v,\n    \"glm4_5v_fp8\": run_glm4_5v_fp8,\n    \"h2ovl_chat\": run_h2ovl,\n    \"hunyuan_vl\": run_hunyuan_vl,\n    \"hyperclovax_seed_vision\": run_hyperclovax_seed_vision,\n    \"idefics3\": run_idefics3,\n    \"interns1\": run_interns1,\n    \"internvl_chat\": run_internvl,\n    \"keye_vl\": run_keye_vl,\n    \"keye_vl1_5\": run_keye_vl1_5,\n    \"kimi_vl\": run_kimi_vl,\n    \"lightonocr\": run_lightonocr,\n    \"llama4\": run_llama4,\n    \"llava\": run_llava,\n    \"llava-next\": run_llava_next,\n    \"llava-next-video\": run_llava_next_video,\n    \"llava-onevision\": run_llava_onevision,\n    \"mantis\": run_mantis,\n    \"minicpmo\": run_minicpmo,\n    \"minicpmv\": run_minicpmv,\n    \"minimax_vl_01\": run_minimax_vl_01,\n    \"mistral3\": run_mistral3,\n    \"molmo\": run_molmo,\n    \"nemotron_vl\": run_nemotron_vl,\n    \"NVLM_D\": run_nvlm_d,\n    \"ovis\": run_ovis,\n    \"ovis2_5\": run_ovis2_5,\n    \"paddleocr_vl\": run_paddleocr_vl,\n    \"paligemma\": run_paligemma,\n    \"paligemma2\": run_paligemma2,\n    \"phi3_v\": run_phi3v,\n    \"phi4_mm\": run_phi4mm,\n    \"phi4_multimodal\": run_phi4_multimodal,\n    \"pixtral_hf\": run_pixtral_hf,\n    \"qwen_vl\": run_qwen_vl,\n    \"qwen2_vl\": run_qwen2_vl,\n    \"qwen2_5_vl\": run_qwen2_5_vl,\n    \"qwen2_5_omni\": run_qwen2_5_omni,\n    \"qwen3_vl\": run_qwen3_vl,\n    \"qwen3_vl_moe\": run_qwen3_vl_moe,\n    \"rvl\": run_r_vl,\n    \"skywork_chat\": run_skyworkr1v,\n    \"smolvlm\": run_smolvlm,\n    \"step3\": run_step3,\n    \"tarsier\": run_tarsier,\n    \"tarsier2\": run_tarsier2,\n}\n\n\nMODELS_NEED_VIDEO_METADATA = [\n    \"glm4_1v\",\n    \"glm4_5v\",\n    \"glm4_5v_fp8\",\n    \"qwen3_vl\",\n    \"qwen3_vl_moe\",\n]\n\n\ndef get_multi_modal_input(args):\n    \"\"\"\n    return {\n        \"data\": image or video,\n        \"question\": question,\n    }\n    \"\"\"\n    if args.modality == \"image\":\n        # Input image and question\n        image = convert_image_mode(ImageAsset(\"cherry_blossom\").pil_image, \"RGB\")\n        img_questions = [\n            \"What is the content of this image?\",\n            \"Describe the content of this image in detail.\",\n            \"What's in the image?\",\n            \"Where is this image taken?\",\n        ]\n\n        return {\n            \"data\": image,\n            \"questions\": img_questions,\n        }\n\n    if args.modality == \"video\":\n        # Input video and question\n        needs_metadata = args.model_type in MODELS_NEED_VIDEO_METADATA\n        video = VideoAsset(name=\"baby_reading\", num_frames=args.num_frames).np_ndarrays\n        metadata = VideoAsset(name=\"baby_reading\", num_frames=args.num_frames).metadata\n        vid_questions = [\"Why is this video funny?\"]\n\n        return {\n            \"data\": ([(video, metadata)] if needs_metadata else video),\n            \"questions\": vid_questions,\n        }\n\n    msg = f\"Modality {args.modality} is not supported.\"\n    raise ValueError(msg)\n\n\ndef apply_image_repeat(\n    image_repeat_prob, num_prompts, data, prompts: list[str], modality\n):\n    \"\"\"Repeats images with provided probability of \"image_repeat_prob\".\n    Used to simulate hit/miss for the MM preprocessor cache.\n    \"\"\"\n    assert image_repeat_prob <= 1.0 and image_repeat_prob >= 0\n    no_yes = [0, 1]\n    probs = [1.0 - image_repeat_prob, image_repeat_prob]\n\n    inputs = []\n    inputs_with_empty_media = []\n    cur_image = data\n    for i in range(num_prompts):\n        if image_repeat_prob is not None:\n            res = random.choices(no_yes, probs)[0]\n            if res == 0:\n                # No repeat => Modify one pixel\n                cur_image = cur_image.copy()\n                new_val = (i // 256 // 256, i // 256, i % 256)\n                cur_image.putpixel((0, 0), new_val)\n\n        uuid = \"uuid_{}\".format(i)\n\n        inputs.append(\n            {\n                \"prompt\": prompts[i % len(prompts)],\n                \"multi_modal_data\": {modality: cur_image},\n                \"multi_modal_uuids\": {modality: uuid},\n            }\n        )\n\n        inputs_with_empty_media.append(\n            {\n                \"prompt\": prompts[i % len(prompts)],\n                \"multi_modal_data\": {modality: None},\n                \"multi_modal_uuids\": {modality: uuid},\n            }\n        )\n\n    return inputs, inputs_with_empty_media\n\n\n@contextmanager\ndef time_counter(enable: bool):\n    if enable:\n        import time\n\n        start_time = time.time()\n        yield\n        elapsed_time = time.time() - start_time\n        print(\"-\" * 50)\n        print(\"-- generate time = {}\".format(elapsed_time))\n        print(\"-\" * 50)\n    else:\n        yield\n\n\ndef parse_args():\n    parser = FlexibleArgumentParser(\n        description=\"Demo on using vLLM for offline inference with \"\n        \"vision language models for text generation\"\n    )\n    parser.add_argument(\n        \"--model-type\",\n        \"-m\",\n        type=str,\n        default=\"llava\",\n        choices=model_example_map.keys(),\n        help='Huggingface \"model_type\".',\n    )\n    parser.add_argument(\n        \"--num-prompts\", type=int, default=4, help=\"Number of prompts to run.\"\n    )\n    parser.add_argument(\n        \"--modality\",\n        type=str,\n        default=\"image\",\n        choices=[\"image\", \"video\"],\n        help=\"Modality of the input.\",\n    )\n    parser.add_argument(\n        \"--num-frames\",\n        type=int,\n        default=16,\n        help=\"Number of frames to extract from the video.\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=0,\n        help=\"Set the seed when initializing `vllm.LLM`.\",\n    )\n\n    parser.add_argument(\n        \"--image-repeat-prob\",\n        type=float,\n        default=None,\n        help=\"Simulates the hit-ratio for multi-modal preprocessor cache (if enabled)\",\n    )\n\n    parser.add_argument(\n        \"--disable-mm-processor-cache\",\n        action=\"store_true\",\n        help=\"If True, disables caching of multi-modal processor.\",\n    )\n\n    parser.add_argument(\n        \"--time-generate\",\n        action=\"store_true\",\n        help=\"If True, then print the total generate() call time\",\n    )\n\n    parser.add_argument(\n        \"--use-different-prompt-per-request\",\n        action=\"store_true\",\n        help=\"If True, then use different prompt (with the same multi-modal \"\n        \"data) for each request.\",\n    )\n\n    parser.add_argument(\n        \"--verify-mm-cache-hit-with-uuids\",\n        action=\"store_true\",\n        help=\"If True, will send all requests in a second batch with empty mm \"\n        \"data to verify cache hits with UUIDs.\",\n    )\n    parser.add_argument(\n        \"--tensor-parallel-size\",\n        \"-tp\",\n        type=int,\n        default=None,\n        help=\"Tensor parallel size to override the model's default setting. \",\n    )\n    return parser.parse_args()\n\n\ndef main(args):\n    model = args.model_type\n    if model not in model_example_map:\n        raise ValueError(f\"Model type {model} is not supported.\")\n\n    if args.tensor_parallel_size is not None and args.tensor_parallel_size < 1:\n        raise ValueError(\n            f\"tensor_parallel_size must be a positive integer, \"\n            f\"got {args.tensor_parallel_size}\"\n        )\n\n    modality = args.modality\n    mm_input = get_multi_modal_input(args)\n    data = mm_input[\"data\"]\n    questions = mm_input[\"questions\"]\n\n    req_data = model_example_map[model](questions, modality)\n\n    # Disable other modalities to save memory\n    default_limits = {\"image\": 0, \"video\": 0, \"audio\": 0}\n    req_data.engine_args.limit_mm_per_prompt = default_limits | dict(\n        req_data.engine_args.limit_mm_per_prompt or {}\n    )\n\n    engine_args = asdict(req_data.engine_args) | {\n        \"seed\": args.seed,\n        \"mm_processor_cache_gb\": 0 if args.disable_mm_processor_cache else 4,\n    }\n    if args.tensor_parallel_size is not None:\n        engine_args[\"tensor_parallel_size\"] = args.tensor_parallel_size\n    llm = LLM(**engine_args)\n\n    # Don't want to check the flag multiple times, so just hijack `prompts`.\n    prompts = (\n        req_data.prompts\n        if args.use_different_prompt_per_request\n        else [req_data.prompts[0]]\n    )\n\n    # We set temperature to 0.2 so that outputs can be different\n    # even when all prompts are identical when running batch inference.\n    sampling_params = (\n        SamplingParams(\n            temperature=0.2, max_tokens=64, stop_token_ids=req_data.stop_token_ids\n        )\n        if req_data.sampling_params is None\n        else req_data.sampling_params\n    )\n\n    assert args.num_prompts > 0\n    if args.num_prompts == 1:\n        # Single inference\n        uuid = \"uuid_0\"\n        inputs = {\n            \"prompt\": prompts[0],\n            \"multi_modal_data\": {modality: data},\n            \"multi_modal_uuids\": {modality: uuid},\n        }\n        inputs_with_empty_media = {\n            \"prompt\": prompts[0],\n            \"multi_modal_data\": {modality: None},\n            \"multi_modal_uuids\": {modality: uuid},\n        }\n    else:\n        # Batch inference\n        if args.image_repeat_prob is not None:\n            # Repeat images with specified probability of \"image_repeat_prob\"\n            inputs, inputs_with_empty_media = apply_image_repeat(\n                args.image_repeat_prob,\n                args.num_prompts,\n                data,\n                prompts,\n                modality,\n            )\n        else:\n            # Use the same image for all prompts\n            inputs = []\n            inputs_with_empty_media = []\n            for i in range(args.num_prompts):\n                uuid = \"uuid_{}\".format(i)\n                inputs.append(\n                    {\n                        \"prompt\": prompts[i % len(prompts)],\n                        \"multi_modal_data\": {modality: data},\n                        \"multi_modal_uuids\": {modality: uuid},\n                    }\n                )\n                inputs_with_empty_media.append(\n                    {\n                        \"prompt\": prompts[i % len(prompts)],\n                        \"multi_modal_data\": {modality: None},\n                        \"multi_modal_uuids\": {modality: uuid},\n                    }\n                )\n\n    # Add LoRA request if applicable\n    lora_request = (\n        req_data.lora_requests * args.num_prompts if req_data.lora_requests else None\n    )\n\n    with time_counter(args.time_generate):\n        outputs = llm.generate(\n            inputs,\n            sampling_params=sampling_params,\n            lora_request=lora_request,\n        )\n\n    print(\"-\" * 50)\n    for o in outputs:\n        generated_text = o.outputs[0].text\n        print(generated_text)\n        print(\"-\" * 50)\n\n    if args.verify_mm_cache_hit_with_uuids:\n        try:\n            # Verify cache hits with UUIDs\n            print(\n                \"Sending a second batch of requests with empty media\"\n                \" and matching UUIDs.\"\n            )\n            outputs = llm.generate(\n                inputs_with_empty_media,\n                sampling_params=sampling_params,\n                lora_request=lora_request,\n            )\n            print(\"-\" * 50)\n            for o in outputs:\n                generated_text = o.outputs[0].text\n                print(generated_text)\n                print(\"-\" * 50)\n        except Exception as e:\n            print(f\"Failed to verify cache hits with UUIDs. Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    main(args)",
      "language": "python"
    },
    {
      "code": "# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\"\"\"\nThis example shows how to use vLLM for running offline inference with\nthe correct prompt format on vision language models for text generation.\n\nFor most models, the prompt format should follow corresponding examples\non HuggingFace model repository.\n\"\"\"\n\nimport os\nimport random\nfrom contextlib import contextmanager\nfrom dataclasses import asdict\nfrom typing import NamedTuple\n\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoTokenizer\n\nfrom vllm import LLM, EngineArgs, SamplingParams\nfrom vllm.assets.image import ImageAsset\nfrom vllm.assets.video import VideoAsset\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal.image import convert_image_mode\nfrom vllm.utils.argparse_utils import FlexibleArgumentParser\n\n\nclass ModelRequestData(NamedTuple):\n    engine_args: EngineArgs\n    prompts: list[str]\n    stop_token_ids: list[int] | None = None\n    lora_requests: list[LoRARequest] | None = None\n    sampling_params: list[SamplingParams] | None = None\n\n\n# NOTE: The default `max_num_seqs` and `max_model_len` may result in OOM on\n# lower-end GPUs.\n# Unless specified, these settings have been tested to work on a single L4.\n\n\n# Aria\ndef run_aria(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"rhymes-ai/Aria\"\n\n    # NOTE: Need L40 (or equivalent) to avoid OOM\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        dtype=\"bfloat16\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        (\n            f\"<|im_start|>user\\n<fim_prefix><|img|><fim_suffix>{question}\"\n            \"<|im_end|>\\n<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    stop_token_ids = [93532, 93653, 944, 93421, 1019, 93653, 93519]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# Aya Vision\ndef run_aya_vision(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"CohereLabs/aya-vision-8b\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=2048,\n        max_num_seqs=2,\n        mm_processor_kwargs={\"crop_to_patches\": True},\n        limit_mm_per_prompt={modality: 1},\n    )\n    prompts = [\n        f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|><image>{question}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        for question in questions\n    ]\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Bee-8B\ndef run_bee(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"Open-Bee/Bee-8B-RL\"\n\n    prompts = [\n        (\n            f\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<image>\\n{question}<|im_end|>\"\n            f\"<|im_start|>assistant\\n<think>\\n\"\n        )\n        for question in questions\n    ]\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=16384,\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_bagel(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"ByteDance-Seed/BAGEL-7B-MoT\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        (\n            f\"<|im_start|>user\\n<|image_pad|>\\n{question}<|im_end|>\\n\"\n            f\"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# BLIP-2\ndef run_blip2(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    # BLIP-2 prompt format is inaccurate on HuggingFace model repository.\n    # See https://huggingface.co/Salesforce/blip2-opt-2.7b/discussions/15#64ff02f3f8cf9e4f5b038262 #noqa\n    prompts = [f\"Question: {question} Answer:\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"Salesforce/blip2-opt-2.7b\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Chameleon\ndef run_chameleon(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"{question}<image>\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"facebook/chameleon-7b\",\n        max_model_len=4096,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_command_a_vision(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"CohereLabs/command-a-vision-07-2025\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=32768,\n        tensor_parallel_size=4,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|><|IMG_PATCH|>{question}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Deepseek-VL2\ndef run_deepseek_vl2(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"deepseek-ai/deepseek-vl2-tiny\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        hf_overrides={\"architectures\": [\"DeepseekVLV2ForCausalLM\"]},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        f\"<|User|>: <image>\\n{question}\\n\\n<|Assistant|>:\" for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_deepseek_ocr(questions: list[str], modality: str) -> ModelRequestData:\n    from vllm.model_executor.models.deepseek_ocr import NGramPerReqLogitsProcessor\n\n    assert modality == \"image\"\n\n    model_name = \"deepseek-ai/DeepSeek-OCR\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        limit_mm_per_prompt={modality: 1},\n        logits_processors=[NGramPerReqLogitsProcessor],\n    )\n\n    # deepseek-ocr use plain prompt template\n    prompts = [f\"<image>\\n{question}\" for question in questions]\n\n    # The following sampling params config is taken from\n    # the official Deepseek-OCR inference example.\n    # (IMPORTANT) Use the custom logits processor and avoid skipping\n    # special tokens for this model for the optimal OCR performance.\n    sampling_params = [\n        SamplingParams(\n            temperature=0.0,\n            max_tokens=8192,\n            # ngram logit processor args\n            extra_args=dict(\n                ngram_size=30,\n                window_size=90,\n                # whitelist: <td>, </td>\n                whitelist_token_ids={128821, 128822},\n            ),\n            skip_special_tokens=False,\n        )\n        for _ in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        sampling_params=sampling_params,\n    )\n\n\n# Dots-OCR\ndef run_dots_ocr(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"<|img|><|imgpad|><|endofimg|>{question}\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"rednote-hilab/dots.ocr\",\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Ernie4.5-VL\ndef run_ernie45_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"baidu/ERNIE-4.5-VL-28B-A3B-PT\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n    )\n\n    if modality == \"image\":\n        placeholder = \"Picture 1:<|IMAGE_START|><|image@placeholder|><|IMAGE_END|>\"\n    elif modality == \"video\":\n        placeholder = \"Video 1:<|VIDEO_START|><|video@placeholder|><|VIDEO_END|>\"\n\n    prompts = [\n        (\n            f\"<|begin_of_sentence|>User: {question}{placeholder}\\n\"\n            \"Assistant: <think></think>\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Fuyu\ndef run_fuyu(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"{question}\\n\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"adept/fuyu-8b\",\n        max_model_len=2048,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Gemma 3\ndef run_gemma3(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"google/gemma-3-4b-it\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=2048,\n        max_num_seqs=2,\n        mm_processor_kwargs={\"do_pan_and_scan\": True},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        (\n            \"<bos><start_of_turn>user\\n\"\n            f\"<start_of_image>{question}<end_of_turn>\\n\"\n            \"<start_of_turn>model\\n\"\n        )\n        for question in questions\n    ]\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Gemma3N\ndef run_gemma3n(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"google/gemma-3n-E2B-it\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=2048,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n    )\n\n    prompts = [\n        (\n            \"<start_of_turn>user\\n\"\n            f\"<image_soft_token>{question}<end_of_turn>\\n\"\n            \"<start_of_turn>model\\n\"\n        )\n        for question in questions\n    ]\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# GLM-4v\ndef run_glm4v(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"zai-org/glm-4v-9b\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=2048,\n        max_num_seqs=2,\n        trust_remote_code=True,\n        enforce_eager=True,\n        hf_overrides={\"architectures\": [\"GLM4VForCausalLM\"]},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        (\n            \"<|user|>\\n<|begin_of_image|><|endoftext|><|end_of_image|>\"\n            f\"{question}<|assistant|>\"\n        )\n        for question in questions\n    ]\n\n    stop_token_ids = [151329, 151336, 151338]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# GLM-4.1V\ndef run_glm4_1v(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"zai-org/GLM-4.1V-9B-Thinking\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        mm_processor_kwargs={\n            \"size\": {\"shortest_edge\": 12544, \"longest_edge\": 47040000},\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|begin_of_image|><|image|><|end_of_image|>\"\n    elif modality == \"video\":\n        placeholder = \"<|begin_of_video|><|video|><|end_of_video|>\"\n\n    prompts = [\n        (\n            \"[gMASK]<sop><|system|>\\nYou are a helpful assistant.<|user|>\\n\"\n            f\"{placeholder}\"\n            f\"{question}<|assistant|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# GLM-4.5V\ndef run_glm4_5v(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"zai-org/GLM-4.5V\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        mm_processor_kwargs={\n            \"size\": {\"shortest_edge\": 12544, \"longest_edge\": 47040000},\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n        tensor_parallel_size=4,\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|begin_of_image|><|image|><|end_of_image|>\"\n    elif modality == \"video\":\n        placeholder = \"<|begin_of_video|><|video|><|end_of_video|>\"\n\n    prompts = [\n        (\n            \"[gMASK]<sop><|system|>\\nYou are a helpful assistant.<|user|>\\n\"\n            f\"{placeholder}\"\n            f\"{question}<|assistant|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# GLM-4.5V-FP8\ndef run_glm4_5v_fp8(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"zai-org/GLM-4.5V-FP8\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        mm_processor_kwargs={\n            \"size\": {\"shortest_edge\": 12544, \"longest_edge\": 47040000},\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n        tensor_parallel_size=4,\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|begin_of_image|><|image|><|end_of_image|>\"\n    elif modality == \"video\":\n        placeholder = \"<|begin_of_video|><|video|><|end_of_video|>\"\n\n    prompts = [\n        (\n            \"[gMASK]<sop><|system|>\\nYou are a helpful assistant.<|user|>\\n\"\n            f\"{placeholder}\"\n            f\"{question}<|assistant|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# H2OVL-Mississippi\ndef run_h2ovl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"h2oai/h2ovl-mississippi-800m\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    # Stop tokens for H2OVL-Mississippi\n    # https://huggingface.co/h2oai/h2ovl-mississippi-800m\n    stop_token_ids = [tokenizer.eos_token_id]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# HunyuanOCR\ndef run_hunyuan_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"tencent/HunyuanOCR\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    placeholder = \"<｜hy_place▁holder▁no▁100｜><｜hy_place▁holder▁no▁102｜><｜hy_place▁holder▁no▁101｜>\"  # noqa: E501\n    prompts = [\n        f\"<｜hy_begin▁of▁sentence｜>{placeholder}{question}<｜hy_User｜>\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=None,\n    )\n\n\n# naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B\ndef run_hyperclovax_seed_vision(\n    questions: list[str], modality: str\n) -> ModelRequestData:\n    model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192 if modality == \"image\" else 16384,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    messages = list()\n    for question in questions:\n        if modality == \"image\":\n            \"\"\"\n            ocr: List the words in the image in raster order.\n                Even if the word order feels unnatural for reading,\n                the model will handle it as long as it follows raster order.\n                e.g. \"Naver, CLOVA, bigshane\"\n            lens_keywords: List the entity names in the image.\n                e.g. \"iPhone\"\n            lens_local_keywords: List the entity names with quads in the image.\n                e.g. \"[0.07, 0.21, 0.92, 0.90] iPhone\"\n            \"\"\"\n            messages.append(\n                [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"image\",\n                                \"ocr\": \"\",\n                                \"lens_keywords\": \"\",\n                                \"lens_local_keywords\": \"\",\n                            },\n                            {\n                                \"type\": \"text\",\n                                \"text\": question,\n                            },\n                        ],\n                    }\n                ]\n            )\n        elif modality == \"video\":\n            messages.append(\n                [\n                    {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"video\",\n                            },\n                            {\n                                \"type\": \"text\",\n                                \"text\": question,\n                            },\n                        ],\n                    }\n                ]\n            )\n        else:\n            raise ValueError(f\"Unsupported modality: {modality}\")\n\n    prompts = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=None,\n    )\n\n\n# Idefics3-8B-Llama3\ndef run_idefics3(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"HuggingFaceM4/Idefics3-8B-Llama3\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        max_num_seqs=2,\n        enforce_eager=True,\n        # if you are running out of memory, you can reduce the \"longest_edge\".\n        # see: https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3#model-optimizations\n        mm_processor_kwargs={\n            \"size\": {\"longest_edge\": 3 * 364},\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n    prompts = [\n        (f\"<|begin_of_text|>User:<image>{question}<end_of_utterance>\\nAssistant:\")\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Intern-S1\ndef run_interns1(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"internlm/Intern-S1-mini\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n        enforce_eager=True,\n    )\n\n    if modality == \"image\":\n        placeholder = \"<IMG_CONTEXT>\"\n    elif modality == \"video\":\n        placeholder = \"<video>\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"{placeholder}\\n{question}\"}]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# InternVL\ndef run_internvl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"OpenGVLab/InternVL3-2B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<image>\"\n    elif modality == \"video\":\n        placeholder = \"<video>\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"{placeholder}\\n{question}\"}]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    # Stop tokens for InternVL\n    # models variants may have different stop tokens\n    # please refer to the model card for the correct \"stop words\":\n    # https://huggingface.co/OpenGVLab/InternVL2-2B/blob/main/conversation.py\n    stop_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|end|>\"]\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n    stop_token_ids = [token_id for token_id in stop_token_ids if token_id is not None]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# Keye-VL\ndef run_keye_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Kwai-Keye/Keye-VL-8B-Preview\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        trust_remote_code=True,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Keye-VL-1.5\ndef run_keye_vl1_5(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Kwai-Keye/Keye-VL-1.5-8B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        trust_remote_code=True,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Kimi-VL\ndef run_kimi_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [\n        \"<|im_user|>user<|im_middle|><|media_start|>image<|media_content|>\"\n        f\"<|media_pad|><|media_end|>{question}<|im_end|>\"\n        \"<|im_assistant|>assistant<|im_middle|>\"\n        for question in questions\n    ]\n\n    engine_args = EngineArgs(\n        model=\"moonshotai/Kimi-VL-A3B-Instruct\",\n        trust_remote_code=True,\n        max_model_len=4096,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# LightOnOCR\ndef run_lightonocr(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [\n        \"<|im_start|>system<|im_end|>\\n<|im_start|>user\\n<|image_pad|><|im_end|>\\n<|im_start|>assistant\\n\"\n        for _ in questions\n    ]\n\n    engine_args = EngineArgs(\n        model=\"lightonai/LightOnOCR-1B\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_llama4(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        max_num_seqs=4,\n        tensor_parallel_size=8,\n        gpu_memory_utilization=0.4,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    messages = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": f\"{question}\"}],\n            }\n        ]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, add_generation_prompt=True, tokenize=False\n    )\n    stop_token_ids = None\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# LLaVA-1.5\ndef run_llava(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"USER: <image>\\n{question}\\nASSISTANT:\" for question in questions]\n\n    engine_args = EngineArgs(\n        model=\"llava-hf/llava-1.5-7b-hf\",\n        max_model_len=4096,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# LLaVA-1.6/LLaVA-NeXT\ndef run_llava_next(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [f\"[INST] <image>\\n{question} [/INST]\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"llava-hf/llava-v1.6-mistral-7b-hf\",\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# LlaVA-NeXT-Video\n# Currently only support for video input\ndef run_llava_next_video(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"video\"\n\n    prompts = [f\"USER: <video>\\n{question} ASSISTANT:\" for question in questions]\n    engine_args = EngineArgs(\n        model=\"llava-hf/LLaVA-NeXT-Video-7B-hf\",\n        max_model_len=8192,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# LLaVA-OneVision\ndef run_llava_onevision(questions: list[str], modality: str) -> ModelRequestData:\n    if modality == \"video\":\n        prompts = [\n            f\"<|im_start|>user <video>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n            for question in questions\n        ]\n\n    elif modality == \"image\":\n        prompts = [\n            f\"<|im_start|>user <image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n            for question in questions\n        ]\n\n    engine_args = EngineArgs(\n        model=\"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n        max_model_len=16384,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Mantis\ndef run_mantis(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    llama3_template = \"<|start_header_id|>user<|end_header_id|>\\n\\n{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"  # noqa: E501\n    prompts = [llama3_template.format(f\"{question}\\n<image>\") for question in questions]\n\n    engine_args = EngineArgs(\n        model=\"TIGER-Lab/Mantis-8B-siglip-llama3\",\n        max_model_len=4096,\n        hf_overrides={\"architectures\": [\"MantisForConditionalGeneration\"]},\n        limit_mm_per_prompt={modality: 1},\n    )\n    stop_token_ids = [128009]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# MiniCPM-V\ndef run_minicpmv_base(questions: list[str], modality: str, model_name):\n    assert modality in [\"image\", \"video\"]\n    # If you want to use `MiniCPM-o-2_6` with audio inputs, check `audio_language.py` # noqa\n\n    # 2.0\n    # The official repo doesn't work yet, so we need to use a fork for now\n    # For more details, please see: See: https://github.com/vllm-project/vllm/pull/4087#issuecomment-2250397630 # noqa\n    # model_name = \"HwwwH/MiniCPM-V-2\"\n\n    # 2.5\n    # model_name = \"openbmb/MiniCPM-Llama3-V-2_5\"\n\n    # 2.6\n    # model_name = \"openbmb/MiniCPM-V-2_6\"\n    # o2.6\n\n    # modality supports\n    # 2.0: image\n    # 2.5: image\n    # 2.6: image, video\n    # o2.6: image, video, audio\n    # model_name = \"openbmb/MiniCPM-o-2_6\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        trust_remote_code=True,\n        limit_mm_per_prompt={modality: 1},\n    )\n    # NOTE The stop_token_ids are different for various versions of MiniCPM-V\n    # 2.0\n    # stop_token_ids = [tokenizer.eos_id]\n\n    # 2.5\n    # stop_token_ids = [tokenizer.eos_id, tokenizer.eot_id]\n\n    # 2.6 / o2.6\n    stop_tokens = [\"<|im_end|>\", \"<|endoftext|>\"]\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n\n    modality_placeholder = {\n        \"image\": \"(<image>./</image>)\",\n        \"video\": \"(<video>./</video>)\",\n    }\n\n    prompts = [\n        tokenizer.apply_chat_template(\n            [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"{modality_placeholder[modality]}\\n{question}\",\n                }\n            ],\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\ndef run_minicpmo(questions: list[str], modality: str) -> ModelRequestData:\n    return run_minicpmv_base(questions, modality, \"openbmb/MiniCPM-o-2_6\")\n\n\ndef run_minicpmv(questions: list[str], modality: str) -> ModelRequestData:\n    return run_minicpmv_base(questions, modality, \"openbmb/MiniCPM-V-2_6\")\n\n\ndef run_minimax_vl_01(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"MiniMaxAI/MiniMax-VL-01\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n        tensor_parallel_size=8,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    messages = [\n        [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": question}],\n            }\n        ]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, add_generation_prompt=True, tokenize=False\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Mistral-3 HF-format\ndef run_mistral3(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\n    # NOTE: Need L40 (or equivalent) to avoid OOM\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        max_num_seqs=2,\n        tensor_parallel_size=2,\n        limit_mm_per_prompt={modality: 1},\n        ignore_patterns=[\"consolidated.safetensors\"],\n    )\n\n    prompts = [f\"<s>[INST]{question}\\n[IMG][/INST]\" for question in questions]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Molmo\ndef run_molmo(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"allenai/Molmo-7B-D-0924\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        dtype=\"bfloat16\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [\n        f\"<|im_start|>user <image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Nemontron_VL\ndef run_nemotron_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=8192,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    assert modality == \"image\"\n    placeholder = \"<image>\"\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"{placeholder}\\n{question}\"}]\n        for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    # Stop tokens for InternVL\n    # models variants may have different stop tokens\n    # please refer to the model card for the correct \"stop words\":\n    # https://huggingface.co/OpenGVLab/InternVL2-2B/blob/main/conversation.py\n    stop_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|end|>\"]\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n    stop_token_ids = [token_id for token_id in stop_token_ids if token_id is not None]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# NVLM-D\ndef run_nvlm_d(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"nvidia/NVLM-D-72B\"\n\n    # Adjust this as necessary to fit in GPU\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=4096,\n        tensor_parallel_size=4,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Ovis\ndef run_ovis(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"AIDC-AI/Ovis2-1B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        trust_remote_code=True,\n        dtype=\"half\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Ovis2_5\ndef run_ovis2_5(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"AIDC-AI/Ovis2.5-2B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        trust_remote_code=True,\n        dtype=\"half\",\n        limit_mm_per_prompt={modality: 1},\n    )\n    if modality == \"image\":\n        placeholder = \"<image>\"\n    elif modality == \"video\":\n        placeholder = \"<video>\"\n\n    prompts = [\n        f\"<|im_start|>user\\n\\n{placeholder}\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# PaddleOCR-VL\ndef run_paddleocr_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"PaddlePaddle/PaddleOCR-VL\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n        trust_remote_code=True,\n    )\n\n    placeholder = \"<|IMAGE_START|><|IMAGE_PLACEHOLDER|><|IMAGE_END|>\"\n    prompts = [\n        (f\"<|begin_of_sentence|>User: {question}{placeholder}\\nAssistant: \")\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# PaliGemma\ndef run_paligemma(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    # PaliGemma has special prompt format for VQA\n    prompts = [\"caption en\" for _ in questions]\n    engine_args = EngineArgs(\n        model=\"google/paligemma-3b-mix-224\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# PaliGemma 2\ndef run_paligemma2(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    # PaliGemma 2 has special prompt format for VQA\n    prompts = [\"caption en\" for _ in questions]\n    engine_args = EngineArgs(\n        model=\"google/paligemma2-3b-ft-docci-448\",\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Phi-3-Vision\ndef run_phi3v(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    prompts = [\n        f\"<|user|>\\n<|image_1|>\\n{question}<|end|>\\n<|assistant|>\\n\"\n        for question in questions\n    ]\n\n    # num_crops is an override kwarg to the multimodal image processor;\n    # For some models, e.g., Phi-3.5-vision-instruct, it is recommended\n    # to use 16 for single frame scenarios, and 4 for multi-frame.\n    #\n    # Generally speaking, a larger value for num_crops results in more\n    # tokens per image instance, because it may scale the image more in\n    # the image preprocessing. Some references in the model docs and the\n    # formula for image tokens after the preprocessing\n    # transform can be found below.\n    #\n    # https://huggingface.co/microsoft/Phi-3.5-vision-instruct#loading-the-model-locally\n    # https://huggingface.co/microsoft/Phi-3.5-vision-instruct/blob/main/processing_phi3_v.py#L194\n    engine_args = EngineArgs(\n        model=\"microsoft/Phi-3.5-vision-instruct\",\n        trust_remote_code=True,\n        max_model_len=4096,\n        max_num_seqs=2,\n        # Note - mm_processor_kwargs can also be passed to generate/chat calls\n        mm_processor_kwargs={\"num_crops\": 16},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Phi-4-multimodal-instruct\ndef run_phi4mm(questions: list[str], modality: str) -> ModelRequestData:\n    \"\"\"\n    Phi-4-multimodal-instruct supports both image and audio inputs. Here, we\n    show how to process image inputs.\n    \"\"\"\n    assert modality == \"image\"\n    model_path = snapshot_download(\"microsoft/Phi-4-multimodal-instruct\")\n    # Since the vision-lora and speech-lora co-exist with the base model,\n    # we have to manually specify the path of the lora weights.\n    vision_lora_path = os.path.join(model_path, \"vision-lora\")\n    prompts = [\n        f\"<|user|><|image_1|>{question}<|end|><|assistant|>\" for question in questions\n    ]\n    engine_args = EngineArgs(\n        model=model_path,\n        trust_remote_code=True,\n        max_model_len=5120,\n        max_num_seqs=2,\n        max_num_batched_tokens=12800,\n        enable_lora=True,\n        max_lora_rank=320,\n        # Note - mm_processor_kwargs can also be passed to generate/chat calls\n        mm_processor_kwargs={\"dynamic_hd\": 16},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        lora_requests=[LoRARequest(\"vision\", 1, vision_lora_path)],\n    )\n\n\n# HF format Phi-4-multimodal-instruct\ndef run_phi4_multimodal(questions: list[str], modality: str) -> ModelRequestData:\n    \"\"\"\n    Phi-4-multimodal-instruct supports both image and audio inputs. Here, we\n    show how to process image inputs.\n    \"\"\"\n    assert modality == \"image\"\n    model_path = snapshot_download(\n        \"microsoft/Phi-4-multimodal-instruct\", revision=\"refs/pr/70\"\n    )\n    # Since the vision-lora and speech-lora co-exist with the base model,\n    # we have to manually specify the path of the lora weights.\n    vision_lora_path = os.path.join(model_path, \"vision-lora\")\n    prompts = [\n        f\"<|user|><|image|>{question}<|end|><|assistant|>\" for question in questions\n    ]\n    engine_args = EngineArgs(\n        model=model_path,\n        max_model_len=5120,\n        max_num_seqs=2,\n        max_num_batched_tokens=12800,\n        enable_lora=True,\n        max_lora_rank=320,\n        # Note - mm_processor_kwargs can also be passed to generate/chat calls\n        mm_processor_kwargs={\"dynamic_hd\": 16},\n        limit_mm_per_prompt={\"image\": 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        lora_requests=[LoRARequest(\"vision\", 1, vision_lora_path)],\n    )\n\n\n# Pixtral HF-format\ndef run_pixtral_hf(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"mistral-community/pixtral-12b\"\n\n    # NOTE: Need L40 (or equivalent) to avoid OOM\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=6144,\n        max_num_seqs=2,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [f\"<s>[INST]{question}\\n[IMG][/INST]\" for question in questions]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen-VL\ndef run_qwen_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    engine_args = EngineArgs(\n        model=\"Qwen/Qwen-VL\",\n        trust_remote_code=True,\n        max_model_len=1024,\n        max_num_seqs=2,\n        hf_overrides={\"architectures\": [\"QwenVLForConditionalGeneration\"]},\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    prompts = [f\"{question}Picture 1: <img></img>\\n\" for question in questions]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen2-VL\ndef run_qwen2_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Qwen/Qwen2-VL-7B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        # Note - mm_processor_kwargs can also be passed to generate/chat calls\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen2.5-VL\ndef run_qwen2_5_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen2.5-Omni\ndef run_qwen2_5_omni(questions: list[str], modality: str):\n    model_name = \"Qwen/Qwen2.5-Omni-7B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|IMAGE|>\"\n    elif modality == \"video\":\n        placeholder = \"<|VIDEO|>\"\n\n    default_system = (\n        \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba \"\n        \"Group, capable of perceiving auditory and visual inputs, as well as \"\n        \"generating text and speech.\"\n    )\n\n    prompts = [\n        (\n            f\"<|im_start|>system\\n{default_system}<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_bos|>{placeholder}<|vision_eos|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen3-VL-Dense\ndef run_qwen3_vl(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Qwen/Qwen3-VL-4B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Qwen3-VL-MOE\ndef run_qwen3_vl_moe(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"Qwen/Qwen3-VL-30B-A3B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        max_num_seqs=5,\n        mm_processor_kwargs={\n            \"min_pixels\": 28 * 28,\n            \"max_pixels\": 1280 * 28 * 28,\n            \"fps\": 1,\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# R-4B\ndef run_r_vl(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"YannQi/R-4B\"\n\n    prompts = [\n        f\"<|im_start|>user <image>\\n{question}<|im_end|><|im_start|>assistant\\n\"\n        for question in questions\n    ]\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=16384,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# SkyworkR1V\ndef run_skyworkr1v(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"Skywork/Skywork-R1V-38B\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=4096,\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    messages = [\n        [{\"role\": \"user\", \"content\": f\"<image>\\n{question}\"}] for question in questions\n    ]\n    prompts = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    # Stop tokens for SkyworkR1V\n    # https://huggingface.co/Skywork/Skywork-R1V-38B/blob/main/conversation.py\n    stop_tokens = [\"<｜end▁of▁sentence｜>\", \"<|endoftext|>\"]\n    stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n        stop_token_ids=stop_token_ids,\n    )\n\n\n# SmolVLM2-2.2B-Instruct\ndef run_smolvlm(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=8192,\n        max_num_seqs=2,\n        enforce_eager=True,\n        mm_processor_kwargs={\n            \"max_image_size\": {\"longest_edge\": 384},\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n    prompts = [\n        (f\"<|im_start|>User:<image>{question}<end_of_utterance>\\nAssistant:\")\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# Step3\ndef run_step3(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n\n    model_name = \"stepfun-ai/step3-fp8\"\n\n    # NOTE: Below are verified configurations for step3-fp8\n    # on 8xH100 GPUs.\n    engine_args = EngineArgs(\n        model=model_name,\n        max_num_batched_tokens=4096,\n        gpu_memory_utilization=0.85,\n        tensor_parallel_size=8,\n        limit_mm_per_prompt={modality: 1},\n        reasoning_parser=\"step3\",\n    )\n\n    prompts = [\n        \"<｜begin▁of▁sentence｜> You are a helpful assistant. <|BOT|>user\\n \"\n        f\"<im_patch>{question} <|EOT|><|BOT|>assistant\\n<think>\\n\"\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\n# omni-research/Tarsier-7b\ndef run_tarsier(questions: list[str], modality: str) -> ModelRequestData:\n    assert modality == \"image\"\n    model_name = \"omni-research/Tarsier-7b\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        trust_remote_code=True,\n        max_model_len=4096,\n        limit_mm_per_prompt={modality: 1},\n    )\n    prompts = [(f\"USER: <image>\\n{question} ASSISTANT:\") for question in questions]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\ndef run_tarsier2(questions: list[str], modality: str) -> ModelRequestData:\n    model_name = \"omni-research/Tarsier2-Recap-7b\"\n\n    engine_args = EngineArgs(\n        model=model_name,\n        max_model_len=4096,\n        hf_overrides={\n            \"architectures\": [\"Tarsier2ForConditionalGeneration\"],\n            \"model_type\": \"tarsier2\",\n        },\n        limit_mm_per_prompt={modality: 1},\n    )\n\n    if modality == \"image\":\n        placeholder = \"<|image_pad|>\"\n    elif modality == \"video\":\n        placeholder = \"<|video_pad|>\"\n\n    prompts = [\n        (\n            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n            f\"<|im_start|>user\\n<|vision_start|>{placeholder}<|vision_end|>\"\n            f\"{question}<|im_end|>\\n\"\n            \"<|im_start|>assistant\\n\"\n        )\n        for question in questions\n    ]\n\n    return ModelRequestData(\n        engine_args=engine_args,\n        prompts=prompts,\n    )\n\n\nmodel_example_map = {\n    \"aria\": run_aria,\n    \"aya_vision\": run_aya_vision,\n    \"bagel\": run_bagel,\n    \"bee\": run_bee,\n    \"blip-2\": run_blip2,\n    \"chameleon\": run_chameleon,\n    \"command_a_vision\": run_command_a_vision,\n    \"deepseek_vl_v2\": run_deepseek_vl2,\n    \"deepseek_ocr\": run_deepseek_ocr,\n    \"dots_ocr\": run_dots_ocr,\n    \"ernie45_vl\": run_ernie45_vl,\n    \"fuyu\": run_fuyu,\n    \"gemma3\": run_gemma3,\n    \"gemma3n\": run_gemma3n,\n    \"glm4v\": run_glm4v,\n    \"glm4_1v\": run_glm4_1v,\n    \"glm4_5v\": run_glm4_5v,\n    \"glm4_5v_fp8\": run_glm4_5v_fp8,\n    \"h2ovl_chat\": run_h2ovl,\n    \"hunyuan_vl\": run_hunyuan_vl,\n    \"hyperclovax_seed_vision\": run_hyperclovax_seed_vision,\n    \"idefics3\": run_idefics3,\n    \"interns1\": run_interns1,\n    \"internvl_chat\": run_internvl,\n    \"keye_vl\": run_keye_vl,\n    \"keye_vl1_5\": run_keye_vl1_5,\n    \"kimi_vl\": run_kimi_vl,\n    \"lightonocr\": run_lightonocr,\n    \"llama4\": run_llama4,\n    \"llava\": run_llava,\n    \"llava-next\": run_llava_next,\n    \"llava-next-video\": run_llava_next_video,\n    \"llava-onevision\": run_llava_onevision,\n    \"mantis\": run_mantis,\n    \"minicpmo\": run_minicpmo,\n    \"minicpmv\": run_minicpmv,\n    \"minimax_vl_01\": run_minimax_vl_01,\n    \"mistral3\": run_mistral3,\n    \"molmo\": run_molmo,\n    \"nemotron_vl\": run_nemotron_vl,\n    \"NVLM_D\": run_nvlm_d,\n    \"ovis\": run_ovis,\n    \"ovis2_5\": run_ovis2_5,\n    \"paddleocr_vl\": run_paddleocr_vl,\n    \"paligemma\": run_paligemma,\n    \"paligemma2\": run_paligemma2,\n    \"phi3_v\": run_phi3v,\n    \"phi4_mm\": run_phi4mm,\n    \"phi4_multimodal\": run_phi4_multimodal,\n    \"pixtral_hf\": run_pixtral_hf,\n    \"qwen_vl\": run_qwen_vl,\n    \"qwen2_vl\": run_qwen2_vl,\n    \"qwen2_5_vl\": run_qwen2_5_vl,\n    \"qwen2_5_omni\": run_qwen2_5_omni,\n    \"qwen3_vl\": run_qwen3_vl,\n    \"qwen3_vl_moe\": run_qwen3_vl_moe,\n    \"rvl\": run_r_vl,\n    \"skywork_chat\": run_skyworkr1v,\n    \"smolvlm\": run_smolvlm,\n    \"step3\": run_step3,\n    \"tarsier\": run_tarsier,\n    \"tarsier2\": run_tarsier2,\n}\n\n\nMODELS_NEED_VIDEO_METADATA = [\n    \"glm4_1v\",\n    \"glm4_5v\",\n    \"glm4_5v_fp8\",\n    \"qwen3_vl\",\n    \"qwen3_vl_moe\",\n]\n\n\ndef get_multi_modal_input(args):\n    \"\"\"\n    return {\n        \"data\": image or video,\n        \"question\": question,\n    }\n    \"\"\"\n    if args.modality == \"image\":\n        # Input image and question\n        image = convert_image_mode(ImageAsset(\"cherry_blossom\").pil_image, \"RGB\")\n        img_questions = [\n            \"What is the content of this image?\",\n            \"Describe the content of this image in detail.\",\n            \"What's in the image?\",\n            \"Where is this image taken?\",\n        ]\n\n        return {\n            \"data\": image,\n            \"questions\": img_questions,\n        }\n\n    if args.modality == \"video\":\n        # Input video and question\n        needs_metadata = args.model_type in MODELS_NEED_VIDEO_METADATA\n        video = VideoAsset(name=\"baby_reading\", num_frames=args.num_frames).np_ndarrays\n        metadata = VideoAsset(name=\"baby_reading\", num_frames=args.num_frames).metadata\n        vid_questions = [\"Why is this video funny?\"]\n\n        return {\n            \"data\": ([(video, metadata)] if needs_metadata else video),\n            \"questions\": vid_questions,\n        }\n\n    msg = f\"Modality {args.modality} is not supported.\"\n    raise ValueError(msg)\n\n\ndef apply_image_repeat(\n    image_repeat_prob, num_prompts, data, prompts: list[str], modality\n):\n    \"\"\"Repeats images with provided probability of \"image_repeat_prob\".\n    Used to simulate hit/miss for the MM preprocessor cache.\n    \"\"\"\n    assert image_repeat_prob <= 1.0 and image_repeat_prob >= 0\n    no_yes = [0, 1]\n    probs = [1.0 - image_repeat_prob, image_repeat_prob]\n\n    inputs = []\n    inputs_with_empty_media = []\n    cur_image = data\n    for i in range(num_prompts):\n        if image_repeat_prob is not None:\n            res = random.choices(no_yes, probs)[0]\n            if res == 0:\n                # No repeat => Modify one pixel\n                cur_image = cur_image.copy()\n                new_val = (i // 256 // 256, i // 256, i % 256)\n                cur_image.putpixel((0, 0), new_val)\n\n        uuid = \"uuid_{}\".format(i)\n\n        inputs.append(\n            {\n                \"prompt\": prompts[i % len(prompts)],\n                \"multi_modal_data\": {modality: cur_image},\n                \"multi_modal_uuids\": {modality: uuid},\n            }\n        )\n\n        inputs_with_empty_media.append(\n            {\n                \"prompt\": prompts[i % len(prompts)],\n                \"multi_modal_data\": {modality: None},\n                \"multi_modal_uuids\": {modality: uuid},\n            }\n        )\n\n    return inputs, inputs_with_empty_media\n\n\n@contextmanager\ndef time_counter(enable: bool):\n    if enable:\n        import time\n\n        start_time = time.time()\n        yield\n        elapsed_time = time.time() - start_time\n        print(\"-\" * 50)\n        print(\"-- generate time = {}\".format(elapsed_time))\n        print(\"-\" * 50)\n    else:\n        yield\n\n\ndef parse_args():\n    parser = FlexibleArgumentParser(\n        description=\"Demo on using vLLM for offline inference with \"\n        \"vision language models for text generation\"\n    )\n    parser.add_argument(\n        \"--model-type\",\n        \"-m\",\n        type=str,\n        default=\"llava\",\n        choices=model_example_map.keys(),\n        help='Huggingface \"model_type\".',\n    )\n    parser.add_argument(\n        \"--num-prompts\", type=int, default=4, help=\"Number of prompts to run.\"\n    )\n    parser.add_argument(\n        \"--modality\",\n        type=str,\n        default=\"image\",\n        choices=[\"image\", \"video\"],\n        help=\"Modality of the input.\",\n    )\n    parser.add_argument(\n        \"--num-frames\",\n        type=int,\n        default=16,\n        help=\"Number of frames to extract from the video.\",\n    )\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=0,\n        help=\"Set the seed when initializing `vllm.LLM`.\",\n    )\n\n    parser.add_argument(\n        \"--image-repeat-prob\",\n        type=float,\n        default=None,\n        help=\"Simulates the hit-ratio for multi-modal preprocessor cache (if enabled)\",\n    )\n\n    parser.add_argument(\n        \"--disable-mm-processor-cache\",\n        action=\"store_true\",\n        help=\"If True, disables caching of multi-modal processor.\",\n    )\n\n    parser.add_argument(\n        \"--time-generate\",\n        action=\"store_true\",\n        help=\"If True, then print the total generate() call time\",\n    )\n\n    parser.add_argument(\n        \"--use-different-prompt-per-request\",\n        action=\"store_true\",\n        help=\"If True, then use different prompt (with the same multi-modal \"\n        \"data) for each request.\",\n    )\n\n    parser.add_argument(\n        \"--verify-mm-cache-hit-with-uuids\",\n        action=\"store_true\",\n        help=\"If True, will send all requests in a second batch with empty mm \"\n        \"data to verify cache hits with UUIDs.\",\n    )\n    parser.add_argument(\n        \"--tensor-parallel-size\",\n        \"-tp\",\n        type=int,\n        default=None,\n        help=\"Tensor parallel size to override the model's default setting. \",\n    )\n    return parser.parse_args()\n\n\ndef main(args):\n    model = args.model_type\n    if model not in model_example_map:\n        raise ValueError(f\"Model type {model} is not supported.\")\n\n    if args.tensor_parallel_size is not None and args.tensor_parallel_size < 1:\n        raise ValueError(\n            f\"tensor_parallel_size must be a positive integer, \"\n            f\"got {args.tensor_parallel_size}\"\n        )\n\n    modality = args.modality\n    mm_input = get_multi_modal_input(args)\n    data = mm_input[\"data\"]\n    questions = mm_input[\"questions\"]\n\n    req_data = model_example_map[model](questions, modality)\n\n    # Disable other modalities to save memory\n    default_limits = {\"image\": 0, \"video\": 0, \"audio\": 0}\n    req_data.engine_args.limit_mm_per_prompt = default_limits | dict(\n        req_data.engine_args.limit_mm_per_prompt or {}\n    )\n\n    engine_args = asdict(req_data.engine_args) | {\n        \"seed\": args.seed,\n        \"mm_processor_cache_gb\": 0 if args.disable_mm_processor_cache else 4,\n    }\n    if args.tensor_parallel_size is not None:\n        engine_args[\"tensor_parallel_size\"] = args.tensor_parallel_size\n    llm = LLM(**engine_args)\n\n    # Don't want to check the flag multiple times, so just hijack `prompts`.\n    prompts = (\n        req_data.prompts\n        if args.use_different_prompt_per_request\n        else [req_data.prompts[0]]\n    )\n\n    # We set temperature to 0.2 so that outputs can be different\n    # even when all prompts are identical when running batch inference.\n    sampling_params = (\n        SamplingParams(\n            temperature=0.2, max_tokens=64, stop_token_ids=req_data.stop_token_ids\n        )\n        if req_data.sampling_params is None\n        else req_data.sampling_params\n    )\n\n    assert args.num_prompts > 0\n    if args.num_prompts == 1:\n        # Single inference\n        uuid = \"uuid_0\"\n        inputs = {\n            \"prompt\": prompts[0],\n            \"multi_modal_data\": {modality: data},\n            \"multi_modal_uuids\": {modality: uuid},\n        }\n        inputs_with_empty_media = {\n            \"prompt\": prompts[0],\n            \"multi_modal_data\": {modality: None},\n            \"multi_modal_uuids\": {modality: uuid},\n        }\n    else:\n        # Batch inference\n        if args.image_repeat_prob is not None:\n            # Repeat images with specified probability of \"image_repeat_prob\"\n            inputs, inputs_with_empty_media = apply_image_repeat(\n                args.image_repeat_prob,\n                args.num_prompts,\n                data,\n                prompts,\n                modality,\n            )\n        else:\n            # Use the same image for all prompts\n            inputs = []\n            inputs_with_empty_media = []\n            for i in range(args.num_prompts):\n                uuid = \"uuid_{}\".format(i)\n                inputs.append(\n                    {\n                        \"prompt\": prompts[i % len(prompts)],\n                        \"multi_modal_data\": {modality: data},\n                        \"multi_modal_uuids\": {modality: uuid},\n                    }\n                )\n                inputs_with_empty_media.append(\n                    {\n                        \"prompt\": prompts[i % len(prompts)],\n                        \"multi_modal_data\": {modality: None},\n                        \"multi_modal_uuids\": {modality: uuid},\n                    }\n                )\n\n    # Add LoRA request if applicable\n    lora_request = (\n        req_data.lora_requests * args.num_prompts if req_data.lora_requests else None\n    )\n\n    with time_counter(args.time_generate):\n        outputs = llm.generate(\n            inputs,\n            sampling_params=sampling_params,\n            lora_request=lora_request,\n        )\n\n    print(\"-\" * 50)\n    for o in outputs:\n        generated_text = o.outputs[0].text\n        print(generated_text)\n        print(\"-\" * 50)\n\n    if args.verify_mm_cache_hit_with_uuids:\n        try:\n            # Verify cache hits with UUIDs\n            print(\n                \"Sending a second batch of requests with empty media\"\n                \" and matching UUIDs.\"\n            )\n            outputs = llm.generate(\n                inputs_with_empty_media,\n                sampling_params=sampling_params,\n                lora_request=lora_request,\n            )\n            print(\"-\" * 50)\n            for o in outputs:\n                generated_text = o.outputs[0].text\n                print(generated_text)\n                print(\"-\" * 50)\n        except Exception as e:\n            print(f\"Failed to verify cache hits with UUIDs. Error: {e}\")\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    main(args)",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}