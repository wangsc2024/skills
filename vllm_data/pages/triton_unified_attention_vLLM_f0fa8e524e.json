{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
  "title": "triton_unified_attention - vLLM",
  "content": "Select tile size with Gemma3-specific optimization.\n\nFor Gemma3, use 32 for both prefill and decode to better utilize the larger head dimension (128/256). For other models, use the default vLLM behavior.\n\nDetect Gemma3 models via unique (head_size, sliding_window) signature.\n\nGemma3 models are the only ones using sliding_window=1024 with head_size 128 (27B) or 256 (1B, 4B, 12B). Other SWA models use different window sizes (Mistral=4096, Phi-3=2047).",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.attention.ops.triton_unified_attention ¶",
      "id": "vllm.attention.ops.triton_unified_attention"
    },
    {
      "level": "h2",
      "text": "float8_info module-attribute ¶",
      "id": "vllm.attention.ops.triton_unified_attention.float8_info"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.attention.ops.triton_unified_attention.logger"
    },
    {
      "level": "h2",
      "text": "_get_tile_size ¶",
      "id": "vllm.attention.ops.triton_unified_attention._get_tile_size"
    },
    {
      "level": "h2",
      "text": "_is_gemma3_attention ¶",
      "id": "vllm.attention.ops.triton_unified_attention._is_gemma3_attention"
    },
    {
      "level": "h2",
      "text": "apply_softcap ¶",
      "id": "vllm.attention.ops.triton_unified_attention.apply_softcap"
    },
    {
      "level": "h2",
      "text": "cdiv_fn ¶",
      "id": "vllm.attention.ops.triton_unified_attention.cdiv_fn"
    },
    {
      "level": "h2",
      "text": "find_seq_idx ¶",
      "id": "vllm.attention.ops.triton_unified_attention.find_seq_idx"
    },
    {
      "level": "h2",
      "text": "kernel_unified_attention_2d ¶",
      "id": "vllm.attention.ops.triton_unified_attention.kernel_unified_attention_2d"
    },
    {
      "level": "h2",
      "text": "kernel_unified_attention_3d ¶",
      "id": "vllm.attention.ops.triton_unified_attention.kernel_unified_attention_3d"
    },
    {
      "level": "h2",
      "text": "reduce_segments ¶",
      "id": "vllm.attention.ops.triton_unified_attention.reduce_segments"
    },
    {
      "level": "h2",
      "text": "unified_attention ¶",
      "id": "vllm.attention.ops.triton_unified_attention.unified_attention"
    }
  ],
  "code_samples": [
    {
      "code": "float8_info = finfo(fp8_dtype())",
      "language": "unknown"
    },
    {
      "code": "float8_info = finfo(fp8_dtype())",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "_get_tile_size(\n    head_size: int,\n    sliding_window: int,\n    element_size: int,\n    is_prefill: bool,\n) -> int",
      "language": "php"
    },
    {
      "code": "_get_tile_size(\n    head_size: int,\n    sliding_window: int,\n    element_size: int,\n    is_prefill: bool,\n) -> int",
      "language": "php"
    },
    {
      "code": "817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836",
      "language": "unknown"
    },
    {
      "code": "def _get_tile_size(\n    head_size: int,\n    sliding_window: int,\n    element_size: int,\n    is_prefill: bool,\n) -> int:\n    \"\"\"Select tile size with Gemma3-specific optimization.\n\n    For Gemma3, use 32 for both prefill and decode to better utilize\n    the larger head dimension (128/256). For other models, use\n    the default vLLM behavior.\n    \"\"\"\n    if _is_gemma3_attention(head_size, sliding_window):\n        # Gemma3: use 32 for decode (default is 16)\n        return 32\n\n    # Default behavior\n    if is_prefill:\n        return 32\n    return 16 if element_size >= 2 else 32",
      "language": "python"
    },
    {
      "code": "def _get_tile_size(\n    head_size: int,\n    sliding_window: int,\n    element_size: int,\n    is_prefill: bool,\n) -> int:\n    \"\"\"Select tile size with Gemma3-specific optimization.\n\n    For Gemma3, use 32 for both prefill and decode to better utilize\n    the larger head dimension (128/256). For other models, use\n    the default vLLM behavior.\n    \"\"\"\n    if _is_gemma3_attention(head_size, sliding_window):\n        # Gemma3: use 32 for decode (default is 16)\n        return 32\n\n    # Default behavior\n    if is_prefill:\n        return 32\n    return 16 if element_size >= 2 else 32",
      "language": "python"
    },
    {
      "code": "_is_gemma3_attention(\n    head_size: int, sliding_window: int\n) -> bool",
      "language": "php"
    },
    {
      "code": "_is_gemma3_attention(\n    head_size: int, sliding_window: int\n) -> bool",
      "language": "php"
    },
    {
      "code": "807\n808\n809\n810\n811\n812\n813\n814",
      "language": "unknown"
    },
    {
      "code": "def _is_gemma3_attention(head_size: int, sliding_window: int) -> bool:\n    \"\"\"Detect Gemma3 models via unique (head_size, sliding_window) signature.\n\n    Gemma3 models are the only ones using sliding_window=1024 with\n    head_size 128 (27B) or 256 (1B, 4B, 12B). Other SWA models use\n    different window sizes (Mistral=4096, Phi-3=2047).\n    \"\"\"\n    return sliding_window == 1024 and head_size in (128, 256)",
      "language": "python"
    },
    {
      "code": "def _is_gemma3_attention(head_size: int, sliding_window: int) -> bool:\n    \"\"\"Detect Gemma3 models via unique (head_size, sliding_window) signature.\n\n    Gemma3 models are the only ones using sliding_window=1024 with\n    head_size 128 (27B) or 256 (1B, 4B, 12B). Other SWA models use\n    different window sizes (Mistral=4096, Phi-3=2047).\n    \"\"\"\n    return sliding_window == 1024 and head_size in (128, 256)",
      "language": "python"
    },
    {
      "code": "apply_softcap(S, x)",
      "language": "unknown"
    },
    {
      "code": "apply_softcap(S, x)",
      "language": "unknown"
    },
    {
      "code": "25\n26\n27\n28\n29\n30",
      "language": "unknown"
    },
    {
      "code": "@triton.jit\ndef apply_softcap(S, x):\n    Sdiv = S / x\n    p1 = tl.exp(Sdiv)\n    p2 = tl.exp(-Sdiv)\n    return x * (p1 - p2) / (p1 + p2)",
      "language": "python"
    },
    {
      "code": "@triton.jit\ndef apply_softcap(S, x):\n    Sdiv = S / x\n    p1 = tl.exp(Sdiv)\n    p2 = tl.exp(-Sdiv)\n    return x * (p1 - p2) / (p1 + p2)",
      "language": "python"
    },
    {
      "code": "cdiv_fn(x, y)",
      "language": "unknown"
    },
    {
      "code": "cdiv_fn(x, y)",
      "language": "unknown"
    },
    {
      "code": "@triton.jit\ndef cdiv_fn(x, y):\n    return (x + y - 1) // y",
      "language": "python"
    },
    {
      "code": "@triton.jit\ndef cdiv_fn(x, y):\n    return (x + y - 1) // y",
      "language": "python"
    },
    {
      "code": "find_seq_idx(\n    query_start_len_ptr,\n    target_idx,\n    num_seqs,\n    BLOCK_Q: constexpr,\n    use_q_block_mode: constexpr,\n)",
      "language": "yaml"
    },
    {
      "code": "find_seq_idx(\n    query_start_len_ptr,\n    target_idx,\n    num_seqs,\n    BLOCK_Q: constexpr,\n    use_q_block_mode: constexpr,\n)",
      "language": "yaml"
    },
    {
      "code": "33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53",
      "language": "unknown"
    },
    {
      "code": "@triton.jit\ndef find_seq_idx(\n    query_start_len_ptr,\n    target_idx,\n    num_seqs,\n    BLOCK_Q: tl.constexpr,\n    use_q_block_mode: tl.constexpr,\n):\n    left: tl.int32 = 0\n    right = num_seqs\n    while left < right:\n        mid = (left + right) // 2\n        val = tl.load(query_start_len_ptr + mid)\n        mid_val = val // BLOCK_Q + mid if use_q_block_mode else val\n\n        if mid_val <= target_idx:\n            left = mid + 1\n        else:\n            right = mid\n\n    return left - 1",
      "language": "python"
    },
    {
      "code": "@triton.jit\ndef find_seq_idx(\n    query_start_len_ptr,\n    target_idx,\n    num_seqs,\n    BLOCK_Q: tl.constexpr,\n    use_q_block_mode: tl.constexpr,\n):\n    left: tl.int32 = 0\n    right = num_seqs\n    while left < right:\n        mid = (left + right) // 2\n        val = tl.load(query_start_len_ptr + mid)\n        mid_val = val // BLOCK_Q + mid if use_q_block_mode else val\n\n        if mid_val <= target_idx:\n            left = mid + 1\n        else:\n            right = mid\n\n    return left - 1",
      "language": "python"
    },
    {
      "code": "kernel_unified_attention_2d(\n    output_ptr,\n    query_ptr,\n    key_cache_ptr,\n    value_cache_ptr,\n    sink_ptr,\n    block_tables_ptr,\n    seq_lens_ptr,\n    alibi_slopes_ptr,\n    qq_bias_ptr,\n    scale,\n    k_scale,\n    v_scale,\n    out_scale,\n    softcap,\n    num_query_heads: constexpr,\n    num_queries_per_kv: constexpr,\n    block_table_stride: int64,\n    query_stride_0: int64,\n    query_stride_1: int64,\n    output_stride_0: int64,\n    output_stride_1: int64,\n    qq_bias_stride_0: int64,\n    BLOCK_SIZE: constexpr,\n    TILE_SIZE: constexpr,\n    HEAD_SIZE: constexpr,\n    HEAD_SIZE_PADDED: constexpr,\n    USE_ALIBI_SLOPES: constexpr,\n    USE_QQ_BIAS: constexpr,\n    USE_SOFTCAP: constexpr,\n    USE_SINKS: constexpr,\n    SLIDING_WINDOW: constexpr,\n    USE_MM_PREFIX: constexpr,\n    MAX_MM_RANGES: constexpr,\n    mm_prefix_range_ptr,\n    stride_k_cache_0: int64,\n    stride_k_cache_1: int64,\n    stride_k_cache_2: int64,\n    stride_k_cache_3: constexpr,\n    stride_v_cache_0: int64,\n    stride_v_cache_1: int64,\n    stride_v_cache_2: int64,\n    stride_v_cache_3: constexpr,\n    query_start_len_ptr,\n    BLOCK_Q: constexpr,\n    num_seqs: int32,\n    BLOCK_M: constexpr,\n    USE_FP8: constexpr,\n    FP8_MIN: constexpr = min,\n    FP8_MAX: constexpr = max,\n)",
      "language": "typescript"
    },
    {
      "code": "kernel_unified_attention_2d(\n    output_ptr,\n    query_ptr,\n    key_cache_ptr,\n    value_cache_ptr,\n    sink_ptr,\n    block_tables_ptr,\n    seq_lens_ptr,\n    alibi_slopes_ptr,\n    qq_bias_ptr,\n    scale,\n    k_scale,\n    v_scale,\n    out_scale,\n    softcap,\n    num_query_heads: constexpr,\n    num_queries_per_kv: constexpr,\n    block_table_stride: int64,\n    query_stride_0: int64,\n    query_stride_1: int64,\n    output_stride_0: int64,\n    output_stride_1: int64,\n    qq_bias_stride_0: int64,\n    BLOCK_SIZE: constexpr,\n    TILE_SIZE: constexpr,\n    HEAD_SIZE: constexpr,\n    HEAD_SIZE_PADDED: constexpr,\n    USE_ALIBI_SLOPES: constexpr,\n    USE_QQ_BIAS: constexpr,\n    USE_SOFTCAP: constexpr,\n    USE_SINKS: constexpr,\n    SLIDING_WINDOW: constexpr,\n    USE_MM_PREFIX: constexpr,\n    MAX_MM_RANGES: constexpr,\n    mm_prefix_range_ptr,\n    stride_k_cache_0: int64,\n    stride_k_cache_1: int64,\n    stride_k_cache_2: int64,\n    stride_k_cache_3: constexpr,\n    stride_v_cache_0: int64,\n    stride_v_cache_1: int64,\n    stride_v_cache_2: int64,\n    stride_v_cache_3: constexpr,\n    query_start_len_ptr,\n    BLOCK_Q: constexpr,\n    num_seqs: int32,\n    BLOCK_M: constexpr,\n    USE_FP8: constexpr,\n    FP8_MIN: constexpr = min,\n    FP8_MAX: constexpr = max,\n)",
      "language": "typescript"
    },
    {
      "code": "56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391",
      "language": "unknown"
    },
    {
      "code": "@triton.jit\ndef kernel_unified_attention_2d(\n    output_ptr,  # [num_tokens, num_query_heads, head_size]\n    query_ptr,  # [num_tokens, num_query_heads, head_size]\n    key_cache_ptr,  # [num_blks, blk_size, num_kv_heads, head_size]\n    value_cache_ptr,  # [num_blks, blk_size, num_kv_heads, head_size]\n    sink_ptr,  # [num_query_heads]\n    block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]\n    seq_lens_ptr,  # [num_seqs]\n    alibi_slopes_ptr,  # [num_query_heads]\n    qq_bias_ptr,  # [num_query_tokens, num_query_tokens]\n    scale,  # float32\n    k_scale,  # float32\n    v_scale,  # float32\n    out_scale,  # float32\n    softcap,  # float32\n    num_query_heads: tl.constexpr,  # int\n    num_queries_per_kv: tl.constexpr,  # int\n    block_table_stride: tl.int64,  # int\n    query_stride_0: tl.int64,  # int\n    query_stride_1: tl.int64,  # int, should be equal to head_size\n    output_stride_0: tl.int64,  # int\n    output_stride_1: tl.int64,  # int, should be equal to head_size\n    qq_bias_stride_0: tl.int64,  # int\n    BLOCK_SIZE: tl.constexpr,  # int\n    TILE_SIZE: tl.constexpr,  # int must be power of 2\n    HEAD_SIZE: tl.constexpr,  # int\n    HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2\n    USE_ALIBI_SLOPES: tl.constexpr,  # bool\n    USE_QQ_BIAS: tl.constexpr,  # bool\n    USE_SOFTCAP: tl.constexpr,  # bool\n    USE_SINKS: tl.constexpr,  # bool\n    SLIDING_WINDOW: tl.constexpr,  # int\n    USE_MM_PREFIX: tl.constexpr,  # bool\n    MAX_MM_RANGES: tl.constexpr,  # int\n    mm_prefix_range_ptr,  # [num_seqs] - prefix length for each sequence\n    stride_k_cache_0: tl.int64,  # int\n    stride_k_cache_1: tl.int64,  # int\n    stride_k_cache_2: tl.int64,  # int\n    stride_k_cache_3: tl.constexpr,  # int\n    stride_v_cache_0: tl.int64,  # int\n    stride_v_cache_1: tl.int64,  # int\n    stride_v_cache_2: tl.int64,  # int\n    stride_v_cache_3: tl.constexpr,  # int\n    query_start_len_ptr,  # [num_seqs+1]\n    BLOCK_Q: tl.constexpr,  # int\n    num_seqs: tl.int32,\n    BLOCK_M: tl.constexpr,  # int\n    USE_FP8: tl.constexpr,  # bool\n    FP8_MIN: tl.constexpr = float8_info.min,\n    FP8_MAX: tl.constexpr = float8_info.max,\n):\n    q_block_global_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n\n    seq_idx = find_seq_idx(\n        query_start_len_ptr, q_block_global_idx, num_seqs, BLOCK_Q, True\n    )\n\n    q_block_start_idx = tl.load(query_start_len_ptr + seq_idx) // BLOCK_Q + seq_idx\n\n    q_block_local_idx = q_block_global_idx - q_block_start_idx\n\n    cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)\n    cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)\n\n    cur_batch_query_len = cur_batch_in_all_stop_index - cur_batch_in_all_start_index\n\n    if q_block_local_idx * BLOCK_Q >= cur_batch_query_len:\n        return\n\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, HEAD_SIZE_PADDED)\n    offs_t = tl.arange(0, TILE_SIZE)\n    query_pos = q_block_local_idx * BLOCK_Q + offs_m // num_queries_per_kv\n\n    query_offset_0 = cur_batch_in_all_start_index + query_pos\n    query_offset_1 = kv_head_idx * num_queries_per_kv + offs_m % num_queries_per_kv\n    query_offset = (\n        query_offset_0[:, None] * query_stride_0\n        + query_offset_1[:, None] * query_stride_1\n        + offs_d[None, :]\n    )\n\n    dim_mask = tl.where(offs_d < HEAD_SIZE, 1, 0).to(tl.int1)\n    query_mask_0 = tl.where(query_pos < cur_batch_query_len, 1, 0).to(tl.int1)\n    query_mask_1 = tl.where(query_offset_1 < num_query_heads, 1, 0).to(tl.int1)\n\n    # Q : (BLOCK_M, HEAD_SIZE_PADDED)\n    Q = tl.load(\n        query_ptr + query_offset,\n        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],\n        other=0.0,\n    )\n\n    block_table_offset = seq_idx * block_table_stride\n\n    if not USE_SINKS:\n        M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n    else:\n        M = tl.load(\n            sink_ptr + query_offset_1,\n            mask=query_mask_1,\n            other=float(\"-inf\"),\n        ).to(dtype=tl.float32)\n\n    L = tl.full([BLOCK_M], 1.0, dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, HEAD_SIZE_PADDED], dtype=tl.float32)\n\n    # sequence len for this particular sequence\n    seq_len = tl.load(seq_lens_ptr + seq_idx)\n\n    # context length for this particular sequences\n    context_len = seq_len - cur_batch_query_len\n\n    # alibi slope for this head\n    if USE_ALIBI_SLOPES:\n        alibi_slope = tl.load(\n            alibi_slopes_ptr + query_offset_1, mask=query_mask_1, other=0.0\n        )\n\n    # query-query attention bias\n    if USE_QQ_BIAS:\n        qq_bias_row_ptrs = (\n            qq_bias_ptr + query_pos[:, None] * qq_bias_stride_0\n        )  # shape: [BLOCK_M]\n\n    # compute the length of the longest sequence prefix spanned by any\n    # query token in the current q_block (q_block_local_idx)\n    max_seq_prefix_len = (\n        context_len\n        + q_block_local_idx * BLOCK_Q\n        + (BLOCK_M - 1) // num_queries_per_kv\n        + 1\n    )\n\n    if USE_MM_PREFIX:\n        # image bidirectional attention ranges require a full range\n        # including q_block padding to make sure doc mask is correct\n        max_seq_prefix_len = tl.maximum(max_seq_prefix_len, seq_len)\n    else:\n        # adjust for potential padding in the last q_block by considering the\n        # actual sequence length\n        max_seq_prefix_len = tl.minimum(max_seq_prefix_len, seq_len)\n\n    # calculate the number of tiles that need to be processed to\n    # cover the longest sequence prefix (due to causal masking, tiles beyond\n    # this prefix can be skipped)\n    num_tiles = cdiv_fn(max_seq_prefix_len, TILE_SIZE)\n\n    # ---- Sliding-window tile pruning --------------------\n    # Default: keep previous global behavior\n    tile_start = 0\n    tile_end = num_tiles\n    # TODO(Isotr0py): sliding window pruning with image bidirectional mask\n    if SLIDING_WINDOW > 0 and not USE_MM_PREFIX:\n        # Query rows covered by this Q-block\n        qpos_lo = q_block_local_idx * BLOCK_Q\n        qpos_hi = tl.minimum(\n            qpos_lo + (BLOCK_M - 1) // num_queries_per_kv,\n            cur_batch_query_len - 1,\n        )\n        # For sliding window, each query position q can only attend to\n        # keys in the range [q_abs - SLIDING_WINDOW + 1, q_abs]\n        # where q_abs = context_len + q\n        # The union of allowed key positions for this Q-block is:\n        # [context_len + qpos_lo - SLIDING_WINDOW + 1, context_len + qpos_hi]\n        first_allowed_key = context_len + qpos_lo - SLIDING_WINDOW + 1\n        last_allowed_key = context_len + qpos_hi\n        # Convert to tile indices and clamp\n        tile_start = tl.maximum(0, first_allowed_key // TILE_SIZE)\n        tile_end = tl.minimum((last_allowed_key // TILE_SIZE) + 1, num_tiles)\n\n    # iterate through tiles (now limited to the sliding window range)\n    for j in range(tile_start, tile_end):\n        seq_offset = j * TILE_SIZE + offs_t\n        tile_mask = seq_offset < max_seq_prefix_len\n\n        physical_block_idx = tl.load(\n            block_tables_ptr + block_table_offset + seq_offset // BLOCK_SIZE\n        ).to(tl.int64)\n\n        v_offset = (\n            physical_block_idx[:, None] * stride_v_cache_0\n            + kv_head_idx * stride_v_cache_2\n            + offs_d[None, :] * stride_v_cache_3\n            + (seq_offset % BLOCK_SIZE)[:, None] * stride_v_cache_1\n        )\n\n        k_offset = (\n            physical_block_idx[None, :] * stride_k_cache_0\n            + kv_head_idx * stride_k_cache_2\n            + offs_d[:, None] * stride_k_cache_3\n            + (seq_offset % BLOCK_SIZE)[None, :] * stride_k_cache_1\n        )\n\n        # K : (HEAD_SIZE, TILE_SIZE)\n        K_load = tl.load(\n            key_cache_ptr + k_offset,\n            mask=dim_mask[:, None] & tile_mask[None, :],\n            other=0.0,\n        )\n\n        if K_load.dtype.is_fp8():\n            if Q.dtype.is_fp8():\n                K = K_load\n            else:\n                K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)\n        else:\n            K = K_load\n\n        # V : (TILE_SIZE, HEAD_SIZE)\n        V_load = tl.load(\n            value_cache_ptr + v_offset,\n            mask=dim_mask[None, :] & tile_mask[:, None],\n            other=0.0,\n        )\n\n        if V_load.dtype.is_fp8():\n            if Q.dtype.is_fp8():\n                V = V_load\n            else:\n                V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)\n        else:\n            V = V_load\n\n        # Compute attention mask: causal by default (key <= query)\n        query_abs_pos = context_len + query_pos[:, None]\n        seq_mask = seq_offset[None, :] <= query_abs_pos\n\n        # Apply sliding window to base mask BEFORE mm_prefix OR.\n        # Order must match FlexAttention: (causal AND sliding_window) OR mm_prefix\n        if SLIDING_WINDOW > 0:\n            seq_mask = seq_mask & ((query_abs_pos - seq_offset) < SLIDING_WINDOW)\n\n        # PrefixLM: extend mask with bidirectional ranges for multimodal tokens.\n        # Applied AFTER sliding window so mm_prefix ranges override SW restriction.\n        if USE_MM_PREFIX:\n            for i in range(MAX_MM_RANGES):\n                range_start = tl.load(\n                    mm_prefix_range_ptr + seq_idx * MAX_MM_RANGES * 2 + i * 2\n                )\n                range_end = tl.load(\n                    mm_prefix_range_ptr + seq_idx * MAX_MM_RANGES * 2 + i * 2 + 1\n                )\n\n                is_valid = range_start < range_end\n                q_in_range = (\n                    (query_abs_pos >= range_start)\n                    & (query_abs_pos <= range_end)\n                    & is_valid\n                )\n                k_in_range = (\n                    (seq_offset[None, :] >= range_start)\n                    & (seq_offset[None, :] <= range_end)\n                    & is_valid\n                )\n                seq_mask |= q_in_range & k_in_range\n\n        # S : (BLOCK_M, TILE_SIZE)\n        S = tl.zeros(shape=(BLOCK_M, TILE_SIZE), dtype=tl.float32)\n\n        S += scale * tl.dot(Q, K)\n\n        if USE_SOFTCAP:\n            S = apply_softcap(S, softcap)\n\n        S = tl.where(\n            query_mask_1[:, None] & query_mask_0[:, None] & seq_mask, S, float(\"-inf\")\n        )\n\n        if USE_ALIBI_SLOPES:\n            S += alibi_slope[:, None] * (seq_offset - context_len)\n\n        if USE_QQ_BIAS:\n            # compute key positions relative to query section\n            key_rel_pos = seq_offset - context_len  # shape: [BLOCK_SIZE]\n            # load bias only for keys that correspond to queries\n            is_query_key = key_rel_pos >= 0 and key_rel_pos < qq_bias_stride_0\n            qq_bias = tl.load(\n                qq_bias_row_ptrs + key_rel_pos[None, :],\n                mask=is_query_key[None, :],  # avoid OOB for context keys\n                other=0.0,\n            )\n            S += qq_bias\n\n        # compute running maximum\n        # m_j : (BLOCK_M,)\n        m_j = tl.maximum(M, tl.max(S, axis=1))\n\n        # For sliding window there's a chance the max is -inf due to masking of\n        # the entire row. In this case we need to set m_j 0 to avoid NaN\n        m_j = tl.where(m_j > float(\"-inf\"), m_j, 0.0)\n\n        # P : (BLOCK_M, TILE_SIZE)\n        P = tl.exp(S - m_j[:, None])\n\n        # l_j : (BLOCK_M,)\n        l_j = tl.sum(P, axis=1)\n\n        # alpha : (BLOCK_M, )\n        alpha = tl.exp(M - m_j)\n\n        # acc : (BLOCK_M, HEAD_SIZE_PADDED)\n        acc = acc * alpha[:, None]\n\n        # update constants\n        L = L * alpha + l_j\n        M = m_j\n\n        if SLIDING_WINDOW:\n            qpos_lo = q_block_local_idx * BLOCK_Q\n            V = tl.where(\n                (context_len + qpos_lo - seq_offset[:, None]) < SLIDING_WINDOW, V, 0.0\n            )\n\n        # acc : (BLOCK_M, HEAD_SIZE_PADDED)\n        acc += tl.dot(P.to(V.dtype), V)\n\n    # epilogue\n    acc = acc / L[:, None]\n    if USE_FP8:\n        acc = acc * tl.load(out_scale)\n        acc = tl.clamp(acc, FP8_MIN, FP8_MAX)\n\n    output_offset = (\n        query_offset_0[:, None] * output_stride_0\n        + query_offset_1[:, None] * output_stride_1\n        + offs_d[None, :]\n    )\n\n    tl.store(\n        output_ptr + output_offset,\n        acc,\n        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],\n    )",
      "language": "python"
    },
    {
      "code": "@triton.jit\ndef kernel_unified_attention_2d(\n    output_ptr,  # [num_tokens, num_query_heads, head_size]\n    query_ptr,  # [num_tokens, num_query_heads, head_size]\n    key_cache_ptr,  # [num_blks, blk_size, num_kv_heads, head_size]\n    value_cache_ptr,  # [num_blks, blk_size, num_kv_heads, head_size]\n    sink_ptr,  # [num_query_heads]\n    block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]\n    seq_lens_ptr,  # [num_seqs]\n    alibi_slopes_ptr,  # [num_query_heads]\n    qq_bias_ptr,  # [num_query_tokens, num_query_tokens]\n    scale,  # float32\n    k_scale,  # float32\n    v_scale,  # float32\n    out_scale,  # float32\n    softcap,  # float32\n    num_query_heads: tl.constexpr,  # int\n    num_queries_per_kv: tl.constexpr,  # int\n    block_table_stride: tl.int64,  # int\n    query_stride_0: tl.int64,  # int\n    query_stride_1: tl.int64,  # int, should be equal to head_size\n    output_stride_0: tl.int64,  # int\n    output_stride_1: tl.int64,  # int, should be equal to head_size\n    qq_bias_stride_0: tl.int64,  # int\n    BLOCK_SIZE: tl.constexpr,  # int\n    TILE_SIZE: tl.constexpr,  # int must be power of 2\n    HEAD_SIZE: tl.constexpr,  # int\n    HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2\n    USE_ALIBI_SLOPES: tl.constexpr,  # bool\n    USE_QQ_BIAS: tl.constexpr,  # bool\n    USE_SOFTCAP: tl.constexpr,  # bool\n    USE_SINKS: tl.constexpr,  # bool\n    SLIDING_WINDOW: tl.constexpr,  # int\n    USE_MM_PREFIX: tl.constexpr,  # bool\n    MAX_MM_RANGES: tl.constexpr,  # int\n    mm_prefix_range_ptr,  # [num_seqs] - prefix length for each sequence\n    stride_k_cache_0: tl.int64,  # int\n    stride_k_cache_1: tl.int64,  # int\n    stride_k_cache_2: tl.int64,  # int\n    stride_k_cache_3: tl.constexpr,  # int\n    stride_v_cache_0: tl.int64,  # int\n    stride_v_cache_1: tl.int64,  # int\n    stride_v_cache_2: tl.int64,  # int\n    stride_v_cache_3: tl.constexpr,  # int\n    query_start_len_ptr,  # [num_seqs+1]\n    BLOCK_Q: tl.constexpr,  # int\n    num_seqs: tl.int32,\n    BLOCK_M: tl.constexpr,  # int\n    USE_FP8: tl.constexpr,  # bool\n    FP8_MIN: tl.constexpr = float8_info.min,\n    FP8_MAX: tl.constexpr = float8_info.max,\n):\n    q_block_global_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n\n    seq_idx = find_seq_idx(\n        query_start_len_ptr, q_block_global_idx, num_seqs, BLOCK_Q, True\n    )\n\n    q_block_start_idx = tl.load(query_start_len_ptr + seq_idx) // BLOCK_Q + seq_idx\n\n    q_block_local_idx = q_block_global_idx - q_block_start_idx\n\n    cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)\n    cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)\n\n    cur_batch_query_len = cur_batch_in_all_stop_index - cur_batch_in_all_start_index\n\n    if q_block_local_idx * BLOCK_Q >= cur_batch_query_len:\n        return\n\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, HEAD_SIZE_PADDED)\n    offs_t = tl.arange(0, TILE_SIZE)\n    query_pos = q_block_local_idx * BLOCK_Q + offs_m // num_queries_per_kv\n\n    query_offset_0 = cur_batch_in_all_start_index + query_pos\n    query_offset_1 = kv_head_idx * num_queries_per_kv + offs_m % num_queries_per_kv\n    query_offset = (\n        query_offset_0[:, None] * query_stride_0\n        + query_offset_1[:, None] * query_stride_1\n        + offs_d[None, :]\n    )\n\n    dim_mask = tl.where(offs_d < HEAD_SIZE, 1, 0).to(tl.int1)\n    query_mask_0 = tl.where(query_pos < cur_batch_query_len, 1, 0).to(tl.int1)\n    query_mask_1 = tl.where(query_offset_1 < num_query_heads, 1, 0).to(tl.int1)\n\n    # Q : (BLOCK_M, HEAD_SIZE_PADDED)\n    Q = tl.load(\n        query_ptr + query_offset,\n        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],\n        other=0.0,\n    )\n\n    block_table_offset = seq_idx * block_table_stride\n\n    if not USE_SINKS:\n        M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n    else:\n        M = tl.load(\n            sink_ptr + query_offset_1,\n            mask=query_mask_1,\n            other=float(\"-inf\"),\n        ).to(dtype=tl.float32)\n\n    L = tl.full([BLOCK_M], 1.0, dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, HEAD_SIZE_PADDED], dtype=tl.float32)\n\n    # sequence len for this particular sequence\n    seq_len = tl.load(seq_lens_ptr + seq_idx)\n\n    # context length for this particular sequences\n    context_len = seq_len - cur_batch_query_len\n\n    # alibi slope for this head\n    if USE_ALIBI_SLOPES:\n        alibi_slope = tl.load(\n            alibi_slopes_ptr + query_offset_1, mask=query_mask_1, other=0.0\n        )\n\n    # query-query attention bias\n    if USE_QQ_BIAS:\n        qq_bias_row_ptrs = (\n            qq_bias_ptr + query_pos[:, None] * qq_bias_stride_0\n        )  # shape: [BLOCK_M]\n\n    # compute the length of the longest sequence prefix spanned by any\n    # query token in the current q_block (q_block_local_idx)\n    max_seq_prefix_len = (\n        context_len\n        + q_block_local_idx * BLOCK_Q\n        + (BLOCK_M - 1) // num_queries_per_kv\n        + 1\n    )\n\n    if USE_MM_PREFIX:\n        # image bidirectional attention ranges require a full range\n        # including q_block padding to make sure doc mask is correct\n        max_seq_prefix_len = tl.maximum(max_seq_prefix_len, seq_len)\n    else:\n        # adjust for potential padding in the last q_block by considering the\n        # actual sequence length\n        max_seq_prefix_len = tl.minimum(max_seq_prefix_len, seq_len)\n\n    # calculate the number of tiles that need to be processed to\n    # cover the longest sequence prefix (due to causal masking, tiles beyond\n    # this prefix can be skipped)\n    num_tiles = cdiv_fn(max_seq_prefix_len, TILE_SIZE)\n\n    # ---- Sliding-window tile pruning --------------------\n    # Default: keep previous global behavior\n    tile_start = 0\n    tile_end = num_tiles\n    # TODO(Isotr0py): sliding window pruning with image bidirectional mask\n    if SLIDING_WINDOW > 0 and not USE_MM_PREFIX:\n        # Query rows covered by this Q-block\n        qpos_lo = q_block_local_idx * BLOCK_Q\n        qpos_hi = tl.minimum(\n            qpos_lo + (BLOCK_M - 1) // num_queries_per_kv,\n            cur_batch_query_len - 1,\n        )\n        # For sliding window, each query position q can only attend to\n        # keys in the range [q_abs - SLIDING_WINDOW + 1, q_abs]\n        # where q_abs = context_len + q\n        # The union of allowed key positions for this Q-block is:\n        # [context_len + qpos_lo - SLIDING_WINDOW + 1, context_len + qpos_hi]\n        first_allowed_key = context_len + qpos_lo - SLIDING_WINDOW + 1\n        last_allowed_key = context_len + qpos_hi\n        # Convert to tile indices and clamp\n        tile_start = tl.maximum(0, first_allowed_key // TILE_SIZE)\n        tile_end = tl.minimum((last_allowed_key // TILE_SIZE) + 1, num_tiles)\n\n    # iterate through tiles (now limited to the sliding window range)\n    for j in range(tile_start, tile_end):\n        seq_offset = j * TILE_SIZE + offs_t\n        tile_mask = seq_offset < max_seq_prefix_len\n\n        physical_block_idx = tl.load(\n            block_tables_ptr + block_table_offset + seq_offset // BLOCK_SIZE\n        ).to(tl.int64)\n\n        v_offset = (\n            physical_block_idx[:, None] * stride_v_cache_0\n            + kv_head_idx * stride_v_cache_2\n            + offs_d[None, :] * stride_v_cache_3\n            + (seq_offset % BLOCK_SIZE)[:, None] * stride_v_cache_1\n        )\n\n        k_offset = (\n            physical_block_idx[None, :] * stride_k_cache_0\n            + kv_head_idx * stride_k_cache_2\n            + offs_d[:, None] * stride_k_cache_3\n            + (seq_offset % BLOCK_SIZE)[None, :] * stride_k_cache_1\n        )\n\n        # K : (HEAD_SIZE, TILE_SIZE)\n        K_load = tl.load(\n            key_cache_ptr + k_offset,\n            mask=dim_mask[:, None] & tile_mask[None, :],\n            other=0.0,\n        )\n\n        if K_load.dtype.is_fp8():\n            if Q.dtype.is_fp8():\n                K = K_load\n            else:\n                K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)\n        else:\n            K = K_load\n\n        # V : (TILE_SIZE, HEAD_SIZE)\n        V_load = tl.load(\n            value_cache_ptr + v_offset,\n            mask=dim_mask[None, :] & tile_mask[:, None],\n            other=0.0,\n        )\n\n        if V_load.dtype.is_fp8():\n            if Q.dtype.is_fp8():\n                V = V_load\n            else:\n                V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)\n        else:\n            V = V_load\n\n        # Compute attention mask: causal by default (key <= query)\n        query_abs_pos = context_len + query_pos[:, None]\n        seq_mask = seq_offset[None, :] <= query_abs_pos\n\n        # Apply sliding window to base mask BEFORE mm_prefix OR.\n        # Order must match FlexAttention: (causal AND sliding_window) OR mm_prefix\n        if SLIDING_WINDOW > 0:\n            seq_mask = seq_mask & ((query_abs_pos - seq_offset) < SLIDING_WINDOW)\n\n        # PrefixLM: extend mask with bidirectional ranges for multimodal tokens.\n        # Applied AFTER sliding window so mm_prefix ranges override SW restriction.\n        if USE_MM_PREFIX:\n            for i in range(MAX_MM_RANGES):\n                range_start = tl.load(\n                    mm_prefix_range_ptr + seq_idx * MAX_MM_RANGES * 2 + i * 2\n                )\n                range_end = tl.load(\n                    mm_prefix_range_ptr + seq_idx * MAX_MM_RANGES * 2 + i * 2 + 1\n                )\n\n                is_valid = range_start < range_end\n                q_in_range = (\n                    (query_abs_pos >= range_start)\n                    & (query_abs_pos <= range_end)\n                    & is_valid\n                )\n                k_in_range = (\n                    (seq_offset[None, :] >= range_start)\n                    & (seq_offset[None, :] <= range_end)\n                    & is_valid\n                )\n                seq_mask |= q_in_range & k_in_range\n\n        # S : (BLOCK_M, TILE_SIZE)\n        S = tl.zeros(shape=(BLOCK_M, TILE_SIZE), dtype=tl.float32)\n\n        S += scale * tl.dot(Q, K)\n\n        if USE_SOFTCAP:\n            S = apply_softcap(S, softcap)\n\n        S = tl.where(\n            query_mask_1[:, None] & query_mask_0[:, None] & seq_mask, S, float(\"-inf\")\n        )\n\n        if USE_ALIBI_SLOPES:\n            S += alibi_slope[:, None] * (seq_offset - context_len)\n\n        if USE_QQ_BIAS:\n            # compute key positions relative to query section\n            key_rel_pos = seq_offset - context_len  # shape: [BLOCK_SIZE]\n            # load bias only for keys that correspond to queries\n            is_query_key = key_rel_pos >= 0 and key_rel_pos < qq_bias_stride_0\n            qq_bias = tl.load(\n                qq_bias_row_ptrs + key_rel_pos[None, :],\n                mask=is_query_key[None, :],  # avoid OOB for context keys\n                other=0.0,\n            )\n            S += qq_bias\n\n        # compute running maximum\n        # m_j : (BLOCK_M,)\n        m_j = tl.maximum(M, tl.max(S, axis=1))\n\n        # For sliding window there's a chance the max is -inf due to masking of\n        # the entire row. In this case we need to set m_j 0 to avoid NaN\n        m_j = tl.where(m_j > float(\"-inf\"), m_j, 0.0)\n\n        # P : (BLOCK_M, TILE_SIZE)\n        P = tl.exp(S - m_j[:, None])\n\n        # l_j : (BLOCK_M,)\n        l_j = tl.sum(P, axis=1)\n\n        # alpha : (BLOCK_M, )\n        alpha = tl.exp(M - m_j)\n\n        # acc : (BLOCK_M, HEAD_SIZE_PADDED)\n        acc = acc * alpha[:, None]\n\n        # update constants\n        L = L * alpha + l_j\n        M = m_j\n\n        if SLIDING_WINDOW:\n            qpos_lo = q_block_local_idx * BLOCK_Q\n            V = tl.where(\n                (context_len + qpos_lo - seq_offset[:, None]) < SLIDING_WINDOW, V, 0.0\n            )\n\n        # acc : (BLOCK_M, HEAD_SIZE_PADDED)\n        acc += tl.dot(P.to(V.dtype), V)\n\n    # epilogue\n    acc = acc / L[:, None]\n    if USE_FP8:\n        acc = acc * tl.load(out_scale)\n        acc = tl.clamp(acc, FP8_MIN, FP8_MAX)\n\n    output_offset = (\n        query_offset_0[:, None] * output_stride_0\n        + query_offset_1[:, None] * output_stride_1\n        + offs_d[None, :]\n    )\n\n    tl.store(\n        output_ptr + output_offset,\n        acc,\n        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],\n    )",
      "language": "python"
    },
    {
      "code": "kernel_unified_attention_3d(\n    segm_output_ptr,\n    segm_max_ptr,\n    segm_expsum_ptr,\n    query_ptr,\n    key_cache_ptr,\n    value_cache_ptr,\n    sink_ptr,\n    block_tables_ptr,\n    seq_lens_ptr,\n    alibi_slopes_ptr,\n    qq_bias_ptr,\n    scale,\n    k_scale,\n    v_scale,\n    softcap,\n    num_query_heads: constexpr,\n    num_queries_per_kv: constexpr,\n    block_table_stride: int64,\n    query_stride_0: int64,\n    query_stride_1: int64,\n    qq_bias_stride_0: int64,\n    BLOCK_SIZE: constexpr,\n    TILE_SIZE: constexpr,\n    HEAD_SIZE: constexpr,\n    HEAD_SIZE_PADDED: constexpr,\n    USE_ALIBI_SLOPES: constexpr,\n    USE_QQ_BIAS: constexpr,\n    USE_SOFTCAP: constexpr,\n    USE_SINKS: constexpr,\n    SLIDING_WINDOW: constexpr,\n    stride_k_cache_0: int64,\n    stride_k_cache_1: int64,\n    stride_k_cache_2: int64,\n    stride_k_cache_3: constexpr,\n    stride_v_cache_0: int64,\n    stride_v_cache_1: int64,\n    stride_v_cache_2: int64,\n    stride_v_cache_3: constexpr,\n    query_start_len_ptr,\n    BLOCK_Q: constexpr,\n    num_seqs: int32,\n    BLOCK_M: constexpr,\n    NUM_SEGMENTS_PER_SEQ: constexpr,\n    USE_MM_PREFIX: constexpr,\n    MAX_MM_RANGES: constexpr,\n    mm_prefix_range_ptr,\n)",
      "language": "yaml"
    },
    {
      "code": "kernel_unified_attention_3d(\n    segm_output_ptr,\n    segm_max_ptr,\n    segm_expsum_ptr,\n    query_ptr,\n    key_cache_ptr,\n    value_cache_ptr,\n    sink_ptr,\n    block_tables_ptr,\n    seq_lens_ptr,\n    alibi_slopes_ptr,\n    qq_bias_ptr,\n    scale,\n    k_scale,\n    v_scale,\n    softcap,\n    num_query_heads: constexpr,\n    num_queries_per_kv: constexpr,\n    block_table_stride: int64,\n    query_stride_0: int64,\n    query_stride_1: int64,\n    qq_bias_stride_0: int64,\n    BLOCK_SIZE: constexpr,\n    TILE_SIZE: constexpr,\n    HEAD_SIZE: constexpr,\n    HEAD_SIZE_PADDED: constexpr,\n    USE_ALIBI_SLOPES: constexpr,\n    USE_QQ_BIAS: constexpr,\n    USE_SOFTCAP: constexpr,\n    USE_SINKS: constexpr,\n    SLIDING_WINDOW: constexpr,\n    stride_k_cache_0: int64,\n    stride_k_cache_1: int64,\n    stride_k_cache_2: int64,\n    stride_k_cache_3: constexpr,\n    stride_v_cache_0: int64,\n    stride_v_cache_1: int64,\n    stride_v_cache_2: int64,\n    stride_v_cache_3: constexpr,\n    query_start_len_ptr,\n    BLOCK_Q: constexpr,\n    num_seqs: int32,\n    BLOCK_M: constexpr,\n    NUM_SEGMENTS_PER_SEQ: constexpr,\n    USE_MM_PREFIX: constexpr,\n    MAX_MM_RANGES: constexpr,\n    mm_prefix_range_ptr,\n)",
      "language": "yaml"
    },
    {
      "code": "394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714",
      "language": "unknown"
    },
    {
      "code": "@triton.jit\ndef kernel_unified_attention_3d(\n    segm_output_ptr,\n    # [num_tokens, num_query_heads, num_segments, head_size_padded]\n    segm_max_ptr,  # [num_tokens, num_query_heads, num_segments]\n    segm_expsum_ptr,  # [num_tokens, num_query_heads, num_segments]\n    query_ptr,  # [num_tokens, num_query_heads, head_size]\n    key_cache_ptr,  # [num_blks, num_kv_heads, head_size // x, blk_size, x]\n    value_cache_ptr,  # [num_blks, num_kv_heads, head_size, blk_size]\n    sink_ptr,  # [num_query_heads]\n    block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]\n    seq_lens_ptr,  # [num_seqs]\n    alibi_slopes_ptr,  # [num_query_heads]\n    qq_bias_ptr,  # [num_query_tokens, num_query_tokens]\n    scale,  # float32\n    k_scale,  # float32\n    v_scale,  # float32\n    softcap,  # float32\n    num_query_heads: tl.constexpr,  # int\n    num_queries_per_kv: tl.constexpr,  # int\n    block_table_stride: tl.int64,  # int\n    query_stride_0: tl.int64,  # int\n    query_stride_1: tl.int64,  # int, should be equal to head_size\n    qq_bias_stride_0: tl.int64,  # int\n    BLOCK_SIZE: tl.constexpr,  # int\n    TILE_SIZE: tl.constexpr,  # int, must be power of 2\n    HEAD_SIZE: tl.constexpr,  # int\n    HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2\n    USE_ALIBI_SLOPES: tl.constexpr,  # bool\n    USE_QQ_BIAS: tl.constexpr,  # bool\n    USE_SOFTCAP: tl.constexpr,  # bool\n    USE_SINKS: tl.constexpr,  # bool\n    SLIDING_WINDOW: tl.constexpr,  # int\n    stride_k_cache_0: tl.int64,  # int\n    stride_k_cache_1: tl.int64,  # int\n    stride_k_cache_2: tl.int64,  # int\n    stride_k_cache_3: tl.constexpr,  # int\n    stride_v_cache_0: tl.int64,  # int\n    stride_v_cache_1: tl.int64,  # int\n    stride_v_cache_2: tl.int64,  # int\n    stride_v_cache_3: tl.constexpr,  # int\n    query_start_len_ptr,  # [num_seqs+1]\n    BLOCK_Q: tl.constexpr,  # int\n    num_seqs: tl.int32,\n    BLOCK_M: tl.constexpr,  # int\n    NUM_SEGMENTS_PER_SEQ: tl.constexpr,  # int\n    USE_MM_PREFIX: tl.constexpr,  # bool\n    MAX_MM_RANGES: tl.constexpr,  # int\n    mm_prefix_range_ptr,  # [num_seqs] - prefix length for each sequence\n):\n    q_block_global_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    segm_idx = tl.program_id(2)\n\n    seq_idx = find_seq_idx(\n        query_start_len_ptr, q_block_global_idx, num_seqs, BLOCK_Q, True\n    )\n\n    q_block_start_idx = tl.load(query_start_len_ptr + seq_idx) // BLOCK_Q + seq_idx\n\n    q_block_local_idx = q_block_global_idx - q_block_start_idx\n\n    cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)\n    cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)\n\n    cur_batch_query_len = cur_batch_in_all_stop_index - cur_batch_in_all_start_index\n\n    if q_block_local_idx * BLOCK_Q >= cur_batch_query_len:\n        return\n\n    # sequence len for this particular sequence\n    seq_len = tl.load(seq_lens_ptr + seq_idx)\n\n    # number of segments for this particular sequence\n    num_segments = NUM_SEGMENTS_PER_SEQ\n    tiles_per_segment = cdiv_fn(seq_len, num_segments * TILE_SIZE)\n\n    if segm_idx * tiles_per_segment * TILE_SIZE >= seq_len:\n        return\n\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, HEAD_SIZE_PADDED)\n    offs_t = tl.arange(0, TILE_SIZE)\n    query_pos = q_block_local_idx * BLOCK_Q + offs_m // num_queries_per_kv\n\n    query_offset_0 = cur_batch_in_all_start_index + query_pos\n    query_offset_1 = kv_head_idx * num_queries_per_kv + offs_m % num_queries_per_kv\n    query_offset = (\n        query_offset_0[:, None] * query_stride_0\n        + query_offset_1[:, None] * query_stride_1\n        + offs_d[None, :]\n    )\n\n    dim_mask = tl.where(offs_d < HEAD_SIZE, 1, 0).to(tl.int1)\n    query_mask_0 = tl.where(query_pos < cur_batch_query_len, 1, 0).to(tl.int1)\n    query_mask_1 = tl.where(query_offset_1 < num_query_heads, 1, 0).to(tl.int1)\n\n    # Q : (BLOCK_M, HEAD_SIZE_PADDED)\n    Q = tl.load(\n        query_ptr + query_offset,\n        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],\n        other=0.0,\n    )\n\n    block_table_offset = seq_idx * block_table_stride\n\n    if USE_SINKS:\n        if segm_idx == 0:\n            M = tl.load(\n                sink_ptr + query_offset_1,\n                mask=query_mask_1,\n                other=float(\"-inf\"),\n            ).to(dtype=tl.float32)\n        else:\n            M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n    else:\n        M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n\n    L = tl.full([BLOCK_M], 1.0, dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, HEAD_SIZE_PADDED], dtype=tl.float32)\n\n    # context length for this particular sequences\n    context_len = seq_len - cur_batch_query_len\n\n    # alibi slope for this head\n    if USE_ALIBI_SLOPES:\n        alibi_slope = tl.load(\n            alibi_slopes_ptr + query_offset_1, mask=query_mask_1, other=0.0\n        )\n\n    # query-query attention bias\n    if USE_QQ_BIAS:\n        qq_bias_row_ptrs = (\n            qq_bias_ptr + query_pos[:, None] * qq_bias_stride_0\n        )  # shape: [BLOCK_M]\n\n    # compute the length of the longest sequence prefix spanned by any\n    # query token in the current q_block (q_block_local_idx)\n    max_seq_prefix_len = (\n        context_len\n        + q_block_local_idx * BLOCK_Q\n        + (BLOCK_M - 1) // num_queries_per_kv\n        + 1\n    )\n\n    # adjust for potential padding in the last q_block by considering the\n    # actual sequence length\n    max_seq_prefix_len = tl.minimum(max_seq_prefix_len, seq_len)\n\n    # calculate the number of tiles that need to be processed to\n    # cover the longest sequence prefix (due to causal masking, tiles beyond\n    # this prefix can be skipped)\n    num_tiles = cdiv_fn(max_seq_prefix_len, TILE_SIZE)\n\n    # iterate through tiles within current segment\n    for j in range(\n        segm_idx * tiles_per_segment,\n        min((segm_idx + 1) * tiles_per_segment, num_tiles),\n    ):\n        seq_offset = j * TILE_SIZE + offs_t\n        tile_mask = seq_offset < max_seq_prefix_len\n\n        physical_block_idx = tl.load(\n            block_tables_ptr + block_table_offset + seq_offset // BLOCK_SIZE\n        ).to(tl.int64)\n\n        v_offset = (\n            physical_block_idx[:, None] * stride_v_cache_0\n            + kv_head_idx * stride_v_cache_2\n            + offs_d[None, :] * stride_v_cache_3\n            + (seq_offset % BLOCK_SIZE)[:, None] * stride_v_cache_1\n        )\n\n        k_offset = (\n            physical_block_idx[None, :] * stride_k_cache_0\n            + kv_head_idx * stride_k_cache_2\n            + offs_d[:, None] * stride_k_cache_3\n            + (seq_offset % BLOCK_SIZE)[None, :] * stride_k_cache_1\n        )\n\n        # K : (HEAD_SIZE, TILE_SIZE)\n        K_load = tl.load(\n            key_cache_ptr + k_offset,\n            mask=dim_mask[:, None] & tile_mask[None, :],\n            other=0.0,\n        )\n\n        if K_load.dtype.is_fp8():\n            if Q.dtype.is_fp8():\n                K = K_load\n            else:\n                K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)\n        else:\n            K = K_load\n\n        # V : (TILE_SIZE, HEAD_SIZE)\n        V_load = tl.load(\n            value_cache_ptr + v_offset,\n            mask=dim_mask[None, :] & tile_mask[:, None],\n            other=0.0,\n        )\n\n        if V_load.dtype.is_fp8():\n            if Q.dtype.is_fp8():\n                V = V_load\n            else:\n                V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)\n        else:\n            V = V_load\n\n        # Compute attention mask: causal by default (key <= query)\n        query_abs_pos = context_len + query_pos[:, None]\n        seq_mask = seq_offset[None, :] <= query_abs_pos\n\n        # Apply sliding window to base mask BEFORE mm_prefix OR.\n        # Order must match FlexAttention: (causal AND sliding_window) OR mm_prefix\n        if SLIDING_WINDOW > 0:\n            seq_mask = seq_mask & ((query_abs_pos - seq_offset) < SLIDING_WINDOW)\n\n        # PrefixLM: extend mask with bidirectional ranges for multimodal tokens.\n        # Applied AFTER sliding window so mm_prefix ranges override SW restriction.\n        if USE_MM_PREFIX:\n            for i in range(MAX_MM_RANGES):\n                range_start = tl.load(\n                    mm_prefix_range_ptr + seq_idx * MAX_MM_RANGES * 2 + i * 2\n                )\n                range_end = tl.load(\n                    mm_prefix_range_ptr + seq_idx * MAX_MM_RANGES * 2 + i * 2 + 1\n                )\n\n                is_valid = range_start < range_end\n                q_in_range = (\n                    (query_abs_pos >= range_start)\n                    & (query_abs_pos <= range_end)\n                    & is_valid\n                )\n                k_in_range = (\n                    (seq_offset[None, :] >= range_start)\n                    & (seq_offset[None, :] <= range_end)\n                    & is_valid\n                )\n                seq_mask |= q_in_range & k_in_range\n\n        # S : (BLOCK_M, TILE_SIZE)\n        S = tl.zeros(shape=(BLOCK_M, TILE_SIZE), dtype=tl.float32)\n        S += scale * tl.dot(Q, K)\n\n        if USE_SOFTCAP:\n            S = apply_softcap(S, softcap)\n\n        S = tl.where(\n            query_mask_1[:, None] & query_mask_0[:, None] & seq_mask, S, float(\"-inf\")\n        )\n\n        if USE_ALIBI_SLOPES:\n            S += alibi_slope[:, None] * (seq_offset - context_len)\n\n        if USE_QQ_BIAS:\n            # compute key positions relative to query section\n            key_rel_pos = seq_offset - context_len  # shape: [BLOCK_SIZE]\n            # load bias only for keys that correspond to queries\n            is_query_key = key_rel_pos >= 0 and key_rel_pos < qq_bias_stride_0\n            qq_bias = tl.load(\n                qq_bias_row_ptrs + key_rel_pos[None, :],\n                mask=is_query_key[None, :],  # avoid OOB for context keys\n                other=0.0,\n            )\n            S += qq_bias\n\n        # compute running maximum\n        # m_j : (BLOCK_M,)\n        m_j = tl.maximum(M, tl.max(S, axis=1))\n\n        # For sliding window there's a chance the max is -inf due to masking of\n        # the entire row. In this case we need to set m_j 0 to avoid NaN\n        m_j = tl.where(m_j > float(\"-inf\"), m_j, 0.0)\n\n        # P : (BLOCK_M, TILE_SIZE,)\n        P = tl.exp(S - m_j[:, None])\n\n        # l_j : (BLOCK_M,)\n        l_j = tl.sum(P, axis=1)\n\n        # alpha : (BLOCK_M, )\n        alpha = tl.exp(M - m_j)\n\n        # acc : (BLOCK_M, HEAD_SIZE_PADDED)\n        acc = acc * alpha[:, None]\n\n        # update constants\n        L = L * alpha + l_j\n        M = m_j\n\n        if SLIDING_WINDOW:\n            qpos_lo = q_block_local_idx * BLOCK_Q\n            V = tl.where(\n                (context_len + qpos_lo - seq_offset[:, None]) < SLIDING_WINDOW, V, 0.0\n            )\n\n        # acc : (BLOCK_M, HEAD_SIZE_PADDED)\n        acc += tl.dot(P.to(V.dtype), V)\n\n    segm_output_offset = (\n        query_offset_0[:, None].to(tl.int64)\n        * (num_query_heads * NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + query_offset_1[:, None] * (NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + segm_idx * HEAD_SIZE_PADDED\n        + tl.arange(0, HEAD_SIZE_PADDED)[None, :]\n    )\n    tl.store(\n        segm_output_ptr + segm_output_offset,\n        acc,\n        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],\n    )\n    segm_offset = (\n        query_offset_0.to(tl.int64) * (num_query_heads * NUM_SEGMENTS_PER_SEQ)\n        + query_offset_1 * NUM_SEGMENTS_PER_SEQ\n        + segm_idx\n    )\n    tl.store(segm_max_ptr + segm_offset, M, mask=query_mask_0 & query_mask_1)\n    tl.store(segm_expsum_ptr + segm_offset, L, mask=query_mask_0 & query_mask_1)",
      "language": "python"
    },
    {
      "code": "@triton.jit\ndef kernel_unified_attention_3d(\n    segm_output_ptr,\n    # [num_tokens, num_query_heads, num_segments, head_size_padded]\n    segm_max_ptr,  # [num_tokens, num_query_heads, num_segments]\n    segm_expsum_ptr,  # [num_tokens, num_query_heads, num_segments]\n    query_ptr,  # [num_tokens, num_query_heads, head_size]\n    key_cache_ptr,  # [num_blks, num_kv_heads, head_size // x, blk_size, x]\n    value_cache_ptr,  # [num_blks, num_kv_heads, head_size, blk_size]\n    sink_ptr,  # [num_query_heads]\n    block_tables_ptr,  # [num_seqs, max_num_blocks_per_seq]\n    seq_lens_ptr,  # [num_seqs]\n    alibi_slopes_ptr,  # [num_query_heads]\n    qq_bias_ptr,  # [num_query_tokens, num_query_tokens]\n    scale,  # float32\n    k_scale,  # float32\n    v_scale,  # float32\n    softcap,  # float32\n    num_query_heads: tl.constexpr,  # int\n    num_queries_per_kv: tl.constexpr,  # int\n    block_table_stride: tl.int64,  # int\n    query_stride_0: tl.int64,  # int\n    query_stride_1: tl.int64,  # int, should be equal to head_size\n    qq_bias_stride_0: tl.int64,  # int\n    BLOCK_SIZE: tl.constexpr,  # int\n    TILE_SIZE: tl.constexpr,  # int, must be power of 2\n    HEAD_SIZE: tl.constexpr,  # int\n    HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2\n    USE_ALIBI_SLOPES: tl.constexpr,  # bool\n    USE_QQ_BIAS: tl.constexpr,  # bool\n    USE_SOFTCAP: tl.constexpr,  # bool\n    USE_SINKS: tl.constexpr,  # bool\n    SLIDING_WINDOW: tl.constexpr,  # int\n    stride_k_cache_0: tl.int64,  # int\n    stride_k_cache_1: tl.int64,  # int\n    stride_k_cache_2: tl.int64,  # int\n    stride_k_cache_3: tl.constexpr,  # int\n    stride_v_cache_0: tl.int64,  # int\n    stride_v_cache_1: tl.int64,  # int\n    stride_v_cache_2: tl.int64,  # int\n    stride_v_cache_3: tl.constexpr,  # int\n    query_start_len_ptr,  # [num_seqs+1]\n    BLOCK_Q: tl.constexpr,  # int\n    num_seqs: tl.int32,\n    BLOCK_M: tl.constexpr,  # int\n    NUM_SEGMENTS_PER_SEQ: tl.constexpr,  # int\n    USE_MM_PREFIX: tl.constexpr,  # bool\n    MAX_MM_RANGES: tl.constexpr,  # int\n    mm_prefix_range_ptr,  # [num_seqs] - prefix length for each sequence\n):\n    q_block_global_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    segm_idx = tl.program_id(2)\n\n    seq_idx = find_seq_idx(\n        query_start_len_ptr, q_block_global_idx, num_seqs, BLOCK_Q, True\n    )\n\n    q_block_start_idx = tl.load(query_start_len_ptr + seq_idx) // BLOCK_Q + seq_idx\n\n    q_block_local_idx = q_block_global_idx - q_block_start_idx\n\n    cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)\n    cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)\n\n    cur_batch_query_len = cur_batch_in_all_stop_index - cur_batch_in_all_start_index\n\n    if q_block_local_idx * BLOCK_Q >= cur_batch_query_len:\n        return\n\n    # sequence len for this particular sequence\n    seq_len = tl.load(seq_lens_ptr + seq_idx)\n\n    # number of segments for this particular sequence\n    num_segments = NUM_SEGMENTS_PER_SEQ\n    tiles_per_segment = cdiv_fn(seq_len, num_segments * TILE_SIZE)\n\n    if segm_idx * tiles_per_segment * TILE_SIZE >= seq_len:\n        return\n\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, HEAD_SIZE_PADDED)\n    offs_t = tl.arange(0, TILE_SIZE)\n    query_pos = q_block_local_idx * BLOCK_Q + offs_m // num_queries_per_kv\n\n    query_offset_0 = cur_batch_in_all_start_index + query_pos\n    query_offset_1 = kv_head_idx * num_queries_per_kv + offs_m % num_queries_per_kv\n    query_offset = (\n        query_offset_0[:, None] * query_stride_0\n        + query_offset_1[:, None] * query_stride_1\n        + offs_d[None, :]\n    )\n\n    dim_mask = tl.where(offs_d < HEAD_SIZE, 1, 0).to(tl.int1)\n    query_mask_0 = tl.where(query_pos < cur_batch_query_len, 1, 0).to(tl.int1)\n    query_mask_1 = tl.where(query_offset_1 < num_query_heads, 1, 0).to(tl.int1)\n\n    # Q : (BLOCK_M, HEAD_SIZE_PADDED)\n    Q = tl.load(\n        query_ptr + query_offset,\n        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],\n        other=0.0,\n    )\n\n    block_table_offset = seq_idx * block_table_stride\n\n    if USE_SINKS:\n        if segm_idx == 0:\n            M = tl.load(\n                sink_ptr + query_offset_1,\n                mask=query_mask_1,\n                other=float(\"-inf\"),\n            ).to(dtype=tl.float32)\n        else:\n            M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n    else:\n        M = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n\n    L = tl.full([BLOCK_M], 1.0, dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, HEAD_SIZE_PADDED], dtype=tl.float32)\n\n    # context length for this particular sequences\n    context_len = seq_len - cur_batch_query_len\n\n    # alibi slope for this head\n    if USE_ALIBI_SLOPES:\n        alibi_slope = tl.load(\n            alibi_slopes_ptr + query_offset_1, mask=query_mask_1, other=0.0\n        )\n\n    # query-query attention bias\n    if USE_QQ_BIAS:\n        qq_bias_row_ptrs = (\n            qq_bias_ptr + query_pos[:, None] * qq_bias_stride_0\n        )  # shape: [BLOCK_M]\n\n    # compute the length of the longest sequence prefix spanned by any\n    # query token in the current q_block (q_block_local_idx)\n    max_seq_prefix_len = (\n        context_len\n        + q_block_local_idx * BLOCK_Q\n        + (BLOCK_M - 1) // num_queries_per_kv\n        + 1\n    )\n\n    # adjust for potential padding in the last q_block by considering the\n    # actual sequence length\n    max_seq_prefix_len = tl.minimum(max_seq_prefix_len, seq_len)\n\n    # calculate the number of tiles that need to be processed to\n    # cover the longest sequence prefix (due to causal masking, tiles beyond\n    # this prefix can be skipped)\n    num_tiles = cdiv_fn(max_seq_prefix_len, TILE_SIZE)\n\n    # iterate through tiles within current segment\n    for j in range(\n        segm_idx * tiles_per_segment,\n        min((segm_idx + 1) * tiles_per_segment, num_tiles),\n    ):\n        seq_offset = j * TILE_SIZE + offs_t\n        tile_mask = seq_offset < max_seq_prefix_len\n\n        physical_block_idx = tl.load(\n            block_tables_ptr + block_table_offset + seq_offset // BLOCK_SIZE\n        ).to(tl.int64)\n\n        v_offset = (\n            physical_block_idx[:, None] * stride_v_cache_0\n            + kv_head_idx * stride_v_cache_2\n            + offs_d[None, :] * stride_v_cache_3\n            + (seq_offset % BLOCK_SIZE)[:, None] * stride_v_cache_1\n        )\n\n        k_offset = (\n            physical_block_idx[None, :] * stride_k_cache_0\n            + kv_head_idx * stride_k_cache_2\n            + offs_d[:, None] * stride_k_cache_3\n            + (seq_offset % BLOCK_SIZE)[None, :] * stride_k_cache_1\n        )\n\n        # K : (HEAD_SIZE, TILE_SIZE)\n        K_load = tl.load(\n            key_cache_ptr + k_offset,\n            mask=dim_mask[:, None] & tile_mask[None, :],\n            other=0.0,\n        )\n\n        if K_load.dtype.is_fp8():\n            if Q.dtype.is_fp8():\n                K = K_load\n            else:\n                K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)\n        else:\n            K = K_load\n\n        # V : (TILE_SIZE, HEAD_SIZE)\n        V_load = tl.load(\n            value_cache_ptr + v_offset,\n            mask=dim_mask[None, :] & tile_mask[:, None],\n            other=0.0,\n        )\n\n        if V_load.dtype.is_fp8():\n            if Q.dtype.is_fp8():\n                V = V_load\n            else:\n                V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)\n        else:\n            V = V_load\n\n        # Compute attention mask: causal by default (key <= query)\n        query_abs_pos = context_len + query_pos[:, None]\n        seq_mask = seq_offset[None, :] <= query_abs_pos\n\n        # Apply sliding window to base mask BEFORE mm_prefix OR.\n        # Order must match FlexAttention: (causal AND sliding_window) OR mm_prefix\n        if SLIDING_WINDOW > 0:\n            seq_mask = seq_mask & ((query_abs_pos - seq_offset) < SLIDING_WINDOW)\n\n        # PrefixLM: extend mask with bidirectional ranges for multimodal tokens.\n        # Applied AFTER sliding window so mm_prefix ranges override SW restriction.\n        if USE_MM_PREFIX:\n            for i in range(MAX_MM_RANGES):\n                range_start = tl.load(\n                    mm_prefix_range_ptr + seq_idx * MAX_MM_RANGES * 2 + i * 2\n                )\n                range_end = tl.load(\n                    mm_prefix_range_ptr + seq_idx * MAX_MM_RANGES * 2 + i * 2 + 1\n                )\n\n                is_valid = range_start < range_end\n                q_in_range = (\n                    (query_abs_pos >= range_start)\n                    & (query_abs_pos <= range_end)\n                    & is_valid\n                )\n                k_in_range = (\n                    (seq_offset[None, :] >= range_start)\n                    & (seq_offset[None, :] <= range_end)\n                    & is_valid\n                )\n                seq_mask |= q_in_range & k_in_range\n\n        # S : (BLOCK_M, TILE_SIZE)\n        S = tl.zeros(shape=(BLOCK_M, TILE_SIZE), dtype=tl.float32)\n        S += scale * tl.dot(Q, K)\n\n        if USE_SOFTCAP:\n            S = apply_softcap(S, softcap)\n\n        S = tl.where(\n            query_mask_1[:, None] & query_mask_0[:, None] & seq_mask, S, float(\"-inf\")\n        )\n\n        if USE_ALIBI_SLOPES:\n            S += alibi_slope[:, None] * (seq_offset - context_len)\n\n        if USE_QQ_BIAS:\n            # compute key positions relative to query section\n            key_rel_pos = seq_offset - context_len  # shape: [BLOCK_SIZE]\n            # load bias only for keys that correspond to queries\n            is_query_key = key_rel_pos >= 0 and key_rel_pos < qq_bias_stride_0\n            qq_bias = tl.load(\n                qq_bias_row_ptrs + key_rel_pos[None, :],\n                mask=is_query_key[None, :],  # avoid OOB for context keys\n                other=0.0,\n            )\n            S += qq_bias\n\n        # compute running maximum\n        # m_j : (BLOCK_M,)\n        m_j = tl.maximum(M, tl.max(S, axis=1))\n\n        # For sliding window there's a chance the max is -inf due to masking of\n        # the entire row. In this case we need to set m_j 0 to avoid NaN\n        m_j = tl.where(m_j > float(\"-inf\"), m_j, 0.0)\n\n        # P : (BLOCK_M, TILE_SIZE,)\n        P = tl.exp(S - m_j[:, None])\n\n        # l_j : (BLOCK_M,)\n        l_j = tl.sum(P, axis=1)\n\n        # alpha : (BLOCK_M, )\n        alpha = tl.exp(M - m_j)\n\n        # acc : (BLOCK_M, HEAD_SIZE_PADDED)\n        acc = acc * alpha[:, None]\n\n        # update constants\n        L = L * alpha + l_j\n        M = m_j\n\n        if SLIDING_WINDOW:\n            qpos_lo = q_block_local_idx * BLOCK_Q\n            V = tl.where(\n                (context_len + qpos_lo - seq_offset[:, None]) < SLIDING_WINDOW, V, 0.0\n            )\n\n        # acc : (BLOCK_M, HEAD_SIZE_PADDED)\n        acc += tl.dot(P.to(V.dtype), V)\n\n    segm_output_offset = (\n        query_offset_0[:, None].to(tl.int64)\n        * (num_query_heads * NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + query_offset_1[:, None] * (NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + segm_idx * HEAD_SIZE_PADDED\n        + tl.arange(0, HEAD_SIZE_PADDED)[None, :]\n    )\n    tl.store(\n        segm_output_ptr + segm_output_offset,\n        acc,\n        mask=dim_mask[None, :] & query_mask_0[:, None] & query_mask_1[:, None],\n    )\n    segm_offset = (\n        query_offset_0.to(tl.int64) * (num_query_heads * NUM_SEGMENTS_PER_SEQ)\n        + query_offset_1 * NUM_SEGMENTS_PER_SEQ\n        + segm_idx\n    )\n    tl.store(segm_max_ptr + segm_offset, M, mask=query_mask_0 & query_mask_1)\n    tl.store(segm_expsum_ptr + segm_offset, L, mask=query_mask_0 & query_mask_1)",
      "language": "python"
    },
    {
      "code": "reduce_segments(\n    output_ptr,\n    segm_output_ptr,\n    segm_max_ptr,\n    segm_expsum_ptr,\n    seq_lens_ptr,\n    num_seqs,\n    num_query_heads: constexpr,\n    out_scale_inv,\n    output_stride_0: int64,\n    output_stride_1: int64,\n    block_table_stride: int64,\n    TILE_SIZE: constexpr,\n    HEAD_SIZE: constexpr,\n    HEAD_SIZE_PADDED: constexpr,\n    query_start_len_ptr,\n    BLOCK_Q: constexpr,\n    NUM_SEGMENTS_PER_SEQ: constexpr,\n    USE_FP8: constexpr,\n    FP8_MIN: constexpr = min,\n    FP8_MAX: constexpr = max,\n)",
      "language": "typescript"
    },
    {
      "code": "reduce_segments(\n    output_ptr,\n    segm_output_ptr,\n    segm_max_ptr,\n    segm_expsum_ptr,\n    seq_lens_ptr,\n    num_seqs,\n    num_query_heads: constexpr,\n    out_scale_inv,\n    output_stride_0: int64,\n    output_stride_1: int64,\n    block_table_stride: int64,\n    TILE_SIZE: constexpr,\n    HEAD_SIZE: constexpr,\n    HEAD_SIZE_PADDED: constexpr,\n    query_start_len_ptr,\n    BLOCK_Q: constexpr,\n    NUM_SEGMENTS_PER_SEQ: constexpr,\n    USE_FP8: constexpr,\n    FP8_MIN: constexpr = min,\n    FP8_MAX: constexpr = max,\n)",
      "language": "typescript"
    },
    {
      "code": "717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804",
      "language": "unknown"
    },
    {
      "code": "@triton.jit\ndef reduce_segments(\n    output_ptr,  # [num_tokens, num_query_heads, head_size]\n    segm_output_ptr,\n    # [num_tokens, num_query_heads, max_num_segments, head_size]\n    segm_max_ptr,  # [num_tokens, num_query_heads, max_num_segments]\n    segm_expsum_ptr,  # [num_tokens, num_query_heads, max_num_segments]\n    seq_lens_ptr,  # [num_seqs]\n    num_seqs,  # int\n    num_query_heads: tl.constexpr,  # int\n    out_scale_inv,  # float32\n    output_stride_0: tl.int64,  # int\n    output_stride_1: tl.int64,  # int, should be equal to head_size\n    block_table_stride: tl.int64,  # int\n    TILE_SIZE: tl.constexpr,  # int\n    HEAD_SIZE: tl.constexpr,  # int, must be power of 2\n    HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2\n    query_start_len_ptr,  # [num_seqs+1]\n    BLOCK_Q: tl.constexpr,  # int\n    NUM_SEGMENTS_PER_SEQ: tl.constexpr,  # int\n    USE_FP8: tl.constexpr,  # bool\n    FP8_MIN: tl.constexpr = float8_info.min,\n    FP8_MAX: tl.constexpr = float8_info.max,\n):\n    query_token_idx = tl.program_id(0)\n    query_head_idx = tl.program_id(1)\n\n    seq_idx = find_seq_idx(\n        query_start_len_ptr, query_token_idx, num_seqs, BLOCK_Q, False\n    )\n\n    # sequence len for this particular sequence\n    seq_len = tl.load(seq_lens_ptr + seq_idx)\n\n    # number of segments for this particular sequence\n    num_segments = NUM_SEGMENTS_PER_SEQ\n    tiles_per_segment = cdiv_fn(seq_len, num_segments * TILE_SIZE)\n\n    # create masks for subsequent loads\n    act_num_segments = cdiv_fn(seq_len, tiles_per_segment * TILE_SIZE)\n    segm_mask = tl.arange(0, NUM_SEGMENTS_PER_SEQ) < tl.full(\n        [NUM_SEGMENTS_PER_SEQ], act_num_segments, dtype=tl.int32\n    )\n    dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1, 0).to(tl.int1)\n\n    # load segment maxima\n    segm_offset = (\n        query_token_idx.to(tl.int64) * (num_query_heads * NUM_SEGMENTS_PER_SEQ)\n        + query_head_idx * NUM_SEGMENTS_PER_SEQ\n        + tl.arange(0, NUM_SEGMENTS_PER_SEQ)\n    )\n    segm_max = tl.load(segm_max_ptr + segm_offset, mask=segm_mask, other=float(\"-inf\"))\n    overall_max = tl.max(segm_max)\n\n    # load and rescale segment exp sums\n    segm_expsum = tl.load(segm_expsum_ptr + segm_offset, mask=segm_mask, other=0.0)\n    segm_expsum = segm_expsum * tl.exp(segm_max - overall_max)\n    overall_expsum = tl.sum(segm_expsum)\n\n    # load, rescale, and add segment attention outputs\n    segm_output_offset = (\n        query_token_idx.to(tl.int64)\n        * (num_query_heads * NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + query_head_idx * (NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + tl.arange(0, NUM_SEGMENTS_PER_SEQ)[:, None] * HEAD_SIZE_PADDED\n        + tl.arange(0, HEAD_SIZE_PADDED)[None, :]\n    )\n    segm_output = tl.load(\n        segm_output_ptr + segm_output_offset,\n        mask=segm_mask[:, None] & dim_mask[None, :],\n        other=0.0,\n    )\n    segm_output *= tl.exp(segm_max - overall_max)[:, None]\n    acc_sum = tl.sum(segm_output, axis=0)\n    # safely divide by overall_expsum, returning 0.0 if overall_expsum is 0\n    acc = tl.where(overall_expsum == 0.0, 0.0, acc_sum / overall_expsum)\n\n    if USE_FP8:\n        acc = acc * tl.load(out_scale_inv)\n        acc = tl.clamp(acc, FP8_MIN, FP8_MAX)\n\n    # write result\n    output_offset = (\n        query_token_idx * output_stride_0\n        + query_head_idx * output_stride_1\n        + tl.arange(0, HEAD_SIZE_PADDED)\n    )\n    tl.store(output_ptr + output_offset, acc, mask=dim_mask)",
      "language": "python"
    },
    {
      "code": "@triton.jit\ndef reduce_segments(\n    output_ptr,  # [num_tokens, num_query_heads, head_size]\n    segm_output_ptr,\n    # [num_tokens, num_query_heads, max_num_segments, head_size]\n    segm_max_ptr,  # [num_tokens, num_query_heads, max_num_segments]\n    segm_expsum_ptr,  # [num_tokens, num_query_heads, max_num_segments]\n    seq_lens_ptr,  # [num_seqs]\n    num_seqs,  # int\n    num_query_heads: tl.constexpr,  # int\n    out_scale_inv,  # float32\n    output_stride_0: tl.int64,  # int\n    output_stride_1: tl.int64,  # int, should be equal to head_size\n    block_table_stride: tl.int64,  # int\n    TILE_SIZE: tl.constexpr,  # int\n    HEAD_SIZE: tl.constexpr,  # int, must be power of 2\n    HEAD_SIZE_PADDED: tl.constexpr,  # int, must be power of 2\n    query_start_len_ptr,  # [num_seqs+1]\n    BLOCK_Q: tl.constexpr,  # int\n    NUM_SEGMENTS_PER_SEQ: tl.constexpr,  # int\n    USE_FP8: tl.constexpr,  # bool\n    FP8_MIN: tl.constexpr = float8_info.min,\n    FP8_MAX: tl.constexpr = float8_info.max,\n):\n    query_token_idx = tl.program_id(0)\n    query_head_idx = tl.program_id(1)\n\n    seq_idx = find_seq_idx(\n        query_start_len_ptr, query_token_idx, num_seqs, BLOCK_Q, False\n    )\n\n    # sequence len for this particular sequence\n    seq_len = tl.load(seq_lens_ptr + seq_idx)\n\n    # number of segments for this particular sequence\n    num_segments = NUM_SEGMENTS_PER_SEQ\n    tiles_per_segment = cdiv_fn(seq_len, num_segments * TILE_SIZE)\n\n    # create masks for subsequent loads\n    act_num_segments = cdiv_fn(seq_len, tiles_per_segment * TILE_SIZE)\n    segm_mask = tl.arange(0, NUM_SEGMENTS_PER_SEQ) < tl.full(\n        [NUM_SEGMENTS_PER_SEQ], act_num_segments, dtype=tl.int32\n    )\n    dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1, 0).to(tl.int1)\n\n    # load segment maxima\n    segm_offset = (\n        query_token_idx.to(tl.int64) * (num_query_heads * NUM_SEGMENTS_PER_SEQ)\n        + query_head_idx * NUM_SEGMENTS_PER_SEQ\n        + tl.arange(0, NUM_SEGMENTS_PER_SEQ)\n    )\n    segm_max = tl.load(segm_max_ptr + segm_offset, mask=segm_mask, other=float(\"-inf\"))\n    overall_max = tl.max(segm_max)\n\n    # load and rescale segment exp sums\n    segm_expsum = tl.load(segm_expsum_ptr + segm_offset, mask=segm_mask, other=0.0)\n    segm_expsum = segm_expsum * tl.exp(segm_max - overall_max)\n    overall_expsum = tl.sum(segm_expsum)\n\n    # load, rescale, and add segment attention outputs\n    segm_output_offset = (\n        query_token_idx.to(tl.int64)\n        * (num_query_heads * NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + query_head_idx * (NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)\n        + tl.arange(0, NUM_SEGMENTS_PER_SEQ)[:, None] * HEAD_SIZE_PADDED\n        + tl.arange(0, HEAD_SIZE_PADDED)[None, :]\n    )\n    segm_output = tl.load(\n        segm_output_ptr + segm_output_offset,\n        mask=segm_mask[:, None] & dim_mask[None, :],\n        other=0.0,\n    )\n    segm_output *= tl.exp(segm_max - overall_max)[:, None]\n    acc_sum = tl.sum(segm_output, axis=0)\n    # safely divide by overall_expsum, returning 0.0 if overall_expsum is 0\n    acc = tl.where(overall_expsum == 0.0, 0.0, acc_sum / overall_expsum)\n\n    if USE_FP8:\n        acc = acc * tl.load(out_scale_inv)\n        acc = tl.clamp(acc, FP8_MIN, FP8_MAX)\n\n    # write result\n    output_offset = (\n        query_token_idx * output_stride_0\n        + query_head_idx * output_stride_1\n        + tl.arange(0, HEAD_SIZE_PADDED)\n    )\n    tl.store(output_ptr + output_offset, acc, mask=dim_mask)",
      "language": "python"
    },
    {
      "code": "unified_attention(\n    q,\n    k,\n    v,\n    out,\n    cu_seqlens_q,\n    max_seqlen_q,\n    seqused_k,\n    max_seqlen_k,\n    softmax_scale,\n    causal,\n    window_size,\n    block_table,\n    softcap,\n    q_descale,\n    k_descale,\n    v_descale,\n    seq_threshold_3D=None,\n    num_par_softmax_segments=None,\n    softmax_segm_output=None,\n    softmax_segm_max=None,\n    softmax_segm_expsum=None,\n    alibi_slopes=None,\n    output_scale=None,\n    qq_bias=None,\n    sinks=None,\n    mm_prefix_range=None,\n)",
      "language": "rust"
    },
    {
      "code": "unified_attention(\n    q,\n    k,\n    v,\n    out,\n    cu_seqlens_q,\n    max_seqlen_q,\n    seqused_k,\n    max_seqlen_k,\n    softmax_scale,\n    causal,\n    window_size,\n    block_table,\n    softcap,\n    q_descale,\n    k_descale,\n    v_descale,\n    seq_threshold_3D=None,\n    num_par_softmax_segments=None,\n    softmax_segm_output=None,\n    softmax_segm_max=None,\n    softmax_segm_expsum=None,\n    alibi_slopes=None,\n    output_scale=None,\n    qq_bias=None,\n    sinks=None,\n    mm_prefix_range=None,\n)",
      "language": "rust"
    },
    {
      "code": "839\n 840\n 841\n 842\n 843\n 844\n 845\n 846\n 847\n 848\n 849\n 850\n 851\n 852\n 853\n 854\n 855\n 856\n 857\n 858\n 859\n 860\n 861\n 862\n 863\n 864\n 865\n 866\n 867\n 868\n 869\n 870\n 871\n 872\n 873\n 874\n 875\n 876\n 877\n 878\n 879\n 880\n 881\n 882\n 883\n 884\n 885\n 886\n 887\n 888\n 889\n 890\n 891\n 892\n 893\n 894\n 895\n 896\n 897\n 898\n 899\n 900\n 901\n 902\n 903\n 904\n 905\n 906\n 907\n 908\n 909\n 910\n 911\n 912\n 913\n 914\n 915\n 916\n 917\n 918\n 919\n 920\n 921\n 922\n 923\n 924\n 925\n 926\n 927\n 928\n 929\n 930\n 931\n 932\n 933\n 934\n 935\n 936\n 937\n 938\n 939\n 940\n 941\n 942\n 943\n 944\n 945\n 946\n 947\n 948\n 949\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065",
      "language": "unknown"
    },
    {
      "code": "def unified_attention(\n    q,\n    k,\n    v,\n    out,\n    cu_seqlens_q,\n    max_seqlen_q,\n    seqused_k,\n    max_seqlen_k,\n    softmax_scale,\n    causal,\n    window_size,\n    block_table,\n    softcap,\n    q_descale,\n    k_descale,\n    v_descale,\n    seq_threshold_3D=None,\n    num_par_softmax_segments=None,\n    softmax_segm_output=None,\n    softmax_segm_max=None,\n    softmax_segm_expsum=None,\n    alibi_slopes=None,\n    output_scale=None,\n    qq_bias=None,\n    # Optional tensor for sinks\n    sinks=None,\n    # Optional tensor for prefix lengths (PrefixLM support)\n    mm_prefix_range=None,\n):\n    assert causal, \"Only causal attention is supported\"\n    assert q_descale is None, \"Q scales not supported\"\n\n    if sinks is not None:\n        assert sinks.shape[0] == q.shape[1], \"Sinks must be num_query_heads size\"\n\n    use_mm_prefix = False\n    max_mm_ranges = 0\n    if mm_prefix_range is not None:\n        if mm_prefix_range.ndim == 3:\n            use_mm_prefix = True\n            max_mm_ranges = mm_prefix_range.shape[1]\n        else:\n            raise ValueError(\n                f\"Unsupported mm_prefix_range shape: {mm_prefix_range.shape}\"\n            )\n\n    use_alibi_slopes = alibi_slopes is not None\n    use_qq_bias = qq_bias is not None\n\n    block_size = v.shape[1]\n    num_seqs = len(seqused_k)\n    num_query_heads = q.shape[1]\n    num_kv_heads = k.shape[2]\n    num_queries_per_kv = num_query_heads // num_kv_heads\n    head_size = q.shape[2]\n\n    BLOCK_M = (\n        16 if num_queries_per_kv <= 16 else triton.next_power_of_2(num_queries_per_kv)\n    )\n    BLOCK_Q = BLOCK_M // num_queries_per_kv\n\n    # Ideally we would launch with kernel with:\n    # \\sum_i[ceil(query_len[i] / BLOCK_Q)] blocks.\n    # However, it is slow to realize the query_lens on cpu.\n    # Instead we use upper-bound:\n    # \\sum_i[ceil(query_len[i] / BLOCK_Q)]\n    #   <= \\sum_i[floor(query_len[i] / BLOCK_Q) + 1]\n    #    = \\sum_i[floor(query_len[i] / BLOCK_Q)] + num_seqs\n    #   <= floor(\\sum_i(query_len[i]) / BLOCK_Q) + num_seqs\n    #    = floor(q.shape[0] / BLOCK_Q) + num_seqs\n    total_num_q_blocks = q.shape[0] // BLOCK_Q + num_seqs\n\n    # Tile sizes for prefill and decode. Gemma3 models use optimized values.\n    # Note: tile size must be at least 32 for fp8 (element_size == 1).\n    sliding_window_val = 1 + window_size[0] if window_size[0] >= 0 else 0\n    TILE_SIZE_PREFILL = _get_tile_size(\n        head_size,\n        sliding_window_val,\n        q.element_size(),\n        is_prefill=True,\n    )\n    TILE_SIZE_DECODE = _get_tile_size(\n        head_size,\n        sliding_window_val,\n        q.element_size(),\n        is_prefill=False,\n    )\n\n    # Launch the 2D kernel if\n    # 1. No intermediate tiled softmax buffers for the 3D kernel have been allocated, or\n    # 2. The batch includes at least one prefill request, or\n    # 3. The number of sequences exceeds the configured threshold\n    if (\n        seq_threshold_3D is None\n        or num_par_softmax_segments is None\n        or softmax_segm_output is None\n        or softmax_segm_max is None\n        or softmax_segm_expsum is None\n        or max_seqlen_q > 1\n        or num_seqs > seq_threshold_3D\n    ):\n        kernel_unified_attention_2d[\n            (\n                total_num_q_blocks,\n                num_kv_heads,\n            )\n        ](\n            output_ptr=out,\n            query_ptr=q,\n            key_cache_ptr=k,\n            value_cache_ptr=v,\n            sink_ptr=sinks,\n            block_tables_ptr=block_table,\n            seq_lens_ptr=seqused_k,\n            alibi_slopes_ptr=alibi_slopes,\n            qq_bias_ptr=qq_bias,\n            scale=softmax_scale,\n            k_scale=k_descale,\n            v_scale=v_descale,\n            out_scale=1 / output_scale if output_scale is not None else 1.0,\n            softcap=softcap,\n            num_query_heads=num_query_heads,\n            num_queries_per_kv=num_queries_per_kv,\n            block_table_stride=block_table.stride(0),\n            query_stride_0=q.stride(0),\n            query_stride_1=q.stride(1),\n            output_stride_0=out.stride(0),\n            output_stride_1=out.stride(1),\n            qq_bias_stride_0=qq_bias.stride(0) if use_qq_bias else 0,\n            BLOCK_SIZE=block_size,\n            TILE_SIZE=TILE_SIZE_PREFILL,\n            HEAD_SIZE=head_size,\n            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n            USE_ALIBI_SLOPES=use_alibi_slopes,\n            USE_QQ_BIAS=use_qq_bias,\n            USE_SOFTCAP=(softcap > 0),\n            USE_SINKS=(sinks is not None),\n            USE_MM_PREFIX=use_mm_prefix,\n            MAX_MM_RANGES=max_mm_ranges,\n            mm_prefix_range_ptr=mm_prefix_range,\n            SLIDING_WINDOW=(1 + window_size[0]),\n            stride_k_cache_0=k.stride(0),\n            stride_k_cache_1=k.stride(1),\n            stride_k_cache_2=k.stride(2),\n            stride_k_cache_3=k.stride(3),\n            stride_v_cache_0=v.stride(0),\n            stride_v_cache_1=v.stride(1),\n            stride_v_cache_2=v.stride(2),\n            stride_v_cache_3=v.stride(3),\n            query_start_len_ptr=cu_seqlens_q,\n            BLOCK_Q=BLOCK_Q,\n            num_seqs=num_seqs,\n            BLOCK_M=BLOCK_M,\n            USE_FP8=output_scale is not None,\n        )\n    else:\n        kernel_unified_attention_3d[\n            (total_num_q_blocks, num_kv_heads, num_par_softmax_segments)\n        ](\n            segm_output_ptr=softmax_segm_output,\n            segm_max_ptr=softmax_segm_max,\n            segm_expsum_ptr=softmax_segm_expsum,\n            query_ptr=q,\n            key_cache_ptr=k,\n            value_cache_ptr=v,\n            sink_ptr=sinks,\n            block_tables_ptr=block_table,\n            seq_lens_ptr=seqused_k,\n            alibi_slopes_ptr=alibi_slopes,\n            qq_bias_ptr=qq_bias,\n            scale=softmax_scale,\n            k_scale=k_descale,\n            v_scale=v_descale,\n            softcap=softcap,\n            num_query_heads=num_query_heads,\n            num_queries_per_kv=num_queries_per_kv,\n            block_table_stride=block_table.stride(0),\n            query_stride_0=q.stride(0),\n            query_stride_1=q.stride(1),\n            qq_bias_stride_0=qq_bias.stride(0) if use_qq_bias else 0,\n            BLOCK_SIZE=block_size,\n            TILE_SIZE=TILE_SIZE_DECODE,\n            HEAD_SIZE=head_size,\n            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n            USE_ALIBI_SLOPES=use_alibi_slopes,\n            USE_QQ_BIAS=use_qq_bias,\n            USE_SOFTCAP=(softcap > 0),\n            USE_SINKS=(sinks is not None),\n            USE_MM_PREFIX=use_mm_prefix,\n            MAX_MM_RANGES=max_mm_ranges,\n            mm_prefix_range_ptr=mm_prefix_range,\n            SLIDING_WINDOW=(1 + window_size[0]),\n            stride_k_cache_0=k.stride(0),\n            stride_k_cache_1=k.stride(1),\n            stride_k_cache_2=k.stride(2),\n            stride_k_cache_3=k.stride(3),\n            stride_v_cache_0=v.stride(0),\n            stride_v_cache_1=v.stride(1),\n            stride_v_cache_2=v.stride(2),\n            stride_v_cache_3=v.stride(3),\n            query_start_len_ptr=cu_seqlens_q,\n            BLOCK_Q=BLOCK_Q,\n            num_seqs=num_seqs,\n            BLOCK_M=BLOCK_M,\n            NUM_SEGMENTS_PER_SEQ=num_par_softmax_segments,\n        )\n        reduce_segments[(q.shape[0], num_query_heads)](\n            output_ptr=out,\n            segm_output_ptr=softmax_segm_output,\n            segm_max_ptr=softmax_segm_max,\n            segm_expsum_ptr=softmax_segm_expsum,\n            seq_lens_ptr=seqused_k,\n            num_seqs=num_seqs,\n            num_query_heads=num_query_heads,\n            out_scale_inv=1 / output_scale if output_scale is not None else 1.0,\n            output_stride_0=out.stride(0),\n            output_stride_1=out.stride(1),\n            block_table_stride=block_table.stride(0),\n            TILE_SIZE=TILE_SIZE_DECODE,\n            HEAD_SIZE=head_size,\n            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n            query_start_len_ptr=cu_seqlens_q,\n            BLOCK_Q=BLOCK_Q,\n            NUM_SEGMENTS_PER_SEQ=num_par_softmax_segments,\n            USE_FP8=output_scale is not None,\n        )",
      "language": "python"
    },
    {
      "code": "def unified_attention(\n    q,\n    k,\n    v,\n    out,\n    cu_seqlens_q,\n    max_seqlen_q,\n    seqused_k,\n    max_seqlen_k,\n    softmax_scale,\n    causal,\n    window_size,\n    block_table,\n    softcap,\n    q_descale,\n    k_descale,\n    v_descale,\n    seq_threshold_3D=None,\n    num_par_softmax_segments=None,\n    softmax_segm_output=None,\n    softmax_segm_max=None,\n    softmax_segm_expsum=None,\n    alibi_slopes=None,\n    output_scale=None,\n    qq_bias=None,\n    # Optional tensor for sinks\n    sinks=None,\n    # Optional tensor for prefix lengths (PrefixLM support)\n    mm_prefix_range=None,\n):\n    assert causal, \"Only causal attention is supported\"\n    assert q_descale is None, \"Q scales not supported\"\n\n    if sinks is not None:\n        assert sinks.shape[0] == q.shape[1], \"Sinks must be num_query_heads size\"\n\n    use_mm_prefix = False\n    max_mm_ranges = 0\n    if mm_prefix_range is not None:\n        if mm_prefix_range.ndim == 3:\n            use_mm_prefix = True\n            max_mm_ranges = mm_prefix_range.shape[1]\n        else:\n            raise ValueError(\n                f\"Unsupported mm_prefix_range shape: {mm_prefix_range.shape}\"\n            )\n\n    use_alibi_slopes = alibi_slopes is not None\n    use_qq_bias = qq_bias is not None\n\n    block_size = v.shape[1]\n    num_seqs = len(seqused_k)\n    num_query_heads = q.shape[1]\n    num_kv_heads = k.shape[2]\n    num_queries_per_kv = num_query_heads // num_kv_heads\n    head_size = q.shape[2]\n\n    BLOCK_M = (\n        16 if num_queries_per_kv <= 16 else triton.next_power_of_2(num_queries_per_kv)\n    )\n    BLOCK_Q = BLOCK_M // num_queries_per_kv\n\n    # Ideally we would launch with kernel with:\n    # \\sum_i[ceil(query_len[i] / BLOCK_Q)] blocks.\n    # However, it is slow to realize the query_lens on cpu.\n    # Instead we use upper-bound:\n    # \\sum_i[ceil(query_len[i] / BLOCK_Q)]\n    #   <= \\sum_i[floor(query_len[i] / BLOCK_Q) + 1]\n    #    = \\sum_i[floor(query_len[i] / BLOCK_Q)] + num_seqs\n    #   <= floor(\\sum_i(query_len[i]) / BLOCK_Q) + num_seqs\n    #    = floor(q.shape[0] / BLOCK_Q) + num_seqs\n    total_num_q_blocks = q.shape[0] // BLOCK_Q + num_seqs\n\n    # Tile sizes for prefill and decode. Gemma3 models use optimized values.\n    # Note: tile size must be at least 32 for fp8 (element_size == 1).\n    sliding_window_val = 1 + window_size[0] if window_size[0] >= 0 else 0\n    TILE_SIZE_PREFILL = _get_tile_size(\n        head_size,\n        sliding_window_val,\n        q.element_size(),\n        is_prefill=True,\n    )\n    TILE_SIZE_DECODE = _get_tile_size(\n        head_size,\n        sliding_window_val,\n        q.element_size(),\n        is_prefill=False,\n    )\n\n    # Launch the 2D kernel if\n    # 1. No intermediate tiled softmax buffers for the 3D kernel have been allocated, or\n    # 2. The batch includes at least one prefill request, or\n    # 3. The number of sequences exceeds the configured threshold\n    if (\n        seq_threshold_3D is None\n        or num_par_softmax_segments is None\n        or softmax_segm_output is None\n        or softmax_segm_max is None\n        or softmax_segm_expsum is None\n        or max_seqlen_q > 1\n        or num_seqs > seq_threshold_3D\n    ):\n        kernel_unified_attention_2d[\n            (\n                total_num_q_blocks,\n                num_kv_heads,\n            )\n        ](\n            output_ptr=out,\n            query_ptr=q,\n            key_cache_ptr=k,\n            value_cache_ptr=v,\n            sink_ptr=sinks,\n            block_tables_ptr=block_table,\n            seq_lens_ptr=seqused_k,\n            alibi_slopes_ptr=alibi_slopes,\n            qq_bias_ptr=qq_bias,\n            scale=softmax_scale,\n            k_scale=k_descale,\n            v_scale=v_descale,\n            out_scale=1 / output_scale if output_scale is not None else 1.0,\n            softcap=softcap,\n            num_query_heads=num_query_heads,\n            num_queries_per_kv=num_queries_per_kv,\n            block_table_stride=block_table.stride(0),\n            query_stride_0=q.stride(0),\n            query_stride_1=q.stride(1),\n            output_stride_0=out.stride(0),\n            output_stride_1=out.stride(1),\n            qq_bias_stride_0=qq_bias.stride(0) if use_qq_bias else 0,\n            BLOCK_SIZE=block_size,\n            TILE_SIZE=TILE_SIZE_PREFILL,\n            HEAD_SIZE=head_size,\n            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n            USE_ALIBI_SLOPES=use_alibi_slopes,\n            USE_QQ_BIAS=use_qq_bias,\n            USE_SOFTCAP=(softcap > 0),\n            USE_SINKS=(sinks is not None),\n            USE_MM_PREFIX=use_mm_prefix,\n            MAX_MM_RANGES=max_mm_ranges,\n            mm_prefix_range_ptr=mm_prefix_range,\n            SLIDING_WINDOW=(1 + window_size[0]),\n            stride_k_cache_0=k.stride(0),\n            stride_k_cache_1=k.stride(1),\n            stride_k_cache_2=k.stride(2),\n            stride_k_cache_3=k.stride(3),\n            stride_v_cache_0=v.stride(0),\n            stride_v_cache_1=v.stride(1),\n            stride_v_cache_2=v.stride(2),\n            stride_v_cache_3=v.stride(3),\n            query_start_len_ptr=cu_seqlens_q,\n            BLOCK_Q=BLOCK_Q,\n            num_seqs=num_seqs,\n            BLOCK_M=BLOCK_M,\n            USE_FP8=output_scale is not None,\n        )\n    else:\n        kernel_unified_attention_3d[\n            (total_num_q_blocks, num_kv_heads, num_par_softmax_segments)\n        ](\n            segm_output_ptr=softmax_segm_output,\n            segm_max_ptr=softmax_segm_max,\n            segm_expsum_ptr=softmax_segm_expsum,\n            query_ptr=q,\n            key_cache_ptr=k,\n            value_cache_ptr=v,\n            sink_ptr=sinks,\n            block_tables_ptr=block_table,\n            seq_lens_ptr=seqused_k,\n            alibi_slopes_ptr=alibi_slopes,\n            qq_bias_ptr=qq_bias,\n            scale=softmax_scale,\n            k_scale=k_descale,\n            v_scale=v_descale,\n            softcap=softcap,\n            num_query_heads=num_query_heads,\n            num_queries_per_kv=num_queries_per_kv,\n            block_table_stride=block_table.stride(0),\n            query_stride_0=q.stride(0),\n            query_stride_1=q.stride(1),\n            qq_bias_stride_0=qq_bias.stride(0) if use_qq_bias else 0,\n            BLOCK_SIZE=block_size,\n            TILE_SIZE=TILE_SIZE_DECODE,\n            HEAD_SIZE=head_size,\n            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n            USE_ALIBI_SLOPES=use_alibi_slopes,\n            USE_QQ_BIAS=use_qq_bias,\n            USE_SOFTCAP=(softcap > 0),\n            USE_SINKS=(sinks is not None),\n            USE_MM_PREFIX=use_mm_prefix,\n            MAX_MM_RANGES=max_mm_ranges,\n            mm_prefix_range_ptr=mm_prefix_range,\n            SLIDING_WINDOW=(1 + window_size[0]),\n            stride_k_cache_0=k.stride(0),\n            stride_k_cache_1=k.stride(1),\n            stride_k_cache_2=k.stride(2),\n            stride_k_cache_3=k.stride(3),\n            stride_v_cache_0=v.stride(0),\n            stride_v_cache_1=v.stride(1),\n            stride_v_cache_2=v.stride(2),\n            stride_v_cache_3=v.stride(3),\n            query_start_len_ptr=cu_seqlens_q,\n            BLOCK_Q=BLOCK_Q,\n            num_seqs=num_seqs,\n            BLOCK_M=BLOCK_M,\n            NUM_SEGMENTS_PER_SEQ=num_par_softmax_segments,\n        )\n        reduce_segments[(q.shape[0], num_query_heads)](\n            output_ptr=out,\n            segm_output_ptr=softmax_segm_output,\n            segm_max_ptr=softmax_segm_max,\n            segm_expsum_ptr=softmax_segm_expsum,\n            seq_lens_ptr=seqused_k,\n            num_seqs=num_seqs,\n            num_query_heads=num_query_heads,\n            out_scale_inv=1 / output_scale if output_scale is not None else 1.0,\n            output_stride_0=out.stride(0),\n            output_stride_1=out.stride(1),\n            block_table_stride=block_table.stride(0),\n            TILE_SIZE=TILE_SIZE_DECODE,\n            HEAD_SIZE=head_size,\n            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n            query_start_len_ptr=cu_seqlens_q,\n            BLOCK_Q=BLOCK_Q,\n            NUM_SEGMENTS_PER_SEQ=num_par_softmax_segments,\n            USE_FP8=output_scale is not None,\n        )",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}