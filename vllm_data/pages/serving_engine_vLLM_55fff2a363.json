{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
  "title": "serving_engine - vLLM",
  "content": "Bases: ServeContext[ClassificationRequest]\n\nBases: ServeContext[EmbeddingRequest]\n\nraised when finish_reason indicates internal server error (500)\n\nPulls the request id to use from a header, if provided\n\nBuild and return a RenderConfig for an endpoint.\n\nUsed by the renderer to control how prompts are prepared (e.g., tokenization and length handling). Endpoints should implement this with logic appropriate to their request type.\n\nDefault response builder. Subclass may override this method to return the appropriate response object.\n\nCollect batch results from the result generator.\n\nConvert GenerationError to ErrorResponse.\n\nConvert GenerationError to streaming error response.\n\nDetermine if there are any active default multimodal loras.\n\nReturn (and cache) an AsyncMicrobatchTokenizer bound to the given tokenizer.\n\nPulls the data parallel rank from a header, if provided\n\nRetrieve the set of types from message content dicts up until _; we use this to match potential multimodal data with default per modality loras.\n\nGet the reasoning parser based on the name.\n\nGet a Renderer instance with the provided tokenizer. Uses shared async tokenizer pool for efficiency.\n\nGet the tool parser based on the name.\n\nExecute the request processing pipeline yielding responses.\n\nSchedule the request and get the result generator.\n\nDefault preprocessing hook. Subclasses may override to prepare ctx (classification, embedding, etc.).\n\nUse the Processor to process inputs for AsyncLLM.\n\nRaise GenerationError if finish_reason indicates an error.\n\nA simpler implementation that tokenizes a single prompt input.\n\nA simpler implementation that tokenizes multiple prompt inputs.\n\nMixin for request processing, handling prompt preparation and engine input.\n\nMixin for response generation, managing result generators and final batch results.\n\nBases: RequestProcessingMixin, ResponseGenerationMixin, Generic[RequestT]",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.entrypoints.openai.serving_engine ¶",
      "id": "vllm.entrypoints.openai.serving_engine"
    },
    {
      "level": "h2",
      "text": "AnyRequest module-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.AnyRequest"
    },
    {
      "level": "h2",
      "text": "AnyResponse module-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.AnyResponse"
    },
    {
      "level": "h2",
      "text": "ChatLikeRequest module-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ChatLikeRequest"
    },
    {
      "level": "h2",
      "text": "CompletionLikeRequest module-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.CompletionLikeRequest"
    },
    {
      "level": "h2",
      "text": "RequestT module-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.RequestT"
    },
    {
      "level": "h2",
      "text": "SpeechToTextRequest module-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.SpeechToTextRequest"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.logger"
    },
    {
      "level": "h2",
      "text": "ClassificationServeContext dataclass ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ClassificationServeContext"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ClassificationServeContext.__init__"
    },
    {
      "level": "h2",
      "text": "EmbeddingServeContext dataclass ¶",
      "id": "vllm.entrypoints.openai.serving_engine.EmbeddingServeContext"
    },
    {
      "level": "h3",
      "text": "chat_template class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.EmbeddingServeContext.chat_template"
    },
    {
      "level": "h3",
      "text": "chat_template_content_format instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.EmbeddingServeContext.chat_template_content_format"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.serving_engine.EmbeddingServeContext.__init__"
    },
    {
      "level": "h2",
      "text": "GenerationError ¶",
      "id": "vllm.entrypoints.openai.serving_engine.GenerationError"
    },
    {
      "level": "h3",
      "text": "status_code instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.GenerationError.status_code"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.serving_engine.GenerationError.__init__"
    },
    {
      "level": "h2",
      "text": "OpenAIServing ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing"
    },
    {
      "level": "h3",
      "text": "_apply_mistral_chat_template_async instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._apply_mistral_chat_template_async"
    },
    {
      "level": "h3",
      "text": "_async_tokenizer_pool instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._async_tokenizer_pool"
    },
    {
      "level": "h3",
      "text": "_tokenizer_executor instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._tokenizer_executor"
    },
    {
      "level": "h3",
      "text": "engine_client instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.engine_client"
    },
    {
      "level": "h3",
      "text": "input_processor instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.input_processor"
    },
    {
      "level": "h3",
      "text": "io_processor instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.io_processor"
    },
    {
      "level": "h3",
      "text": "log_error_stack instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.log_error_stack"
    },
    {
      "level": "h3",
      "text": "max_model_len instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.max_model_len"
    },
    {
      "level": "h3",
      "text": "model_config instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.model_config"
    },
    {
      "level": "h3",
      "text": "models instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.models"
    },
    {
      "level": "h3",
      "text": "request_id_prefix class-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.request_id_prefix"
    },
    {
      "level": "h3",
      "text": "request_logger instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.request_logger"
    },
    {
      "level": "h3",
      "text": "return_tokens_as_token_ids instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.return_tokens_as_token_ids"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.__init__"
    },
    {
      "level": "h3",
      "text": "_base_request_id staticmethod ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._base_request_id"
    },
    {
      "level": "h3",
      "text": "_build_render_config ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._build_render_config"
    },
    {
      "level": "h3",
      "text": "_build_response ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._build_response"
    },
    {
      "level": "h3",
      "text": "_check_model async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._check_model"
    },
    {
      "level": "h3",
      "text": "_collect_batch async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._collect_batch"
    },
    {
      "level": "h3",
      "text": "_convert_generation_error_to_response ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._convert_generation_error_to_response"
    },
    {
      "level": "h3",
      "text": "_convert_generation_error_to_streaming_response ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._convert_generation_error_to_streaming_response"
    },
    {
      "level": "h3",
      "text": "_create_pooling_params ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._create_pooling_params"
    },
    {
      "level": "h3",
      "text": "_generate_with_builtin_tools async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._generate_with_builtin_tools"
    },
    {
      "level": "h3",
      "text": "_get_active_default_mm_loras ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_active_default_mm_loras"
    },
    {
      "level": "h3",
      "text": "_get_async_tokenizer ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_async_tokenizer"
    },
    {
      "level": "h3",
      "text": "_get_data_parallel_rank staticmethod ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_data_parallel_rank"
    },
    {
      "level": "h3",
      "text": "_get_decoded_token staticmethod ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_decoded_token"
    },
    {
      "level": "h3",
      "text": "_get_message_types ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_message_types"
    },
    {
      "level": "h3",
      "text": "_get_prompt_components ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_prompt_components"
    },
    {
      "level": "h3",
      "text": "_get_reasoning_parser ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_reasoning_parser"
    },
    {
      "level": "h3",
      "text": "_get_renderer ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_renderer"
    },
    {
      "level": "h3",
      "text": "_get_tool_parser ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_tool_parser"
    },
    {
      "level": "h3",
      "text": "_get_trace_headers async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._get_trace_headers"
    },
    {
      "level": "h3",
      "text": "_is_model_supported ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._is_model_supported"
    },
    {
      "level": "h3",
      "text": "_log_inputs ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._log_inputs"
    },
    {
      "level": "h3",
      "text": "_maybe_get_adapters ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._maybe_get_adapters"
    },
    {
      "level": "h3",
      "text": "_normalize_prompt_text_to_input async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._normalize_prompt_text_to_input"
    },
    {
      "level": "h3",
      "text": "_normalize_prompt_tokens_to_input async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._normalize_prompt_tokens_to_input"
    },
    {
      "level": "h3",
      "text": "_parse_tool_calls_from_content staticmethod ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._parse_tool_calls_from_content"
    },
    {
      "level": "h3",
      "text": "_pipeline async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._pipeline"
    },
    {
      "level": "h3",
      "text": "_prepare_generators async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._prepare_generators"
    },
    {
      "level": "h3",
      "text": "_preprocess async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._preprocess"
    },
    {
      "level": "h3",
      "text": "_preprocess_chat async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._preprocess_chat"
    },
    {
      "level": "h3",
      "text": "_process_inputs async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._process_inputs"
    },
    {
      "level": "h3",
      "text": "_raise_if_error ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._raise_if_error"
    },
    {
      "level": "h3",
      "text": "_render_next_turn async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._render_next_turn"
    },
    {
      "level": "h3",
      "text": "_tokenize_prompt_input_async async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._tokenize_prompt_input_async"
    },
    {
      "level": "h3",
      "text": "_tokenize_prompt_inputs_async async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._tokenize_prompt_inputs_async"
    },
    {
      "level": "h3",
      "text": "_validate_chat_template ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._validate_chat_template"
    },
    {
      "level": "h3",
      "text": "_validate_input ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._validate_input"
    },
    {
      "level": "h3",
      "text": "_validate_request ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing._validate_request"
    },
    {
      "level": "h3",
      "text": "beam_search async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.beam_search"
    },
    {
      "level": "h3",
      "text": "create_error_response ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.create_error_response"
    },
    {
      "level": "h3",
      "text": "create_streaming_error_response ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.create_streaming_error_response"
    },
    {
      "level": "h3",
      "text": "handle async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.handle"
    },
    {
      "level": "h3",
      "text": "reset_mm_cache async ¶",
      "id": "vllm.entrypoints.openai.serving_engine.OpenAIServing.reset_mm_cache"
    },
    {
      "level": "h2",
      "text": "RequestProcessingMixin dataclass ¶",
      "id": "vllm.entrypoints.openai.serving_engine.RequestProcessingMixin"
    },
    {
      "level": "h3",
      "text": "engine_prompts class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.RequestProcessingMixin.engine_prompts"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.serving_engine.RequestProcessingMixin.__init__"
    },
    {
      "level": "h2",
      "text": "ResponseGenerationMixin dataclass ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ResponseGenerationMixin"
    },
    {
      "level": "h3",
      "text": "final_res_batch class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ResponseGenerationMixin.final_res_batch"
    },
    {
      "level": "h3",
      "text": "model_config class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ResponseGenerationMixin.model_config"
    },
    {
      "level": "h3",
      "text": "result_generator class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ResponseGenerationMixin.result_generator"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ResponseGenerationMixin.__init__"
    },
    {
      "level": "h2",
      "text": "ServeContext dataclass ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext"
    },
    {
      "level": "h3",
      "text": "created_time class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext.created_time"
    },
    {
      "level": "h3",
      "text": "lora_request class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext.lora_request"
    },
    {
      "level": "h3",
      "text": "model_name instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext.model_name"
    },
    {
      "level": "h3",
      "text": "raw_request class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext.raw_request"
    },
    {
      "level": "h3",
      "text": "request instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext.request"
    },
    {
      "level": "h3",
      "text": "request_id instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext.request_id"
    },
    {
      "level": "h3",
      "text": "tokenizer class-attribute instance-attribute ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext.tokenizer"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.serving_engine.ServeContext.__init__"
    },
    {
      "level": "h2",
      "text": "clamp_prompt_logprobs ¶",
      "id": "vllm.entrypoints.openai.serving_engine.clamp_prompt_logprobs"
    }
  ],
  "code_samples": [
    {
      "code": "AnyRequest: TypeAlias = (\n    CompletionLikeRequest\n    | ChatLikeRequest\n    | SpeechToTextRequest\n    | ResponsesRequest\n    | IOProcessorRequest\n    | GenerateRequest\n)",
      "language": "typescript"
    },
    {
      "code": "AnyRequest: TypeAlias = (\n    CompletionLikeRequest\n    | ChatLikeRequest\n    | SpeechToTextRequest\n    | ResponsesRequest\n    | IOProcessorRequest\n    | GenerateRequest\n)",
      "language": "typescript"
    },
    {
      "code": "AnyResponse: TypeAlias = (\n    CompletionResponse\n    | ChatCompletionResponse\n    | EmbeddingResponse\n    | TranscriptionResponse\n    | TokenizeResponse\n    | PoolingResponse\n    | ClassificationResponse\n    | ScoreResponse\n    | GenerateResponse\n)",
      "language": "typescript"
    },
    {
      "code": "AnyResponse: TypeAlias = (\n    CompletionResponse\n    | ChatCompletionResponse\n    | EmbeddingResponse\n    | TranscriptionResponse\n    | TokenizeResponse\n    | PoolingResponse\n    | ClassificationResponse\n    | ScoreResponse\n    | GenerateResponse\n)",
      "language": "typescript"
    },
    {
      "code": "ChatLikeRequest: TypeAlias = (\n    ChatCompletionRequest\n    | EmbeddingChatRequest\n    | TokenizeChatRequest\n    | ClassificationChatRequest\n)",
      "language": "typescript"
    },
    {
      "code": "ChatLikeRequest: TypeAlias = (\n    ChatCompletionRequest\n    | EmbeddingChatRequest\n    | TokenizeChatRequest\n    | ClassificationChatRequest\n)",
      "language": "typescript"
    },
    {
      "code": "CompletionLikeRequest: TypeAlias = (\n    CompletionRequest\n    | DetokenizeRequest\n    | EmbeddingCompletionRequest\n    | RerankRequest\n    | ClassificationCompletionRequest\n    | ScoreRequest\n    | TokenizeCompletionRequest\n)",
      "language": "typescript"
    },
    {
      "code": "CompletionLikeRequest: TypeAlias = (\n    CompletionRequest\n    | DetokenizeRequest\n    | EmbeddingCompletionRequest\n    | RerankRequest\n    | ClassificationCompletionRequest\n    | ScoreRequest\n    | TokenizeCompletionRequest\n)",
      "language": "typescript"
    },
    {
      "code": "RequestT = TypeVar('RequestT', bound=AnyRequest)",
      "language": "unknown"
    },
    {
      "code": "RequestT = TypeVar('RequestT', bound=AnyRequest)",
      "language": "unknown"
    },
    {
      "code": "SpeechToTextRequest: TypeAlias = (\n    TranscriptionRequest | TranslationRequest\n)",
      "language": "typescript"
    },
    {
      "code": "SpeechToTextRequest: TypeAlias = (\n    TranscriptionRequest | TranslationRequest\n)",
      "language": "typescript"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "217\n218\n219",
      "language": "unknown"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass ClassificationServeContext(ServeContext[ClassificationRequest]):\n    pass",
      "language": "php"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass ClassificationServeContext(ServeContext[ClassificationRequest]):\n    pass",
      "language": "php"
    },
    {
      "code": "__init__(\n    *,\n    result_generator: AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None = None,\n    final_res_batch: list[\n        RequestOutput | PoolingRequestOutput\n    ] = list(),\n    engine_prompts: list[TokensPrompt] | None = list(),\n    request: RequestT,\n    raw_request: Request | None = None,\n    model_name: str,\n    request_id: str,\n    created_time: int = (lambda: int(time()))(),\n    lora_request: LoRARequest | None = None,\n    tokenizer: TokenizerLike | None = None,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    *,\n    result_generator: AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None = None,\n    final_res_batch: list[\n        RequestOutput | PoolingRequestOutput\n    ] = list(),\n    engine_prompts: list[TokensPrompt] | None = list(),\n    request: RequestT,\n    raw_request: Request | None = None,\n    model_name: str,\n    request_id: str,\n    created_time: int = (lambda: int(time()))(),\n    lora_request: LoRARequest | None = None,\n    tokenizer: TokenizerLike | None = None,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "222\n223\n224\n225",
      "language": "unknown"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass EmbeddingServeContext(ServeContext[EmbeddingRequest]):\n    chat_template: str | None = None\n    chat_template_content_format: ChatTemplateContentFormatOption",
      "language": "php"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass EmbeddingServeContext(ServeContext[EmbeddingRequest]):\n    chat_template: str | None = None\n    chat_template_content_format: ChatTemplateContentFormatOption",
      "language": "php"
    },
    {
      "code": "chat_template: str | None = None",
      "language": "yaml"
    },
    {
      "code": "chat_template: str | None = None",
      "language": "yaml"
    },
    {
      "code": "chat_template_content_format: (\n    ChatTemplateContentFormatOption\n)",
      "language": "yaml"
    },
    {
      "code": "chat_template_content_format: (\n    ChatTemplateContentFormatOption\n)",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    *,\n    result_generator: AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None = None,\n    final_res_batch: list[\n        RequestOutput | PoolingRequestOutput\n    ] = list(),\n    engine_prompts: list[TokensPrompt] | None = list(),\n    request: RequestT,\n    raw_request: Request | None = None,\n    model_name: str,\n    request_id: str,\n    created_time: int = (lambda: int(time()))(),\n    lora_request: LoRARequest | None = None,\n    tokenizer: TokenizerLike | None = None,\n    chat_template: str | None = None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    *,\n    result_generator: AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None = None,\n    final_res_batch: list[\n        RequestOutput | PoolingRequestOutput\n    ] = list(),\n    engine_prompts: list[TokensPrompt] | None = list(),\n    request: RequestT,\n    raw_request: Request | None = None,\n    model_name: str,\n    request_id: str,\n    created_time: int = (lambda: int(time()))(),\n    lora_request: LoRARequest | None = None,\n    tokenizer: TokenizerLike | None = None,\n    chat_template: str | None = None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "124\n125\n126\n127\n128\n129",
      "language": "unknown"
    },
    {
      "code": "class GenerationError(Exception):\n    \"\"\"raised when finish_reason indicates internal server error (500)\"\"\"\n\n    def __init__(self, message: str = \"Internal server error\"):\n        super().__init__(message)\n        self.status_code = HTTPStatus.INTERNAL_SERVER_ERROR",
      "language": "python"
    },
    {
      "code": "class GenerationError(Exception):\n    \"\"\"raised when finish_reason indicates internal server error (500)\"\"\"\n\n    def __init__(self, message: str = \"Internal server error\"):\n        super().__init__(message)\n        self.status_code = HTTPStatus.INTERNAL_SERVER_ERROR",
      "language": "python"
    },
    {
      "code": "status_code = INTERNAL_SERVER_ERROR",
      "language": "unknown"
    },
    {
      "code": "status_code = INTERNAL_SERVER_ERROR",
      "language": "unknown"
    },
    {
      "code": "__init__(message: str = 'Internal server error')",
      "language": "typescript"
    },
    {
      "code": "__init__(message: str = 'Internal server error')",
      "language": "typescript"
    },
    {
      "code": "127\n128\n129",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, message: str = \"Internal server error\"):\n    super().__init__(message)\n    self.status_code = HTTPStatus.INTERNAL_SERVER_ERROR",
      "language": "python"
    },
    {
      "code": "def __init__(self, message: str = \"Internal server error\"):\n    super().__init__(message)\n    self.status_code = HTTPStatus.INTERNAL_SERVER_ERROR",
      "language": "python"
    },
    {
      "code": "228\n 229\n 230\n 231\n 232\n 233\n 234\n 235\n 236\n 237\n 238\n 239\n 240\n 241\n 242\n 243\n 244\n 245\n 246\n 247\n 248\n 249\n 250\n 251\n 252\n 253\n 254\n 255\n 256\n 257\n 258\n 259\n 260\n 261\n 262\n 263\n 264\n 265\n 266\n 267\n 268\n 269\n 270\n 271\n 272\n 273\n 274\n 275\n 276\n 277\n 278\n 279\n 280\n 281\n 282\n 283\n 284\n 285\n 286\n 287\n 288\n 289\n 290\n 291\n 292\n 293\n 294\n 295\n 296\n 297\n 298\n 299\n 300\n 301\n 302\n 303\n 304\n 305\n 306\n 307\n 308\n 309\n 310\n 311\n 312\n 313\n 314\n 315\n 316\n 317\n 318\n 319\n 320\n 321\n 322\n 323\n 324\n 325\n 326\n 327\n 328\n 329\n 330\n 331\n 332\n 333\n 334\n 335\n 336\n 337\n 338\n 339\n 340\n 341\n 342\n 343\n 344\n 345\n 346\n 347\n 348\n 349\n 350\n 351\n 352\n 353\n 354\n 355\n 356\n 357\n 358\n 359\n 360\n 361\n 362\n 363\n 364\n 365\n 366\n 367\n 368\n 369\n 370\n 371\n 372\n 373\n 374\n 375\n 376\n 377\n 378\n 379\n 380\n 381\n 382\n 383\n 384\n 385\n 386\n 387\n 388\n 389\n 390\n 391\n 392\n 393\n 394\n 395\n 396\n 397\n 398\n 399\n 400\n 401\n 402\n 403\n 404\n 405\n 406\n 407\n 408\n 409\n 410\n 411\n 412\n 413\n 414\n 415\n 416\n 417\n 418\n 419\n 420\n 421\n 422\n 423\n 424\n 425\n 426\n 427\n 428\n 429\n 430\n 431\n 432\n 433\n 434\n 435\n 436\n 437\n 438\n 439\n 440\n 441\n 442\n 443\n 444\n 445\n 446\n 447\n 448\n 449\n 450\n 451\n 452\n 453\n 454\n 455\n 456\n 457\n 458\n 459\n 460\n 461\n 462\n 463\n 464\n 465\n 466\n 467\n 468\n 469\n 470\n 471\n 472\n 473\n 474\n 475\n 476\n 477\n 478\n 479\n 480\n 481\n 482\n 483\n 484\n 485\n 486\n 487\n 488\n 489\n 490\n 491\n 492\n 493\n 494\n 495\n 496\n 497\n 498\n 499\n 500\n 501\n 502\n 503\n 504\n 505\n 506\n 507\n 508\n 509\n 510\n 511\n 512\n 513\n 514\n 515\n 516\n 517\n 518\n 519\n 520\n 521\n 522\n 523\n 524\n 525\n 526\n 527\n 528\n 529\n 530\n 531\n 532\n 533\n 534\n 535\n 536\n 537\n 538\n 539\n 540\n 541\n 542\n 543\n 544\n 545\n 546\n 547\n 548\n 549\n 550\n 551\n 552\n 553\n 554\n 555\n 556\n 557\n 558\n 559\n 560\n 561\n 562\n 563\n 564\n 565\n 566\n 567\n 568\n 569\n 570\n 571\n 572\n 573\n 574\n 575\n 576\n 577\n 578\n 579\n 580\n 581\n 582\n 583\n 584\n 585\n 586\n 587\n 588\n 589\n 590\n 591\n 592\n 593\n 594\n 595\n 596\n 597\n 598\n 599\n 600\n 601\n 602\n 603\n 604\n 605\n 606\n 607\n 608\n 609\n 610\n 611\n 612\n 613\n 614\n 615\n 616\n 617\n 618\n 619\n 620\n 621\n 622\n 623\n 624\n 625\n 626\n 627\n 628\n 629\n 630\n 631\n 632\n 633\n 634\n 635\n 636\n 637\n 638\n 639\n 640\n 641\n 642\n 643\n 644\n 645\n 646\n 647\n 648\n 649\n 650\n 651\n 652\n 653\n 654\n 655\n 656\n 657\n 658\n 659\n 660\n 661\n 662\n 663\n 664\n 665\n 666\n 667\n 668\n 669\n 670\n 671\n 672\n 673\n 674\n 675\n 676\n 677\n 678\n 679\n 680\n 681\n 682\n 683\n 684\n 685\n 686\n 687\n 688\n 689\n 690\n 691\n 692\n 693\n 694\n 695\n 696\n 697\n 698\n 699\n 700\n 701\n 702\n 703\n 704\n 705\n 706\n 707\n 708\n 709\n 710\n 711\n 712\n 713\n 714\n 715\n 716\n 717\n 718\n 719\n 720\n 721\n 722\n 723\n 724\n 725\n 726\n 727\n 728\n 729\n 730\n 731\n 732\n 733\n 734\n 735\n 736\n 737\n 738\n 739\n 740\n 741\n 742\n 743\n 744\n 745\n 746\n 747\n 748\n 749\n 750\n 751\n 752\n 753\n 754\n 755\n 756\n 757\n 758\n 759\n 760\n 761\n 762\n 763\n 764\n 765\n 766\n 767\n 768\n 769\n 770\n 771\n 772\n 773\n 774\n 775\n 776\n 777\n 778\n 779\n 780\n 781\n 782\n 783\n 784\n 785\n 786\n 787\n 788\n 789\n 790\n 791\n 792\n 793\n 794\n 795\n 796\n 797\n 798\n 799\n 800\n 801\n 802\n 803\n 804\n 805\n 806\n 807\n 808\n 809\n 810\n 811\n 812\n 813\n 814\n 815\n 816\n 817\n 818\n 819\n 820\n 821\n 822\n 823\n 824\n 825\n 826\n 827\n 828\n 829\n 830\n 831\n 832\n 833\n 834\n 835\n 836\n 837\n 838\n 839\n 840\n 841\n 842\n 843\n 844\n 845\n 846\n 847\n 848\n 849\n 850\n 851\n 852\n 853\n 854\n 855\n 856\n 857\n 858\n 859\n 860\n 861\n 862\n 863\n 864\n 865\n 866\n 867\n 868\n 869\n 870\n 871\n 872\n 873\n 874\n 875\n 876\n 877\n 878\n 879\n 880\n 881\n 882\n 883\n 884\n 885\n 886\n 887\n 888\n 889\n 890\n 891\n 892\n 893\n 894\n 895\n 896\n 897\n 898\n 899\n 900\n 901\n 902\n 903\n 904\n 905\n 906\n 907\n 908\n 909\n 910\n 911\n 912\n 913\n 914\n 915\n 916\n 917\n 918\n 919\n 920\n 921\n 922\n 923\n 924\n 925\n 926\n 927\n 928\n 929\n 930\n 931\n 932\n 933\n 934\n 935\n 936\n 937\n 938\n 939\n 940\n 941\n 942\n 943\n 944\n 945\n 946\n 947\n 948\n 949\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406\n1407\n1408\n1409\n1410\n1411\n1412\n1413\n1414\n1415\n1416\n1417\n1418\n1419\n1420\n1421\n1422\n1423\n1424\n1425\n1426\n1427\n1428\n1429\n1430\n1431\n1432\n1433\n1434\n1435\n1436\n1437\n1438\n1439\n1440\n1441\n1442\n1443\n1444\n1445\n1446\n1447\n1448\n1449\n1450\n1451\n1452\n1453\n1454\n1455\n1456\n1457\n1458\n1459\n1460\n1461\n1462\n1463\n1464\n1465\n1466\n1467\n1468\n1469\n1470\n1471\n1472\n1473\n1474\n1475\n1476\n1477\n1478\n1479\n1480\n1481\n1482\n1483\n1484\n1485\n1486\n1487\n1488\n1489\n1490\n1491\n1492\n1493\n1494\n1495\n1496\n1497\n1498\n1499\n1500\n1501\n1502\n1503\n1504\n1505\n1506\n1507\n1508\n1509\n1510\n1511\n1512\n1513\n1514\n1515\n1516\n1517\n1518\n1519\n1520\n1521\n1522\n1523\n1524\n1525\n1526\n1527\n1528\n1529\n1530\n1531\n1532\n1533\n1534\n1535\n1536\n1537\n1538\n1539\n1540\n1541\n1542\n1543\n1544\n1545\n1546\n1547\n1548\n1549\n1550\n1551\n1552\n1553\n1554\n1555\n1556\n1557\n1558\n1559\n1560\n1561\n1562\n1563\n1564\n1565\n1566\n1567\n1568\n1569\n1570\n1571\n1572\n1573\n1574\n1575\n1576",
      "language": "unknown"
    },
    {
      "code": "class OpenAIServing:\n    request_id_prefix: ClassVar[str] = \"\"\"\n    A short string prepended to every request’s ID (e.g. \"embd\", \"classify\")\n    so you can easily tell “this ID came from Embedding vs Classification.”\n    \"\"\"\n\n    def __init__(\n        self,\n        engine_client: EngineClient,\n        models: OpenAIServingModels,\n        *,\n        request_logger: RequestLogger | None,\n        return_tokens_as_token_ids: bool = False,\n        log_error_stack: bool = False,\n    ):\n        super().__init__()\n\n        self.engine_client = engine_client\n\n        self.models = models\n\n        self.request_logger = request_logger\n        self.return_tokens_as_token_ids = return_tokens_as_token_ids\n        self._tokenizer_executor = ThreadPoolExecutor(max_workers=1)\n        self._apply_mistral_chat_template_async = make_async(\n            apply_mistral_chat_template, executor=self._tokenizer_executor\n        )\n\n        self._async_tokenizer_pool: dict[TokenizerLike, AsyncMicrobatchTokenizer] = {}\n        self.log_error_stack = log_error_stack\n\n        self.input_processor = self.models.input_processor\n        self.io_processor = self.models.io_processor\n        self.model_config = self.models.model_config\n        self.max_model_len = self.model_config.max_model_len\n\n    def _get_tool_parser(\n        self, tool_parser_name: str | None = None, enable_auto_tools: bool = False\n    ) -> Callable[[TokenizerLike], ToolParser] | None:\n        \"\"\"Get the tool parser based on the name.\"\"\"\n        parser = None\n        if not enable_auto_tools or tool_parser_name is None:\n            return parser\n        logger.info('\"auto\" tool choice has been enabled.')\n\n        try:\n            if tool_parser_name == \"pythonic\" and self.model_config.model.startswith(\n                \"meta-llama/Llama-3.2\"\n            ):\n                logger.warning(\n                    \"Llama3.2 models may struggle to emit valid pythonic tool calls\"\n                )\n            parser = ToolParserManager.get_tool_parser(tool_parser_name)\n        except Exception as e:\n            raise TypeError(\n                \"Error: --enable-auto-tool-choice requires \"\n                f\"tool_parser:'{tool_parser_name}' which has not \"\n                \"been registered\"\n            ) from e\n        return parser\n\n    def _get_reasoning_parser(\n        self,\n        reasoning_parser_name: str,\n    ) -> Callable[[TokenizerLike], ReasoningParser] | None:\n        \"\"\"Get the reasoning parser based on the name.\"\"\"\n        parser = None\n        if not reasoning_parser_name:\n            return None\n        try:\n            parser = ReasoningParserManager.get_reasoning_parser(reasoning_parser_name)\n            assert parser is not None\n        except Exception as e:\n            raise TypeError(f\"{reasoning_parser_name=} has not been registered\") from e\n        return parser\n\n    async def reset_mm_cache(self) -> None:\n        self.input_processor.clear_mm_cache()\n        await self.engine_client.reset_mm_cache()\n\n    async def beam_search(\n        self,\n        prompt: PromptType,\n        request_id: str,\n        params: BeamSearchParams,\n        lora_request: LoRARequest | None = None,\n        trace_headers: Mapping[str, str] | None = None,\n    ) -> AsyncGenerator[RequestOutput, None]:\n        beam_width = params.beam_width\n        max_tokens = params.max_tokens\n        ignore_eos = params.ignore_eos\n        temperature = params.temperature\n        length_penalty = params.length_penalty\n        include_stop_str_in_output = params.include_stop_str_in_output\n\n        input_processor = self.input_processor\n        tokenizer = input_processor.tokenizer\n        if tokenizer is None:\n            raise VLLMValidationError(\n                \"You cannot use beam search when `skip_tokenizer_init=True`\",\n                parameter=\"skip_tokenizer_init\",\n                value=True,\n            )\n\n        eos_token_id: int = tokenizer.eos_token_id  # type: ignore\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            raise NotImplementedError\n\n        prompt_text: str | None\n        prompt_token_ids: list[int]\n        multi_modal_data: MultiModalDataDict | None\n        if isinstance(prompt, str):\n            prompt_text = prompt\n            prompt_token_ids = []\n            multi_modal_data = None\n        else:\n            prompt_text = prompt.get(\"prompt\")  # type: ignore\n            prompt_token_ids = prompt.get(\"prompt_token_ids\", [])  # type: ignore\n            multi_modal_data = prompt.get(\"multi_modal_data\")  # type: ignore\n\n        mm_processor_kwargs: dict[str, Any] | None = None\n\n        # This is a workaround to fix multimodal beam search; this is a\n        # bandaid fix for 2 small problems:\n        # 1. Multi_modal_data on the processed_inputs currently resolves to\n        #    `None`.\n        # 2. preprocessing above expands the multimodal placeholders. However,\n        #    this happens again in generation, so the double expansion causes\n        #    a mismatch.\n        # TODO - would be ideal to handle this more gracefully.\n\n        tokenized_length = len(prompt_token_ids)\n\n        sort_beams_key = create_sort_beams_key_function(eos_token_id, length_penalty)\n\n        logprobs_num = 2 * beam_width\n        beam_search_params = SamplingParams(\n            logprobs=logprobs_num,\n            max_tokens=1,\n            temperature=temperature,\n        )\n        all_beams = [\n            BeamSearchSequence(\n                tokens=prompt_token_ids,\n                cum_logprob=0,\n                logprobs=[],\n                multi_modal_data=multi_modal_data,\n                mm_processor_kwargs=mm_processor_kwargs,\n                lora_request=lora_request,\n            )\n        ]\n        completed = []\n\n        for _ in range(max_tokens):\n            prompts_batch, lora_req_batch = zip(\n                *[\n                    (\n                        TokensPrompt(\n                            prompt_token_ids=beam.tokens,\n                            multi_modal_data=beam.multi_modal_data,\n                            mm_processor_kwargs=beam.mm_processor_kwargs,\n                        ),\n                        beam.lora_request,\n                    )\n                    for beam in all_beams\n                ]\n            )\n\n            tasks = []\n            request_id_batch = f\"{request_id}-{random_uuid()}\"\n\n            for i, (individual_prompt, lora_req) in enumerate(\n                zip(prompts_batch, lora_req_batch)\n            ):\n                request_id_item = f\"{request_id_batch}-beam-{i}\"\n                task = asyncio.create_task(\n                    collect_from_async_generator(\n                        self.engine_client.generate(\n                            individual_prompt,\n                            beam_search_params,\n                            request_id_item,\n                            lora_request=lora_req,\n                            trace_headers=trace_headers,\n                        )\n                    )\n                )\n                tasks.append(task)\n\n            output = [x[0] for x in await asyncio.gather(*tasks)]\n\n            new_beams = []\n            # Store all new tokens generated by beam\n            all_beams_token_id = []\n            # Store the cumulative probability of all tokens\n            # generated by beam search\n            all_beams_logprob = []\n            # Iterate through all beam inference results\n            for i, result in enumerate(output):\n                current_beam = all_beams[i]\n\n                # check for error finish reason and abort beam search\n                if result.outputs[0].finish_reason == \"error\":\n                    # yield error output and terminate beam search\n                    yield RequestOutput(\n                        request_id=request_id,\n                        prompt=prompt_text,\n                        outputs=[\n                            CompletionOutput(\n                                index=0,\n                                text=\"\",\n                                token_ids=[],\n                                cumulative_logprob=None,\n                                logprobs=None,\n                                finish_reason=\"error\",\n                            )\n                        ],\n                        finished=True,\n                        prompt_token_ids=prompt_token_ids,\n                        prompt_logprobs=None,\n                    )\n                    return\n\n                if result.outputs[0].logprobs is not None:\n                    logprobs = result.outputs[0].logprobs[0]\n                    all_beams_token_id.extend(list(logprobs.keys()))\n                    all_beams_logprob.extend(\n                        [\n                            current_beam.cum_logprob + obj.logprob\n                            for obj in logprobs.values()\n                        ]\n                    )\n\n            # Handle the token for the end of sentence (EOS)\n            all_beams_token_id = np.array(all_beams_token_id)\n            all_beams_logprob = np.array(all_beams_logprob)\n\n            if not ignore_eos:\n                # Get the index position of eos token in all generated results\n                eos_idx = np.where(all_beams_token_id == eos_token_id)[0]\n                for idx in eos_idx:\n                    current_beam = all_beams[idx // logprobs_num]\n                    result = output[idx // logprobs_num]\n                    assert result.outputs[0].logprobs is not None\n                    logprobs_entry = result.outputs[0].logprobs[0]\n                    completed.append(\n                        BeamSearchSequence(\n                            tokens=current_beam.tokens + [eos_token_id]\n                            if include_stop_str_in_output\n                            else current_beam.tokens,\n                            logprobs=current_beam.logprobs + [logprobs_entry],\n                            cum_logprob=float(all_beams_logprob[idx]),\n                            finish_reason=\"stop\",\n                            stop_reason=eos_token_id,\n                        )\n                    )\n                # After processing, set the log probability of the eos condition\n                # to negative infinity.\n                all_beams_logprob[eos_idx] = -np.inf\n\n            # Processing non-EOS tokens\n            # Get indices of the top beam_width probabilities\n            topn_idx = np.argpartition(np.negative(all_beams_logprob), beam_width)[\n                :beam_width\n            ]\n\n            for idx in topn_idx:\n                current_beam = all_beams[idx // logprobs_num]\n                result = output[idx // logprobs_num]\n                token_id = int(all_beams_token_id[idx])\n                assert result.outputs[0].logprobs is not None\n                logprobs_entry = result.outputs[0].logprobs[0]\n                new_beams.append(\n                    BeamSearchSequence(\n                        tokens=current_beam.tokens + [token_id],\n                        logprobs=current_beam.logprobs + [logprobs_entry],\n                        lora_request=current_beam.lora_request,\n                        cum_logprob=float(all_beams_logprob[idx]),\n                        multi_modal_data=current_beam.multi_modal_data,\n                        mm_processor_kwargs=current_beam.mm_processor_kwargs,\n                    )\n                )\n\n            all_beams = new_beams\n\n        completed.extend(all_beams)\n        sorted_completed = sorted(completed, key=sort_beams_key, reverse=True)\n        best_beams = sorted_completed[:beam_width]\n\n        for beam in best_beams:\n            if beam.tokens[-1] == eos_token_id and not ignore_eos:\n                # Skip the eos token in the text.\n                tokens = beam.tokens[tokenized_length:-1]\n            else:\n                tokens = beam.tokens[tokenized_length:]\n            beam.text = tokenizer.decode(tokens)\n\n        yield RequestOutput(\n            request_id=request_id,\n            prompt=prompt_text,\n            outputs=[\n                CompletionOutput(\n                    text=beam.text,  # type: ignore\n                    cumulative_logprob=beam.cum_logprob,\n                    token_ids=beam.tokens[tokenized_length:],\n                    index=i,\n                    logprobs=beam.logprobs,\n                    finish_reason=beam.finish_reason\n                    if beam.finish_reason is not None\n                    else \"length\",\n                    stop_reason=beam.stop_reason,\n                )\n                for (i, beam) in enumerate(best_beams)\n            ],\n            finished=True,\n            prompt_token_ids=prompt_token_ids,\n            prompt_logprobs=None,\n        )\n\n    def _get_renderer(self, tokenizer: TokenizerLike | None) -> BaseRenderer:\n        \"\"\"\n        Get a Renderer instance with the provided tokenizer.\n        Uses shared async tokenizer pool for efficiency.\n        \"\"\"\n        return CompletionRenderer(\n            model_config=self.model_config,\n            tokenizer=tokenizer,\n            async_tokenizer_pool=self._async_tokenizer_pool,\n        )\n\n    def _build_render_config(\n        self,\n        request: Any,\n    ) -> RenderConfig:\n        \"\"\"\n        Build and return a `RenderConfig` for an endpoint.\n\n        Used by the renderer to control how prompts are prepared\n        (e.g., tokenization and length handling). Endpoints should\n        implement this with logic appropriate to their request type.\n        \"\"\"\n        raise NotImplementedError\n\n    def _get_async_tokenizer(self, tokenizer) -> AsyncMicrobatchTokenizer:\n        \"\"\"\n        Return (and cache) an `AsyncMicrobatchTokenizer` bound to the\n        given tokenizer.\n        \"\"\"\n        async_tokenizer = self._async_tokenizer_pool.get(tokenizer)\n        if async_tokenizer is None:\n            async_tokenizer = AsyncMicrobatchTokenizer(tokenizer)\n            self._async_tokenizer_pool[tokenizer] = async_tokenizer\n        return async_tokenizer\n\n    async def _preprocess(\n        self,\n        ctx: ServeContext,\n    ) -> ErrorResponse | None:\n        \"\"\"\n        Default preprocessing hook. Subclasses may override\n        to prepare `ctx` (classification, embedding, etc.).\n        \"\"\"\n        return None\n\n    def _build_response(\n        self,\n        ctx: ServeContext,\n    ) -> AnyResponse | ErrorResponse:\n        \"\"\"\n        Default response builder. Subclass may override this method\n        to return the appropriate response object.\n        \"\"\"\n        return self.create_error_response(\"unimplemented endpoint\")\n\n    async def handle(\n        self,\n        ctx: ServeContext,\n    ) -> AnyResponse | ErrorResponse:\n        generation: AsyncGenerator[AnyResponse | ErrorResponse, None]\n        generation = self._pipeline(ctx)\n\n        async for response in generation:\n            return response\n\n        return self.create_error_response(\"No response yielded from pipeline\")\n\n    async def _pipeline(\n        self,\n        ctx: ServeContext,\n    ) -> AsyncGenerator[AnyResponse | ErrorResponse, None]:\n        \"\"\"Execute the request processing pipeline yielding responses.\"\"\"\n        if error := await self._check_model(ctx.request):\n            yield error\n        if error := self._validate_request(ctx):\n            yield error\n\n        preprocess_ret = await self._preprocess(ctx)\n        if isinstance(preprocess_ret, ErrorResponse):\n            yield preprocess_ret\n\n        generators_ret = await self._prepare_generators(ctx)\n        if isinstance(generators_ret, ErrorResponse):\n            yield generators_ret\n\n        collect_ret = await self._collect_batch(ctx)\n        if isinstance(collect_ret, ErrorResponse):\n            yield collect_ret\n\n        yield self._build_response(ctx)\n\n    def _validate_request(self, ctx: ServeContext) -> ErrorResponse | None:\n        truncate_prompt_tokens = getattr(ctx.request, \"truncate_prompt_tokens\", None)\n\n        if (\n            truncate_prompt_tokens is not None\n            and truncate_prompt_tokens > self.max_model_len\n        ):\n            return self.create_error_response(\n                \"truncate_prompt_tokens value is \"\n                \"greater than max_model_len.\"\n                \" Please, select a smaller truncation size.\"\n            )\n        return None\n\n    def _create_pooling_params(\n        self,\n        ctx: ServeContext,\n    ) -> PoolingParams | ErrorResponse:\n        if not hasattr(ctx.request, \"to_pooling_params\"):\n            return self.create_error_response(\n                \"Request type does not support pooling parameters\"\n            )\n\n        return ctx.request.to_pooling_params()\n\n    async def _prepare_generators(\n        self,\n        ctx: ServeContext,\n    ) -> ErrorResponse | None:\n        \"\"\"Schedule the request and get the result generator.\"\"\"\n        generators: list[\n            AsyncGenerator[RequestOutput | PoolingRequestOutput, None]\n        ] = []\n\n        try:\n            trace_headers = (\n                None\n                if ctx.raw_request is None\n                else await self._get_trace_headers(ctx.raw_request.headers)\n            )\n\n            pooling_params = self._create_pooling_params(ctx)\n            if isinstance(pooling_params, ErrorResponse):\n                return pooling_params\n\n            if ctx.engine_prompts is None:\n                return self.create_error_response(\"Engine prompts not available\")\n\n            for i, engine_prompt in enumerate(ctx.engine_prompts):\n                request_id_item = f\"{ctx.request_id}-{i}\"\n\n                self._log_inputs(\n                    request_id_item,\n                    engine_prompt,\n                    params=pooling_params,\n                    lora_request=ctx.lora_request,\n                )\n\n                generator = self.engine_client.encode(\n                    engine_prompt,\n                    pooling_params,\n                    request_id_item,\n                    lora_request=ctx.lora_request,\n                    trace_headers=trace_headers,\n                    priority=getattr(ctx.request, \"priority\", 0),\n                )\n\n                generators.append(generator)\n\n            ctx.result_generator = merge_async_iterators(*generators)\n\n            return None\n\n        except Exception as e:\n            return self.create_error_response(e)\n\n    async def _collect_batch(\n        self,\n        ctx: ServeContext,\n    ) -> ErrorResponse | None:\n        \"\"\"Collect batch results from the result generator.\"\"\"\n        try:\n            if ctx.engine_prompts is None:\n                return self.create_error_response(\"Engine prompts not available\")\n\n            num_prompts = len(ctx.engine_prompts)\n            final_res_batch: list[RequestOutput | PoolingRequestOutput | None]\n            final_res_batch = [None] * num_prompts\n\n            if ctx.result_generator is None:\n                return self.create_error_response(\"Result generator not available\")\n\n            async for i, res in ctx.result_generator:\n                final_res_batch[i] = res\n\n            if None in final_res_batch:\n                return self.create_error_response(\n                    \"Failed to generate results for all prompts\"\n                )\n\n            ctx.final_res_batch = [res for res in final_res_batch if res is not None]\n\n            return None\n\n        except Exception as e:\n            return self.create_error_response(e)\n\n    def create_error_response(\n        self,\n        message: str | Exception,\n        err_type: str = \"BadRequestError\",\n        status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,\n        param: str | None = None,\n    ) -> ErrorResponse:\n        exc: Exception | None = None\n\n        if isinstance(message, Exception):\n            exc = message\n\n            from vllm.entrypoints.openai.protocol import VLLMValidationError\n\n            if isinstance(exc, VLLMValidationError):\n                err_type = \"BadRequestError\"\n                status_code = HTTPStatus.BAD_REQUEST\n                param = exc.parameter\n            elif isinstance(exc, (ValueError, TypeError, RuntimeError)):\n                # Common validation errors from user input\n                err_type = \"BadRequestError\"\n                status_code = HTTPStatus.BAD_REQUEST\n                param = None\n            elif exc.__class__.__name__ == \"TemplateError\":\n                # jinja2.TemplateError (avoid importing jinja2)\n                err_type = \"BadRequestError\"\n                status_code = HTTPStatus.BAD_REQUEST\n                param = None\n            else:\n                err_type = \"InternalServerError\"\n                status_code = HTTPStatus.INTERNAL_SERVER_ERROR\n                param = None\n\n            message = str(exc)\n\n        if self.log_error_stack:\n            exc_type, _, _ = sys.exc_info()\n            if exc_type is not None:\n                traceback.print_exc()\n            else:\n                traceback.print_stack()\n        return ErrorResponse(\n            error=ErrorInfo(\n                message=message,\n                type=err_type,\n                code=status_code.value,\n                param=param,\n            )\n        )\n\n    def create_streaming_error_response(\n        self,\n        message: str | Exception,\n        err_type: str = \"BadRequestError\",\n        status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,\n        param: str | None = None,\n    ) -> str:\n        json_str = json.dumps(\n            self.create_error_response(\n                message=message,\n                err_type=err_type,\n                status_code=status_code,\n                param=param,\n            ).model_dump()\n        )\n        return json_str\n\n    def _raise_if_error(self, finish_reason: str | None, request_id: str) -> None:\n        \"\"\"Raise GenerationError if finish_reason indicates an error.\"\"\"\n        if finish_reason == \"error\":\n            logger.error(\n                \"Request %s failed with an internal error during generation\",\n                request_id,\n            )\n            raise GenerationError(\"Internal server error\")\n\n    def _convert_generation_error_to_response(\n        self, e: GenerationError\n    ) -> ErrorResponse:\n        \"\"\"Convert GenerationError to ErrorResponse.\"\"\"\n        return self.create_error_response(\n            str(e),\n            err_type=\"InternalServerError\",\n            status_code=e.status_code,\n        )\n\n    def _convert_generation_error_to_streaming_response(\n        self, e: GenerationError\n    ) -> str:\n        \"\"\"Convert GenerationError to streaming error response.\"\"\"\n        return self.create_streaming_error_response(\n            str(e),\n            err_type=\"InternalServerError\",\n            status_code=e.status_code,\n        )\n\n    async def _check_model(\n        self,\n        request: AnyRequest,\n    ) -> ErrorResponse | None:\n        error_response = None\n\n        if self._is_model_supported(request.model):\n            return None\n        if request.model in self.models.lora_requests:\n            return None\n        if (\n            envs.VLLM_ALLOW_RUNTIME_LORA_UPDATING\n            and request.model\n            and (load_result := await self.models.resolve_lora(request.model))\n        ):\n            if isinstance(load_result, LoRARequest):\n                return None\n            if (\n                isinstance(load_result, ErrorResponse)\n                and load_result.error.code == HTTPStatus.BAD_REQUEST.value\n            ):\n                error_response = load_result\n\n        return error_response or self.create_error_response(\n            message=f\"The model `{request.model}` does not exist.\",\n            err_type=\"NotFoundError\",\n            status_code=HTTPStatus.NOT_FOUND,\n            param=\"model\",\n        )\n\n    def _get_active_default_mm_loras(self, request: AnyRequest) -> LoRARequest | None:\n        \"\"\"Determine if there are any active default multimodal loras.\"\"\"\n        # TODO: Currently this is only enabled for chat completions\n        # to be better aligned with only being enabled for .generate\n        # when run offline. It would be nice to support additional\n        # tasks types in the future.\n        message_types = self._get_message_types(request)\n        default_mm_loras = set()\n\n        for lora in self.models.lora_requests.values():\n            # Best effort match for default multimodal lora adapters;\n            # There is probably a better way to do this, but currently\n            # this matches against the set of 'types' in any content lists\n            # up until '_', e.g., to match audio_url -> audio\n            if lora.lora_name in message_types:\n                default_mm_loras.add(lora)\n\n        # Currently only support default modality specific loras if\n        # we have exactly one lora matched on the request.\n        if len(default_mm_loras) == 1:\n            return default_mm_loras.pop()\n        return None\n\n    def _maybe_get_adapters(\n        self,\n        request: AnyRequest,\n        supports_default_mm_loras: bool = False,\n    ) -> LoRARequest | None:\n        if request.model in self.models.lora_requests:\n            return self.models.lora_requests[request.model]\n\n        # Currently only support default modality specific loras\n        # if we have exactly one lora matched on the request.\n        if supports_default_mm_loras:\n            default_mm_lora = self._get_active_default_mm_loras(request)\n            if default_mm_lora is not None:\n                return default_mm_lora\n\n        if self._is_model_supported(request.model):\n            return None\n\n        # if _check_model has been called earlier, this will be unreachable\n        raise ValueError(f\"The model `{request.model}` does not exist.\")\n\n    def _get_message_types(self, request: AnyRequest) -> set[str]:\n        \"\"\"Retrieve the set of types from message content dicts up\n        until `_`; we use this to match potential multimodal data\n        with default per modality loras.\n        \"\"\"\n        message_types: set[str] = set()\n\n        if not hasattr(request, \"messages\"):\n            return message_types\n\n        messages = request.messages\n        if messages is None or isinstance(messages, (str, bytes)):\n            return message_types\n\n        for message in messages:\n            if (\n                isinstance(message, dict)\n                and \"content\" in message\n                and isinstance(message[\"content\"], list)\n            ):\n                for content_dict in message[\"content\"]:\n                    if \"type\" in content_dict:\n                        message_types.add(content_dict[\"type\"].split(\"_\")[0])\n        return message_types\n\n    async def _normalize_prompt_text_to_input(\n        self,\n        request: AnyRequest,\n        prompt: str,\n        tokenizer: TokenizerLike,\n        add_special_tokens: bool,\n    ) -> TokensPrompt:\n        async_tokenizer = self._get_async_tokenizer(tokenizer)\n\n        if (\n            self.model_config.encoder_config is not None\n            and self.model_config.encoder_config.get(\"do_lower_case\", False)\n        ):\n            prompt = prompt.lower()\n\n        truncate_prompt_tokens = getattr(request, \"truncate_prompt_tokens\", None)\n\n        if truncate_prompt_tokens is None:\n            encoded = await async_tokenizer(\n                prompt, add_special_tokens=add_special_tokens\n            )\n        elif truncate_prompt_tokens < 0:\n            # Negative means we cap at the model's max length\n            encoded = await async_tokenizer(\n                prompt,\n                add_special_tokens=add_special_tokens,\n                truncation=True,\n                max_length=self.max_model_len,\n            )\n        else:\n            encoded = await async_tokenizer(\n                prompt,\n                add_special_tokens=add_special_tokens,\n                truncation=True,\n                max_length=truncate_prompt_tokens,\n            )\n\n        input_ids = encoded.input_ids\n        input_text = prompt\n\n        return self._validate_input(request, input_ids, input_text)\n\n    async def _normalize_prompt_tokens_to_input(\n        self,\n        request: AnyRequest,\n        prompt_ids: list[int],\n        tokenizer: TokenizerLike | None,\n    ) -> TokensPrompt:\n        truncate_prompt_tokens = getattr(request, \"truncate_prompt_tokens\", None)\n\n        if truncate_prompt_tokens is None:\n            input_ids = prompt_ids\n        elif truncate_prompt_tokens < 0:\n            input_ids = prompt_ids[-self.max_model_len :]\n        else:\n            input_ids = prompt_ids[-truncate_prompt_tokens:]\n\n        if tokenizer is None:\n            input_text = \"\"\n        else:\n            async_tokenizer = self._get_async_tokenizer(tokenizer)\n            input_text = await async_tokenizer.decode(input_ids)\n\n        return self._validate_input(request, input_ids, input_text)\n\n    def _validate_input(\n        self,\n        request: AnyRequest,\n        input_ids: list[int],\n        input_text: str,\n    ) -> TokensPrompt:\n        token_num = len(input_ids)\n\n        # Note: EmbeddingRequest, ClassificationRequest,\n        # and ScoreRequest doesn't have max_tokens\n        if isinstance(\n            request,\n            (\n                EmbeddingChatRequest,\n                EmbeddingCompletionRequest,\n                ScoreRequest,\n                RerankRequest,\n                ClassificationCompletionRequest,\n                ClassificationChatRequest,\n            ),\n        ):\n            # Note: input length can be up to the entire model context length\n            # since these requests don't generate tokens.\n            if token_num > self.max_model_len:\n                operations: dict[type[AnyRequest], str] = {\n                    ScoreRequest: \"score\",\n                    ClassificationCompletionRequest: \"classification\",\n                    ClassificationChatRequest: \"classification\",\n                }\n                operation = operations.get(type(request), \"embedding generation\")\n                raise VLLMValidationError(\n                    f\"This model's maximum context length is \"\n                    f\"{self.max_model_len} tokens. However, you requested \"\n                    f\"{token_num} tokens in the input for {operation}. \"\n                    f\"Please reduce the length of the input.\",\n                    parameter=\"input_tokens\",\n                    value=token_num,\n                )\n            return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n        # Note: TokenizeRequest and DetokenizeRequest doesn't have max_tokens\n        # and does not require model context length validation\n        if isinstance(\n            request,\n            (TokenizeCompletionRequest, TokenizeChatRequest, DetokenizeRequest),\n        ):\n            return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n        # chat completion endpoint supports max_completion_tokens\n        if isinstance(request, ChatCompletionRequest):\n            # TODO(#9845): remove max_tokens when field dropped from OpenAI API\n            max_tokens = request.max_completion_tokens or request.max_tokens\n        else:\n            max_tokens = getattr(request, \"max_tokens\", None)\n\n        # Note: input length can be up to model context length - 1 for\n        # completion-like requests.\n        if token_num >= self.max_model_len:\n            raise VLLMValidationError(\n                f\"This model's maximum context length is \"\n                f\"{self.max_model_len} tokens. However, your request has \"\n                f\"{token_num} input tokens. Please reduce the length of \"\n                \"the input messages.\",\n                parameter=\"input_tokens\",\n                value=token_num,\n            )\n\n        if max_tokens is not None and token_num + max_tokens > self.max_model_len:\n            raise VLLMValidationError(\n                \"'max_tokens' or 'max_completion_tokens' is too large: \"\n                f\"{max_tokens}. This model's maximum context length is \"\n                f\"{self.max_model_len} tokens and your request has \"\n                f\"{token_num} input tokens ({max_tokens} > {self.max_model_len}\"\n                f\" - {token_num}).\",\n                parameter=\"max_tokens\",\n                value=max_tokens,\n            )\n\n        return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n    async def _tokenize_prompt_input_async(\n        self,\n        request: AnyRequest,\n        tokenizer: TokenizerLike,\n        prompt_input: str | list[int],\n        add_special_tokens: bool = True,\n    ) -> TokensPrompt:\n        \"\"\"\n        A simpler implementation that tokenizes a single prompt input.\n        \"\"\"\n        async for result in self._tokenize_prompt_inputs_async(\n            request,\n            tokenizer,\n            [prompt_input],\n            add_special_tokens=add_special_tokens,\n        ):\n            return result\n        raise ValueError(\"No results yielded from tokenization\")\n\n    async def _tokenize_prompt_inputs_async(\n        self,\n        request: AnyRequest,\n        tokenizer: TokenizerLike,\n        prompt_inputs: Iterable[str | list[int]],\n        add_special_tokens: bool = True,\n    ) -> AsyncGenerator[TokensPrompt, None]:\n        \"\"\"\n        A simpler implementation that tokenizes multiple prompt inputs.\n        \"\"\"\n        for prompt in prompt_inputs:\n            if isinstance(prompt, str):\n                yield await self._normalize_prompt_text_to_input(\n                    request,\n                    prompt=prompt,\n                    tokenizer=tokenizer,\n                    add_special_tokens=add_special_tokens,\n                )\n            else:\n                yield await self._normalize_prompt_tokens_to_input(\n                    request,\n                    prompt_ids=prompt,\n                    tokenizer=tokenizer,\n                )\n\n    def _validate_chat_template(\n        self,\n        request_chat_template: str | None,\n        chat_template_kwargs: dict[str, Any] | None,\n        trust_request_chat_template: bool,\n    ) -> ErrorResponse | None:\n        if not trust_request_chat_template and (\n            request_chat_template is not None\n            or (\n                chat_template_kwargs\n                and chat_template_kwargs.get(\"chat_template\") is not None\n            )\n        ):\n            return self.create_error_response(\n                \"Chat template is passed with request, but \"\n                \"--trust-request-chat-template is not set. \"\n                \"Refused request with untrusted chat template.\"\n            )\n        return None\n\n    async def _preprocess_chat(\n        self,\n        request: ChatLikeRequest | ResponsesRequest,\n        tokenizer: TokenizerLike | None,\n        messages: list[ChatCompletionMessageParam],\n        chat_template: str | None,\n        chat_template_content_format: ChatTemplateContentFormatOption,\n        add_generation_prompt: bool = True,\n        continue_final_message: bool = False,\n        tool_dicts: list[dict[str, Any]] | None = None,\n        documents: list[dict[str, str]] | None = None,\n        chat_template_kwargs: dict[str, Any] | None = None,\n        tool_parser: Callable[[TokenizerLike], ToolParser] | None = None,\n        add_special_tokens: bool = False,\n    ) -> tuple[list[ConversationMessage], list[TokensPrompt]]:\n        model_config = self.model_config\n\n        resolved_content_format = resolve_chat_template_content_format(\n            chat_template,\n            tool_dicts,\n            chat_template_content_format,\n            tokenizer,\n            model_config=model_config,\n        )\n        conversation, mm_data_future, mm_uuids = parse_chat_messages_futures(\n            messages,\n            model_config,\n            content_format=resolved_content_format,\n        )\n\n        _chat_template_kwargs: dict[str, Any] = dict(\n            chat_template=chat_template,\n            add_generation_prompt=add_generation_prompt,\n            continue_final_message=continue_final_message,\n            tools=tool_dicts,\n            documents=documents,\n        )\n        _chat_template_kwargs.update(chat_template_kwargs or {})\n\n        request_prompt: str | list[int]\n\n        if tokenizer is None:\n            request_prompt = \"placeholder\"\n        elif isinstance(tokenizer, MistralTokenizer):\n            request_prompt = await self._apply_mistral_chat_template_async(\n                tokenizer,\n                messages=messages,\n                **_chat_template_kwargs,\n            )\n        elif isinstance(tokenizer, DeepseekV32Tokenizer):\n            request_prompt = tokenizer.apply_chat_template(\n                conversation=conversation,\n                messages=messages,\n                model_config=model_config,\n                **_chat_template_kwargs,\n            )\n        else:\n            request_prompt = apply_hf_chat_template(\n                tokenizer=tokenizer,\n                conversation=conversation,\n                model_config=model_config,\n                **_chat_template_kwargs,\n            )\n\n        mm_data = await mm_data_future\n\n        # tool parsing is done only if a tool_parser has been set and if\n        # tool_choice is not \"none\" (if tool_choice is \"none\" but a tool_parser\n        # is set, we want to prevent parsing a tool_call hallucinated by the LLM\n        should_parse_tools = tool_parser is not None and (\n            hasattr(request, \"tool_choice\") and request.tool_choice != \"none\"\n        )\n\n        if should_parse_tools:\n            if not isinstance(request, ChatCompletionRequest | ResponsesRequest):\n                msg = (\n                    \"Tool usage is only supported for Chat Completions API \"\n                    \"or Responses API requests.\"\n                )\n                raise NotImplementedError(msg)\n            request = tool_parser(tokenizer).adjust_request(request=request)  # type: ignore\n\n        if tokenizer is None:\n            assert isinstance(request_prompt, str), (\n                \"Prompt has to be a string\",\n                \"when the tokenizer is not initialised\",\n            )\n            prompt_inputs = TokensPrompt(prompt=request_prompt, prompt_token_ids=[1])\n        elif isinstance(request_prompt, str):\n            prompt_inputs = await self._tokenize_prompt_input_async(\n                request,\n                tokenizer,\n                request_prompt,\n                add_special_tokens=add_special_tokens,\n            )\n        else:\n            # For MistralTokenizer\n            assert is_list_of(request_prompt, int), (\n                \"Prompt has to be either a string or a list of token ids\"\n            )\n            prompt_inputs = TokensPrompt(\n                prompt=tokenizer.decode(request_prompt),\n                prompt_token_ids=request_prompt,\n            )\n\n        engine_prompt = TokensPrompt(prompt_token_ids=prompt_inputs[\"prompt_token_ids\"])\n        if \"prompt\" in prompt_inputs:\n            engine_prompt[\"prompt\"] = prompt_inputs[\"prompt\"]\n\n        if mm_data is not None:\n            engine_prompt[\"multi_modal_data\"] = mm_data\n\n        if mm_uuids is not None:\n            engine_prompt[\"multi_modal_uuids\"] = mm_uuids\n\n        if request.mm_processor_kwargs is not None:\n            engine_prompt[\"mm_processor_kwargs\"] = request.mm_processor_kwargs\n\n        if hasattr(request, \"cache_salt\") and request.cache_salt is not None:\n            engine_prompt[\"cache_salt\"] = request.cache_salt\n\n        return conversation, [engine_prompt]\n\n    async def _process_inputs(\n        self,\n        request_id: str,\n        engine_prompt: PromptType,\n        params: SamplingParams | PoolingParams,\n        *,\n        lora_request: LoRARequest | None,\n        trace_headers: Mapping[str, str] | None,\n        priority: int,\n        data_parallel_rank: int | None = None,\n    ) -> tuple[EngineCoreRequest, dict[str, Any]]:\n        \"\"\"Use the Processor to process inputs for AsyncLLM.\"\"\"\n        tokenization_kwargs: dict[str, Any] = {}\n        _validate_truncation_size(\n            self.max_model_len, params.truncate_prompt_tokens, tokenization_kwargs\n        )\n\n        engine_request = self.input_processor.process_inputs(\n            request_id,\n            engine_prompt,\n            params,\n            lora_request=lora_request,\n            tokenization_kwargs=tokenization_kwargs,\n            trace_headers=trace_headers,\n            priority=priority,\n            data_parallel_rank=data_parallel_rank,\n        )\n        return engine_request, tokenization_kwargs\n\n    async def _render_next_turn(\n        self,\n        request: ResponsesRequest,\n        tokenizer: TokenizerLike | None,\n        messages: list[ResponseInputOutputItem],\n        tool_dicts: list[dict[str, Any]] | None,\n        tool_parser,\n        chat_template: str | None,\n        chat_template_content_format: ChatTemplateContentFormatOption,\n    ):\n        new_messages = construct_input_messages(\n            request_input=messages,\n        )\n\n        _, engine_prompts = await self._preprocess_chat(\n            request,\n            tokenizer,\n            new_messages,\n            tool_dicts=tool_dicts,\n            tool_parser=tool_parser,\n            chat_template=chat_template,\n            chat_template_content_format=chat_template_content_format,\n        )\n        return engine_prompts\n\n    async def _generate_with_builtin_tools(\n        self,\n        request_id: str,\n        engine_prompt: TokensPrompt,\n        sampling_params: SamplingParams,\n        context: ConversationContext,\n        lora_request: LoRARequest | None = None,\n        priority: int = 0,\n        **kwargs,\n    ):\n        prompt_text, _, _ = self._get_prompt_components(engine_prompt)\n\n        orig_priority = priority\n        sub_request = 0\n        while True:\n            # Ensure that each sub-request has a unique request id.\n            sub_request_id = f\"{request_id}_{sub_request}\"\n            self._log_inputs(\n                sub_request_id,\n                engine_prompt,\n                params=sampling_params,\n                lora_request=lora_request,\n            )\n            trace_headers = kwargs.get(\"trace_headers\")\n            engine_request, tokenization_kwargs = await self._process_inputs(\n                sub_request_id,\n                engine_prompt,\n                sampling_params,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                priority=priority,\n            )\n\n            generator = self.engine_client.generate(\n                engine_request,\n                sampling_params,\n                sub_request_id,\n                lora_request=lora_request,\n                priority=priority,\n                prompt_text=prompt_text,\n                tokenization_kwargs=tokenization_kwargs,\n                **kwargs,\n            )\n\n            async for res in generator:\n                context.append_output(res)\n                # NOTE(woosuk): The stop condition is handled by the engine.\n                yield context\n\n            if not context.need_builtin_tool_call():\n                # The model did not ask for a tool call, so we're done.\n                break\n\n            # Call the tool and update the context with the result.\n            tool_output = await context.call_tool()\n            context.append_tool_output(tool_output)\n\n            # TODO: uncomment this and enable tool output streaming\n            # yield context\n\n            # Create inputs for the next turn.\n            # Render the next prompt token ids.\n            if isinstance(context, (HarmonyContext, StreamingHarmonyContext)):\n                prompt_token_ids = context.render_for_completion()\n                engine_prompt = TokensPrompt(prompt_token_ids=prompt_token_ids)\n            elif isinstance(context, ParsableContext):\n                engine_prompts = await self._render_next_turn(\n                    context.request,\n                    context.tokenizer,\n                    context.parser.response_messages,\n                    context.tool_dicts,\n                    context.tool_parser_cls,\n                    context.chat_template,\n                    context.chat_template_content_format,\n                )\n                engine_prompt = engine_prompts[0]\n                prompt_text, _, _ = self._get_prompt_components(engine_prompt)\n\n            # Update the sampling params.\n            sampling_params.max_tokens = self.max_model_len - len(\n                engine_prompt[\"prompt_token_ids\"]\n            )\n            # OPTIMIZATION\n            priority = orig_priority - 1\n            sub_request += 1\n\n    def _get_prompt_components(self, prompt: PromptType) -> PromptComponents:\n        return get_prompt_components(prompt)\n\n    def _log_inputs(\n        self,\n        request_id: str,\n        inputs: PromptType,\n        params: SamplingParams | PoolingParams | BeamSearchParams | None,\n        lora_request: LoRARequest | None,\n    ) -> None:\n        if self.request_logger is None:\n            return\n\n        prompt, prompt_token_ids, prompt_embeds = self._get_prompt_components(inputs)\n\n        self.request_logger.log_inputs(\n            request_id,\n            prompt,\n            prompt_token_ids,\n            prompt_embeds,\n            params=params,\n            lora_request=lora_request,\n        )\n\n    async def _get_trace_headers(\n        self,\n        headers: Headers,\n    ) -> Mapping[str, str] | None:\n        is_tracing_enabled = await self.engine_client.is_tracing_enabled()\n\n        if is_tracing_enabled:\n            return extract_trace_headers(headers)\n\n        if contains_trace_headers(headers):\n            log_tracing_disabled_warning()\n\n        return None\n\n    @staticmethod\n    def _base_request_id(\n        raw_request: Request | None, default: str | None = None\n    ) -> str | None:\n        \"\"\"Pulls the request id to use from a header, if provided\"\"\"\n        if raw_request is not None and (\n            (req_id := raw_request.headers.get(\"X-Request-Id\")) is not None\n        ):\n            return req_id\n\n        return random_uuid() if default is None else default\n\n    @staticmethod\n    def _get_data_parallel_rank(raw_request: Request | None) -> int | None:\n        \"\"\"Pulls the data parallel rank from a header, if provided\"\"\"\n        if raw_request is None:\n            return None\n\n        rank_str = raw_request.headers.get(\"X-data-parallel-rank\")\n        if rank_str is None:\n            return None\n\n        try:\n            return int(rank_str)\n        except ValueError:\n            return None\n\n    @staticmethod\n    def _parse_tool_calls_from_content(\n        request: ResponsesRequest | ChatCompletionRequest,\n        tokenizer: TokenizerLike | None,\n        enable_auto_tools: bool,\n        tool_parser_cls: Callable[[TokenizerLike], ToolParser] | None,\n        content: str | None = None,\n    ) -> tuple[list[FunctionCall] | None, str | None]:\n        function_calls = list[FunctionCall]()\n        if request.tool_choice and isinstance(request.tool_choice, ToolChoiceFunction):\n            assert content is not None\n            # Forced Function Call\n            function_calls.append(\n                FunctionCall(name=request.tool_choice.name, arguments=content)\n            )\n            content = None  # Clear content since tool is called.\n        elif request.tool_choice and isinstance(\n            request.tool_choice, ChatCompletionNamedToolChoiceParam\n        ):\n            assert content is not None\n            # Forced Function Call\n            function_calls.append(\n                FunctionCall(name=request.tool_choice.function.name, arguments=content)\n            )\n            content = None  # Clear content since tool is called.\n        elif request.tool_choice == \"required\":\n            assert content is not None\n            tool_calls = TypeAdapter(list[FunctionDefinition]).validate_json(content)\n            function_calls.extend(\n                [\n                    FunctionCall(\n                        name=tool_call.name,\n                        arguments=json.dumps(tool_call.parameters, ensure_ascii=False),\n                    )\n                    for tool_call in tool_calls\n                ]\n            )\n            content = None  # Clear content since tool is called.\n        elif (\n            tool_parser_cls\n            and enable_auto_tools\n            and (request.tool_choice == \"auto\" or request.tool_choice is None)\n        ):\n            if tokenizer is None:\n                raise ValueError(\n                    \"Tokenizer not available when `skip_tokenizer_init=True`\"\n                )\n\n            # Automatic Tool Call Parsing\n            try:\n                tool_parser = tool_parser_cls(tokenizer)\n            except RuntimeError as e:\n                logger.exception(\"Error in tool parser creation.\")\n                raise e\n            tool_call_info = tool_parser.extract_tool_calls(\n                content if content is not None else \"\",\n                request=request,  # type: ignore\n            )\n            if tool_call_info is not None and tool_call_info.tools_called:\n                # extract_tool_calls() returns a list of tool calls.\n                function_calls.extend(\n                    FunctionCall(\n                        name=tool_call.function.name,\n                        arguments=tool_call.function.arguments,\n                    )\n                    for tool_call in tool_call_info.tool_calls\n                )\n                content = tool_call_info.content\n                if content and content.strip() == \"\":\n                    content = None\n            else:\n                # No tool calls.\n                return None, content\n\n        return function_calls, content\n\n    @staticmethod\n    def _get_decoded_token(\n        logprob: Logprob,\n        token_id: int,\n        tokenizer: TokenizerLike | None,\n        return_as_token_id: bool = False,\n    ) -> str:\n        if return_as_token_id:\n            return f\"token_id:{token_id}\"\n\n        if logprob.decoded_token is not None:\n            return logprob.decoded_token\n\n        if tokenizer is None:\n            raise ValueError(\n                \"Unable to get tokenizer because `skip_tokenizer_init=True`\"\n            )\n\n        return tokenizer.decode(token_id)\n\n    def _is_model_supported(self, model_name: str | None) -> bool:\n        if not model_name:\n            return True\n        return self.models.is_base_model(model_name)",
      "language": "python"
    },
    {
      "code": "class OpenAIServing:\n    request_id_prefix: ClassVar[str] = \"\"\"\n    A short string prepended to every request’s ID (e.g. \"embd\", \"classify\")\n    so you can easily tell “this ID came from Embedding vs Classification.”\n    \"\"\"\n\n    def __init__(\n        self,\n        engine_client: EngineClient,\n        models: OpenAIServingModels,\n        *,\n        request_logger: RequestLogger | None,\n        return_tokens_as_token_ids: bool = False,\n        log_error_stack: bool = False,\n    ):\n        super().__init__()\n\n        self.engine_client = engine_client\n\n        self.models = models\n\n        self.request_logger = request_logger\n        self.return_tokens_as_token_ids = return_tokens_as_token_ids\n        self._tokenizer_executor = ThreadPoolExecutor(max_workers=1)\n        self._apply_mistral_chat_template_async = make_async(\n            apply_mistral_chat_template, executor=self._tokenizer_executor\n        )\n\n        self._async_tokenizer_pool: dict[TokenizerLike, AsyncMicrobatchTokenizer] = {}\n        self.log_error_stack = log_error_stack\n\n        self.input_processor = self.models.input_processor\n        self.io_processor = self.models.io_processor\n        self.model_config = self.models.model_config\n        self.max_model_len = self.model_config.max_model_len\n\n    def _get_tool_parser(\n        self, tool_parser_name: str | None = None, enable_auto_tools: bool = False\n    ) -> Callable[[TokenizerLike], ToolParser] | None:\n        \"\"\"Get the tool parser based on the name.\"\"\"\n        parser = None\n        if not enable_auto_tools or tool_parser_name is None:\n            return parser\n        logger.info('\"auto\" tool choice has been enabled.')\n\n        try:\n            if tool_parser_name == \"pythonic\" and self.model_config.model.startswith(\n                \"meta-llama/Llama-3.2\"\n            ):\n                logger.warning(\n                    \"Llama3.2 models may struggle to emit valid pythonic tool calls\"\n                )\n            parser = ToolParserManager.get_tool_parser(tool_parser_name)\n        except Exception as e:\n            raise TypeError(\n                \"Error: --enable-auto-tool-choice requires \"\n                f\"tool_parser:'{tool_parser_name}' which has not \"\n                \"been registered\"\n            ) from e\n        return parser\n\n    def _get_reasoning_parser(\n        self,\n        reasoning_parser_name: str,\n    ) -> Callable[[TokenizerLike], ReasoningParser] | None:\n        \"\"\"Get the reasoning parser based on the name.\"\"\"\n        parser = None\n        if not reasoning_parser_name:\n            return None\n        try:\n            parser = ReasoningParserManager.get_reasoning_parser(reasoning_parser_name)\n            assert parser is not None\n        except Exception as e:\n            raise TypeError(f\"{reasoning_parser_name=} has not been registered\") from e\n        return parser\n\n    async def reset_mm_cache(self) -> None:\n        self.input_processor.clear_mm_cache()\n        await self.engine_client.reset_mm_cache()\n\n    async def beam_search(\n        self,\n        prompt: PromptType,\n        request_id: str,\n        params: BeamSearchParams,\n        lora_request: LoRARequest | None = None,\n        trace_headers: Mapping[str, str] | None = None,\n    ) -> AsyncGenerator[RequestOutput, None]:\n        beam_width = params.beam_width\n        max_tokens = params.max_tokens\n        ignore_eos = params.ignore_eos\n        temperature = params.temperature\n        length_penalty = params.length_penalty\n        include_stop_str_in_output = params.include_stop_str_in_output\n\n        input_processor = self.input_processor\n        tokenizer = input_processor.tokenizer\n        if tokenizer is None:\n            raise VLLMValidationError(\n                \"You cannot use beam search when `skip_tokenizer_init=True`\",\n                parameter=\"skip_tokenizer_init\",\n                value=True,\n            )\n\n        eos_token_id: int = tokenizer.eos_token_id  # type: ignore\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            raise NotImplementedError\n\n        prompt_text: str | None\n        prompt_token_ids: list[int]\n        multi_modal_data: MultiModalDataDict | None\n        if isinstance(prompt, str):\n            prompt_text = prompt\n            prompt_token_ids = []\n            multi_modal_data = None\n        else:\n            prompt_text = prompt.get(\"prompt\")  # type: ignore\n            prompt_token_ids = prompt.get(\"prompt_token_ids\", [])  # type: ignore\n            multi_modal_data = prompt.get(\"multi_modal_data\")  # type: ignore\n\n        mm_processor_kwargs: dict[str, Any] | None = None\n\n        # This is a workaround to fix multimodal beam search; this is a\n        # bandaid fix for 2 small problems:\n        # 1. Multi_modal_data on the processed_inputs currently resolves to\n        #    `None`.\n        # 2. preprocessing above expands the multimodal placeholders. However,\n        #    this happens again in generation, so the double expansion causes\n        #    a mismatch.\n        # TODO - would be ideal to handle this more gracefully.\n\n        tokenized_length = len(prompt_token_ids)\n\n        sort_beams_key = create_sort_beams_key_function(eos_token_id, length_penalty)\n\n        logprobs_num = 2 * beam_width\n        beam_search_params = SamplingParams(\n            logprobs=logprobs_num,\n            max_tokens=1,\n            temperature=temperature,\n        )\n        all_beams = [\n            BeamSearchSequence(\n                tokens=prompt_token_ids,\n                cum_logprob=0,\n                logprobs=[],\n                multi_modal_data=multi_modal_data,\n                mm_processor_kwargs=mm_processor_kwargs,\n                lora_request=lora_request,\n            )\n        ]\n        completed = []\n\n        for _ in range(max_tokens):\n            prompts_batch, lora_req_batch = zip(\n                *[\n                    (\n                        TokensPrompt(\n                            prompt_token_ids=beam.tokens,\n                            multi_modal_data=beam.multi_modal_data,\n                            mm_processor_kwargs=beam.mm_processor_kwargs,\n                        ),\n                        beam.lora_request,\n                    )\n                    for beam in all_beams\n                ]\n            )\n\n            tasks = []\n            request_id_batch = f\"{request_id}-{random_uuid()}\"\n\n            for i, (individual_prompt, lora_req) in enumerate(\n                zip(prompts_batch, lora_req_batch)\n            ):\n                request_id_item = f\"{request_id_batch}-beam-{i}\"\n                task = asyncio.create_task(\n                    collect_from_async_generator(\n                        self.engine_client.generate(\n                            individual_prompt,\n                            beam_search_params,\n                            request_id_item,\n                            lora_request=lora_req,\n                            trace_headers=trace_headers,\n                        )\n                    )\n                )\n                tasks.append(task)\n\n            output = [x[0] for x in await asyncio.gather(*tasks)]\n\n            new_beams = []\n            # Store all new tokens generated by beam\n            all_beams_token_id = []\n            # Store the cumulative probability of all tokens\n            # generated by beam search\n            all_beams_logprob = []\n            # Iterate through all beam inference results\n            for i, result in enumerate(output):\n                current_beam = all_beams[i]\n\n                # check for error finish reason and abort beam search\n                if result.outputs[0].finish_reason == \"error\":\n                    # yield error output and terminate beam search\n                    yield RequestOutput(\n                        request_id=request_id,\n                        prompt=prompt_text,\n                        outputs=[\n                            CompletionOutput(\n                                index=0,\n                                text=\"\",\n                                token_ids=[],\n                                cumulative_logprob=None,\n                                logprobs=None,\n                                finish_reason=\"error\",\n                            )\n                        ],\n                        finished=True,\n                        prompt_token_ids=prompt_token_ids,\n                        prompt_logprobs=None,\n                    )\n                    return\n\n                if result.outputs[0].logprobs is not None:\n                    logprobs = result.outputs[0].logprobs[0]\n                    all_beams_token_id.extend(list(logprobs.keys()))\n                    all_beams_logprob.extend(\n                        [\n                            current_beam.cum_logprob + obj.logprob\n                            for obj in logprobs.values()\n                        ]\n                    )\n\n            # Handle the token for the end of sentence (EOS)\n            all_beams_token_id = np.array(all_beams_token_id)\n            all_beams_logprob = np.array(all_beams_logprob)\n\n            if not ignore_eos:\n                # Get the index position of eos token in all generated results\n                eos_idx = np.where(all_beams_token_id == eos_token_id)[0]\n                for idx in eos_idx:\n                    current_beam = all_beams[idx // logprobs_num]\n                    result = output[idx // logprobs_num]\n                    assert result.outputs[0].logprobs is not None\n                    logprobs_entry = result.outputs[0].logprobs[0]\n                    completed.append(\n                        BeamSearchSequence(\n                            tokens=current_beam.tokens + [eos_token_id]\n                            if include_stop_str_in_output\n                            else current_beam.tokens,\n                            logprobs=current_beam.logprobs + [logprobs_entry],\n                            cum_logprob=float(all_beams_logprob[idx]),\n                            finish_reason=\"stop\",\n                            stop_reason=eos_token_id,\n                        )\n                    )\n                # After processing, set the log probability of the eos condition\n                # to negative infinity.\n                all_beams_logprob[eos_idx] = -np.inf\n\n            # Processing non-EOS tokens\n            # Get indices of the top beam_width probabilities\n            topn_idx = np.argpartition(np.negative(all_beams_logprob), beam_width)[\n                :beam_width\n            ]\n\n            for idx in topn_idx:\n                current_beam = all_beams[idx // logprobs_num]\n                result = output[idx // logprobs_num]\n                token_id = int(all_beams_token_id[idx])\n                assert result.outputs[0].logprobs is not None\n                logprobs_entry = result.outputs[0].logprobs[0]\n                new_beams.append(\n                    BeamSearchSequence(\n                        tokens=current_beam.tokens + [token_id],\n                        logprobs=current_beam.logprobs + [logprobs_entry],\n                        lora_request=current_beam.lora_request,\n                        cum_logprob=float(all_beams_logprob[idx]),\n                        multi_modal_data=current_beam.multi_modal_data,\n                        mm_processor_kwargs=current_beam.mm_processor_kwargs,\n                    )\n                )\n\n            all_beams = new_beams\n\n        completed.extend(all_beams)\n        sorted_completed = sorted(completed, key=sort_beams_key, reverse=True)\n        best_beams = sorted_completed[:beam_width]\n\n        for beam in best_beams:\n            if beam.tokens[-1] == eos_token_id and not ignore_eos:\n                # Skip the eos token in the text.\n                tokens = beam.tokens[tokenized_length:-1]\n            else:\n                tokens = beam.tokens[tokenized_length:]\n            beam.text = tokenizer.decode(tokens)\n\n        yield RequestOutput(\n            request_id=request_id,\n            prompt=prompt_text,\n            outputs=[\n                CompletionOutput(\n                    text=beam.text,  # type: ignore\n                    cumulative_logprob=beam.cum_logprob,\n                    token_ids=beam.tokens[tokenized_length:],\n                    index=i,\n                    logprobs=beam.logprobs,\n                    finish_reason=beam.finish_reason\n                    if beam.finish_reason is not None\n                    else \"length\",\n                    stop_reason=beam.stop_reason,\n                )\n                for (i, beam) in enumerate(best_beams)\n            ],\n            finished=True,\n            prompt_token_ids=prompt_token_ids,\n            prompt_logprobs=None,\n        )\n\n    def _get_renderer(self, tokenizer: TokenizerLike | None) -> BaseRenderer:\n        \"\"\"\n        Get a Renderer instance with the provided tokenizer.\n        Uses shared async tokenizer pool for efficiency.\n        \"\"\"\n        return CompletionRenderer(\n            model_config=self.model_config,\n            tokenizer=tokenizer,\n            async_tokenizer_pool=self._async_tokenizer_pool,\n        )\n\n    def _build_render_config(\n        self,\n        request: Any,\n    ) -> RenderConfig:\n        \"\"\"\n        Build and return a `RenderConfig` for an endpoint.\n\n        Used by the renderer to control how prompts are prepared\n        (e.g., tokenization and length handling). Endpoints should\n        implement this with logic appropriate to their request type.\n        \"\"\"\n        raise NotImplementedError\n\n    def _get_async_tokenizer(self, tokenizer) -> AsyncMicrobatchTokenizer:\n        \"\"\"\n        Return (and cache) an `AsyncMicrobatchTokenizer` bound to the\n        given tokenizer.\n        \"\"\"\n        async_tokenizer = self._async_tokenizer_pool.get(tokenizer)\n        if async_tokenizer is None:\n            async_tokenizer = AsyncMicrobatchTokenizer(tokenizer)\n            self._async_tokenizer_pool[tokenizer] = async_tokenizer\n        return async_tokenizer\n\n    async def _preprocess(\n        self,\n        ctx: ServeContext,\n    ) -> ErrorResponse | None:\n        \"\"\"\n        Default preprocessing hook. Subclasses may override\n        to prepare `ctx` (classification, embedding, etc.).\n        \"\"\"\n        return None\n\n    def _build_response(\n        self,\n        ctx: ServeContext,\n    ) -> AnyResponse | ErrorResponse:\n        \"\"\"\n        Default response builder. Subclass may override this method\n        to return the appropriate response object.\n        \"\"\"\n        return self.create_error_response(\"unimplemented endpoint\")\n\n    async def handle(\n        self,\n        ctx: ServeContext,\n    ) -> AnyResponse | ErrorResponse:\n        generation: AsyncGenerator[AnyResponse | ErrorResponse, None]\n        generation = self._pipeline(ctx)\n\n        async for response in generation:\n            return response\n\n        return self.create_error_response(\"No response yielded from pipeline\")\n\n    async def _pipeline(\n        self,\n        ctx: ServeContext,\n    ) -> AsyncGenerator[AnyResponse | ErrorResponse, None]:\n        \"\"\"Execute the request processing pipeline yielding responses.\"\"\"\n        if error := await self._check_model(ctx.request):\n            yield error\n        if error := self._validate_request(ctx):\n            yield error\n\n        preprocess_ret = await self._preprocess(ctx)\n        if isinstance(preprocess_ret, ErrorResponse):\n            yield preprocess_ret\n\n        generators_ret = await self._prepare_generators(ctx)\n        if isinstance(generators_ret, ErrorResponse):\n            yield generators_ret\n\n        collect_ret = await self._collect_batch(ctx)\n        if isinstance(collect_ret, ErrorResponse):\n            yield collect_ret\n\n        yield self._build_response(ctx)\n\n    def _validate_request(self, ctx: ServeContext) -> ErrorResponse | None:\n        truncate_prompt_tokens = getattr(ctx.request, \"truncate_prompt_tokens\", None)\n\n        if (\n            truncate_prompt_tokens is not None\n            and truncate_prompt_tokens > self.max_model_len\n        ):\n            return self.create_error_response(\n                \"truncate_prompt_tokens value is \"\n                \"greater than max_model_len.\"\n                \" Please, select a smaller truncation size.\"\n            )\n        return None\n\n    def _create_pooling_params(\n        self,\n        ctx: ServeContext,\n    ) -> PoolingParams | ErrorResponse:\n        if not hasattr(ctx.request, \"to_pooling_params\"):\n            return self.create_error_response(\n                \"Request type does not support pooling parameters\"\n            )\n\n        return ctx.request.to_pooling_params()\n\n    async def _prepare_generators(\n        self,\n        ctx: ServeContext,\n    ) -> ErrorResponse | None:\n        \"\"\"Schedule the request and get the result generator.\"\"\"\n        generators: list[\n            AsyncGenerator[RequestOutput | PoolingRequestOutput, None]\n        ] = []\n\n        try:\n            trace_headers = (\n                None\n                if ctx.raw_request is None\n                else await self._get_trace_headers(ctx.raw_request.headers)\n            )\n\n            pooling_params = self._create_pooling_params(ctx)\n            if isinstance(pooling_params, ErrorResponse):\n                return pooling_params\n\n            if ctx.engine_prompts is None:\n                return self.create_error_response(\"Engine prompts not available\")\n\n            for i, engine_prompt in enumerate(ctx.engine_prompts):\n                request_id_item = f\"{ctx.request_id}-{i}\"\n\n                self._log_inputs(\n                    request_id_item,\n                    engine_prompt,\n                    params=pooling_params,\n                    lora_request=ctx.lora_request,\n                )\n\n                generator = self.engine_client.encode(\n                    engine_prompt,\n                    pooling_params,\n                    request_id_item,\n                    lora_request=ctx.lora_request,\n                    trace_headers=trace_headers,\n                    priority=getattr(ctx.request, \"priority\", 0),\n                )\n\n                generators.append(generator)\n\n            ctx.result_generator = merge_async_iterators(*generators)\n\n            return None\n\n        except Exception as e:\n            return self.create_error_response(e)\n\n    async def _collect_batch(\n        self,\n        ctx: ServeContext,\n    ) -> ErrorResponse | None:\n        \"\"\"Collect batch results from the result generator.\"\"\"\n        try:\n            if ctx.engine_prompts is None:\n                return self.create_error_response(\"Engine prompts not available\")\n\n            num_prompts = len(ctx.engine_prompts)\n            final_res_batch: list[RequestOutput | PoolingRequestOutput | None]\n            final_res_batch = [None] * num_prompts\n\n            if ctx.result_generator is None:\n                return self.create_error_response(\"Result generator not available\")\n\n            async for i, res in ctx.result_generator:\n                final_res_batch[i] = res\n\n            if None in final_res_batch:\n                return self.create_error_response(\n                    \"Failed to generate results for all prompts\"\n                )\n\n            ctx.final_res_batch = [res for res in final_res_batch if res is not None]\n\n            return None\n\n        except Exception as e:\n            return self.create_error_response(e)\n\n    def create_error_response(\n        self,\n        message: str | Exception,\n        err_type: str = \"BadRequestError\",\n        status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,\n        param: str | None = None,\n    ) -> ErrorResponse:\n        exc: Exception | None = None\n\n        if isinstance(message, Exception):\n            exc = message\n\n            from vllm.entrypoints.openai.protocol import VLLMValidationError\n\n            if isinstance(exc, VLLMValidationError):\n                err_type = \"BadRequestError\"\n                status_code = HTTPStatus.BAD_REQUEST\n                param = exc.parameter\n            elif isinstance(exc, (ValueError, TypeError, RuntimeError)):\n                # Common validation errors from user input\n                err_type = \"BadRequestError\"\n                status_code = HTTPStatus.BAD_REQUEST\n                param = None\n            elif exc.__class__.__name__ == \"TemplateError\":\n                # jinja2.TemplateError (avoid importing jinja2)\n                err_type = \"BadRequestError\"\n                status_code = HTTPStatus.BAD_REQUEST\n                param = None\n            else:\n                err_type = \"InternalServerError\"\n                status_code = HTTPStatus.INTERNAL_SERVER_ERROR\n                param = None\n\n            message = str(exc)\n\n        if self.log_error_stack:\n            exc_type, _, _ = sys.exc_info()\n            if exc_type is not None:\n                traceback.print_exc()\n            else:\n                traceback.print_stack()\n        return ErrorResponse(\n            error=ErrorInfo(\n                message=message,\n                type=err_type,\n                code=status_code.value,\n                param=param,\n            )\n        )\n\n    def create_streaming_error_response(\n        self,\n        message: str | Exception,\n        err_type: str = \"BadRequestError\",\n        status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,\n        param: str | None = None,\n    ) -> str:\n        json_str = json.dumps(\n            self.create_error_response(\n                message=message,\n                err_type=err_type,\n                status_code=status_code,\n                param=param,\n            ).model_dump()\n        )\n        return json_str\n\n    def _raise_if_error(self, finish_reason: str | None, request_id: str) -> None:\n        \"\"\"Raise GenerationError if finish_reason indicates an error.\"\"\"\n        if finish_reason == \"error\":\n            logger.error(\n                \"Request %s failed with an internal error during generation\",\n                request_id,\n            )\n            raise GenerationError(\"Internal server error\")\n\n    def _convert_generation_error_to_response(\n        self, e: GenerationError\n    ) -> ErrorResponse:\n        \"\"\"Convert GenerationError to ErrorResponse.\"\"\"\n        return self.create_error_response(\n            str(e),\n            err_type=\"InternalServerError\",\n            status_code=e.status_code,\n        )\n\n    def _convert_generation_error_to_streaming_response(\n        self, e: GenerationError\n    ) -> str:\n        \"\"\"Convert GenerationError to streaming error response.\"\"\"\n        return self.create_streaming_error_response(\n            str(e),\n            err_type=\"InternalServerError\",\n            status_code=e.status_code,\n        )\n\n    async def _check_model(\n        self,\n        request: AnyRequest,\n    ) -> ErrorResponse | None:\n        error_response = None\n\n        if self._is_model_supported(request.model):\n            return None\n        if request.model in self.models.lora_requests:\n            return None\n        if (\n            envs.VLLM_ALLOW_RUNTIME_LORA_UPDATING\n            and request.model\n            and (load_result := await self.models.resolve_lora(request.model))\n        ):\n            if isinstance(load_result, LoRARequest):\n                return None\n            if (\n                isinstance(load_result, ErrorResponse)\n                and load_result.error.code == HTTPStatus.BAD_REQUEST.value\n            ):\n                error_response = load_result\n\n        return error_response or self.create_error_response(\n            message=f\"The model `{request.model}` does not exist.\",\n            err_type=\"NotFoundError\",\n            status_code=HTTPStatus.NOT_FOUND,\n            param=\"model\",\n        )\n\n    def _get_active_default_mm_loras(self, request: AnyRequest) -> LoRARequest | None:\n        \"\"\"Determine if there are any active default multimodal loras.\"\"\"\n        # TODO: Currently this is only enabled for chat completions\n        # to be better aligned with only being enabled for .generate\n        # when run offline. It would be nice to support additional\n        # tasks types in the future.\n        message_types = self._get_message_types(request)\n        default_mm_loras = set()\n\n        for lora in self.models.lora_requests.values():\n            # Best effort match for default multimodal lora adapters;\n            # There is probably a better way to do this, but currently\n            # this matches against the set of 'types' in any content lists\n            # up until '_', e.g., to match audio_url -> audio\n            if lora.lora_name in message_types:\n                default_mm_loras.add(lora)\n\n        # Currently only support default modality specific loras if\n        # we have exactly one lora matched on the request.\n        if len(default_mm_loras) == 1:\n            return default_mm_loras.pop()\n        return None\n\n    def _maybe_get_adapters(\n        self,\n        request: AnyRequest,\n        supports_default_mm_loras: bool = False,\n    ) -> LoRARequest | None:\n        if request.model in self.models.lora_requests:\n            return self.models.lora_requests[request.model]\n\n        # Currently only support default modality specific loras\n        # if we have exactly one lora matched on the request.\n        if supports_default_mm_loras:\n            default_mm_lora = self._get_active_default_mm_loras(request)\n            if default_mm_lora is not None:\n                return default_mm_lora\n\n        if self._is_model_supported(request.model):\n            return None\n\n        # if _check_model has been called earlier, this will be unreachable\n        raise ValueError(f\"The model `{request.model}` does not exist.\")\n\n    def _get_message_types(self, request: AnyRequest) -> set[str]:\n        \"\"\"Retrieve the set of types from message content dicts up\n        until `_`; we use this to match potential multimodal data\n        with default per modality loras.\n        \"\"\"\n        message_types: set[str] = set()\n\n        if not hasattr(request, \"messages\"):\n            return message_types\n\n        messages = request.messages\n        if messages is None or isinstance(messages, (str, bytes)):\n            return message_types\n\n        for message in messages:\n            if (\n                isinstance(message, dict)\n                and \"content\" in message\n                and isinstance(message[\"content\"], list)\n            ):\n                for content_dict in message[\"content\"]:\n                    if \"type\" in content_dict:\n                        message_types.add(content_dict[\"type\"].split(\"_\")[0])\n        return message_types\n\n    async def _normalize_prompt_text_to_input(\n        self,\n        request: AnyRequest,\n        prompt: str,\n        tokenizer: TokenizerLike,\n        add_special_tokens: bool,\n    ) -> TokensPrompt:\n        async_tokenizer = self._get_async_tokenizer(tokenizer)\n\n        if (\n            self.model_config.encoder_config is not None\n            and self.model_config.encoder_config.get(\"do_lower_case\", False)\n        ):\n            prompt = prompt.lower()\n\n        truncate_prompt_tokens = getattr(request, \"truncate_prompt_tokens\", None)\n\n        if truncate_prompt_tokens is None:\n            encoded = await async_tokenizer(\n                prompt, add_special_tokens=add_special_tokens\n            )\n        elif truncate_prompt_tokens < 0:\n            # Negative means we cap at the model's max length\n            encoded = await async_tokenizer(\n                prompt,\n                add_special_tokens=add_special_tokens,\n                truncation=True,\n                max_length=self.max_model_len,\n            )\n        else:\n            encoded = await async_tokenizer(\n                prompt,\n                add_special_tokens=add_special_tokens,\n                truncation=True,\n                max_length=truncate_prompt_tokens,\n            )\n\n        input_ids = encoded.input_ids\n        input_text = prompt\n\n        return self._validate_input(request, input_ids, input_text)\n\n    async def _normalize_prompt_tokens_to_input(\n        self,\n        request: AnyRequest,\n        prompt_ids: list[int],\n        tokenizer: TokenizerLike | None,\n    ) -> TokensPrompt:\n        truncate_prompt_tokens = getattr(request, \"truncate_prompt_tokens\", None)\n\n        if truncate_prompt_tokens is None:\n            input_ids = prompt_ids\n        elif truncate_prompt_tokens < 0:\n            input_ids = prompt_ids[-self.max_model_len :]\n        else:\n            input_ids = prompt_ids[-truncate_prompt_tokens:]\n\n        if tokenizer is None:\n            input_text = \"\"\n        else:\n            async_tokenizer = self._get_async_tokenizer(tokenizer)\n            input_text = await async_tokenizer.decode(input_ids)\n\n        return self._validate_input(request, input_ids, input_text)\n\n    def _validate_input(\n        self,\n        request: AnyRequest,\n        input_ids: list[int],\n        input_text: str,\n    ) -> TokensPrompt:\n        token_num = len(input_ids)\n\n        # Note: EmbeddingRequest, ClassificationRequest,\n        # and ScoreRequest doesn't have max_tokens\n        if isinstance(\n            request,\n            (\n                EmbeddingChatRequest,\n                EmbeddingCompletionRequest,\n                ScoreRequest,\n                RerankRequest,\n                ClassificationCompletionRequest,\n                ClassificationChatRequest,\n            ),\n        ):\n            # Note: input length can be up to the entire model context length\n            # since these requests don't generate tokens.\n            if token_num > self.max_model_len:\n                operations: dict[type[AnyRequest], str] = {\n                    ScoreRequest: \"score\",\n                    ClassificationCompletionRequest: \"classification\",\n                    ClassificationChatRequest: \"classification\",\n                }\n                operation = operations.get(type(request), \"embedding generation\")\n                raise VLLMValidationError(\n                    f\"This model's maximum context length is \"\n                    f\"{self.max_model_len} tokens. However, you requested \"\n                    f\"{token_num} tokens in the input for {operation}. \"\n                    f\"Please reduce the length of the input.\",\n                    parameter=\"input_tokens\",\n                    value=token_num,\n                )\n            return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n        # Note: TokenizeRequest and DetokenizeRequest doesn't have max_tokens\n        # and does not require model context length validation\n        if isinstance(\n            request,\n            (TokenizeCompletionRequest, TokenizeChatRequest, DetokenizeRequest),\n        ):\n            return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n        # chat completion endpoint supports max_completion_tokens\n        if isinstance(request, ChatCompletionRequest):\n            # TODO(#9845): remove max_tokens when field dropped from OpenAI API\n            max_tokens = request.max_completion_tokens or request.max_tokens\n        else:\n            max_tokens = getattr(request, \"max_tokens\", None)\n\n        # Note: input length can be up to model context length - 1 for\n        # completion-like requests.\n        if token_num >= self.max_model_len:\n            raise VLLMValidationError(\n                f\"This model's maximum context length is \"\n                f\"{self.max_model_len} tokens. However, your request has \"\n                f\"{token_num} input tokens. Please reduce the length of \"\n                \"the input messages.\",\n                parameter=\"input_tokens\",\n                value=token_num,\n            )\n\n        if max_tokens is not None and token_num + max_tokens > self.max_model_len:\n            raise VLLMValidationError(\n                \"'max_tokens' or 'max_completion_tokens' is too large: \"\n                f\"{max_tokens}. This model's maximum context length is \"\n                f\"{self.max_model_len} tokens and your request has \"\n                f\"{token_num} input tokens ({max_tokens} > {self.max_model_len}\"\n                f\" - {token_num}).\",\n                parameter=\"max_tokens\",\n                value=max_tokens,\n            )\n\n        return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n    async def _tokenize_prompt_input_async(\n        self,\n        request: AnyRequest,\n        tokenizer: TokenizerLike,\n        prompt_input: str | list[int],\n        add_special_tokens: bool = True,\n    ) -> TokensPrompt:\n        \"\"\"\n        A simpler implementation that tokenizes a single prompt input.\n        \"\"\"\n        async for result in self._tokenize_prompt_inputs_async(\n            request,\n            tokenizer,\n            [prompt_input],\n            add_special_tokens=add_special_tokens,\n        ):\n            return result\n        raise ValueError(\"No results yielded from tokenization\")\n\n    async def _tokenize_prompt_inputs_async(\n        self,\n        request: AnyRequest,\n        tokenizer: TokenizerLike,\n        prompt_inputs: Iterable[str | list[int]],\n        add_special_tokens: bool = True,\n    ) -> AsyncGenerator[TokensPrompt, None]:\n        \"\"\"\n        A simpler implementation that tokenizes multiple prompt inputs.\n        \"\"\"\n        for prompt in prompt_inputs:\n            if isinstance(prompt, str):\n                yield await self._normalize_prompt_text_to_input(\n                    request,\n                    prompt=prompt,\n                    tokenizer=tokenizer,\n                    add_special_tokens=add_special_tokens,\n                )\n            else:\n                yield await self._normalize_prompt_tokens_to_input(\n                    request,\n                    prompt_ids=prompt,\n                    tokenizer=tokenizer,\n                )\n\n    def _validate_chat_template(\n        self,\n        request_chat_template: str | None,\n        chat_template_kwargs: dict[str, Any] | None,\n        trust_request_chat_template: bool,\n    ) -> ErrorResponse | None:\n        if not trust_request_chat_template and (\n            request_chat_template is not None\n            or (\n                chat_template_kwargs\n                and chat_template_kwargs.get(\"chat_template\") is not None\n            )\n        ):\n            return self.create_error_response(\n                \"Chat template is passed with request, but \"\n                \"--trust-request-chat-template is not set. \"\n                \"Refused request with untrusted chat template.\"\n            )\n        return None\n\n    async def _preprocess_chat(\n        self,\n        request: ChatLikeRequest | ResponsesRequest,\n        tokenizer: TokenizerLike | None,\n        messages: list[ChatCompletionMessageParam],\n        chat_template: str | None,\n        chat_template_content_format: ChatTemplateContentFormatOption,\n        add_generation_prompt: bool = True,\n        continue_final_message: bool = False,\n        tool_dicts: list[dict[str, Any]] | None = None,\n        documents: list[dict[str, str]] | None = None,\n        chat_template_kwargs: dict[str, Any] | None = None,\n        tool_parser: Callable[[TokenizerLike], ToolParser] | None = None,\n        add_special_tokens: bool = False,\n    ) -> tuple[list[ConversationMessage], list[TokensPrompt]]:\n        model_config = self.model_config\n\n        resolved_content_format = resolve_chat_template_content_format(\n            chat_template,\n            tool_dicts,\n            chat_template_content_format,\n            tokenizer,\n            model_config=model_config,\n        )\n        conversation, mm_data_future, mm_uuids = parse_chat_messages_futures(\n            messages,\n            model_config,\n            content_format=resolved_content_format,\n        )\n\n        _chat_template_kwargs: dict[str, Any] = dict(\n            chat_template=chat_template,\n            add_generation_prompt=add_generation_prompt,\n            continue_final_message=continue_final_message,\n            tools=tool_dicts,\n            documents=documents,\n        )\n        _chat_template_kwargs.update(chat_template_kwargs or {})\n\n        request_prompt: str | list[int]\n\n        if tokenizer is None:\n            request_prompt = \"placeholder\"\n        elif isinstance(tokenizer, MistralTokenizer):\n            request_prompt = await self._apply_mistral_chat_template_async(\n                tokenizer,\n                messages=messages,\n                **_chat_template_kwargs,\n            )\n        elif isinstance(tokenizer, DeepseekV32Tokenizer):\n            request_prompt = tokenizer.apply_chat_template(\n                conversation=conversation,\n                messages=messages,\n                model_config=model_config,\n                **_chat_template_kwargs,\n            )\n        else:\n            request_prompt = apply_hf_chat_template(\n                tokenizer=tokenizer,\n                conversation=conversation,\n                model_config=model_config,\n                **_chat_template_kwargs,\n            )\n\n        mm_data = await mm_data_future\n\n        # tool parsing is done only if a tool_parser has been set and if\n        # tool_choice is not \"none\" (if tool_choice is \"none\" but a tool_parser\n        # is set, we want to prevent parsing a tool_call hallucinated by the LLM\n        should_parse_tools = tool_parser is not None and (\n            hasattr(request, \"tool_choice\") and request.tool_choice != \"none\"\n        )\n\n        if should_parse_tools:\n            if not isinstance(request, ChatCompletionRequest | ResponsesRequest):\n                msg = (\n                    \"Tool usage is only supported for Chat Completions API \"\n                    \"or Responses API requests.\"\n                )\n                raise NotImplementedError(msg)\n            request = tool_parser(tokenizer).adjust_request(request=request)  # type: ignore\n\n        if tokenizer is None:\n            assert isinstance(request_prompt, str), (\n                \"Prompt has to be a string\",\n                \"when the tokenizer is not initialised\",\n            )\n            prompt_inputs = TokensPrompt(prompt=request_prompt, prompt_token_ids=[1])\n        elif isinstance(request_prompt, str):\n            prompt_inputs = await self._tokenize_prompt_input_async(\n                request,\n                tokenizer,\n                request_prompt,\n                add_special_tokens=add_special_tokens,\n            )\n        else:\n            # For MistralTokenizer\n            assert is_list_of(request_prompt, int), (\n                \"Prompt has to be either a string or a list of token ids\"\n            )\n            prompt_inputs = TokensPrompt(\n                prompt=tokenizer.decode(request_prompt),\n                prompt_token_ids=request_prompt,\n            )\n\n        engine_prompt = TokensPrompt(prompt_token_ids=prompt_inputs[\"prompt_token_ids\"])\n        if \"prompt\" in prompt_inputs:\n            engine_prompt[\"prompt\"] = prompt_inputs[\"prompt\"]\n\n        if mm_data is not None:\n            engine_prompt[\"multi_modal_data\"] = mm_data\n\n        if mm_uuids is not None:\n            engine_prompt[\"multi_modal_uuids\"] = mm_uuids\n\n        if request.mm_processor_kwargs is not None:\n            engine_prompt[\"mm_processor_kwargs\"] = request.mm_processor_kwargs\n\n        if hasattr(request, \"cache_salt\") and request.cache_salt is not None:\n            engine_prompt[\"cache_salt\"] = request.cache_salt\n\n        return conversation, [engine_prompt]\n\n    async def _process_inputs(\n        self,\n        request_id: str,\n        engine_prompt: PromptType,\n        params: SamplingParams | PoolingParams,\n        *,\n        lora_request: LoRARequest | None,\n        trace_headers: Mapping[str, str] | None,\n        priority: int,\n        data_parallel_rank: int | None = None,\n    ) -> tuple[EngineCoreRequest, dict[str, Any]]:\n        \"\"\"Use the Processor to process inputs for AsyncLLM.\"\"\"\n        tokenization_kwargs: dict[str, Any] = {}\n        _validate_truncation_size(\n            self.max_model_len, params.truncate_prompt_tokens, tokenization_kwargs\n        )\n\n        engine_request = self.input_processor.process_inputs(\n            request_id,\n            engine_prompt,\n            params,\n            lora_request=lora_request,\n            tokenization_kwargs=tokenization_kwargs,\n            trace_headers=trace_headers,\n            priority=priority,\n            data_parallel_rank=data_parallel_rank,\n        )\n        return engine_request, tokenization_kwargs\n\n    async def _render_next_turn(\n        self,\n        request: ResponsesRequest,\n        tokenizer: TokenizerLike | None,\n        messages: list[ResponseInputOutputItem],\n        tool_dicts: list[dict[str, Any]] | None,\n        tool_parser,\n        chat_template: str | None,\n        chat_template_content_format: ChatTemplateContentFormatOption,\n    ):\n        new_messages = construct_input_messages(\n            request_input=messages,\n        )\n\n        _, engine_prompts = await self._preprocess_chat(\n            request,\n            tokenizer,\n            new_messages,\n            tool_dicts=tool_dicts,\n            tool_parser=tool_parser,\n            chat_template=chat_template,\n            chat_template_content_format=chat_template_content_format,\n        )\n        return engine_prompts\n\n    async def _generate_with_builtin_tools(\n        self,\n        request_id: str,\n        engine_prompt: TokensPrompt,\n        sampling_params: SamplingParams,\n        context: ConversationContext,\n        lora_request: LoRARequest | None = None,\n        priority: int = 0,\n        **kwargs,\n    ):\n        prompt_text, _, _ = self._get_prompt_components(engine_prompt)\n\n        orig_priority = priority\n        sub_request = 0\n        while True:\n            # Ensure that each sub-request has a unique request id.\n            sub_request_id = f\"{request_id}_{sub_request}\"\n            self._log_inputs(\n                sub_request_id,\n                engine_prompt,\n                params=sampling_params,\n                lora_request=lora_request,\n            )\n            trace_headers = kwargs.get(\"trace_headers\")\n            engine_request, tokenization_kwargs = await self._process_inputs(\n                sub_request_id,\n                engine_prompt,\n                sampling_params,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                priority=priority,\n            )\n\n            generator = self.engine_client.generate(\n                engine_request,\n                sampling_params,\n                sub_request_id,\n                lora_request=lora_request,\n                priority=priority,\n                prompt_text=prompt_text,\n                tokenization_kwargs=tokenization_kwargs,\n                **kwargs,\n            )\n\n            async for res in generator:\n                context.append_output(res)\n                # NOTE(woosuk): The stop condition is handled by the engine.\n                yield context\n\n            if not context.need_builtin_tool_call():\n                # The model did not ask for a tool call, so we're done.\n                break\n\n            # Call the tool and update the context with the result.\n            tool_output = await context.call_tool()\n            context.append_tool_output(tool_output)\n\n            # TODO: uncomment this and enable tool output streaming\n            # yield context\n\n            # Create inputs for the next turn.\n            # Render the next prompt token ids.\n            if isinstance(context, (HarmonyContext, StreamingHarmonyContext)):\n                prompt_token_ids = context.render_for_completion()\n                engine_prompt = TokensPrompt(prompt_token_ids=prompt_token_ids)\n            elif isinstance(context, ParsableContext):\n                engine_prompts = await self._render_next_turn(\n                    context.request,\n                    context.tokenizer,\n                    context.parser.response_messages,\n                    context.tool_dicts,\n                    context.tool_parser_cls,\n                    context.chat_template,\n                    context.chat_template_content_format,\n                )\n                engine_prompt = engine_prompts[0]\n                prompt_text, _, _ = self._get_prompt_components(engine_prompt)\n\n            # Update the sampling params.\n            sampling_params.max_tokens = self.max_model_len - len(\n                engine_prompt[\"prompt_token_ids\"]\n            )\n            # OPTIMIZATION\n            priority = orig_priority - 1\n            sub_request += 1\n\n    def _get_prompt_components(self, prompt: PromptType) -> PromptComponents:\n        return get_prompt_components(prompt)\n\n    def _log_inputs(\n        self,\n        request_id: str,\n        inputs: PromptType,\n        params: SamplingParams | PoolingParams | BeamSearchParams | None,\n        lora_request: LoRARequest | None,\n    ) -> None:\n        if self.request_logger is None:\n            return\n\n        prompt, prompt_token_ids, prompt_embeds = self._get_prompt_components(inputs)\n\n        self.request_logger.log_inputs(\n            request_id,\n            prompt,\n            prompt_token_ids,\n            prompt_embeds,\n            params=params,\n            lora_request=lora_request,\n        )\n\n    async def _get_trace_headers(\n        self,\n        headers: Headers,\n    ) -> Mapping[str, str] | None:\n        is_tracing_enabled = await self.engine_client.is_tracing_enabled()\n\n        if is_tracing_enabled:\n            return extract_trace_headers(headers)\n\n        if contains_trace_headers(headers):\n            log_tracing_disabled_warning()\n\n        return None\n\n    @staticmethod\n    def _base_request_id(\n        raw_request: Request | None, default: str | None = None\n    ) -> str | None:\n        \"\"\"Pulls the request id to use from a header, if provided\"\"\"\n        if raw_request is not None and (\n            (req_id := raw_request.headers.get(\"X-Request-Id\")) is not None\n        ):\n            return req_id\n\n        return random_uuid() if default is None else default\n\n    @staticmethod\n    def _get_data_parallel_rank(raw_request: Request | None) -> int | None:\n        \"\"\"Pulls the data parallel rank from a header, if provided\"\"\"\n        if raw_request is None:\n            return None\n\n        rank_str = raw_request.headers.get(\"X-data-parallel-rank\")\n        if rank_str is None:\n            return None\n\n        try:\n            return int(rank_str)\n        except ValueError:\n            return None\n\n    @staticmethod\n    def _parse_tool_calls_from_content(\n        request: ResponsesRequest | ChatCompletionRequest,\n        tokenizer: TokenizerLike | None,\n        enable_auto_tools: bool,\n        tool_parser_cls: Callable[[TokenizerLike], ToolParser] | None,\n        content: str | None = None,\n    ) -> tuple[list[FunctionCall] | None, str | None]:\n        function_calls = list[FunctionCall]()\n        if request.tool_choice and isinstance(request.tool_choice, ToolChoiceFunction):\n            assert content is not None\n            # Forced Function Call\n            function_calls.append(\n                FunctionCall(name=request.tool_choice.name, arguments=content)\n            )\n            content = None  # Clear content since tool is called.\n        elif request.tool_choice and isinstance(\n            request.tool_choice, ChatCompletionNamedToolChoiceParam\n        ):\n            assert content is not None\n            # Forced Function Call\n            function_calls.append(\n                FunctionCall(name=request.tool_choice.function.name, arguments=content)\n            )\n            content = None  # Clear content since tool is called.\n        elif request.tool_choice == \"required\":\n            assert content is not None\n            tool_calls = TypeAdapter(list[FunctionDefinition]).validate_json(content)\n            function_calls.extend(\n                [\n                    FunctionCall(\n                        name=tool_call.name,\n                        arguments=json.dumps(tool_call.parameters, ensure_ascii=False),\n                    )\n                    for tool_call in tool_calls\n                ]\n            )\n            content = None  # Clear content since tool is called.\n        elif (\n            tool_parser_cls\n            and enable_auto_tools\n            and (request.tool_choice == \"auto\" or request.tool_choice is None)\n        ):\n            if tokenizer is None:\n                raise ValueError(\n                    \"Tokenizer not available when `skip_tokenizer_init=True`\"\n                )\n\n            # Automatic Tool Call Parsing\n            try:\n                tool_parser = tool_parser_cls(tokenizer)\n            except RuntimeError as e:\n                logger.exception(\"Error in tool parser creation.\")\n                raise e\n            tool_call_info = tool_parser.extract_tool_calls(\n                content if content is not None else \"\",\n                request=request,  # type: ignore\n            )\n            if tool_call_info is not None and tool_call_info.tools_called:\n                # extract_tool_calls() returns a list of tool calls.\n                function_calls.extend(\n                    FunctionCall(\n                        name=tool_call.function.name,\n                        arguments=tool_call.function.arguments,\n                    )\n                    for tool_call in tool_call_info.tool_calls\n                )\n                content = tool_call_info.content\n                if content and content.strip() == \"\":\n                    content = None\n            else:\n                # No tool calls.\n                return None, content\n\n        return function_calls, content\n\n    @staticmethod\n    def _get_decoded_token(\n        logprob: Logprob,\n        token_id: int,\n        tokenizer: TokenizerLike | None,\n        return_as_token_id: bool = False,\n    ) -> str:\n        if return_as_token_id:\n            return f\"token_id:{token_id}\"\n\n        if logprob.decoded_token is not None:\n            return logprob.decoded_token\n\n        if tokenizer is None:\n            raise ValueError(\n                \"Unable to get tokenizer because `skip_tokenizer_init=True`\"\n            )\n\n        return tokenizer.decode(token_id)\n\n    def _is_model_supported(self, model_name: str | None) -> bool:\n        if not model_name:\n            return True\n        return self.models.is_base_model(model_name)",
      "language": "python"
    },
    {
      "code": "_apply_mistral_chat_template_async = make_async(\n    apply_mistral_chat_template,\n    executor=_tokenizer_executor,\n)",
      "language": "unknown"
    },
    {
      "code": "_apply_mistral_chat_template_async = make_async(\n    apply_mistral_chat_template,\n    executor=_tokenizer_executor,\n)",
      "language": "unknown"
    },
    {
      "code": "_async_tokenizer_pool: dict[\n    TokenizerLike, AsyncMicrobatchTokenizer\n] = {}",
      "language": "yaml"
    },
    {
      "code": "_async_tokenizer_pool: dict[\n    TokenizerLike, AsyncMicrobatchTokenizer\n] = {}",
      "language": "yaml"
    },
    {
      "code": "_tokenizer_executor = ThreadPoolExecutor(max_workers=1)",
      "language": "unknown"
    },
    {
      "code": "_tokenizer_executor = ThreadPoolExecutor(max_workers=1)",
      "language": "unknown"
    },
    {
      "code": "engine_client = engine_client",
      "language": "unknown"
    },
    {
      "code": "engine_client = engine_client",
      "language": "unknown"
    },
    {
      "code": "input_processor = input_processor",
      "language": "unknown"
    },
    {
      "code": "input_processor = input_processor",
      "language": "unknown"
    },
    {
      "code": "io_processor = io_processor",
      "language": "unknown"
    },
    {
      "code": "io_processor = io_processor",
      "language": "unknown"
    },
    {
      "code": "log_error_stack = log_error_stack",
      "language": "unknown"
    },
    {
      "code": "log_error_stack = log_error_stack",
      "language": "unknown"
    },
    {
      "code": "max_model_len = max_model_len",
      "language": "unknown"
    },
    {
      "code": "max_model_len = max_model_len",
      "language": "unknown"
    },
    {
      "code": "model_config = model_config",
      "language": "unknown"
    },
    {
      "code": "model_config = model_config",
      "language": "unknown"
    },
    {
      "code": "models = models",
      "language": "unknown"
    },
    {
      "code": "models = models",
      "language": "unknown"
    },
    {
      "code": "request_id_prefix: str = '\\n    A short string prepended to every request’s ID (e.g. \"embd\", \"classify\")\\n    so you can easily tell “this ID came from Embedding vs Classification.”\\n    '",
      "language": "typescript"
    },
    {
      "code": "request_id_prefix: str = '\\n    A short string prepended to every request’s ID (e.g. \"embd\", \"classify\")\\n    so you can easily tell “this ID came from Embedding vs Classification.”\\n    '",
      "language": "typescript"
    },
    {
      "code": "request_logger = request_logger",
      "language": "unknown"
    },
    {
      "code": "request_logger = request_logger",
      "language": "unknown"
    },
    {
      "code": "return_tokens_as_token_ids = return_tokens_as_token_ids",
      "language": "unknown"
    },
    {
      "code": "return_tokens_as_token_ids = return_tokens_as_token_ids",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    engine_client: EngineClient,\n    models: OpenAIServingModels,\n    *,\n    request_logger: RequestLogger | None,\n    return_tokens_as_token_ids: bool = False,\n    log_error_stack: bool = False,\n)",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    engine_client: EngineClient,\n    models: OpenAIServingModels,\n    *,\n    request_logger: RequestLogger | None,\n    return_tokens_as_token_ids: bool = False,\n    log_error_stack: bool = False,\n)",
      "language": "typescript"
    },
    {
      "code": "234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    engine_client: EngineClient,\n    models: OpenAIServingModels,\n    *,\n    request_logger: RequestLogger | None,\n    return_tokens_as_token_ids: bool = False,\n    log_error_stack: bool = False,\n):\n    super().__init__()\n\n    self.engine_client = engine_client\n\n    self.models = models\n\n    self.request_logger = request_logger\n    self.return_tokens_as_token_ids = return_tokens_as_token_ids\n    self._tokenizer_executor = ThreadPoolExecutor(max_workers=1)\n    self._apply_mistral_chat_template_async = make_async(\n        apply_mistral_chat_template, executor=self._tokenizer_executor\n    )\n\n    self._async_tokenizer_pool: dict[TokenizerLike, AsyncMicrobatchTokenizer] = {}\n    self.log_error_stack = log_error_stack\n\n    self.input_processor = self.models.input_processor\n    self.io_processor = self.models.io_processor\n    self.model_config = self.models.model_config\n    self.max_model_len = self.model_config.max_model_len",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    engine_client: EngineClient,\n    models: OpenAIServingModels,\n    *,\n    request_logger: RequestLogger | None,\n    return_tokens_as_token_ids: bool = False,\n    log_error_stack: bool = False,\n):\n    super().__init__()\n\n    self.engine_client = engine_client\n\n    self.models = models\n\n    self.request_logger = request_logger\n    self.return_tokens_as_token_ids = return_tokens_as_token_ids\n    self._tokenizer_executor = ThreadPoolExecutor(max_workers=1)\n    self._apply_mistral_chat_template_async = make_async(\n        apply_mistral_chat_template, executor=self._tokenizer_executor\n    )\n\n    self._async_tokenizer_pool: dict[TokenizerLike, AsyncMicrobatchTokenizer] = {}\n    self.log_error_stack = log_error_stack\n\n    self.input_processor = self.models.input_processor\n    self.io_processor = self.models.io_processor\n    self.model_config = self.models.model_config\n    self.max_model_len = self.model_config.max_model_len",
      "language": "python"
    },
    {
      "code": "_base_request_id(\n    raw_request: Request | None, default: str | None = None\n) -> str | None",
      "language": "rust"
    },
    {
      "code": "_base_request_id(\n    raw_request: Request | None, default: str | None = None\n) -> str | None",
      "language": "rust"
    },
    {
      "code": "1450\n1451\n1452\n1453\n1454\n1455\n1456\n1457\n1458\n1459\n1460",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef _base_request_id(\n    raw_request: Request | None, default: str | None = None\n) -> str | None:\n    \"\"\"Pulls the request id to use from a header, if provided\"\"\"\n    if raw_request is not None and (\n        (req_id := raw_request.headers.get(\"X-Request-Id\")) is not None\n    ):\n        return req_id\n\n    return random_uuid() if default is None else default",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef _base_request_id(\n    raw_request: Request | None, default: str | None = None\n) -> str | None:\n    \"\"\"Pulls the request id to use from a header, if provided\"\"\"\n    if raw_request is not None and (\n        (req_id := raw_request.headers.get(\"X-Request-Id\")) is not None\n    ):\n        return req_id\n\n    return random_uuid() if default is None else default",
      "language": "python"
    },
    {
      "code": "_build_render_config(request: Any) -> RenderConfig",
      "language": "php"
    },
    {
      "code": "_build_render_config(request: Any) -> RenderConfig",
      "language": "php"
    },
    {
      "code": "558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569",
      "language": "unknown"
    },
    {
      "code": "def _build_render_config(\n    self,\n    request: Any,\n) -> RenderConfig:\n    \"\"\"\n    Build and return a `RenderConfig` for an endpoint.\n\n    Used by the renderer to control how prompts are prepared\n    (e.g., tokenization and length handling). Endpoints should\n    implement this with logic appropriate to their request type.\n    \"\"\"\n    raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "def _build_render_config(\n    self,\n    request: Any,\n) -> RenderConfig:\n    \"\"\"\n    Build and return a `RenderConfig` for an endpoint.\n\n    Used by the renderer to control how prompts are prepared\n    (e.g., tokenization and length handling). Endpoints should\n    implement this with logic appropriate to their request type.\n    \"\"\"\n    raise NotImplementedError",
      "language": "python"
    },
    {
      "code": "_build_response(\n    ctx: ServeContext,\n) -> AnyResponse | ErrorResponse",
      "language": "php"
    },
    {
      "code": "_build_response(\n    ctx: ServeContext,\n) -> AnyResponse | ErrorResponse",
      "language": "php"
    },
    {
      "code": "592\n593\n594\n595\n596\n597\n598\n599\n600",
      "language": "unknown"
    },
    {
      "code": "def _build_response(\n    self,\n    ctx: ServeContext,\n) -> AnyResponse | ErrorResponse:\n    \"\"\"\n    Default response builder. Subclass may override this method\n    to return the appropriate response object.\n    \"\"\"\n    return self.create_error_response(\"unimplemented endpoint\")",
      "language": "python"
    },
    {
      "code": "def _build_response(\n    self,\n    ctx: ServeContext,\n) -> AnyResponse | ErrorResponse:\n    \"\"\"\n    Default response builder. Subclass may override this method\n    to return the appropriate response object.\n    \"\"\"\n    return self.create_error_response(\"unimplemented endpoint\")",
      "language": "python"
    },
    {
      "code": "_check_model(request: AnyRequest) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "_check_model(request: AnyRequest) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869",
      "language": "unknown"
    },
    {
      "code": "async def _check_model(\n    self,\n    request: AnyRequest,\n) -> ErrorResponse | None:\n    error_response = None\n\n    if self._is_model_supported(request.model):\n        return None\n    if request.model in self.models.lora_requests:\n        return None\n    if (\n        envs.VLLM_ALLOW_RUNTIME_LORA_UPDATING\n        and request.model\n        and (load_result := await self.models.resolve_lora(request.model))\n    ):\n        if isinstance(load_result, LoRARequest):\n            return None\n        if (\n            isinstance(load_result, ErrorResponse)\n            and load_result.error.code == HTTPStatus.BAD_REQUEST.value\n        ):\n            error_response = load_result\n\n    return error_response or self.create_error_response(\n        message=f\"The model `{request.model}` does not exist.\",\n        err_type=\"NotFoundError\",\n        status_code=HTTPStatus.NOT_FOUND,\n        param=\"model\",\n    )",
      "language": "python"
    },
    {
      "code": "async def _check_model(\n    self,\n    request: AnyRequest,\n) -> ErrorResponse | None:\n    error_response = None\n\n    if self._is_model_supported(request.model):\n        return None\n    if request.model in self.models.lora_requests:\n        return None\n    if (\n        envs.VLLM_ALLOW_RUNTIME_LORA_UPDATING\n        and request.model\n        and (load_result := await self.models.resolve_lora(request.model))\n    ):\n        if isinstance(load_result, LoRARequest):\n            return None\n        if (\n            isinstance(load_result, ErrorResponse)\n            and load_result.error.code == HTTPStatus.BAD_REQUEST.value\n        ):\n            error_response = load_result\n\n    return error_response or self.create_error_response(\n        message=f\"The model `{request.model}` does not exist.\",\n        err_type=\"NotFoundError\",\n        status_code=HTTPStatus.NOT_FOUND,\n        param=\"model\",\n    )",
      "language": "python"
    },
    {
      "code": "_collect_batch(ctx: ServeContext) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "_collect_batch(ctx: ServeContext) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743",
      "language": "unknown"
    },
    {
      "code": "async def _collect_batch(\n    self,\n    ctx: ServeContext,\n) -> ErrorResponse | None:\n    \"\"\"Collect batch results from the result generator.\"\"\"\n    try:\n        if ctx.engine_prompts is None:\n            return self.create_error_response(\"Engine prompts not available\")\n\n        num_prompts = len(ctx.engine_prompts)\n        final_res_batch: list[RequestOutput | PoolingRequestOutput | None]\n        final_res_batch = [None] * num_prompts\n\n        if ctx.result_generator is None:\n            return self.create_error_response(\"Result generator not available\")\n\n        async for i, res in ctx.result_generator:\n            final_res_batch[i] = res\n\n        if None in final_res_batch:\n            return self.create_error_response(\n                \"Failed to generate results for all prompts\"\n            )\n\n        ctx.final_res_batch = [res for res in final_res_batch if res is not None]\n\n        return None\n\n    except Exception as e:\n        return self.create_error_response(e)",
      "language": "python"
    },
    {
      "code": "async def _collect_batch(\n    self,\n    ctx: ServeContext,\n) -> ErrorResponse | None:\n    \"\"\"Collect batch results from the result generator.\"\"\"\n    try:\n        if ctx.engine_prompts is None:\n            return self.create_error_response(\"Engine prompts not available\")\n\n        num_prompts = len(ctx.engine_prompts)\n        final_res_batch: list[RequestOutput | PoolingRequestOutput | None]\n        final_res_batch = [None] * num_prompts\n\n        if ctx.result_generator is None:\n            return self.create_error_response(\"Result generator not available\")\n\n        async for i, res in ctx.result_generator:\n            final_res_batch[i] = res\n\n        if None in final_res_batch:\n            return self.create_error_response(\n                \"Failed to generate results for all prompts\"\n            )\n\n        ctx.final_res_batch = [res for res in final_res_batch if res is not None]\n\n        return None\n\n    except Exception as e:\n        return self.create_error_response(e)",
      "language": "python"
    },
    {
      "code": "_convert_generation_error_to_response(\n    e: GenerationError,\n) -> ErrorResponse",
      "language": "php"
    },
    {
      "code": "_convert_generation_error_to_response(\n    e: GenerationError,\n) -> ErrorResponse",
      "language": "php"
    },
    {
      "code": "821\n822\n823\n824\n825\n826\n827\n828\n829",
      "language": "unknown"
    },
    {
      "code": "def _convert_generation_error_to_response(\n    self, e: GenerationError\n) -> ErrorResponse:\n    \"\"\"Convert GenerationError to ErrorResponse.\"\"\"\n    return self.create_error_response(\n        str(e),\n        err_type=\"InternalServerError\",\n        status_code=e.status_code,\n    )",
      "language": "python"
    },
    {
      "code": "def _convert_generation_error_to_response(\n    self, e: GenerationError\n) -> ErrorResponse:\n    \"\"\"Convert GenerationError to ErrorResponse.\"\"\"\n    return self.create_error_response(\n        str(e),\n        err_type=\"InternalServerError\",\n        status_code=e.status_code,\n    )",
      "language": "python"
    },
    {
      "code": "_convert_generation_error_to_streaming_response(\n    e: GenerationError,\n) -> str",
      "language": "php"
    },
    {
      "code": "_convert_generation_error_to_streaming_response(\n    e: GenerationError,\n) -> str",
      "language": "php"
    },
    {
      "code": "831\n832\n833\n834\n835\n836\n837\n838\n839",
      "language": "unknown"
    },
    {
      "code": "def _convert_generation_error_to_streaming_response(\n    self, e: GenerationError\n) -> str:\n    \"\"\"Convert GenerationError to streaming error response.\"\"\"\n    return self.create_streaming_error_response(\n        str(e),\n        err_type=\"InternalServerError\",\n        status_code=e.status_code,\n    )",
      "language": "python"
    },
    {
      "code": "def _convert_generation_error_to_streaming_response(\n    self, e: GenerationError\n) -> str:\n    \"\"\"Convert GenerationError to streaming error response.\"\"\"\n    return self.create_streaming_error_response(\n        str(e),\n        err_type=\"InternalServerError\",\n        status_code=e.status_code,\n    )",
      "language": "python"
    },
    {
      "code": "_create_pooling_params(\n    ctx: ServeContext,\n) -> PoolingParams | ErrorResponse",
      "language": "php"
    },
    {
      "code": "_create_pooling_params(\n    ctx: ServeContext,\n) -> PoolingParams | ErrorResponse",
      "language": "php"
    },
    {
      "code": "652\n653\n654\n655\n656\n657\n658\n659\n660\n661",
      "language": "unknown"
    },
    {
      "code": "def _create_pooling_params(\n    self,\n    ctx: ServeContext,\n) -> PoolingParams | ErrorResponse:\n    if not hasattr(ctx.request, \"to_pooling_params\"):\n        return self.create_error_response(\n            \"Request type does not support pooling parameters\"\n        )\n\n    return ctx.request.to_pooling_params()",
      "language": "python"
    },
    {
      "code": "def _create_pooling_params(\n    self,\n    ctx: ServeContext,\n) -> PoolingParams | ErrorResponse:\n    if not hasattr(ctx.request, \"to_pooling_params\"):\n        return self.create_error_response(\n            \"Request type does not support pooling parameters\"\n        )\n\n    return ctx.request.to_pooling_params()",
      "language": "python"
    },
    {
      "code": "_generate_with_builtin_tools(\n    request_id: str,\n    engine_prompt: TokensPrompt,\n    sampling_params: SamplingParams,\n    context: ConversationContext,\n    lora_request: LoRARequest | None = None,\n    priority: int = 0,\n    **kwargs,\n)",
      "language": "typescript"
    },
    {
      "code": "_generate_with_builtin_tools(\n    request_id: str,\n    engine_prompt: TokensPrompt,\n    sampling_params: SamplingParams,\n    context: ConversationContext,\n    lora_request: LoRARequest | None = None,\n    priority: int = 0,\n    **kwargs,\n)",
      "language": "typescript"
    },
    {
      "code": "1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406\n1407\n1408\n1409\n1410",
      "language": "unknown"
    },
    {
      "code": "async def _generate_with_builtin_tools(\n    self,\n    request_id: str,\n    engine_prompt: TokensPrompt,\n    sampling_params: SamplingParams,\n    context: ConversationContext,\n    lora_request: LoRARequest | None = None,\n    priority: int = 0,\n    **kwargs,\n):\n    prompt_text, _, _ = self._get_prompt_components(engine_prompt)\n\n    orig_priority = priority\n    sub_request = 0\n    while True:\n        # Ensure that each sub-request has a unique request id.\n        sub_request_id = f\"{request_id}_{sub_request}\"\n        self._log_inputs(\n            sub_request_id,\n            engine_prompt,\n            params=sampling_params,\n            lora_request=lora_request,\n        )\n        trace_headers = kwargs.get(\"trace_headers\")\n        engine_request, tokenization_kwargs = await self._process_inputs(\n            sub_request_id,\n            engine_prompt,\n            sampling_params,\n            lora_request=lora_request,\n            trace_headers=trace_headers,\n            priority=priority,\n        )\n\n        generator = self.engine_client.generate(\n            engine_request,\n            sampling_params,\n            sub_request_id,\n            lora_request=lora_request,\n            priority=priority,\n            prompt_text=prompt_text,\n            tokenization_kwargs=tokenization_kwargs,\n            **kwargs,\n        )\n\n        async for res in generator:\n            context.append_output(res)\n            # NOTE(woosuk): The stop condition is handled by the engine.\n            yield context\n\n        if not context.need_builtin_tool_call():\n            # The model did not ask for a tool call, so we're done.\n            break\n\n        # Call the tool and update the context with the result.\n        tool_output = await context.call_tool()\n        context.append_tool_output(tool_output)\n\n        # TODO: uncomment this and enable tool output streaming\n        # yield context\n\n        # Create inputs for the next turn.\n        # Render the next prompt token ids.\n        if isinstance(context, (HarmonyContext, StreamingHarmonyContext)):\n            prompt_token_ids = context.render_for_completion()\n            engine_prompt = TokensPrompt(prompt_token_ids=prompt_token_ids)\n        elif isinstance(context, ParsableContext):\n            engine_prompts = await self._render_next_turn(\n                context.request,\n                context.tokenizer,\n                context.parser.response_messages,\n                context.tool_dicts,\n                context.tool_parser_cls,\n                context.chat_template,\n                context.chat_template_content_format,\n            )\n            engine_prompt = engine_prompts[0]\n            prompt_text, _, _ = self._get_prompt_components(engine_prompt)\n\n        # Update the sampling params.\n        sampling_params.max_tokens = self.max_model_len - len(\n            engine_prompt[\"prompt_token_ids\"]\n        )\n        # OPTIMIZATION\n        priority = orig_priority - 1\n        sub_request += 1",
      "language": "python"
    },
    {
      "code": "async def _generate_with_builtin_tools(\n    self,\n    request_id: str,\n    engine_prompt: TokensPrompt,\n    sampling_params: SamplingParams,\n    context: ConversationContext,\n    lora_request: LoRARequest | None = None,\n    priority: int = 0,\n    **kwargs,\n):\n    prompt_text, _, _ = self._get_prompt_components(engine_prompt)\n\n    orig_priority = priority\n    sub_request = 0\n    while True:\n        # Ensure that each sub-request has a unique request id.\n        sub_request_id = f\"{request_id}_{sub_request}\"\n        self._log_inputs(\n            sub_request_id,\n            engine_prompt,\n            params=sampling_params,\n            lora_request=lora_request,\n        )\n        trace_headers = kwargs.get(\"trace_headers\")\n        engine_request, tokenization_kwargs = await self._process_inputs(\n            sub_request_id,\n            engine_prompt,\n            sampling_params,\n            lora_request=lora_request,\n            trace_headers=trace_headers,\n            priority=priority,\n        )\n\n        generator = self.engine_client.generate(\n            engine_request,\n            sampling_params,\n            sub_request_id,\n            lora_request=lora_request,\n            priority=priority,\n            prompt_text=prompt_text,\n            tokenization_kwargs=tokenization_kwargs,\n            **kwargs,\n        )\n\n        async for res in generator:\n            context.append_output(res)\n            # NOTE(woosuk): The stop condition is handled by the engine.\n            yield context\n\n        if not context.need_builtin_tool_call():\n            # The model did not ask for a tool call, so we're done.\n            break\n\n        # Call the tool and update the context with the result.\n        tool_output = await context.call_tool()\n        context.append_tool_output(tool_output)\n\n        # TODO: uncomment this and enable tool output streaming\n        # yield context\n\n        # Create inputs for the next turn.\n        # Render the next prompt token ids.\n        if isinstance(context, (HarmonyContext, StreamingHarmonyContext)):\n            prompt_token_ids = context.render_for_completion()\n            engine_prompt = TokensPrompt(prompt_token_ids=prompt_token_ids)\n        elif isinstance(context, ParsableContext):\n            engine_prompts = await self._render_next_turn(\n                context.request,\n                context.tokenizer,\n                context.parser.response_messages,\n                context.tool_dicts,\n                context.tool_parser_cls,\n                context.chat_template,\n                context.chat_template_content_format,\n            )\n            engine_prompt = engine_prompts[0]\n            prompt_text, _, _ = self._get_prompt_components(engine_prompt)\n\n        # Update the sampling params.\n        sampling_params.max_tokens = self.max_model_len - len(\n            engine_prompt[\"prompt_token_ids\"]\n        )\n        # OPTIMIZATION\n        priority = orig_priority - 1\n        sub_request += 1",
      "language": "python"
    },
    {
      "code": "_get_active_default_mm_loras(\n    request: AnyRequest,\n) -> LoRARequest | None",
      "language": "rust"
    },
    {
      "code": "_get_active_default_mm_loras(\n    request: AnyRequest,\n) -> LoRARequest | None",
      "language": "rust"
    },
    {
      "code": "871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892",
      "language": "unknown"
    },
    {
      "code": "def _get_active_default_mm_loras(self, request: AnyRequest) -> LoRARequest | None:\n    \"\"\"Determine if there are any active default multimodal loras.\"\"\"\n    # TODO: Currently this is only enabled for chat completions\n    # to be better aligned with only being enabled for .generate\n    # when run offline. It would be nice to support additional\n    # tasks types in the future.\n    message_types = self._get_message_types(request)\n    default_mm_loras = set()\n\n    for lora in self.models.lora_requests.values():\n        # Best effort match for default multimodal lora adapters;\n        # There is probably a better way to do this, but currently\n        # this matches against the set of 'types' in any content lists\n        # up until '_', e.g., to match audio_url -> audio\n        if lora.lora_name in message_types:\n            default_mm_loras.add(lora)\n\n    # Currently only support default modality specific loras if\n    # we have exactly one lora matched on the request.\n    if len(default_mm_loras) == 1:\n        return default_mm_loras.pop()\n    return None",
      "language": "python"
    },
    {
      "code": "def _get_active_default_mm_loras(self, request: AnyRequest) -> LoRARequest | None:\n    \"\"\"Determine if there are any active default multimodal loras.\"\"\"\n    # TODO: Currently this is only enabled for chat completions\n    # to be better aligned with only being enabled for .generate\n    # when run offline. It would be nice to support additional\n    # tasks types in the future.\n    message_types = self._get_message_types(request)\n    default_mm_loras = set()\n\n    for lora in self.models.lora_requests.values():\n        # Best effort match for default multimodal lora adapters;\n        # There is probably a better way to do this, but currently\n        # this matches against the set of 'types' in any content lists\n        # up until '_', e.g., to match audio_url -> audio\n        if lora.lora_name in message_types:\n            default_mm_loras.add(lora)\n\n    # Currently only support default modality specific loras if\n    # we have exactly one lora matched on the request.\n    if len(default_mm_loras) == 1:\n        return default_mm_loras.pop()\n    return None",
      "language": "python"
    },
    {
      "code": "_get_async_tokenizer(tokenizer) -> AsyncMicrobatchTokenizer",
      "language": "php"
    },
    {
      "code": "_get_async_tokenizer(tokenizer) -> AsyncMicrobatchTokenizer",
      "language": "php"
    },
    {
      "code": "571\n572\n573\n574\n575\n576\n577\n578\n579\n580",
      "language": "unknown"
    },
    {
      "code": "def _get_async_tokenizer(self, tokenizer) -> AsyncMicrobatchTokenizer:\n    \"\"\"\n    Return (and cache) an `AsyncMicrobatchTokenizer` bound to the\n    given tokenizer.\n    \"\"\"\n    async_tokenizer = self._async_tokenizer_pool.get(tokenizer)\n    if async_tokenizer is None:\n        async_tokenizer = AsyncMicrobatchTokenizer(tokenizer)\n        self._async_tokenizer_pool[tokenizer] = async_tokenizer\n    return async_tokenizer",
      "language": "python"
    },
    {
      "code": "def _get_async_tokenizer(self, tokenizer) -> AsyncMicrobatchTokenizer:\n    \"\"\"\n    Return (and cache) an `AsyncMicrobatchTokenizer` bound to the\n    given tokenizer.\n    \"\"\"\n    async_tokenizer = self._async_tokenizer_pool.get(tokenizer)\n    if async_tokenizer is None:\n        async_tokenizer = AsyncMicrobatchTokenizer(tokenizer)\n        self._async_tokenizer_pool[tokenizer] = async_tokenizer\n    return async_tokenizer",
      "language": "python"
    },
    {
      "code": "_get_data_parallel_rank(\n    raw_request: Request | None,\n) -> int | None",
      "language": "rust"
    },
    {
      "code": "_get_data_parallel_rank(\n    raw_request: Request | None,\n) -> int | None",
      "language": "rust"
    },
    {
      "code": "1462\n1463\n1464\n1465\n1466\n1467\n1468\n1469\n1470\n1471\n1472\n1473\n1474\n1475",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef _get_data_parallel_rank(raw_request: Request | None) -> int | None:\n    \"\"\"Pulls the data parallel rank from a header, if provided\"\"\"\n    if raw_request is None:\n        return None\n\n    rank_str = raw_request.headers.get(\"X-data-parallel-rank\")\n    if rank_str is None:\n        return None\n\n    try:\n        return int(rank_str)\n    except ValueError:\n        return None",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef _get_data_parallel_rank(raw_request: Request | None) -> int | None:\n    \"\"\"Pulls the data parallel rank from a header, if provided\"\"\"\n    if raw_request is None:\n        return None\n\n    rank_str = raw_request.headers.get(\"X-data-parallel-rank\")\n    if rank_str is None:\n        return None\n\n    try:\n        return int(rank_str)\n    except ValueError:\n        return None",
      "language": "python"
    },
    {
      "code": "_get_decoded_token(\n    logprob: Logprob,\n    token_id: int,\n    tokenizer: TokenizerLike | None,\n    return_as_token_id: bool = False,\n) -> str",
      "language": "typescript"
    },
    {
      "code": "_get_decoded_token(\n    logprob: Logprob,\n    token_id: int,\n    tokenizer: TokenizerLike | None,\n    return_as_token_id: bool = False,\n) -> str",
      "language": "typescript"
    },
    {
      "code": "1553\n1554\n1555\n1556\n1557\n1558\n1559\n1560\n1561\n1562\n1563\n1564\n1565\n1566\n1567\n1568\n1569\n1570\n1571",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef _get_decoded_token(\n    logprob: Logprob,\n    token_id: int,\n    tokenizer: TokenizerLike | None,\n    return_as_token_id: bool = False,\n) -> str:\n    if return_as_token_id:\n        return f\"token_id:{token_id}\"\n\n    if logprob.decoded_token is not None:\n        return logprob.decoded_token\n\n    if tokenizer is None:\n        raise ValueError(\n            \"Unable to get tokenizer because `skip_tokenizer_init=True`\"\n        )\n\n    return tokenizer.decode(token_id)",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef _get_decoded_token(\n    logprob: Logprob,\n    token_id: int,\n    tokenizer: TokenizerLike | None,\n    return_as_token_id: bool = False,\n) -> str:\n    if return_as_token_id:\n        return f\"token_id:{token_id}\"\n\n    if logprob.decoded_token is not None:\n        return logprob.decoded_token\n\n    if tokenizer is None:\n        raise ValueError(\n            \"Unable to get tokenizer because `skip_tokenizer_init=True`\"\n        )\n\n    return tokenizer.decode(token_id)",
      "language": "python"
    },
    {
      "code": "_get_message_types(request: AnyRequest) -> set[str]",
      "language": "php"
    },
    {
      "code": "_get_message_types(request: AnyRequest) -> set[str]",
      "language": "php"
    },
    {
      "code": "915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938",
      "language": "unknown"
    },
    {
      "code": "def _get_message_types(self, request: AnyRequest) -> set[str]:\n    \"\"\"Retrieve the set of types from message content dicts up\n    until `_`; we use this to match potential multimodal data\n    with default per modality loras.\n    \"\"\"\n    message_types: set[str] = set()\n\n    if not hasattr(request, \"messages\"):\n        return message_types\n\n    messages = request.messages\n    if messages is None or isinstance(messages, (str, bytes)):\n        return message_types\n\n    for message in messages:\n        if (\n            isinstance(message, dict)\n            and \"content\" in message\n            and isinstance(message[\"content\"], list)\n        ):\n            for content_dict in message[\"content\"]:\n                if \"type\" in content_dict:\n                    message_types.add(content_dict[\"type\"].split(\"_\")[0])\n    return message_types",
      "language": "python"
    },
    {
      "code": "def _get_message_types(self, request: AnyRequest) -> set[str]:\n    \"\"\"Retrieve the set of types from message content dicts up\n    until `_`; we use this to match potential multimodal data\n    with default per modality loras.\n    \"\"\"\n    message_types: set[str] = set()\n\n    if not hasattr(request, \"messages\"):\n        return message_types\n\n    messages = request.messages\n    if messages is None or isinstance(messages, (str, bytes)):\n        return message_types\n\n    for message in messages:\n        if (\n            isinstance(message, dict)\n            and \"content\" in message\n            and isinstance(message[\"content\"], list)\n        ):\n            for content_dict in message[\"content\"]:\n                if \"type\" in content_dict:\n                    message_types.add(content_dict[\"type\"].split(\"_\")[0])\n    return message_types",
      "language": "python"
    },
    {
      "code": "_get_prompt_components(\n    prompt: PromptType,\n) -> PromptComponents",
      "language": "php"
    },
    {
      "code": "_get_prompt_components(\n    prompt: PromptType,\n) -> PromptComponents",
      "language": "php"
    },
    {
      "code": "def _get_prompt_components(self, prompt: PromptType) -> PromptComponents:\n    return get_prompt_components(prompt)",
      "language": "python"
    },
    {
      "code": "def _get_prompt_components(self, prompt: PromptType) -> PromptComponents:\n    return get_prompt_components(prompt)",
      "language": "python"
    },
    {
      "code": "_get_reasoning_parser(\n    reasoning_parser_name: str,\n) -> Callable[[TokenizerLike], ReasoningParser] | None",
      "language": "rust"
    },
    {
      "code": "_get_reasoning_parser(\n    reasoning_parser_name: str,\n) -> Callable[[TokenizerLike], ReasoningParser] | None",
      "language": "rust"
    },
    {
      "code": "289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302",
      "language": "unknown"
    },
    {
      "code": "def _get_reasoning_parser(\n    self,\n    reasoning_parser_name: str,\n) -> Callable[[TokenizerLike], ReasoningParser] | None:\n    \"\"\"Get the reasoning parser based on the name.\"\"\"\n    parser = None\n    if not reasoning_parser_name:\n        return None\n    try:\n        parser = ReasoningParserManager.get_reasoning_parser(reasoning_parser_name)\n        assert parser is not None\n    except Exception as e:\n        raise TypeError(f\"{reasoning_parser_name=} has not been registered\") from e\n    return parser",
      "language": "typescript"
    },
    {
      "code": "def _get_reasoning_parser(\n    self,\n    reasoning_parser_name: str,\n) -> Callable[[TokenizerLike], ReasoningParser] | None:\n    \"\"\"Get the reasoning parser based on the name.\"\"\"\n    parser = None\n    if not reasoning_parser_name:\n        return None\n    try:\n        parser = ReasoningParserManager.get_reasoning_parser(reasoning_parser_name)\n        assert parser is not None\n    except Exception as e:\n        raise TypeError(f\"{reasoning_parser_name=} has not been registered\") from e\n    return parser",
      "language": "typescript"
    },
    {
      "code": "_get_renderer(\n    tokenizer: TokenizerLike | None,\n) -> BaseRenderer",
      "language": "rust"
    },
    {
      "code": "_get_renderer(\n    tokenizer: TokenizerLike | None,\n) -> BaseRenderer",
      "language": "rust"
    },
    {
      "code": "547\n548\n549\n550\n551\n552\n553\n554\n555\n556",
      "language": "unknown"
    },
    {
      "code": "def _get_renderer(self, tokenizer: TokenizerLike | None) -> BaseRenderer:\n    \"\"\"\n    Get a Renderer instance with the provided tokenizer.\n    Uses shared async tokenizer pool for efficiency.\n    \"\"\"\n    return CompletionRenderer(\n        model_config=self.model_config,\n        tokenizer=tokenizer,\n        async_tokenizer_pool=self._async_tokenizer_pool,\n    )",
      "language": "python"
    },
    {
      "code": "def _get_renderer(self, tokenizer: TokenizerLike | None) -> BaseRenderer:\n    \"\"\"\n    Get a Renderer instance with the provided tokenizer.\n    Uses shared async tokenizer pool for efficiency.\n    \"\"\"\n    return CompletionRenderer(\n        model_config=self.model_config,\n        tokenizer=tokenizer,\n        async_tokenizer_pool=self._async_tokenizer_pool,\n    )",
      "language": "python"
    },
    {
      "code": "_get_tool_parser(\n    tool_parser_name: str | None = None,\n    enable_auto_tools: bool = False,\n) -> Callable[[TokenizerLike], ToolParser] | None",
      "language": "typescript"
    },
    {
      "code": "_get_tool_parser(\n    tool_parser_name: str | None = None,\n    enable_auto_tools: bool = False,\n) -> Callable[[TokenizerLike], ToolParser] | None",
      "language": "typescript"
    },
    {
      "code": "264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287",
      "language": "unknown"
    },
    {
      "code": "def _get_tool_parser(\n    self, tool_parser_name: str | None = None, enable_auto_tools: bool = False\n) -> Callable[[TokenizerLike], ToolParser] | None:\n    \"\"\"Get the tool parser based on the name.\"\"\"\n    parser = None\n    if not enable_auto_tools or tool_parser_name is None:\n        return parser\n    logger.info('\"auto\" tool choice has been enabled.')\n\n    try:\n        if tool_parser_name == \"pythonic\" and self.model_config.model.startswith(\n            \"meta-llama/Llama-3.2\"\n        ):\n            logger.warning(\n                \"Llama3.2 models may struggle to emit valid pythonic tool calls\"\n            )\n        parser = ToolParserManager.get_tool_parser(tool_parser_name)\n    except Exception as e:\n        raise TypeError(\n            \"Error: --enable-auto-tool-choice requires \"\n            f\"tool_parser:'{tool_parser_name}' which has not \"\n            \"been registered\"\n        ) from e\n    return parser",
      "language": "python"
    },
    {
      "code": "def _get_tool_parser(\n    self, tool_parser_name: str | None = None, enable_auto_tools: bool = False\n) -> Callable[[TokenizerLike], ToolParser] | None:\n    \"\"\"Get the tool parser based on the name.\"\"\"\n    parser = None\n    if not enable_auto_tools or tool_parser_name is None:\n        return parser\n    logger.info('\"auto\" tool choice has been enabled.')\n\n    try:\n        if tool_parser_name == \"pythonic\" and self.model_config.model.startswith(\n            \"meta-llama/Llama-3.2\"\n        ):\n            logger.warning(\n                \"Llama3.2 models may struggle to emit valid pythonic tool calls\"\n            )\n        parser = ToolParserManager.get_tool_parser(tool_parser_name)\n    except Exception as e:\n        raise TypeError(\n            \"Error: --enable-auto-tool-choice requires \"\n            f\"tool_parser:'{tool_parser_name}' which has not \"\n            \"been registered\"\n        ) from e\n    return parser",
      "language": "python"
    },
    {
      "code": "_get_trace_headers(\n    headers: Headers,\n) -> Mapping[str, str] | None",
      "language": "rust"
    },
    {
      "code": "_get_trace_headers(\n    headers: Headers,\n) -> Mapping[str, str] | None",
      "language": "rust"
    },
    {
      "code": "1436\n1437\n1438\n1439\n1440\n1441\n1442\n1443\n1444\n1445\n1446\n1447\n1448",
      "language": "unknown"
    },
    {
      "code": "async def _get_trace_headers(\n    self,\n    headers: Headers,\n) -> Mapping[str, str] | None:\n    is_tracing_enabled = await self.engine_client.is_tracing_enabled()\n\n    if is_tracing_enabled:\n        return extract_trace_headers(headers)\n\n    if contains_trace_headers(headers):\n        log_tracing_disabled_warning()\n\n    return None",
      "language": "python"
    },
    {
      "code": "async def _get_trace_headers(\n    self,\n    headers: Headers,\n) -> Mapping[str, str] | None:\n    is_tracing_enabled = await self.engine_client.is_tracing_enabled()\n\n    if is_tracing_enabled:\n        return extract_trace_headers(headers)\n\n    if contains_trace_headers(headers):\n        log_tracing_disabled_warning()\n\n    return None",
      "language": "python"
    },
    {
      "code": "_is_model_supported(model_name: str | None) -> bool",
      "language": "rust"
    },
    {
      "code": "_is_model_supported(model_name: str | None) -> bool",
      "language": "rust"
    },
    {
      "code": "1573\n1574\n1575\n1576",
      "language": "unknown"
    },
    {
      "code": "def _is_model_supported(self, model_name: str | None) -> bool:\n    if not model_name:\n        return True\n    return self.models.is_base_model(model_name)",
      "language": "python"
    },
    {
      "code": "def _is_model_supported(self, model_name: str | None) -> bool:\n    if not model_name:\n        return True\n    return self.models.is_base_model(model_name)",
      "language": "python"
    },
    {
      "code": "_log_inputs(\n    request_id: str,\n    inputs: PromptType,\n    params: SamplingParams\n    | PoolingParams\n    | BeamSearchParams\n    | None,\n    lora_request: LoRARequest | None,\n) -> None",
      "language": "rust"
    },
    {
      "code": "_log_inputs(\n    request_id: str,\n    inputs: PromptType,\n    params: SamplingParams\n    | PoolingParams\n    | BeamSearchParams\n    | None,\n    lora_request: LoRARequest | None,\n) -> None",
      "language": "rust"
    },
    {
      "code": "1415\n1416\n1417\n1418\n1419\n1420\n1421\n1422\n1423\n1424\n1425\n1426\n1427\n1428\n1429\n1430\n1431\n1432\n1433\n1434",
      "language": "unknown"
    },
    {
      "code": "def _log_inputs(\n    self,\n    request_id: str,\n    inputs: PromptType,\n    params: SamplingParams | PoolingParams | BeamSearchParams | None,\n    lora_request: LoRARequest | None,\n) -> None:\n    if self.request_logger is None:\n        return\n\n    prompt, prompt_token_ids, prompt_embeds = self._get_prompt_components(inputs)\n\n    self.request_logger.log_inputs(\n        request_id,\n        prompt,\n        prompt_token_ids,\n        prompt_embeds,\n        params=params,\n        lora_request=lora_request,\n    )",
      "language": "python"
    },
    {
      "code": "def _log_inputs(\n    self,\n    request_id: str,\n    inputs: PromptType,\n    params: SamplingParams | PoolingParams | BeamSearchParams | None,\n    lora_request: LoRARequest | None,\n) -> None:\n    if self.request_logger is None:\n        return\n\n    prompt, prompt_token_ids, prompt_embeds = self._get_prompt_components(inputs)\n\n    self.request_logger.log_inputs(\n        request_id,\n        prompt,\n        prompt_token_ids,\n        prompt_embeds,\n        params=params,\n        lora_request=lora_request,\n    )",
      "language": "python"
    },
    {
      "code": "_maybe_get_adapters(\n    request: AnyRequest,\n    supports_default_mm_loras: bool = False,\n) -> LoRARequest | None",
      "language": "typescript"
    },
    {
      "code": "_maybe_get_adapters(\n    request: AnyRequest,\n    supports_default_mm_loras: bool = False,\n) -> LoRARequest | None",
      "language": "typescript"
    },
    {
      "code": "894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913",
      "language": "unknown"
    },
    {
      "code": "def _maybe_get_adapters(\n    self,\n    request: AnyRequest,\n    supports_default_mm_loras: bool = False,\n) -> LoRARequest | None:\n    if request.model in self.models.lora_requests:\n        return self.models.lora_requests[request.model]\n\n    # Currently only support default modality specific loras\n    # if we have exactly one lora matched on the request.\n    if supports_default_mm_loras:\n        default_mm_lora = self._get_active_default_mm_loras(request)\n        if default_mm_lora is not None:\n            return default_mm_lora\n\n    if self._is_model_supported(request.model):\n        return None\n\n    # if _check_model has been called earlier, this will be unreachable\n    raise ValueError(f\"The model `{request.model}` does not exist.\")",
      "language": "python"
    },
    {
      "code": "def _maybe_get_adapters(\n    self,\n    request: AnyRequest,\n    supports_default_mm_loras: bool = False,\n) -> LoRARequest | None:\n    if request.model in self.models.lora_requests:\n        return self.models.lora_requests[request.model]\n\n    # Currently only support default modality specific loras\n    # if we have exactly one lora matched on the request.\n    if supports_default_mm_loras:\n        default_mm_lora = self._get_active_default_mm_loras(request)\n        if default_mm_lora is not None:\n            return default_mm_lora\n\n    if self._is_model_supported(request.model):\n        return None\n\n    # if _check_model has been called earlier, this will be unreachable\n    raise ValueError(f\"The model `{request.model}` does not exist.\")",
      "language": "python"
    },
    {
      "code": "_normalize_prompt_text_to_input(\n    request: AnyRequest,\n    prompt: str,\n    tokenizer: TokenizerLike,\n    add_special_tokens: bool,\n) -> TokensPrompt",
      "language": "php"
    },
    {
      "code": "_normalize_prompt_text_to_input(\n    request: AnyRequest,\n    prompt: str,\n    tokenizer: TokenizerLike,\n    add_special_tokens: bool,\n) -> TokensPrompt",
      "language": "php"
    },
    {
      "code": "940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980",
      "language": "unknown"
    },
    {
      "code": "async def _normalize_prompt_text_to_input(\n    self,\n    request: AnyRequest,\n    prompt: str,\n    tokenizer: TokenizerLike,\n    add_special_tokens: bool,\n) -> TokensPrompt:\n    async_tokenizer = self._get_async_tokenizer(tokenizer)\n\n    if (\n        self.model_config.encoder_config is not None\n        and self.model_config.encoder_config.get(\"do_lower_case\", False)\n    ):\n        prompt = prompt.lower()\n\n    truncate_prompt_tokens = getattr(request, \"truncate_prompt_tokens\", None)\n\n    if truncate_prompt_tokens is None:\n        encoded = await async_tokenizer(\n            prompt, add_special_tokens=add_special_tokens\n        )\n    elif truncate_prompt_tokens < 0:\n        # Negative means we cap at the model's max length\n        encoded = await async_tokenizer(\n            prompt,\n            add_special_tokens=add_special_tokens,\n            truncation=True,\n            max_length=self.max_model_len,\n        )\n    else:\n        encoded = await async_tokenizer(\n            prompt,\n            add_special_tokens=add_special_tokens,\n            truncation=True,\n            max_length=truncate_prompt_tokens,\n        )\n\n    input_ids = encoded.input_ids\n    input_text = prompt\n\n    return self._validate_input(request, input_ids, input_text)",
      "language": "python"
    },
    {
      "code": "async def _normalize_prompt_text_to_input(\n    self,\n    request: AnyRequest,\n    prompt: str,\n    tokenizer: TokenizerLike,\n    add_special_tokens: bool,\n) -> TokensPrompt:\n    async_tokenizer = self._get_async_tokenizer(tokenizer)\n\n    if (\n        self.model_config.encoder_config is not None\n        and self.model_config.encoder_config.get(\"do_lower_case\", False)\n    ):\n        prompt = prompt.lower()\n\n    truncate_prompt_tokens = getattr(request, \"truncate_prompt_tokens\", None)\n\n    if truncate_prompt_tokens is None:\n        encoded = await async_tokenizer(\n            prompt, add_special_tokens=add_special_tokens\n        )\n    elif truncate_prompt_tokens < 0:\n        # Negative means we cap at the model's max length\n        encoded = await async_tokenizer(\n            prompt,\n            add_special_tokens=add_special_tokens,\n            truncation=True,\n            max_length=self.max_model_len,\n        )\n    else:\n        encoded = await async_tokenizer(\n            prompt,\n            add_special_tokens=add_special_tokens,\n            truncation=True,\n            max_length=truncate_prompt_tokens,\n        )\n\n    input_ids = encoded.input_ids\n    input_text = prompt\n\n    return self._validate_input(request, input_ids, input_text)",
      "language": "python"
    },
    {
      "code": "_normalize_prompt_tokens_to_input(\n    request: AnyRequest,\n    prompt_ids: list[int],\n    tokenizer: TokenizerLike | None,\n) -> TokensPrompt",
      "language": "rust"
    },
    {
      "code": "_normalize_prompt_tokens_to_input(\n    request: AnyRequest,\n    prompt_ids: list[int],\n    tokenizer: TokenizerLike | None,\n) -> TokensPrompt",
      "language": "rust"
    },
    {
      "code": "982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003",
      "language": "unknown"
    },
    {
      "code": "async def _normalize_prompt_tokens_to_input(\n    self,\n    request: AnyRequest,\n    prompt_ids: list[int],\n    tokenizer: TokenizerLike | None,\n) -> TokensPrompt:\n    truncate_prompt_tokens = getattr(request, \"truncate_prompt_tokens\", None)\n\n    if truncate_prompt_tokens is None:\n        input_ids = prompt_ids\n    elif truncate_prompt_tokens < 0:\n        input_ids = prompt_ids[-self.max_model_len :]\n    else:\n        input_ids = prompt_ids[-truncate_prompt_tokens:]\n\n    if tokenizer is None:\n        input_text = \"\"\n    else:\n        async_tokenizer = self._get_async_tokenizer(tokenizer)\n        input_text = await async_tokenizer.decode(input_ids)\n\n    return self._validate_input(request, input_ids, input_text)",
      "language": "python"
    },
    {
      "code": "async def _normalize_prompt_tokens_to_input(\n    self,\n    request: AnyRequest,\n    prompt_ids: list[int],\n    tokenizer: TokenizerLike | None,\n) -> TokensPrompt:\n    truncate_prompt_tokens = getattr(request, \"truncate_prompt_tokens\", None)\n\n    if truncate_prompt_tokens is None:\n        input_ids = prompt_ids\n    elif truncate_prompt_tokens < 0:\n        input_ids = prompt_ids[-self.max_model_len :]\n    else:\n        input_ids = prompt_ids[-truncate_prompt_tokens:]\n\n    if tokenizer is None:\n        input_text = \"\"\n    else:\n        async_tokenizer = self._get_async_tokenizer(tokenizer)\n        input_text = await async_tokenizer.decode(input_ids)\n\n    return self._validate_input(request, input_ids, input_text)",
      "language": "python"
    },
    {
      "code": "_parse_tool_calls_from_content(\n    request: ResponsesRequest | ChatCompletionRequest,\n    tokenizer: TokenizerLike | None,\n    enable_auto_tools: bool,\n    tool_parser_cls: Callable[[TokenizerLike], ToolParser]\n    | None,\n    content: str | None = None,\n) -> tuple[list[FunctionCall] | None, str | None]",
      "language": "rust"
    },
    {
      "code": "_parse_tool_calls_from_content(\n    request: ResponsesRequest | ChatCompletionRequest,\n    tokenizer: TokenizerLike | None,\n    enable_auto_tools: bool,\n    tool_parser_cls: Callable[[TokenizerLike], ToolParser]\n    | None,\n    content: str | None = None,\n) -> tuple[list[FunctionCall] | None, str | None]",
      "language": "rust"
    },
    {
      "code": "1477\n1478\n1479\n1480\n1481\n1482\n1483\n1484\n1485\n1486\n1487\n1488\n1489\n1490\n1491\n1492\n1493\n1494\n1495\n1496\n1497\n1498\n1499\n1500\n1501\n1502\n1503\n1504\n1505\n1506\n1507\n1508\n1509\n1510\n1511\n1512\n1513\n1514\n1515\n1516\n1517\n1518\n1519\n1520\n1521\n1522\n1523\n1524\n1525\n1526\n1527\n1528\n1529\n1530\n1531\n1532\n1533\n1534\n1535\n1536\n1537\n1538\n1539\n1540\n1541\n1542\n1543\n1544\n1545\n1546\n1547\n1548\n1549\n1550\n1551",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef _parse_tool_calls_from_content(\n    request: ResponsesRequest | ChatCompletionRequest,\n    tokenizer: TokenizerLike | None,\n    enable_auto_tools: bool,\n    tool_parser_cls: Callable[[TokenizerLike], ToolParser] | None,\n    content: str | None = None,\n) -> tuple[list[FunctionCall] | None, str | None]:\n    function_calls = list[FunctionCall]()\n    if request.tool_choice and isinstance(request.tool_choice, ToolChoiceFunction):\n        assert content is not None\n        # Forced Function Call\n        function_calls.append(\n            FunctionCall(name=request.tool_choice.name, arguments=content)\n        )\n        content = None  # Clear content since tool is called.\n    elif request.tool_choice and isinstance(\n        request.tool_choice, ChatCompletionNamedToolChoiceParam\n    ):\n        assert content is not None\n        # Forced Function Call\n        function_calls.append(\n            FunctionCall(name=request.tool_choice.function.name, arguments=content)\n        )\n        content = None  # Clear content since tool is called.\n    elif request.tool_choice == \"required\":\n        assert content is not None\n        tool_calls = TypeAdapter(list[FunctionDefinition]).validate_json(content)\n        function_calls.extend(\n            [\n                FunctionCall(\n                    name=tool_call.name,\n                    arguments=json.dumps(tool_call.parameters, ensure_ascii=False),\n                )\n                for tool_call in tool_calls\n            ]\n        )\n        content = None  # Clear content since tool is called.\n    elif (\n        tool_parser_cls\n        and enable_auto_tools\n        and (request.tool_choice == \"auto\" or request.tool_choice is None)\n    ):\n        if tokenizer is None:\n            raise ValueError(\n                \"Tokenizer not available when `skip_tokenizer_init=True`\"\n            )\n\n        # Automatic Tool Call Parsing\n        try:\n            tool_parser = tool_parser_cls(tokenizer)\n        except RuntimeError as e:\n            logger.exception(\"Error in tool parser creation.\")\n            raise e\n        tool_call_info = tool_parser.extract_tool_calls(\n            content if content is not None else \"\",\n            request=request,  # type: ignore\n        )\n        if tool_call_info is not None and tool_call_info.tools_called:\n            # extract_tool_calls() returns a list of tool calls.\n            function_calls.extend(\n                FunctionCall(\n                    name=tool_call.function.name,\n                    arguments=tool_call.function.arguments,\n                )\n                for tool_call in tool_call_info.tool_calls\n            )\n            content = tool_call_info.content\n            if content and content.strip() == \"\":\n                content = None\n        else:\n            # No tool calls.\n            return None, content\n\n    return function_calls, content",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef _parse_tool_calls_from_content(\n    request: ResponsesRequest | ChatCompletionRequest,\n    tokenizer: TokenizerLike | None,\n    enable_auto_tools: bool,\n    tool_parser_cls: Callable[[TokenizerLike], ToolParser] | None,\n    content: str | None = None,\n) -> tuple[list[FunctionCall] | None, str | None]:\n    function_calls = list[FunctionCall]()\n    if request.tool_choice and isinstance(request.tool_choice, ToolChoiceFunction):\n        assert content is not None\n        # Forced Function Call\n        function_calls.append(\n            FunctionCall(name=request.tool_choice.name, arguments=content)\n        )\n        content = None  # Clear content since tool is called.\n    elif request.tool_choice and isinstance(\n        request.tool_choice, ChatCompletionNamedToolChoiceParam\n    ):\n        assert content is not None\n        # Forced Function Call\n        function_calls.append(\n            FunctionCall(name=request.tool_choice.function.name, arguments=content)\n        )\n        content = None  # Clear content since tool is called.\n    elif request.tool_choice == \"required\":\n        assert content is not None\n        tool_calls = TypeAdapter(list[FunctionDefinition]).validate_json(content)\n        function_calls.extend(\n            [\n                FunctionCall(\n                    name=tool_call.name,\n                    arguments=json.dumps(tool_call.parameters, ensure_ascii=False),\n                )\n                for tool_call in tool_calls\n            ]\n        )\n        content = None  # Clear content since tool is called.\n    elif (\n        tool_parser_cls\n        and enable_auto_tools\n        and (request.tool_choice == \"auto\" or request.tool_choice is None)\n    ):\n        if tokenizer is None:\n            raise ValueError(\n                \"Tokenizer not available when `skip_tokenizer_init=True`\"\n            )\n\n        # Automatic Tool Call Parsing\n        try:\n            tool_parser = tool_parser_cls(tokenizer)\n        except RuntimeError as e:\n            logger.exception(\"Error in tool parser creation.\")\n            raise e\n        tool_call_info = tool_parser.extract_tool_calls(\n            content if content is not None else \"\",\n            request=request,  # type: ignore\n        )\n        if tool_call_info is not None and tool_call_info.tools_called:\n            # extract_tool_calls() returns a list of tool calls.\n            function_calls.extend(\n                FunctionCall(\n                    name=tool_call.function.name,\n                    arguments=tool_call.function.arguments,\n                )\n                for tool_call in tool_call_info.tool_calls\n            )\n            content = tool_call_info.content\n            if content and content.strip() == \"\":\n                content = None\n        else:\n            # No tool calls.\n            return None, content\n\n    return function_calls, content",
      "language": "python"
    },
    {
      "code": "_pipeline(\n    ctx: ServeContext,\n) -> AsyncGenerator[AnyResponse | ErrorResponse, None]",
      "language": "rust"
    },
    {
      "code": "_pipeline(\n    ctx: ServeContext,\n) -> AsyncGenerator[AnyResponse | ErrorResponse, None]",
      "language": "rust"
    },
    {
      "code": "614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636",
      "language": "unknown"
    },
    {
      "code": "async def _pipeline(\n    self,\n    ctx: ServeContext,\n) -> AsyncGenerator[AnyResponse | ErrorResponse, None]:\n    \"\"\"Execute the request processing pipeline yielding responses.\"\"\"\n    if error := await self._check_model(ctx.request):\n        yield error\n    if error := self._validate_request(ctx):\n        yield error\n\n    preprocess_ret = await self._preprocess(ctx)\n    if isinstance(preprocess_ret, ErrorResponse):\n        yield preprocess_ret\n\n    generators_ret = await self._prepare_generators(ctx)\n    if isinstance(generators_ret, ErrorResponse):\n        yield generators_ret\n\n    collect_ret = await self._collect_batch(ctx)\n    if isinstance(collect_ret, ErrorResponse):\n        yield collect_ret\n\n    yield self._build_response(ctx)",
      "language": "python"
    },
    {
      "code": "async def _pipeline(\n    self,\n    ctx: ServeContext,\n) -> AsyncGenerator[AnyResponse | ErrorResponse, None]:\n    \"\"\"Execute the request processing pipeline yielding responses.\"\"\"\n    if error := await self._check_model(ctx.request):\n        yield error\n    if error := self._validate_request(ctx):\n        yield error\n\n    preprocess_ret = await self._preprocess(ctx)\n    if isinstance(preprocess_ret, ErrorResponse):\n        yield preprocess_ret\n\n    generators_ret = await self._prepare_generators(ctx)\n    if isinstance(generators_ret, ErrorResponse):\n        yield generators_ret\n\n    collect_ret = await self._collect_batch(ctx)\n    if isinstance(collect_ret, ErrorResponse):\n        yield collect_ret\n\n    yield self._build_response(ctx)",
      "language": "python"
    },
    {
      "code": "_prepare_generators(\n    ctx: ServeContext,\n) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "_prepare_generators(\n    ctx: ServeContext,\n) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712",
      "language": "unknown"
    },
    {
      "code": "async def _prepare_generators(\n    self,\n    ctx: ServeContext,\n) -> ErrorResponse | None:\n    \"\"\"Schedule the request and get the result generator.\"\"\"\n    generators: list[\n        AsyncGenerator[RequestOutput | PoolingRequestOutput, None]\n    ] = []\n\n    try:\n        trace_headers = (\n            None\n            if ctx.raw_request is None\n            else await self._get_trace_headers(ctx.raw_request.headers)\n        )\n\n        pooling_params = self._create_pooling_params(ctx)\n        if isinstance(pooling_params, ErrorResponse):\n            return pooling_params\n\n        if ctx.engine_prompts is None:\n            return self.create_error_response(\"Engine prompts not available\")\n\n        for i, engine_prompt in enumerate(ctx.engine_prompts):\n            request_id_item = f\"{ctx.request_id}-{i}\"\n\n            self._log_inputs(\n                request_id_item,\n                engine_prompt,\n                params=pooling_params,\n                lora_request=ctx.lora_request,\n            )\n\n            generator = self.engine_client.encode(\n                engine_prompt,\n                pooling_params,\n                request_id_item,\n                lora_request=ctx.lora_request,\n                trace_headers=trace_headers,\n                priority=getattr(ctx.request, \"priority\", 0),\n            )\n\n            generators.append(generator)\n\n        ctx.result_generator = merge_async_iterators(*generators)\n\n        return None\n\n    except Exception as e:\n        return self.create_error_response(e)",
      "language": "python"
    },
    {
      "code": "async def _prepare_generators(\n    self,\n    ctx: ServeContext,\n) -> ErrorResponse | None:\n    \"\"\"Schedule the request and get the result generator.\"\"\"\n    generators: list[\n        AsyncGenerator[RequestOutput | PoolingRequestOutput, None]\n    ] = []\n\n    try:\n        trace_headers = (\n            None\n            if ctx.raw_request is None\n            else await self._get_trace_headers(ctx.raw_request.headers)\n        )\n\n        pooling_params = self._create_pooling_params(ctx)\n        if isinstance(pooling_params, ErrorResponse):\n            return pooling_params\n\n        if ctx.engine_prompts is None:\n            return self.create_error_response(\"Engine prompts not available\")\n\n        for i, engine_prompt in enumerate(ctx.engine_prompts):\n            request_id_item = f\"{ctx.request_id}-{i}\"\n\n            self._log_inputs(\n                request_id_item,\n                engine_prompt,\n                params=pooling_params,\n                lora_request=ctx.lora_request,\n            )\n\n            generator = self.engine_client.encode(\n                engine_prompt,\n                pooling_params,\n                request_id_item,\n                lora_request=ctx.lora_request,\n                trace_headers=trace_headers,\n                priority=getattr(ctx.request, \"priority\", 0),\n            )\n\n            generators.append(generator)\n\n        ctx.result_generator = merge_async_iterators(*generators)\n\n        return None\n\n    except Exception as e:\n        return self.create_error_response(e)",
      "language": "python"
    },
    {
      "code": "_preprocess(ctx: ServeContext) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "_preprocess(ctx: ServeContext) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "582\n583\n584\n585\n586\n587\n588\n589\n590",
      "language": "unknown"
    },
    {
      "code": "async def _preprocess(\n    self,\n    ctx: ServeContext,\n) -> ErrorResponse | None:\n    \"\"\"\n    Default preprocessing hook. Subclasses may override\n    to prepare `ctx` (classification, embedding, etc.).\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "async def _preprocess(\n    self,\n    ctx: ServeContext,\n) -> ErrorResponse | None:\n    \"\"\"\n    Default preprocessing hook. Subclasses may override\n    to prepare `ctx` (classification, embedding, etc.).\n    \"\"\"\n    return None",
      "language": "python"
    },
    {
      "code": "_preprocess_chat(\n    request: ChatLikeRequest | ResponsesRequest,\n    tokenizer: TokenizerLike | None,\n    messages: list[ChatCompletionMessageParam],\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n    add_generation_prompt: bool = True,\n    continue_final_message: bool = False,\n    tool_dicts: list[dict[str, Any]] | None = None,\n    documents: list[dict[str, str]] | None = None,\n    chat_template_kwargs: dict[str, Any] | None = None,\n    tool_parser: Callable[[TokenizerLike], ToolParser]\n    | None = None,\n    add_special_tokens: bool = False,\n) -> tuple[list[ConversationMessage], list[TokensPrompt]]",
      "language": "typescript"
    },
    {
      "code": "_preprocess_chat(\n    request: ChatLikeRequest | ResponsesRequest,\n    tokenizer: TokenizerLike | None,\n    messages: list[ChatCompletionMessageParam],\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n    add_generation_prompt: bool = True,\n    continue_final_message: bool = False,\n    tool_dicts: list[dict[str, Any]] | None = None,\n    documents: list[dict[str, str]] | None = None,\n    chat_template_kwargs: dict[str, Any] | None = None,\n    tool_parser: Callable[[TokenizerLike], ToolParser]\n    | None = None,\n    add_special_tokens: bool = False,\n) -> tuple[list[ConversationMessage], list[TokensPrompt]]",
      "language": "typescript"
    },
    {
      "code": "1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270",
      "language": "unknown"
    },
    {
      "code": "async def _preprocess_chat(\n    self,\n    request: ChatLikeRequest | ResponsesRequest,\n    tokenizer: TokenizerLike | None,\n    messages: list[ChatCompletionMessageParam],\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n    add_generation_prompt: bool = True,\n    continue_final_message: bool = False,\n    tool_dicts: list[dict[str, Any]] | None = None,\n    documents: list[dict[str, str]] | None = None,\n    chat_template_kwargs: dict[str, Any] | None = None,\n    tool_parser: Callable[[TokenizerLike], ToolParser] | None = None,\n    add_special_tokens: bool = False,\n) -> tuple[list[ConversationMessage], list[TokensPrompt]]:\n    model_config = self.model_config\n\n    resolved_content_format = resolve_chat_template_content_format(\n        chat_template,\n        tool_dicts,\n        chat_template_content_format,\n        tokenizer,\n        model_config=model_config,\n    )\n    conversation, mm_data_future, mm_uuids = parse_chat_messages_futures(\n        messages,\n        model_config,\n        content_format=resolved_content_format,\n    )\n\n    _chat_template_kwargs: dict[str, Any] = dict(\n        chat_template=chat_template,\n        add_generation_prompt=add_generation_prompt,\n        continue_final_message=continue_final_message,\n        tools=tool_dicts,\n        documents=documents,\n    )\n    _chat_template_kwargs.update(chat_template_kwargs or {})\n\n    request_prompt: str | list[int]\n\n    if tokenizer is None:\n        request_prompt = \"placeholder\"\n    elif isinstance(tokenizer, MistralTokenizer):\n        request_prompt = await self._apply_mistral_chat_template_async(\n            tokenizer,\n            messages=messages,\n            **_chat_template_kwargs,\n        )\n    elif isinstance(tokenizer, DeepseekV32Tokenizer):\n        request_prompt = tokenizer.apply_chat_template(\n            conversation=conversation,\n            messages=messages,\n            model_config=model_config,\n            **_chat_template_kwargs,\n        )\n    else:\n        request_prompt = apply_hf_chat_template(\n            tokenizer=tokenizer,\n            conversation=conversation,\n            model_config=model_config,\n            **_chat_template_kwargs,\n        )\n\n    mm_data = await mm_data_future\n\n    # tool parsing is done only if a tool_parser has been set and if\n    # tool_choice is not \"none\" (if tool_choice is \"none\" but a tool_parser\n    # is set, we want to prevent parsing a tool_call hallucinated by the LLM\n    should_parse_tools = tool_parser is not None and (\n        hasattr(request, \"tool_choice\") and request.tool_choice != \"none\"\n    )\n\n    if should_parse_tools:\n        if not isinstance(request, ChatCompletionRequest | ResponsesRequest):\n            msg = (\n                \"Tool usage is only supported for Chat Completions API \"\n                \"or Responses API requests.\"\n            )\n            raise NotImplementedError(msg)\n        request = tool_parser(tokenizer).adjust_request(request=request)  # type: ignore\n\n    if tokenizer is None:\n        assert isinstance(request_prompt, str), (\n            \"Prompt has to be a string\",\n            \"when the tokenizer is not initialised\",\n        )\n        prompt_inputs = TokensPrompt(prompt=request_prompt, prompt_token_ids=[1])\n    elif isinstance(request_prompt, str):\n        prompt_inputs = await self._tokenize_prompt_input_async(\n            request,\n            tokenizer,\n            request_prompt,\n            add_special_tokens=add_special_tokens,\n        )\n    else:\n        # For MistralTokenizer\n        assert is_list_of(request_prompt, int), (\n            \"Prompt has to be either a string or a list of token ids\"\n        )\n        prompt_inputs = TokensPrompt(\n            prompt=tokenizer.decode(request_prompt),\n            prompt_token_ids=request_prompt,\n        )\n\n    engine_prompt = TokensPrompt(prompt_token_ids=prompt_inputs[\"prompt_token_ids\"])\n    if \"prompt\" in prompt_inputs:\n        engine_prompt[\"prompt\"] = prompt_inputs[\"prompt\"]\n\n    if mm_data is not None:\n        engine_prompt[\"multi_modal_data\"] = mm_data\n\n    if mm_uuids is not None:\n        engine_prompt[\"multi_modal_uuids\"] = mm_uuids\n\n    if request.mm_processor_kwargs is not None:\n        engine_prompt[\"mm_processor_kwargs\"] = request.mm_processor_kwargs\n\n    if hasattr(request, \"cache_salt\") and request.cache_salt is not None:\n        engine_prompt[\"cache_salt\"] = request.cache_salt\n\n    return conversation, [engine_prompt]",
      "language": "python"
    },
    {
      "code": "async def _preprocess_chat(\n    self,\n    request: ChatLikeRequest | ResponsesRequest,\n    tokenizer: TokenizerLike | None,\n    messages: list[ChatCompletionMessageParam],\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n    add_generation_prompt: bool = True,\n    continue_final_message: bool = False,\n    tool_dicts: list[dict[str, Any]] | None = None,\n    documents: list[dict[str, str]] | None = None,\n    chat_template_kwargs: dict[str, Any] | None = None,\n    tool_parser: Callable[[TokenizerLike], ToolParser] | None = None,\n    add_special_tokens: bool = False,\n) -> tuple[list[ConversationMessage], list[TokensPrompt]]:\n    model_config = self.model_config\n\n    resolved_content_format = resolve_chat_template_content_format(\n        chat_template,\n        tool_dicts,\n        chat_template_content_format,\n        tokenizer,\n        model_config=model_config,\n    )\n    conversation, mm_data_future, mm_uuids = parse_chat_messages_futures(\n        messages,\n        model_config,\n        content_format=resolved_content_format,\n    )\n\n    _chat_template_kwargs: dict[str, Any] = dict(\n        chat_template=chat_template,\n        add_generation_prompt=add_generation_prompt,\n        continue_final_message=continue_final_message,\n        tools=tool_dicts,\n        documents=documents,\n    )\n    _chat_template_kwargs.update(chat_template_kwargs or {})\n\n    request_prompt: str | list[int]\n\n    if tokenizer is None:\n        request_prompt = \"placeholder\"\n    elif isinstance(tokenizer, MistralTokenizer):\n        request_prompt = await self._apply_mistral_chat_template_async(\n            tokenizer,\n            messages=messages,\n            **_chat_template_kwargs,\n        )\n    elif isinstance(tokenizer, DeepseekV32Tokenizer):\n        request_prompt = tokenizer.apply_chat_template(\n            conversation=conversation,\n            messages=messages,\n            model_config=model_config,\n            **_chat_template_kwargs,\n        )\n    else:\n        request_prompt = apply_hf_chat_template(\n            tokenizer=tokenizer,\n            conversation=conversation,\n            model_config=model_config,\n            **_chat_template_kwargs,\n        )\n\n    mm_data = await mm_data_future\n\n    # tool parsing is done only if a tool_parser has been set and if\n    # tool_choice is not \"none\" (if tool_choice is \"none\" but a tool_parser\n    # is set, we want to prevent parsing a tool_call hallucinated by the LLM\n    should_parse_tools = tool_parser is not None and (\n        hasattr(request, \"tool_choice\") and request.tool_choice != \"none\"\n    )\n\n    if should_parse_tools:\n        if not isinstance(request, ChatCompletionRequest | ResponsesRequest):\n            msg = (\n                \"Tool usage is only supported for Chat Completions API \"\n                \"or Responses API requests.\"\n            )\n            raise NotImplementedError(msg)\n        request = tool_parser(tokenizer).adjust_request(request=request)  # type: ignore\n\n    if tokenizer is None:\n        assert isinstance(request_prompt, str), (\n            \"Prompt has to be a string\",\n            \"when the tokenizer is not initialised\",\n        )\n        prompt_inputs = TokensPrompt(prompt=request_prompt, prompt_token_ids=[1])\n    elif isinstance(request_prompt, str):\n        prompt_inputs = await self._tokenize_prompt_input_async(\n            request,\n            tokenizer,\n            request_prompt,\n            add_special_tokens=add_special_tokens,\n        )\n    else:\n        # For MistralTokenizer\n        assert is_list_of(request_prompt, int), (\n            \"Prompt has to be either a string or a list of token ids\"\n        )\n        prompt_inputs = TokensPrompt(\n            prompt=tokenizer.decode(request_prompt),\n            prompt_token_ids=request_prompt,\n        )\n\n    engine_prompt = TokensPrompt(prompt_token_ids=prompt_inputs[\"prompt_token_ids\"])\n    if \"prompt\" in prompt_inputs:\n        engine_prompt[\"prompt\"] = prompt_inputs[\"prompt\"]\n\n    if mm_data is not None:\n        engine_prompt[\"multi_modal_data\"] = mm_data\n\n    if mm_uuids is not None:\n        engine_prompt[\"multi_modal_uuids\"] = mm_uuids\n\n    if request.mm_processor_kwargs is not None:\n        engine_prompt[\"mm_processor_kwargs\"] = request.mm_processor_kwargs\n\n    if hasattr(request, \"cache_salt\") and request.cache_salt is not None:\n        engine_prompt[\"cache_salt\"] = request.cache_salt\n\n    return conversation, [engine_prompt]",
      "language": "python"
    },
    {
      "code": "_process_inputs(\n    request_id: str,\n    engine_prompt: PromptType,\n    params: SamplingParams | PoolingParams,\n    *,\n    lora_request: LoRARequest | None,\n    trace_headers: Mapping[str, str] | None,\n    priority: int,\n    data_parallel_rank: int | None = None,\n) -> tuple[EngineCoreRequest, dict[str, Any]]",
      "language": "rust"
    },
    {
      "code": "_process_inputs(\n    request_id: str,\n    engine_prompt: PromptType,\n    params: SamplingParams | PoolingParams,\n    *,\n    lora_request: LoRARequest | None,\n    trace_headers: Mapping[str, str] | None,\n    priority: int,\n    data_parallel_rank: int | None = None,\n) -> tuple[EngineCoreRequest, dict[str, Any]]",
      "language": "rust"
    },
    {
      "code": "1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299",
      "language": "unknown"
    },
    {
      "code": "async def _process_inputs(\n    self,\n    request_id: str,\n    engine_prompt: PromptType,\n    params: SamplingParams | PoolingParams,\n    *,\n    lora_request: LoRARequest | None,\n    trace_headers: Mapping[str, str] | None,\n    priority: int,\n    data_parallel_rank: int | None = None,\n) -> tuple[EngineCoreRequest, dict[str, Any]]:\n    \"\"\"Use the Processor to process inputs for AsyncLLM.\"\"\"\n    tokenization_kwargs: dict[str, Any] = {}\n    _validate_truncation_size(\n        self.max_model_len, params.truncate_prompt_tokens, tokenization_kwargs\n    )\n\n    engine_request = self.input_processor.process_inputs(\n        request_id,\n        engine_prompt,\n        params,\n        lora_request=lora_request,\n        tokenization_kwargs=tokenization_kwargs,\n        trace_headers=trace_headers,\n        priority=priority,\n        data_parallel_rank=data_parallel_rank,\n    )\n    return engine_request, tokenization_kwargs",
      "language": "python"
    },
    {
      "code": "async def _process_inputs(\n    self,\n    request_id: str,\n    engine_prompt: PromptType,\n    params: SamplingParams | PoolingParams,\n    *,\n    lora_request: LoRARequest | None,\n    trace_headers: Mapping[str, str] | None,\n    priority: int,\n    data_parallel_rank: int | None = None,\n) -> tuple[EngineCoreRequest, dict[str, Any]]:\n    \"\"\"Use the Processor to process inputs for AsyncLLM.\"\"\"\n    tokenization_kwargs: dict[str, Any] = {}\n    _validate_truncation_size(\n        self.max_model_len, params.truncate_prompt_tokens, tokenization_kwargs\n    )\n\n    engine_request = self.input_processor.process_inputs(\n        request_id,\n        engine_prompt,\n        params,\n        lora_request=lora_request,\n        tokenization_kwargs=tokenization_kwargs,\n        trace_headers=trace_headers,\n        priority=priority,\n        data_parallel_rank=data_parallel_rank,\n    )\n    return engine_request, tokenization_kwargs",
      "language": "python"
    },
    {
      "code": "_raise_if_error(\n    finish_reason: str | None, request_id: str\n) -> None",
      "language": "rust"
    },
    {
      "code": "_raise_if_error(\n    finish_reason: str | None, request_id: str\n) -> None",
      "language": "rust"
    },
    {
      "code": "812\n813\n814\n815\n816\n817\n818\n819",
      "language": "unknown"
    },
    {
      "code": "def _raise_if_error(self, finish_reason: str | None, request_id: str) -> None:\n    \"\"\"Raise GenerationError if finish_reason indicates an error.\"\"\"\n    if finish_reason == \"error\":\n        logger.error(\n            \"Request %s failed with an internal error during generation\",\n            request_id,\n        )\n        raise GenerationError(\"Internal server error\")",
      "language": "json"
    },
    {
      "code": "def _raise_if_error(self, finish_reason: str | None, request_id: str) -> None:\n    \"\"\"Raise GenerationError if finish_reason indicates an error.\"\"\"\n    if finish_reason == \"error\":\n        logger.error(\n            \"Request %s failed with an internal error during generation\",\n            request_id,\n        )\n        raise GenerationError(\"Internal server error\")",
      "language": "json"
    },
    {
      "code": "_render_next_turn(\n    request: ResponsesRequest,\n    tokenizer: TokenizerLike | None,\n    messages: list[ResponseInputOutputItem],\n    tool_dicts: list[dict[str, Any]] | None,\n    tool_parser,\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n)",
      "language": "rust"
    },
    {
      "code": "_render_next_turn(\n    request: ResponsesRequest,\n    tokenizer: TokenizerLike | None,\n    messages: list[ResponseInputOutputItem],\n    tool_dicts: list[dict[str, Any]] | None,\n    tool_parser,\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n)",
      "language": "rust"
    },
    {
      "code": "1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324",
      "language": "unknown"
    },
    {
      "code": "async def _render_next_turn(\n    self,\n    request: ResponsesRequest,\n    tokenizer: TokenizerLike | None,\n    messages: list[ResponseInputOutputItem],\n    tool_dicts: list[dict[str, Any]] | None,\n    tool_parser,\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n):\n    new_messages = construct_input_messages(\n        request_input=messages,\n    )\n\n    _, engine_prompts = await self._preprocess_chat(\n        request,\n        tokenizer,\n        new_messages,\n        tool_dicts=tool_dicts,\n        tool_parser=tool_parser,\n        chat_template=chat_template,\n        chat_template_content_format=chat_template_content_format,\n    )\n    return engine_prompts",
      "language": "python"
    },
    {
      "code": "async def _render_next_turn(\n    self,\n    request: ResponsesRequest,\n    tokenizer: TokenizerLike | None,\n    messages: list[ResponseInputOutputItem],\n    tool_dicts: list[dict[str, Any]] | None,\n    tool_parser,\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n):\n    new_messages = construct_input_messages(\n        request_input=messages,\n    )\n\n    _, engine_prompts = await self._preprocess_chat(\n        request,\n        tokenizer,\n        new_messages,\n        tool_dicts=tool_dicts,\n        tool_parser=tool_parser,\n        chat_template=chat_template,\n        chat_template_content_format=chat_template_content_format,\n    )\n    return engine_prompts",
      "language": "python"
    },
    {
      "code": "_tokenize_prompt_input_async(\n    request: AnyRequest,\n    tokenizer: TokenizerLike,\n    prompt_input: str | list[int],\n    add_special_tokens: bool = True,\n) -> TokensPrompt",
      "language": "typescript"
    },
    {
      "code": "_tokenize_prompt_input_async(\n    request: AnyRequest,\n    tokenizer: TokenizerLike,\n    prompt_input: str | list[int],\n    add_special_tokens: bool = True,\n) -> TokensPrompt",
      "language": "typescript"
    },
    {
      "code": "1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102",
      "language": "unknown"
    },
    {
      "code": "async def _tokenize_prompt_input_async(\n    self,\n    request: AnyRequest,\n    tokenizer: TokenizerLike,\n    prompt_input: str | list[int],\n    add_special_tokens: bool = True,\n) -> TokensPrompt:\n    \"\"\"\n    A simpler implementation that tokenizes a single prompt input.\n    \"\"\"\n    async for result in self._tokenize_prompt_inputs_async(\n        request,\n        tokenizer,\n        [prompt_input],\n        add_special_tokens=add_special_tokens,\n    ):\n        return result\n    raise ValueError(\"No results yielded from tokenization\")",
      "language": "python"
    },
    {
      "code": "async def _tokenize_prompt_input_async(\n    self,\n    request: AnyRequest,\n    tokenizer: TokenizerLike,\n    prompt_input: str | list[int],\n    add_special_tokens: bool = True,\n) -> TokensPrompt:\n    \"\"\"\n    A simpler implementation that tokenizes a single prompt input.\n    \"\"\"\n    async for result in self._tokenize_prompt_inputs_async(\n        request,\n        tokenizer,\n        [prompt_input],\n        add_special_tokens=add_special_tokens,\n    ):\n        return result\n    raise ValueError(\"No results yielded from tokenization\")",
      "language": "python"
    },
    {
      "code": "_tokenize_prompt_inputs_async(\n    request: AnyRequest,\n    tokenizer: TokenizerLike,\n    prompt_inputs: Iterable[str | list[int]],\n    add_special_tokens: bool = True,\n) -> AsyncGenerator[TokensPrompt, None]",
      "language": "typescript"
    },
    {
      "code": "_tokenize_prompt_inputs_async(\n    request: AnyRequest,\n    tokenizer: TokenizerLike,\n    prompt_inputs: Iterable[str | list[int]],\n    add_special_tokens: bool = True,\n) -> AsyncGenerator[TokensPrompt, None]",
      "language": "typescript"
    },
    {
      "code": "1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127",
      "language": "unknown"
    },
    {
      "code": "async def _tokenize_prompt_inputs_async(\n    self,\n    request: AnyRequest,\n    tokenizer: TokenizerLike,\n    prompt_inputs: Iterable[str | list[int]],\n    add_special_tokens: bool = True,\n) -> AsyncGenerator[TokensPrompt, None]:\n    \"\"\"\n    A simpler implementation that tokenizes multiple prompt inputs.\n    \"\"\"\n    for prompt in prompt_inputs:\n        if isinstance(prompt, str):\n            yield await self._normalize_prompt_text_to_input(\n                request,\n                prompt=prompt,\n                tokenizer=tokenizer,\n                add_special_tokens=add_special_tokens,\n            )\n        else:\n            yield await self._normalize_prompt_tokens_to_input(\n                request,\n                prompt_ids=prompt,\n                tokenizer=tokenizer,\n            )",
      "language": "python"
    },
    {
      "code": "async def _tokenize_prompt_inputs_async(\n    self,\n    request: AnyRequest,\n    tokenizer: TokenizerLike,\n    prompt_inputs: Iterable[str | list[int]],\n    add_special_tokens: bool = True,\n) -> AsyncGenerator[TokensPrompt, None]:\n    \"\"\"\n    A simpler implementation that tokenizes multiple prompt inputs.\n    \"\"\"\n    for prompt in prompt_inputs:\n        if isinstance(prompt, str):\n            yield await self._normalize_prompt_text_to_input(\n                request,\n                prompt=prompt,\n                tokenizer=tokenizer,\n                add_special_tokens=add_special_tokens,\n            )\n        else:\n            yield await self._normalize_prompt_tokens_to_input(\n                request,\n                prompt_ids=prompt,\n                tokenizer=tokenizer,\n            )",
      "language": "python"
    },
    {
      "code": "_validate_chat_template(\n    request_chat_template: str | None,\n    chat_template_kwargs: dict[str, Any] | None,\n    trust_request_chat_template: bool,\n) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "_validate_chat_template(\n    request_chat_template: str | None,\n    chat_template_kwargs: dict[str, Any] | None,\n    trust_request_chat_template: bool,\n) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147",
      "language": "unknown"
    },
    {
      "code": "def _validate_chat_template(\n    self,\n    request_chat_template: str | None,\n    chat_template_kwargs: dict[str, Any] | None,\n    trust_request_chat_template: bool,\n) -> ErrorResponse | None:\n    if not trust_request_chat_template and (\n        request_chat_template is not None\n        or (\n            chat_template_kwargs\n            and chat_template_kwargs.get(\"chat_template\") is not None\n        )\n    ):\n        return self.create_error_response(\n            \"Chat template is passed with request, but \"\n            \"--trust-request-chat-template is not set. \"\n            \"Refused request with untrusted chat template.\"\n        )\n    return None",
      "language": "python"
    },
    {
      "code": "def _validate_chat_template(\n    self,\n    request_chat_template: str | None,\n    chat_template_kwargs: dict[str, Any] | None,\n    trust_request_chat_template: bool,\n) -> ErrorResponse | None:\n    if not trust_request_chat_template and (\n        request_chat_template is not None\n        or (\n            chat_template_kwargs\n            and chat_template_kwargs.get(\"chat_template\") is not None\n        )\n    ):\n        return self.create_error_response(\n            \"Chat template is passed with request, but \"\n            \"--trust-request-chat-template is not set. \"\n            \"Refused request with untrusted chat template.\"\n        )\n    return None",
      "language": "python"
    },
    {
      "code": "_validate_input(\n    request: AnyRequest,\n    input_ids: list[int],\n    input_text: str,\n) -> TokensPrompt",
      "language": "php"
    },
    {
      "code": "_validate_input(\n    request: AnyRequest,\n    input_ids: list[int],\n    input_text: str,\n) -> TokensPrompt",
      "language": "php"
    },
    {
      "code": "1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083",
      "language": "unknown"
    },
    {
      "code": "def _validate_input(\n    self,\n    request: AnyRequest,\n    input_ids: list[int],\n    input_text: str,\n) -> TokensPrompt:\n    token_num = len(input_ids)\n\n    # Note: EmbeddingRequest, ClassificationRequest,\n    # and ScoreRequest doesn't have max_tokens\n    if isinstance(\n        request,\n        (\n            EmbeddingChatRequest,\n            EmbeddingCompletionRequest,\n            ScoreRequest,\n            RerankRequest,\n            ClassificationCompletionRequest,\n            ClassificationChatRequest,\n        ),\n    ):\n        # Note: input length can be up to the entire model context length\n        # since these requests don't generate tokens.\n        if token_num > self.max_model_len:\n            operations: dict[type[AnyRequest], str] = {\n                ScoreRequest: \"score\",\n                ClassificationCompletionRequest: \"classification\",\n                ClassificationChatRequest: \"classification\",\n            }\n            operation = operations.get(type(request), \"embedding generation\")\n            raise VLLMValidationError(\n                f\"This model's maximum context length is \"\n                f\"{self.max_model_len} tokens. However, you requested \"\n                f\"{token_num} tokens in the input for {operation}. \"\n                f\"Please reduce the length of the input.\",\n                parameter=\"input_tokens\",\n                value=token_num,\n            )\n        return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n    # Note: TokenizeRequest and DetokenizeRequest doesn't have max_tokens\n    # and does not require model context length validation\n    if isinstance(\n        request,\n        (TokenizeCompletionRequest, TokenizeChatRequest, DetokenizeRequest),\n    ):\n        return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n    # chat completion endpoint supports max_completion_tokens\n    if isinstance(request, ChatCompletionRequest):\n        # TODO(#9845): remove max_tokens when field dropped from OpenAI API\n        max_tokens = request.max_completion_tokens or request.max_tokens\n    else:\n        max_tokens = getattr(request, \"max_tokens\", None)\n\n    # Note: input length can be up to model context length - 1 for\n    # completion-like requests.\n    if token_num >= self.max_model_len:\n        raise VLLMValidationError(\n            f\"This model's maximum context length is \"\n            f\"{self.max_model_len} tokens. However, your request has \"\n            f\"{token_num} input tokens. Please reduce the length of \"\n            \"the input messages.\",\n            parameter=\"input_tokens\",\n            value=token_num,\n        )\n\n    if max_tokens is not None and token_num + max_tokens > self.max_model_len:\n        raise VLLMValidationError(\n            \"'max_tokens' or 'max_completion_tokens' is too large: \"\n            f\"{max_tokens}. This model's maximum context length is \"\n            f\"{self.max_model_len} tokens and your request has \"\n            f\"{token_num} input tokens ({max_tokens} > {self.max_model_len}\"\n            f\" - {token_num}).\",\n            parameter=\"max_tokens\",\n            value=max_tokens,\n        )\n\n    return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)",
      "language": "python"
    },
    {
      "code": "def _validate_input(\n    self,\n    request: AnyRequest,\n    input_ids: list[int],\n    input_text: str,\n) -> TokensPrompt:\n    token_num = len(input_ids)\n\n    # Note: EmbeddingRequest, ClassificationRequest,\n    # and ScoreRequest doesn't have max_tokens\n    if isinstance(\n        request,\n        (\n            EmbeddingChatRequest,\n            EmbeddingCompletionRequest,\n            ScoreRequest,\n            RerankRequest,\n            ClassificationCompletionRequest,\n            ClassificationChatRequest,\n        ),\n    ):\n        # Note: input length can be up to the entire model context length\n        # since these requests don't generate tokens.\n        if token_num > self.max_model_len:\n            operations: dict[type[AnyRequest], str] = {\n                ScoreRequest: \"score\",\n                ClassificationCompletionRequest: \"classification\",\n                ClassificationChatRequest: \"classification\",\n            }\n            operation = operations.get(type(request), \"embedding generation\")\n            raise VLLMValidationError(\n                f\"This model's maximum context length is \"\n                f\"{self.max_model_len} tokens. However, you requested \"\n                f\"{token_num} tokens in the input for {operation}. \"\n                f\"Please reduce the length of the input.\",\n                parameter=\"input_tokens\",\n                value=token_num,\n            )\n        return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n    # Note: TokenizeRequest and DetokenizeRequest doesn't have max_tokens\n    # and does not require model context length validation\n    if isinstance(\n        request,\n        (TokenizeCompletionRequest, TokenizeChatRequest, DetokenizeRequest),\n    ):\n        return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)\n\n    # chat completion endpoint supports max_completion_tokens\n    if isinstance(request, ChatCompletionRequest):\n        # TODO(#9845): remove max_tokens when field dropped from OpenAI API\n        max_tokens = request.max_completion_tokens or request.max_tokens\n    else:\n        max_tokens = getattr(request, \"max_tokens\", None)\n\n    # Note: input length can be up to model context length - 1 for\n    # completion-like requests.\n    if token_num >= self.max_model_len:\n        raise VLLMValidationError(\n            f\"This model's maximum context length is \"\n            f\"{self.max_model_len} tokens. However, your request has \"\n            f\"{token_num} input tokens. Please reduce the length of \"\n            \"the input messages.\",\n            parameter=\"input_tokens\",\n            value=token_num,\n        )\n\n    if max_tokens is not None and token_num + max_tokens > self.max_model_len:\n        raise VLLMValidationError(\n            \"'max_tokens' or 'max_completion_tokens' is too large: \"\n            f\"{max_tokens}. This model's maximum context length is \"\n            f\"{self.max_model_len} tokens and your request has \"\n            f\"{token_num} input tokens ({max_tokens} > {self.max_model_len}\"\n            f\" - {token_num}).\",\n            parameter=\"max_tokens\",\n            value=max_tokens,\n        )\n\n    return TokensPrompt(prompt=input_text, prompt_token_ids=input_ids)",
      "language": "python"
    },
    {
      "code": "_validate_request(\n    ctx: ServeContext,\n) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "_validate_request(\n    ctx: ServeContext,\n) -> ErrorResponse | None",
      "language": "rust"
    },
    {
      "code": "638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650",
      "language": "unknown"
    },
    {
      "code": "def _validate_request(self, ctx: ServeContext) -> ErrorResponse | None:\n    truncate_prompt_tokens = getattr(ctx.request, \"truncate_prompt_tokens\", None)\n\n    if (\n        truncate_prompt_tokens is not None\n        and truncate_prompt_tokens > self.max_model_len\n    ):\n        return self.create_error_response(\n            \"truncate_prompt_tokens value is \"\n            \"greater than max_model_len.\"\n            \" Please, select a smaller truncation size.\"\n        )\n    return None",
      "language": "python"
    },
    {
      "code": "def _validate_request(self, ctx: ServeContext) -> ErrorResponse | None:\n    truncate_prompt_tokens = getattr(ctx.request, \"truncate_prompt_tokens\", None)\n\n    if (\n        truncate_prompt_tokens is not None\n        and truncate_prompt_tokens > self.max_model_len\n    ):\n        return self.create_error_response(\n            \"truncate_prompt_tokens value is \"\n            \"greater than max_model_len.\"\n            \" Please, select a smaller truncation size.\"\n        )\n    return None",
      "language": "python"
    },
    {
      "code": "beam_search(\n    prompt: PromptType,\n    request_id: str,\n    params: BeamSearchParams,\n    lora_request: LoRARequest | None = None,\n    trace_headers: Mapping[str, str] | None = None,\n) -> AsyncGenerator[RequestOutput, None]",
      "language": "rust"
    },
    {
      "code": "beam_search(\n    prompt: PromptType,\n    request_id: str,\n    params: BeamSearchParams,\n    lora_request: LoRARequest | None = None,\n    trace_headers: Mapping[str, str] | None = None,\n) -> AsyncGenerator[RequestOutput, None]",
      "language": "rust"
    },
    {
      "code": "308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545",
      "language": "unknown"
    },
    {
      "code": "async def beam_search(\n    self,\n    prompt: PromptType,\n    request_id: str,\n    params: BeamSearchParams,\n    lora_request: LoRARequest | None = None,\n    trace_headers: Mapping[str, str] | None = None,\n) -> AsyncGenerator[RequestOutput, None]:\n    beam_width = params.beam_width\n    max_tokens = params.max_tokens\n    ignore_eos = params.ignore_eos\n    temperature = params.temperature\n    length_penalty = params.length_penalty\n    include_stop_str_in_output = params.include_stop_str_in_output\n\n    input_processor = self.input_processor\n    tokenizer = input_processor.tokenizer\n    if tokenizer is None:\n        raise VLLMValidationError(\n            \"You cannot use beam search when `skip_tokenizer_init=True`\",\n            parameter=\"skip_tokenizer_init\",\n            value=True,\n        )\n\n    eos_token_id: int = tokenizer.eos_token_id  # type: ignore\n\n    if is_explicit_encoder_decoder_prompt(prompt):\n        raise NotImplementedError\n\n    prompt_text: str | None\n    prompt_token_ids: list[int]\n    multi_modal_data: MultiModalDataDict | None\n    if isinstance(prompt, str):\n        prompt_text = prompt\n        prompt_token_ids = []\n        multi_modal_data = None\n    else:\n        prompt_text = prompt.get(\"prompt\")  # type: ignore\n        prompt_token_ids = prompt.get(\"prompt_token_ids\", [])  # type: ignore\n        multi_modal_data = prompt.get(\"multi_modal_data\")  # type: ignore\n\n    mm_processor_kwargs: dict[str, Any] | None = None\n\n    # This is a workaround to fix multimodal beam search; this is a\n    # bandaid fix for 2 small problems:\n    # 1. Multi_modal_data on the processed_inputs currently resolves to\n    #    `None`.\n    # 2. preprocessing above expands the multimodal placeholders. However,\n    #    this happens again in generation, so the double expansion causes\n    #    a mismatch.\n    # TODO - would be ideal to handle this more gracefully.\n\n    tokenized_length = len(prompt_token_ids)\n\n    sort_beams_key = create_sort_beams_key_function(eos_token_id, length_penalty)\n\n    logprobs_num = 2 * beam_width\n    beam_search_params = SamplingParams(\n        logprobs=logprobs_num,\n        max_tokens=1,\n        temperature=temperature,\n    )\n    all_beams = [\n        BeamSearchSequence(\n            tokens=prompt_token_ids,\n            cum_logprob=0,\n            logprobs=[],\n            multi_modal_data=multi_modal_data,\n            mm_processor_kwargs=mm_processor_kwargs,\n            lora_request=lora_request,\n        )\n    ]\n    completed = []\n\n    for _ in range(max_tokens):\n        prompts_batch, lora_req_batch = zip(\n            *[\n                (\n                    TokensPrompt(\n                        prompt_token_ids=beam.tokens,\n                        multi_modal_data=beam.multi_modal_data,\n                        mm_processor_kwargs=beam.mm_processor_kwargs,\n                    ),\n                    beam.lora_request,\n                )\n                for beam in all_beams\n            ]\n        )\n\n        tasks = []\n        request_id_batch = f\"{request_id}-{random_uuid()}\"\n\n        for i, (individual_prompt, lora_req) in enumerate(\n            zip(prompts_batch, lora_req_batch)\n        ):\n            request_id_item = f\"{request_id_batch}-beam-{i}\"\n            task = asyncio.create_task(\n                collect_from_async_generator(\n                    self.engine_client.generate(\n                        individual_prompt,\n                        beam_search_params,\n                        request_id_item,\n                        lora_request=lora_req,\n                        trace_headers=trace_headers,\n                    )\n                )\n            )\n            tasks.append(task)\n\n        output = [x[0] for x in await asyncio.gather(*tasks)]\n\n        new_beams = []\n        # Store all new tokens generated by beam\n        all_beams_token_id = []\n        # Store the cumulative probability of all tokens\n        # generated by beam search\n        all_beams_logprob = []\n        # Iterate through all beam inference results\n        for i, result in enumerate(output):\n            current_beam = all_beams[i]\n\n            # check for error finish reason and abort beam search\n            if result.outputs[0].finish_reason == \"error\":\n                # yield error output and terminate beam search\n                yield RequestOutput(\n                    request_id=request_id,\n                    prompt=prompt_text,\n                    outputs=[\n                        CompletionOutput(\n                            index=0,\n                            text=\"\",\n                            token_ids=[],\n                            cumulative_logprob=None,\n                            logprobs=None,\n                            finish_reason=\"error\",\n                        )\n                    ],\n                    finished=True,\n                    prompt_token_ids=prompt_token_ids,\n                    prompt_logprobs=None,\n                )\n                return\n\n            if result.outputs[0].logprobs is not None:\n                logprobs = result.outputs[0].logprobs[0]\n                all_beams_token_id.extend(list(logprobs.keys()))\n                all_beams_logprob.extend(\n                    [\n                        current_beam.cum_logprob + obj.logprob\n                        for obj in logprobs.values()\n                    ]\n                )\n\n        # Handle the token for the end of sentence (EOS)\n        all_beams_token_id = np.array(all_beams_token_id)\n        all_beams_logprob = np.array(all_beams_logprob)\n\n        if not ignore_eos:\n            # Get the index position of eos token in all generated results\n            eos_idx = np.where(all_beams_token_id == eos_token_id)[0]\n            for idx in eos_idx:\n                current_beam = all_beams[idx // logprobs_num]\n                result = output[idx // logprobs_num]\n                assert result.outputs[0].logprobs is not None\n                logprobs_entry = result.outputs[0].logprobs[0]\n                completed.append(\n                    BeamSearchSequence(\n                        tokens=current_beam.tokens + [eos_token_id]\n                        if include_stop_str_in_output\n                        else current_beam.tokens,\n                        logprobs=current_beam.logprobs + [logprobs_entry],\n                        cum_logprob=float(all_beams_logprob[idx]),\n                        finish_reason=\"stop\",\n                        stop_reason=eos_token_id,\n                    )\n                )\n            # After processing, set the log probability of the eos condition\n            # to negative infinity.\n            all_beams_logprob[eos_idx] = -np.inf\n\n        # Processing non-EOS tokens\n        # Get indices of the top beam_width probabilities\n        topn_idx = np.argpartition(np.negative(all_beams_logprob), beam_width)[\n            :beam_width\n        ]\n\n        for idx in topn_idx:\n            current_beam = all_beams[idx // logprobs_num]\n            result = output[idx // logprobs_num]\n            token_id = int(all_beams_token_id[idx])\n            assert result.outputs[0].logprobs is not None\n            logprobs_entry = result.outputs[0].logprobs[0]\n            new_beams.append(\n                BeamSearchSequence(\n                    tokens=current_beam.tokens + [token_id],\n                    logprobs=current_beam.logprobs + [logprobs_entry],\n                    lora_request=current_beam.lora_request,\n                    cum_logprob=float(all_beams_logprob[idx]),\n                    multi_modal_data=current_beam.multi_modal_data,\n                    mm_processor_kwargs=current_beam.mm_processor_kwargs,\n                )\n            )\n\n        all_beams = new_beams\n\n    completed.extend(all_beams)\n    sorted_completed = sorted(completed, key=sort_beams_key, reverse=True)\n    best_beams = sorted_completed[:beam_width]\n\n    for beam in best_beams:\n        if beam.tokens[-1] == eos_token_id and not ignore_eos:\n            # Skip the eos token in the text.\n            tokens = beam.tokens[tokenized_length:-1]\n        else:\n            tokens = beam.tokens[tokenized_length:]\n        beam.text = tokenizer.decode(tokens)\n\n    yield RequestOutput(\n        request_id=request_id,\n        prompt=prompt_text,\n        outputs=[\n            CompletionOutput(\n                text=beam.text,  # type: ignore\n                cumulative_logprob=beam.cum_logprob,\n                token_ids=beam.tokens[tokenized_length:],\n                index=i,\n                logprobs=beam.logprobs,\n                finish_reason=beam.finish_reason\n                if beam.finish_reason is not None\n                else \"length\",\n                stop_reason=beam.stop_reason,\n            )\n            for (i, beam) in enumerate(best_beams)\n        ],\n        finished=True,\n        prompt_token_ids=prompt_token_ids,\n        prompt_logprobs=None,\n    )",
      "language": "python"
    },
    {
      "code": "async def beam_search(\n    self,\n    prompt: PromptType,\n    request_id: str,\n    params: BeamSearchParams,\n    lora_request: LoRARequest | None = None,\n    trace_headers: Mapping[str, str] | None = None,\n) -> AsyncGenerator[RequestOutput, None]:\n    beam_width = params.beam_width\n    max_tokens = params.max_tokens\n    ignore_eos = params.ignore_eos\n    temperature = params.temperature\n    length_penalty = params.length_penalty\n    include_stop_str_in_output = params.include_stop_str_in_output\n\n    input_processor = self.input_processor\n    tokenizer = input_processor.tokenizer\n    if tokenizer is None:\n        raise VLLMValidationError(\n            \"You cannot use beam search when `skip_tokenizer_init=True`\",\n            parameter=\"skip_tokenizer_init\",\n            value=True,\n        )\n\n    eos_token_id: int = tokenizer.eos_token_id  # type: ignore\n\n    if is_explicit_encoder_decoder_prompt(prompt):\n        raise NotImplementedError\n\n    prompt_text: str | None\n    prompt_token_ids: list[int]\n    multi_modal_data: MultiModalDataDict | None\n    if isinstance(prompt, str):\n        prompt_text = prompt\n        prompt_token_ids = []\n        multi_modal_data = None\n    else:\n        prompt_text = prompt.get(\"prompt\")  # type: ignore\n        prompt_token_ids = prompt.get(\"prompt_token_ids\", [])  # type: ignore\n        multi_modal_data = prompt.get(\"multi_modal_data\")  # type: ignore\n\n    mm_processor_kwargs: dict[str, Any] | None = None\n\n    # This is a workaround to fix multimodal beam search; this is a\n    # bandaid fix for 2 small problems:\n    # 1. Multi_modal_data on the processed_inputs currently resolves to\n    #    `None`.\n    # 2. preprocessing above expands the multimodal placeholders. However,\n    #    this happens again in generation, so the double expansion causes\n    #    a mismatch.\n    # TODO - would be ideal to handle this more gracefully.\n\n    tokenized_length = len(prompt_token_ids)\n\n    sort_beams_key = create_sort_beams_key_function(eos_token_id, length_penalty)\n\n    logprobs_num = 2 * beam_width\n    beam_search_params = SamplingParams(\n        logprobs=logprobs_num,\n        max_tokens=1,\n        temperature=temperature,\n    )\n    all_beams = [\n        BeamSearchSequence(\n            tokens=prompt_token_ids,\n            cum_logprob=0,\n            logprobs=[],\n            multi_modal_data=multi_modal_data,\n            mm_processor_kwargs=mm_processor_kwargs,\n            lora_request=lora_request,\n        )\n    ]\n    completed = []\n\n    for _ in range(max_tokens):\n        prompts_batch, lora_req_batch = zip(\n            *[\n                (\n                    TokensPrompt(\n                        prompt_token_ids=beam.tokens,\n                        multi_modal_data=beam.multi_modal_data,\n                        mm_processor_kwargs=beam.mm_processor_kwargs,\n                    ),\n                    beam.lora_request,\n                )\n                for beam in all_beams\n            ]\n        )\n\n        tasks = []\n        request_id_batch = f\"{request_id}-{random_uuid()}\"\n\n        for i, (individual_prompt, lora_req) in enumerate(\n            zip(prompts_batch, lora_req_batch)\n        ):\n            request_id_item = f\"{request_id_batch}-beam-{i}\"\n            task = asyncio.create_task(\n                collect_from_async_generator(\n                    self.engine_client.generate(\n                        individual_prompt,\n                        beam_search_params,\n                        request_id_item,\n                        lora_request=lora_req,\n                        trace_headers=trace_headers,\n                    )\n                )\n            )\n            tasks.append(task)\n\n        output = [x[0] for x in await asyncio.gather(*tasks)]\n\n        new_beams = []\n        # Store all new tokens generated by beam\n        all_beams_token_id = []\n        # Store the cumulative probability of all tokens\n        # generated by beam search\n        all_beams_logprob = []\n        # Iterate through all beam inference results\n        for i, result in enumerate(output):\n            current_beam = all_beams[i]\n\n            # check for error finish reason and abort beam search\n            if result.outputs[0].finish_reason == \"error\":\n                # yield error output and terminate beam search\n                yield RequestOutput(\n                    request_id=request_id,\n                    prompt=prompt_text,\n                    outputs=[\n                        CompletionOutput(\n                            index=0,\n                            text=\"\",\n                            token_ids=[],\n                            cumulative_logprob=None,\n                            logprobs=None,\n                            finish_reason=\"error\",\n                        )\n                    ],\n                    finished=True,\n                    prompt_token_ids=prompt_token_ids,\n                    prompt_logprobs=None,\n                )\n                return\n\n            if result.outputs[0].logprobs is not None:\n                logprobs = result.outputs[0].logprobs[0]\n                all_beams_token_id.extend(list(logprobs.keys()))\n                all_beams_logprob.extend(\n                    [\n                        current_beam.cum_logprob + obj.logprob\n                        for obj in logprobs.values()\n                    ]\n                )\n\n        # Handle the token for the end of sentence (EOS)\n        all_beams_token_id = np.array(all_beams_token_id)\n        all_beams_logprob = np.array(all_beams_logprob)\n\n        if not ignore_eos:\n            # Get the index position of eos token in all generated results\n            eos_idx = np.where(all_beams_token_id == eos_token_id)[0]\n            for idx in eos_idx:\n                current_beam = all_beams[idx // logprobs_num]\n                result = output[idx // logprobs_num]\n                assert result.outputs[0].logprobs is not None\n                logprobs_entry = result.outputs[0].logprobs[0]\n                completed.append(\n                    BeamSearchSequence(\n                        tokens=current_beam.tokens + [eos_token_id]\n                        if include_stop_str_in_output\n                        else current_beam.tokens,\n                        logprobs=current_beam.logprobs + [logprobs_entry],\n                        cum_logprob=float(all_beams_logprob[idx]),\n                        finish_reason=\"stop\",\n                        stop_reason=eos_token_id,\n                    )\n                )\n            # After processing, set the log probability of the eos condition\n            # to negative infinity.\n            all_beams_logprob[eos_idx] = -np.inf\n\n        # Processing non-EOS tokens\n        # Get indices of the top beam_width probabilities\n        topn_idx = np.argpartition(np.negative(all_beams_logprob), beam_width)[\n            :beam_width\n        ]\n\n        for idx in topn_idx:\n            current_beam = all_beams[idx // logprobs_num]\n            result = output[idx // logprobs_num]\n            token_id = int(all_beams_token_id[idx])\n            assert result.outputs[0].logprobs is not None\n            logprobs_entry = result.outputs[0].logprobs[0]\n            new_beams.append(\n                BeamSearchSequence(\n                    tokens=current_beam.tokens + [token_id],\n                    logprobs=current_beam.logprobs + [logprobs_entry],\n                    lora_request=current_beam.lora_request,\n                    cum_logprob=float(all_beams_logprob[idx]),\n                    multi_modal_data=current_beam.multi_modal_data,\n                    mm_processor_kwargs=current_beam.mm_processor_kwargs,\n                )\n            )\n\n        all_beams = new_beams\n\n    completed.extend(all_beams)\n    sorted_completed = sorted(completed, key=sort_beams_key, reverse=True)\n    best_beams = sorted_completed[:beam_width]\n\n    for beam in best_beams:\n        if beam.tokens[-1] == eos_token_id and not ignore_eos:\n            # Skip the eos token in the text.\n            tokens = beam.tokens[tokenized_length:-1]\n        else:\n            tokens = beam.tokens[tokenized_length:]\n        beam.text = tokenizer.decode(tokens)\n\n    yield RequestOutput(\n        request_id=request_id,\n        prompt=prompt_text,\n        outputs=[\n            CompletionOutput(\n                text=beam.text,  # type: ignore\n                cumulative_logprob=beam.cum_logprob,\n                token_ids=beam.tokens[tokenized_length:],\n                index=i,\n                logprobs=beam.logprobs,\n                finish_reason=beam.finish_reason\n                if beam.finish_reason is not None\n                else \"length\",\n                stop_reason=beam.stop_reason,\n            )\n            for (i, beam) in enumerate(best_beams)\n        ],\n        finished=True,\n        prompt_token_ids=prompt_token_ids,\n        prompt_logprobs=None,\n    )",
      "language": "python"
    },
    {
      "code": "create_error_response(\n    message: str | Exception,\n    err_type: str = \"BadRequestError\",\n    status_code: HTTPStatus = BAD_REQUEST,\n    param: str | None = None,\n) -> ErrorResponse",
      "language": "typescript"
    },
    {
      "code": "create_error_response(\n    message: str | Exception,\n    err_type: str = \"BadRequestError\",\n    status_code: HTTPStatus = BAD_REQUEST,\n    param: str | None = None,\n) -> ErrorResponse",
      "language": "typescript"
    },
    {
      "code": "745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793",
      "language": "unknown"
    },
    {
      "code": "def create_error_response(\n    self,\n    message: str | Exception,\n    err_type: str = \"BadRequestError\",\n    status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,\n    param: str | None = None,\n) -> ErrorResponse:\n    exc: Exception | None = None\n\n    if isinstance(message, Exception):\n        exc = message\n\n        from vllm.entrypoints.openai.protocol import VLLMValidationError\n\n        if isinstance(exc, VLLMValidationError):\n            err_type = \"BadRequestError\"\n            status_code = HTTPStatus.BAD_REQUEST\n            param = exc.parameter\n        elif isinstance(exc, (ValueError, TypeError, RuntimeError)):\n            # Common validation errors from user input\n            err_type = \"BadRequestError\"\n            status_code = HTTPStatus.BAD_REQUEST\n            param = None\n        elif exc.__class__.__name__ == \"TemplateError\":\n            # jinja2.TemplateError (avoid importing jinja2)\n            err_type = \"BadRequestError\"\n            status_code = HTTPStatus.BAD_REQUEST\n            param = None\n        else:\n            err_type = \"InternalServerError\"\n            status_code = HTTPStatus.INTERNAL_SERVER_ERROR\n            param = None\n\n        message = str(exc)\n\n    if self.log_error_stack:\n        exc_type, _, _ = sys.exc_info()\n        if exc_type is not None:\n            traceback.print_exc()\n        else:\n            traceback.print_stack()\n    return ErrorResponse(\n        error=ErrorInfo(\n            message=message,\n            type=err_type,\n            code=status_code.value,\n            param=param,\n        )\n    )",
      "language": "python"
    },
    {
      "code": "def create_error_response(\n    self,\n    message: str | Exception,\n    err_type: str = \"BadRequestError\",\n    status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,\n    param: str | None = None,\n) -> ErrorResponse:\n    exc: Exception | None = None\n\n    if isinstance(message, Exception):\n        exc = message\n\n        from vllm.entrypoints.openai.protocol import VLLMValidationError\n\n        if isinstance(exc, VLLMValidationError):\n            err_type = \"BadRequestError\"\n            status_code = HTTPStatus.BAD_REQUEST\n            param = exc.parameter\n        elif isinstance(exc, (ValueError, TypeError, RuntimeError)):\n            # Common validation errors from user input\n            err_type = \"BadRequestError\"\n            status_code = HTTPStatus.BAD_REQUEST\n            param = None\n        elif exc.__class__.__name__ == \"TemplateError\":\n            # jinja2.TemplateError (avoid importing jinja2)\n            err_type = \"BadRequestError\"\n            status_code = HTTPStatus.BAD_REQUEST\n            param = None\n        else:\n            err_type = \"InternalServerError\"\n            status_code = HTTPStatus.INTERNAL_SERVER_ERROR\n            param = None\n\n        message = str(exc)\n\n    if self.log_error_stack:\n        exc_type, _, _ = sys.exc_info()\n        if exc_type is not None:\n            traceback.print_exc()\n        else:\n            traceback.print_stack()\n    return ErrorResponse(\n        error=ErrorInfo(\n            message=message,\n            type=err_type,\n            code=status_code.value,\n            param=param,\n        )\n    )",
      "language": "python"
    },
    {
      "code": "create_streaming_error_response(\n    message: str | Exception,\n    err_type: str = \"BadRequestError\",\n    status_code: HTTPStatus = BAD_REQUEST,\n    param: str | None = None,\n) -> str",
      "language": "typescript"
    },
    {
      "code": "create_streaming_error_response(\n    message: str | Exception,\n    err_type: str = \"BadRequestError\",\n    status_code: HTTPStatus = BAD_REQUEST,\n    param: str | None = None,\n) -> str",
      "language": "typescript"
    },
    {
      "code": "795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810",
      "language": "unknown"
    },
    {
      "code": "def create_streaming_error_response(\n    self,\n    message: str | Exception,\n    err_type: str = \"BadRequestError\",\n    status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,\n    param: str | None = None,\n) -> str:\n    json_str = json.dumps(\n        self.create_error_response(\n            message=message,\n            err_type=err_type,\n            status_code=status_code,\n            param=param,\n        ).model_dump()\n    )\n    return json_str",
      "language": "python"
    },
    {
      "code": "def create_streaming_error_response(\n    self,\n    message: str | Exception,\n    err_type: str = \"BadRequestError\",\n    status_code: HTTPStatus = HTTPStatus.BAD_REQUEST,\n    param: str | None = None,\n) -> str:\n    json_str = json.dumps(\n        self.create_error_response(\n            message=message,\n            err_type=err_type,\n            status_code=status_code,\n            param=param,\n        ).model_dump()\n    )\n    return json_str",
      "language": "python"
    },
    {
      "code": "handle(ctx: ServeContext) -> AnyResponse | ErrorResponse",
      "language": "php"
    },
    {
      "code": "handle(ctx: ServeContext) -> AnyResponse | ErrorResponse",
      "language": "php"
    },
    {
      "code": "602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612",
      "language": "unknown"
    },
    {
      "code": "async def handle(\n    self,\n    ctx: ServeContext,\n) -> AnyResponse | ErrorResponse:\n    generation: AsyncGenerator[AnyResponse | ErrorResponse, None]\n    generation = self._pipeline(ctx)\n\n    async for response in generation:\n        return response\n\n    return self.create_error_response(\"No response yielded from pipeline\")",
      "language": "python"
    },
    {
      "code": "async def handle(\n    self,\n    ctx: ServeContext,\n) -> AnyResponse | ErrorResponse:\n    generation: AsyncGenerator[AnyResponse | ErrorResponse, None]\n    generation = self._pipeline(ctx)\n\n    async for response in generation:\n        return response\n\n    return self.create_error_response(\"No response yielded from pipeline\")",
      "language": "python"
    },
    {
      "code": "reset_mm_cache() -> None",
      "language": "rust"
    },
    {
      "code": "reset_mm_cache() -> None",
      "language": "rust"
    },
    {
      "code": "304\n305\n306",
      "language": "unknown"
    },
    {
      "code": "async def reset_mm_cache(self) -> None:\n    self.input_processor.clear_mm_cache()\n    await self.engine_client.reset_mm_cache()",
      "language": "python"
    },
    {
      "code": "async def reset_mm_cache(self) -> None:\n    self.input_processor.clear_mm_cache()\n    await self.engine_client.reset_mm_cache()",
      "language": "python"
    },
    {
      "code": "176\n177\n178\n179\n180\n181\n182\n183",
      "language": "unknown"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass RequestProcessingMixin:\n    \"\"\"\n    Mixin for request processing,\n    handling prompt preparation and engine input.\n    \"\"\"\n\n    engine_prompts: list[TokensPrompt] | None = field(default_factory=list)",
      "language": "python"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass RequestProcessingMixin:\n    \"\"\"\n    Mixin for request processing,\n    handling prompt preparation and engine input.\n    \"\"\"\n\n    engine_prompts: list[TokensPrompt] | None = field(default_factory=list)",
      "language": "python"
    },
    {
      "code": "engine_prompts: list[TokensPrompt] | None = field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "engine_prompts: list[TokensPrompt] | None = field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    *, engine_prompts: list[TokensPrompt] | None = list()\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    *, engine_prompts: list[TokensPrompt] | None = list()\n) -> None",
      "language": "python"
    },
    {
      "code": "186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200",
      "language": "unknown"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass ResponseGenerationMixin:\n    \"\"\"\n    Mixin for response generation,\n    managing result generators and final batch results.\n    \"\"\"\n\n    result_generator: (\n        AsyncGenerator[tuple[int, RequestOutput | PoolingRequestOutput], None] | None\n    ) = None\n    final_res_batch: list[RequestOutput | PoolingRequestOutput] = field(\n        default_factory=list\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)",
      "language": "python"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass ResponseGenerationMixin:\n    \"\"\"\n    Mixin for response generation,\n    managing result generators and final batch results.\n    \"\"\"\n\n    result_generator: (\n        AsyncGenerator[tuple[int, RequestOutput | PoolingRequestOutput], None] | None\n    ) = None\n    final_res_batch: list[RequestOutput | PoolingRequestOutput] = field(\n        default_factory=list\n    )\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)",
      "language": "python"
    },
    {
      "code": "final_res_batch: list[\n    RequestOutput | PoolingRequestOutput\n] = field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "final_res_batch: list[\n    RequestOutput | PoolingRequestOutput\n] = field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "model_config = ConfigDict(arbitrary_types_allowed=True)",
      "language": "unknown"
    },
    {
      "code": "model_config = ConfigDict(arbitrary_types_allowed=True)",
      "language": "unknown"
    },
    {
      "code": "result_generator: (\n    AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "result_generator: (\n    AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None\n) = None",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    *,\n    result_generator: AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None = None,\n    final_res_batch: list[\n        RequestOutput | PoolingRequestOutput\n    ] = list(),\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    *,\n    result_generator: AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None = None,\n    final_res_batch: list[\n        RequestOutput | PoolingRequestOutput\n    ] = list(),\n) -> None",
      "language": "python"
    },
    {
      "code": "203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214",
      "language": "unknown"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass ServeContext(RequestProcessingMixin, ResponseGenerationMixin, Generic[RequestT]):\n    # Shared across all requests\n    request: RequestT\n    raw_request: Request | None = None\n    model_name: str\n    request_id: str\n    created_time: int = field(default_factory=lambda: int(time.time()))\n    lora_request: LoRARequest | None = None\n\n    # Shared across most requests\n    tokenizer: TokenizerLike | None = None",
      "language": "typescript"
    },
    {
      "code": "@dataclass(kw_only=True)\nclass ServeContext(RequestProcessingMixin, ResponseGenerationMixin, Generic[RequestT]):\n    # Shared across all requests\n    request: RequestT\n    raw_request: Request | None = None\n    model_name: str\n    request_id: str\n    created_time: int = field(default_factory=lambda: int(time.time()))\n    lora_request: LoRARequest | None = None\n\n    # Shared across most requests\n    tokenizer: TokenizerLike | None = None",
      "language": "typescript"
    },
    {
      "code": "created_time: int = field(\n    default_factory=lambda: int(time())\n)",
      "language": "typescript"
    },
    {
      "code": "created_time: int = field(\n    default_factory=lambda: int(time())\n)",
      "language": "typescript"
    },
    {
      "code": "lora_request: LoRARequest | None = None",
      "language": "yaml"
    },
    {
      "code": "lora_request: LoRARequest | None = None",
      "language": "yaml"
    },
    {
      "code": "model_name: str",
      "language": "yaml"
    },
    {
      "code": "model_name: str",
      "language": "yaml"
    },
    {
      "code": "raw_request: Request | None = None",
      "language": "yaml"
    },
    {
      "code": "raw_request: Request | None = None",
      "language": "yaml"
    },
    {
      "code": "request: RequestT",
      "language": "yaml"
    },
    {
      "code": "request: RequestT",
      "language": "yaml"
    },
    {
      "code": "request_id: str",
      "language": "yaml"
    },
    {
      "code": "request_id: str",
      "language": "yaml"
    },
    {
      "code": "tokenizer: TokenizerLike | None = None",
      "language": "yaml"
    },
    {
      "code": "tokenizer: TokenizerLike | None = None",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    *,\n    result_generator: AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None = None,\n    final_res_batch: list[\n        RequestOutput | PoolingRequestOutput\n    ] = list(),\n    engine_prompts: list[TokensPrompt] | None = list(),\n    request: RequestT,\n    raw_request: Request | None = None,\n    model_name: str,\n    request_id: str,\n    created_time: int = (lambda: int(time()))(),\n    lora_request: LoRARequest | None = None,\n    tokenizer: TokenizerLike | None = None,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    *,\n    result_generator: AsyncGenerator[\n        tuple[int, RequestOutput | PoolingRequestOutput],\n        None,\n    ]\n    | None = None,\n    final_res_batch: list[\n        RequestOutput | PoolingRequestOutput\n    ] = list(),\n    engine_prompts: list[TokensPrompt] | None = list(),\n    request: RequestT,\n    raw_request: Request | None = None,\n    model_name: str,\n    request_id: str,\n    created_time: int = (lambda: int(time()))(),\n    lora_request: LoRARequest | None = None,\n    tokenizer: TokenizerLike | None = None,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "clamp_prompt_logprobs(\n    prompt_logprobs: PromptLogprobs | None,\n) -> PromptLogprobs | None",
      "language": "rust"
    },
    {
      "code": "clamp_prompt_logprobs(\n    prompt_logprobs: PromptLogprobs | None,\n) -> PromptLogprobs | None",
      "language": "rust"
    },
    {
      "code": "1579\n1580\n1581\n1582\n1583\n1584\n1585\n1586\n1587\n1588\n1589\n1590\n1591",
      "language": "unknown"
    },
    {
      "code": "def clamp_prompt_logprobs(\n    prompt_logprobs: PromptLogprobs | None,\n) -> PromptLogprobs | None:\n    if prompt_logprobs is None:\n        return prompt_logprobs\n\n    for logprob_dict in prompt_logprobs:\n        if logprob_dict is None:\n            continue\n        for logprob_values in logprob_dict.values():\n            if logprob_values.logprob == float(\"-inf\"):\n                logprob_values.logprob = -9999.0\n    return prompt_logprobs",
      "language": "python"
    },
    {
      "code": "def clamp_prompt_logprobs(\n    prompt_logprobs: PromptLogprobs | None,\n) -> PromptLogprobs | None:\n    if prompt_logprobs is None:\n        return prompt_logprobs\n\n    for logprob_dict in prompt_logprobs:\n        if logprob_dict is None:\n            continue\n        for logprob_values in logprob_dict.values():\n            if logprob_values.logprob == float(\"-inf\"):\n                logprob_values.logprob = -9999.0\n    return prompt_logprobs",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}