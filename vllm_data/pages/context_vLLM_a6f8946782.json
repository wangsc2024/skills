{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
  "title": "context - vLLM",
  "content": "Bases: ConversationContext\n\nUpdate token usage statistics for the decode phase of generation.\n\nThe decode phase processes the generated output tokens. This method: 1. Counts output tokens from all completion outputs 2. Updates the total output token count 3. Tracks tokens generated in the current turn\n\nIn streaming mode, this is called for each token generated. In non-streaming mode, this is called once with all output tokens.\n\nThe RequestOutput containing generated token information\n\nNumber of output tokens processed in this call\n\nUpdate token usage statistics for the prefill phase of generation.\n\nThe prefill phase processes the input prompt tokens. This method: 1. Counts the prompt tokens for this turn 2. Calculates tool output tokens for multi-turn conversations 3. Updates cached token counts 4. Tracks state for next turn calculations\n\nTool output tokens are calculated as: current_prompt_tokens - last_turn_prompt_tokens - last_turn_output_tokens This represents tokens added between turns (typically tool responses).\n\nThe RequestOutput containing prompt token information\n\nCall container tool. Expect this to be run in a stateful docker with command line terminal. The official container tool would at least expect the following format: - for tool name: exec - args: { \"cmd\":List[str] \"command to execute\", \"workdir\":optional[str] \"current working directory\", \"env\":optional[object/dict] \"environment variables\", \"session_name\":optional[str] \"session name\", \"timeout\":optional[int] \"timeout in seconds\", \"user\":optional[str] \"user name\", }\n\nCan be used as coro to used in aexit\n\nBases: ConversationContext\n\nCall container tool. Expect this to be run in a stateful docker with command line terminal. The official container tool would at least expect the following format: - for tool name: exec - args: { \"cmd\":List[str] \"command to execute\", \"workdir\":optional[str] \"current working directory\", \"env\":optional[object/dict] \"environment variables\", \"session_name\":optional[str] \"session name\", \"timeout\":optional[int] \"timeout in seconds\", \"user\":optional[str] \"user name\", }\n\nCan be used as coro to used in aexit\n\nReturn true if the last message is a MCP tool call\n\nBases: ConversationContext\n\nThis is a context that cannot handle MCP tool calls\n\nReturn the final output, with complete text/token_ids/logprobs.\n\nBases: HarmonyContext\n\nTracks token and toolcall details for a single conversation turn.\n\nCreate a copy of this turn's token counts.\n\nReset counters for a new turn.\n\nCreates an error message when json parse failed.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.entrypoints.context ¶",
      "id": "vllm.entrypoints.context"
    },
    {
      "level": "h2",
      "text": "_TOOL_NAME_TO_TYPE_MAP module-attribute ¶",
      "id": "vllm.entrypoints.context._TOOL_NAME_TO_TYPE_MAP"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.entrypoints.context.logger"
    },
    {
      "level": "h2",
      "text": "ConversationContext ¶",
      "id": "vllm.entrypoints.context.ConversationContext"
    },
    {
      "level": "h3",
      "text": "append_output abstractmethod ¶",
      "id": "vllm.entrypoints.context.ConversationContext.append_output"
    },
    {
      "level": "h3",
      "text": "append_tool_output abstractmethod ¶",
      "id": "vllm.entrypoints.context.ConversationContext.append_tool_output"
    },
    {
      "level": "h3",
      "text": "call_tool abstractmethod async ¶",
      "id": "vllm.entrypoints.context.ConversationContext.call_tool"
    },
    {
      "level": "h3",
      "text": "cleanup_session abstractmethod async ¶",
      "id": "vllm.entrypoints.context.ConversationContext.cleanup_session"
    },
    {
      "level": "h3",
      "text": "init_tool_sessions abstractmethod async ¶",
      "id": "vllm.entrypoints.context.ConversationContext.init_tool_sessions"
    },
    {
      "level": "h3",
      "text": "need_builtin_tool_call abstractmethod ¶",
      "id": "vllm.entrypoints.context.ConversationContext.need_builtin_tool_call"
    },
    {
      "level": "h3",
      "text": "render_for_completion abstractmethod ¶",
      "id": "vllm.entrypoints.context.ConversationContext.render_for_completion"
    },
    {
      "level": "h2",
      "text": "HarmonyContext ¶",
      "id": "vllm.entrypoints.context.HarmonyContext"
    },
    {
      "level": "h3",
      "text": "_messages instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext._messages"
    },
    {
      "level": "h3",
      "text": "_tool_sessions instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext._tool_sessions"
    },
    {
      "level": "h3",
      "text": "all_turn_metrics instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.all_turn_metrics"
    },
    {
      "level": "h3",
      "text": "available_tools instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.available_tools"
    },
    {
      "level": "h3",
      "text": "called_tools instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.called_tools"
    },
    {
      "level": "h3",
      "text": "current_turn_metrics instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.current_turn_metrics"
    },
    {
      "level": "h3",
      "text": "finish_reason instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.finish_reason"
    },
    {
      "level": "h3",
      "text": "first_tok_of_message instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.first_tok_of_message"
    },
    {
      "level": "h3",
      "text": "is_first_turn instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.is_first_turn"
    },
    {
      "level": "h3",
      "text": "messages property ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.messages"
    },
    {
      "level": "h3",
      "text": "num_cached_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.num_cached_tokens"
    },
    {
      "level": "h3",
      "text": "num_init_messages instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.num_init_messages"
    },
    {
      "level": "h3",
      "text": "num_output_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.num_output_tokens"
    },
    {
      "level": "h3",
      "text": "num_prompt_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.num_prompt_tokens"
    },
    {
      "level": "h3",
      "text": "num_reasoning_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.num_reasoning_tokens"
    },
    {
      "level": "h3",
      "text": "num_tool_output_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.num_tool_output_tokens"
    },
    {
      "level": "h3",
      "text": "parser instance-attribute ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.parser"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.__init__"
    },
    {
      "level": "h3",
      "text": "_update_decode_token_usage ¶",
      "id": "vllm.entrypoints.context.HarmonyContext._update_decode_token_usage"
    },
    {
      "level": "h3",
      "text": "_update_num_reasoning_tokens ¶",
      "id": "vllm.entrypoints.context.HarmonyContext._update_num_reasoning_tokens"
    },
    {
      "level": "h3",
      "text": "_update_prefill_token_usage ¶",
      "id": "vllm.entrypoints.context.HarmonyContext._update_prefill_token_usage"
    },
    {
      "level": "h3",
      "text": "append_output ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.append_output"
    },
    {
      "level": "h3",
      "text": "append_tool_output ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.append_tool_output"
    },
    {
      "level": "h3",
      "text": "call_container_tool async ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.call_container_tool"
    },
    {
      "level": "h3",
      "text": "call_python_tool async ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.call_python_tool"
    },
    {
      "level": "h3",
      "text": "call_search_tool async ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.call_search_tool"
    },
    {
      "level": "h3",
      "text": "call_tool async ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.call_tool"
    },
    {
      "level": "h3",
      "text": "cleanup_session async ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.cleanup_session"
    },
    {
      "level": "h3",
      "text": "init_tool_sessions async ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.init_tool_sessions"
    },
    {
      "level": "h3",
      "text": "need_builtin_tool_call ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.need_builtin_tool_call"
    },
    {
      "level": "h3",
      "text": "render_for_completion ¶",
      "id": "vllm.entrypoints.context.HarmonyContext.render_for_completion"
    },
    {
      "level": "h2",
      "text": "ParsableContext ¶",
      "id": "vllm.entrypoints.context.ParsableContext"
    },
    {
      "level": "h3",
      "text": "_tool_sessions instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext._tool_sessions"
    },
    {
      "level": "h3",
      "text": "all_turn_metrics instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.all_turn_metrics"
    },
    {
      "level": "h3",
      "text": "available_tools instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.available_tools"
    },
    {
      "level": "h3",
      "text": "called_tools instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.called_tools"
    },
    {
      "level": "h3",
      "text": "chat_template instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.chat_template"
    },
    {
      "level": "h3",
      "text": "chat_template_content_format instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.chat_template_content_format"
    },
    {
      "level": "h3",
      "text": "input_messages instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.input_messages"
    },
    {
      "level": "h3",
      "text": "num_cached_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.num_cached_tokens"
    },
    {
      "level": "h3",
      "text": "num_output_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.num_output_tokens"
    },
    {
      "level": "h3",
      "text": "num_prompt_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.num_prompt_tokens"
    },
    {
      "level": "h3",
      "text": "num_reasoning_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.num_reasoning_tokens"
    },
    {
      "level": "h3",
      "text": "output_messages instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.output_messages"
    },
    {
      "level": "h3",
      "text": "parser instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.parser"
    },
    {
      "level": "h3",
      "text": "request instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.request"
    },
    {
      "level": "h3",
      "text": "tokenizer instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.tokenizer"
    },
    {
      "level": "h3",
      "text": "tool_dicts instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.tool_dicts"
    },
    {
      "level": "h3",
      "text": "tool_parser_cls instance-attribute ¶",
      "id": "vllm.entrypoints.context.ParsableContext.tool_parser_cls"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.context.ParsableContext.__init__"
    },
    {
      "level": "h3",
      "text": "append_output ¶",
      "id": "vllm.entrypoints.context.ParsableContext.append_output"
    },
    {
      "level": "h3",
      "text": "append_tool_output ¶",
      "id": "vllm.entrypoints.context.ParsableContext.append_tool_output"
    },
    {
      "level": "h3",
      "text": "call_container_tool async ¶",
      "id": "vllm.entrypoints.context.ParsableContext.call_container_tool"
    },
    {
      "level": "h3",
      "text": "call_python_tool async ¶",
      "id": "vllm.entrypoints.context.ParsableContext.call_python_tool"
    },
    {
      "level": "h3",
      "text": "call_search_tool async ¶",
      "id": "vllm.entrypoints.context.ParsableContext.call_search_tool"
    },
    {
      "level": "h3",
      "text": "call_tool async ¶",
      "id": "vllm.entrypoints.context.ParsableContext.call_tool"
    },
    {
      "level": "h3",
      "text": "cleanup_session async ¶",
      "id": "vllm.entrypoints.context.ParsableContext.cleanup_session"
    },
    {
      "level": "h3",
      "text": "init_tool_sessions async ¶",
      "id": "vllm.entrypoints.context.ParsableContext.init_tool_sessions"
    },
    {
      "level": "h3",
      "text": "need_builtin_tool_call ¶",
      "id": "vllm.entrypoints.context.ParsableContext.need_builtin_tool_call"
    },
    {
      "level": "h3",
      "text": "render_for_completion ¶",
      "id": "vllm.entrypoints.context.ParsableContext.render_for_completion"
    },
    {
      "level": "h2",
      "text": "SimpleContext ¶",
      "id": "vllm.entrypoints.context.SimpleContext"
    },
    {
      "level": "h3",
      "text": "_accumulated_logprobs instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext._accumulated_logprobs"
    },
    {
      "level": "h3",
      "text": "_accumulated_text instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext._accumulated_text"
    },
    {
      "level": "h3",
      "text": "_accumulated_token_ids instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext._accumulated_token_ids"
    },
    {
      "level": "h3",
      "text": "all_turn_metrics instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext.all_turn_metrics"
    },
    {
      "level": "h3",
      "text": "final_output property ¶",
      "id": "vllm.entrypoints.context.SimpleContext.final_output"
    },
    {
      "level": "h3",
      "text": "input_messages instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext.input_messages"
    },
    {
      "level": "h3",
      "text": "last_output instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext.last_output"
    },
    {
      "level": "h3",
      "text": "num_cached_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext.num_cached_tokens"
    },
    {
      "level": "h3",
      "text": "num_output_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext.num_output_tokens"
    },
    {
      "level": "h3",
      "text": "num_prompt_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext.num_prompt_tokens"
    },
    {
      "level": "h3",
      "text": "num_reasoning_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext.num_reasoning_tokens"
    },
    {
      "level": "h3",
      "text": "output_messages instance-attribute ¶",
      "id": "vllm.entrypoints.context.SimpleContext.output_messages"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.context.SimpleContext.__init__"
    },
    {
      "level": "h3",
      "text": "append_output ¶",
      "id": "vllm.entrypoints.context.SimpleContext.append_output"
    },
    {
      "level": "h3",
      "text": "append_tool_output ¶",
      "id": "vllm.entrypoints.context.SimpleContext.append_tool_output"
    },
    {
      "level": "h3",
      "text": "call_tool async ¶",
      "id": "vllm.entrypoints.context.SimpleContext.call_tool"
    },
    {
      "level": "h3",
      "text": "cleanup_session async ¶",
      "id": "vllm.entrypoints.context.SimpleContext.cleanup_session"
    },
    {
      "level": "h3",
      "text": "init_tool_sessions async ¶",
      "id": "vllm.entrypoints.context.SimpleContext.init_tool_sessions"
    },
    {
      "level": "h3",
      "text": "need_builtin_tool_call ¶",
      "id": "vllm.entrypoints.context.SimpleContext.need_builtin_tool_call"
    },
    {
      "level": "h3",
      "text": "render_for_completion ¶",
      "id": "vllm.entrypoints.context.SimpleContext.render_for_completion"
    },
    {
      "level": "h2",
      "text": "StreamingHarmonyContext ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext"
    },
    {
      "level": "h3",
      "text": "encoding instance-attribute ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.encoding"
    },
    {
      "level": "h3",
      "text": "first_tok_of_message instance-attribute ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.first_tok_of_message"
    },
    {
      "level": "h3",
      "text": "last_output instance-attribute ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.last_output"
    },
    {
      "level": "h3",
      "text": "last_tok instance-attribute ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.last_tok"
    },
    {
      "level": "h3",
      "text": "messages property ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.messages"
    },
    {
      "level": "h3",
      "text": "parser instance-attribute ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.parser"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.__init__"
    },
    {
      "level": "h3",
      "text": "append_output ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.append_output"
    },
    {
      "level": "h3",
      "text": "append_tool_output ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.append_tool_output"
    },
    {
      "level": "h3",
      "text": "is_assistant_action_turn ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.is_assistant_action_turn"
    },
    {
      "level": "h3",
      "text": "is_expecting_start ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.is_expecting_start"
    },
    {
      "level": "h3",
      "text": "render_for_completion ¶",
      "id": "vllm.entrypoints.context.StreamingHarmonyContext.render_for_completion"
    },
    {
      "level": "h2",
      "text": "TurnMetrics ¶",
      "id": "vllm.entrypoints.context.TurnMetrics"
    },
    {
      "level": "h3",
      "text": "cached_input_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.TurnMetrics.cached_input_tokens"
    },
    {
      "level": "h3",
      "text": "input_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.TurnMetrics.input_tokens"
    },
    {
      "level": "h3",
      "text": "output_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.TurnMetrics.output_tokens"
    },
    {
      "level": "h3",
      "text": "tool_output_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.context.TurnMetrics.tool_output_tokens"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.context.TurnMetrics.__init__"
    },
    {
      "level": "h3",
      "text": "copy ¶",
      "id": "vllm.entrypoints.context.TurnMetrics.copy"
    },
    {
      "level": "h3",
      "text": "reset ¶",
      "id": "vllm.entrypoints.context.TurnMetrics.reset"
    },
    {
      "level": "h2",
      "text": "_create_json_parse_error_messages ¶",
      "id": "vllm.entrypoints.context._create_json_parse_error_messages"
    },
    {
      "level": "h2",
      "text": "_map_tool_name_to_tool_type ¶",
      "id": "vllm.entrypoints.context._map_tool_name_to_tool_type"
    }
  ],
  "code_samples": [
    {
      "code": "_TOOL_NAME_TO_TYPE_MAP = {\n    \"browser\": \"web_search_preview\",\n    \"python\": \"code_interpreter\",\n    \"container\": \"container\",\n}",
      "language": "json"
    },
    {
      "code": "_TOOL_NAME_TO_TYPE_MAP = {\n    \"browser\": \"web_search_preview\",\n    \"python\": \"code_interpreter\",\n    \"container\": \"container\",\n}",
      "language": "json"
    },
    {
      "code": "logger = getLogger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = getLogger(__name__)",
      "language": "unknown"
    },
    {
      "code": "105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138",
      "language": "unknown"
    },
    {
      "code": "class ConversationContext(ABC):\n    @abstractmethod\n    def append_output(self, output: RequestOutput) -> None:\n        pass\n\n    @abstractmethod\n    def append_tool_output(self, output) -> None:\n        pass\n\n    @abstractmethod\n    async def call_tool(self) -> list[Message]:\n        pass\n\n    @abstractmethod\n    def need_builtin_tool_call(self) -> bool:\n        pass\n\n    @abstractmethod\n    def render_for_completion(self) -> list[int]:\n        pass\n\n    @abstractmethod\n    async def init_tool_sessions(\n        self,\n        tool_server: ToolServer | None,\n        exit_stack: AsyncExitStack,\n        request_id: str,\n        mcp_tools: dict[str, Mcp],\n    ) -> None:\n        pass\n\n    @abstractmethod\n    async def cleanup_session(self) -> None:\n        raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "class ConversationContext(ABC):\n    @abstractmethod\n    def append_output(self, output: RequestOutput) -> None:\n        pass\n\n    @abstractmethod\n    def append_tool_output(self, output) -> None:\n        pass\n\n    @abstractmethod\n    async def call_tool(self) -> list[Message]:\n        pass\n\n    @abstractmethod\n    def need_builtin_tool_call(self) -> bool:\n        pass\n\n    @abstractmethod\n    def render_for_completion(self) -> list[int]:\n        pass\n\n    @abstractmethod\n    async def init_tool_sessions(\n        self,\n        tool_server: ToolServer | None,\n        exit_stack: AsyncExitStack,\n        request_id: str,\n        mcp_tools: dict[str, Mcp],\n    ) -> None:\n        pass\n\n    @abstractmethod\n    async def cleanup_session(self) -> None:\n        raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "append_output(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "append_output(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "106\n107\n108",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef append_output(self, output: RequestOutput) -> None:\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef append_output(self, output: RequestOutput) -> None:\n    pass",
      "language": "python"
    },
    {
      "code": "append_tool_output(output) -> None",
      "language": "rust"
    },
    {
      "code": "append_tool_output(output) -> None",
      "language": "rust"
    },
    {
      "code": "110\n111\n112",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef append_tool_output(self, output) -> None:\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef append_tool_output(self, output) -> None:\n    pass",
      "language": "python"
    },
    {
      "code": "call_tool() -> list[Message]",
      "language": "php"
    },
    {
      "code": "call_tool() -> list[Message]",
      "language": "php"
    },
    {
      "code": "114\n115\n116",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\nasync def call_tool(self) -> list[Message]:\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\nasync def call_tool(self) -> list[Message]:\n    pass",
      "language": "python"
    },
    {
      "code": "cleanup_session() -> None",
      "language": "rust"
    },
    {
      "code": "cleanup_session() -> None",
      "language": "rust"
    },
    {
      "code": "136\n137\n138",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\nasync def cleanup_session(self) -> None:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "@abstractmethod\nasync def cleanup_session(self) -> None:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "init_tool_sessions(\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n) -> None",
      "language": "rust"
    },
    {
      "code": "init_tool_sessions(\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n) -> None",
      "language": "rust"
    },
    {
      "code": "126\n127\n128\n129\n130\n131\n132\n133\n134",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\nasync def init_tool_sessions(\n    self,\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n) -> None:\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\nasync def init_tool_sessions(\n    self,\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n) -> None:\n    pass",
      "language": "python"
    },
    {
      "code": "need_builtin_tool_call() -> bool",
      "language": "php"
    },
    {
      "code": "need_builtin_tool_call() -> bool",
      "language": "php"
    },
    {
      "code": "118\n119\n120",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef need_builtin_tool_call(self) -> bool:\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef need_builtin_tool_call(self) -> bool:\n    pass",
      "language": "python"
    },
    {
      "code": "render_for_completion() -> list[int]",
      "language": "php"
    },
    {
      "code": "render_for_completion() -> list[int]",
      "language": "php"
    },
    {
      "code": "122\n123\n124",
      "language": "unknown"
    },
    {
      "code": "@abstractmethod\ndef render_for_completion(self) -> list[int]:\n    pass",
      "language": "python"
    },
    {
      "code": "@abstractmethod\ndef render_for_completion(self) -> list[int]:\n    pass",
      "language": "python"
    },
    {
      "code": "503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815",
      "language": "unknown"
    },
    {
      "code": "class HarmonyContext(ConversationContext):\n    def __init__(\n        self,\n        messages: list,\n        available_tools: list[str],\n    ):\n        self._messages = messages\n        self.finish_reason: str | None = None\n        self.available_tools = available_tools\n        self._tool_sessions: dict[str, ClientSession | Tool] = {}\n        self.called_tools: set[str] = set()\n\n        self.parser = get_streamable_parser_for_assistant()\n        self.num_init_messages = len(messages)\n        self.num_prompt_tokens = 0\n        self.num_output_tokens = 0\n        self.num_cached_tokens = 0\n        self.num_reasoning_tokens = 0\n        self.num_tool_output_tokens = 0\n\n        # Turn tracking - replaces multiple individual tracking variables\n        self.current_turn_metrics = TurnMetrics()\n        # Track metrics for all turns\n        self.all_turn_metrics: list[TurnMetrics] = []\n        self.is_first_turn = True\n        self.first_tok_of_message = True  # For streaming support\n\n    def _update_num_reasoning_tokens(self):\n        # Count all analysis and commentary channels as reasoning tokens\n        if self.parser.current_channel in {\"analysis\", \"commentary\"}:\n            self.num_reasoning_tokens += 1\n\n    def append_output(self, output: RequestOutput) -> None:\n        output_token_ids = output.outputs[0].token_ids\n        self.parser = get_streamable_parser_for_assistant()\n        for token_id in output_token_ids:\n            self.parser.process(token_id)\n            # Check if the current token is part of reasoning content\n            self._update_num_reasoning_tokens()\n        self._update_prefill_token_usage(output)\n        self._update_decode_token_usage(output)\n        # Append current turn to all turn list for next turn's calculations\n        self.all_turn_metrics.append(self.current_turn_metrics.copy())\n        self.current_turn_metrics.reset()\n        # append_output is called only once before tool calling\n        # in non-streaming case\n        # so we can append all the parser messages to _messages\n        output_msgs = self.parser.messages\n        # The responses finish reason is set in the last message\n        self.finish_reason = output.outputs[0].finish_reason\n        self._messages.extend(output_msgs)\n\n    def append_tool_output(self, output: list[Message]) -> None:\n        output_msgs = output\n        self._messages.extend(output_msgs)\n\n    def _update_prefill_token_usage(self, output: RequestOutput) -> None:\n        \"\"\"Update token usage statistics for the prefill phase of generation.\n\n        The prefill phase processes the input prompt tokens. This method:\n        1. Counts the prompt tokens for this turn\n        2. Calculates tool output tokens for multi-turn conversations\n        3. Updates cached token counts\n        4. Tracks state for next turn calculations\n\n        Tool output tokens are calculated as:\n        current_prompt_tokens - last_turn_prompt_tokens -\n        last_turn_output_tokens\n        This represents tokens added between turns (typically tool responses).\n\n        Args:\n            output: The RequestOutput containing prompt token information\n        \"\"\"\n        if output.prompt_token_ids is not None:\n            this_turn_input_tokens = len(output.prompt_token_ids)\n        else:\n            this_turn_input_tokens = 0\n            logger.error(\"RequestOutput appended contains no prompt_token_ids.\")\n\n        # Update current turn input tokens\n        self.current_turn_metrics.input_tokens = this_turn_input_tokens\n        self.num_prompt_tokens += this_turn_input_tokens\n\n        # Calculate tool tokens (except on first turn)\n        if self.is_first_turn:\n            self.is_first_turn = False\n        else:\n            previous_turn = self.all_turn_metrics[-1]\n            # start counting tool after first turn\n            # tool tokens = this turn prefill - last turn prefill -\n            # last turn decode\n            this_turn_tool_tokens = (\n                self.current_turn_metrics.input_tokens\n                - previous_turn.input_tokens\n                - previous_turn.output_tokens\n            )\n\n            # Handle negative tool token counts (shouldn't happen in normal\n            # cases)\n            if this_turn_tool_tokens < 0:\n                logger.error(\n                    \"Negative tool output tokens calculated: %d \"\n                    \"(current_input=%d, previous_input=%d, \"\n                    \"previous_output=%d). Setting to 0.\",\n                    this_turn_tool_tokens,\n                    self.current_turn_metrics.input_tokens,\n                    previous_turn.input_tokens,\n                    previous_turn.output_tokens,\n                )\n                this_turn_tool_tokens = 0\n\n            self.num_tool_output_tokens += this_turn_tool_tokens\n            self.current_turn_metrics.tool_output_tokens = this_turn_tool_tokens\n\n        # Update cached tokens\n        num_cached_token = output.num_cached_tokens\n        if num_cached_token is not None:\n            self.num_cached_tokens += num_cached_token\n            self.current_turn_metrics.cached_input_tokens = num_cached_token\n\n    def _update_decode_token_usage(self, output: RequestOutput) -> int:\n        \"\"\"Update token usage statistics for the decode phase of generation.\n\n        The decode phase processes the generated output tokens. This method:\n        1. Counts output tokens from all completion outputs\n        2. Updates the total output token count\n        3. Tracks tokens generated in the current turn\n\n        In streaming mode, this is called for each token generated.\n        In non-streaming mode, this is called once with all output tokens.\n\n        Args:\n            output: The RequestOutput containing generated token information\n\n        Returns:\n            int: Number of output tokens processed in this call\n        \"\"\"\n        updated_output_token_count = 0\n        if output.outputs:\n            for completion_output in output.outputs:\n                # only keep last round\n                updated_output_token_count += len(completion_output.token_ids)\n            self.num_output_tokens += updated_output_token_count\n            self.current_turn_metrics.output_tokens += updated_output_token_count\n        return updated_output_token_count\n\n    @property\n    def messages(self) -> list:\n        return self._messages\n\n    def need_builtin_tool_call(self) -> bool:\n        last_msg = self.messages[-1]\n        recipient = last_msg.recipient\n        return recipient is not None and (\n            recipient.startswith(\"browser.\")\n            or recipient.startswith(\"python\")\n            or recipient.startswith(\"container.\")\n        )\n\n    async def call_tool(self) -> list[Message]:\n        if not self.messages:\n            return []\n        last_msg = self.messages[-1]\n        recipient = last_msg.recipient\n        if recipient is not None:\n            if recipient.startswith(\"browser.\"):\n                return await self.call_search_tool(\n                    self._tool_sessions[\"browser\"], last_msg\n                )\n            elif recipient.startswith(\"python\"):\n                return await self.call_python_tool(\n                    self._tool_sessions[\"python\"], last_msg\n                )\n            elif recipient.startswith(\"container.\"):\n                return await self.call_container_tool(\n                    self._tool_sessions[\"container\"], last_msg\n                )\n        raise ValueError(\"No tool call found\")\n\n    def render_for_completion(self) -> list[int]:\n        return render_for_completion(self.messages)\n\n    async def call_search_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n    ) -> list[Message]:\n        self.called_tools.add(\"browser\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result(self)\n        tool_name = last_msg.recipient.split(\".\")[1]\n        if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n            try:\n                args = json.loads(last_msg.content[0].text)\n            except json.JSONDecodeError as e:\n                return _create_json_parse_error_messages(last_msg, e)\n        else:\n            args = json.loads(last_msg.content[0].text)\n        result = await tool_session.call_tool(tool_name, args)\n        result_str = result.content[0].text\n        content = TextContent(text=result_str)\n        author = Author(role=Role.TOOL, name=last_msg.recipient)\n        return [\n            Message(\n                author=author,\n                content=[content],\n                recipient=Role.ASSISTANT,\n                channel=last_msg.channel,\n            )\n        ]\n\n    async def call_python_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n    ) -> list[Message]:\n        self.called_tools.add(\"python\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result(self)\n        param = {\n            \"code\": last_msg.content[0].text,\n        }\n        result = await tool_session.call_tool(\"python\", param)\n        result_str = result.content[0].text\n\n        content = TextContent(text=result_str)\n        author = Author(role=Role.TOOL, name=\"python\")\n\n        return [\n            Message(\n                author=author,\n                content=[content],\n                channel=last_msg.channel,\n                recipient=Role.ASSISTANT,\n            )\n        ]\n\n    async def init_tool_sessions(\n        self,\n        tool_server: ToolServer | None,\n        exit_stack: AsyncExitStack,\n        request_id: str,\n        mcp_tools: dict[str, Mcp],\n    ):\n        if tool_server:\n            for tool_name in self.available_tools:\n                if tool_name not in self._tool_sessions:\n                    tool_type = _map_tool_name_to_tool_type(tool_name)\n                    headers = (\n                        mcp_tools[tool_type].headers if tool_type in mcp_tools else None\n                    )\n                    tool_session = await exit_stack.enter_async_context(\n                        tool_server.new_session(tool_name, request_id, headers)\n                    )\n                    self._tool_sessions[tool_name] = tool_session\n                    exit_stack.push_async_exit(self.cleanup_session)\n\n    async def call_container_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n    ) -> list[Message]:\n        \"\"\"\n        Call container tool. Expect this to be run in a stateful docker\n        with command line terminal.\n        The official container tool would at least\n        expect the following format:\n        - for tool name: exec\n            - args:\n                {\n                    \"cmd\":List[str] \"command to execute\",\n                    \"workdir\":optional[str] \"current working directory\",\n                    \"env\":optional[object/dict] \"environment variables\",\n                    \"session_name\":optional[str] \"session name\",\n                    \"timeout\":optional[int] \"timeout in seconds\",\n                    \"user\":optional[str] \"user name\",\n                }\n        \"\"\"\n        self.called_tools.add(\"container\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result(self)\n        tool_name = last_msg.recipient.split(\".\")[1].split(\" \")[0]\n        if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n            try:\n                args = json.loads(last_msg.content[0].text)\n            except json.JSONDecodeError as e:\n                return _create_json_parse_error_messages(last_msg, e)\n        else:\n            args = json.loads(last_msg.content[0].text)\n        result = await tool_session.call_tool(tool_name, args)\n        result_str = result.content[0].text\n        content = TextContent(text=result_str)\n        author = Author(role=Role.TOOL, name=last_msg.recipient)\n        return [\n            Message(\n                author=author,\n                content=[content],\n                recipient=Role.ASSISTANT,\n                channel=last_msg.channel,\n            )\n        ]\n\n    async def cleanup_session(self, *args, **kwargs) -> None:\n        \"\"\"Can be used as coro to used in __aexit__\"\"\"\n\n        async def cleanup_tool_session(tool_session):\n            if not isinstance(tool_session, Tool):\n                logger.info(\n                    \"Cleaning up tool session for %s\", tool_session._client_info\n                )\n                with contextlib.suppress(Exception):\n                    await tool_session.call_tool(\"cleanup_session\", {})\n\n        await asyncio.gather(\n            *(\n                cleanup_tool_session(self._tool_sessions[tool])\n                for tool in self.called_tools\n            )\n        )",
      "language": "python"
    },
    {
      "code": "class HarmonyContext(ConversationContext):\n    def __init__(\n        self,\n        messages: list,\n        available_tools: list[str],\n    ):\n        self._messages = messages\n        self.finish_reason: str | None = None\n        self.available_tools = available_tools\n        self._tool_sessions: dict[str, ClientSession | Tool] = {}\n        self.called_tools: set[str] = set()\n\n        self.parser = get_streamable_parser_for_assistant()\n        self.num_init_messages = len(messages)\n        self.num_prompt_tokens = 0\n        self.num_output_tokens = 0\n        self.num_cached_tokens = 0\n        self.num_reasoning_tokens = 0\n        self.num_tool_output_tokens = 0\n\n        # Turn tracking - replaces multiple individual tracking variables\n        self.current_turn_metrics = TurnMetrics()\n        # Track metrics for all turns\n        self.all_turn_metrics: list[TurnMetrics] = []\n        self.is_first_turn = True\n        self.first_tok_of_message = True  # For streaming support\n\n    def _update_num_reasoning_tokens(self):\n        # Count all analysis and commentary channels as reasoning tokens\n        if self.parser.current_channel in {\"analysis\", \"commentary\"}:\n            self.num_reasoning_tokens += 1\n\n    def append_output(self, output: RequestOutput) -> None:\n        output_token_ids = output.outputs[0].token_ids\n        self.parser = get_streamable_parser_for_assistant()\n        for token_id in output_token_ids:\n            self.parser.process(token_id)\n            # Check if the current token is part of reasoning content\n            self._update_num_reasoning_tokens()\n        self._update_prefill_token_usage(output)\n        self._update_decode_token_usage(output)\n        # Append current turn to all turn list for next turn's calculations\n        self.all_turn_metrics.append(self.current_turn_metrics.copy())\n        self.current_turn_metrics.reset()\n        # append_output is called only once before tool calling\n        # in non-streaming case\n        # so we can append all the parser messages to _messages\n        output_msgs = self.parser.messages\n        # The responses finish reason is set in the last message\n        self.finish_reason = output.outputs[0].finish_reason\n        self._messages.extend(output_msgs)\n\n    def append_tool_output(self, output: list[Message]) -> None:\n        output_msgs = output\n        self._messages.extend(output_msgs)\n\n    def _update_prefill_token_usage(self, output: RequestOutput) -> None:\n        \"\"\"Update token usage statistics for the prefill phase of generation.\n\n        The prefill phase processes the input prompt tokens. This method:\n        1. Counts the prompt tokens for this turn\n        2. Calculates tool output tokens for multi-turn conversations\n        3. Updates cached token counts\n        4. Tracks state for next turn calculations\n\n        Tool output tokens are calculated as:\n        current_prompt_tokens - last_turn_prompt_tokens -\n        last_turn_output_tokens\n        This represents tokens added between turns (typically tool responses).\n\n        Args:\n            output: The RequestOutput containing prompt token information\n        \"\"\"\n        if output.prompt_token_ids is not None:\n            this_turn_input_tokens = len(output.prompt_token_ids)\n        else:\n            this_turn_input_tokens = 0\n            logger.error(\"RequestOutput appended contains no prompt_token_ids.\")\n\n        # Update current turn input tokens\n        self.current_turn_metrics.input_tokens = this_turn_input_tokens\n        self.num_prompt_tokens += this_turn_input_tokens\n\n        # Calculate tool tokens (except on first turn)\n        if self.is_first_turn:\n            self.is_first_turn = False\n        else:\n            previous_turn = self.all_turn_metrics[-1]\n            # start counting tool after first turn\n            # tool tokens = this turn prefill - last turn prefill -\n            # last turn decode\n            this_turn_tool_tokens = (\n                self.current_turn_metrics.input_tokens\n                - previous_turn.input_tokens\n                - previous_turn.output_tokens\n            )\n\n            # Handle negative tool token counts (shouldn't happen in normal\n            # cases)\n            if this_turn_tool_tokens < 0:\n                logger.error(\n                    \"Negative tool output tokens calculated: %d \"\n                    \"(current_input=%d, previous_input=%d, \"\n                    \"previous_output=%d). Setting to 0.\",\n                    this_turn_tool_tokens,\n                    self.current_turn_metrics.input_tokens,\n                    previous_turn.input_tokens,\n                    previous_turn.output_tokens,\n                )\n                this_turn_tool_tokens = 0\n\n            self.num_tool_output_tokens += this_turn_tool_tokens\n            self.current_turn_metrics.tool_output_tokens = this_turn_tool_tokens\n\n        # Update cached tokens\n        num_cached_token = output.num_cached_tokens\n        if num_cached_token is not None:\n            self.num_cached_tokens += num_cached_token\n            self.current_turn_metrics.cached_input_tokens = num_cached_token\n\n    def _update_decode_token_usage(self, output: RequestOutput) -> int:\n        \"\"\"Update token usage statistics for the decode phase of generation.\n\n        The decode phase processes the generated output tokens. This method:\n        1. Counts output tokens from all completion outputs\n        2. Updates the total output token count\n        3. Tracks tokens generated in the current turn\n\n        In streaming mode, this is called for each token generated.\n        In non-streaming mode, this is called once with all output tokens.\n\n        Args:\n            output: The RequestOutput containing generated token information\n\n        Returns:\n            int: Number of output tokens processed in this call\n        \"\"\"\n        updated_output_token_count = 0\n        if output.outputs:\n            for completion_output in output.outputs:\n                # only keep last round\n                updated_output_token_count += len(completion_output.token_ids)\n            self.num_output_tokens += updated_output_token_count\n            self.current_turn_metrics.output_tokens += updated_output_token_count\n        return updated_output_token_count\n\n    @property\n    def messages(self) -> list:\n        return self._messages\n\n    def need_builtin_tool_call(self) -> bool:\n        last_msg = self.messages[-1]\n        recipient = last_msg.recipient\n        return recipient is not None and (\n            recipient.startswith(\"browser.\")\n            or recipient.startswith(\"python\")\n            or recipient.startswith(\"container.\")\n        )\n\n    async def call_tool(self) -> list[Message]:\n        if not self.messages:\n            return []\n        last_msg = self.messages[-1]\n        recipient = last_msg.recipient\n        if recipient is not None:\n            if recipient.startswith(\"browser.\"):\n                return await self.call_search_tool(\n                    self._tool_sessions[\"browser\"], last_msg\n                )\n            elif recipient.startswith(\"python\"):\n                return await self.call_python_tool(\n                    self._tool_sessions[\"python\"], last_msg\n                )\n            elif recipient.startswith(\"container.\"):\n                return await self.call_container_tool(\n                    self._tool_sessions[\"container\"], last_msg\n                )\n        raise ValueError(\"No tool call found\")\n\n    def render_for_completion(self) -> list[int]:\n        return render_for_completion(self.messages)\n\n    async def call_search_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n    ) -> list[Message]:\n        self.called_tools.add(\"browser\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result(self)\n        tool_name = last_msg.recipient.split(\".\")[1]\n        if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n            try:\n                args = json.loads(last_msg.content[0].text)\n            except json.JSONDecodeError as e:\n                return _create_json_parse_error_messages(last_msg, e)\n        else:\n            args = json.loads(last_msg.content[0].text)\n        result = await tool_session.call_tool(tool_name, args)\n        result_str = result.content[0].text\n        content = TextContent(text=result_str)\n        author = Author(role=Role.TOOL, name=last_msg.recipient)\n        return [\n            Message(\n                author=author,\n                content=[content],\n                recipient=Role.ASSISTANT,\n                channel=last_msg.channel,\n            )\n        ]\n\n    async def call_python_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n    ) -> list[Message]:\n        self.called_tools.add(\"python\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result(self)\n        param = {\n            \"code\": last_msg.content[0].text,\n        }\n        result = await tool_session.call_tool(\"python\", param)\n        result_str = result.content[0].text\n\n        content = TextContent(text=result_str)\n        author = Author(role=Role.TOOL, name=\"python\")\n\n        return [\n            Message(\n                author=author,\n                content=[content],\n                channel=last_msg.channel,\n                recipient=Role.ASSISTANT,\n            )\n        ]\n\n    async def init_tool_sessions(\n        self,\n        tool_server: ToolServer | None,\n        exit_stack: AsyncExitStack,\n        request_id: str,\n        mcp_tools: dict[str, Mcp],\n    ):\n        if tool_server:\n            for tool_name in self.available_tools:\n                if tool_name not in self._tool_sessions:\n                    tool_type = _map_tool_name_to_tool_type(tool_name)\n                    headers = (\n                        mcp_tools[tool_type].headers if tool_type in mcp_tools else None\n                    )\n                    tool_session = await exit_stack.enter_async_context(\n                        tool_server.new_session(tool_name, request_id, headers)\n                    )\n                    self._tool_sessions[tool_name] = tool_session\n                    exit_stack.push_async_exit(self.cleanup_session)\n\n    async def call_container_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n    ) -> list[Message]:\n        \"\"\"\n        Call container tool. Expect this to be run in a stateful docker\n        with command line terminal.\n        The official container tool would at least\n        expect the following format:\n        - for tool name: exec\n            - args:\n                {\n                    \"cmd\":List[str] \"command to execute\",\n                    \"workdir\":optional[str] \"current working directory\",\n                    \"env\":optional[object/dict] \"environment variables\",\n                    \"session_name\":optional[str] \"session name\",\n                    \"timeout\":optional[int] \"timeout in seconds\",\n                    \"user\":optional[str] \"user name\",\n                }\n        \"\"\"\n        self.called_tools.add(\"container\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result(self)\n        tool_name = last_msg.recipient.split(\".\")[1].split(\" \")[0]\n        if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n            try:\n                args = json.loads(last_msg.content[0].text)\n            except json.JSONDecodeError as e:\n                return _create_json_parse_error_messages(last_msg, e)\n        else:\n            args = json.loads(last_msg.content[0].text)\n        result = await tool_session.call_tool(tool_name, args)\n        result_str = result.content[0].text\n        content = TextContent(text=result_str)\n        author = Author(role=Role.TOOL, name=last_msg.recipient)\n        return [\n            Message(\n                author=author,\n                content=[content],\n                recipient=Role.ASSISTANT,\n                channel=last_msg.channel,\n            )\n        ]\n\n    async def cleanup_session(self, *args, **kwargs) -> None:\n        \"\"\"Can be used as coro to used in __aexit__\"\"\"\n\n        async def cleanup_tool_session(tool_session):\n            if not isinstance(tool_session, Tool):\n                logger.info(\n                    \"Cleaning up tool session for %s\", tool_session._client_info\n                )\n                with contextlib.suppress(Exception):\n                    await tool_session.call_tool(\"cleanup_session\", {})\n\n        await asyncio.gather(\n            *(\n                cleanup_tool_session(self._tool_sessions[tool])\n                for tool in self.called_tools\n            )\n        )",
      "language": "python"
    },
    {
      "code": "_messages = messages",
      "language": "unknown"
    },
    {
      "code": "_messages = messages",
      "language": "unknown"
    },
    {
      "code": "_tool_sessions: dict[str, ClientSession | Tool] = {}",
      "language": "yaml"
    },
    {
      "code": "_tool_sessions: dict[str, ClientSession | Tool] = {}",
      "language": "yaml"
    },
    {
      "code": "all_turn_metrics: list[TurnMetrics] = []",
      "language": "yaml"
    },
    {
      "code": "all_turn_metrics: list[TurnMetrics] = []",
      "language": "yaml"
    },
    {
      "code": "available_tools = available_tools",
      "language": "unknown"
    },
    {
      "code": "available_tools = available_tools",
      "language": "unknown"
    },
    {
      "code": "called_tools: set[str] = set()",
      "language": "yaml"
    },
    {
      "code": "called_tools: set[str] = set()",
      "language": "yaml"
    },
    {
      "code": "current_turn_metrics = TurnMetrics()",
      "language": "unknown"
    },
    {
      "code": "current_turn_metrics = TurnMetrics()",
      "language": "unknown"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "finish_reason: str | None = None",
      "language": "yaml"
    },
    {
      "code": "first_tok_of_message = True",
      "language": "unknown"
    },
    {
      "code": "first_tok_of_message = True",
      "language": "unknown"
    },
    {
      "code": "is_first_turn = True",
      "language": "unknown"
    },
    {
      "code": "is_first_turn = True",
      "language": "unknown"
    },
    {
      "code": "messages: list",
      "language": "yaml"
    },
    {
      "code": "messages: list",
      "language": "yaml"
    },
    {
      "code": "num_cached_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_cached_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_init_messages = len(messages)",
      "language": "unknown"
    },
    {
      "code": "num_init_messages = len(messages)",
      "language": "unknown"
    },
    {
      "code": "num_output_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_output_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_prompt_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_prompt_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_reasoning_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_reasoning_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_tool_output_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_tool_output_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "parser = get_streamable_parser_for_assistant()",
      "language": "unknown"
    },
    {
      "code": "parser = get_streamable_parser_for_assistant()",
      "language": "unknown"
    },
    {
      "code": "__init__(messages: list, available_tools: list[str])",
      "language": "python"
    },
    {
      "code": "__init__(messages: list, available_tools: list[str])",
      "language": "python"
    },
    {
      "code": "504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    messages: list,\n    available_tools: list[str],\n):\n    self._messages = messages\n    self.finish_reason: str | None = None\n    self.available_tools = available_tools\n    self._tool_sessions: dict[str, ClientSession | Tool] = {}\n    self.called_tools: set[str] = set()\n\n    self.parser = get_streamable_parser_for_assistant()\n    self.num_init_messages = len(messages)\n    self.num_prompt_tokens = 0\n    self.num_output_tokens = 0\n    self.num_cached_tokens = 0\n    self.num_reasoning_tokens = 0\n    self.num_tool_output_tokens = 0\n\n    # Turn tracking - replaces multiple individual tracking variables\n    self.current_turn_metrics = TurnMetrics()\n    # Track metrics for all turns\n    self.all_turn_metrics: list[TurnMetrics] = []\n    self.is_first_turn = True\n    self.first_tok_of_message = True  # For streaming support",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    messages: list,\n    available_tools: list[str],\n):\n    self._messages = messages\n    self.finish_reason: str | None = None\n    self.available_tools = available_tools\n    self._tool_sessions: dict[str, ClientSession | Tool] = {}\n    self.called_tools: set[str] = set()\n\n    self.parser = get_streamable_parser_for_assistant()\n    self.num_init_messages = len(messages)\n    self.num_prompt_tokens = 0\n    self.num_output_tokens = 0\n    self.num_cached_tokens = 0\n    self.num_reasoning_tokens = 0\n    self.num_tool_output_tokens = 0\n\n    # Turn tracking - replaces multiple individual tracking variables\n    self.current_turn_metrics = TurnMetrics()\n    # Track metrics for all turns\n    self.all_turn_metrics: list[TurnMetrics] = []\n    self.is_first_turn = True\n    self.first_tok_of_message = True  # For streaming support",
      "language": "python"
    },
    {
      "code": "_update_decode_token_usage(output: RequestOutput) -> int",
      "language": "php"
    },
    {
      "code": "_update_decode_token_usage(output: RequestOutput) -> int",
      "language": "php"
    },
    {
      "code": "623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647",
      "language": "unknown"
    },
    {
      "code": "def _update_decode_token_usage(self, output: RequestOutput) -> int:\n    \"\"\"Update token usage statistics for the decode phase of generation.\n\n    The decode phase processes the generated output tokens. This method:\n    1. Counts output tokens from all completion outputs\n    2. Updates the total output token count\n    3. Tracks tokens generated in the current turn\n\n    In streaming mode, this is called for each token generated.\n    In non-streaming mode, this is called once with all output tokens.\n\n    Args:\n        output: The RequestOutput containing generated token information\n\n    Returns:\n        int: Number of output tokens processed in this call\n    \"\"\"\n    updated_output_token_count = 0\n    if output.outputs:\n        for completion_output in output.outputs:\n            # only keep last round\n            updated_output_token_count += len(completion_output.token_ids)\n        self.num_output_tokens += updated_output_token_count\n        self.current_turn_metrics.output_tokens += updated_output_token_count\n    return updated_output_token_count",
      "language": "python"
    },
    {
      "code": "def _update_decode_token_usage(self, output: RequestOutput) -> int:\n    \"\"\"Update token usage statistics for the decode phase of generation.\n\n    The decode phase processes the generated output tokens. This method:\n    1. Counts output tokens from all completion outputs\n    2. Updates the total output token count\n    3. Tracks tokens generated in the current turn\n\n    In streaming mode, this is called for each token generated.\n    In non-streaming mode, this is called once with all output tokens.\n\n    Args:\n        output: The RequestOutput containing generated token information\n\n    Returns:\n        int: Number of output tokens processed in this call\n    \"\"\"\n    updated_output_token_count = 0\n    if output.outputs:\n        for completion_output in output.outputs:\n            # only keep last round\n            updated_output_token_count += len(completion_output.token_ids)\n        self.num_output_tokens += updated_output_token_count\n        self.current_turn_metrics.output_tokens += updated_output_token_count\n    return updated_output_token_count",
      "language": "python"
    },
    {
      "code": "_update_num_reasoning_tokens()",
      "language": "unknown"
    },
    {
      "code": "_update_num_reasoning_tokens()",
      "language": "unknown"
    },
    {
      "code": "530\n531\n532\n533",
      "language": "unknown"
    },
    {
      "code": "def _update_num_reasoning_tokens(self):\n    # Count all analysis and commentary channels as reasoning tokens\n    if self.parser.current_channel in {\"analysis\", \"commentary\"}:\n        self.num_reasoning_tokens += 1",
      "language": "python"
    },
    {
      "code": "def _update_num_reasoning_tokens(self):\n    # Count all analysis and commentary channels as reasoning tokens\n    if self.parser.current_channel in {\"analysis\", \"commentary\"}:\n        self.num_reasoning_tokens += 1",
      "language": "python"
    },
    {
      "code": "_update_prefill_token_usage(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "_update_prefill_token_usage(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621",
      "language": "unknown"
    },
    {
      "code": "def _update_prefill_token_usage(self, output: RequestOutput) -> None:\n    \"\"\"Update token usage statistics for the prefill phase of generation.\n\n    The prefill phase processes the input prompt tokens. This method:\n    1. Counts the prompt tokens for this turn\n    2. Calculates tool output tokens for multi-turn conversations\n    3. Updates cached token counts\n    4. Tracks state for next turn calculations\n\n    Tool output tokens are calculated as:\n    current_prompt_tokens - last_turn_prompt_tokens -\n    last_turn_output_tokens\n    This represents tokens added between turns (typically tool responses).\n\n    Args:\n        output: The RequestOutput containing prompt token information\n    \"\"\"\n    if output.prompt_token_ids is not None:\n        this_turn_input_tokens = len(output.prompt_token_ids)\n    else:\n        this_turn_input_tokens = 0\n        logger.error(\"RequestOutput appended contains no prompt_token_ids.\")\n\n    # Update current turn input tokens\n    self.current_turn_metrics.input_tokens = this_turn_input_tokens\n    self.num_prompt_tokens += this_turn_input_tokens\n\n    # Calculate tool tokens (except on first turn)\n    if self.is_first_turn:\n        self.is_first_turn = False\n    else:\n        previous_turn = self.all_turn_metrics[-1]\n        # start counting tool after first turn\n        # tool tokens = this turn prefill - last turn prefill -\n        # last turn decode\n        this_turn_tool_tokens = (\n            self.current_turn_metrics.input_tokens\n            - previous_turn.input_tokens\n            - previous_turn.output_tokens\n        )\n\n        # Handle negative tool token counts (shouldn't happen in normal\n        # cases)\n        if this_turn_tool_tokens < 0:\n            logger.error(\n                \"Negative tool output tokens calculated: %d \"\n                \"(current_input=%d, previous_input=%d, \"\n                \"previous_output=%d). Setting to 0.\",\n                this_turn_tool_tokens,\n                self.current_turn_metrics.input_tokens,\n                previous_turn.input_tokens,\n                previous_turn.output_tokens,\n            )\n            this_turn_tool_tokens = 0\n\n        self.num_tool_output_tokens += this_turn_tool_tokens\n        self.current_turn_metrics.tool_output_tokens = this_turn_tool_tokens\n\n    # Update cached tokens\n    num_cached_token = output.num_cached_tokens\n    if num_cached_token is not None:\n        self.num_cached_tokens += num_cached_token\n        self.current_turn_metrics.cached_input_tokens = num_cached_token",
      "language": "python"
    },
    {
      "code": "def _update_prefill_token_usage(self, output: RequestOutput) -> None:\n    \"\"\"Update token usage statistics for the prefill phase of generation.\n\n    The prefill phase processes the input prompt tokens. This method:\n    1. Counts the prompt tokens for this turn\n    2. Calculates tool output tokens for multi-turn conversations\n    3. Updates cached token counts\n    4. Tracks state for next turn calculations\n\n    Tool output tokens are calculated as:\n    current_prompt_tokens - last_turn_prompt_tokens -\n    last_turn_output_tokens\n    This represents tokens added between turns (typically tool responses).\n\n    Args:\n        output: The RequestOutput containing prompt token information\n    \"\"\"\n    if output.prompt_token_ids is not None:\n        this_turn_input_tokens = len(output.prompt_token_ids)\n    else:\n        this_turn_input_tokens = 0\n        logger.error(\"RequestOutput appended contains no prompt_token_ids.\")\n\n    # Update current turn input tokens\n    self.current_turn_metrics.input_tokens = this_turn_input_tokens\n    self.num_prompt_tokens += this_turn_input_tokens\n\n    # Calculate tool tokens (except on first turn)\n    if self.is_first_turn:\n        self.is_first_turn = False\n    else:\n        previous_turn = self.all_turn_metrics[-1]\n        # start counting tool after first turn\n        # tool tokens = this turn prefill - last turn prefill -\n        # last turn decode\n        this_turn_tool_tokens = (\n            self.current_turn_metrics.input_tokens\n            - previous_turn.input_tokens\n            - previous_turn.output_tokens\n        )\n\n        # Handle negative tool token counts (shouldn't happen in normal\n        # cases)\n        if this_turn_tool_tokens < 0:\n            logger.error(\n                \"Negative tool output tokens calculated: %d \"\n                \"(current_input=%d, previous_input=%d, \"\n                \"previous_output=%d). Setting to 0.\",\n                this_turn_tool_tokens,\n                self.current_turn_metrics.input_tokens,\n                previous_turn.input_tokens,\n                previous_turn.output_tokens,\n            )\n            this_turn_tool_tokens = 0\n\n        self.num_tool_output_tokens += this_turn_tool_tokens\n        self.current_turn_metrics.tool_output_tokens = this_turn_tool_tokens\n\n    # Update cached tokens\n    num_cached_token = output.num_cached_tokens\n    if num_cached_token is not None:\n        self.num_cached_tokens += num_cached_token\n        self.current_turn_metrics.cached_input_tokens = num_cached_token",
      "language": "python"
    },
    {
      "code": "append_output(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "append_output(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553",
      "language": "unknown"
    },
    {
      "code": "def append_output(self, output: RequestOutput) -> None:\n    output_token_ids = output.outputs[0].token_ids\n    self.parser = get_streamable_parser_for_assistant()\n    for token_id in output_token_ids:\n        self.parser.process(token_id)\n        # Check if the current token is part of reasoning content\n        self._update_num_reasoning_tokens()\n    self._update_prefill_token_usage(output)\n    self._update_decode_token_usage(output)\n    # Append current turn to all turn list for next turn's calculations\n    self.all_turn_metrics.append(self.current_turn_metrics.copy())\n    self.current_turn_metrics.reset()\n    # append_output is called only once before tool calling\n    # in non-streaming case\n    # so we can append all the parser messages to _messages\n    output_msgs = self.parser.messages\n    # The responses finish reason is set in the last message\n    self.finish_reason = output.outputs[0].finish_reason\n    self._messages.extend(output_msgs)",
      "language": "python"
    },
    {
      "code": "def append_output(self, output: RequestOutput) -> None:\n    output_token_ids = output.outputs[0].token_ids\n    self.parser = get_streamable_parser_for_assistant()\n    for token_id in output_token_ids:\n        self.parser.process(token_id)\n        # Check if the current token is part of reasoning content\n        self._update_num_reasoning_tokens()\n    self._update_prefill_token_usage(output)\n    self._update_decode_token_usage(output)\n    # Append current turn to all turn list for next turn's calculations\n    self.all_turn_metrics.append(self.current_turn_metrics.copy())\n    self.current_turn_metrics.reset()\n    # append_output is called only once before tool calling\n    # in non-streaming case\n    # so we can append all the parser messages to _messages\n    output_msgs = self.parser.messages\n    # The responses finish reason is set in the last message\n    self.finish_reason = output.outputs[0].finish_reason\n    self._messages.extend(output_msgs)",
      "language": "python"
    },
    {
      "code": "append_tool_output(output: list[Message]) -> None",
      "language": "rust"
    },
    {
      "code": "append_tool_output(output: list[Message]) -> None",
      "language": "rust"
    },
    {
      "code": "555\n556\n557",
      "language": "unknown"
    },
    {
      "code": "def append_tool_output(self, output: list[Message]) -> None:\n    output_msgs = output\n    self._messages.extend(output_msgs)",
      "language": "python"
    },
    {
      "code": "def append_tool_output(self, output: list[Message]) -> None:\n    output_msgs = output\n    self._messages.extend(output_msgs)",
      "language": "python"
    },
    {
      "code": "call_container_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: Message,\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "call_container_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: Message,\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797",
      "language": "unknown"
    },
    {
      "code": "async def call_container_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n) -> list[Message]:\n    \"\"\"\n    Call container tool. Expect this to be run in a stateful docker\n    with command line terminal.\n    The official container tool would at least\n    expect the following format:\n    - for tool name: exec\n        - args:\n            {\n                \"cmd\":List[str] \"command to execute\",\n                \"workdir\":optional[str] \"current working directory\",\n                \"env\":optional[object/dict] \"environment variables\",\n                \"session_name\":optional[str] \"session name\",\n                \"timeout\":optional[int] \"timeout in seconds\",\n                \"user\":optional[str] \"user name\",\n            }\n    \"\"\"\n    self.called_tools.add(\"container\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result(self)\n    tool_name = last_msg.recipient.split(\".\")[1].split(\" \")[0]\n    if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n        try:\n            args = json.loads(last_msg.content[0].text)\n        except json.JSONDecodeError as e:\n            return _create_json_parse_error_messages(last_msg, e)\n    else:\n        args = json.loads(last_msg.content[0].text)\n    result = await tool_session.call_tool(tool_name, args)\n    result_str = result.content[0].text\n    content = TextContent(text=result_str)\n    author = Author(role=Role.TOOL, name=last_msg.recipient)\n    return [\n        Message(\n            author=author,\n            content=[content],\n            recipient=Role.ASSISTANT,\n            channel=last_msg.channel,\n        )\n    ]",
      "language": "python"
    },
    {
      "code": "async def call_container_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n) -> list[Message]:\n    \"\"\"\n    Call container tool. Expect this to be run in a stateful docker\n    with command line terminal.\n    The official container tool would at least\n    expect the following format:\n    - for tool name: exec\n        - args:\n            {\n                \"cmd\":List[str] \"command to execute\",\n                \"workdir\":optional[str] \"current working directory\",\n                \"env\":optional[object/dict] \"environment variables\",\n                \"session_name\":optional[str] \"session name\",\n                \"timeout\":optional[int] \"timeout in seconds\",\n                \"user\":optional[str] \"user name\",\n            }\n    \"\"\"\n    self.called_tools.add(\"container\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result(self)\n    tool_name = last_msg.recipient.split(\".\")[1].split(\" \")[0]\n    if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n        try:\n            args = json.loads(last_msg.content[0].text)\n        except json.JSONDecodeError as e:\n            return _create_json_parse_error_messages(last_msg, e)\n    else:\n        args = json.loads(last_msg.content[0].text)\n    result = await tool_session.call_tool(tool_name, args)\n    result_str = result.content[0].text\n    content = TextContent(text=result_str)\n    author = Author(role=Role.TOOL, name=last_msg.recipient)\n    return [\n        Message(\n            author=author,\n            content=[content],\n            recipient=Role.ASSISTANT,\n            channel=last_msg.channel,\n        )\n    ]",
      "language": "python"
    },
    {
      "code": "call_python_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: Message,\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "call_python_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: Message,\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734",
      "language": "unknown"
    },
    {
      "code": "async def call_python_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n) -> list[Message]:\n    self.called_tools.add(\"python\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result(self)\n    param = {\n        \"code\": last_msg.content[0].text,\n    }\n    result = await tool_session.call_tool(\"python\", param)\n    result_str = result.content[0].text\n\n    content = TextContent(text=result_str)\n    author = Author(role=Role.TOOL, name=\"python\")\n\n    return [\n        Message(\n            author=author,\n            content=[content],\n            channel=last_msg.channel,\n            recipient=Role.ASSISTANT,\n        )\n    ]",
      "language": "python"
    },
    {
      "code": "async def call_python_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n) -> list[Message]:\n    self.called_tools.add(\"python\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result(self)\n    param = {\n        \"code\": last_msg.content[0].text,\n    }\n    result = await tool_session.call_tool(\"python\", param)\n    result_str = result.content[0].text\n\n    content = TextContent(text=result_str)\n    author = Author(role=Role.TOOL, name=\"python\")\n\n    return [\n        Message(\n            author=author,\n            content=[content],\n            channel=last_msg.channel,\n            recipient=Role.ASSISTANT,\n        )\n    ]",
      "language": "python"
    },
    {
      "code": "call_search_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: Message,\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "call_search_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: Message,\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710",
      "language": "unknown"
    },
    {
      "code": "async def call_search_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n) -> list[Message]:\n    self.called_tools.add(\"browser\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result(self)\n    tool_name = last_msg.recipient.split(\".\")[1]\n    if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n        try:\n            args = json.loads(last_msg.content[0].text)\n        except json.JSONDecodeError as e:\n            return _create_json_parse_error_messages(last_msg, e)\n    else:\n        args = json.loads(last_msg.content[0].text)\n    result = await tool_session.call_tool(tool_name, args)\n    result_str = result.content[0].text\n    content = TextContent(text=result_str)\n    author = Author(role=Role.TOOL, name=last_msg.recipient)\n    return [\n        Message(\n            author=author,\n            content=[content],\n            recipient=Role.ASSISTANT,\n            channel=last_msg.channel,\n        )\n    ]",
      "language": "python"
    },
    {
      "code": "async def call_search_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n) -> list[Message]:\n    self.called_tools.add(\"browser\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result(self)\n    tool_name = last_msg.recipient.split(\".\")[1]\n    if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n        try:\n            args = json.loads(last_msg.content[0].text)\n        except json.JSONDecodeError as e:\n            return _create_json_parse_error_messages(last_msg, e)\n    else:\n        args = json.loads(last_msg.content[0].text)\n    result = await tool_session.call_tool(tool_name, args)\n    result_str = result.content[0].text\n    content = TextContent(text=result_str)\n    author = Author(role=Role.TOOL, name=last_msg.recipient)\n    return [\n        Message(\n            author=author,\n            content=[content],\n            recipient=Role.ASSISTANT,\n            channel=last_msg.channel,\n        )\n    ]",
      "language": "python"
    },
    {
      "code": "call_tool() -> list[Message]",
      "language": "php"
    },
    {
      "code": "call_tool() -> list[Message]",
      "language": "php"
    },
    {
      "code": "662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680",
      "language": "unknown"
    },
    {
      "code": "async def call_tool(self) -> list[Message]:\n    if not self.messages:\n        return []\n    last_msg = self.messages[-1]\n    recipient = last_msg.recipient\n    if recipient is not None:\n        if recipient.startswith(\"browser.\"):\n            return await self.call_search_tool(\n                self._tool_sessions[\"browser\"], last_msg\n            )\n        elif recipient.startswith(\"python\"):\n            return await self.call_python_tool(\n                self._tool_sessions[\"python\"], last_msg\n            )\n        elif recipient.startswith(\"container.\"):\n            return await self.call_container_tool(\n                self._tool_sessions[\"container\"], last_msg\n            )\n    raise ValueError(\"No tool call found\")",
      "language": "python"
    },
    {
      "code": "async def call_tool(self) -> list[Message]:\n    if not self.messages:\n        return []\n    last_msg = self.messages[-1]\n    recipient = last_msg.recipient\n    if recipient is not None:\n        if recipient.startswith(\"browser.\"):\n            return await self.call_search_tool(\n                self._tool_sessions[\"browser\"], last_msg\n            )\n        elif recipient.startswith(\"python\"):\n            return await self.call_python_tool(\n                self._tool_sessions[\"python\"], last_msg\n            )\n        elif recipient.startswith(\"container.\"):\n            return await self.call_container_tool(\n                self._tool_sessions[\"container\"], last_msg\n            )\n    raise ValueError(\"No tool call found\")",
      "language": "python"
    },
    {
      "code": "cleanup_session(*args, **kwargs) -> None",
      "language": "rust"
    },
    {
      "code": "cleanup_session(*args, **kwargs) -> None",
      "language": "rust"
    },
    {
      "code": "799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815",
      "language": "unknown"
    },
    {
      "code": "async def cleanup_session(self, *args, **kwargs) -> None:\n    \"\"\"Can be used as coro to used in __aexit__\"\"\"\n\n    async def cleanup_tool_session(tool_session):\n        if not isinstance(tool_session, Tool):\n            logger.info(\n                \"Cleaning up tool session for %s\", tool_session._client_info\n            )\n            with contextlib.suppress(Exception):\n                await tool_session.call_tool(\"cleanup_session\", {})\n\n    await asyncio.gather(\n        *(\n            cleanup_tool_session(self._tool_sessions[tool])\n            for tool in self.called_tools\n        )\n    )",
      "language": "python"
    },
    {
      "code": "async def cleanup_session(self, *args, **kwargs) -> None:\n    \"\"\"Can be used as coro to used in __aexit__\"\"\"\n\n    async def cleanup_tool_session(tool_session):\n        if not isinstance(tool_session, Tool):\n            logger.info(\n                \"Cleaning up tool session for %s\", tool_session._client_info\n            )\n            with contextlib.suppress(Exception):\n                await tool_session.call_tool(\"cleanup_session\", {})\n\n    await asyncio.gather(\n        *(\n            cleanup_tool_session(self._tool_sessions[tool])\n            for tool in self.called_tools\n        )\n    )",
      "language": "python"
    },
    {
      "code": "init_tool_sessions(\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n)",
      "language": "rust"
    },
    {
      "code": "init_tool_sessions(\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n)",
      "language": "rust"
    },
    {
      "code": "736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754",
      "language": "unknown"
    },
    {
      "code": "async def init_tool_sessions(\n    self,\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n):\n    if tool_server:\n        for tool_name in self.available_tools:\n            if tool_name not in self._tool_sessions:\n                tool_type = _map_tool_name_to_tool_type(tool_name)\n                headers = (\n                    mcp_tools[tool_type].headers if tool_type in mcp_tools else None\n                )\n                tool_session = await exit_stack.enter_async_context(\n                    tool_server.new_session(tool_name, request_id, headers)\n                )\n                self._tool_sessions[tool_name] = tool_session\n                exit_stack.push_async_exit(self.cleanup_session)",
      "language": "python"
    },
    {
      "code": "async def init_tool_sessions(\n    self,\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n):\n    if tool_server:\n        for tool_name in self.available_tools:\n            if tool_name not in self._tool_sessions:\n                tool_type = _map_tool_name_to_tool_type(tool_name)\n                headers = (\n                    mcp_tools[tool_type].headers if tool_type in mcp_tools else None\n                )\n                tool_session = await exit_stack.enter_async_context(\n                    tool_server.new_session(tool_name, request_id, headers)\n                )\n                self._tool_sessions[tool_name] = tool_session\n                exit_stack.push_async_exit(self.cleanup_session)",
      "language": "python"
    },
    {
      "code": "need_builtin_tool_call() -> bool",
      "language": "php"
    },
    {
      "code": "need_builtin_tool_call() -> bool",
      "language": "php"
    },
    {
      "code": "653\n654\n655\n656\n657\n658\n659\n660",
      "language": "unknown"
    },
    {
      "code": "def need_builtin_tool_call(self) -> bool:\n    last_msg = self.messages[-1]\n    recipient = last_msg.recipient\n    return recipient is not None and (\n        recipient.startswith(\"browser.\")\n        or recipient.startswith(\"python\")\n        or recipient.startswith(\"container.\")\n    )",
      "language": "python"
    },
    {
      "code": "def need_builtin_tool_call(self) -> bool:\n    last_msg = self.messages[-1]\n    recipient = last_msg.recipient\n    return recipient is not None and (\n        recipient.startswith(\"browser.\")\n        or recipient.startswith(\"python\")\n        or recipient.startswith(\"container.\")\n    )",
      "language": "python"
    },
    {
      "code": "render_for_completion() -> list[int]",
      "language": "php"
    },
    {
      "code": "render_for_completion() -> list[int]",
      "language": "php"
    },
    {
      "code": "def render_for_completion(self) -> list[int]:\n    return render_for_completion(self.messages)",
      "language": "python"
    },
    {
      "code": "def render_for_completion(self) -> list[int]:\n    return render_for_completion(self.messages)",
      "language": "python"
    },
    {
      "code": "256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500",
      "language": "unknown"
    },
    {
      "code": "class ParsableContext(ConversationContext):\n    def __init__(\n        self,\n        *,\n        response_messages: list[ResponseInputOutputItem],\n        tokenizer: TokenizerLike,\n        reasoning_parser_cls: Callable[[TokenizerLike], ReasoningParser] | None,\n        request: ResponsesRequest,\n        available_tools: list[str] | None,\n        tool_parser_cls: Callable[[TokenizerLike], ToolParser] | None,\n        chat_template: str | None,\n        chat_template_content_format: ChatTemplateContentFormatOption,\n    ):\n        self.num_prompt_tokens = 0\n        self.num_output_tokens = 0\n        self.num_cached_tokens = 0\n        # TODO: num_reasoning_tokens is not implemented yet.\n        self.num_reasoning_tokens = 0\n        # not implemented yet for ParsableContext\n        self.all_turn_metrics: list[TurnMetrics] = []\n\n        if reasoning_parser_cls is None:\n            raise ValueError(\"reasoning_parser_cls must be provided.\")\n\n        self.parser = get_responses_parser_for_simple_context(\n            tokenizer=tokenizer,\n            reasoning_parser_cls=reasoning_parser_cls,\n            response_messages=response_messages,\n            request=request,\n            tool_parser_cls=tool_parser_cls,\n        )\n        self.tool_parser_cls = tool_parser_cls\n        self.request = request\n        self.tokenizer = tokenizer\n\n        self.available_tools = available_tools or []\n        self._tool_sessions: dict[str, ClientSession | Tool] = {}\n        self.called_tools: set[str] = set()\n\n        self.tool_dicts = construct_tool_dicts(request.tools, request.tool_choice)\n        self.chat_template = chat_template\n        self.chat_template_content_format = chat_template_content_format\n\n        self.input_messages: list[ResponseRawMessageAndToken] = []\n        self.output_messages: list[ResponseRawMessageAndToken] = []\n\n    def append_output(self, output: RequestOutput) -> None:\n        self.num_prompt_tokens = len(output.prompt_token_ids or [])\n        self.num_cached_tokens = output.num_cached_tokens or 0\n        self.num_output_tokens += len(output.outputs[0].token_ids or [])\n        self.parser.process(output.outputs[0])\n\n        # only store if enable_response_messages is True, save memory\n        if self.request.enable_response_messages:\n            output_prompt = output.prompt or \"\"\n            output_prompt_token_ids = output.prompt_token_ids or []\n            if len(self.input_messages) == 0:\n                self.input_messages.append(\n                    ResponseRawMessageAndToken(\n                        message=output_prompt,\n                        tokens=output_prompt_token_ids,\n                    )\n                )\n            else:\n                self.output_messages.append(\n                    ResponseRawMessageAndToken(\n                        message=output_prompt,\n                        tokens=output_prompt_token_ids,\n                    )\n                )\n            self.output_messages.append(\n                ResponseRawMessageAndToken(\n                    message=output.outputs[0].text,\n                    tokens=output.outputs[0].token_ids,\n                )\n            )\n\n    def append_tool_output(self, output: list[ResponseInputOutputItem]) -> None:\n        self.parser.response_messages.extend(output)\n\n    def need_builtin_tool_call(self) -> bool:\n        \"\"\"Return true if the last message is a MCP tool call\"\"\"\n        last_message = self.parser.response_messages[-1]\n        # TODO(qandrew): figure out which tools are MCP tools\n        if last_message.type == \"function_call\":  # noqa: SIM102\n            if last_message.name in (\n                \"code_interpreter\",\n                \"python\",\n                \"web_search_preview\",\n            ) or last_message.name.startswith(\"container\"):\n                return True\n\n        return False\n\n    async def call_python_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: FunctionCall\n    ) -> list[ResponseInputOutputItem]:\n        self.called_tools.add(\"python\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result_parsable_context(self)\n        args = json.loads(last_msg.arguments)\n        param = {\n            \"code\": args[\"code\"],\n        }\n        result = await tool_session.call_tool(\"python\", param)\n        result_str = result.content[0].text\n\n        message = ResponseFunctionToolCallOutputItem(\n            id=f\"mcpo_{random_uuid()}\",\n            type=\"function_call_output\",\n            call_id=f\"call_{random_uuid()}\",\n            output=result_str,\n            status=\"completed\",\n        )\n\n        return [message]\n\n    async def call_search_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: FunctionCall\n    ) -> list[ResponseInputOutputItem]:\n        self.called_tools.add(\"browser\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result_parsable_context(self)\n        if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n            try:\n                args = json.loads(last_msg.arguments)\n            except json.JSONDecodeError as e:\n                return _create_json_parse_error_messages(last_msg, e)\n        else:\n            args = json.loads(last_msg.arguments)\n        result = await tool_session.call_tool(\"search\", args)\n        result_str = result.content[0].text\n\n        message = ResponseFunctionToolCallOutputItem(\n            id=f\"fco_{random_uuid()}\",\n            type=\"function_call_output\",\n            call_id=f\"call_{random_uuid()}\",\n            output=result_str,\n            status=\"completed\",\n        )\n\n        return [message]\n\n    async def call_container_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n    ) -> list[Message]:\n        \"\"\"\n        Call container tool. Expect this to be run in a stateful docker\n        with command line terminal.\n        The official container tool would at least\n        expect the following format:\n        - for tool name: exec\n            - args:\n                {\n                    \"cmd\":List[str] \"command to execute\",\n                    \"workdir\":optional[str] \"current working directory\",\n                    \"env\":optional[object/dict] \"environment variables\",\n                    \"session_name\":optional[str] \"session name\",\n                    \"timeout\":optional[int] \"timeout in seconds\",\n                    \"user\":optional[str] \"user name\",\n                }\n        \"\"\"\n        self.called_tools.add(\"container\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result_parsable_context(self)\n        # tool_name = last_msg.recipient.split(\".\")[1].split(\" \")[0]\n        if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n            try:\n                args = json.loads(last_msg.arguments)\n            except json.JSONDecodeError as e:\n                return _create_json_parse_error_messages(last_msg, e)\n        else:\n            args = json.loads(last_msg.arguments)\n        result = await tool_session.call_tool(\"exec\", args)\n        result_str = result.content[0].text\n\n        message = ResponseFunctionToolCallOutputItem(\n            id=f\"fco_{random_uuid()}\",\n            type=\"function_call_output\",\n            call_id=f\"call_{random_uuid()}\",\n            output=result_str,\n            status=\"completed\",\n        )\n\n        return [message]\n\n    async def call_tool(self) -> list[ResponseInputOutputItem]:\n        if not self.parser.response_messages:\n            return []\n        last_msg = self.parser.response_messages[-1]\n        # change this to a mcp_ function call\n        last_msg.id = f\"{MCP_PREFIX}{random_uuid()}\"\n        self.parser.response_messages[-1] = last_msg\n        if last_msg.name == \"code_interpreter\":\n            return await self.call_python_tool(self._tool_sessions[\"python\"], last_msg)\n        elif last_msg.name == \"web_search_preview\":\n            return await self.call_search_tool(self._tool_sessions[\"browser\"], last_msg)\n        elif last_msg.name.startswith(\"container\"):\n            return await self.call_container_tool(\n                self._tool_sessions[\"container\"], last_msg\n            )\n        return []\n\n    def render_for_completion(self):\n        raise NotImplementedError(\"Should not be called.\")\n\n    async def init_tool_sessions(\n        self,\n        tool_server: ToolServer | None,\n        exit_stack: AsyncExitStack,\n        request_id: str,\n        mcp_tools: dict[str, Mcp],\n    ):\n        if tool_server:\n            for tool_name in self.available_tools:\n                if tool_name in self._tool_sessions:\n                    continue\n\n                tool_type = _map_tool_name_to_tool_type(tool_name)\n                headers = (\n                    mcp_tools[tool_type].headers if tool_type in mcp_tools else None\n                )\n                tool_session = await exit_stack.enter_async_context(\n                    tool_server.new_session(tool_name, request_id, headers)\n                )\n                self._tool_sessions[tool_name] = tool_session\n                exit_stack.push_async_exit(self.cleanup_session)\n\n    async def cleanup_session(self, *args, **kwargs) -> None:\n        \"\"\"Can be used as coro to used in __aexit__\"\"\"\n\n        async def cleanup_tool_session(tool_session):\n            if not isinstance(tool_session, Tool):\n                logger.info(\n                    \"Cleaning up tool session for %s\", tool_session._client_info\n                )\n                with contextlib.suppress(Exception):\n                    await tool_session.call_tool(\"cleanup_session\", {})\n\n        await asyncio.gather(\n            *(\n                cleanup_tool_session(self._tool_sessions[tool])\n                for tool in self.called_tools\n            )\n        )",
      "language": "python"
    },
    {
      "code": "class ParsableContext(ConversationContext):\n    def __init__(\n        self,\n        *,\n        response_messages: list[ResponseInputOutputItem],\n        tokenizer: TokenizerLike,\n        reasoning_parser_cls: Callable[[TokenizerLike], ReasoningParser] | None,\n        request: ResponsesRequest,\n        available_tools: list[str] | None,\n        tool_parser_cls: Callable[[TokenizerLike], ToolParser] | None,\n        chat_template: str | None,\n        chat_template_content_format: ChatTemplateContentFormatOption,\n    ):\n        self.num_prompt_tokens = 0\n        self.num_output_tokens = 0\n        self.num_cached_tokens = 0\n        # TODO: num_reasoning_tokens is not implemented yet.\n        self.num_reasoning_tokens = 0\n        # not implemented yet for ParsableContext\n        self.all_turn_metrics: list[TurnMetrics] = []\n\n        if reasoning_parser_cls is None:\n            raise ValueError(\"reasoning_parser_cls must be provided.\")\n\n        self.parser = get_responses_parser_for_simple_context(\n            tokenizer=tokenizer,\n            reasoning_parser_cls=reasoning_parser_cls,\n            response_messages=response_messages,\n            request=request,\n            tool_parser_cls=tool_parser_cls,\n        )\n        self.tool_parser_cls = tool_parser_cls\n        self.request = request\n        self.tokenizer = tokenizer\n\n        self.available_tools = available_tools or []\n        self._tool_sessions: dict[str, ClientSession | Tool] = {}\n        self.called_tools: set[str] = set()\n\n        self.tool_dicts = construct_tool_dicts(request.tools, request.tool_choice)\n        self.chat_template = chat_template\n        self.chat_template_content_format = chat_template_content_format\n\n        self.input_messages: list[ResponseRawMessageAndToken] = []\n        self.output_messages: list[ResponseRawMessageAndToken] = []\n\n    def append_output(self, output: RequestOutput) -> None:\n        self.num_prompt_tokens = len(output.prompt_token_ids or [])\n        self.num_cached_tokens = output.num_cached_tokens or 0\n        self.num_output_tokens += len(output.outputs[0].token_ids or [])\n        self.parser.process(output.outputs[0])\n\n        # only store if enable_response_messages is True, save memory\n        if self.request.enable_response_messages:\n            output_prompt = output.prompt or \"\"\n            output_prompt_token_ids = output.prompt_token_ids or []\n            if len(self.input_messages) == 0:\n                self.input_messages.append(\n                    ResponseRawMessageAndToken(\n                        message=output_prompt,\n                        tokens=output_prompt_token_ids,\n                    )\n                )\n            else:\n                self.output_messages.append(\n                    ResponseRawMessageAndToken(\n                        message=output_prompt,\n                        tokens=output_prompt_token_ids,\n                    )\n                )\n            self.output_messages.append(\n                ResponseRawMessageAndToken(\n                    message=output.outputs[0].text,\n                    tokens=output.outputs[0].token_ids,\n                )\n            )\n\n    def append_tool_output(self, output: list[ResponseInputOutputItem]) -> None:\n        self.parser.response_messages.extend(output)\n\n    def need_builtin_tool_call(self) -> bool:\n        \"\"\"Return true if the last message is a MCP tool call\"\"\"\n        last_message = self.parser.response_messages[-1]\n        # TODO(qandrew): figure out which tools are MCP tools\n        if last_message.type == \"function_call\":  # noqa: SIM102\n            if last_message.name in (\n                \"code_interpreter\",\n                \"python\",\n                \"web_search_preview\",\n            ) or last_message.name.startswith(\"container\"):\n                return True\n\n        return False\n\n    async def call_python_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: FunctionCall\n    ) -> list[ResponseInputOutputItem]:\n        self.called_tools.add(\"python\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result_parsable_context(self)\n        args = json.loads(last_msg.arguments)\n        param = {\n            \"code\": args[\"code\"],\n        }\n        result = await tool_session.call_tool(\"python\", param)\n        result_str = result.content[0].text\n\n        message = ResponseFunctionToolCallOutputItem(\n            id=f\"mcpo_{random_uuid()}\",\n            type=\"function_call_output\",\n            call_id=f\"call_{random_uuid()}\",\n            output=result_str,\n            status=\"completed\",\n        )\n\n        return [message]\n\n    async def call_search_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: FunctionCall\n    ) -> list[ResponseInputOutputItem]:\n        self.called_tools.add(\"browser\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result_parsable_context(self)\n        if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n            try:\n                args = json.loads(last_msg.arguments)\n            except json.JSONDecodeError as e:\n                return _create_json_parse_error_messages(last_msg, e)\n        else:\n            args = json.loads(last_msg.arguments)\n        result = await tool_session.call_tool(\"search\", args)\n        result_str = result.content[0].text\n\n        message = ResponseFunctionToolCallOutputItem(\n            id=f\"fco_{random_uuid()}\",\n            type=\"function_call_output\",\n            call_id=f\"call_{random_uuid()}\",\n            output=result_str,\n            status=\"completed\",\n        )\n\n        return [message]\n\n    async def call_container_tool(\n        self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n    ) -> list[Message]:\n        \"\"\"\n        Call container tool. Expect this to be run in a stateful docker\n        with command line terminal.\n        The official container tool would at least\n        expect the following format:\n        - for tool name: exec\n            - args:\n                {\n                    \"cmd\":List[str] \"command to execute\",\n                    \"workdir\":optional[str] \"current working directory\",\n                    \"env\":optional[object/dict] \"environment variables\",\n                    \"session_name\":optional[str] \"session name\",\n                    \"timeout\":optional[int] \"timeout in seconds\",\n                    \"user\":optional[str] \"user name\",\n                }\n        \"\"\"\n        self.called_tools.add(\"container\")\n        if isinstance(tool_session, Tool):\n            return await tool_session.get_result_parsable_context(self)\n        # tool_name = last_msg.recipient.split(\".\")[1].split(\" \")[0]\n        if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n            try:\n                args = json.loads(last_msg.arguments)\n            except json.JSONDecodeError as e:\n                return _create_json_parse_error_messages(last_msg, e)\n        else:\n            args = json.loads(last_msg.arguments)\n        result = await tool_session.call_tool(\"exec\", args)\n        result_str = result.content[0].text\n\n        message = ResponseFunctionToolCallOutputItem(\n            id=f\"fco_{random_uuid()}\",\n            type=\"function_call_output\",\n            call_id=f\"call_{random_uuid()}\",\n            output=result_str,\n            status=\"completed\",\n        )\n\n        return [message]\n\n    async def call_tool(self) -> list[ResponseInputOutputItem]:\n        if not self.parser.response_messages:\n            return []\n        last_msg = self.parser.response_messages[-1]\n        # change this to a mcp_ function call\n        last_msg.id = f\"{MCP_PREFIX}{random_uuid()}\"\n        self.parser.response_messages[-1] = last_msg\n        if last_msg.name == \"code_interpreter\":\n            return await self.call_python_tool(self._tool_sessions[\"python\"], last_msg)\n        elif last_msg.name == \"web_search_preview\":\n            return await self.call_search_tool(self._tool_sessions[\"browser\"], last_msg)\n        elif last_msg.name.startswith(\"container\"):\n            return await self.call_container_tool(\n                self._tool_sessions[\"container\"], last_msg\n            )\n        return []\n\n    def render_for_completion(self):\n        raise NotImplementedError(\"Should not be called.\")\n\n    async def init_tool_sessions(\n        self,\n        tool_server: ToolServer | None,\n        exit_stack: AsyncExitStack,\n        request_id: str,\n        mcp_tools: dict[str, Mcp],\n    ):\n        if tool_server:\n            for tool_name in self.available_tools:\n                if tool_name in self._tool_sessions:\n                    continue\n\n                tool_type = _map_tool_name_to_tool_type(tool_name)\n                headers = (\n                    mcp_tools[tool_type].headers if tool_type in mcp_tools else None\n                )\n                tool_session = await exit_stack.enter_async_context(\n                    tool_server.new_session(tool_name, request_id, headers)\n                )\n                self._tool_sessions[tool_name] = tool_session\n                exit_stack.push_async_exit(self.cleanup_session)\n\n    async def cleanup_session(self, *args, **kwargs) -> None:\n        \"\"\"Can be used as coro to used in __aexit__\"\"\"\n\n        async def cleanup_tool_session(tool_session):\n            if not isinstance(tool_session, Tool):\n                logger.info(\n                    \"Cleaning up tool session for %s\", tool_session._client_info\n                )\n                with contextlib.suppress(Exception):\n                    await tool_session.call_tool(\"cleanup_session\", {})\n\n        await asyncio.gather(\n            *(\n                cleanup_tool_session(self._tool_sessions[tool])\n                for tool in self.called_tools\n            )\n        )",
      "language": "python"
    },
    {
      "code": "_tool_sessions: dict[str, ClientSession | Tool] = {}",
      "language": "yaml"
    },
    {
      "code": "_tool_sessions: dict[str, ClientSession | Tool] = {}",
      "language": "yaml"
    },
    {
      "code": "all_turn_metrics: list[TurnMetrics] = []",
      "language": "yaml"
    },
    {
      "code": "all_turn_metrics: list[TurnMetrics] = []",
      "language": "yaml"
    },
    {
      "code": "available_tools = available_tools or []",
      "language": "unknown"
    },
    {
      "code": "available_tools = available_tools or []",
      "language": "unknown"
    },
    {
      "code": "called_tools: set[str] = set()",
      "language": "yaml"
    },
    {
      "code": "called_tools: set[str] = set()",
      "language": "yaml"
    },
    {
      "code": "chat_template = chat_template",
      "language": "unknown"
    },
    {
      "code": "chat_template = chat_template",
      "language": "unknown"
    },
    {
      "code": "chat_template_content_format = chat_template_content_format",
      "language": "unknown"
    },
    {
      "code": "chat_template_content_format = chat_template_content_format",
      "language": "unknown"
    },
    {
      "code": "input_messages: list[ResponseRawMessageAndToken] = []",
      "language": "yaml"
    },
    {
      "code": "input_messages: list[ResponseRawMessageAndToken] = []",
      "language": "yaml"
    },
    {
      "code": "num_cached_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_cached_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_output_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_output_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_prompt_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_prompt_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_reasoning_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_reasoning_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "output_messages: list[ResponseRawMessageAndToken] = []",
      "language": "yaml"
    },
    {
      "code": "output_messages: list[ResponseRawMessageAndToken] = []",
      "language": "yaml"
    },
    {
      "code": "parser = get_responses_parser_for_simple_context(\n    tokenizer=tokenizer,\n    reasoning_parser_cls=reasoning_parser_cls,\n    response_messages=response_messages,\n    request=request,\n    tool_parser_cls=tool_parser_cls,\n)",
      "language": "unknown"
    },
    {
      "code": "parser = get_responses_parser_for_simple_context(\n    tokenizer=tokenizer,\n    reasoning_parser_cls=reasoning_parser_cls,\n    response_messages=response_messages,\n    request=request,\n    tool_parser_cls=tool_parser_cls,\n)",
      "language": "unknown"
    },
    {
      "code": "request = request",
      "language": "unknown"
    },
    {
      "code": "request = request",
      "language": "unknown"
    },
    {
      "code": "tokenizer = tokenizer",
      "language": "unknown"
    },
    {
      "code": "tokenizer = tokenizer",
      "language": "unknown"
    },
    {
      "code": "tool_dicts = construct_tool_dicts(tools, tool_choice)",
      "language": "unknown"
    },
    {
      "code": "tool_dicts = construct_tool_dicts(tools, tool_choice)",
      "language": "unknown"
    },
    {
      "code": "tool_parser_cls = tool_parser_cls",
      "language": "unknown"
    },
    {
      "code": "tool_parser_cls = tool_parser_cls",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    *,\n    response_messages: list[ResponseInputOutputItem],\n    tokenizer: TokenizerLike,\n    reasoning_parser_cls: Callable[\n        [TokenizerLike], ReasoningParser\n    ]\n    | None,\n    request: ResponsesRequest,\n    available_tools: list[str] | None,\n    tool_parser_cls: Callable[[TokenizerLike], ToolParser]\n    | None,\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n)",
      "language": "python"
    },
    {
      "code": "__init__(\n    *,\n    response_messages: list[ResponseInputOutputItem],\n    tokenizer: TokenizerLike,\n    reasoning_parser_cls: Callable[\n        [TokenizerLike], ReasoningParser\n    ]\n    | None,\n    request: ResponsesRequest,\n    available_tools: list[str] | None,\n    tool_parser_cls: Callable[[TokenizerLike], ToolParser]\n    | None,\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n)",
      "language": "python"
    },
    {
      "code": "257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    *,\n    response_messages: list[ResponseInputOutputItem],\n    tokenizer: TokenizerLike,\n    reasoning_parser_cls: Callable[[TokenizerLike], ReasoningParser] | None,\n    request: ResponsesRequest,\n    available_tools: list[str] | None,\n    tool_parser_cls: Callable[[TokenizerLike], ToolParser] | None,\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n):\n    self.num_prompt_tokens = 0\n    self.num_output_tokens = 0\n    self.num_cached_tokens = 0\n    # TODO: num_reasoning_tokens is not implemented yet.\n    self.num_reasoning_tokens = 0\n    # not implemented yet for ParsableContext\n    self.all_turn_metrics: list[TurnMetrics] = []\n\n    if reasoning_parser_cls is None:\n        raise ValueError(\"reasoning_parser_cls must be provided.\")\n\n    self.parser = get_responses_parser_for_simple_context(\n        tokenizer=tokenizer,\n        reasoning_parser_cls=reasoning_parser_cls,\n        response_messages=response_messages,\n        request=request,\n        tool_parser_cls=tool_parser_cls,\n    )\n    self.tool_parser_cls = tool_parser_cls\n    self.request = request\n    self.tokenizer = tokenizer\n\n    self.available_tools = available_tools or []\n    self._tool_sessions: dict[str, ClientSession | Tool] = {}\n    self.called_tools: set[str] = set()\n\n    self.tool_dicts = construct_tool_dicts(request.tools, request.tool_choice)\n    self.chat_template = chat_template\n    self.chat_template_content_format = chat_template_content_format\n\n    self.input_messages: list[ResponseRawMessageAndToken] = []\n    self.output_messages: list[ResponseRawMessageAndToken] = []",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    *,\n    response_messages: list[ResponseInputOutputItem],\n    tokenizer: TokenizerLike,\n    reasoning_parser_cls: Callable[[TokenizerLike], ReasoningParser] | None,\n    request: ResponsesRequest,\n    available_tools: list[str] | None,\n    tool_parser_cls: Callable[[TokenizerLike], ToolParser] | None,\n    chat_template: str | None,\n    chat_template_content_format: ChatTemplateContentFormatOption,\n):\n    self.num_prompt_tokens = 0\n    self.num_output_tokens = 0\n    self.num_cached_tokens = 0\n    # TODO: num_reasoning_tokens is not implemented yet.\n    self.num_reasoning_tokens = 0\n    # not implemented yet for ParsableContext\n    self.all_turn_metrics: list[TurnMetrics] = []\n\n    if reasoning_parser_cls is None:\n        raise ValueError(\"reasoning_parser_cls must be provided.\")\n\n    self.parser = get_responses_parser_for_simple_context(\n        tokenizer=tokenizer,\n        reasoning_parser_cls=reasoning_parser_cls,\n        response_messages=response_messages,\n        request=request,\n        tool_parser_cls=tool_parser_cls,\n    )\n    self.tool_parser_cls = tool_parser_cls\n    self.request = request\n    self.tokenizer = tokenizer\n\n    self.available_tools = available_tools or []\n    self._tool_sessions: dict[str, ClientSession | Tool] = {}\n    self.called_tools: set[str] = set()\n\n    self.tool_dicts = construct_tool_dicts(request.tools, request.tool_choice)\n    self.chat_template = chat_template\n    self.chat_template_content_format = chat_template_content_format\n\n    self.input_messages: list[ResponseRawMessageAndToken] = []\n    self.output_messages: list[ResponseRawMessageAndToken] = []",
      "language": "python"
    },
    {
      "code": "append_output(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "append_output(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331",
      "language": "unknown"
    },
    {
      "code": "def append_output(self, output: RequestOutput) -> None:\n    self.num_prompt_tokens = len(output.prompt_token_ids or [])\n    self.num_cached_tokens = output.num_cached_tokens or 0\n    self.num_output_tokens += len(output.outputs[0].token_ids or [])\n    self.parser.process(output.outputs[0])\n\n    # only store if enable_response_messages is True, save memory\n    if self.request.enable_response_messages:\n        output_prompt = output.prompt or \"\"\n        output_prompt_token_ids = output.prompt_token_ids or []\n        if len(self.input_messages) == 0:\n            self.input_messages.append(\n                ResponseRawMessageAndToken(\n                    message=output_prompt,\n                    tokens=output_prompt_token_ids,\n                )\n            )\n        else:\n            self.output_messages.append(\n                ResponseRawMessageAndToken(\n                    message=output_prompt,\n                    tokens=output_prompt_token_ids,\n                )\n            )\n        self.output_messages.append(\n            ResponseRawMessageAndToken(\n                message=output.outputs[0].text,\n                tokens=output.outputs[0].token_ids,\n            )\n        )",
      "language": "python"
    },
    {
      "code": "def append_output(self, output: RequestOutput) -> None:\n    self.num_prompt_tokens = len(output.prompt_token_ids or [])\n    self.num_cached_tokens = output.num_cached_tokens or 0\n    self.num_output_tokens += len(output.outputs[0].token_ids or [])\n    self.parser.process(output.outputs[0])\n\n    # only store if enable_response_messages is True, save memory\n    if self.request.enable_response_messages:\n        output_prompt = output.prompt or \"\"\n        output_prompt_token_ids = output.prompt_token_ids or []\n        if len(self.input_messages) == 0:\n            self.input_messages.append(\n                ResponseRawMessageAndToken(\n                    message=output_prompt,\n                    tokens=output_prompt_token_ids,\n                )\n            )\n        else:\n            self.output_messages.append(\n                ResponseRawMessageAndToken(\n                    message=output_prompt,\n                    tokens=output_prompt_token_ids,\n                )\n            )\n        self.output_messages.append(\n            ResponseRawMessageAndToken(\n                message=output.outputs[0].text,\n                tokens=output.outputs[0].token_ids,\n            )\n        )",
      "language": "python"
    },
    {
      "code": "append_tool_output(\n    output: list[ResponseInputOutputItem],\n) -> None",
      "language": "rust"
    },
    {
      "code": "append_tool_output(\n    output: list[ResponseInputOutputItem],\n) -> None",
      "language": "rust"
    },
    {
      "code": "def append_tool_output(self, output: list[ResponseInputOutputItem]) -> None:\n    self.parser.response_messages.extend(output)",
      "language": "python"
    },
    {
      "code": "def append_tool_output(self, output: list[ResponseInputOutputItem]) -> None:\n    self.parser.response_messages.extend(output)",
      "language": "python"
    },
    {
      "code": "call_container_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: Message,\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "call_container_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: Message,\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440",
      "language": "unknown"
    },
    {
      "code": "async def call_container_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n) -> list[Message]:\n    \"\"\"\n    Call container tool. Expect this to be run in a stateful docker\n    with command line terminal.\n    The official container tool would at least\n    expect the following format:\n    - for tool name: exec\n        - args:\n            {\n                \"cmd\":List[str] \"command to execute\",\n                \"workdir\":optional[str] \"current working directory\",\n                \"env\":optional[object/dict] \"environment variables\",\n                \"session_name\":optional[str] \"session name\",\n                \"timeout\":optional[int] \"timeout in seconds\",\n                \"user\":optional[str] \"user name\",\n            }\n    \"\"\"\n    self.called_tools.add(\"container\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result_parsable_context(self)\n    # tool_name = last_msg.recipient.split(\".\")[1].split(\" \")[0]\n    if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n        try:\n            args = json.loads(last_msg.arguments)\n        except json.JSONDecodeError as e:\n            return _create_json_parse_error_messages(last_msg, e)\n    else:\n        args = json.loads(last_msg.arguments)\n    result = await tool_session.call_tool(\"exec\", args)\n    result_str = result.content[0].text\n\n    message = ResponseFunctionToolCallOutputItem(\n        id=f\"fco_{random_uuid()}\",\n        type=\"function_call_output\",\n        call_id=f\"call_{random_uuid()}\",\n        output=result_str,\n        status=\"completed\",\n    )\n\n    return [message]",
      "language": "python"
    },
    {
      "code": "async def call_container_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: Message\n) -> list[Message]:\n    \"\"\"\n    Call container tool. Expect this to be run in a stateful docker\n    with command line terminal.\n    The official container tool would at least\n    expect the following format:\n    - for tool name: exec\n        - args:\n            {\n                \"cmd\":List[str] \"command to execute\",\n                \"workdir\":optional[str] \"current working directory\",\n                \"env\":optional[object/dict] \"environment variables\",\n                \"session_name\":optional[str] \"session name\",\n                \"timeout\":optional[int] \"timeout in seconds\",\n                \"user\":optional[str] \"user name\",\n            }\n    \"\"\"\n    self.called_tools.add(\"container\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result_parsable_context(self)\n    # tool_name = last_msg.recipient.split(\".\")[1].split(\" \")[0]\n    if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n        try:\n            args = json.loads(last_msg.arguments)\n        except json.JSONDecodeError as e:\n            return _create_json_parse_error_messages(last_msg, e)\n    else:\n        args = json.loads(last_msg.arguments)\n    result = await tool_session.call_tool(\"exec\", args)\n    result_str = result.content[0].text\n\n    message = ResponseFunctionToolCallOutputItem(\n        id=f\"fco_{random_uuid()}\",\n        type=\"function_call_output\",\n        call_id=f\"call_{random_uuid()}\",\n        output=result_str,\n        status=\"completed\",\n    )\n\n    return [message]",
      "language": "python"
    },
    {
      "code": "call_python_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: FunctionCall,\n) -> list[ResponseInputOutputItem]",
      "language": "php"
    },
    {
      "code": "call_python_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: FunctionCall,\n) -> list[ResponseInputOutputItem]",
      "language": "php"
    },
    {
      "code": "350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371",
      "language": "unknown"
    },
    {
      "code": "async def call_python_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: FunctionCall\n) -> list[ResponseInputOutputItem]:\n    self.called_tools.add(\"python\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result_parsable_context(self)\n    args = json.loads(last_msg.arguments)\n    param = {\n        \"code\": args[\"code\"],\n    }\n    result = await tool_session.call_tool(\"python\", param)\n    result_str = result.content[0].text\n\n    message = ResponseFunctionToolCallOutputItem(\n        id=f\"mcpo_{random_uuid()}\",\n        type=\"function_call_output\",\n        call_id=f\"call_{random_uuid()}\",\n        output=result_str,\n        status=\"completed\",\n    )\n\n    return [message]",
      "language": "python"
    },
    {
      "code": "async def call_python_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: FunctionCall\n) -> list[ResponseInputOutputItem]:\n    self.called_tools.add(\"python\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result_parsable_context(self)\n    args = json.loads(last_msg.arguments)\n    param = {\n        \"code\": args[\"code\"],\n    }\n    result = await tool_session.call_tool(\"python\", param)\n    result_str = result.content[0].text\n\n    message = ResponseFunctionToolCallOutputItem(\n        id=f\"mcpo_{random_uuid()}\",\n        type=\"function_call_output\",\n        call_id=f\"call_{random_uuid()}\",\n        output=result_str,\n        status=\"completed\",\n    )\n\n    return [message]",
      "language": "python"
    },
    {
      "code": "call_search_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: FunctionCall,\n) -> list[ResponseInputOutputItem]",
      "language": "php"
    },
    {
      "code": "call_search_tool(\n    tool_session: Union[ClientSession, Tool],\n    last_msg: FunctionCall,\n) -> list[ResponseInputOutputItem]",
      "language": "php"
    },
    {
      "code": "373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397",
      "language": "unknown"
    },
    {
      "code": "async def call_search_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: FunctionCall\n) -> list[ResponseInputOutputItem]:\n    self.called_tools.add(\"browser\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result_parsable_context(self)\n    if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n        try:\n            args = json.loads(last_msg.arguments)\n        except json.JSONDecodeError as e:\n            return _create_json_parse_error_messages(last_msg, e)\n    else:\n        args = json.loads(last_msg.arguments)\n    result = await tool_session.call_tool(\"search\", args)\n    result_str = result.content[0].text\n\n    message = ResponseFunctionToolCallOutputItem(\n        id=f\"fco_{random_uuid()}\",\n        type=\"function_call_output\",\n        call_id=f\"call_{random_uuid()}\",\n        output=result_str,\n        status=\"completed\",\n    )\n\n    return [message]",
      "language": "python"
    },
    {
      "code": "async def call_search_tool(\n    self, tool_session: Union[\"ClientSession\", Tool], last_msg: FunctionCall\n) -> list[ResponseInputOutputItem]:\n    self.called_tools.add(\"browser\")\n    if isinstance(tool_session, Tool):\n        return await tool_session.get_result_parsable_context(self)\n    if envs.VLLM_TOOL_JSON_ERROR_AUTOMATIC_RETRY:\n        try:\n            args = json.loads(last_msg.arguments)\n        except json.JSONDecodeError as e:\n            return _create_json_parse_error_messages(last_msg, e)\n    else:\n        args = json.loads(last_msg.arguments)\n    result = await tool_session.call_tool(\"search\", args)\n    result_str = result.content[0].text\n\n    message = ResponseFunctionToolCallOutputItem(\n        id=f\"fco_{random_uuid()}\",\n        type=\"function_call_output\",\n        call_id=f\"call_{random_uuid()}\",\n        output=result_str,\n        status=\"completed\",\n    )\n\n    return [message]",
      "language": "python"
    },
    {
      "code": "call_tool() -> list[ResponseInputOutputItem]",
      "language": "php"
    },
    {
      "code": "call_tool() -> list[ResponseInputOutputItem]",
      "language": "php"
    },
    {
      "code": "442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457",
      "language": "unknown"
    },
    {
      "code": "async def call_tool(self) -> list[ResponseInputOutputItem]:\n    if not self.parser.response_messages:\n        return []\n    last_msg = self.parser.response_messages[-1]\n    # change this to a mcp_ function call\n    last_msg.id = f\"{MCP_PREFIX}{random_uuid()}\"\n    self.parser.response_messages[-1] = last_msg\n    if last_msg.name == \"code_interpreter\":\n        return await self.call_python_tool(self._tool_sessions[\"python\"], last_msg)\n    elif last_msg.name == \"web_search_preview\":\n        return await self.call_search_tool(self._tool_sessions[\"browser\"], last_msg)\n    elif last_msg.name.startswith(\"container\"):\n        return await self.call_container_tool(\n            self._tool_sessions[\"container\"], last_msg\n        )\n    return []",
      "language": "python"
    },
    {
      "code": "async def call_tool(self) -> list[ResponseInputOutputItem]:\n    if not self.parser.response_messages:\n        return []\n    last_msg = self.parser.response_messages[-1]\n    # change this to a mcp_ function call\n    last_msg.id = f\"{MCP_PREFIX}{random_uuid()}\"\n    self.parser.response_messages[-1] = last_msg\n    if last_msg.name == \"code_interpreter\":\n        return await self.call_python_tool(self._tool_sessions[\"python\"], last_msg)\n    elif last_msg.name == \"web_search_preview\":\n        return await self.call_search_tool(self._tool_sessions[\"browser\"], last_msg)\n    elif last_msg.name.startswith(\"container\"):\n        return await self.call_container_tool(\n            self._tool_sessions[\"container\"], last_msg\n        )\n    return []",
      "language": "python"
    },
    {
      "code": "cleanup_session(*args, **kwargs) -> None",
      "language": "rust"
    },
    {
      "code": "cleanup_session(*args, **kwargs) -> None",
      "language": "rust"
    },
    {
      "code": "484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500",
      "language": "unknown"
    },
    {
      "code": "async def cleanup_session(self, *args, **kwargs) -> None:\n    \"\"\"Can be used as coro to used in __aexit__\"\"\"\n\n    async def cleanup_tool_session(tool_session):\n        if not isinstance(tool_session, Tool):\n            logger.info(\n                \"Cleaning up tool session for %s\", tool_session._client_info\n            )\n            with contextlib.suppress(Exception):\n                await tool_session.call_tool(\"cleanup_session\", {})\n\n    await asyncio.gather(\n        *(\n            cleanup_tool_session(self._tool_sessions[tool])\n            for tool in self.called_tools\n        )\n    )",
      "language": "python"
    },
    {
      "code": "async def cleanup_session(self, *args, **kwargs) -> None:\n    \"\"\"Can be used as coro to used in __aexit__\"\"\"\n\n    async def cleanup_tool_session(tool_session):\n        if not isinstance(tool_session, Tool):\n            logger.info(\n                \"Cleaning up tool session for %s\", tool_session._client_info\n            )\n            with contextlib.suppress(Exception):\n                await tool_session.call_tool(\"cleanup_session\", {})\n\n    await asyncio.gather(\n        *(\n            cleanup_tool_session(self._tool_sessions[tool])\n            for tool in self.called_tools\n        )\n    )",
      "language": "python"
    },
    {
      "code": "init_tool_sessions(\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n)",
      "language": "rust"
    },
    {
      "code": "init_tool_sessions(\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n)",
      "language": "rust"
    },
    {
      "code": "462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482",
      "language": "unknown"
    },
    {
      "code": "async def init_tool_sessions(\n    self,\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n):\n    if tool_server:\n        for tool_name in self.available_tools:\n            if tool_name in self._tool_sessions:\n                continue\n\n            tool_type = _map_tool_name_to_tool_type(tool_name)\n            headers = (\n                mcp_tools[tool_type].headers if tool_type in mcp_tools else None\n            )\n            tool_session = await exit_stack.enter_async_context(\n                tool_server.new_session(tool_name, request_id, headers)\n            )\n            self._tool_sessions[tool_name] = tool_session\n            exit_stack.push_async_exit(self.cleanup_session)",
      "language": "python"
    },
    {
      "code": "async def init_tool_sessions(\n    self,\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n):\n    if tool_server:\n        for tool_name in self.available_tools:\n            if tool_name in self._tool_sessions:\n                continue\n\n            tool_type = _map_tool_name_to_tool_type(tool_name)\n            headers = (\n                mcp_tools[tool_type].headers if tool_type in mcp_tools else None\n            )\n            tool_session = await exit_stack.enter_async_context(\n                tool_server.new_session(tool_name, request_id, headers)\n            )\n            self._tool_sessions[tool_name] = tool_session\n            exit_stack.push_async_exit(self.cleanup_session)",
      "language": "python"
    },
    {
      "code": "need_builtin_tool_call() -> bool",
      "language": "php"
    },
    {
      "code": "need_builtin_tool_call() -> bool",
      "language": "php"
    },
    {
      "code": "336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348",
      "language": "unknown"
    },
    {
      "code": "def need_builtin_tool_call(self) -> bool:\n    \"\"\"Return true if the last message is a MCP tool call\"\"\"\n    last_message = self.parser.response_messages[-1]\n    # TODO(qandrew): figure out which tools are MCP tools\n    if last_message.type == \"function_call\":  # noqa: SIM102\n        if last_message.name in (\n            \"code_interpreter\",\n            \"python\",\n            \"web_search_preview\",\n        ) or last_message.name.startswith(\"container\"):\n            return True\n\n    return False",
      "language": "python"
    },
    {
      "code": "def need_builtin_tool_call(self) -> bool:\n    \"\"\"Return true if the last message is a MCP tool call\"\"\"\n    last_message = self.parser.response_messages[-1]\n    # TODO(qandrew): figure out which tools are MCP tools\n    if last_message.type == \"function_call\":  # noqa: SIM102\n        if last_message.name in (\n            \"code_interpreter\",\n            \"python\",\n            \"web_search_preview\",\n        ) or last_message.name.startswith(\"container\"):\n            return True\n\n    return False",
      "language": "python"
    },
    {
      "code": "render_for_completion()",
      "language": "unknown"
    },
    {
      "code": "render_for_completion()",
      "language": "unknown"
    },
    {
      "code": "def render_for_completion(self):\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "def render_for_completion(self):\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253",
      "language": "unknown"
    },
    {
      "code": "class SimpleContext(ConversationContext):\n    \"\"\"This is a context that cannot handle MCP tool calls\"\"\"\n\n    def __init__(self):\n        self.last_output = None\n\n        # Accumulated final output for streaming mode\n        self._accumulated_text: str = \"\"\n        self._accumulated_token_ids: list[int] = []\n        self._accumulated_logprobs: list = []\n\n        self.num_prompt_tokens = 0\n        self.num_output_tokens = 0\n        self.num_cached_tokens = 0\n        # todo num_reasoning_tokens is not implemented yet.\n        self.num_reasoning_tokens = 0\n        # not implemented yet for SimpleContext\n        self.all_turn_metrics = []\n\n        self.input_messages: list[ResponseRawMessageAndToken] = []\n        self.output_messages: list[ResponseRawMessageAndToken] = []\n\n    def append_output(self, output) -> None:\n        self.last_output = output\n        if not isinstance(output, RequestOutput):\n            raise ValueError(\"SimpleContext only supports RequestOutput.\")\n        self.num_prompt_tokens = len(output.prompt_token_ids or [])\n        self.num_cached_tokens = output.num_cached_tokens or 0\n        self.num_output_tokens += len(output.outputs[0].token_ids or [])\n\n        # Accumulate text, token_ids, and logprobs for streaming mode\n        delta_output = output.outputs[0]\n        self._accumulated_text += delta_output.text\n        self._accumulated_token_ids.extend(delta_output.token_ids)\n        if delta_output.logprobs is not None:\n            self._accumulated_logprobs.extend(delta_output.logprobs)\n\n        if len(self.input_messages) == 0:\n            output_prompt = output.prompt or \"\"\n            output_prompt_token_ids = output.prompt_token_ids or []\n            self.input_messages.append(\n                ResponseRawMessageAndToken(\n                    message=output_prompt,\n                    tokens=output_prompt_token_ids,\n                )\n            )\n        self.output_messages.append(\n            ResponseRawMessageAndToken(\n                message=delta_output.text,\n                tokens=delta_output.token_ids,\n            )\n        )\n\n    @property\n    def final_output(self) -> RequestOutput | None:\n        \"\"\"Return the final output, with complete text/token_ids/logprobs.\"\"\"\n        if self.last_output is not None and self.last_output.outputs:\n            assert isinstance(self.last_output, RequestOutput)\n            final_output = copy.copy(self.last_output)\n            # copy inner item to avoid modify last_output\n            final_output.outputs = [replace(item) for item in self.last_output.outputs]\n            final_output.outputs[0].text = self._accumulated_text\n            final_output.outputs[0].token_ids = tuple(self._accumulated_token_ids)\n            if self._accumulated_logprobs:\n                final_output.outputs[0].logprobs = self._accumulated_logprobs\n            return final_output\n        return self.last_output\n\n    def append_tool_output(self, output) -> None:\n        raise NotImplementedError(\"Should not be called.\")\n\n    def need_builtin_tool_call(self) -> bool:\n        return False\n\n    async def call_tool(self) -> list[Message]:\n        raise NotImplementedError(\"Should not be called.\")\n\n    def render_for_completion(self) -> list[int]:\n        raise NotImplementedError(\"Should not be called.\")\n\n    async def init_tool_sessions(\n        self,\n        tool_server: ToolServer | None,\n        exit_stack: AsyncExitStack,\n        request_id: str,\n        mcp_tools: dict[str, Mcp],\n    ) -> None:\n        pass\n\n    async def cleanup_session(self) -> None:\n        raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "class SimpleContext(ConversationContext):\n    \"\"\"This is a context that cannot handle MCP tool calls\"\"\"\n\n    def __init__(self):\n        self.last_output = None\n\n        # Accumulated final output for streaming mode\n        self._accumulated_text: str = \"\"\n        self._accumulated_token_ids: list[int] = []\n        self._accumulated_logprobs: list = []\n\n        self.num_prompt_tokens = 0\n        self.num_output_tokens = 0\n        self.num_cached_tokens = 0\n        # todo num_reasoning_tokens is not implemented yet.\n        self.num_reasoning_tokens = 0\n        # not implemented yet for SimpleContext\n        self.all_turn_metrics = []\n\n        self.input_messages: list[ResponseRawMessageAndToken] = []\n        self.output_messages: list[ResponseRawMessageAndToken] = []\n\n    def append_output(self, output) -> None:\n        self.last_output = output\n        if not isinstance(output, RequestOutput):\n            raise ValueError(\"SimpleContext only supports RequestOutput.\")\n        self.num_prompt_tokens = len(output.prompt_token_ids or [])\n        self.num_cached_tokens = output.num_cached_tokens or 0\n        self.num_output_tokens += len(output.outputs[0].token_ids or [])\n\n        # Accumulate text, token_ids, and logprobs for streaming mode\n        delta_output = output.outputs[0]\n        self._accumulated_text += delta_output.text\n        self._accumulated_token_ids.extend(delta_output.token_ids)\n        if delta_output.logprobs is not None:\n            self._accumulated_logprobs.extend(delta_output.logprobs)\n\n        if len(self.input_messages) == 0:\n            output_prompt = output.prompt or \"\"\n            output_prompt_token_ids = output.prompt_token_ids or []\n            self.input_messages.append(\n                ResponseRawMessageAndToken(\n                    message=output_prompt,\n                    tokens=output_prompt_token_ids,\n                )\n            )\n        self.output_messages.append(\n            ResponseRawMessageAndToken(\n                message=delta_output.text,\n                tokens=delta_output.token_ids,\n            )\n        )\n\n    @property\n    def final_output(self) -> RequestOutput | None:\n        \"\"\"Return the final output, with complete text/token_ids/logprobs.\"\"\"\n        if self.last_output is not None and self.last_output.outputs:\n            assert isinstance(self.last_output, RequestOutput)\n            final_output = copy.copy(self.last_output)\n            # copy inner item to avoid modify last_output\n            final_output.outputs = [replace(item) for item in self.last_output.outputs]\n            final_output.outputs[0].text = self._accumulated_text\n            final_output.outputs[0].token_ids = tuple(self._accumulated_token_ids)\n            if self._accumulated_logprobs:\n                final_output.outputs[0].logprobs = self._accumulated_logprobs\n            return final_output\n        return self.last_output\n\n    def append_tool_output(self, output) -> None:\n        raise NotImplementedError(\"Should not be called.\")\n\n    def need_builtin_tool_call(self) -> bool:\n        return False\n\n    async def call_tool(self) -> list[Message]:\n        raise NotImplementedError(\"Should not be called.\")\n\n    def render_for_completion(self) -> list[int]:\n        raise NotImplementedError(\"Should not be called.\")\n\n    async def init_tool_sessions(\n        self,\n        tool_server: ToolServer | None,\n        exit_stack: AsyncExitStack,\n        request_id: str,\n        mcp_tools: dict[str, Mcp],\n    ) -> None:\n        pass\n\n    async def cleanup_session(self) -> None:\n        raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "_accumulated_logprobs: list = []",
      "language": "typescript"
    },
    {
      "code": "_accumulated_logprobs: list = []",
      "language": "typescript"
    },
    {
      "code": "_accumulated_text: str = ''",
      "language": "typescript"
    },
    {
      "code": "_accumulated_text: str = ''",
      "language": "typescript"
    },
    {
      "code": "_accumulated_token_ids: list[int] = []",
      "language": "yaml"
    },
    {
      "code": "_accumulated_token_ids: list[int] = []",
      "language": "yaml"
    },
    {
      "code": "all_turn_metrics = []",
      "language": "unknown"
    },
    {
      "code": "all_turn_metrics = []",
      "language": "unknown"
    },
    {
      "code": "final_output: RequestOutput | None",
      "language": "yaml"
    },
    {
      "code": "final_output: RequestOutput | None",
      "language": "yaml"
    },
    {
      "code": "input_messages: list[ResponseRawMessageAndToken] = []",
      "language": "yaml"
    },
    {
      "code": "input_messages: list[ResponseRawMessageAndToken] = []",
      "language": "yaml"
    },
    {
      "code": "last_output = None",
      "language": "rust"
    },
    {
      "code": "last_output = None",
      "language": "rust"
    },
    {
      "code": "num_cached_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_cached_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_output_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_output_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_prompt_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_prompt_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_reasoning_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "num_reasoning_tokens = 0",
      "language": "unknown"
    },
    {
      "code": "output_messages: list[ResponseRawMessageAndToken] = []",
      "language": "yaml"
    },
    {
      "code": "output_messages: list[ResponseRawMessageAndToken] = []",
      "language": "yaml"
    },
    {
      "code": "166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183",
      "language": "unknown"
    },
    {
      "code": "def __init__(self):\n    self.last_output = None\n\n    # Accumulated final output for streaming mode\n    self._accumulated_text: str = \"\"\n    self._accumulated_token_ids: list[int] = []\n    self._accumulated_logprobs: list = []\n\n    self.num_prompt_tokens = 0\n    self.num_output_tokens = 0\n    self.num_cached_tokens = 0\n    # todo num_reasoning_tokens is not implemented yet.\n    self.num_reasoning_tokens = 0\n    # not implemented yet for SimpleContext\n    self.all_turn_metrics = []\n\n    self.input_messages: list[ResponseRawMessageAndToken] = []\n    self.output_messages: list[ResponseRawMessageAndToken] = []",
      "language": "python"
    },
    {
      "code": "def __init__(self):\n    self.last_output = None\n\n    # Accumulated final output for streaming mode\n    self._accumulated_text: str = \"\"\n    self._accumulated_token_ids: list[int] = []\n    self._accumulated_logprobs: list = []\n\n    self.num_prompt_tokens = 0\n    self.num_output_tokens = 0\n    self.num_cached_tokens = 0\n    # todo num_reasoning_tokens is not implemented yet.\n    self.num_reasoning_tokens = 0\n    # not implemented yet for SimpleContext\n    self.all_turn_metrics = []\n\n    self.input_messages: list[ResponseRawMessageAndToken] = []\n    self.output_messages: list[ResponseRawMessageAndToken] = []",
      "language": "python"
    },
    {
      "code": "append_output(output) -> None",
      "language": "rust"
    },
    {
      "code": "append_output(output) -> None",
      "language": "rust"
    },
    {
      "code": "185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214",
      "language": "unknown"
    },
    {
      "code": "def append_output(self, output) -> None:\n    self.last_output = output\n    if not isinstance(output, RequestOutput):\n        raise ValueError(\"SimpleContext only supports RequestOutput.\")\n    self.num_prompt_tokens = len(output.prompt_token_ids or [])\n    self.num_cached_tokens = output.num_cached_tokens or 0\n    self.num_output_tokens += len(output.outputs[0].token_ids or [])\n\n    # Accumulate text, token_ids, and logprobs for streaming mode\n    delta_output = output.outputs[0]\n    self._accumulated_text += delta_output.text\n    self._accumulated_token_ids.extend(delta_output.token_ids)\n    if delta_output.logprobs is not None:\n        self._accumulated_logprobs.extend(delta_output.logprobs)\n\n    if len(self.input_messages) == 0:\n        output_prompt = output.prompt or \"\"\n        output_prompt_token_ids = output.prompt_token_ids or []\n        self.input_messages.append(\n            ResponseRawMessageAndToken(\n                message=output_prompt,\n                tokens=output_prompt_token_ids,\n            )\n        )\n    self.output_messages.append(\n        ResponseRawMessageAndToken(\n            message=delta_output.text,\n            tokens=delta_output.token_ids,\n        )\n    )",
      "language": "python"
    },
    {
      "code": "def append_output(self, output) -> None:\n    self.last_output = output\n    if not isinstance(output, RequestOutput):\n        raise ValueError(\"SimpleContext only supports RequestOutput.\")\n    self.num_prompt_tokens = len(output.prompt_token_ids or [])\n    self.num_cached_tokens = output.num_cached_tokens or 0\n    self.num_output_tokens += len(output.outputs[0].token_ids or [])\n\n    # Accumulate text, token_ids, and logprobs for streaming mode\n    delta_output = output.outputs[0]\n    self._accumulated_text += delta_output.text\n    self._accumulated_token_ids.extend(delta_output.token_ids)\n    if delta_output.logprobs is not None:\n        self._accumulated_logprobs.extend(delta_output.logprobs)\n\n    if len(self.input_messages) == 0:\n        output_prompt = output.prompt or \"\"\n        output_prompt_token_ids = output.prompt_token_ids or []\n        self.input_messages.append(\n            ResponseRawMessageAndToken(\n                message=output_prompt,\n                tokens=output_prompt_token_ids,\n            )\n        )\n    self.output_messages.append(\n        ResponseRawMessageAndToken(\n            message=delta_output.text,\n            tokens=delta_output.token_ids,\n        )\n    )",
      "language": "python"
    },
    {
      "code": "append_tool_output(output) -> None",
      "language": "rust"
    },
    {
      "code": "append_tool_output(output) -> None",
      "language": "rust"
    },
    {
      "code": "def append_tool_output(self, output) -> None:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "def append_tool_output(self, output) -> None:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "call_tool() -> list[Message]",
      "language": "php"
    },
    {
      "code": "call_tool() -> list[Message]",
      "language": "php"
    },
    {
      "code": "async def call_tool(self) -> list[Message]:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "async def call_tool(self) -> list[Message]:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "cleanup_session() -> None",
      "language": "rust"
    },
    {
      "code": "cleanup_session() -> None",
      "language": "rust"
    },
    {
      "code": "async def cleanup_session(self) -> None:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "async def cleanup_session(self) -> None:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "init_tool_sessions(\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n) -> None",
      "language": "rust"
    },
    {
      "code": "init_tool_sessions(\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n) -> None",
      "language": "rust"
    },
    {
      "code": "243\n244\n245\n246\n247\n248\n249\n250",
      "language": "unknown"
    },
    {
      "code": "async def init_tool_sessions(\n    self,\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n) -> None:\n    pass",
      "language": "python"
    },
    {
      "code": "async def init_tool_sessions(\n    self,\n    tool_server: ToolServer | None,\n    exit_stack: AsyncExitStack,\n    request_id: str,\n    mcp_tools: dict[str, Mcp],\n) -> None:\n    pass",
      "language": "python"
    },
    {
      "code": "need_builtin_tool_call() -> bool",
      "language": "php"
    },
    {
      "code": "need_builtin_tool_call() -> bool",
      "language": "php"
    },
    {
      "code": "def need_builtin_tool_call(self) -> bool:\n    return False",
      "language": "python"
    },
    {
      "code": "def need_builtin_tool_call(self) -> bool:\n    return False",
      "language": "python"
    },
    {
      "code": "render_for_completion() -> list[int]",
      "language": "php"
    },
    {
      "code": "render_for_completion() -> list[int]",
      "language": "php"
    },
    {
      "code": "def render_for_completion(self) -> list[int]:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "def render_for_completion(self) -> list[int]:\n    raise NotImplementedError(\"Should not be called.\")",
      "language": "python"
    },
    {
      "code": "818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892",
      "language": "unknown"
    },
    {
      "code": "class StreamingHarmonyContext(HarmonyContext):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.last_output = None\n\n        self.parser = get_streamable_parser_for_assistant()\n        self.encoding = get_encoding()\n        self.last_tok = None\n        self.first_tok_of_message = True\n\n    @property\n    def messages(self) -> list:\n        return self._messages\n\n    def append_output(self, output: RequestOutput) -> None:\n        # append_output is called for each output token in streaming case,\n        # so we only want to add the prompt tokens once for each message.\n        if self.first_tok_of_message:\n            self._update_prefill_token_usage(output)\n        # Reset self.first_tok_of_message if needed:\n        # if the current token is the last one of the current message\n        # (finished=True), then the next token processed will mark the\n        # beginning of a new message\n        self.first_tok_of_message = output.finished\n        for tok in output.outputs[0].token_ids:\n            self.parser.process(tok)\n        self._update_decode_token_usage(output)\n\n        # For streaming, update previous turn when message is complete\n        if output.finished:\n            self.all_turn_metrics.append(self.current_turn_metrics.copy())\n            self.current_turn_metrics.reset()\n        # Check if the current token is part of reasoning content\n        self._update_num_reasoning_tokens()\n        self.last_tok = tok\n        if len(self._messages) - self.num_init_messages < len(self.parser.messages):\n            self._messages.extend(\n                self.parser.messages[len(self._messages) - self.num_init_messages :]\n            )\n\n    def append_tool_output(self, output: list[Message]) -> None:\n        # Handle the case of tool output in direct message format\n        assert len(output) == 1, \"Tool output should be a single message\"\n        msg = output[0]\n        # Sometimes the recipient is not set for tool messages,\n        # so we set it to \"assistant\"\n        if msg.author.role == Role.TOOL and msg.recipient is None:\n            msg.recipient = \"assistant\"\n        toks = self.encoding.render(msg)\n        for tok in toks:\n            self.parser.process(tok)\n        self.last_tok = toks[-1]\n        # TODO: add tool_output messages to self._messages\n\n    def is_expecting_start(self) -> bool:\n        return self.parser.state == StreamState.EXPECT_START\n\n    def is_assistant_action_turn(self) -> bool:\n        return self.last_tok in self.encoding.stop_tokens_for_assistant_actions()\n\n    def render_for_completion(self) -> list[int]:\n        # now this list of tokens as next turn's starting tokens\n        # `<|start|>assistant`,\n        # we need to process them in parser.\n        rendered_tokens = super().render_for_completion()\n\n        last_n = -1\n        to_process = []\n        while rendered_tokens[last_n] != self.last_tok:\n            to_process.append(rendered_tokens[last_n])\n            last_n -= 1\n        for tok in reversed(to_process):\n            self.parser.process(tok)\n\n        return rendered_tokens",
      "language": "python"
    },
    {
      "code": "class StreamingHarmonyContext(HarmonyContext):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.last_output = None\n\n        self.parser = get_streamable_parser_for_assistant()\n        self.encoding = get_encoding()\n        self.last_tok = None\n        self.first_tok_of_message = True\n\n    @property\n    def messages(self) -> list:\n        return self._messages\n\n    def append_output(self, output: RequestOutput) -> None:\n        # append_output is called for each output token in streaming case,\n        # so we only want to add the prompt tokens once for each message.\n        if self.first_tok_of_message:\n            self._update_prefill_token_usage(output)\n        # Reset self.first_tok_of_message if needed:\n        # if the current token is the last one of the current message\n        # (finished=True), then the next token processed will mark the\n        # beginning of a new message\n        self.first_tok_of_message = output.finished\n        for tok in output.outputs[0].token_ids:\n            self.parser.process(tok)\n        self._update_decode_token_usage(output)\n\n        # For streaming, update previous turn when message is complete\n        if output.finished:\n            self.all_turn_metrics.append(self.current_turn_metrics.copy())\n            self.current_turn_metrics.reset()\n        # Check if the current token is part of reasoning content\n        self._update_num_reasoning_tokens()\n        self.last_tok = tok\n        if len(self._messages) - self.num_init_messages < len(self.parser.messages):\n            self._messages.extend(\n                self.parser.messages[len(self._messages) - self.num_init_messages :]\n            )\n\n    def append_tool_output(self, output: list[Message]) -> None:\n        # Handle the case of tool output in direct message format\n        assert len(output) == 1, \"Tool output should be a single message\"\n        msg = output[0]\n        # Sometimes the recipient is not set for tool messages,\n        # so we set it to \"assistant\"\n        if msg.author.role == Role.TOOL and msg.recipient is None:\n            msg.recipient = \"assistant\"\n        toks = self.encoding.render(msg)\n        for tok in toks:\n            self.parser.process(tok)\n        self.last_tok = toks[-1]\n        # TODO: add tool_output messages to self._messages\n\n    def is_expecting_start(self) -> bool:\n        return self.parser.state == StreamState.EXPECT_START\n\n    def is_assistant_action_turn(self) -> bool:\n        return self.last_tok in self.encoding.stop_tokens_for_assistant_actions()\n\n    def render_for_completion(self) -> list[int]:\n        # now this list of tokens as next turn's starting tokens\n        # `<|start|>assistant`,\n        # we need to process them in parser.\n        rendered_tokens = super().render_for_completion()\n\n        last_n = -1\n        to_process = []\n        while rendered_tokens[last_n] != self.last_tok:\n            to_process.append(rendered_tokens[last_n])\n            last_n -= 1\n        for tok in reversed(to_process):\n            self.parser.process(tok)\n\n        return rendered_tokens",
      "language": "python"
    },
    {
      "code": "encoding = get_encoding()",
      "language": "unknown"
    },
    {
      "code": "encoding = get_encoding()",
      "language": "unknown"
    },
    {
      "code": "first_tok_of_message = True",
      "language": "unknown"
    },
    {
      "code": "first_tok_of_message = True",
      "language": "unknown"
    },
    {
      "code": "last_output = None",
      "language": "rust"
    },
    {
      "code": "last_output = None",
      "language": "rust"
    },
    {
      "code": "last_tok = None",
      "language": "rust"
    },
    {
      "code": "last_tok = None",
      "language": "rust"
    },
    {
      "code": "messages: list",
      "language": "yaml"
    },
    {
      "code": "messages: list",
      "language": "yaml"
    },
    {
      "code": "parser = get_streamable_parser_for_assistant()",
      "language": "unknown"
    },
    {
      "code": "parser = get_streamable_parser_for_assistant()",
      "language": "unknown"
    },
    {
      "code": "__init__(*args, **kwargs)",
      "language": "python"
    },
    {
      "code": "__init__(*args, **kwargs)",
      "language": "python"
    },
    {
      "code": "819\n820\n821\n822\n823\n824\n825\n826",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.last_output = None\n\n    self.parser = get_streamable_parser_for_assistant()\n    self.encoding = get_encoding()\n    self.last_tok = None\n    self.first_tok_of_message = True",
      "language": "python"
    },
    {
      "code": "def __init__(self, *args, **kwargs):\n    super().__init__(*args, **kwargs)\n    self.last_output = None\n\n    self.parser = get_streamable_parser_for_assistant()\n    self.encoding = get_encoding()\n    self.last_tok = None\n    self.first_tok_of_message = True",
      "language": "python"
    },
    {
      "code": "append_output(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "append_output(output: RequestOutput) -> None",
      "language": "rust"
    },
    {
      "code": "832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856",
      "language": "unknown"
    },
    {
      "code": "def append_output(self, output: RequestOutput) -> None:\n    # append_output is called for each output token in streaming case,\n    # so we only want to add the prompt tokens once for each message.\n    if self.first_tok_of_message:\n        self._update_prefill_token_usage(output)\n    # Reset self.first_tok_of_message if needed:\n    # if the current token is the last one of the current message\n    # (finished=True), then the next token processed will mark the\n    # beginning of a new message\n    self.first_tok_of_message = output.finished\n    for tok in output.outputs[0].token_ids:\n        self.parser.process(tok)\n    self._update_decode_token_usage(output)\n\n    # For streaming, update previous turn when message is complete\n    if output.finished:\n        self.all_turn_metrics.append(self.current_turn_metrics.copy())\n        self.current_turn_metrics.reset()\n    # Check if the current token is part of reasoning content\n    self._update_num_reasoning_tokens()\n    self.last_tok = tok\n    if len(self._messages) - self.num_init_messages < len(self.parser.messages):\n        self._messages.extend(\n            self.parser.messages[len(self._messages) - self.num_init_messages :]\n        )",
      "language": "python"
    },
    {
      "code": "def append_output(self, output: RequestOutput) -> None:\n    # append_output is called for each output token in streaming case,\n    # so we only want to add the prompt tokens once for each message.\n    if self.first_tok_of_message:\n        self._update_prefill_token_usage(output)\n    # Reset self.first_tok_of_message if needed:\n    # if the current token is the last one of the current message\n    # (finished=True), then the next token processed will mark the\n    # beginning of a new message\n    self.first_tok_of_message = output.finished\n    for tok in output.outputs[0].token_ids:\n        self.parser.process(tok)\n    self._update_decode_token_usage(output)\n\n    # For streaming, update previous turn when message is complete\n    if output.finished:\n        self.all_turn_metrics.append(self.current_turn_metrics.copy())\n        self.current_turn_metrics.reset()\n    # Check if the current token is part of reasoning content\n    self._update_num_reasoning_tokens()\n    self.last_tok = tok\n    if len(self._messages) - self.num_init_messages < len(self.parser.messages):\n        self._messages.extend(\n            self.parser.messages[len(self._messages) - self.num_init_messages :]\n        )",
      "language": "python"
    },
    {
      "code": "append_tool_output(output: list[Message]) -> None",
      "language": "rust"
    },
    {
      "code": "append_tool_output(output: list[Message]) -> None",
      "language": "rust"
    },
    {
      "code": "858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869",
      "language": "unknown"
    },
    {
      "code": "def append_tool_output(self, output: list[Message]) -> None:\n    # Handle the case of tool output in direct message format\n    assert len(output) == 1, \"Tool output should be a single message\"\n    msg = output[0]\n    # Sometimes the recipient is not set for tool messages,\n    # so we set it to \"assistant\"\n    if msg.author.role == Role.TOOL and msg.recipient is None:\n        msg.recipient = \"assistant\"\n    toks = self.encoding.render(msg)\n    for tok in toks:\n        self.parser.process(tok)\n    self.last_tok = toks[-1]",
      "language": "python"
    },
    {
      "code": "def append_tool_output(self, output: list[Message]) -> None:\n    # Handle the case of tool output in direct message format\n    assert len(output) == 1, \"Tool output should be a single message\"\n    msg = output[0]\n    # Sometimes the recipient is not set for tool messages,\n    # so we set it to \"assistant\"\n    if msg.author.role == Role.TOOL and msg.recipient is None:\n        msg.recipient = \"assistant\"\n    toks = self.encoding.render(msg)\n    for tok in toks:\n        self.parser.process(tok)\n    self.last_tok = toks[-1]",
      "language": "python"
    },
    {
      "code": "is_assistant_action_turn() -> bool",
      "language": "php"
    },
    {
      "code": "is_assistant_action_turn() -> bool",
      "language": "php"
    },
    {
      "code": "def is_assistant_action_turn(self) -> bool:\n    return self.last_tok in self.encoding.stop_tokens_for_assistant_actions()",
      "language": "python"
    },
    {
      "code": "def is_assistant_action_turn(self) -> bool:\n    return self.last_tok in self.encoding.stop_tokens_for_assistant_actions()",
      "language": "python"
    },
    {
      "code": "is_expecting_start() -> bool",
      "language": "php"
    },
    {
      "code": "is_expecting_start() -> bool",
      "language": "php"
    },
    {
      "code": "def is_expecting_start(self) -> bool:\n    return self.parser.state == StreamState.EXPECT_START",
      "language": "python"
    },
    {
      "code": "def is_expecting_start(self) -> bool:\n    return self.parser.state == StreamState.EXPECT_START",
      "language": "python"
    },
    {
      "code": "render_for_completion() -> list[int]",
      "language": "php"
    },
    {
      "code": "render_for_completion() -> list[int]",
      "language": "php"
    },
    {
      "code": "878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892",
      "language": "unknown"
    },
    {
      "code": "def render_for_completion(self) -> list[int]:\n    # now this list of tokens as next turn's starting tokens\n    # `<|start|>assistant`,\n    # we need to process them in parser.\n    rendered_tokens = super().render_for_completion()\n\n    last_n = -1\n    to_process = []\n    while rendered_tokens[last_n] != self.last_tok:\n        to_process.append(rendered_tokens[last_n])\n        last_n -= 1\n    for tok in reversed(to_process):\n        self.parser.process(tok)\n\n    return rendered_tokens",
      "language": "python"
    },
    {
      "code": "def render_for_completion(self) -> list[int]:\n    # now this list of tokens as next turn's starting tokens\n    # `<|start|>assistant`,\n    # we need to process them in parser.\n    rendered_tokens = super().render_for_completion()\n\n    last_n = -1\n    to_process = []\n    while rendered_tokens[last_n] != self.last_tok:\n        to_process.append(rendered_tokens[last_n])\n        last_n -= 1\n    for tok in reversed(to_process):\n        self.parser.process(tok)\n\n    return rendered_tokens",
      "language": "python"
    },
    {
      "code": "73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102",
      "language": "unknown"
    },
    {
      "code": "class TurnMetrics:\n    \"\"\"Tracks token and toolcall details for a single conversation turn.\"\"\"\n\n    def __init__(\n        self,\n        input_tokens: int = 0,\n        output_tokens: int = 0,\n        cached_input_tokens: int = 0,\n        tool_output_tokens: int = 0,\n    ) -> None:\n        self.input_tokens = input_tokens\n        self.output_tokens = output_tokens\n        self.cached_input_tokens = cached_input_tokens\n        self.tool_output_tokens = tool_output_tokens\n\n    def reset(self) -> None:\n        \"\"\"Reset counters for a new turn.\"\"\"\n        self.input_tokens = 0\n        self.output_tokens = 0\n        self.cached_input_tokens = 0\n        self.tool_output_tokens = 0\n\n    def copy(self) -> \"TurnMetrics\":\n        \"\"\"Create a copy of this turn's token counts.\"\"\"\n        return TurnMetrics(\n            self.input_tokens,\n            self.output_tokens,\n            self.cached_input_tokens,\n            self.tool_output_tokens,\n        )",
      "language": "python"
    },
    {
      "code": "class TurnMetrics:\n    \"\"\"Tracks token and toolcall details for a single conversation turn.\"\"\"\n\n    def __init__(\n        self,\n        input_tokens: int = 0,\n        output_tokens: int = 0,\n        cached_input_tokens: int = 0,\n        tool_output_tokens: int = 0,\n    ) -> None:\n        self.input_tokens = input_tokens\n        self.output_tokens = output_tokens\n        self.cached_input_tokens = cached_input_tokens\n        self.tool_output_tokens = tool_output_tokens\n\n    def reset(self) -> None:\n        \"\"\"Reset counters for a new turn.\"\"\"\n        self.input_tokens = 0\n        self.output_tokens = 0\n        self.cached_input_tokens = 0\n        self.tool_output_tokens = 0\n\n    def copy(self) -> \"TurnMetrics\":\n        \"\"\"Create a copy of this turn's token counts.\"\"\"\n        return TurnMetrics(\n            self.input_tokens,\n            self.output_tokens,\n            self.cached_input_tokens,\n            self.tool_output_tokens,\n        )",
      "language": "python"
    },
    {
      "code": "cached_input_tokens = cached_input_tokens",
      "language": "unknown"
    },
    {
      "code": "cached_input_tokens = cached_input_tokens",
      "language": "unknown"
    },
    {
      "code": "input_tokens = input_tokens",
      "language": "unknown"
    },
    {
      "code": "input_tokens = input_tokens",
      "language": "unknown"
    },
    {
      "code": "output_tokens = output_tokens",
      "language": "unknown"
    },
    {
      "code": "output_tokens = output_tokens",
      "language": "unknown"
    },
    {
      "code": "tool_output_tokens = tool_output_tokens",
      "language": "unknown"
    },
    {
      "code": "tool_output_tokens = tool_output_tokens",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cached_input_tokens: int = 0,\n    tool_output_tokens: int = 0,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cached_input_tokens: int = 0,\n    tool_output_tokens: int = 0,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cached_input_tokens: int = 0,\n    tool_output_tokens: int = 0,\n) -> None:\n    self.input_tokens = input_tokens\n    self.output_tokens = output_tokens\n    self.cached_input_tokens = cached_input_tokens\n    self.tool_output_tokens = tool_output_tokens",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    input_tokens: int = 0,\n    output_tokens: int = 0,\n    cached_input_tokens: int = 0,\n    tool_output_tokens: int = 0,\n) -> None:\n    self.input_tokens = input_tokens\n    self.output_tokens = output_tokens\n    self.cached_input_tokens = cached_input_tokens\n    self.tool_output_tokens = tool_output_tokens",
      "language": "python"
    },
    {
      "code": "copy() -> TurnMetrics",
      "language": "php"
    },
    {
      "code": "copy() -> TurnMetrics",
      "language": "php"
    },
    {
      "code": "95\n 96\n 97\n 98\n 99\n100\n101\n102",
      "language": "unknown"
    },
    {
      "code": "def copy(self) -> \"TurnMetrics\":\n    \"\"\"Create a copy of this turn's token counts.\"\"\"\n    return TurnMetrics(\n        self.input_tokens,\n        self.output_tokens,\n        self.cached_input_tokens,\n        self.tool_output_tokens,\n    )",
      "language": "python"
    },
    {
      "code": "def copy(self) -> \"TurnMetrics\":\n    \"\"\"Create a copy of this turn's token counts.\"\"\"\n    return TurnMetrics(\n        self.input_tokens,\n        self.output_tokens,\n        self.cached_input_tokens,\n        self.tool_output_tokens,\n    )",
      "language": "python"
    },
    {
      "code": "reset() -> None",
      "language": "rust"
    },
    {
      "code": "reset() -> None",
      "language": "rust"
    },
    {
      "code": "88\n89\n90\n91\n92\n93",
      "language": "unknown"
    },
    {
      "code": "def reset(self) -> None:\n    \"\"\"Reset counters for a new turn.\"\"\"\n    self.input_tokens = 0\n    self.output_tokens = 0\n    self.cached_input_tokens = 0\n    self.tool_output_tokens = 0",
      "language": "python"
    },
    {
      "code": "def reset(self) -> None:\n    \"\"\"Reset counters for a new turn.\"\"\"\n    self.input_tokens = 0\n    self.output_tokens = 0\n    self.cached_input_tokens = 0\n    self.tool_output_tokens = 0",
      "language": "python"
    },
    {
      "code": "_create_json_parse_error_messages(\n    last_msg: Message, e: JSONDecodeError\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "_create_json_parse_error_messages(\n    last_msg: Message, e: JSONDecodeError\n) -> list[Message]",
      "language": "php"
    },
    {
      "code": "141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160",
      "language": "unknown"
    },
    {
      "code": "def _create_json_parse_error_messages(\n    last_msg: Message, e: json.JSONDecodeError\n) -> list[Message]:\n    \"\"\"\n    Creates an error message when json parse failed.\n    \"\"\"\n    error_msg = (\n        f\"Error parsing tool arguments as JSON: {str(e)}. \"\n        \"Please ensure the tool call arguments are valid JSON and try again.\"\n    )\n    content = TextContent(text=error_msg)\n    author = Author(role=Role.TOOL, name=last_msg.recipient)\n    return [\n        Message(\n            author=author,\n            content=[content],\n            recipient=Role.ASSISTANT,\n            channel=last_msg.channel,\n        )\n    ]",
      "language": "python"
    },
    {
      "code": "def _create_json_parse_error_messages(\n    last_msg: Message, e: json.JSONDecodeError\n) -> list[Message]:\n    \"\"\"\n    Creates an error message when json parse failed.\n    \"\"\"\n    error_msg = (\n        f\"Error parsing tool arguments as JSON: {str(e)}. \"\n        \"Please ensure the tool call arguments are valid JSON and try again.\"\n    )\n    content = TextContent(text=error_msg)\n    author = Author(role=Role.TOOL, name=last_msg.recipient)\n    return [\n        Message(\n            author=author,\n            content=[content],\n            recipient=Role.ASSISTANT,\n            channel=last_msg.channel,\n        )\n    ]",
      "language": "python"
    },
    {
      "code": "_map_tool_name_to_tool_type(tool_name: str) -> str",
      "language": "php"
    },
    {
      "code": "_map_tool_name_to_tool_type(tool_name: str) -> str",
      "language": "php"
    },
    {
      "code": "63\n64\n65\n66\n67\n68\n69\n70",
      "language": "unknown"
    },
    {
      "code": "def _map_tool_name_to_tool_type(tool_name: str) -> str:\n    if tool_name not in _TOOL_NAME_TO_TYPE_MAP:\n        available_tools = \", \".join(_TOOL_NAME_TO_TYPE_MAP.keys())\n        raise ValueError(\n            f\"Built-in tool name '{tool_name}' not defined in mapping. \"\n            f\"Available tools: {available_tools}\"\n        )\n    return _TOOL_NAME_TO_TYPE_MAP[tool_name]",
      "language": "python"
    },
    {
      "code": "def _map_tool_name_to_tool_type(tool_name: str) -> str:\n    if tool_name not in _TOOL_NAME_TO_TYPE_MAP:\n        available_tools = \", \".join(_TOOL_NAME_TO_TYPE_MAP.keys())\n        raise ValueError(\n            f\"Built-in tool name '{tool_name}' not defined in mapping. \"\n            f\"Available tools: {available_tools}\"\n        )\n    return _TOOL_NAME_TO_TYPE_MAP[tool_name]",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}