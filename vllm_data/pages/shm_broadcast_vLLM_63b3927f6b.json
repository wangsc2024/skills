{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
  "title": "shm_broadcast - vLLM",
  "content": "Creates a MessageQueue for a distributed process group with one writer and multiple readers.\n\nThis method is designed for scenarios where one process (the writer) sends messages, and all other processes (the readers) receive messages. It sets up the shared memory buffer and socket communication handles accordingly, and broadcasts the handle from the writer to all readers.\n\nThe torch distributed process group.\n\nMaximum size in bytes for each chunk in the buffer.\n\nMaximum number of chunks in the buffer.\n\nThe global rank that will act as the writer. Defaults to 0.\n\nUsed when there is a handle from an external Message Queue. If provided, use this handle to init PG writer message queue instead of creating a new one. Defaults to None.\n\nIf True, blocks until all processes are ready. Defaults to True.\n\nThe MessageQueue instance for the calling process.\n\nCreates a MessageQueue for a process group with a single reader.\n\nThis method is designed for scenarios where only one process (the reader) will consume messages, and all other processes are writers. It sets up the shared memory buffer and communication handles accordingly, and gathers the handles from all processes to the reader.\n\nThe torch distributed process group.\n\nMaximum size in bytes for each chunk in the buffer.\n\nMaximum number of chunks in the buffer.\n\nThe global rank that will act as the reader. Defaults to 0.\n\nIf True, blocks until all processes are ready. Defaults to False.\n\ntuple[MessageQueue, list[Handle]]:\n\nThe MessageQueue instance for the calling process,\n\nand a list of handles (only non-empty for the reader process).\n\nRead from message queue with optional timeout (in seconds)\n\nWrite to message queue with optional timeout (in seconds)\n\nThis is a collective operation. All processes (including the readers and the writer) should call this function.\n\nA shared memory ring buffer implementation for broadcast communication. Essentially, it is a queue where only one will enqueue and multiple will dequeue. The max size of each item, together with the max number of items that can be stored in the buffer are known in advance. In this case, we don't need to synchronize the access to the buffer.\n\ndata metadata | | | (current_idx) | (current_idx) v v\n\n+-------------------------------+----------------------------------------+ | chunk0 | chunk1 | ... | chunk | metadata0 | metadata1 | ... | metadata | +-------------------------------+----------------------------------------+ | max_chunks x max_chunk_bytes | max_chunks x (1 + n_reader) bytes |\n\nmetadata memory layout: each byte is a flag, the first byte is the written flag, and the rest are reader flags. The flags are set to 0 by default. +--------------+--------------+--------------+-----+--------------+ | written_flag | reader0_flag | reader1_flag | ... | readerN_flag | +--------------+--------------+--------------+-----+--------------+\n\nThe state of metadata is as follows:\n\n(case 1) 0???...???: the block is not written yet, cannot read, can write (case 2) 1000...000: the block is just written, can read, cannot write (case 3) 1???...???: the block is written and read by some readers, can read if not read, cannot write (case 4) 1111...111: the block is written and read by all readers, cannot read, can write\n\nState transition for readers:\n\nWhen a reader finds a block that it can read (case 2 or 3), it can yield the block for caller to read. Only after the caller finishes reading the block, the reader can mark the block as read. Readers only mark the block as read (from 0 to 1), the writer marks the block as ready to read (from 1 to 0).\n\nState transition for writer:\n\nWhen the writer writes to a block (case 1 or 4), it first resets the written flag to 0, converting either case to case 1. Then it can yield the block for caller to write. After the caller finishes writing the block, the writer can reset the reader flags to 0, and mark the block as written (from 0 to 1). NOTE: the order is important here, first reset the reader flags (so that we are still in case 1), then mark the block as written. The state transition is atomic. If we do it in the reverse order, it will go through case 3 and then back to case 2, and readers might read the intermediate case 3, which is not correct.\n\nDuring creation, name is None and the buffer is created. We can pass the created object to other processes by pickling it. The other processes will get the name of the shared memory and open it, so that they can access the same shared memory buffer.\n\nIn setups which have long inactivity periods it is desirable to reduce system power consumption when vllm does nothing. This would lead to more CPU thermal headroom when a request eventually comes, especially when multiple GPUs are connected as each GPU would otherwise pin one thread at 100% CPU usage.\n\nThe simplest solution is to reduce polling frequency when there is no activity for a certain period of time.\n\nFull memory barrier for shared memory synchronization.\n\nEnsures all prior memory writes are visible to other processes before any subsequent reads. This is critical for lock-free producer-consumer patterns using shared memory.\n\nImplementation acquires and immediately releases a lock. Python's threading.Lock provides sequentially consistent memory barrier semantics across all major platforms (POSIX, Windows). This is a lightweight operation (~20ns) that guarantees: - All stores before the barrier are visible to other threads/processes - All loads after the barrier see the latest values",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.distributed.device_communicators.shm_broadcast ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast"
    },
    {
      "level": "h2",
      "text": "VLLM_RINGBUFFER_WARNING_INTERVAL module-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.VLLM_RINGBUFFER_WARNING_INTERVAL"
    },
    {
      "level": "h2",
      "text": "_memory_fence_lock module-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast._memory_fence_lock"
    },
    {
      "level": "h2",
      "text": "from_bytes_big module-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.from_bytes_big"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.logger"
    },
    {
      "level": "h2",
      "text": "Handle dataclass ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.Handle"
    },
    {
      "level": "h3",
      "text": "buffer_handle class-attribute instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.Handle.buffer_handle"
    },
    {
      "level": "h3",
      "text": "local_reader_ranks class-attribute instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.Handle.local_reader_ranks"
    },
    {
      "level": "h3",
      "text": "local_subscribe_addr class-attribute instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.Handle.local_subscribe_addr"
    },
    {
      "level": "h3",
      "text": "remote_addr_ipv6 class-attribute instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.Handle.remote_addr_ipv6"
    },
    {
      "level": "h3",
      "text": "remote_subscribe_addr class-attribute instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.Handle.remote_subscribe_addr"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.Handle.__init__"
    },
    {
      "level": "h2",
      "text": "MessageQueue ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue"
    },
    {
      "level": "h3",
      "text": "_is_local_reader instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue._is_local_reader"
    },
    {
      "level": "h3",
      "text": "_is_remote_reader instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue._is_remote_reader"
    },
    {
      "level": "h3",
      "text": "_is_writer instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue._is_writer"
    },
    {
      "level": "h3",
      "text": "_read_spin_timer instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue._read_spin_timer"
    },
    {
      "level": "h3",
      "text": "buffer instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.buffer"
    },
    {
      "level": "h3",
      "text": "current_idx instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.current_idx"
    },
    {
      "level": "h3",
      "text": "handle instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.handle"
    },
    {
      "level": "h3",
      "text": "local_reader_rank instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.local_reader_rank"
    },
    {
      "level": "h3",
      "text": "local_socket instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.local_socket"
    },
    {
      "level": "h3",
      "text": "n_local_reader instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.n_local_reader"
    },
    {
      "level": "h3",
      "text": "n_remote_reader instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.n_remote_reader"
    },
    {
      "level": "h3",
      "text": "remote_socket instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.remote_socket"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.__init__"
    },
    {
      "level": "h3",
      "text": "acquire_read ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.acquire_read"
    },
    {
      "level": "h3",
      "text": "acquire_write ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.acquire_write"
    },
    {
      "level": "h3",
      "text": "broadcast_object ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.broadcast_object"
    },
    {
      "level": "h3",
      "text": "create_from_handle staticmethod ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.create_from_handle"
    },
    {
      "level": "h3",
      "text": "create_from_process_group staticmethod ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.create_from_process_group"
    },
    {
      "level": "h3",
      "text": "create_from_process_group_single_reader staticmethod ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.create_from_process_group_single_reader"
    },
    {
      "level": "h3",
      "text": "dequeue ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.dequeue"
    },
    {
      "level": "h3",
      "text": "enqueue ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.enqueue"
    },
    {
      "level": "h3",
      "text": "export_handle ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.export_handle"
    },
    {
      "level": "h3",
      "text": "recv staticmethod ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.recv"
    },
    {
      "level": "h3",
      "text": "wait_until_ready ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.MessageQueue.wait_until_ready"
    },
    {
      "level": "h2",
      "text": "ShmRingBuffer ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer"
    },
    {
      "level": "h3",
      "text": "data_offset instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.data_offset"
    },
    {
      "level": "h3",
      "text": "is_creator instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.is_creator"
    },
    {
      "level": "h3",
      "text": "max_chunk_bytes instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.max_chunk_bytes"
    },
    {
      "level": "h3",
      "text": "max_chunks instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.max_chunks"
    },
    {
      "level": "h3",
      "text": "metadata_offset instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.metadata_offset"
    },
    {
      "level": "h3",
      "text": "metadata_size instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.metadata_size"
    },
    {
      "level": "h3",
      "text": "n_reader instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.n_reader"
    },
    {
      "level": "h3",
      "text": "shared_memory instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.shared_memory"
    },
    {
      "level": "h3",
      "text": "total_bytes_of_buffer instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.total_bytes_of_buffer"
    },
    {
      "level": "h3",
      "text": "__del__ ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.__del__"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.__init__"
    },
    {
      "level": "h3",
      "text": "__reduce__ ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.__reduce__"
    },
    {
      "level": "h3",
      "text": "get_data ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.get_data"
    },
    {
      "level": "h3",
      "text": "get_metadata ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.get_metadata"
    },
    {
      "level": "h3",
      "text": "handle ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer.handle"
    },
    {
      "level": "h2",
      "text": "SpinSleepTimer ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinSleepTimer"
    },
    {
      "level": "h3",
      "text": "busy_loop_s instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinSleepTimer.busy_loop_s"
    },
    {
      "level": "h3",
      "text": "last_activity instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinSleepTimer.last_activity"
    },
    {
      "level": "h3",
      "text": "wait_sleep_s instance-attribute ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinSleepTimer.wait_sleep_s"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinSleepTimer.__init__"
    },
    {
      "level": "h3",
      "text": "record_activity ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinSleepTimer.record_activity"
    },
    {
      "level": "h3",
      "text": "spin ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinSleepTimer.spin"
    },
    {
      "level": "h2",
      "text": "SpinTimer ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinTimer"
    },
    {
      "level": "h3",
      "text": "record_activity ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinTimer.record_activity"
    },
    {
      "level": "h3",
      "text": "spin ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.SpinTimer.spin"
    },
    {
      "level": "h2",
      "text": "long_wait_time_msg ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.long_wait_time_msg"
    },
    {
      "level": "h2",
      "text": "memory_fence ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.memory_fence"
    },
    {
      "level": "h2",
      "text": "to_bytes_big ¶",
      "id": "vllm.distributed.device_communicators.shm_broadcast.to_bytes_big"
    }
  ],
  "code_samples": [
    {
      "code": "VLLM_RINGBUFFER_WARNING_INTERVAL = (\n    VLLM_RINGBUFFER_WARNING_INTERVAL\n)",
      "language": "unknown"
    },
    {
      "code": "VLLM_RINGBUFFER_WARNING_INTERVAL = (\n    VLLM_RINGBUFFER_WARNING_INTERVAL\n)",
      "language": "unknown"
    },
    {
      "code": "_memory_fence_lock = Lock()",
      "language": "unknown"
    },
    {
      "code": "_memory_fence_lock = Lock()",
      "language": "unknown"
    },
    {
      "code": "from_bytes_big = partial(from_bytes, byteorder='big')",
      "language": "unknown"
    },
    {
      "code": "from_bytes_big = partial(from_bytes, byteorder='big')",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "262\n263\n264\n265\n266\n267\n268\n269",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass Handle:\n    local_reader_ranks: list[int] = field(default_factory=list)\n\n    buffer_handle: tuple[int, int, int, str] | None = None\n    local_subscribe_addr: str | None = None\n    remote_subscribe_addr: str | None = None\n    remote_addr_ipv6: bool = False",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass Handle:\n    local_reader_ranks: list[int] = field(default_factory=list)\n\n    buffer_handle: tuple[int, int, int, str] | None = None\n    local_subscribe_addr: str | None = None\n    remote_subscribe_addr: str | None = None\n    remote_addr_ipv6: bool = False",
      "language": "python"
    },
    {
      "code": "buffer_handle: tuple[int, int, int, str] | None = None",
      "language": "yaml"
    },
    {
      "code": "buffer_handle: tuple[int, int, int, str] | None = None",
      "language": "yaml"
    },
    {
      "code": "local_reader_ranks: list[int] = field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "local_reader_ranks: list[int] = field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "local_subscribe_addr: str | None = None",
      "language": "yaml"
    },
    {
      "code": "local_subscribe_addr: str | None = None",
      "language": "yaml"
    },
    {
      "code": "remote_addr_ipv6: bool = False",
      "language": "typescript"
    },
    {
      "code": "remote_addr_ipv6: bool = False",
      "language": "typescript"
    },
    {
      "code": "remote_subscribe_addr: str | None = None",
      "language": "yaml"
    },
    {
      "code": "remote_subscribe_addr: str | None = None",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    local_reader_ranks: list[int] = list(),\n    buffer_handle: tuple[int, int, int, str] | None = None,\n    local_subscribe_addr: str | None = None,\n    remote_subscribe_addr: str | None = None,\n    remote_addr_ipv6: bool = False,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    local_reader_ranks: list[int] = list(),\n    buffer_handle: tuple[int, int, int, str] | None = None,\n    local_subscribe_addr: str | None = None,\n    remote_subscribe_addr: str | None = None,\n    remote_addr_ipv6: bool = False,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778",
      "language": "unknown"
    },
    {
      "code": "class MessageQueue:\n    def __init__(\n        self,\n        n_reader,  # number of all readers\n        n_local_reader,  # number of local readers through shared memory\n        local_reader_ranks: list[int] | None = None,\n        # Default of 24MiB chosen to be large enough to accommodate grammar\n        # bitmask tensors for large batches (1024 requests).\n        max_chunk_bytes: int = 1024 * 1024 * 24,\n        max_chunks: int = 10,\n        connect_ip: str | None = None,\n    ):\n        if local_reader_ranks is None:\n            local_reader_ranks = list(range(n_local_reader))\n        else:\n            assert len(local_reader_ranks) == n_local_reader\n        self.n_local_reader = n_local_reader\n        n_remote_reader = n_reader - n_local_reader\n        self.n_remote_reader = n_remote_reader\n\n        context = Context()\n\n        if n_local_reader > 0:\n            # for local readers, we will:\n            # 1. create a shared memory ring buffer to communicate small data\n            # 2. create a publish-subscribe socket to communicate large data\n            self.buffer = ShmRingBuffer(n_local_reader, max_chunk_bytes, max_chunks)\n\n            # XPUB is very similar to PUB,\n            # except that it can receive subscription messages\n            # to confirm the number of subscribers\n            self.local_socket = context.socket(XPUB)\n            # set the verbose option so that we can receive every subscription\n            # message. otherwise, we will only receive the first subscription\n            # see http://api.zeromq.org/3-3:zmq-setsockopt for more details\n            self.local_socket.setsockopt(XPUB_VERBOSE, True)\n            local_subscribe_addr = get_open_zmq_ipc_path()\n            logger.debug(\"Binding to %s\", local_subscribe_addr)\n            self.local_socket.bind(local_subscribe_addr)\n\n            self.current_idx = 0\n        else:\n            self.buffer = None  # type: ignore\n            local_subscribe_addr = None\n            self.local_socket = None\n            self.current_idx = -1\n\n        remote_addr_ipv6 = False\n        if n_remote_reader > 0:\n            # for remote readers, we will:\n            # create a publish-subscribe socket to communicate large data\n            if not connect_ip:\n                connect_ip = get_ip()\n            self.remote_socket = context.socket(XPUB)\n            self.remote_socket.setsockopt(XPUB_VERBOSE, True)\n            remote_subscribe_port = get_open_port()\n            if is_valid_ipv6_address(connect_ip):\n                self.remote_socket.setsockopt(IPV6, 1)\n                remote_addr_ipv6 = True\n                connect_ip = f\"[{connect_ip}]\"\n            socket_addr = f\"tcp://{connect_ip}:{remote_subscribe_port}\"\n            self.remote_socket.bind(socket_addr)\n            remote_subscribe_addr = f\"tcp://{connect_ip}:{remote_subscribe_port}\"\n        else:\n            remote_subscribe_addr = None\n            self.remote_socket = None\n\n        self._is_writer = True\n        self._is_local_reader = False\n        self.local_reader_rank = -1\n        # rank does not matter for remote readers\n        self._is_remote_reader = False\n        self._read_spin_timer = SpinTimer()\n\n        self.handle = Handle(\n            local_reader_ranks=local_reader_ranks,\n            buffer_handle=self.buffer.handle() if self.buffer is not None else None,\n            local_subscribe_addr=local_subscribe_addr,\n            remote_subscribe_addr=remote_subscribe_addr,\n            remote_addr_ipv6=remote_addr_ipv6,\n        )\n\n        logger.debug(\"vLLM message queue communication handle: %s\", self.handle)\n\n    def export_handle(self) -> Handle:\n        return self.handle\n\n    @staticmethod\n    def create_from_handle(handle: Handle, rank) -> \"MessageQueue\":\n        self = MessageQueue.__new__(MessageQueue)\n        self.handle = handle\n        self._is_writer = False\n\n        context = Context()\n\n        if rank in handle.local_reader_ranks:\n            assert handle.buffer_handle is not None\n            self.buffer = ShmRingBuffer(*handle.buffer_handle)\n            self.current_idx = 0\n            self.local_reader_rank = handle.local_reader_ranks.index(rank)\n            self._is_local_reader = True\n            self._is_remote_reader = False\n\n            self.local_socket = context.socket(SUB)\n            self.local_socket.setsockopt_string(SUBSCRIBE, \"\")\n            socket_addr = handle.local_subscribe_addr\n            logger.debug(\"Connecting to %s\", socket_addr)\n            self.local_socket.connect(socket_addr)\n\n            self.remote_socket = None\n\n            self._read_spin_timer = (\n                SpinSleepTimer() if envs.VLLM_SLEEP_WHEN_IDLE else SpinTimer()\n            )\n        else:\n            self.buffer = None  # type: ignore\n            self.current_idx = -1\n            self.local_reader_rank = -1\n            self._is_local_reader = False\n            self._is_remote_reader = True\n\n            self.local_socket = None\n\n            self.remote_socket = context.socket(SUB)\n            self.remote_socket.setsockopt_string(SUBSCRIBE, \"\")\n            if handle.remote_addr_ipv6:\n                self.remote_socket.setsockopt(IPV6, 1)\n            socket_addr = handle.remote_subscribe_addr\n            logger.debug(\"Connecting to %s\", socket_addr)\n            self.remote_socket.connect(socket_addr)\n\n        return self\n\n    def wait_until_ready(self):\n        \"\"\"This is a collective operation. All processes (including the\n        readers and the writer) should call this function.\n        \"\"\"\n        if self._is_writer:\n            # wait for all readers to connect\n\n            # local readers\n            for i in range(self.n_local_reader):\n                # wait for subscription messages from all local readers\n                self.local_socket.recv()\n            if self.n_local_reader > 0:\n                # send a message to all local readers\n                # to make sure the publish channel is working\n                self.local_socket.send(b\"READY\")\n\n            # remote readers\n            for i in range(self.n_remote_reader):\n                # wait for subscription messages from all remote readers\n                self.remote_socket.recv()\n            if self.n_remote_reader > 0:\n                # send a message to all remote readers\n                # to make sure the publish channel is working\n                self.remote_socket.send(b\"READY\")\n        elif self._is_local_reader:\n            # wait for the writer to send a message\n            recv = self.local_socket.recv()\n            assert recv == b\"READY\"\n        elif self._is_remote_reader:\n            # wait for the writer to send a message\n            recv = self.remote_socket.recv()\n            assert recv == b\"READY\"\n\n    @contextmanager\n    def acquire_write(self, timeout: float | None = None):\n        assert self._is_writer, \"Only writers can acquire write\"\n        start_time = time.monotonic()\n        n_warning = 1\n        while True:\n            with self.buffer.get_metadata(self.current_idx) as metadata_buffer:\n                # Memory fence ensures we see the latest read flags from readers.\n                # Without this, we may read stale flags from our CPU cache and\n                # spin indefinitely even though readers have completed.\n                memory_fence()\n                read_count = sum(metadata_buffer[1:])\n                written_flag = metadata_buffer[0]\n                if written_flag and read_count != self.buffer.n_reader:\n                    # this block is written and not read by all readers\n                    # for writers, `self.current_idx` is the next block to write\n                    # if this block is not ready to write,\n                    # we need to wait until it is read by all readers\n\n                    # Release the processor to other threads\n                    sched_yield()\n\n                    # if we time out, raise an exception\n                    elapsed = time.monotonic() - start_time\n                    if timeout is not None and elapsed > timeout:\n                        raise TimeoutError\n\n                    # if we wait for a long time, log a message\n                    if elapsed > VLLM_RINGBUFFER_WARNING_INTERVAL * n_warning:\n                        logger.info(\n                            long_wait_time_msg(VLLM_RINGBUFFER_WARNING_INTERVAL)\n                        )\n                        n_warning += 1\n\n                    continue\n                # found a block that is either\n                # (1) not written\n                # (2) read by all readers\n\n                # mark the block as not written\n                metadata_buffer[0] = 0\n                # let caller write to the buffer\n                with self.buffer.get_data(self.current_idx) as buf:\n                    yield buf\n\n                # caller has written to the buffer\n                # NOTE: order is important here\n                # first set the read flags to 0\n                # then set the written flag to 1\n                # otherwise, the readers may think they already read the block\n                for i in range(1, self.buffer.n_reader + 1):\n                    # set read flag to 0, meaning it is not read yet\n                    metadata_buffer[i] = 0\n                # mark the block as written\n                metadata_buffer[0] = 1\n                # Memory fence ensures the write is visible to readers on other cores\n                # before we proceed. Without this, readers may spin indefinitely\n                # waiting for a write that's stuck in our CPU's store buffer.\n                memory_fence()\n                self.current_idx = (self.current_idx + 1) % self.buffer.max_chunks\n                break\n\n    @contextmanager\n    def acquire_read(\n        self,\n        timeout: float | None = None,\n        cancel: Event | None = None,\n        indefinite: bool = False,\n    ):\n        assert self._is_local_reader, \"Only readers can acquire read\"\n        start_time = time.monotonic()\n        n_warning = 1\n        while True:\n            with self.buffer.get_metadata(self.current_idx) as metadata_buffer:\n                # Memory fence ensures we see the latest writes from the writer.\n                # Without this, we may read stale flags from our CPU cache\n                # and spin indefinitely even though writer has updated them.\n                memory_fence()\n                read_flag = metadata_buffer[self.local_reader_rank + 1]\n                written_flag = metadata_buffer[0]\n                if not written_flag or read_flag:\n                    # this block is either\n                    # (1) not written\n                    # (2) already read by this reader\n\n                    # for readers, `self.current_idx` is the next block to read\n                    # if this block is not ready,\n                    # we need to wait until it is written\n\n                    # Release the processor to other threads\n                    self._read_spin_timer.spin()\n\n                    if cancel is not None and cancel.is_set():\n                        raise RuntimeError(\"cancelled\")\n\n                    # if we time out, raise an exception\n                    elapsed = time.monotonic() - start_time\n                    if timeout is not None and elapsed > timeout:\n                        raise TimeoutError\n\n                    # if we wait for a long time, log a message\n                    if not indefinite and (\n                        elapsed > VLLM_RINGBUFFER_WARNING_INTERVAL * n_warning\n                    ):\n                        logger.info(\n                            long_wait_time_msg(VLLM_RINGBUFFER_WARNING_INTERVAL)\n                        )\n                        n_warning += 1\n\n                    continue\n                # found a block that is not read by this reader\n                # let caller read from the buffer\n                with self.buffer.get_data(self.current_idx) as buf:\n                    yield buf\n\n                # caller has read from the buffer\n                # set the read flag\n                metadata_buffer[self.local_reader_rank + 1] = 1\n                # Memory fence ensures the read flag is visible to the writer.\n                # Without this, writer may not see our read completion and\n                # could wait indefinitely for all readers to finish.\n                memory_fence()\n                self.current_idx = (self.current_idx + 1) % self.buffer.max_chunks\n\n                self._read_spin_timer.record_activity()\n                break\n\n    def enqueue(self, obj, timeout: float | None = None):\n        \"\"\"Write to message queue with optional timeout (in seconds)\"\"\"\n        assert self._is_writer, \"Only writers can enqueue\"\n        all_buffers: list[SizedBuffer] = [b\"\"]\n        total_bytes = 6  # 2 bytes for oob buffer count, 4 for main buffer size\n\n        def oob_callback(buf: PickleBuffer) -> bool:\n            raw_buf = buf.raw()\n            if len(raw_buf) < 1024 * 1024:\n                # In-line buffers smaller than 1MiB.\n                return True\n            all_buffers.append(raw_buf)\n            nonlocal total_bytes\n            total_bytes += len(raw_buf) + 4\n            return False\n\n        all_buffers[0] = pickle.dumps(\n            obj, protocol=pickle.HIGHEST_PROTOCOL, buffer_callback=oob_callback\n        )\n        if self.n_local_reader > 0:\n            if total_bytes + len(all_buffers[0]) >= self.buffer.max_chunk_bytes:\n                with self.acquire_write(timeout) as buf:\n                    buf[0] = 1  # overflow\n                self.local_socket.send_multipart(all_buffers, copy=False)\n            else:\n                # Byte 0: 0\n                # Bytes 1-2: Count of buffers\n                # Then each buffer follows, preceded by 4 bytes containing its length:\n                # [4 byte int L][L bytes of buffer content] ...\n                with self.acquire_write(timeout) as buf:\n                    buf[0] = 0  # not overflow\n                    offset = 3\n                    buf[1:offset] = to_bytes_big(len(all_buffers), 2)  # oob buf count\n                    for buffer in all_buffers:\n                        buf_len = len(buffer)\n                        # prepend each buffer with 4 bytes containing its size.\n                        buf_offset = offset + 4\n                        buf[offset:buf_offset] = to_bytes_big(buf_len, 4)\n                        buf[buf_offset : (offset := buf_offset + buf_len)] = buffer\n\n        if self.n_remote_reader > 0:\n            self.remote_socket.send_multipart(all_buffers, copy=False)\n\n    def dequeue(\n        self,\n        timeout: float | None = None,\n        cancel: Event | None = None,\n        indefinite: bool = False,\n    ):\n        \"\"\"Read from message queue with optional timeout (in seconds)\"\"\"\n        if self._is_local_reader:\n            with self.acquire_read(timeout, cancel, indefinite) as buf:\n                overflow = buf[0] == 1\n                if not overflow:\n                    offset = 3\n                    buf_count = from_bytes_big(buf[1:offset])\n                    all_buffers = []\n                    for i in range(buf_count):\n                        buf_offset = offset + 4\n                        buf_len = from_bytes_big(buf[offset:buf_offset])\n                        offset = buf_offset + buf_len\n                        all_buffers.append(buf[buf_offset:offset])\n                    obj = pickle.loads(all_buffers[0], buffers=all_buffers[1:])\n            if overflow:\n                obj = MessageQueue.recv(self.local_socket, timeout)\n        elif self._is_remote_reader:\n            obj = MessageQueue.recv(self.remote_socket, timeout)\n        else:\n            raise RuntimeError(\"Only readers can dequeue\")\n        return obj\n\n    @staticmethod\n    def recv(socket: zmq.Socket, timeout: float | None) -> Any:\n        timeout_ms = None if timeout is None else int(timeout * 1000)\n        if not socket.poll(timeout=timeout_ms):\n            raise TimeoutError\n        recv, *recv_oob = socket.recv_multipart(copy=False)\n        return pickle.loads(recv, buffers=recv_oob)\n\n    def broadcast_object(self, obj=None):\n        if self._is_writer:\n            self.enqueue(obj)\n            return obj\n        return self.dequeue()\n\n    @staticmethod\n    def create_from_process_group_single_reader(\n        pg: ProcessGroup,\n        max_chunk_bytes,\n        max_chunks,\n        reader_rank: int = 0,\n        blocking: bool = False,\n    ) -> tuple[\"MessageQueue\", list[Handle]]:\n        \"\"\"\n        Creates a MessageQueue for a process group with a single reader.\n\n        This method is designed for scenarios where only one process (the reader)\n        will consume messages, and all other processes are writers. It sets up\n        the shared memory buffer and communication handles accordingly, and\n        gathers the handles from all processes to the reader.\n\n        Args:\n            pg (ProcessGroup): The torch distributed process group.\n            max_chunk_bytes (int): Maximum size in bytes for each chunk in the buffer.\n            max_chunks (int): Maximum number of chunks in the buffer.\n            reader_rank (int, optional): The global rank that will act as the reader.\n                Defaults to 0.\n            blocking (bool, optional): If True, blocks until all processes are ready.\n                Defaults to False.\n\n        Returns:\n            tuple[MessageQueue, list[Handle]]:\n            The MessageQueue instance for the calling process,\n            and a list of handles (only non-empty for the reader process).\n        \"\"\"\n        local_size = current_platform.device_count()\n        rank = dist.get_rank()\n        same_node = rank // local_size == reader_rank // local_size\n        buffer_io = MessageQueue(\n            n_reader=1,\n            n_local_reader=1 if same_node else 0,\n            max_chunk_bytes=max_chunk_bytes,\n            max_chunks=max_chunks,\n        )\n        handle = buffer_io.export_handle()\n        handles = [None] * dist.get_world_size(pg) if rank == reader_rank else None\n        dist.gather_object(handle, handles, dst=reader_rank, group=pg)\n        if blocking:\n            buffer_io.wait_until_ready()\n        return buffer_io, cast(list[Handle], handles or [])\n\n    @staticmethod\n    def create_from_process_group(\n        pg: ProcessGroup | StatelessProcessGroup,\n        max_chunk_bytes,\n        max_chunks,\n        writer_rank: int = 0,\n        external_writer_handle=None,\n        blocking: bool = True,\n    ) -> \"MessageQueue\":\n        \"\"\"\n        Creates a MessageQueue for a distributed process group with one writer and\n        multiple readers.\n\n        This method is designed for scenarios where one process (the writer) sends\n        messages, and all other processes (the readers) receive messages. It sets up\n        the shared memory buffer and socket communication handles accordingly, and\n        broadcasts the handle from the writer to all readers.\n\n        Args:\n            pg (ProcessGroup | StatelessProcessGroup): The torch distributed process\n                group.\n            max_chunk_bytes (int): Maximum size in bytes for each chunk in the buffer.\n            max_chunks (int): Maximum number of chunks in the buffer.\n            writer_rank (int, optional): The global rank that will act as the writer.\n                Defaults to 0.\n            external_writer_handle (Handle, optional): Used when there is a handle\n                from an external Message Queue. If provided, use this handle to init\n                PG writer message queue instead of creating a new one. Defaults to None.\n            blocking (bool, optional): If True, blocks until all processes are ready.\n                Defaults to True.\n\n        Returns:\n            MessageQueue: The MessageQueue instance for the calling process.\n\n        \"\"\"\n        if isinstance(pg, ProcessGroup):\n            group_rank = dist.get_rank(pg)\n            group_world_size = dist.get_world_size(pg)\n            global_ranks = dist.get_process_group_ranks(pg)\n        else:\n            group_rank = pg.rank\n            group_world_size = pg.world_size\n            global_ranks = list(range(pg.world_size))\n        from vllm.distributed.parallel_state import in_the_same_node_as\n\n        status = in_the_same_node_as(pg, source_rank=writer_rank)\n        if group_rank == writer_rank:\n            if external_writer_handle is not None:\n                buffer_io = MessageQueue.create_from_handle(\n                    external_writer_handle, group_rank\n                )\n            else:\n                same_node_ranks = [i for i, s in enumerate(status) if s]\n                n_reader = group_world_size - 1\n                n_local_reader = len(same_node_ranks) - 1\n                local_reader_ranks = [i for i in same_node_ranks if i != writer_rank]\n                buffer_io = MessageQueue(\n                    n_reader=n_reader,\n                    n_local_reader=n_local_reader,\n                    local_reader_ranks=local_reader_ranks,\n                    max_chunk_bytes=max_chunk_bytes,\n                    max_chunks=max_chunks,\n                )\n            handle = buffer_io.export_handle()\n            if isinstance(pg, ProcessGroup):\n                dist.broadcast_object_list(\n                    [handle], src=global_ranks[writer_rank], group=pg\n                )\n            else:\n                pg.broadcast_obj(handle, writer_rank)\n        else:\n            if isinstance(pg, ProcessGroup):\n                recv = [None]\n                dist.broadcast_object_list(\n                    recv, src=global_ranks[writer_rank], group=pg\n                )\n                handle = recv[0]  # type: ignore\n            else:\n                handle = pg.broadcast_obj(None, writer_rank)\n            buffer_io = MessageQueue.create_from_handle(handle, group_rank)\n        if blocking:\n            buffer_io.wait_until_ready()\n        return buffer_io",
      "language": "python"
    },
    {
      "code": "class MessageQueue:\n    def __init__(\n        self,\n        n_reader,  # number of all readers\n        n_local_reader,  # number of local readers through shared memory\n        local_reader_ranks: list[int] | None = None,\n        # Default of 24MiB chosen to be large enough to accommodate grammar\n        # bitmask tensors for large batches (1024 requests).\n        max_chunk_bytes: int = 1024 * 1024 * 24,\n        max_chunks: int = 10,\n        connect_ip: str | None = None,\n    ):\n        if local_reader_ranks is None:\n            local_reader_ranks = list(range(n_local_reader))\n        else:\n            assert len(local_reader_ranks) == n_local_reader\n        self.n_local_reader = n_local_reader\n        n_remote_reader = n_reader - n_local_reader\n        self.n_remote_reader = n_remote_reader\n\n        context = Context()\n\n        if n_local_reader > 0:\n            # for local readers, we will:\n            # 1. create a shared memory ring buffer to communicate small data\n            # 2. create a publish-subscribe socket to communicate large data\n            self.buffer = ShmRingBuffer(n_local_reader, max_chunk_bytes, max_chunks)\n\n            # XPUB is very similar to PUB,\n            # except that it can receive subscription messages\n            # to confirm the number of subscribers\n            self.local_socket = context.socket(XPUB)\n            # set the verbose option so that we can receive every subscription\n            # message. otherwise, we will only receive the first subscription\n            # see http://api.zeromq.org/3-3:zmq-setsockopt for more details\n            self.local_socket.setsockopt(XPUB_VERBOSE, True)\n            local_subscribe_addr = get_open_zmq_ipc_path()\n            logger.debug(\"Binding to %s\", local_subscribe_addr)\n            self.local_socket.bind(local_subscribe_addr)\n\n            self.current_idx = 0\n        else:\n            self.buffer = None  # type: ignore\n            local_subscribe_addr = None\n            self.local_socket = None\n            self.current_idx = -1\n\n        remote_addr_ipv6 = False\n        if n_remote_reader > 0:\n            # for remote readers, we will:\n            # create a publish-subscribe socket to communicate large data\n            if not connect_ip:\n                connect_ip = get_ip()\n            self.remote_socket = context.socket(XPUB)\n            self.remote_socket.setsockopt(XPUB_VERBOSE, True)\n            remote_subscribe_port = get_open_port()\n            if is_valid_ipv6_address(connect_ip):\n                self.remote_socket.setsockopt(IPV6, 1)\n                remote_addr_ipv6 = True\n                connect_ip = f\"[{connect_ip}]\"\n            socket_addr = f\"tcp://{connect_ip}:{remote_subscribe_port}\"\n            self.remote_socket.bind(socket_addr)\n            remote_subscribe_addr = f\"tcp://{connect_ip}:{remote_subscribe_port}\"\n        else:\n            remote_subscribe_addr = None\n            self.remote_socket = None\n\n        self._is_writer = True\n        self._is_local_reader = False\n        self.local_reader_rank = -1\n        # rank does not matter for remote readers\n        self._is_remote_reader = False\n        self._read_spin_timer = SpinTimer()\n\n        self.handle = Handle(\n            local_reader_ranks=local_reader_ranks,\n            buffer_handle=self.buffer.handle() if self.buffer is not None else None,\n            local_subscribe_addr=local_subscribe_addr,\n            remote_subscribe_addr=remote_subscribe_addr,\n            remote_addr_ipv6=remote_addr_ipv6,\n        )\n\n        logger.debug(\"vLLM message queue communication handle: %s\", self.handle)\n\n    def export_handle(self) -> Handle:\n        return self.handle\n\n    @staticmethod\n    def create_from_handle(handle: Handle, rank) -> \"MessageQueue\":\n        self = MessageQueue.__new__(MessageQueue)\n        self.handle = handle\n        self._is_writer = False\n\n        context = Context()\n\n        if rank in handle.local_reader_ranks:\n            assert handle.buffer_handle is not None\n            self.buffer = ShmRingBuffer(*handle.buffer_handle)\n            self.current_idx = 0\n            self.local_reader_rank = handle.local_reader_ranks.index(rank)\n            self._is_local_reader = True\n            self._is_remote_reader = False\n\n            self.local_socket = context.socket(SUB)\n            self.local_socket.setsockopt_string(SUBSCRIBE, \"\")\n            socket_addr = handle.local_subscribe_addr\n            logger.debug(\"Connecting to %s\", socket_addr)\n            self.local_socket.connect(socket_addr)\n\n            self.remote_socket = None\n\n            self._read_spin_timer = (\n                SpinSleepTimer() if envs.VLLM_SLEEP_WHEN_IDLE else SpinTimer()\n            )\n        else:\n            self.buffer = None  # type: ignore\n            self.current_idx = -1\n            self.local_reader_rank = -1\n            self._is_local_reader = False\n            self._is_remote_reader = True\n\n            self.local_socket = None\n\n            self.remote_socket = context.socket(SUB)\n            self.remote_socket.setsockopt_string(SUBSCRIBE, \"\")\n            if handle.remote_addr_ipv6:\n                self.remote_socket.setsockopt(IPV6, 1)\n            socket_addr = handle.remote_subscribe_addr\n            logger.debug(\"Connecting to %s\", socket_addr)\n            self.remote_socket.connect(socket_addr)\n\n        return self\n\n    def wait_until_ready(self):\n        \"\"\"This is a collective operation. All processes (including the\n        readers and the writer) should call this function.\n        \"\"\"\n        if self._is_writer:\n            # wait for all readers to connect\n\n            # local readers\n            for i in range(self.n_local_reader):\n                # wait for subscription messages from all local readers\n                self.local_socket.recv()\n            if self.n_local_reader > 0:\n                # send a message to all local readers\n                # to make sure the publish channel is working\n                self.local_socket.send(b\"READY\")\n\n            # remote readers\n            for i in range(self.n_remote_reader):\n                # wait for subscription messages from all remote readers\n                self.remote_socket.recv()\n            if self.n_remote_reader > 0:\n                # send a message to all remote readers\n                # to make sure the publish channel is working\n                self.remote_socket.send(b\"READY\")\n        elif self._is_local_reader:\n            # wait for the writer to send a message\n            recv = self.local_socket.recv()\n            assert recv == b\"READY\"\n        elif self._is_remote_reader:\n            # wait for the writer to send a message\n            recv = self.remote_socket.recv()\n            assert recv == b\"READY\"\n\n    @contextmanager\n    def acquire_write(self, timeout: float | None = None):\n        assert self._is_writer, \"Only writers can acquire write\"\n        start_time = time.monotonic()\n        n_warning = 1\n        while True:\n            with self.buffer.get_metadata(self.current_idx) as metadata_buffer:\n                # Memory fence ensures we see the latest read flags from readers.\n                # Without this, we may read stale flags from our CPU cache and\n                # spin indefinitely even though readers have completed.\n                memory_fence()\n                read_count = sum(metadata_buffer[1:])\n                written_flag = metadata_buffer[0]\n                if written_flag and read_count != self.buffer.n_reader:\n                    # this block is written and not read by all readers\n                    # for writers, `self.current_idx` is the next block to write\n                    # if this block is not ready to write,\n                    # we need to wait until it is read by all readers\n\n                    # Release the processor to other threads\n                    sched_yield()\n\n                    # if we time out, raise an exception\n                    elapsed = time.monotonic() - start_time\n                    if timeout is not None and elapsed > timeout:\n                        raise TimeoutError\n\n                    # if we wait for a long time, log a message\n                    if elapsed > VLLM_RINGBUFFER_WARNING_INTERVAL * n_warning:\n                        logger.info(\n                            long_wait_time_msg(VLLM_RINGBUFFER_WARNING_INTERVAL)\n                        )\n                        n_warning += 1\n\n                    continue\n                # found a block that is either\n                # (1) not written\n                # (2) read by all readers\n\n                # mark the block as not written\n                metadata_buffer[0] = 0\n                # let caller write to the buffer\n                with self.buffer.get_data(self.current_idx) as buf:\n                    yield buf\n\n                # caller has written to the buffer\n                # NOTE: order is important here\n                # first set the read flags to 0\n                # then set the written flag to 1\n                # otherwise, the readers may think they already read the block\n                for i in range(1, self.buffer.n_reader + 1):\n                    # set read flag to 0, meaning it is not read yet\n                    metadata_buffer[i] = 0\n                # mark the block as written\n                metadata_buffer[0] = 1\n                # Memory fence ensures the write is visible to readers on other cores\n                # before we proceed. Without this, readers may spin indefinitely\n                # waiting for a write that's stuck in our CPU's store buffer.\n                memory_fence()\n                self.current_idx = (self.current_idx + 1) % self.buffer.max_chunks\n                break\n\n    @contextmanager\n    def acquire_read(\n        self,\n        timeout: float | None = None,\n        cancel: Event | None = None,\n        indefinite: bool = False,\n    ):\n        assert self._is_local_reader, \"Only readers can acquire read\"\n        start_time = time.monotonic()\n        n_warning = 1\n        while True:\n            with self.buffer.get_metadata(self.current_idx) as metadata_buffer:\n                # Memory fence ensures we see the latest writes from the writer.\n                # Without this, we may read stale flags from our CPU cache\n                # and spin indefinitely even though writer has updated them.\n                memory_fence()\n                read_flag = metadata_buffer[self.local_reader_rank + 1]\n                written_flag = metadata_buffer[0]\n                if not written_flag or read_flag:\n                    # this block is either\n                    # (1) not written\n                    # (2) already read by this reader\n\n                    # for readers, `self.current_idx` is the next block to read\n                    # if this block is not ready,\n                    # we need to wait until it is written\n\n                    # Release the processor to other threads\n                    self._read_spin_timer.spin()\n\n                    if cancel is not None and cancel.is_set():\n                        raise RuntimeError(\"cancelled\")\n\n                    # if we time out, raise an exception\n                    elapsed = time.monotonic() - start_time\n                    if timeout is not None and elapsed > timeout:\n                        raise TimeoutError\n\n                    # if we wait for a long time, log a message\n                    if not indefinite and (\n                        elapsed > VLLM_RINGBUFFER_WARNING_INTERVAL * n_warning\n                    ):\n                        logger.info(\n                            long_wait_time_msg(VLLM_RINGBUFFER_WARNING_INTERVAL)\n                        )\n                        n_warning += 1\n\n                    continue\n                # found a block that is not read by this reader\n                # let caller read from the buffer\n                with self.buffer.get_data(self.current_idx) as buf:\n                    yield buf\n\n                # caller has read from the buffer\n                # set the read flag\n                metadata_buffer[self.local_reader_rank + 1] = 1\n                # Memory fence ensures the read flag is visible to the writer.\n                # Without this, writer may not see our read completion and\n                # could wait indefinitely for all readers to finish.\n                memory_fence()\n                self.current_idx = (self.current_idx + 1) % self.buffer.max_chunks\n\n                self._read_spin_timer.record_activity()\n                break\n\n    def enqueue(self, obj, timeout: float | None = None):\n        \"\"\"Write to message queue with optional timeout (in seconds)\"\"\"\n        assert self._is_writer, \"Only writers can enqueue\"\n        all_buffers: list[SizedBuffer] = [b\"\"]\n        total_bytes = 6  # 2 bytes for oob buffer count, 4 for main buffer size\n\n        def oob_callback(buf: PickleBuffer) -> bool:\n            raw_buf = buf.raw()\n            if len(raw_buf) < 1024 * 1024:\n                # In-line buffers smaller than 1MiB.\n                return True\n            all_buffers.append(raw_buf)\n            nonlocal total_bytes\n            total_bytes += len(raw_buf) + 4\n            return False\n\n        all_buffers[0] = pickle.dumps(\n            obj, protocol=pickle.HIGHEST_PROTOCOL, buffer_callback=oob_callback\n        )\n        if self.n_local_reader > 0:\n            if total_bytes + len(all_buffers[0]) >= self.buffer.max_chunk_bytes:\n                with self.acquire_write(timeout) as buf:\n                    buf[0] = 1  # overflow\n                self.local_socket.send_multipart(all_buffers, copy=False)\n            else:\n                # Byte 0: 0\n                # Bytes 1-2: Count of buffers\n                # Then each buffer follows, preceded by 4 bytes containing its length:\n                # [4 byte int L][L bytes of buffer content] ...\n                with self.acquire_write(timeout) as buf:\n                    buf[0] = 0  # not overflow\n                    offset = 3\n                    buf[1:offset] = to_bytes_big(len(all_buffers), 2)  # oob buf count\n                    for buffer in all_buffers:\n                        buf_len = len(buffer)\n                        # prepend each buffer with 4 bytes containing its size.\n                        buf_offset = offset + 4\n                        buf[offset:buf_offset] = to_bytes_big(buf_len, 4)\n                        buf[buf_offset : (offset := buf_offset + buf_len)] = buffer\n\n        if self.n_remote_reader > 0:\n            self.remote_socket.send_multipart(all_buffers, copy=False)\n\n    def dequeue(\n        self,\n        timeout: float | None = None,\n        cancel: Event | None = None,\n        indefinite: bool = False,\n    ):\n        \"\"\"Read from message queue with optional timeout (in seconds)\"\"\"\n        if self._is_local_reader:\n            with self.acquire_read(timeout, cancel, indefinite) as buf:\n                overflow = buf[0] == 1\n                if not overflow:\n                    offset = 3\n                    buf_count = from_bytes_big(buf[1:offset])\n                    all_buffers = []\n                    for i in range(buf_count):\n                        buf_offset = offset + 4\n                        buf_len = from_bytes_big(buf[offset:buf_offset])\n                        offset = buf_offset + buf_len\n                        all_buffers.append(buf[buf_offset:offset])\n                    obj = pickle.loads(all_buffers[0], buffers=all_buffers[1:])\n            if overflow:\n                obj = MessageQueue.recv(self.local_socket, timeout)\n        elif self._is_remote_reader:\n            obj = MessageQueue.recv(self.remote_socket, timeout)\n        else:\n            raise RuntimeError(\"Only readers can dequeue\")\n        return obj\n\n    @staticmethod\n    def recv(socket: zmq.Socket, timeout: float | None) -> Any:\n        timeout_ms = None if timeout is None else int(timeout * 1000)\n        if not socket.poll(timeout=timeout_ms):\n            raise TimeoutError\n        recv, *recv_oob = socket.recv_multipart(copy=False)\n        return pickle.loads(recv, buffers=recv_oob)\n\n    def broadcast_object(self, obj=None):\n        if self._is_writer:\n            self.enqueue(obj)\n            return obj\n        return self.dequeue()\n\n    @staticmethod\n    def create_from_process_group_single_reader(\n        pg: ProcessGroup,\n        max_chunk_bytes,\n        max_chunks,\n        reader_rank: int = 0,\n        blocking: bool = False,\n    ) -> tuple[\"MessageQueue\", list[Handle]]:\n        \"\"\"\n        Creates a MessageQueue for a process group with a single reader.\n\n        This method is designed for scenarios where only one process (the reader)\n        will consume messages, and all other processes are writers. It sets up\n        the shared memory buffer and communication handles accordingly, and\n        gathers the handles from all processes to the reader.\n\n        Args:\n            pg (ProcessGroup): The torch distributed process group.\n            max_chunk_bytes (int): Maximum size in bytes for each chunk in the buffer.\n            max_chunks (int): Maximum number of chunks in the buffer.\n            reader_rank (int, optional): The global rank that will act as the reader.\n                Defaults to 0.\n            blocking (bool, optional): If True, blocks until all processes are ready.\n                Defaults to False.\n\n        Returns:\n            tuple[MessageQueue, list[Handle]]:\n            The MessageQueue instance for the calling process,\n            and a list of handles (only non-empty for the reader process).\n        \"\"\"\n        local_size = current_platform.device_count()\n        rank = dist.get_rank()\n        same_node = rank // local_size == reader_rank // local_size\n        buffer_io = MessageQueue(\n            n_reader=1,\n            n_local_reader=1 if same_node else 0,\n            max_chunk_bytes=max_chunk_bytes,\n            max_chunks=max_chunks,\n        )\n        handle = buffer_io.export_handle()\n        handles = [None] * dist.get_world_size(pg) if rank == reader_rank else None\n        dist.gather_object(handle, handles, dst=reader_rank, group=pg)\n        if blocking:\n            buffer_io.wait_until_ready()\n        return buffer_io, cast(list[Handle], handles or [])\n\n    @staticmethod\n    def create_from_process_group(\n        pg: ProcessGroup | StatelessProcessGroup,\n        max_chunk_bytes,\n        max_chunks,\n        writer_rank: int = 0,\n        external_writer_handle=None,\n        blocking: bool = True,\n    ) -> \"MessageQueue\":\n        \"\"\"\n        Creates a MessageQueue for a distributed process group with one writer and\n        multiple readers.\n\n        This method is designed for scenarios where one process (the writer) sends\n        messages, and all other processes (the readers) receive messages. It sets up\n        the shared memory buffer and socket communication handles accordingly, and\n        broadcasts the handle from the writer to all readers.\n\n        Args:\n            pg (ProcessGroup | StatelessProcessGroup): The torch distributed process\n                group.\n            max_chunk_bytes (int): Maximum size in bytes for each chunk in the buffer.\n            max_chunks (int): Maximum number of chunks in the buffer.\n            writer_rank (int, optional): The global rank that will act as the writer.\n                Defaults to 0.\n            external_writer_handle (Handle, optional): Used when there is a handle\n                from an external Message Queue. If provided, use this handle to init\n                PG writer message queue instead of creating a new one. Defaults to None.\n            blocking (bool, optional): If True, blocks until all processes are ready.\n                Defaults to True.\n\n        Returns:\n            MessageQueue: The MessageQueue instance for the calling process.\n\n        \"\"\"\n        if isinstance(pg, ProcessGroup):\n            group_rank = dist.get_rank(pg)\n            group_world_size = dist.get_world_size(pg)\n            global_ranks = dist.get_process_group_ranks(pg)\n        else:\n            group_rank = pg.rank\n            group_world_size = pg.world_size\n            global_ranks = list(range(pg.world_size))\n        from vllm.distributed.parallel_state import in_the_same_node_as\n\n        status = in_the_same_node_as(pg, source_rank=writer_rank)\n        if group_rank == writer_rank:\n            if external_writer_handle is not None:\n                buffer_io = MessageQueue.create_from_handle(\n                    external_writer_handle, group_rank\n                )\n            else:\n                same_node_ranks = [i for i, s in enumerate(status) if s]\n                n_reader = group_world_size - 1\n                n_local_reader = len(same_node_ranks) - 1\n                local_reader_ranks = [i for i in same_node_ranks if i != writer_rank]\n                buffer_io = MessageQueue(\n                    n_reader=n_reader,\n                    n_local_reader=n_local_reader,\n                    local_reader_ranks=local_reader_ranks,\n                    max_chunk_bytes=max_chunk_bytes,\n                    max_chunks=max_chunks,\n                )\n            handle = buffer_io.export_handle()\n            if isinstance(pg, ProcessGroup):\n                dist.broadcast_object_list(\n                    [handle], src=global_ranks[writer_rank], group=pg\n                )\n            else:\n                pg.broadcast_obj(handle, writer_rank)\n        else:\n            if isinstance(pg, ProcessGroup):\n                recv = [None]\n                dist.broadcast_object_list(\n                    recv, src=global_ranks[writer_rank], group=pg\n                )\n                handle = recv[0]  # type: ignore\n            else:\n                handle = pg.broadcast_obj(None, writer_rank)\n            buffer_io = MessageQueue.create_from_handle(handle, group_rank)\n        if blocking:\n            buffer_io.wait_until_ready()\n        return buffer_io",
      "language": "python"
    },
    {
      "code": "_is_local_reader = False",
      "language": "unknown"
    },
    {
      "code": "_is_local_reader = False",
      "language": "unknown"
    },
    {
      "code": "_is_remote_reader = False",
      "language": "unknown"
    },
    {
      "code": "_is_remote_reader = False",
      "language": "unknown"
    },
    {
      "code": "_is_writer = True",
      "language": "unknown"
    },
    {
      "code": "_is_writer = True",
      "language": "unknown"
    },
    {
      "code": "_read_spin_timer = SpinTimer()",
      "language": "unknown"
    },
    {
      "code": "_read_spin_timer = SpinTimer()",
      "language": "unknown"
    },
    {
      "code": "buffer = ShmRingBuffer(\n    n_local_reader, max_chunk_bytes, max_chunks\n)",
      "language": "unknown"
    },
    {
      "code": "buffer = ShmRingBuffer(\n    n_local_reader, max_chunk_bytes, max_chunks\n)",
      "language": "unknown"
    },
    {
      "code": "current_idx = 0",
      "language": "unknown"
    },
    {
      "code": "current_idx = 0",
      "language": "unknown"
    },
    {
      "code": "handle = Handle(\n    local_reader_ranks=local_reader_ranks,\n    buffer_handle=handle() if buffer is not None else None,\n    local_subscribe_addr=local_subscribe_addr,\n    remote_subscribe_addr=remote_subscribe_addr,\n    remote_addr_ipv6=remote_addr_ipv6,\n)",
      "language": "rust"
    },
    {
      "code": "handle = Handle(\n    local_reader_ranks=local_reader_ranks,\n    buffer_handle=handle() if buffer is not None else None,\n    local_subscribe_addr=local_subscribe_addr,\n    remote_subscribe_addr=remote_subscribe_addr,\n    remote_addr_ipv6=remote_addr_ipv6,\n)",
      "language": "rust"
    },
    {
      "code": "local_reader_rank = -1",
      "language": "unknown"
    },
    {
      "code": "local_reader_rank = -1",
      "language": "unknown"
    },
    {
      "code": "local_socket = socket(XPUB)",
      "language": "unknown"
    },
    {
      "code": "local_socket = socket(XPUB)",
      "language": "unknown"
    },
    {
      "code": "n_local_reader = n_local_reader",
      "language": "unknown"
    },
    {
      "code": "n_local_reader = n_local_reader",
      "language": "unknown"
    },
    {
      "code": "n_remote_reader = n_remote_reader",
      "language": "unknown"
    },
    {
      "code": "n_remote_reader = n_remote_reader",
      "language": "unknown"
    },
    {
      "code": "remote_socket = socket(XPUB)",
      "language": "unknown"
    },
    {
      "code": "remote_socket = socket(XPUB)",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    n_reader,\n    n_local_reader,\n    local_reader_ranks: list[int] | None = None,\n    max_chunk_bytes: int = 1024 * 1024 * 24,\n    max_chunks: int = 10,\n    connect_ip: str | None = None,\n)",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    n_reader,\n    n_local_reader,\n    local_reader_ranks: list[int] | None = None,\n    max_chunk_bytes: int = 1024 * 1024 * 24,\n    max_chunks: int = 10,\n    connect_ip: str | None = None,\n)",
      "language": "typescript"
    },
    {
      "code": "273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    n_reader,  # number of all readers\n    n_local_reader,  # number of local readers through shared memory\n    local_reader_ranks: list[int] | None = None,\n    # Default of 24MiB chosen to be large enough to accommodate grammar\n    # bitmask tensors for large batches (1024 requests).\n    max_chunk_bytes: int = 1024 * 1024 * 24,\n    max_chunks: int = 10,\n    connect_ip: str | None = None,\n):\n    if local_reader_ranks is None:\n        local_reader_ranks = list(range(n_local_reader))\n    else:\n        assert len(local_reader_ranks) == n_local_reader\n    self.n_local_reader = n_local_reader\n    n_remote_reader = n_reader - n_local_reader\n    self.n_remote_reader = n_remote_reader\n\n    context = Context()\n\n    if n_local_reader > 0:\n        # for local readers, we will:\n        # 1. create a shared memory ring buffer to communicate small data\n        # 2. create a publish-subscribe socket to communicate large data\n        self.buffer = ShmRingBuffer(n_local_reader, max_chunk_bytes, max_chunks)\n\n        # XPUB is very similar to PUB,\n        # except that it can receive subscription messages\n        # to confirm the number of subscribers\n        self.local_socket = context.socket(XPUB)\n        # set the verbose option so that we can receive every subscription\n        # message. otherwise, we will only receive the first subscription\n        # see http://api.zeromq.org/3-3:zmq-setsockopt for more details\n        self.local_socket.setsockopt(XPUB_VERBOSE, True)\n        local_subscribe_addr = get_open_zmq_ipc_path()\n        logger.debug(\"Binding to %s\", local_subscribe_addr)\n        self.local_socket.bind(local_subscribe_addr)\n\n        self.current_idx = 0\n    else:\n        self.buffer = None  # type: ignore\n        local_subscribe_addr = None\n        self.local_socket = None\n        self.current_idx = -1\n\n    remote_addr_ipv6 = False\n    if n_remote_reader > 0:\n        # for remote readers, we will:\n        # create a publish-subscribe socket to communicate large data\n        if not connect_ip:\n            connect_ip = get_ip()\n        self.remote_socket = context.socket(XPUB)\n        self.remote_socket.setsockopt(XPUB_VERBOSE, True)\n        remote_subscribe_port = get_open_port()\n        if is_valid_ipv6_address(connect_ip):\n            self.remote_socket.setsockopt(IPV6, 1)\n            remote_addr_ipv6 = True\n            connect_ip = f\"[{connect_ip}]\"\n        socket_addr = f\"tcp://{connect_ip}:{remote_subscribe_port}\"\n        self.remote_socket.bind(socket_addr)\n        remote_subscribe_addr = f\"tcp://{connect_ip}:{remote_subscribe_port}\"\n    else:\n        remote_subscribe_addr = None\n        self.remote_socket = None\n\n    self._is_writer = True\n    self._is_local_reader = False\n    self.local_reader_rank = -1\n    # rank does not matter for remote readers\n    self._is_remote_reader = False\n    self._read_spin_timer = SpinTimer()\n\n    self.handle = Handle(\n        local_reader_ranks=local_reader_ranks,\n        buffer_handle=self.buffer.handle() if self.buffer is not None else None,\n        local_subscribe_addr=local_subscribe_addr,\n        remote_subscribe_addr=remote_subscribe_addr,\n        remote_addr_ipv6=remote_addr_ipv6,\n    )\n\n    logger.debug(\"vLLM message queue communication handle: %s\", self.handle)",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    n_reader,  # number of all readers\n    n_local_reader,  # number of local readers through shared memory\n    local_reader_ranks: list[int] | None = None,\n    # Default of 24MiB chosen to be large enough to accommodate grammar\n    # bitmask tensors for large batches (1024 requests).\n    max_chunk_bytes: int = 1024 * 1024 * 24,\n    max_chunks: int = 10,\n    connect_ip: str | None = None,\n):\n    if local_reader_ranks is None:\n        local_reader_ranks = list(range(n_local_reader))\n    else:\n        assert len(local_reader_ranks) == n_local_reader\n    self.n_local_reader = n_local_reader\n    n_remote_reader = n_reader - n_local_reader\n    self.n_remote_reader = n_remote_reader\n\n    context = Context()\n\n    if n_local_reader > 0:\n        # for local readers, we will:\n        # 1. create a shared memory ring buffer to communicate small data\n        # 2. create a publish-subscribe socket to communicate large data\n        self.buffer = ShmRingBuffer(n_local_reader, max_chunk_bytes, max_chunks)\n\n        # XPUB is very similar to PUB,\n        # except that it can receive subscription messages\n        # to confirm the number of subscribers\n        self.local_socket = context.socket(XPUB)\n        # set the verbose option so that we can receive every subscription\n        # message. otherwise, we will only receive the first subscription\n        # see http://api.zeromq.org/3-3:zmq-setsockopt for more details\n        self.local_socket.setsockopt(XPUB_VERBOSE, True)\n        local_subscribe_addr = get_open_zmq_ipc_path()\n        logger.debug(\"Binding to %s\", local_subscribe_addr)\n        self.local_socket.bind(local_subscribe_addr)\n\n        self.current_idx = 0\n    else:\n        self.buffer = None  # type: ignore\n        local_subscribe_addr = None\n        self.local_socket = None\n        self.current_idx = -1\n\n    remote_addr_ipv6 = False\n    if n_remote_reader > 0:\n        # for remote readers, we will:\n        # create a publish-subscribe socket to communicate large data\n        if not connect_ip:\n            connect_ip = get_ip()\n        self.remote_socket = context.socket(XPUB)\n        self.remote_socket.setsockopt(XPUB_VERBOSE, True)\n        remote_subscribe_port = get_open_port()\n        if is_valid_ipv6_address(connect_ip):\n            self.remote_socket.setsockopt(IPV6, 1)\n            remote_addr_ipv6 = True\n            connect_ip = f\"[{connect_ip}]\"\n        socket_addr = f\"tcp://{connect_ip}:{remote_subscribe_port}\"\n        self.remote_socket.bind(socket_addr)\n        remote_subscribe_addr = f\"tcp://{connect_ip}:{remote_subscribe_port}\"\n    else:\n        remote_subscribe_addr = None\n        self.remote_socket = None\n\n    self._is_writer = True\n    self._is_local_reader = False\n    self.local_reader_rank = -1\n    # rank does not matter for remote readers\n    self._is_remote_reader = False\n    self._read_spin_timer = SpinTimer()\n\n    self.handle = Handle(\n        local_reader_ranks=local_reader_ranks,\n        buffer_handle=self.buffer.handle() if self.buffer is not None else None,\n        local_subscribe_addr=local_subscribe_addr,\n        remote_subscribe_addr=remote_subscribe_addr,\n        remote_addr_ipv6=remote_addr_ipv6,\n    )\n\n    logger.debug(\"vLLM message queue communication handle: %s\", self.handle)",
      "language": "python"
    },
    {
      "code": "acquire_read(\n    timeout: float | None = None,\n    cancel: Event | None = None,\n    indefinite: bool = False,\n)",
      "language": "typescript"
    },
    {
      "code": "acquire_read(\n    timeout: float | None = None,\n    cancel: Event | None = None,\n    indefinite: bool = False,\n)",
      "language": "typescript"
    },
    {
      "code": "500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563",
      "language": "unknown"
    },
    {
      "code": "@contextmanager\ndef acquire_read(\n    self,\n    timeout: float | None = None,\n    cancel: Event | None = None,\n    indefinite: bool = False,\n):\n    assert self._is_local_reader, \"Only readers can acquire read\"\n    start_time = time.monotonic()\n    n_warning = 1\n    while True:\n        with self.buffer.get_metadata(self.current_idx) as metadata_buffer:\n            # Memory fence ensures we see the latest writes from the writer.\n            # Without this, we may read stale flags from our CPU cache\n            # and spin indefinitely even though writer has updated them.\n            memory_fence()\n            read_flag = metadata_buffer[self.local_reader_rank + 1]\n            written_flag = metadata_buffer[0]\n            if not written_flag or read_flag:\n                # this block is either\n                # (1) not written\n                # (2) already read by this reader\n\n                # for readers, `self.current_idx` is the next block to read\n                # if this block is not ready,\n                # we need to wait until it is written\n\n                # Release the processor to other threads\n                self._read_spin_timer.spin()\n\n                if cancel is not None and cancel.is_set():\n                    raise RuntimeError(\"cancelled\")\n\n                # if we time out, raise an exception\n                elapsed = time.monotonic() - start_time\n                if timeout is not None and elapsed > timeout:\n                    raise TimeoutError\n\n                # if we wait for a long time, log a message\n                if not indefinite and (\n                    elapsed > VLLM_RINGBUFFER_WARNING_INTERVAL * n_warning\n                ):\n                    logger.info(\n                        long_wait_time_msg(VLLM_RINGBUFFER_WARNING_INTERVAL)\n                    )\n                    n_warning += 1\n\n                continue\n            # found a block that is not read by this reader\n            # let caller read from the buffer\n            with self.buffer.get_data(self.current_idx) as buf:\n                yield buf\n\n            # caller has read from the buffer\n            # set the read flag\n            metadata_buffer[self.local_reader_rank + 1] = 1\n            # Memory fence ensures the read flag is visible to the writer.\n            # Without this, writer may not see our read completion and\n            # could wait indefinitely for all readers to finish.\n            memory_fence()\n            self.current_idx = (self.current_idx + 1) % self.buffer.max_chunks\n\n            self._read_spin_timer.record_activity()\n            break",
      "language": "python"
    },
    {
      "code": "@contextmanager\ndef acquire_read(\n    self,\n    timeout: float | None = None,\n    cancel: Event | None = None,\n    indefinite: bool = False,\n):\n    assert self._is_local_reader, \"Only readers can acquire read\"\n    start_time = time.monotonic()\n    n_warning = 1\n    while True:\n        with self.buffer.get_metadata(self.current_idx) as metadata_buffer:\n            # Memory fence ensures we see the latest writes from the writer.\n            # Without this, we may read stale flags from our CPU cache\n            # and spin indefinitely even though writer has updated them.\n            memory_fence()\n            read_flag = metadata_buffer[self.local_reader_rank + 1]\n            written_flag = metadata_buffer[0]\n            if not written_flag or read_flag:\n                # this block is either\n                # (1) not written\n                # (2) already read by this reader\n\n                # for readers, `self.current_idx` is the next block to read\n                # if this block is not ready,\n                # we need to wait until it is written\n\n                # Release the processor to other threads\n                self._read_spin_timer.spin()\n\n                if cancel is not None and cancel.is_set():\n                    raise RuntimeError(\"cancelled\")\n\n                # if we time out, raise an exception\n                elapsed = time.monotonic() - start_time\n                if timeout is not None and elapsed > timeout:\n                    raise TimeoutError\n\n                # if we wait for a long time, log a message\n                if not indefinite and (\n                    elapsed > VLLM_RINGBUFFER_WARNING_INTERVAL * n_warning\n                ):\n                    logger.info(\n                        long_wait_time_msg(VLLM_RINGBUFFER_WARNING_INTERVAL)\n                    )\n                    n_warning += 1\n\n                continue\n            # found a block that is not read by this reader\n            # let caller read from the buffer\n            with self.buffer.get_data(self.current_idx) as buf:\n                yield buf\n\n            # caller has read from the buffer\n            # set the read flag\n            metadata_buffer[self.local_reader_rank + 1] = 1\n            # Memory fence ensures the read flag is visible to the writer.\n            # Without this, writer may not see our read completion and\n            # could wait indefinitely for all readers to finish.\n            memory_fence()\n            self.current_idx = (self.current_idx + 1) % self.buffer.max_chunks\n\n            self._read_spin_timer.record_activity()\n            break",
      "language": "python"
    },
    {
      "code": "acquire_write(timeout: float | None = None)",
      "language": "rust"
    },
    {
      "code": "acquire_write(timeout: float | None = None)",
      "language": "rust"
    },
    {
      "code": "438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498",
      "language": "unknown"
    },
    {
      "code": "@contextmanager\ndef acquire_write(self, timeout: float | None = None):\n    assert self._is_writer, \"Only writers can acquire write\"\n    start_time = time.monotonic()\n    n_warning = 1\n    while True:\n        with self.buffer.get_metadata(self.current_idx) as metadata_buffer:\n            # Memory fence ensures we see the latest read flags from readers.\n            # Without this, we may read stale flags from our CPU cache and\n            # spin indefinitely even though readers have completed.\n            memory_fence()\n            read_count = sum(metadata_buffer[1:])\n            written_flag = metadata_buffer[0]\n            if written_flag and read_count != self.buffer.n_reader:\n                # this block is written and not read by all readers\n                # for writers, `self.current_idx` is the next block to write\n                # if this block is not ready to write,\n                # we need to wait until it is read by all readers\n\n                # Release the processor to other threads\n                sched_yield()\n\n                # if we time out, raise an exception\n                elapsed = time.monotonic() - start_time\n                if timeout is not None and elapsed > timeout:\n                    raise TimeoutError\n\n                # if we wait for a long time, log a message\n                if elapsed > VLLM_RINGBUFFER_WARNING_INTERVAL * n_warning:\n                    logger.info(\n                        long_wait_time_msg(VLLM_RINGBUFFER_WARNING_INTERVAL)\n                    )\n                    n_warning += 1\n\n                continue\n            # found a block that is either\n            # (1) not written\n            # (2) read by all readers\n\n            # mark the block as not written\n            metadata_buffer[0] = 0\n            # let caller write to the buffer\n            with self.buffer.get_data(self.current_idx) as buf:\n                yield buf\n\n            # caller has written to the buffer\n            # NOTE: order is important here\n            # first set the read flags to 0\n            # then set the written flag to 1\n            # otherwise, the readers may think they already read the block\n            for i in range(1, self.buffer.n_reader + 1):\n                # set read flag to 0, meaning it is not read yet\n                metadata_buffer[i] = 0\n            # mark the block as written\n            metadata_buffer[0] = 1\n            # Memory fence ensures the write is visible to readers on other cores\n            # before we proceed. Without this, readers may spin indefinitely\n            # waiting for a write that's stuck in our CPU's store buffer.\n            memory_fence()\n            self.current_idx = (self.current_idx + 1) % self.buffer.max_chunks\n            break",
      "language": "python"
    },
    {
      "code": "@contextmanager\ndef acquire_write(self, timeout: float | None = None):\n    assert self._is_writer, \"Only writers can acquire write\"\n    start_time = time.monotonic()\n    n_warning = 1\n    while True:\n        with self.buffer.get_metadata(self.current_idx) as metadata_buffer:\n            # Memory fence ensures we see the latest read flags from readers.\n            # Without this, we may read stale flags from our CPU cache and\n            # spin indefinitely even though readers have completed.\n            memory_fence()\n            read_count = sum(metadata_buffer[1:])\n            written_flag = metadata_buffer[0]\n            if written_flag and read_count != self.buffer.n_reader:\n                # this block is written and not read by all readers\n                # for writers, `self.current_idx` is the next block to write\n                # if this block is not ready to write,\n                # we need to wait until it is read by all readers\n\n                # Release the processor to other threads\n                sched_yield()\n\n                # if we time out, raise an exception\n                elapsed = time.monotonic() - start_time\n                if timeout is not None and elapsed > timeout:\n                    raise TimeoutError\n\n                # if we wait for a long time, log a message\n                if elapsed > VLLM_RINGBUFFER_WARNING_INTERVAL * n_warning:\n                    logger.info(\n                        long_wait_time_msg(VLLM_RINGBUFFER_WARNING_INTERVAL)\n                    )\n                    n_warning += 1\n\n                continue\n            # found a block that is either\n            # (1) not written\n            # (2) read by all readers\n\n            # mark the block as not written\n            metadata_buffer[0] = 0\n            # let caller write to the buffer\n            with self.buffer.get_data(self.current_idx) as buf:\n                yield buf\n\n            # caller has written to the buffer\n            # NOTE: order is important here\n            # first set the read flags to 0\n            # then set the written flag to 1\n            # otherwise, the readers may think they already read the block\n            for i in range(1, self.buffer.n_reader + 1):\n                # set read flag to 0, meaning it is not read yet\n                metadata_buffer[i] = 0\n            # mark the block as written\n            metadata_buffer[0] = 1\n            # Memory fence ensures the write is visible to readers on other cores\n            # before we proceed. Without this, readers may spin indefinitely\n            # waiting for a write that's stuck in our CPU's store buffer.\n            memory_fence()\n            self.current_idx = (self.current_idx + 1) % self.buffer.max_chunks\n            break",
      "language": "python"
    },
    {
      "code": "broadcast_object(obj=None)",
      "language": "rust"
    },
    {
      "code": "broadcast_object(obj=None)",
      "language": "rust"
    },
    {
      "code": "644\n645\n646\n647\n648",
      "language": "unknown"
    },
    {
      "code": "def broadcast_object(self, obj=None):\n    if self._is_writer:\n        self.enqueue(obj)\n        return obj\n    return self.dequeue()",
      "language": "python"
    },
    {
      "code": "def broadcast_object(self, obj=None):\n    if self._is_writer:\n        self.enqueue(obj)\n        return obj\n    return self.dequeue()",
      "language": "python"
    },
    {
      "code": "create_from_handle(handle: Handle, rank) -> MessageQueue",
      "language": "php"
    },
    {
      "code": "create_from_handle(handle: Handle, rank) -> MessageQueue",
      "language": "php"
    },
    {
      "code": "359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef create_from_handle(handle: Handle, rank) -> \"MessageQueue\":\n    self = MessageQueue.__new__(MessageQueue)\n    self.handle = handle\n    self._is_writer = False\n\n    context = Context()\n\n    if rank in handle.local_reader_ranks:\n        assert handle.buffer_handle is not None\n        self.buffer = ShmRingBuffer(*handle.buffer_handle)\n        self.current_idx = 0\n        self.local_reader_rank = handle.local_reader_ranks.index(rank)\n        self._is_local_reader = True\n        self._is_remote_reader = False\n\n        self.local_socket = context.socket(SUB)\n        self.local_socket.setsockopt_string(SUBSCRIBE, \"\")\n        socket_addr = handle.local_subscribe_addr\n        logger.debug(\"Connecting to %s\", socket_addr)\n        self.local_socket.connect(socket_addr)\n\n        self.remote_socket = None\n\n        self._read_spin_timer = (\n            SpinSleepTimer() if envs.VLLM_SLEEP_WHEN_IDLE else SpinTimer()\n        )\n    else:\n        self.buffer = None  # type: ignore\n        self.current_idx = -1\n        self.local_reader_rank = -1\n        self._is_local_reader = False\n        self._is_remote_reader = True\n\n        self.local_socket = None\n\n        self.remote_socket = context.socket(SUB)\n        self.remote_socket.setsockopt_string(SUBSCRIBE, \"\")\n        if handle.remote_addr_ipv6:\n            self.remote_socket.setsockopt(IPV6, 1)\n        socket_addr = handle.remote_subscribe_addr\n        logger.debug(\"Connecting to %s\", socket_addr)\n        self.remote_socket.connect(socket_addr)\n\n    return self",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef create_from_handle(handle: Handle, rank) -> \"MessageQueue\":\n    self = MessageQueue.__new__(MessageQueue)\n    self.handle = handle\n    self._is_writer = False\n\n    context = Context()\n\n    if rank in handle.local_reader_ranks:\n        assert handle.buffer_handle is not None\n        self.buffer = ShmRingBuffer(*handle.buffer_handle)\n        self.current_idx = 0\n        self.local_reader_rank = handle.local_reader_ranks.index(rank)\n        self._is_local_reader = True\n        self._is_remote_reader = False\n\n        self.local_socket = context.socket(SUB)\n        self.local_socket.setsockopt_string(SUBSCRIBE, \"\")\n        socket_addr = handle.local_subscribe_addr\n        logger.debug(\"Connecting to %s\", socket_addr)\n        self.local_socket.connect(socket_addr)\n\n        self.remote_socket = None\n\n        self._read_spin_timer = (\n            SpinSleepTimer() if envs.VLLM_SLEEP_WHEN_IDLE else SpinTimer()\n        )\n    else:\n        self.buffer = None  # type: ignore\n        self.current_idx = -1\n        self.local_reader_rank = -1\n        self._is_local_reader = False\n        self._is_remote_reader = True\n\n        self.local_socket = None\n\n        self.remote_socket = context.socket(SUB)\n        self.remote_socket.setsockopt_string(SUBSCRIBE, \"\")\n        if handle.remote_addr_ipv6:\n            self.remote_socket.setsockopt(IPV6, 1)\n        socket_addr = handle.remote_subscribe_addr\n        logger.debug(\"Connecting to %s\", socket_addr)\n        self.remote_socket.connect(socket_addr)\n\n    return self",
      "language": "python"
    },
    {
      "code": "create_from_process_group(\n    pg: ProcessGroup | StatelessProcessGroup,\n    max_chunk_bytes,\n    max_chunks,\n    writer_rank: int = 0,\n    external_writer_handle=None,\n    blocking: bool = True,\n) -> MessageQueue",
      "language": "typescript"
    },
    {
      "code": "create_from_process_group(\n    pg: ProcessGroup | StatelessProcessGroup,\n    max_chunk_bytes,\n    max_chunks,\n    writer_rank: int = 0,\n    external_writer_handle=None,\n    blocking: bool = True,\n) -> MessageQueue",
      "language": "typescript"
    },
    {
      "code": "696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef create_from_process_group(\n    pg: ProcessGroup | StatelessProcessGroup,\n    max_chunk_bytes,\n    max_chunks,\n    writer_rank: int = 0,\n    external_writer_handle=None,\n    blocking: bool = True,\n) -> \"MessageQueue\":\n    \"\"\"\n    Creates a MessageQueue for a distributed process group with one writer and\n    multiple readers.\n\n    This method is designed for scenarios where one process (the writer) sends\n    messages, and all other processes (the readers) receive messages. It sets up\n    the shared memory buffer and socket communication handles accordingly, and\n    broadcasts the handle from the writer to all readers.\n\n    Args:\n        pg (ProcessGroup | StatelessProcessGroup): The torch distributed process\n            group.\n        max_chunk_bytes (int): Maximum size in bytes for each chunk in the buffer.\n        max_chunks (int): Maximum number of chunks in the buffer.\n        writer_rank (int, optional): The global rank that will act as the writer.\n            Defaults to 0.\n        external_writer_handle (Handle, optional): Used when there is a handle\n            from an external Message Queue. If provided, use this handle to init\n            PG writer message queue instead of creating a new one. Defaults to None.\n        blocking (bool, optional): If True, blocks until all processes are ready.\n            Defaults to True.\n\n    Returns:\n        MessageQueue: The MessageQueue instance for the calling process.\n\n    \"\"\"\n    if isinstance(pg, ProcessGroup):\n        group_rank = dist.get_rank(pg)\n        group_world_size = dist.get_world_size(pg)\n        global_ranks = dist.get_process_group_ranks(pg)\n    else:\n        group_rank = pg.rank\n        group_world_size = pg.world_size\n        global_ranks = list(range(pg.world_size))\n    from vllm.distributed.parallel_state import in_the_same_node_as\n\n    status = in_the_same_node_as(pg, source_rank=writer_rank)\n    if group_rank == writer_rank:\n        if external_writer_handle is not None:\n            buffer_io = MessageQueue.create_from_handle(\n                external_writer_handle, group_rank\n            )\n        else:\n            same_node_ranks = [i for i, s in enumerate(status) if s]\n            n_reader = group_world_size - 1\n            n_local_reader = len(same_node_ranks) - 1\n            local_reader_ranks = [i for i in same_node_ranks if i != writer_rank]\n            buffer_io = MessageQueue(\n                n_reader=n_reader,\n                n_local_reader=n_local_reader,\n                local_reader_ranks=local_reader_ranks,\n                max_chunk_bytes=max_chunk_bytes,\n                max_chunks=max_chunks,\n            )\n        handle = buffer_io.export_handle()\n        if isinstance(pg, ProcessGroup):\n            dist.broadcast_object_list(\n                [handle], src=global_ranks[writer_rank], group=pg\n            )\n        else:\n            pg.broadcast_obj(handle, writer_rank)\n    else:\n        if isinstance(pg, ProcessGroup):\n            recv = [None]\n            dist.broadcast_object_list(\n                recv, src=global_ranks[writer_rank], group=pg\n            )\n            handle = recv[0]  # type: ignore\n        else:\n            handle = pg.broadcast_obj(None, writer_rank)\n        buffer_io = MessageQueue.create_from_handle(handle, group_rank)\n    if blocking:\n        buffer_io.wait_until_ready()\n    return buffer_io",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef create_from_process_group(\n    pg: ProcessGroup | StatelessProcessGroup,\n    max_chunk_bytes,\n    max_chunks,\n    writer_rank: int = 0,\n    external_writer_handle=None,\n    blocking: bool = True,\n) -> \"MessageQueue\":\n    \"\"\"\n    Creates a MessageQueue for a distributed process group with one writer and\n    multiple readers.\n\n    This method is designed for scenarios where one process (the writer) sends\n    messages, and all other processes (the readers) receive messages. It sets up\n    the shared memory buffer and socket communication handles accordingly, and\n    broadcasts the handle from the writer to all readers.\n\n    Args:\n        pg (ProcessGroup | StatelessProcessGroup): The torch distributed process\n            group.\n        max_chunk_bytes (int): Maximum size in bytes for each chunk in the buffer.\n        max_chunks (int): Maximum number of chunks in the buffer.\n        writer_rank (int, optional): The global rank that will act as the writer.\n            Defaults to 0.\n        external_writer_handle (Handle, optional): Used when there is a handle\n            from an external Message Queue. If provided, use this handle to init\n            PG writer message queue instead of creating a new one. Defaults to None.\n        blocking (bool, optional): If True, blocks until all processes are ready.\n            Defaults to True.\n\n    Returns:\n        MessageQueue: The MessageQueue instance for the calling process.\n\n    \"\"\"\n    if isinstance(pg, ProcessGroup):\n        group_rank = dist.get_rank(pg)\n        group_world_size = dist.get_world_size(pg)\n        global_ranks = dist.get_process_group_ranks(pg)\n    else:\n        group_rank = pg.rank\n        group_world_size = pg.world_size\n        global_ranks = list(range(pg.world_size))\n    from vllm.distributed.parallel_state import in_the_same_node_as\n\n    status = in_the_same_node_as(pg, source_rank=writer_rank)\n    if group_rank == writer_rank:\n        if external_writer_handle is not None:\n            buffer_io = MessageQueue.create_from_handle(\n                external_writer_handle, group_rank\n            )\n        else:\n            same_node_ranks = [i for i, s in enumerate(status) if s]\n            n_reader = group_world_size - 1\n            n_local_reader = len(same_node_ranks) - 1\n            local_reader_ranks = [i for i in same_node_ranks if i != writer_rank]\n            buffer_io = MessageQueue(\n                n_reader=n_reader,\n                n_local_reader=n_local_reader,\n                local_reader_ranks=local_reader_ranks,\n                max_chunk_bytes=max_chunk_bytes,\n                max_chunks=max_chunks,\n            )\n        handle = buffer_io.export_handle()\n        if isinstance(pg, ProcessGroup):\n            dist.broadcast_object_list(\n                [handle], src=global_ranks[writer_rank], group=pg\n            )\n        else:\n            pg.broadcast_obj(handle, writer_rank)\n    else:\n        if isinstance(pg, ProcessGroup):\n            recv = [None]\n            dist.broadcast_object_list(\n                recv, src=global_ranks[writer_rank], group=pg\n            )\n            handle = recv[0]  # type: ignore\n        else:\n            handle = pg.broadcast_obj(None, writer_rank)\n        buffer_io = MessageQueue.create_from_handle(handle, group_rank)\n    if blocking:\n        buffer_io.wait_until_ready()\n    return buffer_io",
      "language": "python"
    },
    {
      "code": "create_from_process_group_single_reader(\n    pg: ProcessGroup,\n    max_chunk_bytes,\n    max_chunks,\n    reader_rank: int = 0,\n    blocking: bool = False,\n) -> tuple[MessageQueue, list[Handle]]",
      "language": "typescript"
    },
    {
      "code": "create_from_process_group_single_reader(\n    pg: ProcessGroup,\n    max_chunk_bytes,\n    max_chunks,\n    reader_rank: int = 0,\n    blocking: bool = False,\n) -> tuple[MessageQueue, list[Handle]]",
      "language": "typescript"
    },
    {
      "code": "650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef create_from_process_group_single_reader(\n    pg: ProcessGroup,\n    max_chunk_bytes,\n    max_chunks,\n    reader_rank: int = 0,\n    blocking: bool = False,\n) -> tuple[\"MessageQueue\", list[Handle]]:\n    \"\"\"\n    Creates a MessageQueue for a process group with a single reader.\n\n    This method is designed for scenarios where only one process (the reader)\n    will consume messages, and all other processes are writers. It sets up\n    the shared memory buffer and communication handles accordingly, and\n    gathers the handles from all processes to the reader.\n\n    Args:\n        pg (ProcessGroup): The torch distributed process group.\n        max_chunk_bytes (int): Maximum size in bytes for each chunk in the buffer.\n        max_chunks (int): Maximum number of chunks in the buffer.\n        reader_rank (int, optional): The global rank that will act as the reader.\n            Defaults to 0.\n        blocking (bool, optional): If True, blocks until all processes are ready.\n            Defaults to False.\n\n    Returns:\n        tuple[MessageQueue, list[Handle]]:\n        The MessageQueue instance for the calling process,\n        and a list of handles (only non-empty for the reader process).\n    \"\"\"\n    local_size = current_platform.device_count()\n    rank = dist.get_rank()\n    same_node = rank // local_size == reader_rank // local_size\n    buffer_io = MessageQueue(\n        n_reader=1,\n        n_local_reader=1 if same_node else 0,\n        max_chunk_bytes=max_chunk_bytes,\n        max_chunks=max_chunks,\n    )\n    handle = buffer_io.export_handle()\n    handles = [None] * dist.get_world_size(pg) if rank == reader_rank else None\n    dist.gather_object(handle, handles, dst=reader_rank, group=pg)\n    if blocking:\n        buffer_io.wait_until_ready()\n    return buffer_io, cast(list[Handle], handles or [])",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef create_from_process_group_single_reader(\n    pg: ProcessGroup,\n    max_chunk_bytes,\n    max_chunks,\n    reader_rank: int = 0,\n    blocking: bool = False,\n) -> tuple[\"MessageQueue\", list[Handle]]:\n    \"\"\"\n    Creates a MessageQueue for a process group with a single reader.\n\n    This method is designed for scenarios where only one process (the reader)\n    will consume messages, and all other processes are writers. It sets up\n    the shared memory buffer and communication handles accordingly, and\n    gathers the handles from all processes to the reader.\n\n    Args:\n        pg (ProcessGroup): The torch distributed process group.\n        max_chunk_bytes (int): Maximum size in bytes for each chunk in the buffer.\n        max_chunks (int): Maximum number of chunks in the buffer.\n        reader_rank (int, optional): The global rank that will act as the reader.\n            Defaults to 0.\n        blocking (bool, optional): If True, blocks until all processes are ready.\n            Defaults to False.\n\n    Returns:\n        tuple[MessageQueue, list[Handle]]:\n        The MessageQueue instance for the calling process,\n        and a list of handles (only non-empty for the reader process).\n    \"\"\"\n    local_size = current_platform.device_count()\n    rank = dist.get_rank()\n    same_node = rank // local_size == reader_rank // local_size\n    buffer_io = MessageQueue(\n        n_reader=1,\n        n_local_reader=1 if same_node else 0,\n        max_chunk_bytes=max_chunk_bytes,\n        max_chunks=max_chunks,\n    )\n    handle = buffer_io.export_handle()\n    handles = [None] * dist.get_world_size(pg) if rank == reader_rank else None\n    dist.gather_object(handle, handles, dst=reader_rank, group=pg)\n    if blocking:\n        buffer_io.wait_until_ready()\n    return buffer_io, cast(list[Handle], handles or [])",
      "language": "python"
    },
    {
      "code": "dequeue(\n    timeout: float | None = None,\n    cancel: Event | None = None,\n    indefinite: bool = False,\n)",
      "language": "typescript"
    },
    {
      "code": "dequeue(\n    timeout: float | None = None,\n    cancel: Event | None = None,\n    indefinite: bool = False,\n)",
      "language": "typescript"
    },
    {
      "code": "608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634",
      "language": "unknown"
    },
    {
      "code": "def dequeue(\n    self,\n    timeout: float | None = None,\n    cancel: Event | None = None,\n    indefinite: bool = False,\n):\n    \"\"\"Read from message queue with optional timeout (in seconds)\"\"\"\n    if self._is_local_reader:\n        with self.acquire_read(timeout, cancel, indefinite) as buf:\n            overflow = buf[0] == 1\n            if not overflow:\n                offset = 3\n                buf_count = from_bytes_big(buf[1:offset])\n                all_buffers = []\n                for i in range(buf_count):\n                    buf_offset = offset + 4\n                    buf_len = from_bytes_big(buf[offset:buf_offset])\n                    offset = buf_offset + buf_len\n                    all_buffers.append(buf[buf_offset:offset])\n                obj = pickle.loads(all_buffers[0], buffers=all_buffers[1:])\n        if overflow:\n            obj = MessageQueue.recv(self.local_socket, timeout)\n    elif self._is_remote_reader:\n        obj = MessageQueue.recv(self.remote_socket, timeout)\n    else:\n        raise RuntimeError(\"Only readers can dequeue\")\n    return obj",
      "language": "python"
    },
    {
      "code": "def dequeue(\n    self,\n    timeout: float | None = None,\n    cancel: Event | None = None,\n    indefinite: bool = False,\n):\n    \"\"\"Read from message queue with optional timeout (in seconds)\"\"\"\n    if self._is_local_reader:\n        with self.acquire_read(timeout, cancel, indefinite) as buf:\n            overflow = buf[0] == 1\n            if not overflow:\n                offset = 3\n                buf_count = from_bytes_big(buf[1:offset])\n                all_buffers = []\n                for i in range(buf_count):\n                    buf_offset = offset + 4\n                    buf_len = from_bytes_big(buf[offset:buf_offset])\n                    offset = buf_offset + buf_len\n                    all_buffers.append(buf[buf_offset:offset])\n                obj = pickle.loads(all_buffers[0], buffers=all_buffers[1:])\n        if overflow:\n            obj = MessageQueue.recv(self.local_socket, timeout)\n    elif self._is_remote_reader:\n        obj = MessageQueue.recv(self.remote_socket, timeout)\n    else:\n        raise RuntimeError(\"Only readers can dequeue\")\n    return obj",
      "language": "python"
    },
    {
      "code": "enqueue(obj, timeout: float | None = None)",
      "language": "rust"
    },
    {
      "code": "enqueue(obj, timeout: float | None = None)",
      "language": "rust"
    },
    {
      "code": "565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606",
      "language": "unknown"
    },
    {
      "code": "def enqueue(self, obj, timeout: float | None = None):\n    \"\"\"Write to message queue with optional timeout (in seconds)\"\"\"\n    assert self._is_writer, \"Only writers can enqueue\"\n    all_buffers: list[SizedBuffer] = [b\"\"]\n    total_bytes = 6  # 2 bytes for oob buffer count, 4 for main buffer size\n\n    def oob_callback(buf: PickleBuffer) -> bool:\n        raw_buf = buf.raw()\n        if len(raw_buf) < 1024 * 1024:\n            # In-line buffers smaller than 1MiB.\n            return True\n        all_buffers.append(raw_buf)\n        nonlocal total_bytes\n        total_bytes += len(raw_buf) + 4\n        return False\n\n    all_buffers[0] = pickle.dumps(\n        obj, protocol=pickle.HIGHEST_PROTOCOL, buffer_callback=oob_callback\n    )\n    if self.n_local_reader > 0:\n        if total_bytes + len(all_buffers[0]) >= self.buffer.max_chunk_bytes:\n            with self.acquire_write(timeout) as buf:\n                buf[0] = 1  # overflow\n            self.local_socket.send_multipart(all_buffers, copy=False)\n        else:\n            # Byte 0: 0\n            # Bytes 1-2: Count of buffers\n            # Then each buffer follows, preceded by 4 bytes containing its length:\n            # [4 byte int L][L bytes of buffer content] ...\n            with self.acquire_write(timeout) as buf:\n                buf[0] = 0  # not overflow\n                offset = 3\n                buf[1:offset] = to_bytes_big(len(all_buffers), 2)  # oob buf count\n                for buffer in all_buffers:\n                    buf_len = len(buffer)\n                    # prepend each buffer with 4 bytes containing its size.\n                    buf_offset = offset + 4\n                    buf[offset:buf_offset] = to_bytes_big(buf_len, 4)\n                    buf[buf_offset : (offset := buf_offset + buf_len)] = buffer\n\n    if self.n_remote_reader > 0:\n        self.remote_socket.send_multipart(all_buffers, copy=False)",
      "language": "python"
    },
    {
      "code": "def enqueue(self, obj, timeout: float | None = None):\n    \"\"\"Write to message queue with optional timeout (in seconds)\"\"\"\n    assert self._is_writer, \"Only writers can enqueue\"\n    all_buffers: list[SizedBuffer] = [b\"\"]\n    total_bytes = 6  # 2 bytes for oob buffer count, 4 for main buffer size\n\n    def oob_callback(buf: PickleBuffer) -> bool:\n        raw_buf = buf.raw()\n        if len(raw_buf) < 1024 * 1024:\n            # In-line buffers smaller than 1MiB.\n            return True\n        all_buffers.append(raw_buf)\n        nonlocal total_bytes\n        total_bytes += len(raw_buf) + 4\n        return False\n\n    all_buffers[0] = pickle.dumps(\n        obj, protocol=pickle.HIGHEST_PROTOCOL, buffer_callback=oob_callback\n    )\n    if self.n_local_reader > 0:\n        if total_bytes + len(all_buffers[0]) >= self.buffer.max_chunk_bytes:\n            with self.acquire_write(timeout) as buf:\n                buf[0] = 1  # overflow\n            self.local_socket.send_multipart(all_buffers, copy=False)\n        else:\n            # Byte 0: 0\n            # Bytes 1-2: Count of buffers\n            # Then each buffer follows, preceded by 4 bytes containing its length:\n            # [4 byte int L][L bytes of buffer content] ...\n            with self.acquire_write(timeout) as buf:\n                buf[0] = 0  # not overflow\n                offset = 3\n                buf[1:offset] = to_bytes_big(len(all_buffers), 2)  # oob buf count\n                for buffer in all_buffers:\n                    buf_len = len(buffer)\n                    # prepend each buffer with 4 bytes containing its size.\n                    buf_offset = offset + 4\n                    buf[offset:buf_offset] = to_bytes_big(buf_len, 4)\n                    buf[buf_offset : (offset := buf_offset + buf_len)] = buffer\n\n    if self.n_remote_reader > 0:\n        self.remote_socket.send_multipart(all_buffers, copy=False)",
      "language": "python"
    },
    {
      "code": "export_handle() -> Handle",
      "language": "php"
    },
    {
      "code": "export_handle() -> Handle",
      "language": "php"
    },
    {
      "code": "def export_handle(self) -> Handle:\n    return self.handle",
      "language": "python"
    },
    {
      "code": "def export_handle(self) -> Handle:\n    return self.handle",
      "language": "python"
    },
    {
      "code": "recv(socket: Socket, timeout: float | None) -> Any",
      "language": "rust"
    },
    {
      "code": "recv(socket: Socket, timeout: float | None) -> Any",
      "language": "rust"
    },
    {
      "code": "636\n637\n638\n639\n640\n641\n642",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef recv(socket: zmq.Socket, timeout: float | None) -> Any:\n    timeout_ms = None if timeout is None else int(timeout * 1000)\n    if not socket.poll(timeout=timeout_ms):\n        raise TimeoutError\n    recv, *recv_oob = socket.recv_multipart(copy=False)\n    return pickle.loads(recv, buffers=recv_oob)",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef recv(socket: zmq.Socket, timeout: float | None) -> Any:\n    timeout_ms = None if timeout is None else int(timeout * 1000)\n    if not socket.poll(timeout=timeout_ms):\n        raise TimeoutError\n    recv, *recv_oob = socket.recv_multipart(copy=False)\n    return pickle.loads(recv, buffers=recv_oob)",
      "language": "python"
    },
    {
      "code": "wait_until_ready()",
      "language": "unknown"
    },
    {
      "code": "wait_until_ready()",
      "language": "unknown"
    },
    {
      "code": "405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436",
      "language": "unknown"
    },
    {
      "code": "def wait_until_ready(self):\n    \"\"\"This is a collective operation. All processes (including the\n    readers and the writer) should call this function.\n    \"\"\"\n    if self._is_writer:\n        # wait for all readers to connect\n\n        # local readers\n        for i in range(self.n_local_reader):\n            # wait for subscription messages from all local readers\n            self.local_socket.recv()\n        if self.n_local_reader > 0:\n            # send a message to all local readers\n            # to make sure the publish channel is working\n            self.local_socket.send(b\"READY\")\n\n        # remote readers\n        for i in range(self.n_remote_reader):\n            # wait for subscription messages from all remote readers\n            self.remote_socket.recv()\n        if self.n_remote_reader > 0:\n            # send a message to all remote readers\n            # to make sure the publish channel is working\n            self.remote_socket.send(b\"READY\")\n    elif self._is_local_reader:\n        # wait for the writer to send a message\n        recv = self.local_socket.recv()\n        assert recv == b\"READY\"\n    elif self._is_remote_reader:\n        # wait for the writer to send a message\n        recv = self.remote_socket.recv()\n        assert recv == b\"READY\"",
      "language": "python"
    },
    {
      "code": "def wait_until_ready(self):\n    \"\"\"This is a collective operation. All processes (including the\n    readers and the writer) should call this function.\n    \"\"\"\n    if self._is_writer:\n        # wait for all readers to connect\n\n        # local readers\n        for i in range(self.n_local_reader):\n            # wait for subscription messages from all local readers\n            self.local_socket.recv()\n        if self.n_local_reader > 0:\n            # send a message to all local readers\n            # to make sure the publish channel is working\n            self.local_socket.send(b\"READY\")\n\n        # remote readers\n        for i in range(self.n_remote_reader):\n            # wait for subscription messages from all remote readers\n            self.remote_socket.recv()\n        if self.n_remote_reader > 0:\n            # send a message to all remote readers\n            # to make sure the publish channel is working\n            self.remote_socket.send(b\"READY\")\n    elif self._is_local_reader:\n        # wait for the writer to send a message\n        recv = self.local_socket.recv()\n        assert recv == b\"READY\"\n    elif self._is_remote_reader:\n        # wait for the writer to send a message\n        recv = self.remote_socket.recv()\n        assert recv == b\"READY\"",
      "language": "python"
    },
    {
      "code": "127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259",
      "language": "unknown"
    },
    {
      "code": "class ShmRingBuffer:\n    def __init__(\n        self,\n        n_reader: int,\n        max_chunk_bytes: int,\n        max_chunks: int,\n        name: str | None = None,\n    ):\n        \"\"\"\n        A shared memory ring buffer implementation for broadcast communication.\n        Essentially, it is a queue where only one will `enqueue` and multiple\n        will `dequeue`. The max size of each item, together with the max number\n        of items that can be stored in the buffer are known in advance.\n        In this case, we don't need to synchronize the access to\n         the buffer.\n\n        Buffer memory layout:\n                  data                                 metadata\n                    |                                      |\n                    | (current_idx)                        | (current_idx)\n                    v                                      v\n        +-------------------------------+----------------------------------------+\n        | chunk0 | chunk1 | ... | chunk | metadata0 | metadata1 | ... | metadata |\n        +-------------------------------+----------------------------------------+\n        | max_chunks x max_chunk_bytes  | max_chunks x (1 + n_reader) bytes      |\n\n        metadata memory layout: each byte is a flag, the first byte is the written\n        flag, and the rest are reader flags. The flags are set to 0 by default.\n        +--------------+--------------+--------------+-----+--------------+\n        | written_flag | reader0_flag | reader1_flag | ... | readerN_flag |\n        +--------------+--------------+--------------+-----+--------------+\n\n        The state of metadata is as follows:\n\n        (case 1) 0???...???: the block is not written yet, cannot read, can write\n        (case 2) 1000...000: the block is just written, can read, cannot write\n        (case 3) 1???...???: the block is written and read by some readers, can read if not read, cannot write\n        (case 4) 1111...111: the block is written and read by all readers, cannot read, can write\n\n        State transition for readers:\n\n        When a reader finds a block that it can read (case 2 or 3), it can yield the block for caller to read.\n        Only after the caller finishes reading the block, the reader can mark the block as read.\n        Readers only mark the block as read (from 0 to 1), the writer marks the block as ready to read (from 1 to 0).\n\n        State transition for writer:\n\n        When the writer writes to a block (case 1 or 4), it first resets the written flag to 0, converting either case\n        to case 1. Then it can yield the block for caller to write. After the caller finishes writing the block, the writer\n        can reset the reader flags to 0, and mark the block as written (from 0 to 1).\n        NOTE: the order is important here, first reset the reader flags (so that we are still in case 1), then mark the block as written. The state transition is atomic. If we do it in the reverse order, it will go through case 3 and then back to case 2, and readers might read the intermediate case 3, which is not correct.\n\n        During creation, `name` is None and the buffer is created. We can pass the\n        created object to other processes by pickling it. The other processes will\n        get the name of the shared memory and open it, so that they can access the\n        same shared memory buffer.\n        \"\"\"  # noqa\n        self.n_reader = n_reader\n        self.metadata_size = 1 + n_reader\n        self.max_chunk_bytes = max_chunk_bytes\n        self.max_chunks = max_chunks\n        self.total_bytes_of_buffer = (\n            self.max_chunk_bytes + self.metadata_size\n        ) * self.max_chunks\n        self.data_offset = 0\n        self.metadata_offset = self.max_chunk_bytes * self.max_chunks\n\n        if name is None:\n            # we are creating a buffer\n            self.is_creator = True\n            self.shared_memory = shared_memory.SharedMemory(\n                create=True, size=self.total_bytes_of_buffer\n            )\n            # initialize the metadata section to 0\n            with self.shared_memory.buf[self.metadata_offset :] as metadata_buffer:\n                torch.frombuffer(metadata_buffer, dtype=torch.uint8).fill_(0)\n        else:\n            # we are opening an existing buffer\n            self.is_creator = False\n            # fix to https://stackoverflow.com/q/62748654/9191338\n            # Python incorrectly tracks shared memory even if it is not\n            # created by the process. The following patch is a workaround.\n            with patch(\n                \"multiprocessing.resource_tracker.register\",\n                lambda *args, **kwargs: None,\n            ):\n                try:\n                    self.shared_memory = shared_memory.SharedMemory(name=name)\n                    # See https://docs.python.org/3/library/multiprocessing.shared_memory.html # noqa\n                    # Some platforms allocate memory based on page size,\n                    # so the shared memory block size may be larger or equal\n                    # to the requested size. The size parameter is ignored\n                    # when attaching to an existing block.\n                    assert self.shared_memory.size >= self.total_bytes_of_buffer\n                except FileNotFoundError:\n                    # we might deserialize the object in a different node\n                    # in this case, this object is not used,\n                    # and we should suppress the error\n                    pass\n\n    def handle(self):\n        return (\n            self.n_reader,\n            self.max_chunk_bytes,\n            self.max_chunks,\n            self.shared_memory.name,\n        )\n\n    def __reduce__(self):\n        return (\n            self.__class__,\n            self.handle(),\n        )\n\n    def __del__(self):\n        if hasattr(self, \"shared_memory\"):\n            self.shared_memory.close()\n            if self.is_creator:\n                self.shared_memory.unlink()\n\n    @contextmanager\n    def get_data(self, current_idx: int):\n        start = self.data_offset + current_idx * self.max_chunk_bytes\n        end = start + self.max_chunk_bytes\n        with self.shared_memory.buf[start:end] as buf:\n            yield buf\n\n    @contextmanager\n    def get_metadata(self, current_idx: int):\n        start = self.metadata_offset + current_idx * self.metadata_size\n        end = start + self.metadata_size\n        with self.shared_memory.buf[start:end] as buf:\n            yield buf",
      "language": "python"
    },
    {
      "code": "class ShmRingBuffer:\n    def __init__(\n        self,\n        n_reader: int,\n        max_chunk_bytes: int,\n        max_chunks: int,\n        name: str | None = None,\n    ):\n        \"\"\"\n        A shared memory ring buffer implementation for broadcast communication.\n        Essentially, it is a queue where only one will `enqueue` and multiple\n        will `dequeue`. The max size of each item, together with the max number\n        of items that can be stored in the buffer are known in advance.\n        In this case, we don't need to synchronize the access to\n         the buffer.\n\n        Buffer memory layout:\n                  data                                 metadata\n                    |                                      |\n                    | (current_idx)                        | (current_idx)\n                    v                                      v\n        +-------------------------------+----------------------------------------+\n        | chunk0 | chunk1 | ... | chunk | metadata0 | metadata1 | ... | metadata |\n        +-------------------------------+----------------------------------------+\n        | max_chunks x max_chunk_bytes  | max_chunks x (1 + n_reader) bytes      |\n\n        metadata memory layout: each byte is a flag, the first byte is the written\n        flag, and the rest are reader flags. The flags are set to 0 by default.\n        +--------------+--------------+--------------+-----+--------------+\n        | written_flag | reader0_flag | reader1_flag | ... | readerN_flag |\n        +--------------+--------------+--------------+-----+--------------+\n\n        The state of metadata is as follows:\n\n        (case 1) 0???...???: the block is not written yet, cannot read, can write\n        (case 2) 1000...000: the block is just written, can read, cannot write\n        (case 3) 1???...???: the block is written and read by some readers, can read if not read, cannot write\n        (case 4) 1111...111: the block is written and read by all readers, cannot read, can write\n\n        State transition for readers:\n\n        When a reader finds a block that it can read (case 2 or 3), it can yield the block for caller to read.\n        Only after the caller finishes reading the block, the reader can mark the block as read.\n        Readers only mark the block as read (from 0 to 1), the writer marks the block as ready to read (from 1 to 0).\n\n        State transition for writer:\n\n        When the writer writes to a block (case 1 or 4), it first resets the written flag to 0, converting either case\n        to case 1. Then it can yield the block for caller to write. After the caller finishes writing the block, the writer\n        can reset the reader flags to 0, and mark the block as written (from 0 to 1).\n        NOTE: the order is important here, first reset the reader flags (so that we are still in case 1), then mark the block as written. The state transition is atomic. If we do it in the reverse order, it will go through case 3 and then back to case 2, and readers might read the intermediate case 3, which is not correct.\n\n        During creation, `name` is None and the buffer is created. We can pass the\n        created object to other processes by pickling it. The other processes will\n        get the name of the shared memory and open it, so that they can access the\n        same shared memory buffer.\n        \"\"\"  # noqa\n        self.n_reader = n_reader\n        self.metadata_size = 1 + n_reader\n        self.max_chunk_bytes = max_chunk_bytes\n        self.max_chunks = max_chunks\n        self.total_bytes_of_buffer = (\n            self.max_chunk_bytes + self.metadata_size\n        ) * self.max_chunks\n        self.data_offset = 0\n        self.metadata_offset = self.max_chunk_bytes * self.max_chunks\n\n        if name is None:\n            # we are creating a buffer\n            self.is_creator = True\n            self.shared_memory = shared_memory.SharedMemory(\n                create=True, size=self.total_bytes_of_buffer\n            )\n            # initialize the metadata section to 0\n            with self.shared_memory.buf[self.metadata_offset :] as metadata_buffer:\n                torch.frombuffer(metadata_buffer, dtype=torch.uint8).fill_(0)\n        else:\n            # we are opening an existing buffer\n            self.is_creator = False\n            # fix to https://stackoverflow.com/q/62748654/9191338\n            # Python incorrectly tracks shared memory even if it is not\n            # created by the process. The following patch is a workaround.\n            with patch(\n                \"multiprocessing.resource_tracker.register\",\n                lambda *args, **kwargs: None,\n            ):\n                try:\n                    self.shared_memory = shared_memory.SharedMemory(name=name)\n                    # See https://docs.python.org/3/library/multiprocessing.shared_memory.html # noqa\n                    # Some platforms allocate memory based on page size,\n                    # so the shared memory block size may be larger or equal\n                    # to the requested size. The size parameter is ignored\n                    # when attaching to an existing block.\n                    assert self.shared_memory.size >= self.total_bytes_of_buffer\n                except FileNotFoundError:\n                    # we might deserialize the object in a different node\n                    # in this case, this object is not used,\n                    # and we should suppress the error\n                    pass\n\n    def handle(self):\n        return (\n            self.n_reader,\n            self.max_chunk_bytes,\n            self.max_chunks,\n            self.shared_memory.name,\n        )\n\n    def __reduce__(self):\n        return (\n            self.__class__,\n            self.handle(),\n        )\n\n    def __del__(self):\n        if hasattr(self, \"shared_memory\"):\n            self.shared_memory.close()\n            if self.is_creator:\n                self.shared_memory.unlink()\n\n    @contextmanager\n    def get_data(self, current_idx: int):\n        start = self.data_offset + current_idx * self.max_chunk_bytes\n        end = start + self.max_chunk_bytes\n        with self.shared_memory.buf[start:end] as buf:\n            yield buf\n\n    @contextmanager\n    def get_metadata(self, current_idx: int):\n        start = self.metadata_offset + current_idx * self.metadata_size\n        end = start + self.metadata_size\n        with self.shared_memory.buf[start:end] as buf:\n            yield buf",
      "language": "python"
    },
    {
      "code": "data_offset = 0",
      "language": "unknown"
    },
    {
      "code": "data_offset = 0",
      "language": "unknown"
    },
    {
      "code": "is_creator = True",
      "language": "unknown"
    },
    {
      "code": "is_creator = True",
      "language": "unknown"
    },
    {
      "code": "max_chunk_bytes = max_chunk_bytes",
      "language": "unknown"
    },
    {
      "code": "max_chunk_bytes = max_chunk_bytes",
      "language": "unknown"
    },
    {
      "code": "max_chunks = max_chunks",
      "language": "unknown"
    },
    {
      "code": "max_chunks = max_chunks",
      "language": "unknown"
    },
    {
      "code": "metadata_offset = max_chunk_bytes * max_chunks",
      "language": "unknown"
    },
    {
      "code": "metadata_offset = max_chunk_bytes * max_chunks",
      "language": "unknown"
    },
    {
      "code": "metadata_size = 1 + n_reader",
      "language": "unknown"
    },
    {
      "code": "metadata_size = 1 + n_reader",
      "language": "unknown"
    },
    {
      "code": "n_reader = n_reader",
      "language": "unknown"
    },
    {
      "code": "n_reader = n_reader",
      "language": "unknown"
    },
    {
      "code": "shared_memory = SharedMemory(name=name)",
      "language": "unknown"
    },
    {
      "code": "shared_memory = SharedMemory(name=name)",
      "language": "unknown"
    },
    {
      "code": "total_bytes_of_buffer = (\n    max_chunk_bytes + metadata_size\n) * max_chunks",
      "language": "unknown"
    },
    {
      "code": "total_bytes_of_buffer = (\n    max_chunk_bytes + metadata_size\n) * max_chunks",
      "language": "unknown"
    },
    {
      "code": "241\n242\n243\n244\n245",
      "language": "unknown"
    },
    {
      "code": "def __del__(self):\n    if hasattr(self, \"shared_memory\"):\n        self.shared_memory.close()\n        if self.is_creator:\n            self.shared_memory.unlink()",
      "language": "python"
    },
    {
      "code": "def __del__(self):\n    if hasattr(self, \"shared_memory\"):\n        self.shared_memory.close()\n        if self.is_creator:\n            self.shared_memory.unlink()",
      "language": "python"
    },
    {
      "code": "__init__(\n    n_reader: int,\n    max_chunk_bytes: int,\n    max_chunks: int,\n    name: str | None = None,\n)",
      "language": "python"
    },
    {
      "code": "__init__(\n    n_reader: int,\n    max_chunk_bytes: int,\n    max_chunks: int,\n    name: str | None = None,\n)",
      "language": "python"
    },
    {
      "code": "128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    n_reader: int,\n    max_chunk_bytes: int,\n    max_chunks: int,\n    name: str | None = None,\n):\n    \"\"\"\n    A shared memory ring buffer implementation for broadcast communication.\n    Essentially, it is a queue where only one will `enqueue` and multiple\n    will `dequeue`. The max size of each item, together with the max number\n    of items that can be stored in the buffer are known in advance.\n    In this case, we don't need to synchronize the access to\n     the buffer.\n\n    Buffer memory layout:\n              data                                 metadata\n                |                                      |\n                | (current_idx)                        | (current_idx)\n                v                                      v\n    +-------------------------------+----------------------------------------+\n    | chunk0 | chunk1 | ... | chunk | metadata0 | metadata1 | ... | metadata |\n    +-------------------------------+----------------------------------------+\n    | max_chunks x max_chunk_bytes  | max_chunks x (1 + n_reader) bytes      |\n\n    metadata memory layout: each byte is a flag, the first byte is the written\n    flag, and the rest are reader flags. The flags are set to 0 by default.\n    +--------------+--------------+--------------+-----+--------------+\n    | written_flag | reader0_flag | reader1_flag | ... | readerN_flag |\n    +--------------+--------------+--------------+-----+--------------+\n\n    The state of metadata is as follows:\n\n    (case 1) 0???...???: the block is not written yet, cannot read, can write\n    (case 2) 1000...000: the block is just written, can read, cannot write\n    (case 3) 1???...???: the block is written and read by some readers, can read if not read, cannot write\n    (case 4) 1111...111: the block is written and read by all readers, cannot read, can write\n\n    State transition for readers:\n\n    When a reader finds a block that it can read (case 2 or 3), it can yield the block for caller to read.\n    Only after the caller finishes reading the block, the reader can mark the block as read.\n    Readers only mark the block as read (from 0 to 1), the writer marks the block as ready to read (from 1 to 0).\n\n    State transition for writer:\n\n    When the writer writes to a block (case 1 or 4), it first resets the written flag to 0, converting either case\n    to case 1. Then it can yield the block for caller to write. After the caller finishes writing the block, the writer\n    can reset the reader flags to 0, and mark the block as written (from 0 to 1).\n    NOTE: the order is important here, first reset the reader flags (so that we are still in case 1), then mark the block as written. The state transition is atomic. If we do it in the reverse order, it will go through case 3 and then back to case 2, and readers might read the intermediate case 3, which is not correct.\n\n    During creation, `name` is None and the buffer is created. We can pass the\n    created object to other processes by pickling it. The other processes will\n    get the name of the shared memory and open it, so that they can access the\n    same shared memory buffer.\n    \"\"\"  # noqa\n    self.n_reader = n_reader\n    self.metadata_size = 1 + n_reader\n    self.max_chunk_bytes = max_chunk_bytes\n    self.max_chunks = max_chunks\n    self.total_bytes_of_buffer = (\n        self.max_chunk_bytes + self.metadata_size\n    ) * self.max_chunks\n    self.data_offset = 0\n    self.metadata_offset = self.max_chunk_bytes * self.max_chunks\n\n    if name is None:\n        # we are creating a buffer\n        self.is_creator = True\n        self.shared_memory = shared_memory.SharedMemory(\n            create=True, size=self.total_bytes_of_buffer\n        )\n        # initialize the metadata section to 0\n        with self.shared_memory.buf[self.metadata_offset :] as metadata_buffer:\n            torch.frombuffer(metadata_buffer, dtype=torch.uint8).fill_(0)\n    else:\n        # we are opening an existing buffer\n        self.is_creator = False\n        # fix to https://stackoverflow.com/q/62748654/9191338\n        # Python incorrectly tracks shared memory even if it is not\n        # created by the process. The following patch is a workaround.\n        with patch(\n            \"multiprocessing.resource_tracker.register\",\n            lambda *args, **kwargs: None,\n        ):\n            try:\n                self.shared_memory = shared_memory.SharedMemory(name=name)\n                # See https://docs.python.org/3/library/multiprocessing.shared_memory.html # noqa\n                # Some platforms allocate memory based on page size,\n                # so the shared memory block size may be larger or equal\n                # to the requested size. The size parameter is ignored\n                # when attaching to an existing block.\n                assert self.shared_memory.size >= self.total_bytes_of_buffer\n            except FileNotFoundError:\n                # we might deserialize the object in a different node\n                # in this case, this object is not used,\n                # and we should suppress the error\n                pass",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    n_reader: int,\n    max_chunk_bytes: int,\n    max_chunks: int,\n    name: str | None = None,\n):\n    \"\"\"\n    A shared memory ring buffer implementation for broadcast communication.\n    Essentially, it is a queue where only one will `enqueue` and multiple\n    will `dequeue`. The max size of each item, together with the max number\n    of items that can be stored in the buffer are known in advance.\n    In this case, we don't need to synchronize the access to\n     the buffer.\n\n    Buffer memory layout:\n              data                                 metadata\n                |                                      |\n                | (current_idx)                        | (current_idx)\n                v                                      v\n    +-------------------------------+----------------------------------------+\n    | chunk0 | chunk1 | ... | chunk | metadata0 | metadata1 | ... | metadata |\n    +-------------------------------+----------------------------------------+\n    | max_chunks x max_chunk_bytes  | max_chunks x (1 + n_reader) bytes      |\n\n    metadata memory layout: each byte is a flag, the first byte is the written\n    flag, and the rest are reader flags. The flags are set to 0 by default.\n    +--------------+--------------+--------------+-----+--------------+\n    | written_flag | reader0_flag | reader1_flag | ... | readerN_flag |\n    +--------------+--------------+--------------+-----+--------------+\n\n    The state of metadata is as follows:\n\n    (case 1) 0???...???: the block is not written yet, cannot read, can write\n    (case 2) 1000...000: the block is just written, can read, cannot write\n    (case 3) 1???...???: the block is written and read by some readers, can read if not read, cannot write\n    (case 4) 1111...111: the block is written and read by all readers, cannot read, can write\n\n    State transition for readers:\n\n    When a reader finds a block that it can read (case 2 or 3), it can yield the block for caller to read.\n    Only after the caller finishes reading the block, the reader can mark the block as read.\n    Readers only mark the block as read (from 0 to 1), the writer marks the block as ready to read (from 1 to 0).\n\n    State transition for writer:\n\n    When the writer writes to a block (case 1 or 4), it first resets the written flag to 0, converting either case\n    to case 1. Then it can yield the block for caller to write. After the caller finishes writing the block, the writer\n    can reset the reader flags to 0, and mark the block as written (from 0 to 1).\n    NOTE: the order is important here, first reset the reader flags (so that we are still in case 1), then mark the block as written. The state transition is atomic. If we do it in the reverse order, it will go through case 3 and then back to case 2, and readers might read the intermediate case 3, which is not correct.\n\n    During creation, `name` is None and the buffer is created. We can pass the\n    created object to other processes by pickling it. The other processes will\n    get the name of the shared memory and open it, so that they can access the\n    same shared memory buffer.\n    \"\"\"  # noqa\n    self.n_reader = n_reader\n    self.metadata_size = 1 + n_reader\n    self.max_chunk_bytes = max_chunk_bytes\n    self.max_chunks = max_chunks\n    self.total_bytes_of_buffer = (\n        self.max_chunk_bytes + self.metadata_size\n    ) * self.max_chunks\n    self.data_offset = 0\n    self.metadata_offset = self.max_chunk_bytes * self.max_chunks\n\n    if name is None:\n        # we are creating a buffer\n        self.is_creator = True\n        self.shared_memory = shared_memory.SharedMemory(\n            create=True, size=self.total_bytes_of_buffer\n        )\n        # initialize the metadata section to 0\n        with self.shared_memory.buf[self.metadata_offset :] as metadata_buffer:\n            torch.frombuffer(metadata_buffer, dtype=torch.uint8).fill_(0)\n    else:\n        # we are opening an existing buffer\n        self.is_creator = False\n        # fix to https://stackoverflow.com/q/62748654/9191338\n        # Python incorrectly tracks shared memory even if it is not\n        # created by the process. The following patch is a workaround.\n        with patch(\n            \"multiprocessing.resource_tracker.register\",\n            lambda *args, **kwargs: None,\n        ):\n            try:\n                self.shared_memory = shared_memory.SharedMemory(name=name)\n                # See https://docs.python.org/3/library/multiprocessing.shared_memory.html # noqa\n                # Some platforms allocate memory based on page size,\n                # so the shared memory block size may be larger or equal\n                # to the requested size. The size parameter is ignored\n                # when attaching to an existing block.\n                assert self.shared_memory.size >= self.total_bytes_of_buffer\n            except FileNotFoundError:\n                # we might deserialize the object in a different node\n                # in this case, this object is not used,\n                # and we should suppress the error\n                pass",
      "language": "python"
    },
    {
      "code": "__reduce__()",
      "language": "unknown"
    },
    {
      "code": "__reduce__()",
      "language": "unknown"
    },
    {
      "code": "235\n236\n237\n238\n239",
      "language": "unknown"
    },
    {
      "code": "def __reduce__(self):\n    return (\n        self.__class__,\n        self.handle(),\n    )",
      "language": "python"
    },
    {
      "code": "def __reduce__(self):\n    return (\n        self.__class__,\n        self.handle(),\n    )",
      "language": "python"
    },
    {
      "code": "get_data(current_idx: int)",
      "language": "unknown"
    },
    {
      "code": "get_data(current_idx: int)",
      "language": "unknown"
    },
    {
      "code": "247\n248\n249\n250\n251\n252",
      "language": "unknown"
    },
    {
      "code": "@contextmanager\ndef get_data(self, current_idx: int):\n    start = self.data_offset + current_idx * self.max_chunk_bytes\n    end = start + self.max_chunk_bytes\n    with self.shared_memory.buf[start:end] as buf:\n        yield buf",
      "language": "python"
    },
    {
      "code": "@contextmanager\ndef get_data(self, current_idx: int):\n    start = self.data_offset + current_idx * self.max_chunk_bytes\n    end = start + self.max_chunk_bytes\n    with self.shared_memory.buf[start:end] as buf:\n        yield buf",
      "language": "python"
    },
    {
      "code": "get_metadata(current_idx: int)",
      "language": "unknown"
    },
    {
      "code": "get_metadata(current_idx: int)",
      "language": "unknown"
    },
    {
      "code": "254\n255\n256\n257\n258\n259",
      "language": "unknown"
    },
    {
      "code": "@contextmanager\ndef get_metadata(self, current_idx: int):\n    start = self.metadata_offset + current_idx * self.metadata_size\n    end = start + self.metadata_size\n    with self.shared_memory.buf[start:end] as buf:\n        yield buf",
      "language": "python"
    },
    {
      "code": "@contextmanager\ndef get_metadata(self, current_idx: int):\n    start = self.metadata_offset + current_idx * self.metadata_size\n    end = start + self.metadata_size\n    with self.shared_memory.buf[start:end] as buf:\n        yield buf",
      "language": "python"
    },
    {
      "code": "227\n228\n229\n230\n231\n232\n233",
      "language": "unknown"
    },
    {
      "code": "def handle(self):\n    return (\n        self.n_reader,\n        self.max_chunk_bytes,\n        self.max_chunks,\n        self.shared_memory.name,\n    )",
      "language": "python"
    },
    {
      "code": "def handle(self):\n    return (\n        self.n_reader,\n        self.max_chunk_bytes,\n        self.max_chunks,\n        self.shared_memory.name,\n    )",
      "language": "python"
    },
    {
      "code": "99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124",
      "language": "unknown"
    },
    {
      "code": "class SpinSleepTimer(SpinTimer):\n    \"\"\"\n    In setups which have long inactivity periods it is desirable to reduce\n    system power consumption when vllm does nothing. This would lead to more\n    CPU thermal headroom when a request eventually comes, especially when\n    multiple GPUs are connected as each GPU would otherwise pin one thread at\n    100% CPU usage.\n\n    The simplest solution is to reduce polling frequency when there is no\n    activity for a certain period of time.\n    \"\"\"\n\n    def __init__(self, busy_loop_s: float = 3.0, wait_sleep_s: float = 0.1):\n        self.last_activity = time.monotonic()\n        self.busy_loop_s = busy_loop_s\n        self.wait_sleep_s = wait_sleep_s\n\n    def record_activity(self):\n        self.last_activity = time.monotonic()\n\n    def spin(self):\n        curr_time = time.monotonic()\n        if curr_time >= self.last_activity + self.busy_loop_s:\n            time.sleep(self.wait_sleep_s)\n        else:\n            sched_yield()",
      "language": "python"
    },
    {
      "code": "class SpinSleepTimer(SpinTimer):\n    \"\"\"\n    In setups which have long inactivity periods it is desirable to reduce\n    system power consumption when vllm does nothing. This would lead to more\n    CPU thermal headroom when a request eventually comes, especially when\n    multiple GPUs are connected as each GPU would otherwise pin one thread at\n    100% CPU usage.\n\n    The simplest solution is to reduce polling frequency when there is no\n    activity for a certain period of time.\n    \"\"\"\n\n    def __init__(self, busy_loop_s: float = 3.0, wait_sleep_s: float = 0.1):\n        self.last_activity = time.monotonic()\n        self.busy_loop_s = busy_loop_s\n        self.wait_sleep_s = wait_sleep_s\n\n    def record_activity(self):\n        self.last_activity = time.monotonic()\n\n    def spin(self):\n        curr_time = time.monotonic()\n        if curr_time >= self.last_activity + self.busy_loop_s:\n            time.sleep(self.wait_sleep_s)\n        else:\n            sched_yield()",
      "language": "python"
    },
    {
      "code": "busy_loop_s = busy_loop_s",
      "language": "unknown"
    },
    {
      "code": "busy_loop_s = busy_loop_s",
      "language": "unknown"
    },
    {
      "code": "last_activity = monotonic()",
      "language": "unknown"
    },
    {
      "code": "last_activity = monotonic()",
      "language": "unknown"
    },
    {
      "code": "wait_sleep_s = wait_sleep_s",
      "language": "unknown"
    },
    {
      "code": "wait_sleep_s = wait_sleep_s",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    busy_loop_s: float = 3.0, wait_sleep_s: float = 0.1\n)",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    busy_loop_s: float = 3.0, wait_sleep_s: float = 0.1\n)",
      "language": "typescript"
    },
    {
      "code": "111\n112\n113\n114",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, busy_loop_s: float = 3.0, wait_sleep_s: float = 0.1):\n    self.last_activity = time.monotonic()\n    self.busy_loop_s = busy_loop_s\n    self.wait_sleep_s = wait_sleep_s",
      "language": "python"
    },
    {
      "code": "def __init__(self, busy_loop_s: float = 3.0, wait_sleep_s: float = 0.1):\n    self.last_activity = time.monotonic()\n    self.busy_loop_s = busy_loop_s\n    self.wait_sleep_s = wait_sleep_s",
      "language": "python"
    },
    {
      "code": "record_activity()",
      "language": "unknown"
    },
    {
      "code": "record_activity()",
      "language": "unknown"
    },
    {
      "code": "def record_activity(self):\n    self.last_activity = time.monotonic()",
      "language": "python"
    },
    {
      "code": "def record_activity(self):\n    self.last_activity = time.monotonic()",
      "language": "python"
    },
    {
      "code": "119\n120\n121\n122\n123\n124",
      "language": "unknown"
    },
    {
      "code": "def spin(self):\n    curr_time = time.monotonic()\n    if curr_time >= self.last_activity + self.busy_loop_s:\n        time.sleep(self.wait_sleep_s)\n    else:\n        sched_yield()",
      "language": "python"
    },
    {
      "code": "def spin(self):\n    curr_time = time.monotonic()\n    if curr_time >= self.last_activity + self.busy_loop_s:\n        time.sleep(self.wait_sleep_s)\n    else:\n        sched_yield()",
      "language": "python"
    },
    {
      "code": "91\n92\n93\n94\n95\n96",
      "language": "unknown"
    },
    {
      "code": "class SpinTimer:\n    def record_activity(self):\n        pass\n\n    def spin(self):\n        sched_yield()",
      "language": "python"
    },
    {
      "code": "class SpinTimer:\n    def record_activity(self):\n        pass\n\n    def spin(self):\n        sched_yield()",
      "language": "python"
    },
    {
      "code": "record_activity()",
      "language": "unknown"
    },
    {
      "code": "record_activity()",
      "language": "unknown"
    },
    {
      "code": "def record_activity(self):\n    pass",
      "language": "python"
    },
    {
      "code": "def record_activity(self):\n    pass",
      "language": "python"
    },
    {
      "code": "def spin(self):\n    sched_yield()",
      "language": "python"
    },
    {
      "code": "def spin(self):\n    sched_yield()",
      "language": "python"
    },
    {
      "code": "long_wait_time_msg(threshold: int) -> str",
      "language": "php"
    },
    {
      "code": "long_wait_time_msg(threshold: int) -> str",
      "language": "php"
    },
    {
      "code": "81\n82\n83\n84\n85\n86\n87\n88",
      "language": "unknown"
    },
    {
      "code": "def long_wait_time_msg(threshold: int) -> str:\n    return (\n        \"No available shared memory broadcast block found \"\n        f\"in {threshold} seconds. This typically happens \"\n        \"when some processes are hanging or doing some \"\n        \"time-consuming work (e.g. compilation, \"\n        \"weight/kv cache quantization).\"\n    )",
      "language": "python"
    },
    {
      "code": "def long_wait_time_msg(threshold: int) -> str:\n    return (\n        \"No available shared memory broadcast block found \"\n        f\"in {threshold} seconds. This typically happens \"\n        \"when some processes are hanging or doing some \"\n        \"time-consuming work (e.g. compilation, \"\n        \"weight/kv cache quantization).\"\n    )",
      "language": "python"
    },
    {
      "code": "memory_fence()",
      "language": "unknown"
    },
    {
      "code": "memory_fence()",
      "language": "unknown"
    },
    {
      "code": "53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71",
      "language": "unknown"
    },
    {
      "code": "def memory_fence():\n    \"\"\"\n    Full memory barrier for shared memory synchronization.\n\n    Ensures all prior memory writes are visible to other processes before\n    any subsequent reads. This is critical for lock-free producer-consumer\n    patterns using shared memory.\n\n    Implementation acquires and immediately releases a lock. Python's\n    threading.Lock provides sequentially consistent memory barrier semantics\n    across all major platforms (POSIX, Windows). This is a lightweight\n    operation (~20ns) that guarantees:\n    - All stores before the barrier are visible to other threads/processes\n    - All loads after the barrier see the latest values\n    \"\"\"\n    # Lock acquire/release provides full memory barrier semantics.\n    # Using context manager ensures lock release even on exceptions.\n    with _memory_fence_lock:\n        pass",
      "language": "python"
    },
    {
      "code": "def memory_fence():\n    \"\"\"\n    Full memory barrier for shared memory synchronization.\n\n    Ensures all prior memory writes are visible to other processes before\n    any subsequent reads. This is critical for lock-free producer-consumer\n    patterns using shared memory.\n\n    Implementation acquires and immediately releases a lock. Python's\n    threading.Lock provides sequentially consistent memory barrier semantics\n    across all major platforms (POSIX, Windows). This is a lightweight\n    operation (~20ns) that guarantees:\n    - All stores before the barrier are visible to other threads/processes\n    - All loads after the barrier see the latest values\n    \"\"\"\n    # Lock acquire/release provides full memory barrier semantics.\n    # Using context manager ensures lock release even on exceptions.\n    with _memory_fence_lock:\n        pass",
      "language": "python"
    },
    {
      "code": "to_bytes_big(value: int, size: int) -> bytes",
      "language": "php"
    },
    {
      "code": "to_bytes_big(value: int, size: int) -> bytes",
      "language": "php"
    },
    {
      "code": "def to_bytes_big(value: int, size: int) -> bytes:\n    return value.to_bytes(size, byteorder=\"big\")",
      "language": "python"
    },
    {
      "code": "def to_bytes_big(value: int, size: int) -> bytes:\n    return value.to_bytes(size, byteorder=\"big\")",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}