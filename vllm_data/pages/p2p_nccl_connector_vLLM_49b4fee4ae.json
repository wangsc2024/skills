{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
  "title": "p2p_nccl_connector - vLLM",
  "content": "Bases: KVConnectorBase_V1\n\nBuild the connector metadata for this step.\n\nThis function should NOT modify any fields in the scheduler_output. Also, calling this function will reset the state of the connector.\n\nthe scheduler output object.\n\nNotifies worker-side connector ids of requests that have finished generating tokens.\n\nids of requests that have finished asynchronous transfer,\n\ntuple of (sending/saving ids, recving/loading ids).\n\nThe finished saves/sends req ids must belong to a set provided in a\n\ncall to this method (this call or a prior one).\n\nGet number of new tokens that can be loaded from the external KV cache beyond the num_computed_tokens.\n\nthe number of locally computed tokens for this request\n\nthe number of tokens that can be loaded from the\n\nexternal KV cache beyond what is already computed.\n\nCalled when a request has finished, before its blocks are freed.\n\nTrue if the request is being saved/sent asynchronously and blocks\n\nshould not be freed until the request_id is returned from\n\nOptional KVTransferParams to be included in the request outputs\n\nreturned by the engine.\n\nStart saving the KV cache of the layer from vLLM's paged buffer to the connector.\n\nthe name of the layer.\n\nthe paged KV buffer of the current layer in vLLM.\n\nthe attention metadata.\n\nadditional arguments for the save operation.\n\nStart loading the KV cache from the connector buffer to vLLM's paged KV buffer.\n\nadditional arguments for the load operation\n\nThe number of elements in kv_caches and layer_names should be the same.\n\nUpdate KVConnector state after block allocation.\n\nBlocking until the KV for a specific layer is loaded into vLLM's paged buffer.\n\nThis interface will be useful for layer-by-layer pipelining.\n\nthe name of that layer\n\nBases: KVConnectorMetadata",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector"
    },
    {
      "level": "h2",
      "text": "logger module-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.logger"
    },
    {
      "level": "h2",
      "text": "P2pNcclConnector Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector"
    },
    {
      "level": "h3",
      "text": "_block_size instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector._block_size"
    },
    {
      "level": "h3",
      "text": "_local_rank instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector._local_rank"
    },
    {
      "level": "h3",
      "text": "_rank instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector._rank"
    },
    {
      "level": "h3",
      "text": "_requests_need_load instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector._requests_need_load"
    },
    {
      "level": "h3",
      "text": "chunked_prefill instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.chunked_prefill"
    },
    {
      "level": "h3",
      "text": "is_producer instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.is_producer"
    },
    {
      "level": "h3",
      "text": "p2p_nccl_engine instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.p2p_nccl_engine"
    },
    {
      "level": "h3",
      "text": "__init__ Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.__init__"
    },
    {
      "level": "h3",
      "text": "build_connector_meta Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.build_connector_meta"
    },
    {
      "level": "h3",
      "text": "check_tensors_except_dim staticmethod Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.check_tensors_except_dim"
    },
    {
      "level": "h3",
      "text": "get_finished Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.get_finished"
    },
    {
      "level": "h3",
      "text": "get_num_new_matched_tokens Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.get_num_new_matched_tokens"
    },
    {
      "level": "h3",
      "text": "parse_request_id staticmethod Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.parse_request_id"
    },
    {
      "level": "h3",
      "text": "request_finished Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.request_finished"
    },
    {
      "level": "h3",
      "text": "save_kv_layer Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.save_kv_layer"
    },
    {
      "level": "h3",
      "text": "start_load_kv Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.start_load_kv"
    },
    {
      "level": "h3",
      "text": "update_state_after_alloc Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.update_state_after_alloc"
    },
    {
      "level": "h3",
      "text": "wait_for_layer_load Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.wait_for_layer_load"
    },
    {
      "level": "h3",
      "text": "wait_for_save Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnector.wait_for_save"
    },
    {
      "level": "h2",
      "text": "P2pNcclConnectorMetadata dataclass Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnectorMetadata"
    },
    {
      "level": "h3",
      "text": "requests instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnectorMetadata.requests"
    },
    {
      "level": "h3",
      "text": "__init__ Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnectorMetadata.__init__"
    },
    {
      "level": "h3",
      "text": "add_request Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.P2pNcclConnectorMetadata.add_request"
    },
    {
      "level": "h2",
      "text": "ReqMeta dataclass Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.ReqMeta"
    },
    {
      "level": "h3",
      "text": "block_ids instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.ReqMeta.block_ids"
    },
    {
      "level": "h3",
      "text": "num_tokens instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.ReqMeta.num_tokens"
    },
    {
      "level": "h3",
      "text": "request_id instance-attribute Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.ReqMeta.request_id"
    },
    {
      "level": "h3",
      "text": "__init__ Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.ReqMeta.__init__"
    },
    {
      "level": "h3",
      "text": "make_meta staticmethod Â¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_connector.ReqMeta.make_meta"
    }
  ],
  "code_samples": [
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531",
      "language": "unknown"
    },
    {
      "code": "class P2pNcclConnector(KVConnectorBase_V1):\n    def __init__(\n        self,\n        vllm_config: \"VllmConfig\",\n        role: KVConnectorRole,\n        kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n    ):\n        super().__init__(\n            vllm_config=vllm_config,\n            role=role,\n            kv_cache_config=kv_cache_config,\n        )\n        self._block_size = vllm_config.cache_config.block_size\n        self._requests_need_load: dict[str, Any] = {}\n        self.is_producer = self._kv_transfer_config.is_kv_producer\n        self.chunked_prefill: dict[str, tuple[list[int], list[int] | None]] = {}\n\n        self._rank = get_world_group().rank if role == KVConnectorRole.WORKER else 0\n        self._local_rank = (\n            get_world_group().local_rank if role == KVConnectorRole.WORKER else 0\n        )\n\n        self.p2p_nccl_engine = (\n            P2pNcclEngine(\n                local_rank=self._local_rank,\n                config=self._kv_transfer_config,\n                hostname=\"\",\n                port_offset=self._rank,\n            )\n            if role == KVConnectorRole.WORKER\n            else None\n        )\n\n    # ==============================\n    # Worker-side methods\n    # ==============================\n\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n        \"\"\"Start loading the KV cache from the connector buffer to vLLM's\n        paged KV buffer.\n\n        Args:\n            forward_context (ForwardContext): the forward context.\n            **kwargs: additional arguments for the load operation\n\n        Note:\n            The number of elements in kv_caches and layer_names should be\n            the same.\n        \"\"\"\n\n        # Only consumer/decode loads KV Cache\n        if self.is_producer:\n            return\n\n        assert self.p2p_nccl_engine is not None\n\n        attn_metadata = forward_context.attn_metadata\n        if attn_metadata is None:\n            return\n\n        def inject_kv_into_layer(\n            layer: torch.Tensor,\n            kv_cache: torch.Tensor,\n            block_ids: torch.Tensor,\n            request_id: str,\n        ) -> None:\n            \"\"\"\n            Inject KV cache data into a given attention layer tensor.\n\n            This function updates `layer` in-place with values from `kv_cache`,\n            handling different backend layouts:\n              - MLA (Multi-Linear Attention) or FlashInfer: KV tensors are\n                indexed along the first dimension.\n              - FlashAttention: KV tensors are indexed along the second\n                dimension.\n\n            If the number of provided block IDs does not match the number of KV\n            blocks, only the overlapping portion is updated, and a warning is\n            logged.\n\n            Args:\n                layer (torch.Tensor): The attention layer KV tensor to update.\n                kv_cache (torch.Tensor): The KV cache tensor to inject.\n                block_ids (torch.Tensor): Indices of the blocks to update.\n                request_id (str): Request identifier used for logging.\n\n            Returns:\n                None. The function modifies `layer` in-place.\n            \"\"\"\n            if (\n                isinstance(attn_metadata, MLACommonMetadata) or layer.shape[1] == 2\n            ):  # MLA or FlashInfer\n                num_block = kv_cache.shape[0]\n                self.check_tensors_except_dim(layer, kv_cache, 0)\n                if len(block_ids) == num_block:\n                    layer[block_ids, ...] = kv_cache\n                else:\n                    layer[block_ids[:num_block], ...] = kv_cache\n                    logger.warning(\n                        \"ðŸš§kv_cache does not match, block_ids:%d, \"\n                        \"num_block:%d, request_id:%s\",\n                        len(block_ids),\n                        num_block,\n                        request_id,\n                    )\n\n            elif layer.shape[0] == 2:  # FlashAttention\n                num_block = kv_cache.shape[1]\n                self.check_tensors_except_dim(layer, kv_cache, 1)\n                if len(block_ids) == num_block:\n                    layer[:, block_ids, ...] = kv_cache\n                else:\n                    layer[:, block_ids[:num_block], ...] = kv_cache\n                    logger.warning(\n                        \"ðŸš§kv_cache does not match, block_ids:%d, \"\n                        \"num_block:%d, request_id:%s\",\n                        len(block_ids),\n                        num_block,\n                        request_id,\n                    )\n\n        # Get the metadata\n        metadata: KVConnectorMetadata = self._get_connector_metadata()\n        assert isinstance(metadata, P2pNcclConnectorMetadata)\n\n        if metadata is None:\n            return\n\n        # Load the KV for each request each layer\n        for request in metadata.requests:\n            request_id = request.request_id\n            ip, port = self.parse_request_id(request_id, False)\n            remote_address = ip + \":\" + str(port + self._rank)\n            for layer_name in forward_context.no_compile_layers:\n                layer = forward_context.no_compile_layers[layer_name]\n\n                # Only process layers that have kv_cache\n                # attribute (attention layers) Skip non-attention\n                # layers like FusedMoE\n                kv_cache = getattr(layer, \"kv_cache\", None)\n                if kv_cache is None:\n                    continue\n\n                layer = kv_cache[forward_context.virtual_engine]\n\n                kv_cache = self.p2p_nccl_engine.recv_tensor(\n                    request.request_id + \"#\" + layer_name, remote_address\n                )\n\n                if kv_cache is None:\n                    logger.warning(\"ðŸš§kv_cache is None, %s\", request.request_id)\n                    continue\n\n                inject_kv_into_layer(\n                    layer, kv_cache, request.block_ids, request.request_id\n                )\n\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n        paged buffer.\n\n        This interface will be useful for layer-by-layer pipelining.\n\n        Args:\n            layer_name: the name of that layer\n        \"\"\"\n        return\n\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Start saving the KV cache of the layer from vLLM's paged buffer\n        to the connector.\n\n        Args:\n            layer_name (str): the name of the layer.\n            kv_layer (torch.Tensor): the paged KV buffer of the current\n                layer in vLLM.\n            attn_metadata (AttentionMetadata): the attention metadata.\n            **kwargs: additional arguments for the save operation.\n        \"\"\"\n\n        # Only producer/prefill saves KV Cache\n        if not self.is_producer:\n            return\n\n        assert self.p2p_nccl_engine is not None\n\n        def extract_kv_from_layer(\n            layer: torch.Tensor,\n            block_ids: torch.Tensor,\n        ) -> torch.Tensor:\n            \"\"\"\n            Extract KV cache slices from a given attention layer tensor.\n\n            This function handles multiple backend layouts:\n              - MLA (Multi-Linear Attention) or FlashInfer: KV tensors are\n                indexed along the first dimension.\n              - FlashAttention: KV tensors are indexed along the second\n                dimension.\n\n            Args:\n                layer (torch.Tensor): The KV cache from the attention layer.\n                block_ids (torch.Tensor): Indices of blocks to extract.\n\n            Returns:\n                torch.Tensor: A tensor containing the extracted KV slices.\n                Returns None if the layout is unsupported.\n            \"\"\"\n            if (\n                isinstance(attn_metadata, MLACommonMetadata) or layer.shape[1] == 2\n            ):  # MLA or FlashInfer\n                return layer[block_ids, ...]\n\n            if layer.shape[0] == 2:  # FlashAttention\n                return layer[:, block_ids, ...]\n\n            return None\n\n        connector_metadata = self._get_connector_metadata()\n        assert isinstance(connector_metadata, P2pNcclConnectorMetadata)\n        for request in connector_metadata.requests:\n            request_id = request.request_id\n            ip, port = self.parse_request_id(request_id, True)\n            remote_address = ip + \":\" + str(port + self._rank)\n\n            kv_cache = extract_kv_from_layer(kv_layer, request.block_ids)\n            self.p2p_nccl_engine.send_tensor(\n                request_id + \"#\" + layer_name, kv_cache, remote_address\n            )\n\n    def wait_for_save(self):\n        if self.is_producer:\n            assert self.p2p_nccl_engine is not None\n            self.p2p_nccl_engine.wait_for_sent()\n\n    def get_finished(\n        self, finished_req_ids: set[str], **kwargs: Any\n    ) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"\n        Notifies worker-side connector ids of requests that have\n        finished generating tokens.\n\n        Returns:\n            ids of requests that have finished asynchronous transfer,\n            tuple of (sending/saving ids, recving/loading ids).\n            The finished saves/sends req ids must belong to a set provided in a\n            call to this method (this call or a prior one).\n        \"\"\"\n\n        assert self.p2p_nccl_engine is not None\n\n        no_compile_layers = self._vllm_config.compilation_config.static_forward_context\n        return self.p2p_nccl_engine.get_finished(finished_req_ids, no_compile_layers)\n\n    # ==============================\n    # Scheduler-side methods\n    # ==============================\n\n    def get_num_new_matched_tokens(\n        self,\n        request: \"Request\",\n        num_computed_tokens: int,\n    ) -> tuple[int, bool]:\n        \"\"\"\n        Get number of new tokens that can be loaded from the\n        external KV cache beyond the num_computed_tokens.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n\n        Returns:\n            the number of tokens that can be loaded from the\n            external KV cache beyond what is already computed.\n        \"\"\"\n        if self.is_producer:\n            return 0, False\n\n        prompt_token_ids = request.prompt_token_ids or []\n        num_external_tokens = len(prompt_token_ids) - 1 - num_computed_tokens\n\n        if num_external_tokens < 0:\n            num_external_tokens = 0\n\n        return num_external_tokens, False\n\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        \"\"\"\n        Update KVConnector state after block allocation.\n        \"\"\"\n        if not self.is_producer and num_external_tokens > 0:\n            self._requests_need_load[request.request_id] = (\n                request,\n                blocks.get_block_ids()[0],\n            )\n\n    def build_connector_meta(\n        self,\n        scheduler_output: SchedulerOutput,\n    ) -> KVConnectorMetadata:\n        \"\"\"Build the connector metadata for this step.\n\n        This function should NOT modify any fields in the scheduler_output.\n        Also, calling this function will reset the state of the connector.\n\n        Args:\n            scheduler_output (SchedulerOutput): the scheduler output object.\n        \"\"\"\n\n        meta = P2pNcclConnectorMetadata()\n\n        for new_req in scheduler_output.scheduled_new_reqs:\n            if self.is_producer:\n                num_scheduled_tokens = (scheduler_output.num_scheduled_tokens)[\n                    new_req.req_id\n                ]\n                num_tokens = num_scheduled_tokens + new_req.num_computed_tokens\n                # the request's prompt is chunked prefill\n                if num_tokens < len(new_req.prompt_token_ids or []):\n                    # 'CachedRequestData' has no attribute 'prompt_token_ids'\n                    self.chunked_prefill[new_req.req_id] = (\n                        new_req.block_ids[0],\n                        new_req.prompt_token_ids,\n                    )\n                    continue\n                # the request's prompt is not chunked prefill\n                meta.add_request(\n                    request_id=new_req.req_id,\n                    token_ids=new_req.prompt_token_ids or [],\n                    block_ids=new_req.block_ids[0],\n                    block_size=self._block_size,\n                )\n                continue\n            if new_req.req_id in self._requests_need_load:\n                meta.add_request(\n                    request_id=new_req.req_id,\n                    token_ids=new_req.prompt_token_ids or [],\n                    block_ids=new_req.block_ids[0],\n                    block_size=self._block_size,\n                )\n                self._requests_need_load.pop(new_req.req_id)\n\n        cached_reqs = scheduler_output.scheduled_cached_reqs\n        for i, req_id in enumerate(cached_reqs.req_ids):\n            num_computed_tokens = cached_reqs.num_computed_tokens[i]\n            new_block_ids = cached_reqs.new_block_ids[i]\n            resumed_from_preemption = req_id in cached_reqs.resumed_req_ids\n\n            if self.is_producer:\n                num_scheduled_tokens = scheduler_output.num_scheduled_tokens[req_id]\n                num_tokens = num_scheduled_tokens + num_computed_tokens\n                assert req_id in self.chunked_prefill\n                assert new_block_ids is not None\n                block_ids = new_block_ids[0]\n                if not resumed_from_preemption:\n                    block_ids = self.chunked_prefill[req_id][0] + block_ids\n                prompt_token_ids = self.chunked_prefill[req_id][1]\n                assert prompt_token_ids is not None\n                # the request's prompt is chunked prefill again\n                if num_tokens < len(prompt_token_ids):\n                    self.chunked_prefill[req_id] = (block_ids, prompt_token_ids)\n                    continue\n                # the request's prompt is all prefilled finally\n                meta.add_request(\n                    request_id=req_id,\n                    token_ids=prompt_token_ids,\n                    block_ids=block_ids,\n                    block_size=self._block_size,\n                )\n                self.chunked_prefill.pop(req_id, None)\n                continue\n\n            # NOTE(rob): here we rely on the resumed requests being\n            # the first N requests in the list scheduled_cache_reqs.\n            if not resumed_from_preemption:\n                break\n            if req_id in self._requests_need_load:\n                request, _ = self._requests_need_load.pop(req_id)\n                total_tokens = num_computed_tokens + 1\n                token_ids = request.all_token_ids[:total_tokens]\n\n                # NOTE(rob): For resumed req, new_block_ids is all\n                # of the block_ids for the request.\n                assert new_block_ids is not None\n                block_ids = new_block_ids[0]\n\n                meta.add_request(\n                    request_id=req_id,\n                    token_ids=token_ids,\n                    block_ids=block_ids,\n                    block_size=self._block_size,\n                )\n\n        self._requests_need_load.clear()\n        return meta\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Called when a request has finished, before its blocks are freed.\n\n        Returns:\n            True if the request is being saved/sent asynchronously and blocks\n            should not be freed until the request_id is returned from\n            get_finished().\n            Optional KVTransferParams to be included in the request outputs\n            returned by the engine.\n        \"\"\"\n\n        self.chunked_prefill.pop(request.request_id, None)\n\n        return False, None\n\n    # ==============================\n    # Static methods\n    # ==============================\n\n    @staticmethod\n    def parse_request_id(request_id: str, is_prefill=True) -> tuple[str, int]:\n        # Regular expression to match the string hostname and integer port\n        if is_prefill:\n            pattern = r\"___decode_addr_(.*):(\\d+)\"\n        else:\n            pattern = r\"___prefill_addr_(.*):(\\d+)___\"\n\n        # Use re.search to find the pattern in the request_id\n        match = re.search(pattern, request_id)\n        if match:\n            # Extract the ranks\n            ip = match.group(1)\n            port = int(match.group(2))\n\n            return ip, port\n        raise ValueError(f\"Request id {request_id} does not contain hostname and port\")\n\n    @staticmethod\n    def check_tensors_except_dim(tensor1, tensor2, dim):\n        shape1 = tensor1.size()\n        shape2 = tensor2.size()\n\n        if len(shape1) != len(shape2) or not all(\n            s1 == s2 for i, (s1, s2) in enumerate(zip(shape1, shape2)) if i != dim\n        ):\n            raise NotImplementedError(\n                \"Currently, only symmetric TP is supported. Asymmetric TP, PP,\"\n                \"and others will be supported in future PRs.\"\n            )",
      "language": "python"
    },
    {
      "code": "class P2pNcclConnector(KVConnectorBase_V1):\n    def __init__(\n        self,\n        vllm_config: \"VllmConfig\",\n        role: KVConnectorRole,\n        kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n    ):\n        super().__init__(\n            vllm_config=vllm_config,\n            role=role,\n            kv_cache_config=kv_cache_config,\n        )\n        self._block_size = vllm_config.cache_config.block_size\n        self._requests_need_load: dict[str, Any] = {}\n        self.is_producer = self._kv_transfer_config.is_kv_producer\n        self.chunked_prefill: dict[str, tuple[list[int], list[int] | None]] = {}\n\n        self._rank = get_world_group().rank if role == KVConnectorRole.WORKER else 0\n        self._local_rank = (\n            get_world_group().local_rank if role == KVConnectorRole.WORKER else 0\n        )\n\n        self.p2p_nccl_engine = (\n            P2pNcclEngine(\n                local_rank=self._local_rank,\n                config=self._kv_transfer_config,\n                hostname=\"\",\n                port_offset=self._rank,\n            )\n            if role == KVConnectorRole.WORKER\n            else None\n        )\n\n    # ==============================\n    # Worker-side methods\n    # ==============================\n\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n        \"\"\"Start loading the KV cache from the connector buffer to vLLM's\n        paged KV buffer.\n\n        Args:\n            forward_context (ForwardContext): the forward context.\n            **kwargs: additional arguments for the load operation\n\n        Note:\n            The number of elements in kv_caches and layer_names should be\n            the same.\n        \"\"\"\n\n        # Only consumer/decode loads KV Cache\n        if self.is_producer:\n            return\n\n        assert self.p2p_nccl_engine is not None\n\n        attn_metadata = forward_context.attn_metadata\n        if attn_metadata is None:\n            return\n\n        def inject_kv_into_layer(\n            layer: torch.Tensor,\n            kv_cache: torch.Tensor,\n            block_ids: torch.Tensor,\n            request_id: str,\n        ) -> None:\n            \"\"\"\n            Inject KV cache data into a given attention layer tensor.\n\n            This function updates `layer` in-place with values from `kv_cache`,\n            handling different backend layouts:\n              - MLA (Multi-Linear Attention) or FlashInfer: KV tensors are\n                indexed along the first dimension.\n              - FlashAttention: KV tensors are indexed along the second\n                dimension.\n\n            If the number of provided block IDs does not match the number of KV\n            blocks, only the overlapping portion is updated, and a warning is\n            logged.\n\n            Args:\n                layer (torch.Tensor): The attention layer KV tensor to update.\n                kv_cache (torch.Tensor): The KV cache tensor to inject.\n                block_ids (torch.Tensor): Indices of the blocks to update.\n                request_id (str): Request identifier used for logging.\n\n            Returns:\n                None. The function modifies `layer` in-place.\n            \"\"\"\n            if (\n                isinstance(attn_metadata, MLACommonMetadata) or layer.shape[1] == 2\n            ):  # MLA or FlashInfer\n                num_block = kv_cache.shape[0]\n                self.check_tensors_except_dim(layer, kv_cache, 0)\n                if len(block_ids) == num_block:\n                    layer[block_ids, ...] = kv_cache\n                else:\n                    layer[block_ids[:num_block], ...] = kv_cache\n                    logger.warning(\n                        \"ðŸš§kv_cache does not match, block_ids:%d, \"\n                        \"num_block:%d, request_id:%s\",\n                        len(block_ids),\n                        num_block,\n                        request_id,\n                    )\n\n            elif layer.shape[0] == 2:  # FlashAttention\n                num_block = kv_cache.shape[1]\n                self.check_tensors_except_dim(layer, kv_cache, 1)\n                if len(block_ids) == num_block:\n                    layer[:, block_ids, ...] = kv_cache\n                else:\n                    layer[:, block_ids[:num_block], ...] = kv_cache\n                    logger.warning(\n                        \"ðŸš§kv_cache does not match, block_ids:%d, \"\n                        \"num_block:%d, request_id:%s\",\n                        len(block_ids),\n                        num_block,\n                        request_id,\n                    )\n\n        # Get the metadata\n        metadata: KVConnectorMetadata = self._get_connector_metadata()\n        assert isinstance(metadata, P2pNcclConnectorMetadata)\n\n        if metadata is None:\n            return\n\n        # Load the KV for each request each layer\n        for request in metadata.requests:\n            request_id = request.request_id\n            ip, port = self.parse_request_id(request_id, False)\n            remote_address = ip + \":\" + str(port + self._rank)\n            for layer_name in forward_context.no_compile_layers:\n                layer = forward_context.no_compile_layers[layer_name]\n\n                # Only process layers that have kv_cache\n                # attribute (attention layers) Skip non-attention\n                # layers like FusedMoE\n                kv_cache = getattr(layer, \"kv_cache\", None)\n                if kv_cache is None:\n                    continue\n\n                layer = kv_cache[forward_context.virtual_engine]\n\n                kv_cache = self.p2p_nccl_engine.recv_tensor(\n                    request.request_id + \"#\" + layer_name, remote_address\n                )\n\n                if kv_cache is None:\n                    logger.warning(\"ðŸš§kv_cache is None, %s\", request.request_id)\n                    continue\n\n                inject_kv_into_layer(\n                    layer, kv_cache, request.block_ids, request.request_id\n                )\n\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n        paged buffer.\n\n        This interface will be useful for layer-by-layer pipelining.\n\n        Args:\n            layer_name: the name of that layer\n        \"\"\"\n        return\n\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        **kwargs: Any,\n    ) -> None:\n        \"\"\"Start saving the KV cache of the layer from vLLM's paged buffer\n        to the connector.\n\n        Args:\n            layer_name (str): the name of the layer.\n            kv_layer (torch.Tensor): the paged KV buffer of the current\n                layer in vLLM.\n            attn_metadata (AttentionMetadata): the attention metadata.\n            **kwargs: additional arguments for the save operation.\n        \"\"\"\n\n        # Only producer/prefill saves KV Cache\n        if not self.is_producer:\n            return\n\n        assert self.p2p_nccl_engine is not None\n\n        def extract_kv_from_layer(\n            layer: torch.Tensor,\n            block_ids: torch.Tensor,\n        ) -> torch.Tensor:\n            \"\"\"\n            Extract KV cache slices from a given attention layer tensor.\n\n            This function handles multiple backend layouts:\n              - MLA (Multi-Linear Attention) or FlashInfer: KV tensors are\n                indexed along the first dimension.\n              - FlashAttention: KV tensors are indexed along the second\n                dimension.\n\n            Args:\n                layer (torch.Tensor): The KV cache from the attention layer.\n                block_ids (torch.Tensor): Indices of blocks to extract.\n\n            Returns:\n                torch.Tensor: A tensor containing the extracted KV slices.\n                Returns None if the layout is unsupported.\n            \"\"\"\n            if (\n                isinstance(attn_metadata, MLACommonMetadata) or layer.shape[1] == 2\n            ):  # MLA or FlashInfer\n                return layer[block_ids, ...]\n\n            if layer.shape[0] == 2:  # FlashAttention\n                return layer[:, block_ids, ...]\n\n            return None\n\n        connector_metadata = self._get_connector_metadata()\n        assert isinstance(connector_metadata, P2pNcclConnectorMetadata)\n        for request in connector_metadata.requests:\n            request_id = request.request_id\n            ip, port = self.parse_request_id(request_id, True)\n            remote_address = ip + \":\" + str(port + self._rank)\n\n            kv_cache = extract_kv_from_layer(kv_layer, request.block_ids)\n            self.p2p_nccl_engine.send_tensor(\n                request_id + \"#\" + layer_name, kv_cache, remote_address\n            )\n\n    def wait_for_save(self):\n        if self.is_producer:\n            assert self.p2p_nccl_engine is not None\n            self.p2p_nccl_engine.wait_for_sent()\n\n    def get_finished(\n        self, finished_req_ids: set[str], **kwargs: Any\n    ) -> tuple[set[str] | None, set[str] | None]:\n        \"\"\"\n        Notifies worker-side connector ids of requests that have\n        finished generating tokens.\n\n        Returns:\n            ids of requests that have finished asynchronous transfer,\n            tuple of (sending/saving ids, recving/loading ids).\n            The finished saves/sends req ids must belong to a set provided in a\n            call to this method (this call or a prior one).\n        \"\"\"\n\n        assert self.p2p_nccl_engine is not None\n\n        no_compile_layers = self._vllm_config.compilation_config.static_forward_context\n        return self.p2p_nccl_engine.get_finished(finished_req_ids, no_compile_layers)\n\n    # ==============================\n    # Scheduler-side methods\n    # ==============================\n\n    def get_num_new_matched_tokens(\n        self,\n        request: \"Request\",\n        num_computed_tokens: int,\n    ) -> tuple[int, bool]:\n        \"\"\"\n        Get number of new tokens that can be loaded from the\n        external KV cache beyond the num_computed_tokens.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n\n        Returns:\n            the number of tokens that can be loaded from the\n            external KV cache beyond what is already computed.\n        \"\"\"\n        if self.is_producer:\n            return 0, False\n\n        prompt_token_ids = request.prompt_token_ids or []\n        num_external_tokens = len(prompt_token_ids) - 1 - num_computed_tokens\n\n        if num_external_tokens < 0:\n            num_external_tokens = 0\n\n        return num_external_tokens, False\n\n    def update_state_after_alloc(\n        self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n    ):\n        \"\"\"\n        Update KVConnector state after block allocation.\n        \"\"\"\n        if not self.is_producer and num_external_tokens > 0:\n            self._requests_need_load[request.request_id] = (\n                request,\n                blocks.get_block_ids()[0],\n            )\n\n    def build_connector_meta(\n        self,\n        scheduler_output: SchedulerOutput,\n    ) -> KVConnectorMetadata:\n        \"\"\"Build the connector metadata for this step.\n\n        This function should NOT modify any fields in the scheduler_output.\n        Also, calling this function will reset the state of the connector.\n\n        Args:\n            scheduler_output (SchedulerOutput): the scheduler output object.\n        \"\"\"\n\n        meta = P2pNcclConnectorMetadata()\n\n        for new_req in scheduler_output.scheduled_new_reqs:\n            if self.is_producer:\n                num_scheduled_tokens = (scheduler_output.num_scheduled_tokens)[\n                    new_req.req_id\n                ]\n                num_tokens = num_scheduled_tokens + new_req.num_computed_tokens\n                # the request's prompt is chunked prefill\n                if num_tokens < len(new_req.prompt_token_ids or []):\n                    # 'CachedRequestData' has no attribute 'prompt_token_ids'\n                    self.chunked_prefill[new_req.req_id] = (\n                        new_req.block_ids[0],\n                        new_req.prompt_token_ids,\n                    )\n                    continue\n                # the request's prompt is not chunked prefill\n                meta.add_request(\n                    request_id=new_req.req_id,\n                    token_ids=new_req.prompt_token_ids or [],\n                    block_ids=new_req.block_ids[0],\n                    block_size=self._block_size,\n                )\n                continue\n            if new_req.req_id in self._requests_need_load:\n                meta.add_request(\n                    request_id=new_req.req_id,\n                    token_ids=new_req.prompt_token_ids or [],\n                    block_ids=new_req.block_ids[0],\n                    block_size=self._block_size,\n                )\n                self._requests_need_load.pop(new_req.req_id)\n\n        cached_reqs = scheduler_output.scheduled_cached_reqs\n        for i, req_id in enumerate(cached_reqs.req_ids):\n            num_computed_tokens = cached_reqs.num_computed_tokens[i]\n            new_block_ids = cached_reqs.new_block_ids[i]\n            resumed_from_preemption = req_id in cached_reqs.resumed_req_ids\n\n            if self.is_producer:\n                num_scheduled_tokens = scheduler_output.num_scheduled_tokens[req_id]\n                num_tokens = num_scheduled_tokens + num_computed_tokens\n                assert req_id in self.chunked_prefill\n                assert new_block_ids is not None\n                block_ids = new_block_ids[0]\n                if not resumed_from_preemption:\n                    block_ids = self.chunked_prefill[req_id][0] + block_ids\n                prompt_token_ids = self.chunked_prefill[req_id][1]\n                assert prompt_token_ids is not None\n                # the request's prompt is chunked prefill again\n                if num_tokens < len(prompt_token_ids):\n                    self.chunked_prefill[req_id] = (block_ids, prompt_token_ids)\n                    continue\n                # the request's prompt is all prefilled finally\n                meta.add_request(\n                    request_id=req_id,\n                    token_ids=prompt_token_ids,\n                    block_ids=block_ids,\n                    block_size=self._block_size,\n                )\n                self.chunked_prefill.pop(req_id, None)\n                continue\n\n            # NOTE(rob): here we rely on the resumed requests being\n            # the first N requests in the list scheduled_cache_reqs.\n            if not resumed_from_preemption:\n                break\n            if req_id in self._requests_need_load:\n                request, _ = self._requests_need_load.pop(req_id)\n                total_tokens = num_computed_tokens + 1\n                token_ids = request.all_token_ids[:total_tokens]\n\n                # NOTE(rob): For resumed req, new_block_ids is all\n                # of the block_ids for the request.\n                assert new_block_ids is not None\n                block_ids = new_block_ids[0]\n\n                meta.add_request(\n                    request_id=req_id,\n                    token_ids=token_ids,\n                    block_ids=block_ids,\n                    block_size=self._block_size,\n                )\n\n        self._requests_need_load.clear()\n        return meta\n\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        \"\"\"\n        Called when a request has finished, before its blocks are freed.\n\n        Returns:\n            True if the request is being saved/sent asynchronously and blocks\n            should not be freed until the request_id is returned from\n            get_finished().\n            Optional KVTransferParams to be included in the request outputs\n            returned by the engine.\n        \"\"\"\n\n        self.chunked_prefill.pop(request.request_id, None)\n\n        return False, None\n\n    # ==============================\n    # Static methods\n    # ==============================\n\n    @staticmethod\n    def parse_request_id(request_id: str, is_prefill=True) -> tuple[str, int]:\n        # Regular expression to match the string hostname and integer port\n        if is_prefill:\n            pattern = r\"___decode_addr_(.*):(\\d+)\"\n        else:\n            pattern = r\"___prefill_addr_(.*):(\\d+)___\"\n\n        # Use re.search to find the pattern in the request_id\n        match = re.search(pattern, request_id)\n        if match:\n            # Extract the ranks\n            ip = match.group(1)\n            port = int(match.group(2))\n\n            return ip, port\n        raise ValueError(f\"Request id {request_id} does not contain hostname and port\")\n\n    @staticmethod\n    def check_tensors_except_dim(tensor1, tensor2, dim):\n        shape1 = tensor1.size()\n        shape2 = tensor2.size()\n\n        if len(shape1) != len(shape2) or not all(\n            s1 == s2 for i, (s1, s2) in enumerate(zip(shape1, shape2)) if i != dim\n        ):\n            raise NotImplementedError(\n                \"Currently, only symmetric TP is supported. Asymmetric TP, PP,\"\n                \"and others will be supported in future PRs.\"\n            )",
      "language": "python"
    },
    {
      "code": "_block_size = block_size",
      "language": "unknown"
    },
    {
      "code": "_block_size = block_size",
      "language": "unknown"
    },
    {
      "code": "_local_rank = local_rank if role == WORKER else 0",
      "language": "unknown"
    },
    {
      "code": "_local_rank = local_rank if role == WORKER else 0",
      "language": "unknown"
    },
    {
      "code": "_rank = rank if role == WORKER else 0",
      "language": "unknown"
    },
    {
      "code": "_rank = rank if role == WORKER else 0",
      "language": "unknown"
    },
    {
      "code": "_requests_need_load: dict[str, Any] = {}",
      "language": "yaml"
    },
    {
      "code": "_requests_need_load: dict[str, Any] = {}",
      "language": "yaml"
    },
    {
      "code": "chunked_prefill: dict[\n    str, tuple[list[int], list[int] | None]\n] = {}",
      "language": "yaml"
    },
    {
      "code": "chunked_prefill: dict[\n    str, tuple[list[int], list[int] | None]\n] = {}",
      "language": "yaml"
    },
    {
      "code": "is_producer = is_kv_producer",
      "language": "unknown"
    },
    {
      "code": "is_producer = is_kv_producer",
      "language": "unknown"
    },
    {
      "code": "p2p_nccl_engine = (\n    P2pNcclEngine(\n        local_rank=_local_rank,\n        config=_kv_transfer_config,\n        hostname=\"\",\n        port_offset=_rank,\n    )\n    if role == WORKER\n    else None\n)",
      "language": "rust"
    },
    {
      "code": "p2p_nccl_engine = (\n    P2pNcclEngine(\n        local_rank=_local_rank,\n        config=_kv_transfer_config,\n        hostname=\"\",\n        port_offset=_rank,\n    )\n    if role == WORKER\n    else None\n)",
      "language": "rust"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n)",
      "language": "python"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    kv_cache_config: Optional[KVCacheConfig] = None,\n)",
      "language": "python"
    },
    {
      "code": "75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: \"VllmConfig\",\n    role: KVConnectorRole,\n    kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n):\n    super().__init__(\n        vllm_config=vllm_config,\n        role=role,\n        kv_cache_config=kv_cache_config,\n    )\n    self._block_size = vllm_config.cache_config.block_size\n    self._requests_need_load: dict[str, Any] = {}\n    self.is_producer = self._kv_transfer_config.is_kv_producer\n    self.chunked_prefill: dict[str, tuple[list[int], list[int] | None]] = {}\n\n    self._rank = get_world_group().rank if role == KVConnectorRole.WORKER else 0\n    self._local_rank = (\n        get_world_group().local_rank if role == KVConnectorRole.WORKER else 0\n    )\n\n    self.p2p_nccl_engine = (\n        P2pNcclEngine(\n            local_rank=self._local_rank,\n            config=self._kv_transfer_config,\n            hostname=\"\",\n            port_offset=self._rank,\n        )\n        if role == KVConnectorRole.WORKER\n        else None\n    )",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: \"VllmConfig\",\n    role: KVConnectorRole,\n    kv_cache_config: Optional[\"KVCacheConfig\"] = None,\n):\n    super().__init__(\n        vllm_config=vllm_config,\n        role=role,\n        kv_cache_config=kv_cache_config,\n    )\n    self._block_size = vllm_config.cache_config.block_size\n    self._requests_need_load: dict[str, Any] = {}\n    self.is_producer = self._kv_transfer_config.is_kv_producer\n    self.chunked_prefill: dict[str, tuple[list[int], list[int] | None]] = {}\n\n    self._rank = get_world_group().rank if role == KVConnectorRole.WORKER else 0\n    self._local_rank = (\n        get_world_group().local_rank if role == KVConnectorRole.WORKER else 0\n    )\n\n    self.p2p_nccl_engine = (\n        P2pNcclEngine(\n            local_rank=self._local_rank,\n            config=self._kv_transfer_config,\n            hostname=\"\",\n            port_offset=self._rank,\n        )\n        if role == KVConnectorRole.WORKER\n        else None\n    )",
      "language": "python"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476",
      "language": "unknown"
    },
    {
      "code": "def build_connector_meta(\n    self,\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata:\n    \"\"\"Build the connector metadata for this step.\n\n    This function should NOT modify any fields in the scheduler_output.\n    Also, calling this function will reset the state of the connector.\n\n    Args:\n        scheduler_output (SchedulerOutput): the scheduler output object.\n    \"\"\"\n\n    meta = P2pNcclConnectorMetadata()\n\n    for new_req in scheduler_output.scheduled_new_reqs:\n        if self.is_producer:\n            num_scheduled_tokens = (scheduler_output.num_scheduled_tokens)[\n                new_req.req_id\n            ]\n            num_tokens = num_scheduled_tokens + new_req.num_computed_tokens\n            # the request's prompt is chunked prefill\n            if num_tokens < len(new_req.prompt_token_ids or []):\n                # 'CachedRequestData' has no attribute 'prompt_token_ids'\n                self.chunked_prefill[new_req.req_id] = (\n                    new_req.block_ids[0],\n                    new_req.prompt_token_ids,\n                )\n                continue\n            # the request's prompt is not chunked prefill\n            meta.add_request(\n                request_id=new_req.req_id,\n                token_ids=new_req.prompt_token_ids or [],\n                block_ids=new_req.block_ids[0],\n                block_size=self._block_size,\n            )\n            continue\n        if new_req.req_id in self._requests_need_load:\n            meta.add_request(\n                request_id=new_req.req_id,\n                token_ids=new_req.prompt_token_ids or [],\n                block_ids=new_req.block_ids[0],\n                block_size=self._block_size,\n            )\n            self._requests_need_load.pop(new_req.req_id)\n\n    cached_reqs = scheduler_output.scheduled_cached_reqs\n    for i, req_id in enumerate(cached_reqs.req_ids):\n        num_computed_tokens = cached_reqs.num_computed_tokens[i]\n        new_block_ids = cached_reqs.new_block_ids[i]\n        resumed_from_preemption = req_id in cached_reqs.resumed_req_ids\n\n        if self.is_producer:\n            num_scheduled_tokens = scheduler_output.num_scheduled_tokens[req_id]\n            num_tokens = num_scheduled_tokens + num_computed_tokens\n            assert req_id in self.chunked_prefill\n            assert new_block_ids is not None\n            block_ids = new_block_ids[0]\n            if not resumed_from_preemption:\n                block_ids = self.chunked_prefill[req_id][0] + block_ids\n            prompt_token_ids = self.chunked_prefill[req_id][1]\n            assert prompt_token_ids is not None\n            # the request's prompt is chunked prefill again\n            if num_tokens < len(prompt_token_ids):\n                self.chunked_prefill[req_id] = (block_ids, prompt_token_ids)\n                continue\n            # the request's prompt is all prefilled finally\n            meta.add_request(\n                request_id=req_id,\n                token_ids=prompt_token_ids,\n                block_ids=block_ids,\n                block_size=self._block_size,\n            )\n            self.chunked_prefill.pop(req_id, None)\n            continue\n\n        # NOTE(rob): here we rely on the resumed requests being\n        # the first N requests in the list scheduled_cache_reqs.\n        if not resumed_from_preemption:\n            break\n        if req_id in self._requests_need_load:\n            request, _ = self._requests_need_load.pop(req_id)\n            total_tokens = num_computed_tokens + 1\n            token_ids = request.all_token_ids[:total_tokens]\n\n            # NOTE(rob): For resumed req, new_block_ids is all\n            # of the block_ids for the request.\n            assert new_block_ids is not None\n            block_ids = new_block_ids[0]\n\n            meta.add_request(\n                request_id=req_id,\n                token_ids=token_ids,\n                block_ids=block_ids,\n                block_size=self._block_size,\n            )\n\n    self._requests_need_load.clear()\n    return meta",
      "language": "python"
    },
    {
      "code": "def build_connector_meta(\n    self,\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata:\n    \"\"\"Build the connector metadata for this step.\n\n    This function should NOT modify any fields in the scheduler_output.\n    Also, calling this function will reset the state of the connector.\n\n    Args:\n        scheduler_output (SchedulerOutput): the scheduler output object.\n    \"\"\"\n\n    meta = P2pNcclConnectorMetadata()\n\n    for new_req in scheduler_output.scheduled_new_reqs:\n        if self.is_producer:\n            num_scheduled_tokens = (scheduler_output.num_scheduled_tokens)[\n                new_req.req_id\n            ]\n            num_tokens = num_scheduled_tokens + new_req.num_computed_tokens\n            # the request's prompt is chunked prefill\n            if num_tokens < len(new_req.prompt_token_ids or []):\n                # 'CachedRequestData' has no attribute 'prompt_token_ids'\n                self.chunked_prefill[new_req.req_id] = (\n                    new_req.block_ids[0],\n                    new_req.prompt_token_ids,\n                )\n                continue\n            # the request's prompt is not chunked prefill\n            meta.add_request(\n                request_id=new_req.req_id,\n                token_ids=new_req.prompt_token_ids or [],\n                block_ids=new_req.block_ids[0],\n                block_size=self._block_size,\n            )\n            continue\n        if new_req.req_id in self._requests_need_load:\n            meta.add_request(\n                request_id=new_req.req_id,\n                token_ids=new_req.prompt_token_ids or [],\n                block_ids=new_req.block_ids[0],\n                block_size=self._block_size,\n            )\n            self._requests_need_load.pop(new_req.req_id)\n\n    cached_reqs = scheduler_output.scheduled_cached_reqs\n    for i, req_id in enumerate(cached_reqs.req_ids):\n        num_computed_tokens = cached_reqs.num_computed_tokens[i]\n        new_block_ids = cached_reqs.new_block_ids[i]\n        resumed_from_preemption = req_id in cached_reqs.resumed_req_ids\n\n        if self.is_producer:\n            num_scheduled_tokens = scheduler_output.num_scheduled_tokens[req_id]\n            num_tokens = num_scheduled_tokens + num_computed_tokens\n            assert req_id in self.chunked_prefill\n            assert new_block_ids is not None\n            block_ids = new_block_ids[0]\n            if not resumed_from_preemption:\n                block_ids = self.chunked_prefill[req_id][0] + block_ids\n            prompt_token_ids = self.chunked_prefill[req_id][1]\n            assert prompt_token_ids is not None\n            # the request's prompt is chunked prefill again\n            if num_tokens < len(prompt_token_ids):\n                self.chunked_prefill[req_id] = (block_ids, prompt_token_ids)\n                continue\n            # the request's prompt is all prefilled finally\n            meta.add_request(\n                request_id=req_id,\n                token_ids=prompt_token_ids,\n                block_ids=block_ids,\n                block_size=self._block_size,\n            )\n            self.chunked_prefill.pop(req_id, None)\n            continue\n\n        # NOTE(rob): here we rely on the resumed requests being\n        # the first N requests in the list scheduled_cache_reqs.\n        if not resumed_from_preemption:\n            break\n        if req_id in self._requests_need_load:\n            request, _ = self._requests_need_load.pop(req_id)\n            total_tokens = num_computed_tokens + 1\n            token_ids = request.all_token_ids[:total_tokens]\n\n            # NOTE(rob): For resumed req, new_block_ids is all\n            # of the block_ids for the request.\n            assert new_block_ids is not None\n            block_ids = new_block_ids[0]\n\n            meta.add_request(\n                request_id=req_id,\n                token_ids=token_ids,\n                block_ids=block_ids,\n                block_size=self._block_size,\n            )\n\n    self._requests_need_load.clear()\n    return meta",
      "language": "python"
    },
    {
      "code": "check_tensors_except_dim(tensor1, tensor2, dim)",
      "language": "unknown"
    },
    {
      "code": "check_tensors_except_dim(tensor1, tensor2, dim)",
      "language": "unknown"
    },
    {
      "code": "520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef check_tensors_except_dim(tensor1, tensor2, dim):\n    shape1 = tensor1.size()\n    shape2 = tensor2.size()\n\n    if len(shape1) != len(shape2) or not all(\n        s1 == s2 for i, (s1, s2) in enumerate(zip(shape1, shape2)) if i != dim\n    ):\n        raise NotImplementedError(\n            \"Currently, only symmetric TP is supported. Asymmetric TP, PP,\"\n            \"and others will be supported in future PRs.\"\n        )",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef check_tensors_except_dim(tensor1, tensor2, dim):\n    shape1 = tensor1.size()\n    shape2 = tensor2.size()\n\n    if len(shape1) != len(shape2) or not all(\n        s1 == s2 for i, (s1, s2) in enumerate(zip(shape1, shape2)) if i != dim\n    ):\n        raise NotImplementedError(\n            \"Currently, only symmetric TP is supported. Asymmetric TP, PP,\"\n            \"and others will be supported in future PRs.\"\n        )",
      "language": "python"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str], **kwargs: Any\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str], **kwargs: Any\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331",
      "language": "unknown"
    },
    {
      "code": "def get_finished(\n    self, finished_req_ids: set[str], **kwargs: Any\n) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"\n    Notifies worker-side connector ids of requests that have\n    finished generating tokens.\n\n    Returns:\n        ids of requests that have finished asynchronous transfer,\n        tuple of (sending/saving ids, recving/loading ids).\n        The finished saves/sends req ids must belong to a set provided in a\n        call to this method (this call or a prior one).\n    \"\"\"\n\n    assert self.p2p_nccl_engine is not None\n\n    no_compile_layers = self._vllm_config.compilation_config.static_forward_context\n    return self.p2p_nccl_engine.get_finished(finished_req_ids, no_compile_layers)",
      "language": "python"
    },
    {
      "code": "def get_finished(\n    self, finished_req_ids: set[str], **kwargs: Any\n) -> tuple[set[str] | None, set[str] | None]:\n    \"\"\"\n    Notifies worker-side connector ids of requests that have\n    finished generating tokens.\n\n    Returns:\n        ids of requests that have finished asynchronous transfer,\n        tuple of (sending/saving ids, recving/loading ids).\n        The finished saves/sends req ids must belong to a set provided in a\n        call to this method (this call or a prior one).\n    \"\"\"\n\n    assert self.p2p_nccl_engine is not None\n\n    no_compile_layers = self._vllm_config.compilation_config.static_forward_context\n    return self.p2p_nccl_engine.get_finished(finished_req_ids, no_compile_layers)",
      "language": "python"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int, bool]",
      "language": "php"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> tuple[int, bool]",
      "language": "php"
    },
    {
      "code": "337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364",
      "language": "unknown"
    },
    {
      "code": "def get_num_new_matched_tokens(\n    self,\n    request: \"Request\",\n    num_computed_tokens: int,\n) -> tuple[int, bool]:\n    \"\"\"\n    Get number of new tokens that can be loaded from the\n    external KV cache beyond the num_computed_tokens.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n\n    Returns:\n        the number of tokens that can be loaded from the\n        external KV cache beyond what is already computed.\n    \"\"\"\n    if self.is_producer:\n        return 0, False\n\n    prompt_token_ids = request.prompt_token_ids or []\n    num_external_tokens = len(prompt_token_ids) - 1 - num_computed_tokens\n\n    if num_external_tokens < 0:\n        num_external_tokens = 0\n\n    return num_external_tokens, False",
      "language": "python"
    },
    {
      "code": "def get_num_new_matched_tokens(\n    self,\n    request: \"Request\",\n    num_computed_tokens: int,\n) -> tuple[int, bool]:\n    \"\"\"\n    Get number of new tokens that can be loaded from the\n    external KV cache beyond the num_computed_tokens.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n\n    Returns:\n        the number of tokens that can be loaded from the\n        external KV cache beyond what is already computed.\n    \"\"\"\n    if self.is_producer:\n        return 0, False\n\n    prompt_token_ids = request.prompt_token_ids or []\n    num_external_tokens = len(prompt_token_ids) - 1 - num_computed_tokens\n\n    if num_external_tokens < 0:\n        num_external_tokens = 0\n\n    return num_external_tokens, False",
      "language": "python"
    },
    {
      "code": "parse_request_id(\n    request_id: str, is_prefill=True\n) -> tuple[str, int]",
      "language": "php"
    },
    {
      "code": "parse_request_id(\n    request_id: str, is_prefill=True\n) -> tuple[str, int]",
      "language": "php"
    },
    {
      "code": "502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef parse_request_id(request_id: str, is_prefill=True) -> tuple[str, int]:\n    # Regular expression to match the string hostname and integer port\n    if is_prefill:\n        pattern = r\"___decode_addr_(.*):(\\d+)\"\n    else:\n        pattern = r\"___prefill_addr_(.*):(\\d+)___\"\n\n    # Use re.search to find the pattern in the request_id\n    match = re.search(pattern, request_id)\n    if match:\n        # Extract the ranks\n        ip = match.group(1)\n        port = int(match.group(2))\n\n        return ip, port\n    raise ValueError(f\"Request id {request_id} does not contain hostname and port\")",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef parse_request_id(request_id: str, is_prefill=True) -> tuple[str, int]:\n    # Regular expression to match the string hostname and integer port\n    if is_prefill:\n        pattern = r\"___decode_addr_(.*):(\\d+)\"\n    else:\n        pattern = r\"___prefill_addr_(.*):(\\d+)___\"\n\n    # Use re.search to find the pattern in the request_id\n    match = re.search(pattern, request_id)\n    if match:\n        # Extract the ranks\n        ip = match.group(1)\n        port = int(match.group(2))\n\n        return ip, port\n    raise ValueError(f\"Request id {request_id} does not contain hostname and port\")",
      "language": "python"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496",
      "language": "unknown"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Called when a request has finished, before its blocks are freed.\n\n    Returns:\n        True if the request is being saved/sent asynchronously and blocks\n        should not be freed until the request_id is returned from\n        get_finished().\n        Optional KVTransferParams to be included in the request outputs\n        returned by the engine.\n    \"\"\"\n\n    self.chunked_prefill.pop(request.request_id, None)\n\n    return False, None",
      "language": "python"
    },
    {
      "code": "def request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    \"\"\"\n    Called when a request has finished, before its blocks are freed.\n\n    Returns:\n        True if the request is being saved/sent asynchronously and blocks\n        should not be freed until the request_id is returned from\n        get_finished().\n        Optional KVTransferParams to be included in the request outputs\n        returned by the engine.\n    \"\"\"\n\n    self.chunked_prefill.pop(request.request_id, None)\n\n    return False, None",
      "language": "python"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None",
      "language": "rust"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None",
      "language": "rust"
    },
    {
      "code": "242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307",
      "language": "unknown"
    },
    {
      "code": "def save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None:\n    \"\"\"Start saving the KV cache of the layer from vLLM's paged buffer\n    to the connector.\n\n    Args:\n        layer_name (str): the name of the layer.\n        kv_layer (torch.Tensor): the paged KV buffer of the current\n            layer in vLLM.\n        attn_metadata (AttentionMetadata): the attention metadata.\n        **kwargs: additional arguments for the save operation.\n    \"\"\"\n\n    # Only producer/prefill saves KV Cache\n    if not self.is_producer:\n        return\n\n    assert self.p2p_nccl_engine is not None\n\n    def extract_kv_from_layer(\n        layer: torch.Tensor,\n        block_ids: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Extract KV cache slices from a given attention layer tensor.\n\n        This function handles multiple backend layouts:\n          - MLA (Multi-Linear Attention) or FlashInfer: KV tensors are\n            indexed along the first dimension.\n          - FlashAttention: KV tensors are indexed along the second\n            dimension.\n\n        Args:\n            layer (torch.Tensor): The KV cache from the attention layer.\n            block_ids (torch.Tensor): Indices of blocks to extract.\n\n        Returns:\n            torch.Tensor: A tensor containing the extracted KV slices.\n            Returns None if the layout is unsupported.\n        \"\"\"\n        if (\n            isinstance(attn_metadata, MLACommonMetadata) or layer.shape[1] == 2\n        ):  # MLA or FlashInfer\n            return layer[block_ids, ...]\n\n        if layer.shape[0] == 2:  # FlashAttention\n            return layer[:, block_ids, ...]\n\n        return None\n\n    connector_metadata = self._get_connector_metadata()\n    assert isinstance(connector_metadata, P2pNcclConnectorMetadata)\n    for request in connector_metadata.requests:\n        request_id = request.request_id\n        ip, port = self.parse_request_id(request_id, True)\n        remote_address = ip + \":\" + str(port + self._rank)\n\n        kv_cache = extract_kv_from_layer(kv_layer, request.block_ids)\n        self.p2p_nccl_engine.send_tensor(\n            request_id + \"#\" + layer_name, kv_cache, remote_address\n        )",
      "language": "python"
    },
    {
      "code": "def save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs: Any,\n) -> None:\n    \"\"\"Start saving the KV cache of the layer from vLLM's paged buffer\n    to the connector.\n\n    Args:\n        layer_name (str): the name of the layer.\n        kv_layer (torch.Tensor): the paged KV buffer of the current\n            layer in vLLM.\n        attn_metadata (AttentionMetadata): the attention metadata.\n        **kwargs: additional arguments for the save operation.\n    \"\"\"\n\n    # Only producer/prefill saves KV Cache\n    if not self.is_producer:\n        return\n\n    assert self.p2p_nccl_engine is not None\n\n    def extract_kv_from_layer(\n        layer: torch.Tensor,\n        block_ids: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Extract KV cache slices from a given attention layer tensor.\n\n        This function handles multiple backend layouts:\n          - MLA (Multi-Linear Attention) or FlashInfer: KV tensors are\n            indexed along the first dimension.\n          - FlashAttention: KV tensors are indexed along the second\n            dimension.\n\n        Args:\n            layer (torch.Tensor): The KV cache from the attention layer.\n            block_ids (torch.Tensor): Indices of blocks to extract.\n\n        Returns:\n            torch.Tensor: A tensor containing the extracted KV slices.\n            Returns None if the layout is unsupported.\n        \"\"\"\n        if (\n            isinstance(attn_metadata, MLACommonMetadata) or layer.shape[1] == 2\n        ):  # MLA or FlashInfer\n            return layer[block_ids, ...]\n\n        if layer.shape[0] == 2:  # FlashAttention\n            return layer[:, block_ids, ...]\n\n        return None\n\n    connector_metadata = self._get_connector_metadata()\n    assert isinstance(connector_metadata, P2pNcclConnectorMetadata)\n    for request in connector_metadata.requests:\n        request_id = request.request_id\n        ip, port = self.parse_request_id(request_id, True)\n        remote_address = ip + \":\" + str(port + self._rank)\n\n        kv_cache = extract_kv_from_layer(kv_layer, request.block_ids)\n        self.p2p_nccl_engine.send_tensor(\n            request_id + \"#\" + layer_name, kv_cache, remote_address\n        )",
      "language": "python"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs: Any\n) -> None",
      "language": "rust"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs: Any\n) -> None",
      "language": "rust"
    },
    {
      "code": "111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229",
      "language": "unknown"
    },
    {
      "code": "def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n    \"\"\"Start loading the KV cache from the connector buffer to vLLM's\n    paged KV buffer.\n\n    Args:\n        forward_context (ForwardContext): the forward context.\n        **kwargs: additional arguments for the load operation\n\n    Note:\n        The number of elements in kv_caches and layer_names should be\n        the same.\n    \"\"\"\n\n    # Only consumer/decode loads KV Cache\n    if self.is_producer:\n        return\n\n    assert self.p2p_nccl_engine is not None\n\n    attn_metadata = forward_context.attn_metadata\n    if attn_metadata is None:\n        return\n\n    def inject_kv_into_layer(\n        layer: torch.Tensor,\n        kv_cache: torch.Tensor,\n        block_ids: torch.Tensor,\n        request_id: str,\n    ) -> None:\n        \"\"\"\n        Inject KV cache data into a given attention layer tensor.\n\n        This function updates `layer` in-place with values from `kv_cache`,\n        handling different backend layouts:\n          - MLA (Multi-Linear Attention) or FlashInfer: KV tensors are\n            indexed along the first dimension.\n          - FlashAttention: KV tensors are indexed along the second\n            dimension.\n\n        If the number of provided block IDs does not match the number of KV\n        blocks, only the overlapping portion is updated, and a warning is\n        logged.\n\n        Args:\n            layer (torch.Tensor): The attention layer KV tensor to update.\n            kv_cache (torch.Tensor): The KV cache tensor to inject.\n            block_ids (torch.Tensor): Indices of the blocks to update.\n            request_id (str): Request identifier used for logging.\n\n        Returns:\n            None. The function modifies `layer` in-place.\n        \"\"\"\n        if (\n            isinstance(attn_metadata, MLACommonMetadata) or layer.shape[1] == 2\n        ):  # MLA or FlashInfer\n            num_block = kv_cache.shape[0]\n            self.check_tensors_except_dim(layer, kv_cache, 0)\n            if len(block_ids) == num_block:\n                layer[block_ids, ...] = kv_cache\n            else:\n                layer[block_ids[:num_block], ...] = kv_cache\n                logger.warning(\n                    \"ðŸš§kv_cache does not match, block_ids:%d, \"\n                    \"num_block:%d, request_id:%s\",\n                    len(block_ids),\n                    num_block,\n                    request_id,\n                )\n\n        elif layer.shape[0] == 2:  # FlashAttention\n            num_block = kv_cache.shape[1]\n            self.check_tensors_except_dim(layer, kv_cache, 1)\n            if len(block_ids) == num_block:\n                layer[:, block_ids, ...] = kv_cache\n            else:\n                layer[:, block_ids[:num_block], ...] = kv_cache\n                logger.warning(\n                    \"ðŸš§kv_cache does not match, block_ids:%d, \"\n                    \"num_block:%d, request_id:%s\",\n                    len(block_ids),\n                    num_block,\n                    request_id,\n                )\n\n    # Get the metadata\n    metadata: KVConnectorMetadata = self._get_connector_metadata()\n    assert isinstance(metadata, P2pNcclConnectorMetadata)\n\n    if metadata is None:\n        return\n\n    # Load the KV for each request each layer\n    for request in metadata.requests:\n        request_id = request.request_id\n        ip, port = self.parse_request_id(request_id, False)\n        remote_address = ip + \":\" + str(port + self._rank)\n        for layer_name in forward_context.no_compile_layers:\n            layer = forward_context.no_compile_layers[layer_name]\n\n            # Only process layers that have kv_cache\n            # attribute (attention layers) Skip non-attention\n            # layers like FusedMoE\n            kv_cache = getattr(layer, \"kv_cache\", None)\n            if kv_cache is None:\n                continue\n\n            layer = kv_cache[forward_context.virtual_engine]\n\n            kv_cache = self.p2p_nccl_engine.recv_tensor(\n                request.request_id + \"#\" + layer_name, remote_address\n            )\n\n            if kv_cache is None:\n                logger.warning(\"ðŸš§kv_cache is None, %s\", request.request_id)\n                continue\n\n            inject_kv_into_layer(\n                layer, kv_cache, request.block_ids, request.request_id\n            )",
      "language": "python"
    },
    {
      "code": "def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs: Any) -> None:\n    \"\"\"Start loading the KV cache from the connector buffer to vLLM's\n    paged KV buffer.\n\n    Args:\n        forward_context (ForwardContext): the forward context.\n        **kwargs: additional arguments for the load operation\n\n    Note:\n        The number of elements in kv_caches and layer_names should be\n        the same.\n    \"\"\"\n\n    # Only consumer/decode loads KV Cache\n    if self.is_producer:\n        return\n\n    assert self.p2p_nccl_engine is not None\n\n    attn_metadata = forward_context.attn_metadata\n    if attn_metadata is None:\n        return\n\n    def inject_kv_into_layer(\n        layer: torch.Tensor,\n        kv_cache: torch.Tensor,\n        block_ids: torch.Tensor,\n        request_id: str,\n    ) -> None:\n        \"\"\"\n        Inject KV cache data into a given attention layer tensor.\n\n        This function updates `layer` in-place with values from `kv_cache`,\n        handling different backend layouts:\n          - MLA (Multi-Linear Attention) or FlashInfer: KV tensors are\n            indexed along the first dimension.\n          - FlashAttention: KV tensors are indexed along the second\n            dimension.\n\n        If the number of provided block IDs does not match the number of KV\n        blocks, only the overlapping portion is updated, and a warning is\n        logged.\n\n        Args:\n            layer (torch.Tensor): The attention layer KV tensor to update.\n            kv_cache (torch.Tensor): The KV cache tensor to inject.\n            block_ids (torch.Tensor): Indices of the blocks to update.\n            request_id (str): Request identifier used for logging.\n\n        Returns:\n            None. The function modifies `layer` in-place.\n        \"\"\"\n        if (\n            isinstance(attn_metadata, MLACommonMetadata) or layer.shape[1] == 2\n        ):  # MLA or FlashInfer\n            num_block = kv_cache.shape[0]\n            self.check_tensors_except_dim(layer, kv_cache, 0)\n            if len(block_ids) == num_block:\n                layer[block_ids, ...] = kv_cache\n            else:\n                layer[block_ids[:num_block], ...] = kv_cache\n                logger.warning(\n                    \"ðŸš§kv_cache does not match, block_ids:%d, \"\n                    \"num_block:%d, request_id:%s\",\n                    len(block_ids),\n                    num_block,\n                    request_id,\n                )\n\n        elif layer.shape[0] == 2:  # FlashAttention\n            num_block = kv_cache.shape[1]\n            self.check_tensors_except_dim(layer, kv_cache, 1)\n            if len(block_ids) == num_block:\n                layer[:, block_ids, ...] = kv_cache\n            else:\n                layer[:, block_ids[:num_block], ...] = kv_cache\n                logger.warning(\n                    \"ðŸš§kv_cache does not match, block_ids:%d, \"\n                    \"num_block:%d, request_id:%s\",\n                    len(block_ids),\n                    num_block,\n                    request_id,\n                )\n\n    # Get the metadata\n    metadata: KVConnectorMetadata = self._get_connector_metadata()\n    assert isinstance(metadata, P2pNcclConnectorMetadata)\n\n    if metadata is None:\n        return\n\n    # Load the KV for each request each layer\n    for request in metadata.requests:\n        request_id = request.request_id\n        ip, port = self.parse_request_id(request_id, False)\n        remote_address = ip + \":\" + str(port + self._rank)\n        for layer_name in forward_context.no_compile_layers:\n            layer = forward_context.no_compile_layers[layer_name]\n\n            # Only process layers that have kv_cache\n            # attribute (attention layers) Skip non-attention\n            # layers like FusedMoE\n            kv_cache = getattr(layer, \"kv_cache\", None)\n            if kv_cache is None:\n                continue\n\n            layer = kv_cache[forward_context.virtual_engine]\n\n            kv_cache = self.p2p_nccl_engine.recv_tensor(\n                request.request_id + \"#\" + layer_name, remote_address\n            )\n\n            if kv_cache is None:\n                logger.warning(\"ðŸš§kv_cache is None, %s\", request.request_id)\n                continue\n\n            inject_kv_into_layer(\n                layer, kv_cache, request.block_ids, request.request_id\n            )",
      "language": "python"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request,\n    blocks: KVCacheBlocks,\n    num_external_tokens: int,\n)",
      "language": "yaml"
    },
    {
      "code": "366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376",
      "language": "unknown"
    },
    {
      "code": "def update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    \"\"\"\n    Update KVConnector state after block allocation.\n    \"\"\"\n    if not self.is_producer and num_external_tokens > 0:\n        self._requests_need_load[request.request_id] = (\n            request,\n            blocks.get_block_ids()[0],\n        )",
      "language": "python"
    },
    {
      "code": "def update_state_after_alloc(\n    self, request: \"Request\", blocks: \"KVCacheBlocks\", num_external_tokens: int\n):\n    \"\"\"\n    Update KVConnector state after block allocation.\n    \"\"\"\n    if not self.is_producer and num_external_tokens > 0:\n        self._requests_need_load[request.request_id] = (\n            request,\n            blocks.get_block_ids()[0],\n        )",
      "language": "python"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "231\n232\n233\n234\n235\n236\n237\n238\n239\n240",
      "language": "unknown"
    },
    {
      "code": "def wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n    paged buffer.\n\n    This interface will be useful for layer-by-layer pipelining.\n\n    Args:\n        layer_name: the name of that layer\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "def wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n    paged buffer.\n\n    This interface will be useful for layer-by-layer pipelining.\n\n    Args:\n        layer_name: the name of that layer\n    \"\"\"\n    return",
      "language": "python"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "309\n310\n311\n312",
      "language": "unknown"
    },
    {
      "code": "def wait_for_save(self):\n    if self.is_producer:\n        assert self.p2p_nccl_engine is not None\n        self.p2p_nccl_engine.wait_for_sent()",
      "language": "python"
    },
    {
      "code": "def wait_for_save(self):\n    if self.is_producer:\n        assert self.p2p_nccl_engine is not None\n        self.p2p_nccl_engine.wait_for_sent()",
      "language": "python"
    },
    {
      "code": "55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass P2pNcclConnectorMetadata(KVConnectorMetadata):\n    requests: list[ReqMeta]\n\n    def __init__(self):\n        self.requests = []\n\n    def add_request(\n        self,\n        request_id: str,\n        token_ids: list[int],\n        block_ids: list[int],\n        block_size: int,\n    ) -> None:\n        self.requests.append(\n            ReqMeta.make_meta(request_id, token_ids, block_ids, block_size)\n        )",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass P2pNcclConnectorMetadata(KVConnectorMetadata):\n    requests: list[ReqMeta]\n\n    def __init__(self):\n        self.requests = []\n\n    def add_request(\n        self,\n        request_id: str,\n        token_ids: list[int],\n        block_ids: list[int],\n        block_size: int,\n    ) -> None:\n        self.requests.append(\n            ReqMeta.make_meta(request_id, token_ids, block_ids, block_size)\n        )",
      "language": "python"
    },
    {
      "code": "requests: list[ReqMeta] = []",
      "language": "yaml"
    },
    {
      "code": "requests: list[ReqMeta] = []",
      "language": "yaml"
    },
    {
      "code": "def __init__(self):\n    self.requests = []",
      "language": "python"
    },
    {
      "code": "def __init__(self):\n    self.requests = []",
      "language": "python"
    },
    {
      "code": "add_request(\n    request_id: str,\n    token_ids: list[int],\n    block_ids: list[int],\n    block_size: int,\n) -> None",
      "language": "rust"
    },
    {
      "code": "add_request(\n    request_id: str,\n    token_ids: list[int],\n    block_ids: list[int],\n    block_size: int,\n) -> None",
      "language": "rust"
    },
    {
      "code": "62\n63\n64\n65\n66\n67\n68\n69\n70\n71",
      "language": "unknown"
    },
    {
      "code": "def add_request(\n    self,\n    request_id: str,\n    token_ids: list[int],\n    block_ids: list[int],\n    block_size: int,\n) -> None:\n    self.requests.append(\n        ReqMeta.make_meta(request_id, token_ids, block_ids, block_size)\n    )",
      "language": "python"
    },
    {
      "code": "def add_request(\n    self,\n    request_id: str,\n    token_ids: list[int],\n    block_ids: list[int],\n    block_size: int,\n) -> None:\n    self.requests.append(\n        ReqMeta.make_meta(request_id, token_ids, block_ids, block_size)\n    )",
      "language": "python"
    },
    {
      "code": "34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass ReqMeta:\n    # Request Id\n    request_id: str\n    # Request block ids\n    block_ids: torch.Tensor\n    # Request num tokens\n    num_tokens: int\n\n    @staticmethod\n    def make_meta(\n        request_id: str, token_ids: list[int], block_ids: list[int], block_size: int\n    ) -> \"ReqMeta\":\n        block_ids_tensor = torch.tensor(block_ids)\n        return ReqMeta(\n            request_id=request_id,\n            block_ids=block_ids_tensor,\n            num_tokens=len(token_ids),\n        )",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass ReqMeta:\n    # Request Id\n    request_id: str\n    # Request block ids\n    block_ids: torch.Tensor\n    # Request num tokens\n    num_tokens: int\n\n    @staticmethod\n    def make_meta(\n        request_id: str, token_ids: list[int], block_ids: list[int], block_size: int\n    ) -> \"ReqMeta\":\n        block_ids_tensor = torch.tensor(block_ids)\n        return ReqMeta(\n            request_id=request_id,\n            block_ids=block_ids_tensor,\n            num_tokens=len(token_ids),\n        )",
      "language": "python"
    },
    {
      "code": "block_ids: Tensor",
      "language": "yaml"
    },
    {
      "code": "block_ids: Tensor",
      "language": "yaml"
    },
    {
      "code": "num_tokens: int",
      "language": "yaml"
    },
    {
      "code": "num_tokens: int",
      "language": "yaml"
    },
    {
      "code": "request_id: str",
      "language": "yaml"
    },
    {
      "code": "request_id: str",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    request_id: str, block_ids: Tensor, num_tokens: int\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    request_id: str, block_ids: Tensor, num_tokens: int\n) -> None",
      "language": "python"
    },
    {
      "code": "make_meta(\n    request_id: str,\n    token_ids: list[int],\n    block_ids: list[int],\n    block_size: int,\n) -> ReqMeta",
      "language": "php"
    },
    {
      "code": "make_meta(\n    request_id: str,\n    token_ids: list[int],\n    block_ids: list[int],\n    block_size: int,\n) -> ReqMeta",
      "language": "php"
    },
    {
      "code": "43\n44\n45\n46\n47\n48\n49\n50\n51\n52",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef make_meta(\n    request_id: str, token_ids: list[int], block_ids: list[int], block_size: int\n) -> \"ReqMeta\":\n    block_ids_tensor = torch.tensor(block_ids)\n    return ReqMeta(\n        request_id=request_id,\n        block_ids=block_ids_tensor,\n        num_tokens=len(token_ids),\n    )",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef make_meta(\n    request_id: str, token_ids: list[int], block_ids: list[int], block_size: int\n) -> \"ReqMeta\":\n    block_ids_tensor = torch.tensor(block_ids)\n    return ReqMeta(\n        request_id=request_id,\n        block_ids=block_ids_tensor,\n        num_tokens=len(token_ids),\n    )",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}