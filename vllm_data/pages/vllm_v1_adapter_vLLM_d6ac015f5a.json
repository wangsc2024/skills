{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
  "title": "vllm_v1_adapter - vLLM",
  "content": "Bases: KVConnectorMetadata\n\nAdd a request to the metadata.\n\nthe request metadata.\n\nAttach the connector metadata to the request object.\n\nThis function should NOT modify other fields in the scheduler_output except the kv_connector_metadata field. Also, calling this function will reset the state of the connector.\n\nthe scheduler output object.\n\nGet inference information including vLLM config and related details.\n\nDictionary containing inference information\n\nGet vLLM version information.\n\nCheck for external KV cache hit.\n\nthe number of locally computed tokens for this request\n\nthe number of tokens that can be loaded from the\n\nexternal KV cache beyond what is already computed.\n\nStart saving the a layer of KV cache from vLLM's paged buffer to the connector.\n\nthe name of the layer.\n\nthe paged KV buffer of the current layer in vLLM.\n\nthe attention metadata.\n\nStart loading the KV cache from the connector buffer to vLLM's paged KV buffer.\n\nThe number of elements in kv_caches and layer_names should be the same.\n\nUpdate KVConnector state after temporary buffer alloc.\n\nFor SharedStorageConnector, update _request_needs_load if the CacheManager this allocated blocks for us.\n\nBlocking until the KV for a specific layer is loaded into vLLM's paged buffer.\n\nThis interface will be useful for layer-by-layer pipelining.\n\nthe name of that layer\n\nBlocking until the KV cache is saved to the connector buffer.\n\nCreate the request metadata from a request tracker.\n\nthe block size in vLLM.\n\nthe chunk size for LMCache.\n\nthe load spec for KV cache loading.\n\nwhether to discard partial chunks.\n\nwhether to save the cache in decode phase.\n\nthe request metadata if we need to perform load/save\n\noperations, None otherwise.\n\nCreate the request tracker from a new request.\n\nthe LMCache engine config.\n\nthe new request data.\n\nthe number of tokens that will be 'computed', including the num_computed_tokens (vLLM's local cache hit) and new tokens that will be scheduled.\n\nthe number of tokens that are cached in LMCache.\n\nwhether the request cache should be saved\n\nUpdate the request tracker when a running request is scheduled again\n\nInitialize the LMCache engine by the given model config and parallel config. This function will check the environment variable LMCACHE_CONFIG_FILE to load the configuration file. If that environment variable is not set, this function will return None.\n\n:param lmcache_config: The LMCache configuration. :type lmcache_config: LMCacheEngineConfig :param vllm_config: The vLLM configuration. :type vllm_config: VllmConfig\n\n:return: The initialized LMCache engine :rtype: LMCacheEngine",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.logger"
    },
    {
      "level": "h2",
      "text": "tmp_disagg_tracker module-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.tmp_disagg_tracker"
    },
    {
      "level": "h2",
      "text": "DisaggSpec dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec"
    },
    {
      "level": "h3",
      "text": "is_last_prefill class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec.is_last_prefill"
    },
    {
      "level": "h3",
      "text": "num_transferred_tokens class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec.num_transferred_tokens"
    },
    {
      "level": "h3",
      "text": "receiver_alloc_port instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec.receiver_alloc_port"
    },
    {
      "level": "h3",
      "text": "receiver_host instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec.receiver_host"
    },
    {
      "level": "h3",
      "text": "receiver_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec.receiver_id"
    },
    {
      "level": "h3",
      "text": "receiver_init_port instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec.receiver_init_port"
    },
    {
      "level": "h3",
      "text": "req_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec.req_id"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.DisaggSpec.__init__"
    },
    {
      "level": "h2",
      "text": "LMCacheConnectorMetadata dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorMetadata"
    },
    {
      "level": "h3",
      "text": "lookup_requests_in_step class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorMetadata.lookup_requests_in_step"
    },
    {
      "level": "h3",
      "text": "requests class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorMetadata.requests"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorMetadata.__init__"
    },
    {
      "level": "h3",
      "text": "add_request ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorMetadata.add_request"
    },
    {
      "level": "h2",
      "text": "LMCacheConnectorV1Impl ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl"
    },
    {
      "level": "h3",
      "text": "_block_size instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._block_size"
    },
    {
      "level": "h3",
      "text": "_discard_partial_chunks instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._discard_partial_chunks"
    },
    {
      "level": "h3",
      "text": "_lmcache_chunk_size instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._lmcache_chunk_size"
    },
    {
      "level": "h3",
      "text": "_lookup_requests_in_step instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._lookup_requests_in_step"
    },
    {
      "level": "h3",
      "text": "_parent instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._parent"
    },
    {
      "level": "h3",
      "text": "_request_trackers instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._request_trackers"
    },
    {
      "level": "h3",
      "text": "_requests_priority instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._requests_priority"
    },
    {
      "level": "h3",
      "text": "_save_decode_cache instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._save_decode_cache"
    },
    {
      "level": "h3",
      "text": "_stats_monitor instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._stats_monitor"
    },
    {
      "level": "h3",
      "text": "_unfinished_requests instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._unfinished_requests"
    },
    {
      "level": "h3",
      "text": "_vllm_config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._vllm_config"
    },
    {
      "level": "h3",
      "text": "api_server instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.api_server"
    },
    {
      "level": "h3",
      "text": "async_loading instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.async_loading"
    },
    {
      "level": "h3",
      "text": "blender instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.blender"
    },
    {
      "level": "h3",
      "text": "config instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.config"
    },
    {
      "level": "h3",
      "text": "current_layer instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.current_layer"
    },
    {
      "level": "h3",
      "text": "enable_blending instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.enable_blending"
    },
    {
      "level": "h3",
      "text": "force_skip_save instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.force_skip_save"
    },
    {
      "level": "h3",
      "text": "kv_cache_manager instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.kv_cache_manager"
    },
    {
      "level": "h3",
      "text": "kv_caches instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.kv_caches"
    },
    {
      "level": "h3",
      "text": "kv_role instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.kv_role"
    },
    {
      "level": "h3",
      "text": "layerwise_retrievers instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.layerwise_retrievers"
    },
    {
      "level": "h3",
      "text": "lmcache_engine instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.lmcache_engine"
    },
    {
      "level": "h3",
      "text": "load_specs instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.load_specs"
    },
    {
      "level": "h3",
      "text": "lookup_client instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.lookup_client"
    },
    {
      "level": "h3",
      "text": "lookup_server instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.lookup_server"
    },
    {
      "level": "h3",
      "text": "num_layers instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.num_layers"
    },
    {
      "level": "h3",
      "text": "offload_server instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.offload_server"
    },
    {
      "level": "h3",
      "text": "plugin_launcher instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.plugin_launcher"
    },
    {
      "level": "h3",
      "text": "skip_last_n_tokens instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.skip_last_n_tokens"
    },
    {
      "level": "h3",
      "text": "use_layerwise instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.use_layerwise"
    },
    {
      "level": "h3",
      "text": "worker_count instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.worker_count"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.__init__"
    },
    {
      "level": "h3",
      "text": "_init_kv_caches_from_forward_context ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl._init_kv_caches_from_forward_context"
    },
    {
      "level": "h3",
      "text": "build_connector_meta ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.build_connector_meta"
    },
    {
      "level": "h3",
      "text": "get_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.get_finished"
    },
    {
      "level": "h3",
      "text": "get_inference_info ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.get_inference_info"
    },
    {
      "level": "h3",
      "text": "get_inference_version ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.get_inference_version"
    },
    {
      "level": "h3",
      "text": "get_num_new_matched_tokens ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.get_num_new_matched_tokens"
    },
    {
      "level": "h3",
      "text": "request_finished ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.request_finished"
    },
    {
      "level": "h3",
      "text": "save_kv_layer ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.save_kv_layer"
    },
    {
      "level": "h3",
      "text": "start_load_kv ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.start_load_kv"
    },
    {
      "level": "h3",
      "text": "update_state_after_alloc ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.update_state_after_alloc"
    },
    {
      "level": "h3",
      "text": "wait_for_layer_load ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.wait_for_layer_load"
    },
    {
      "level": "h3",
      "text": "wait_for_save ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LMCacheConnectorV1Impl.wait_for_save"
    },
    {
      "level": "h2",
      "text": "LoadSpec dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LoadSpec"
    },
    {
      "level": "h3",
      "text": "can_load instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LoadSpec.can_load"
    },
    {
      "level": "h3",
      "text": "lmcache_cached_tokens instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LoadSpec.lmcache_cached_tokens"
    },
    {
      "level": "h3",
      "text": "vllm_cached_tokens instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LoadSpec.vllm_cached_tokens"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.LoadSpec.__init__"
    },
    {
      "level": "h2",
      "text": "ReqMeta dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta"
    },
    {
      "level": "h3",
      "text": "disagg_spec class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.disagg_spec"
    },
    {
      "level": "h3",
      "text": "is_last_prefill class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.is_last_prefill"
    },
    {
      "level": "h3",
      "text": "load_spec class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.load_spec"
    },
    {
      "level": "h3",
      "text": "req_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.req_id"
    },
    {
      "level": "h3",
      "text": "request_configs class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.request_configs"
    },
    {
      "level": "h3",
      "text": "save_spec class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.save_spec"
    },
    {
      "level": "h3",
      "text": "slot_mapping instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.slot_mapping"
    },
    {
      "level": "h3",
      "text": "token_ids instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.token_ids"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.__init__"
    },
    {
      "level": "h3",
      "text": "from_request_tracker staticmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.ReqMeta.from_request_tracker"
    },
    {
      "level": "h2",
      "text": "RequestTracker dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker"
    },
    {
      "level": "h3",
      "text": "allocated_block_ids instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.allocated_block_ids"
    },
    {
      "level": "h3",
      "text": "disagg_spec class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.disagg_spec"
    },
    {
      "level": "h3",
      "text": "is_decode_phase class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.is_decode_phase"
    },
    {
      "level": "h3",
      "text": "mm_hashes class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.mm_hashes"
    },
    {
      "level": "h3",
      "text": "mm_positions class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.mm_positions"
    },
    {
      "level": "h3",
      "text": "num_saved_tokens class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.num_saved_tokens"
    },
    {
      "level": "h3",
      "text": "prompt_len instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.prompt_len"
    },
    {
      "level": "h3",
      "text": "req_id instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.req_id"
    },
    {
      "level": "h3",
      "text": "request_configs class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.request_configs"
    },
    {
      "level": "h3",
      "text": "skip_save class-attribute instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.skip_save"
    },
    {
      "level": "h3",
      "text": "token_ids instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.token_ids"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.__init__"
    },
    {
      "level": "h3",
      "text": "from_new_request staticmethod ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.from_new_request"
    },
    {
      "level": "h3",
      "text": "update ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.RequestTracker.update"
    },
    {
      "level": "h2",
      "text": "SaveSpec dataclass ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.SaveSpec"
    },
    {
      "level": "h3",
      "text": "can_save instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.SaveSpec.can_save"
    },
    {
      "level": "h3",
      "text": "skip_leading_tokens instance-attribute ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.SaveSpec.skip_leading_tokens"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.SaveSpec.__init__"
    },
    {
      "level": "h2",
      "text": "_calculate_mtp_layers ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter._calculate_mtp_layers"
    },
    {
      "level": "h2",
      "text": "_init_lmcache_engine ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter._init_lmcache_engine"
    },
    {
      "level": "h2",
      "text": "extract_request_configs ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.extract_request_configs"
    },
    {
      "level": "h2",
      "text": "need_gpu_interm_buffer ¶",
      "id": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_integration.vllm_v1_adapter.need_gpu_interm_buffer"
    }
  ],
  "code_samples": [
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger(__name__)",
      "language": "unknown"
    },
    {
      "code": "tmp_disagg_tracker: dict[str, DisaggSpec] = {}",
      "language": "yaml"
    },
    {
      "code": "tmp_disagg_tracker: dict[str, DisaggSpec] = {}",
      "language": "yaml"
    },
    {
      "code": "88\n89\n90\n91\n92\n93\n94\n95\n96",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass DisaggSpec:\n    req_id: str\n    receiver_id: str\n    receiver_host: str\n    receiver_init_port: int\n    receiver_alloc_port: int\n    is_last_prefill: bool = False\n    num_transferred_tokens: int = 0",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass DisaggSpec:\n    req_id: str\n    receiver_id: str\n    receiver_host: str\n    receiver_init_port: int\n    receiver_alloc_port: int\n    is_last_prefill: bool = False\n    num_transferred_tokens: int = 0",
      "language": "python"
    },
    {
      "code": "is_last_prefill: bool = False",
      "language": "typescript"
    },
    {
      "code": "is_last_prefill: bool = False",
      "language": "typescript"
    },
    {
      "code": "num_transferred_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_transferred_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "receiver_alloc_port: int",
      "language": "yaml"
    },
    {
      "code": "receiver_alloc_port: int",
      "language": "yaml"
    },
    {
      "code": "receiver_host: str",
      "language": "yaml"
    },
    {
      "code": "receiver_host: str",
      "language": "yaml"
    },
    {
      "code": "receiver_id: str",
      "language": "yaml"
    },
    {
      "code": "receiver_id: str",
      "language": "yaml"
    },
    {
      "code": "receiver_init_port: int",
      "language": "yaml"
    },
    {
      "code": "receiver_init_port: int",
      "language": "yaml"
    },
    {
      "code": "req_id: str",
      "language": "yaml"
    },
    {
      "code": "req_id: str",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    req_id: str,\n    receiver_id: str,\n    receiver_host: str,\n    receiver_init_port: int,\n    receiver_alloc_port: int,\n    is_last_prefill: bool = False,\n    num_transferred_tokens: int = 0,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    req_id: str,\n    receiver_id: str,\n    receiver_host: str,\n    receiver_init_port: int,\n    receiver_alloc_port: int,\n    is_last_prefill: bool = False,\n    num_transferred_tokens: int = 0,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass LMCacheConnectorMetadata(KVConnectorMetadata):\n    requests: list[ReqMeta] = field(default_factory=list)\n    lookup_requests_in_step: list[str] = field(default_factory=list)\n\n    @_lmcache_nvtx_annotate\n    def add_request(self, req_meta: ReqMeta) -> None:\n        \"\"\"Add a request to the metadata.\n\n        Args:\n            req_meta (ReqMeta): the request metadata.\n        \"\"\"\n        self.requests.append(req_meta)",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass LMCacheConnectorMetadata(KVConnectorMetadata):\n    requests: list[ReqMeta] = field(default_factory=list)\n    lookup_requests_in_step: list[str] = field(default_factory=list)\n\n    @_lmcache_nvtx_annotate\n    def add_request(self, req_meta: ReqMeta) -> None:\n        \"\"\"Add a request to the metadata.\n\n        Args:\n            req_meta (ReqMeta): the request metadata.\n        \"\"\"\n        self.requests.append(req_meta)",
      "language": "python"
    },
    {
      "code": "lookup_requests_in_step: list[str] = field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "lookup_requests_in_step: list[str] = field(\n    default_factory=list\n)",
      "language": "yaml"
    },
    {
      "code": "requests: list[ReqMeta] = field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "requests: list[ReqMeta] = field(default_factory=list)",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    requests: list[ReqMeta] = list(),\n    lookup_requests_in_step: list[str] = list(),\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    requests: list[ReqMeta] = list(),\n    lookup_requests_in_step: list[str] = list(),\n) -> None",
      "language": "python"
    },
    {
      "code": "add_request(req_meta: ReqMeta) -> None",
      "language": "rust"
    },
    {
      "code": "add_request(req_meta: ReqMeta) -> None",
      "language": "rust"
    },
    {
      "code": "557\n558\n559\n560\n561\n562\n563\n564",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef add_request(self, req_meta: ReqMeta) -> None:\n    \"\"\"Add a request to the metadata.\n\n    Args:\n        req_meta (ReqMeta): the request metadata.\n    \"\"\"\n    self.requests.append(req_meta)",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef add_request(self, req_meta: ReqMeta) -> None:\n    \"\"\"Add a request to the metadata.\n\n    Args:\n        req_meta (ReqMeta): the request metadata.\n    \"\"\"\n    self.requests.append(req_meta)",
      "language": "python"
    },
    {
      "code": "567\n 568\n 569\n 570\n 571\n 572\n 573\n 574\n 575\n 576\n 577\n 578\n 579\n 580\n 581\n 582\n 583\n 584\n 585\n 586\n 587\n 588\n 589\n 590\n 591\n 592\n 593\n 594\n 595\n 596\n 597\n 598\n 599\n 600\n 601\n 602\n 603\n 604\n 605\n 606\n 607\n 608\n 609\n 610\n 611\n 612\n 613\n 614\n 615\n 616\n 617\n 618\n 619\n 620\n 621\n 622\n 623\n 624\n 625\n 626\n 627\n 628\n 629\n 630\n 631\n 632\n 633\n 634\n 635\n 636\n 637\n 638\n 639\n 640\n 641\n 642\n 643\n 644\n 645\n 646\n 647\n 648\n 649\n 650\n 651\n 652\n 653\n 654\n 655\n 656\n 657\n 658\n 659\n 660\n 661\n 662\n 663\n 664\n 665\n 666\n 667\n 668\n 669\n 670\n 671\n 672\n 673\n 674\n 675\n 676\n 677\n 678\n 679\n 680\n 681\n 682\n 683\n 684\n 685\n 686\n 687\n 688\n 689\n 690\n 691\n 692\n 693\n 694\n 695\n 696\n 697\n 698\n 699\n 700\n 701\n 702\n 703\n 704\n 705\n 706\n 707\n 708\n 709\n 710\n 711\n 712\n 713\n 714\n 715\n 716\n 717\n 718\n 719\n 720\n 721\n 722\n 723\n 724\n 725\n 726\n 727\n 728\n 729\n 730\n 731\n 732\n 733\n 734\n 735\n 736\n 737\n 738\n 739\n 740\n 741\n 742\n 743\n 744\n 745\n 746\n 747\n 748\n 749\n 750\n 751\n 752\n 753\n 754\n 755\n 756\n 757\n 758\n 759\n 760\n 761\n 762\n 763\n 764\n 765\n 766\n 767\n 768\n 769\n 770\n 771\n 772\n 773\n 774\n 775\n 776\n 777\n 778\n 779\n 780\n 781\n 782\n 783\n 784\n 785\n 786\n 787\n 788\n 789\n 790\n 791\n 792\n 793\n 794\n 795\n 796\n 797\n 798\n 799\n 800\n 801\n 802\n 803\n 804\n 805\n 806\n 807\n 808\n 809\n 810\n 811\n 812\n 813\n 814\n 815\n 816\n 817\n 818\n 819\n 820\n 821\n 822\n 823\n 824\n 825\n 826\n 827\n 828\n 829\n 830\n 831\n 832\n 833\n 834\n 835\n 836\n 837\n 838\n 839\n 840\n 841\n 842\n 843\n 844\n 845\n 846\n 847\n 848\n 849\n 850\n 851\n 852\n 853\n 854\n 855\n 856\n 857\n 858\n 859\n 860\n 861\n 862\n 863\n 864\n 865\n 866\n 867\n 868\n 869\n 870\n 871\n 872\n 873\n 874\n 875\n 876\n 877\n 878\n 879\n 880\n 881\n 882\n 883\n 884\n 885\n 886\n 887\n 888\n 889\n 890\n 891\n 892\n 893\n 894\n 895\n 896\n 897\n 898\n 899\n 900\n 901\n 902\n 903\n 904\n 905\n 906\n 907\n 908\n 909\n 910\n 911\n 912\n 913\n 914\n 915\n 916\n 917\n 918\n 919\n 920\n 921\n 922\n 923\n 924\n 925\n 926\n 927\n 928\n 929\n 930\n 931\n 932\n 933\n 934\n 935\n 936\n 937\n 938\n 939\n 940\n 941\n 942\n 943\n 944\n 945\n 946\n 947\n 948\n 949\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396\n1397\n1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406\n1407\n1408\n1409\n1410\n1411\n1412\n1413\n1414\n1415\n1416\n1417\n1418",
      "language": "unknown"
    },
    {
      "code": "class LMCacheConnectorV1Impl:\n    def __init__(\n        self,\n        vllm_config: \"VllmConfig\",\n        role: KVConnectorRole,\n        parent: KVConnectorBase_V1,\n    ):\n        assert vllm_config.kv_transfer_config is not None\n        self._parent = parent\n        self._vllm_config = vllm_config\n        self.kv_role = vllm_config.kv_transfer_config.kv_role\n        self.worker_count = vllm_config.parallel_config.tensor_parallel_size\n        config = lmcache_get_or_create_config()\n        assert isinstance(config, LMCacheEngineConfig), (\n            \"LMCache v1 configuration is should be passed for vLLM v1.\"\n        )\n        # Put the leading with \"lmcache.\" and matched configs from\n        # vllm extra_config to the config\n        kv_connector_extra_config = (\n            vllm_config.kv_transfer_config.kv_connector_extra_config\n        )\n        if kv_connector_extra_config:\n            for key, value in kv_connector_extra_config.items():\n                if key.startswith(\"lmcache.\"):\n                    config_key = key[8:]  # Remove \"lmcache.\" prefix\n                    if _validate_and_set_config_value(config, config_key, value):\n                        logger.info(\n                            \"Updated config %s from vLLM extra config: %s\",\n                            config_key,\n                            value,\n                        )\n\n        self.config = config\n\n        self.async_loading = config.enable_async_loading\n        self.layerwise_retrievers: list[Generator[torch.Tensor | None, None, None]] = []\n        self._stats_monitor = LMCStatsMonitor.GetOrCreate()\n        if role == KVConnectorRole.SCHEDULER:\n            # Create lookup client using factory\n            self.lookup_client = LookupClientFactory.create_lookup_client(\n                vllm_config, config\n            )\n            self._unfinished_requests: dict[str, Request] = {}\n            self._lookup_requests_in_step: list[str] = []\n            self.lmcache_engine = None\n        else:\n            self.lmcache_engine = _init_lmcache_engine(\n                config,\n                vllm_config,\n            )\n\n            self.use_layerwise = config.use_layerwise\n            self.enable_blending = config.enable_blending\n\n            if self.enable_blending:\n                self.blender = LMCBlenderBuilder.get_or_create(\n                    ENGINE_NAME,\n                    self.lmcache_engine,\n                    self.lmcache_engine.gpu_connector,\n                    config,\n                )\n\n            # Create lookup server using factory\n            assert self.lmcache_engine is not None\n            self.lookup_server = LookupClientFactory.create_lookup_server(\n                self.lmcache_engine, vllm_config\n            )\n\n            self.offload_server = ZMQOffloadServer(\n                self.lmcache_engine,\n                vllm_config,\n                get_tensor_model_parallel_rank(),\n            )\n\n            # In case of MLA, the lookup server is only created on worker 0\n            if self.async_loading and self.lookup_server is not None:\n                assert isinstance(self.lookup_server, LMCacheAsyncLookupServer)\n                self.lmcache_engine.post_init(async_lookup_server=self.lookup_server)\n\n        self.kv_caches: dict[str, torch.Tensor] = {}\n\n        self._block_size = vllm_config.cache_config.block_size\n\n        # request_id -> (vllm cached tokens, lmcache cached tokens)\n        self.load_specs: dict[str, LoadSpec] = {}\n\n        self.kv_cache_manager: KVCacheManager | None = None\n\n        # request_id -> full_token_ids\n        self._request_trackers: dict[str, RequestTracker] = {}\n\n        # Whether to discard partial chunks\n        self._discard_partial_chunks = (\n            vllm_config.kv_transfer_config.get_from_extra_config(\n                \"discard_partial_chunks\", False\n            )\n            or not config.save_unfull_chunk\n        )\n\n        self._lmcache_chunk_size = config.chunk_size\n        self._save_decode_cache = config.save_decode_cache\n\n        self.skip_last_n_tokens = vllm_config.kv_transfer_config.get_from_extra_config(\n            \"skip_last_n_tokens\", 0\n        )\n\n        self.num_layers = vllm_config.model_config.get_num_layers(\n            vllm_config.parallel_config\n        )\n        self.current_layer = 0\n\n        self.force_skip_save = bool(os.environ.get(\"LMCACHE_FORCE_SKIP_SAVE\", False))\n\n        self._requests_priority: dict[str, int] = {}\n\n        # TODO(baoloongmao): Internal api server & plugin framework support\n        # dp > 1\n        if (\n            vllm_config.parallel_config.data_parallel_size_local == 1\n            or vllm_config.parallel_config.data_parallel_rank_local == 0\n        ):\n            # Start internal API server if enabled\n            # The enabled check is in the InternalAPIServer constructor\n            self.api_server = InternalAPIServer(self)\n            self.api_server.start()\n            # Launch plugins\n            self.plugin_launcher = RuntimePluginLauncher(\n                self.config,\n                role,\n                self.worker_count,\n                -1\n                if self.lmcache_engine is None  # scheduler side\n                else self.lmcache_engine.metadata.worker_id,\n            )\n            self.plugin_launcher.launch_plugins()\n        else:\n            self.api_server = None  # type: ignore[assignment]\n            self.plugin_launcher = None  # type: ignore[assignment]\n        logger.info(\n            \"LMCache initialized for role %s with version %s, \"\n            \"vllm version %s, lmcache cache_engine metadata: %s\",\n            role,\n            utils.get_version(),\n            VLLM_VERSION,\n            getattr(self.lmcache_engine, \"metadata\", None),\n        )\n\n    def get_inference_info(self) -> dict:\n        \"\"\"Get inference information including vLLM config and related details.\n\n        Returns:\n            dict: Dictionary containing inference information\n        \"\"\"\n        # Get vLLM config information\n        vllm_config = self._vllm_config\n\n        # Use vLLM config's string representation and add specific configs\n        inference_info = {\n            \"vllm_version\": VLLM_VERSION,\n            \"lmcache_version\": utils.get_version(),\n            \"vllm_config\": str(vllm_config),\n            \"model_config\": {\n                \"model\": getattr(vllm_config.model_config, \"model\", None),\n                \"dtype\": str(getattr(vllm_config.model_config, \"dtype\", None)),\n                \"max_model_len\": getattr(\n                    vllm_config.model_config, \"max_model_len\", None\n                ),\n                \"vocab_size\": vllm_config.model_config.get_vocab_size(),\n                \"num_layers\": getattr(\n                    vllm_config.model_config, \"get_num_layers\", lambda _: None\n                )(vllm_config.parallel_config),\n                \"num_attention_heads\": getattr(\n                    vllm_config.model_config, \"get_num_attention_heads\", lambda _: None\n                )(vllm_config.parallel_config),\n                \"num_kv_heads\": getattr(\n                    vllm_config.model_config, \"get_num_kv_heads\", lambda _: None\n                )(vllm_config.parallel_config),\n                \"head_size\": getattr(\n                    vllm_config.model_config, \"get_head_size\", lambda: None\n                )(),\n            },\n            \"cache_config\": {\n                \"block_size\": getattr(vllm_config.cache_config, \"block_size\", None),\n                \"cache_dtype\": str(\n                    getattr(vllm_config.cache_config, \"cache_dtype\", None)\n                ),\n                \"gpu_memory_utilization\": getattr(\n                    vllm_config.cache_config, \"gpu_memory_utilization\", None\n                ),\n            },\n        }\n\n        return inference_info\n\n    def get_inference_version(self) -> str:\n        \"\"\"Get vLLM version information.\n\n        Returns:\n            str: vLLM version string\n        \"\"\"\n        return VLLM_VERSION\n\n    @_lmcache_nvtx_annotate\n    def _init_kv_caches_from_forward_context(self, forward_context: \"ForwardContext\"):\n        for layer_name in forward_context.no_compile_layers:\n            attn_layer = forward_context.no_compile_layers[layer_name]\n            if not hasattr(attn_layer, \"kv_cache\"):\n                logger.debug(\"The layer %s does not have kv_cache, skip it\", layer_name)\n                continue\n\n            if layer_name not in self.kv_caches:\n                self.kv_caches[layer_name] = attn_layer.kv_cache[\n                    forward_context.virtual_engine\n                ]\n\n    ####################\n    # Worker side APIs\n    ####################\n\n    @_lmcache_nvtx_annotate\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs) -> None:\n        \"\"\"Start loading the KV cache from the connector buffer to vLLM's\n        paged KV buffer.\n\n        Args:\n            forward_context (ForwardContext): the forward context.\n\n        Note:\n            The number of elements in kv_caches and layer_names should be\n            the same.\n        \"\"\"\n        self.current_layer = 0\n\n        if len(self.kv_caches) == 0:\n            self._init_kv_caches_from_forward_context(forward_context)\n\n        metadata = self._parent._get_connector_metadata()\n        assert isinstance(metadata, LMCacheConnectorMetadata)\n\n        assert len(self.kv_caches) > 0\n        kvcaches = list(self.kv_caches.values())\n\n        attn_metadata = forward_context.attn_metadata\n        if attn_metadata is None:\n            logger.debug(\"In connector.start_load_kv, but the attn_metadata is None\")\n            return\n\n        assert self.lmcache_engine is not None\n\n        self.lmcache_engine.post_init(kvcaches=kvcaches)\n\n        self.layerwise_retrievers = []\n\n        for idx, request in enumerate(metadata.requests):\n            if request.load_spec is None:\n                continue\n            last_idx = idx\n\n        for idx, request in enumerate(metadata.requests):\n            if request.load_spec is None:\n                continue\n\n            tokens = request.token_ids\n            # TODO: have a pre-allocated buffer to hold the slot_mappings\n            slot_mapping = request.slot_mapping.cuda()\n            assert len(tokens) == len(slot_mapping)\n\n            self._stats_monitor.update_interval_vllm_hit_tokens(\n                request.load_spec.vllm_cached_tokens\n            )\n            token_mask = torch.ones(len(tokens), dtype=torch.bool)\n            masked_token_count = (\n                request.load_spec.vllm_cached_tokens\n                // self._lmcache_chunk_size\n                * self._lmcache_chunk_size\n            )\n            token_mask[:masked_token_count] = False\n\n            lmcache_cached_tokens = request.load_spec.lmcache_cached_tokens\n            if self.use_layerwise:\n                sync = idx == last_idx\n                # NOTE(Jiayi): Perform blending before layerwise prefix caching\n                if self.enable_blending:\n                    # TODO(Jiayi): Need to make prefix caching and blending\n                    # compatible\n                    self.blender.blend(\n                        tokens[:lmcache_cached_tokens],\n                        token_mask[:lmcache_cached_tokens],\n                        kvcaches=kvcaches,\n                        slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                    )\n                else:\n                    layerwise_retriever = self.lmcache_engine.retrieve_layer(\n                        tokens[:lmcache_cached_tokens],\n                        token_mask[:lmcache_cached_tokens],\n                        kvcaches=kvcaches,\n                        slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                        sync=sync,\n                    )\n                    # NOTE: retrieve for two layers at the first layer\n                    next(layerwise_retriever)\n                    next(layerwise_retriever)\n                    self.layerwise_retrievers.append(layerwise_retriever)\n            else:\n                ret_token_mask = self.lmcache_engine.retrieve(\n                    tokens[:lmcache_cached_tokens],\n                    token_mask[:lmcache_cached_tokens],\n                    kvcaches=kvcaches,\n                    slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                    request_configs=request.request_configs,\n                    req_id=request.req_id,\n                )\n\n                # Check the result\n                num_retrieved_tokens = ret_token_mask.sum().item()\n                num_expected_tokens = (\n                    lmcache_cached_tokens - request.load_spec.vllm_cached_tokens\n                )\n                if num_retrieved_tokens < num_expected_tokens:\n                    logger.error(\n                        \"The number of retrieved tokens is less than the \"\n                        \"expected number of tokens! This should not happen!\"\n                    )\n                    logger.error(\n                        \"Num retrieved tokens: %d, num expected tokens: %d\",\n                        num_retrieved_tokens,\n                        num_expected_tokens,\n                    )\n\n    @_lmcache_nvtx_annotate\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n        paged buffer.\n\n        This interface will be useful for layer-by-layer pipelining.\n\n        Args:\n            layer_name: the name of that layer\n        \"\"\"\n        if self.layerwise_retrievers:\n            logger.debug(\"Waiting for layer %s to be loaded\", self.current_layer)\n\n        # Wait for the layer to be loaded\n        for layerwise_retriever in self.layerwise_retrievers:\n            ret_token_mask = next(layerwise_retriever)\n\n            if self.current_layer == self.num_layers - 1:\n                assert ret_token_mask is not None\n                num_retrieved_tokens = ret_token_mask.sum().item()\n                logger.info(\"Retrieved %s tokens\", num_retrieved_tokens)\n\n        return\n\n    @_lmcache_nvtx_annotate\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        **kwargs,\n    ) -> None:\n        \"\"\"Start saving the a layer of KV cache from vLLM's paged buffer\n        to the connector.\n\n        Args:\n            layer_name (str): the name of the layer.\n            kv_layer (torch.Tensor): the paged KV buffer of the current\n                layer in vLLM.\n            attn_metadata (AttentionMetadata): the attention metadata.\n        \"\"\"\n        assert self.lmcache_engine is not None\n\n        if not self.use_layerwise:\n            return\n\n        if self.kv_role == \"kv_consumer\":\n            # Don't do save if the role is kv_consumer\n            return\n        if self._parent._connector_metadata is None:\n            logger.warning(\n                \"In connector.save_kv_layer, but the connector metadata is None\"\n            )\n            return\n        connector_metadata = self._parent._get_connector_metadata()\n        assert isinstance(connector_metadata, LMCacheConnectorMetadata)\n\n        assert len(self.kv_caches) > 0\n\n        kvcaches = list(self.kv_caches.values())\n        if self.current_layer == 0:\n            self.layerwise_storers = []\n\n            is_first = True\n\n            for idx, request in enumerate(connector_metadata.requests):\n                save_spec = request.save_spec\n                if save_spec is None or not save_spec.can_save:\n                    continue\n\n                token_ids = request.token_ids\n                assert isinstance(token_ids, list)\n\n                slot_mapping = request.slot_mapping\n                assert isinstance(slot_mapping, torch.Tensor)\n                assert len(slot_mapping) == len(token_ids)\n\n                # TODO: have a pre-allocated buffer to hold the slot_mappings\n                slot_mapping = slot_mapping.cuda()\n\n                if self.kv_role == \"kv_producer\":\n                    skip_leading_tokens = 0\n                else:\n                    skip_leading_tokens = save_spec.skip_leading_tokens\n\n                    if skip_leading_tokens == len(token_ids):\n                        continue  # skip this request\n                    # Align to lmcache chunk size\n                    skip_leading_tokens = (\n                        skip_leading_tokens\n                        // self._lmcache_chunk_size\n                        * self._lmcache_chunk_size\n                    )\n\n                store_mask = torch.ones(len(token_ids), dtype=torch.bool)\n                store_mask[:skip_leading_tokens] = False\n\n                logger.info(\n                    \"Storing KV cache for %d out of %d tokens \"\n                    \"(skip_leading_tokens=%d) for request %s\",\n                    len(token_ids) - skip_leading_tokens,\n                    len(token_ids),\n                    skip_leading_tokens,\n                    request.req_id,\n                )\n\n                # TODO (Jiayi): need to make layerwise storing\n                # compatible with disagg spec\n                layerwise_storer = self.lmcache_engine.store_layer(\n                    token_ids,\n                    mask=store_mask,\n                    kvcaches=kvcaches,\n                    slot_mapping=slot_mapping,\n                    offset=skip_leading_tokens,\n                    sync=is_first,\n                )\n                self.layerwise_storers.append(layerwise_storer)\n                if is_first:\n                    is_first = False\n\n        for layerwise_storer in self.layerwise_storers:\n            next(layerwise_storer)\n\n        self.current_layer += 1\n\n    @_lmcache_nvtx_annotate\n    def wait_for_save(self):\n        \"\"\"Blocking until the KV cache is saved to the connector buffer.\"\"\"\n\n        connector_metadata = self._parent._get_connector_metadata()\n        assert isinstance(connector_metadata, LMCacheConnectorMetadata)\n\n        self.lmcache_engine.lookup_unpin(  # type: ignore\n            connector_metadata.lookup_requests_in_step\n        )\n\n        if self.kv_role == \"kv_consumer\":\n            # Don't do save if the role is kv_consumer\n            return\n\n        if self.use_layerwise:\n            for layerwise_storer in self.layerwise_storers:\n                next(layerwise_storer)\n            return\n\n        assert len(self.kv_caches) > 0\n        kvcaches = list(self.kv_caches.values())\n\n        assert self.lmcache_engine is not None\n\n        for request in connector_metadata.requests:\n            save_spec = request.save_spec\n            if (\n                save_spec is None or not save_spec.can_save\n            ) and self.kv_role != \"kv_producer\":\n                continue\n\n            token_ids = request.token_ids\n\n            slot_mapping = request.slot_mapping\n            assert isinstance(slot_mapping, torch.Tensor)\n            assert len(slot_mapping) == len(token_ids)\n            assert save_spec is not None\n\n            # TODO: have a pre-allocated buffer to hold the slot_mappings\n            slot_mapping = slot_mapping.cuda()\n\n            skip_leading_tokens = save_spec.skip_leading_tokens\n            if self.kv_role == \"kv_producer\":\n                assert request.disagg_spec is not None\n                skip_leading_tokens = min(\n                    skip_leading_tokens, request.disagg_spec.num_transferred_tokens\n                )\n\n            if skip_leading_tokens == len(token_ids):\n                continue  # skip this request\n            # Align to lmcache chunk size\n            skip_leading_tokens = (\n                skip_leading_tokens\n                // self._lmcache_chunk_size\n                * self._lmcache_chunk_size\n            )\n\n            store_mask = torch.ones(len(token_ids), dtype=torch.bool)\n            store_mask[:skip_leading_tokens] = False\n\n            logger.info(\n                \"Storing KV cache for %d out of %d tokens \"\n                \"(skip_leading_tokens=%d) for request %s\",\n                len(token_ids) - skip_leading_tokens,\n                len(token_ids),\n                skip_leading_tokens,\n                request.req_id,\n            )\n\n            is_last_prefill = request.is_last_prefill\n            if is_last_prefill:\n                if request.disagg_spec:\n                    request.disagg_spec.is_last_prefill = True\n            else:\n                token_len = len(token_ids)\n                aligned_token_len = (\n                    token_len // self._lmcache_chunk_size * self._lmcache_chunk_size\n                )\n                token_ids = token_ids[:aligned_token_len]\n                store_mask = store_mask[:aligned_token_len]\n                slot_mapping = slot_mapping[:aligned_token_len]\n\n            self.lmcache_engine.store(\n                token_ids,\n                mask=store_mask,\n                kvcaches=kvcaches,\n                slot_mapping=slot_mapping,\n                offset=skip_leading_tokens,\n                transfer_spec=request.disagg_spec,\n                request_configs=request.request_configs,\n            )\n\n            # NOTE(Jiayi): We assume all tokens are saved\n            save_spec.skip_leading_tokens = len(token_ids)\n            if request.disagg_spec:\n                request.disagg_spec.num_transferred_tokens = len(token_ids)\n\n    @_lmcache_nvtx_annotate\n    def get_finished(\n        self, finished_req_ids: set[str]\n    ) -> tuple[set[str] | None, set[str] | None]:\n        return None, None\n\n    ###################\n    # Scheduler side APIs\n    ####################\n\n    @_lmcache_nvtx_annotate\n    def get_num_new_matched_tokens(\n        self,\n        request: \"Request\",\n        num_computed_tokens: int,\n    ) -> int | None:\n        \"\"\"\n        Check for external KV cache hit.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n\n        Returns:\n            the number of tokens that can be loaded from the\n            external KV cache beyond what is already computed.\n        \"\"\"\n        if self.kv_role == \"kv_producer\" and not hasattr(\n            self.lookup_client, \"supports_producer_reuse\"\n        ):\n            return 0\n\n        self._requests_priority[request.request_id] = request.priority\n\n        token_ids = request.prompt_token_ids\n\n        # If the request has multimodal hashes, apply them to the token ids\n        mm_hashes, mm_positions = extract_mm_features(request)\n        if mm_hashes and mm_positions:\n            # TODO(Jiayi): Optimize this\n            token_ids_tensor = torch.tensor(request.prompt_token_ids)\n            apply_mm_hashes_to_token_ids(token_ids_tensor, mm_hashes, mm_positions)\n            token_ids = token_ids_tensor.tolist()\n\n        if request.sampling_params:\n            request_configs = extract_request_configs(request.sampling_params)\n        else:\n            request_configs = None\n\n        if self.skip_last_n_tokens > 0:\n            assert token_ids is not None\n            token_ids = token_ids[: -self.skip_last_n_tokens]\n        lookup_id = request.request_id if self.async_loading else str(uuid.uuid4())\n\n        self._lookup_requests_in_step.append(lookup_id)\n\n        num_external_hit_tokens = self.lookup_client.lookup(\n            token_ids,\n            lookup_id=lookup_id,\n            request_configs=request_configs,\n        )\n\n        if num_external_hit_tokens is None:\n            logger.info(\n                \"Reqid: %s, Total tokens %d, LMCache hit tokens: None.\",\n                request.request_id,\n                request.num_tokens,\n            )\n            return None\n\n        # When prompt length is divisible by the block size and all\n        # blocks are cached, we need to recompute the last token.\n        # This will be removed in the future if vLLM's scheduler provides\n        # a better support for this case.\n        need_to_allocate = num_external_hit_tokens - num_computed_tokens\n\n        # In, full-prompt-hit case, we need to recompute the last token\n        if num_external_hit_tokens == request.num_tokens:\n            need_to_allocate -= 1\n\n        logger.info(\n            \"Reqid: %s, Total tokens %d, LMCache hit tokens: %d, need to load: %d\",\n            request.request_id,\n            request.num_tokens,\n            num_external_hit_tokens,\n            need_to_allocate,\n        )\n\n        self.load_specs[request.request_id] = LoadSpec(\n            vllm_cached_tokens=num_computed_tokens,\n            lmcache_cached_tokens=num_external_hit_tokens,\n            can_load=False,\n        )\n\n        if need_to_allocate <= 0:\n            return 0\n\n        return need_to_allocate\n\n    @_lmcache_nvtx_annotate\n    def update_state_after_alloc(self, request: \"Request\", num_external_tokens: int):\n        \"\"\"\n        Update KVConnector state after temporary buffer alloc.\n\n        For SharedStorageConnector, update _request_needs_load\n        if the CacheManager this allocated blocks for us.\n        \"\"\"\n\n        # Clear local status in lookup client when a new request is\n        # successfully scheduled.\n        self.lookup_client.clear_lookup_status(request.request_id)\n\n        kv_transfer_params = (\n            request.kv_transfer_params\n            if hasattr(request, \"kv_transfer_params\")\n            else None\n        )\n\n        if kv_transfer_params is not None and \"disagg_spec\" in kv_transfer_params:\n            req_disagg_spec = kv_transfer_params[\"disagg_spec\"]\n\n            receiver_id = req_disagg_spec[\"receiver_host\"] + str(\n                req_disagg_spec[\"receiver_init_port\"]\n            )\n\n            disagg_spec = DisaggSpec(\n                req_id=req_disagg_spec[\"req_id\"],\n                receiver_id=receiver_id,\n                receiver_host=req_disagg_spec[\"receiver_host\"],\n                receiver_init_port=req_disagg_spec[\"receiver_init_port\"],\n                receiver_alloc_port=req_disagg_spec[\"receiver_alloc_port\"],\n            )\n\n            tmp_disagg_tracker[request.request_id] = disagg_spec\n        self._unfinished_requests[request.request_id] = request\n\n        if request.request_id not in self.load_specs:\n            # No KV tokens from external KV cache, return\n            return\n\n        if num_external_tokens == 0:\n            # No need to load anything\n            self.load_specs[request.request_id].can_load = False\n            return\n\n        # Only check for non-prompt-hit case\n        if (\n            self.load_specs[request.request_id].lmcache_cached_tokens\n            != request.num_tokens\n        ):\n            assert (\n                num_external_tokens > 0\n                and num_external_tokens\n                == self.load_specs[request.request_id].lmcache_cached_tokens\n                - self.load_specs[request.request_id].vllm_cached_tokens\n            ), (\n                f\"Mismatch in number of tokens: {num_external_tokens} vs \"\n                f\"{self.load_specs[request.request_id].lmcache_cached_tokens} -\"\n                f\" {self.load_specs[request.request_id].vllm_cached_tokens}\"\n                f\" for request {request.request_id}\"\n            )\n\n        self.load_specs[request.request_id].can_load = True\n\n    @_lmcache_nvtx_annotate\n    def build_connector_meta(\n        self, scheduler_output: SchedulerOutput\n    ) -> KVConnectorMetadata:\n        \"\"\"Attach the connector metadata to the request object.\n\n        This function should NOT modify other fields in the scheduler_output\n        except the `kv_connector_metadata` field.\n        Also, calling this function will reset the state of the connector.\n\n        Args:\n            scheduler_output (SchedulerOutput): the scheduler output object.\n        \"\"\"\n\n        force_skip_save = self.kv_role == \"kv_consumer\" or self.force_skip_save\n\n        meta = LMCacheConnectorMetadata()\n\n        # set and update lookup requests for unpin\n        meta.lookup_requests_in_step = self._lookup_requests_in_step\n        self._lookup_requests_in_step = []\n\n        for finished_req_id in scheduler_output.finished_req_ids:\n            self._request_trackers.pop(finished_req_id, None)\n            self._unfinished_requests.pop(finished_req_id, None)\n\n        for request in scheduler_output.scheduled_new_reqs:\n            # Right now, we only load KV for new requests\n            load_spec = self.load_specs.pop(request.req_id, None)\n            num_tokens_to_compute = (\n                request.num_computed_tokens\n                + scheduler_output.num_scheduled_tokens[request.req_id]\n            )\n            lmcache_cached_tokens = 0\n            if load_spec is not None:\n                lmcache_cached_tokens = load_spec.lmcache_cached_tokens\n            request_priority = self._requests_priority.pop(request.req_id, 0)\n\n            skip_save = force_skip_save or (\n                self.config.priority_limit is not None\n                and request_priority > self.config.priority_limit\n            )\n\n            request_tracker = RequestTracker.from_new_request(\n                self.config,\n                request,\n                num_tokens_to_compute,\n                lmcache_cached_tokens,\n                skip_save,\n            )\n            self._request_trackers[request.req_id] = request_tracker\n\n            req_meta = ReqMeta.from_request_tracker(\n                request_tracker,\n                self._block_size,\n                self._lmcache_chunk_size,\n                load_spec=load_spec,\n                discard_partial_chunks=self._discard_partial_chunks,\n                save_decode_cache=self._save_decode_cache,\n            )\n            if req_meta is not None:\n                meta.add_request(req_meta)\n\n        cached_reqs = scheduler_output.scheduled_cached_reqs\n\n        # NOTE: For backward compatibility with vllm version < 0.9.2,\n        # In the latest vllm version, the type of scheduled_cached_reqs has\n        # changed from list to object `CachedRequestData`\n        if isinstance(cached_reqs, list):\n            for i, req in enumerate(cached_reqs):\n                request_tracker = self._request_trackers[req.req_id]\n                request_tracker.update(req.new_token_ids, req.new_block_ids)\n\n                req_meta = ReqMeta.from_request_tracker(\n                    request_tracker,\n                    self._block_size,\n                    self._lmcache_chunk_size,\n                    load_spec=None,\n                    discard_partial_chunks=self._discard_partial_chunks,\n                )\n                if req_meta is not None:\n                    meta.add_request(req_meta)\n            return meta\n\n        for i, req_id in enumerate(cached_reqs.req_ids):\n            request_tracker = self._request_trackers[req_id]\n            num_new_tokens = scheduler_output.num_scheduled_tokens[req_id]\n            if cached_request := self._unfinished_requests.get(req_id):\n                num_current_tokens = len(request_tracker.token_ids)\n                new_token_ids = cached_request.all_token_ids[\n                    num_current_tokens : num_current_tokens + num_new_tokens\n                ]\n            else:\n                raise ValueError(\n                    f\"Request {req_id} is not in _unfinished_requests, \"\n                    f\"but it is scheduled to be cached\"\n                )\n            new_block_ids = cached_reqs.new_block_ids[i]\n\n            request_tracker.update(new_token_ids, new_block_ids)\n\n            req_meta = ReqMeta.from_request_tracker(\n                request_tracker,\n                self._block_size,\n                self._lmcache_chunk_size,\n                load_spec=None,\n                discard_partial_chunks=self._discard_partial_chunks,\n                save_decode_cache=self._save_decode_cache,\n            )\n            if req_meta is not None:\n                meta.add_request(req_meta)\n\n        return meta\n\n    @_lmcache_nvtx_annotate\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        params = (\n            request.kv_transfer_params\n            if hasattr(request, \"kv_transfer_params\")\n            else None\n        )\n        return_params = None\n\n        # NOTE: Used to stream back the first token\n        # for disagg prefill\n        if params is not None and \"ret_first_tok\" in params:\n            return_params = {\n                \"first_tok\": request._output_token_ids[0],\n            }\n\n        return False, return_params",
      "language": "python"
    },
    {
      "code": "class LMCacheConnectorV1Impl:\n    def __init__(\n        self,\n        vllm_config: \"VllmConfig\",\n        role: KVConnectorRole,\n        parent: KVConnectorBase_V1,\n    ):\n        assert vllm_config.kv_transfer_config is not None\n        self._parent = parent\n        self._vllm_config = vllm_config\n        self.kv_role = vllm_config.kv_transfer_config.kv_role\n        self.worker_count = vllm_config.parallel_config.tensor_parallel_size\n        config = lmcache_get_or_create_config()\n        assert isinstance(config, LMCacheEngineConfig), (\n            \"LMCache v1 configuration is should be passed for vLLM v1.\"\n        )\n        # Put the leading with \"lmcache.\" and matched configs from\n        # vllm extra_config to the config\n        kv_connector_extra_config = (\n            vllm_config.kv_transfer_config.kv_connector_extra_config\n        )\n        if kv_connector_extra_config:\n            for key, value in kv_connector_extra_config.items():\n                if key.startswith(\"lmcache.\"):\n                    config_key = key[8:]  # Remove \"lmcache.\" prefix\n                    if _validate_and_set_config_value(config, config_key, value):\n                        logger.info(\n                            \"Updated config %s from vLLM extra config: %s\",\n                            config_key,\n                            value,\n                        )\n\n        self.config = config\n\n        self.async_loading = config.enable_async_loading\n        self.layerwise_retrievers: list[Generator[torch.Tensor | None, None, None]] = []\n        self._stats_monitor = LMCStatsMonitor.GetOrCreate()\n        if role == KVConnectorRole.SCHEDULER:\n            # Create lookup client using factory\n            self.lookup_client = LookupClientFactory.create_lookup_client(\n                vllm_config, config\n            )\n            self._unfinished_requests: dict[str, Request] = {}\n            self._lookup_requests_in_step: list[str] = []\n            self.lmcache_engine = None\n        else:\n            self.lmcache_engine = _init_lmcache_engine(\n                config,\n                vllm_config,\n            )\n\n            self.use_layerwise = config.use_layerwise\n            self.enable_blending = config.enable_blending\n\n            if self.enable_blending:\n                self.blender = LMCBlenderBuilder.get_or_create(\n                    ENGINE_NAME,\n                    self.lmcache_engine,\n                    self.lmcache_engine.gpu_connector,\n                    config,\n                )\n\n            # Create lookup server using factory\n            assert self.lmcache_engine is not None\n            self.lookup_server = LookupClientFactory.create_lookup_server(\n                self.lmcache_engine, vllm_config\n            )\n\n            self.offload_server = ZMQOffloadServer(\n                self.lmcache_engine,\n                vllm_config,\n                get_tensor_model_parallel_rank(),\n            )\n\n            # In case of MLA, the lookup server is only created on worker 0\n            if self.async_loading and self.lookup_server is not None:\n                assert isinstance(self.lookup_server, LMCacheAsyncLookupServer)\n                self.lmcache_engine.post_init(async_lookup_server=self.lookup_server)\n\n        self.kv_caches: dict[str, torch.Tensor] = {}\n\n        self._block_size = vllm_config.cache_config.block_size\n\n        # request_id -> (vllm cached tokens, lmcache cached tokens)\n        self.load_specs: dict[str, LoadSpec] = {}\n\n        self.kv_cache_manager: KVCacheManager | None = None\n\n        # request_id -> full_token_ids\n        self._request_trackers: dict[str, RequestTracker] = {}\n\n        # Whether to discard partial chunks\n        self._discard_partial_chunks = (\n            vllm_config.kv_transfer_config.get_from_extra_config(\n                \"discard_partial_chunks\", False\n            )\n            or not config.save_unfull_chunk\n        )\n\n        self._lmcache_chunk_size = config.chunk_size\n        self._save_decode_cache = config.save_decode_cache\n\n        self.skip_last_n_tokens = vllm_config.kv_transfer_config.get_from_extra_config(\n            \"skip_last_n_tokens\", 0\n        )\n\n        self.num_layers = vllm_config.model_config.get_num_layers(\n            vllm_config.parallel_config\n        )\n        self.current_layer = 0\n\n        self.force_skip_save = bool(os.environ.get(\"LMCACHE_FORCE_SKIP_SAVE\", False))\n\n        self._requests_priority: dict[str, int] = {}\n\n        # TODO(baoloongmao): Internal api server & plugin framework support\n        # dp > 1\n        if (\n            vllm_config.parallel_config.data_parallel_size_local == 1\n            or vllm_config.parallel_config.data_parallel_rank_local == 0\n        ):\n            # Start internal API server if enabled\n            # The enabled check is in the InternalAPIServer constructor\n            self.api_server = InternalAPIServer(self)\n            self.api_server.start()\n            # Launch plugins\n            self.plugin_launcher = RuntimePluginLauncher(\n                self.config,\n                role,\n                self.worker_count,\n                -1\n                if self.lmcache_engine is None  # scheduler side\n                else self.lmcache_engine.metadata.worker_id,\n            )\n            self.plugin_launcher.launch_plugins()\n        else:\n            self.api_server = None  # type: ignore[assignment]\n            self.plugin_launcher = None  # type: ignore[assignment]\n        logger.info(\n            \"LMCache initialized for role %s with version %s, \"\n            \"vllm version %s, lmcache cache_engine metadata: %s\",\n            role,\n            utils.get_version(),\n            VLLM_VERSION,\n            getattr(self.lmcache_engine, \"metadata\", None),\n        )\n\n    def get_inference_info(self) -> dict:\n        \"\"\"Get inference information including vLLM config and related details.\n\n        Returns:\n            dict: Dictionary containing inference information\n        \"\"\"\n        # Get vLLM config information\n        vllm_config = self._vllm_config\n\n        # Use vLLM config's string representation and add specific configs\n        inference_info = {\n            \"vllm_version\": VLLM_VERSION,\n            \"lmcache_version\": utils.get_version(),\n            \"vllm_config\": str(vllm_config),\n            \"model_config\": {\n                \"model\": getattr(vllm_config.model_config, \"model\", None),\n                \"dtype\": str(getattr(vllm_config.model_config, \"dtype\", None)),\n                \"max_model_len\": getattr(\n                    vllm_config.model_config, \"max_model_len\", None\n                ),\n                \"vocab_size\": vllm_config.model_config.get_vocab_size(),\n                \"num_layers\": getattr(\n                    vllm_config.model_config, \"get_num_layers\", lambda _: None\n                )(vllm_config.parallel_config),\n                \"num_attention_heads\": getattr(\n                    vllm_config.model_config, \"get_num_attention_heads\", lambda _: None\n                )(vllm_config.parallel_config),\n                \"num_kv_heads\": getattr(\n                    vllm_config.model_config, \"get_num_kv_heads\", lambda _: None\n                )(vllm_config.parallel_config),\n                \"head_size\": getattr(\n                    vllm_config.model_config, \"get_head_size\", lambda: None\n                )(),\n            },\n            \"cache_config\": {\n                \"block_size\": getattr(vllm_config.cache_config, \"block_size\", None),\n                \"cache_dtype\": str(\n                    getattr(vllm_config.cache_config, \"cache_dtype\", None)\n                ),\n                \"gpu_memory_utilization\": getattr(\n                    vllm_config.cache_config, \"gpu_memory_utilization\", None\n                ),\n            },\n        }\n\n        return inference_info\n\n    def get_inference_version(self) -> str:\n        \"\"\"Get vLLM version information.\n\n        Returns:\n            str: vLLM version string\n        \"\"\"\n        return VLLM_VERSION\n\n    @_lmcache_nvtx_annotate\n    def _init_kv_caches_from_forward_context(self, forward_context: \"ForwardContext\"):\n        for layer_name in forward_context.no_compile_layers:\n            attn_layer = forward_context.no_compile_layers[layer_name]\n            if not hasattr(attn_layer, \"kv_cache\"):\n                logger.debug(\"The layer %s does not have kv_cache, skip it\", layer_name)\n                continue\n\n            if layer_name not in self.kv_caches:\n                self.kv_caches[layer_name] = attn_layer.kv_cache[\n                    forward_context.virtual_engine\n                ]\n\n    ####################\n    # Worker side APIs\n    ####################\n\n    @_lmcache_nvtx_annotate\n    def start_load_kv(self, forward_context: \"ForwardContext\", **kwargs) -> None:\n        \"\"\"Start loading the KV cache from the connector buffer to vLLM's\n        paged KV buffer.\n\n        Args:\n            forward_context (ForwardContext): the forward context.\n\n        Note:\n            The number of elements in kv_caches and layer_names should be\n            the same.\n        \"\"\"\n        self.current_layer = 0\n\n        if len(self.kv_caches) == 0:\n            self._init_kv_caches_from_forward_context(forward_context)\n\n        metadata = self._parent._get_connector_metadata()\n        assert isinstance(metadata, LMCacheConnectorMetadata)\n\n        assert len(self.kv_caches) > 0\n        kvcaches = list(self.kv_caches.values())\n\n        attn_metadata = forward_context.attn_metadata\n        if attn_metadata is None:\n            logger.debug(\"In connector.start_load_kv, but the attn_metadata is None\")\n            return\n\n        assert self.lmcache_engine is not None\n\n        self.lmcache_engine.post_init(kvcaches=kvcaches)\n\n        self.layerwise_retrievers = []\n\n        for idx, request in enumerate(metadata.requests):\n            if request.load_spec is None:\n                continue\n            last_idx = idx\n\n        for idx, request in enumerate(metadata.requests):\n            if request.load_spec is None:\n                continue\n\n            tokens = request.token_ids\n            # TODO: have a pre-allocated buffer to hold the slot_mappings\n            slot_mapping = request.slot_mapping.cuda()\n            assert len(tokens) == len(slot_mapping)\n\n            self._stats_monitor.update_interval_vllm_hit_tokens(\n                request.load_spec.vllm_cached_tokens\n            )\n            token_mask = torch.ones(len(tokens), dtype=torch.bool)\n            masked_token_count = (\n                request.load_spec.vllm_cached_tokens\n                // self._lmcache_chunk_size\n                * self._lmcache_chunk_size\n            )\n            token_mask[:masked_token_count] = False\n\n            lmcache_cached_tokens = request.load_spec.lmcache_cached_tokens\n            if self.use_layerwise:\n                sync = idx == last_idx\n                # NOTE(Jiayi): Perform blending before layerwise prefix caching\n                if self.enable_blending:\n                    # TODO(Jiayi): Need to make prefix caching and blending\n                    # compatible\n                    self.blender.blend(\n                        tokens[:lmcache_cached_tokens],\n                        token_mask[:lmcache_cached_tokens],\n                        kvcaches=kvcaches,\n                        slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                    )\n                else:\n                    layerwise_retriever = self.lmcache_engine.retrieve_layer(\n                        tokens[:lmcache_cached_tokens],\n                        token_mask[:lmcache_cached_tokens],\n                        kvcaches=kvcaches,\n                        slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                        sync=sync,\n                    )\n                    # NOTE: retrieve for two layers at the first layer\n                    next(layerwise_retriever)\n                    next(layerwise_retriever)\n                    self.layerwise_retrievers.append(layerwise_retriever)\n            else:\n                ret_token_mask = self.lmcache_engine.retrieve(\n                    tokens[:lmcache_cached_tokens],\n                    token_mask[:lmcache_cached_tokens],\n                    kvcaches=kvcaches,\n                    slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                    request_configs=request.request_configs,\n                    req_id=request.req_id,\n                )\n\n                # Check the result\n                num_retrieved_tokens = ret_token_mask.sum().item()\n                num_expected_tokens = (\n                    lmcache_cached_tokens - request.load_spec.vllm_cached_tokens\n                )\n                if num_retrieved_tokens < num_expected_tokens:\n                    logger.error(\n                        \"The number of retrieved tokens is less than the \"\n                        \"expected number of tokens! This should not happen!\"\n                    )\n                    logger.error(\n                        \"Num retrieved tokens: %d, num expected tokens: %d\",\n                        num_retrieved_tokens,\n                        num_expected_tokens,\n                    )\n\n    @_lmcache_nvtx_annotate\n    def wait_for_layer_load(self, layer_name: str) -> None:\n        \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n        paged buffer.\n\n        This interface will be useful for layer-by-layer pipelining.\n\n        Args:\n            layer_name: the name of that layer\n        \"\"\"\n        if self.layerwise_retrievers:\n            logger.debug(\"Waiting for layer %s to be loaded\", self.current_layer)\n\n        # Wait for the layer to be loaded\n        for layerwise_retriever in self.layerwise_retrievers:\n            ret_token_mask = next(layerwise_retriever)\n\n            if self.current_layer == self.num_layers - 1:\n                assert ret_token_mask is not None\n                num_retrieved_tokens = ret_token_mask.sum().item()\n                logger.info(\"Retrieved %s tokens\", num_retrieved_tokens)\n\n        return\n\n    @_lmcache_nvtx_annotate\n    def save_kv_layer(\n        self,\n        layer_name: str,\n        kv_layer: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        **kwargs,\n    ) -> None:\n        \"\"\"Start saving the a layer of KV cache from vLLM's paged buffer\n        to the connector.\n\n        Args:\n            layer_name (str): the name of the layer.\n            kv_layer (torch.Tensor): the paged KV buffer of the current\n                layer in vLLM.\n            attn_metadata (AttentionMetadata): the attention metadata.\n        \"\"\"\n        assert self.lmcache_engine is not None\n\n        if not self.use_layerwise:\n            return\n\n        if self.kv_role == \"kv_consumer\":\n            # Don't do save if the role is kv_consumer\n            return\n        if self._parent._connector_metadata is None:\n            logger.warning(\n                \"In connector.save_kv_layer, but the connector metadata is None\"\n            )\n            return\n        connector_metadata = self._parent._get_connector_metadata()\n        assert isinstance(connector_metadata, LMCacheConnectorMetadata)\n\n        assert len(self.kv_caches) > 0\n\n        kvcaches = list(self.kv_caches.values())\n        if self.current_layer == 0:\n            self.layerwise_storers = []\n\n            is_first = True\n\n            for idx, request in enumerate(connector_metadata.requests):\n                save_spec = request.save_spec\n                if save_spec is None or not save_spec.can_save:\n                    continue\n\n                token_ids = request.token_ids\n                assert isinstance(token_ids, list)\n\n                slot_mapping = request.slot_mapping\n                assert isinstance(slot_mapping, torch.Tensor)\n                assert len(slot_mapping) == len(token_ids)\n\n                # TODO: have a pre-allocated buffer to hold the slot_mappings\n                slot_mapping = slot_mapping.cuda()\n\n                if self.kv_role == \"kv_producer\":\n                    skip_leading_tokens = 0\n                else:\n                    skip_leading_tokens = save_spec.skip_leading_tokens\n\n                    if skip_leading_tokens == len(token_ids):\n                        continue  # skip this request\n                    # Align to lmcache chunk size\n                    skip_leading_tokens = (\n                        skip_leading_tokens\n                        // self._lmcache_chunk_size\n                        * self._lmcache_chunk_size\n                    )\n\n                store_mask = torch.ones(len(token_ids), dtype=torch.bool)\n                store_mask[:skip_leading_tokens] = False\n\n                logger.info(\n                    \"Storing KV cache for %d out of %d tokens \"\n                    \"(skip_leading_tokens=%d) for request %s\",\n                    len(token_ids) - skip_leading_tokens,\n                    len(token_ids),\n                    skip_leading_tokens,\n                    request.req_id,\n                )\n\n                # TODO (Jiayi): need to make layerwise storing\n                # compatible with disagg spec\n                layerwise_storer = self.lmcache_engine.store_layer(\n                    token_ids,\n                    mask=store_mask,\n                    kvcaches=kvcaches,\n                    slot_mapping=slot_mapping,\n                    offset=skip_leading_tokens,\n                    sync=is_first,\n                )\n                self.layerwise_storers.append(layerwise_storer)\n                if is_first:\n                    is_first = False\n\n        for layerwise_storer in self.layerwise_storers:\n            next(layerwise_storer)\n\n        self.current_layer += 1\n\n    @_lmcache_nvtx_annotate\n    def wait_for_save(self):\n        \"\"\"Blocking until the KV cache is saved to the connector buffer.\"\"\"\n\n        connector_metadata = self._parent._get_connector_metadata()\n        assert isinstance(connector_metadata, LMCacheConnectorMetadata)\n\n        self.lmcache_engine.lookup_unpin(  # type: ignore\n            connector_metadata.lookup_requests_in_step\n        )\n\n        if self.kv_role == \"kv_consumer\":\n            # Don't do save if the role is kv_consumer\n            return\n\n        if self.use_layerwise:\n            for layerwise_storer in self.layerwise_storers:\n                next(layerwise_storer)\n            return\n\n        assert len(self.kv_caches) > 0\n        kvcaches = list(self.kv_caches.values())\n\n        assert self.lmcache_engine is not None\n\n        for request in connector_metadata.requests:\n            save_spec = request.save_spec\n            if (\n                save_spec is None or not save_spec.can_save\n            ) and self.kv_role != \"kv_producer\":\n                continue\n\n            token_ids = request.token_ids\n\n            slot_mapping = request.slot_mapping\n            assert isinstance(slot_mapping, torch.Tensor)\n            assert len(slot_mapping) == len(token_ids)\n            assert save_spec is not None\n\n            # TODO: have a pre-allocated buffer to hold the slot_mappings\n            slot_mapping = slot_mapping.cuda()\n\n            skip_leading_tokens = save_spec.skip_leading_tokens\n            if self.kv_role == \"kv_producer\":\n                assert request.disagg_spec is not None\n                skip_leading_tokens = min(\n                    skip_leading_tokens, request.disagg_spec.num_transferred_tokens\n                )\n\n            if skip_leading_tokens == len(token_ids):\n                continue  # skip this request\n            # Align to lmcache chunk size\n            skip_leading_tokens = (\n                skip_leading_tokens\n                // self._lmcache_chunk_size\n                * self._lmcache_chunk_size\n            )\n\n            store_mask = torch.ones(len(token_ids), dtype=torch.bool)\n            store_mask[:skip_leading_tokens] = False\n\n            logger.info(\n                \"Storing KV cache for %d out of %d tokens \"\n                \"(skip_leading_tokens=%d) for request %s\",\n                len(token_ids) - skip_leading_tokens,\n                len(token_ids),\n                skip_leading_tokens,\n                request.req_id,\n            )\n\n            is_last_prefill = request.is_last_prefill\n            if is_last_prefill:\n                if request.disagg_spec:\n                    request.disagg_spec.is_last_prefill = True\n            else:\n                token_len = len(token_ids)\n                aligned_token_len = (\n                    token_len // self._lmcache_chunk_size * self._lmcache_chunk_size\n                )\n                token_ids = token_ids[:aligned_token_len]\n                store_mask = store_mask[:aligned_token_len]\n                slot_mapping = slot_mapping[:aligned_token_len]\n\n            self.lmcache_engine.store(\n                token_ids,\n                mask=store_mask,\n                kvcaches=kvcaches,\n                slot_mapping=slot_mapping,\n                offset=skip_leading_tokens,\n                transfer_spec=request.disagg_spec,\n                request_configs=request.request_configs,\n            )\n\n            # NOTE(Jiayi): We assume all tokens are saved\n            save_spec.skip_leading_tokens = len(token_ids)\n            if request.disagg_spec:\n                request.disagg_spec.num_transferred_tokens = len(token_ids)\n\n    @_lmcache_nvtx_annotate\n    def get_finished(\n        self, finished_req_ids: set[str]\n    ) -> tuple[set[str] | None, set[str] | None]:\n        return None, None\n\n    ###################\n    # Scheduler side APIs\n    ####################\n\n    @_lmcache_nvtx_annotate\n    def get_num_new_matched_tokens(\n        self,\n        request: \"Request\",\n        num_computed_tokens: int,\n    ) -> int | None:\n        \"\"\"\n        Check for external KV cache hit.\n\n        Args:\n            request (Request): the request object.\n            num_computed_tokens (int): the number of locally\n                computed tokens for this request\n\n        Returns:\n            the number of tokens that can be loaded from the\n            external KV cache beyond what is already computed.\n        \"\"\"\n        if self.kv_role == \"kv_producer\" and not hasattr(\n            self.lookup_client, \"supports_producer_reuse\"\n        ):\n            return 0\n\n        self._requests_priority[request.request_id] = request.priority\n\n        token_ids = request.prompt_token_ids\n\n        # If the request has multimodal hashes, apply them to the token ids\n        mm_hashes, mm_positions = extract_mm_features(request)\n        if mm_hashes and mm_positions:\n            # TODO(Jiayi): Optimize this\n            token_ids_tensor = torch.tensor(request.prompt_token_ids)\n            apply_mm_hashes_to_token_ids(token_ids_tensor, mm_hashes, mm_positions)\n            token_ids = token_ids_tensor.tolist()\n\n        if request.sampling_params:\n            request_configs = extract_request_configs(request.sampling_params)\n        else:\n            request_configs = None\n\n        if self.skip_last_n_tokens > 0:\n            assert token_ids is not None\n            token_ids = token_ids[: -self.skip_last_n_tokens]\n        lookup_id = request.request_id if self.async_loading else str(uuid.uuid4())\n\n        self._lookup_requests_in_step.append(lookup_id)\n\n        num_external_hit_tokens = self.lookup_client.lookup(\n            token_ids,\n            lookup_id=lookup_id,\n            request_configs=request_configs,\n        )\n\n        if num_external_hit_tokens is None:\n            logger.info(\n                \"Reqid: %s, Total tokens %d, LMCache hit tokens: None.\",\n                request.request_id,\n                request.num_tokens,\n            )\n            return None\n\n        # When prompt length is divisible by the block size and all\n        # blocks are cached, we need to recompute the last token.\n        # This will be removed in the future if vLLM's scheduler provides\n        # a better support for this case.\n        need_to_allocate = num_external_hit_tokens - num_computed_tokens\n\n        # In, full-prompt-hit case, we need to recompute the last token\n        if num_external_hit_tokens == request.num_tokens:\n            need_to_allocate -= 1\n\n        logger.info(\n            \"Reqid: %s, Total tokens %d, LMCache hit tokens: %d, need to load: %d\",\n            request.request_id,\n            request.num_tokens,\n            num_external_hit_tokens,\n            need_to_allocate,\n        )\n\n        self.load_specs[request.request_id] = LoadSpec(\n            vllm_cached_tokens=num_computed_tokens,\n            lmcache_cached_tokens=num_external_hit_tokens,\n            can_load=False,\n        )\n\n        if need_to_allocate <= 0:\n            return 0\n\n        return need_to_allocate\n\n    @_lmcache_nvtx_annotate\n    def update_state_after_alloc(self, request: \"Request\", num_external_tokens: int):\n        \"\"\"\n        Update KVConnector state after temporary buffer alloc.\n\n        For SharedStorageConnector, update _request_needs_load\n        if the CacheManager this allocated blocks for us.\n        \"\"\"\n\n        # Clear local status in lookup client when a new request is\n        # successfully scheduled.\n        self.lookup_client.clear_lookup_status(request.request_id)\n\n        kv_transfer_params = (\n            request.kv_transfer_params\n            if hasattr(request, \"kv_transfer_params\")\n            else None\n        )\n\n        if kv_transfer_params is not None and \"disagg_spec\" in kv_transfer_params:\n            req_disagg_spec = kv_transfer_params[\"disagg_spec\"]\n\n            receiver_id = req_disagg_spec[\"receiver_host\"] + str(\n                req_disagg_spec[\"receiver_init_port\"]\n            )\n\n            disagg_spec = DisaggSpec(\n                req_id=req_disagg_spec[\"req_id\"],\n                receiver_id=receiver_id,\n                receiver_host=req_disagg_spec[\"receiver_host\"],\n                receiver_init_port=req_disagg_spec[\"receiver_init_port\"],\n                receiver_alloc_port=req_disagg_spec[\"receiver_alloc_port\"],\n            )\n\n            tmp_disagg_tracker[request.request_id] = disagg_spec\n        self._unfinished_requests[request.request_id] = request\n\n        if request.request_id not in self.load_specs:\n            # No KV tokens from external KV cache, return\n            return\n\n        if num_external_tokens == 0:\n            # No need to load anything\n            self.load_specs[request.request_id].can_load = False\n            return\n\n        # Only check for non-prompt-hit case\n        if (\n            self.load_specs[request.request_id].lmcache_cached_tokens\n            != request.num_tokens\n        ):\n            assert (\n                num_external_tokens > 0\n                and num_external_tokens\n                == self.load_specs[request.request_id].lmcache_cached_tokens\n                - self.load_specs[request.request_id].vllm_cached_tokens\n            ), (\n                f\"Mismatch in number of tokens: {num_external_tokens} vs \"\n                f\"{self.load_specs[request.request_id].lmcache_cached_tokens} -\"\n                f\" {self.load_specs[request.request_id].vllm_cached_tokens}\"\n                f\" for request {request.request_id}\"\n            )\n\n        self.load_specs[request.request_id].can_load = True\n\n    @_lmcache_nvtx_annotate\n    def build_connector_meta(\n        self, scheduler_output: SchedulerOutput\n    ) -> KVConnectorMetadata:\n        \"\"\"Attach the connector metadata to the request object.\n\n        This function should NOT modify other fields in the scheduler_output\n        except the `kv_connector_metadata` field.\n        Also, calling this function will reset the state of the connector.\n\n        Args:\n            scheduler_output (SchedulerOutput): the scheduler output object.\n        \"\"\"\n\n        force_skip_save = self.kv_role == \"kv_consumer\" or self.force_skip_save\n\n        meta = LMCacheConnectorMetadata()\n\n        # set and update lookup requests for unpin\n        meta.lookup_requests_in_step = self._lookup_requests_in_step\n        self._lookup_requests_in_step = []\n\n        for finished_req_id in scheduler_output.finished_req_ids:\n            self._request_trackers.pop(finished_req_id, None)\n            self._unfinished_requests.pop(finished_req_id, None)\n\n        for request in scheduler_output.scheduled_new_reqs:\n            # Right now, we only load KV for new requests\n            load_spec = self.load_specs.pop(request.req_id, None)\n            num_tokens_to_compute = (\n                request.num_computed_tokens\n                + scheduler_output.num_scheduled_tokens[request.req_id]\n            )\n            lmcache_cached_tokens = 0\n            if load_spec is not None:\n                lmcache_cached_tokens = load_spec.lmcache_cached_tokens\n            request_priority = self._requests_priority.pop(request.req_id, 0)\n\n            skip_save = force_skip_save or (\n                self.config.priority_limit is not None\n                and request_priority > self.config.priority_limit\n            )\n\n            request_tracker = RequestTracker.from_new_request(\n                self.config,\n                request,\n                num_tokens_to_compute,\n                lmcache_cached_tokens,\n                skip_save,\n            )\n            self._request_trackers[request.req_id] = request_tracker\n\n            req_meta = ReqMeta.from_request_tracker(\n                request_tracker,\n                self._block_size,\n                self._lmcache_chunk_size,\n                load_spec=load_spec,\n                discard_partial_chunks=self._discard_partial_chunks,\n                save_decode_cache=self._save_decode_cache,\n            )\n            if req_meta is not None:\n                meta.add_request(req_meta)\n\n        cached_reqs = scheduler_output.scheduled_cached_reqs\n\n        # NOTE: For backward compatibility with vllm version < 0.9.2,\n        # In the latest vllm version, the type of scheduled_cached_reqs has\n        # changed from list to object `CachedRequestData`\n        if isinstance(cached_reqs, list):\n            for i, req in enumerate(cached_reqs):\n                request_tracker = self._request_trackers[req.req_id]\n                request_tracker.update(req.new_token_ids, req.new_block_ids)\n\n                req_meta = ReqMeta.from_request_tracker(\n                    request_tracker,\n                    self._block_size,\n                    self._lmcache_chunk_size,\n                    load_spec=None,\n                    discard_partial_chunks=self._discard_partial_chunks,\n                )\n                if req_meta is not None:\n                    meta.add_request(req_meta)\n            return meta\n\n        for i, req_id in enumerate(cached_reqs.req_ids):\n            request_tracker = self._request_trackers[req_id]\n            num_new_tokens = scheduler_output.num_scheduled_tokens[req_id]\n            if cached_request := self._unfinished_requests.get(req_id):\n                num_current_tokens = len(request_tracker.token_ids)\n                new_token_ids = cached_request.all_token_ids[\n                    num_current_tokens : num_current_tokens + num_new_tokens\n                ]\n            else:\n                raise ValueError(\n                    f\"Request {req_id} is not in _unfinished_requests, \"\n                    f\"but it is scheduled to be cached\"\n                )\n            new_block_ids = cached_reqs.new_block_ids[i]\n\n            request_tracker.update(new_token_ids, new_block_ids)\n\n            req_meta = ReqMeta.from_request_tracker(\n                request_tracker,\n                self._block_size,\n                self._lmcache_chunk_size,\n                load_spec=None,\n                discard_partial_chunks=self._discard_partial_chunks,\n                save_decode_cache=self._save_decode_cache,\n            )\n            if req_meta is not None:\n                meta.add_request(req_meta)\n\n        return meta\n\n    @_lmcache_nvtx_annotate\n    def request_finished(\n        self,\n        request: \"Request\",\n        block_ids: list[int],\n    ) -> tuple[bool, dict[str, Any] | None]:\n        params = (\n            request.kv_transfer_params\n            if hasattr(request, \"kv_transfer_params\")\n            else None\n        )\n        return_params = None\n\n        # NOTE: Used to stream back the first token\n        # for disagg prefill\n        if params is not None and \"ret_first_tok\" in params:\n            return_params = {\n                \"first_tok\": request._output_token_ids[0],\n            }\n\n        return False, return_params",
      "language": "python"
    },
    {
      "code": "_block_size = block_size",
      "language": "unknown"
    },
    {
      "code": "_block_size = block_size",
      "language": "unknown"
    },
    {
      "code": "_discard_partial_chunks = (\n    get_from_extra_config(\"discard_partial_chunks\", False)\n    or not save_unfull_chunk\n)",
      "language": "unknown"
    },
    {
      "code": "_discard_partial_chunks = (\n    get_from_extra_config(\"discard_partial_chunks\", False)\n    or not save_unfull_chunk\n)",
      "language": "unknown"
    },
    {
      "code": "_lmcache_chunk_size = chunk_size",
      "language": "unknown"
    },
    {
      "code": "_lmcache_chunk_size = chunk_size",
      "language": "unknown"
    },
    {
      "code": "_lookup_requests_in_step: list[str] = []",
      "language": "yaml"
    },
    {
      "code": "_lookup_requests_in_step: list[str] = []",
      "language": "yaml"
    },
    {
      "code": "_parent = parent",
      "language": "unknown"
    },
    {
      "code": "_parent = parent",
      "language": "unknown"
    },
    {
      "code": "_request_trackers: dict[str, RequestTracker] = {}",
      "language": "yaml"
    },
    {
      "code": "_request_trackers: dict[str, RequestTracker] = {}",
      "language": "yaml"
    },
    {
      "code": "_requests_priority: dict[str, int] = {}",
      "language": "yaml"
    },
    {
      "code": "_requests_priority: dict[str, int] = {}",
      "language": "yaml"
    },
    {
      "code": "_save_decode_cache = save_decode_cache",
      "language": "unknown"
    },
    {
      "code": "_save_decode_cache = save_decode_cache",
      "language": "unknown"
    },
    {
      "code": "_stats_monitor = GetOrCreate()",
      "language": "unknown"
    },
    {
      "code": "_stats_monitor = GetOrCreate()",
      "language": "unknown"
    },
    {
      "code": "_unfinished_requests: dict[str, Request] = {}",
      "language": "yaml"
    },
    {
      "code": "_unfinished_requests: dict[str, Request] = {}",
      "language": "yaml"
    },
    {
      "code": "_vllm_config = vllm_config",
      "language": "unknown"
    },
    {
      "code": "_vllm_config = vllm_config",
      "language": "unknown"
    },
    {
      "code": "api_server = InternalAPIServer(self)",
      "language": "unknown"
    },
    {
      "code": "api_server = InternalAPIServer(self)",
      "language": "unknown"
    },
    {
      "code": "async_loading = enable_async_loading",
      "language": "unknown"
    },
    {
      "code": "async_loading = enable_async_loading",
      "language": "unknown"
    },
    {
      "code": "blender = get_or_create(\n    ENGINE_NAME, lmcache_engine, gpu_connector, config\n)",
      "language": "unknown"
    },
    {
      "code": "blender = get_or_create(\n    ENGINE_NAME, lmcache_engine, gpu_connector, config\n)",
      "language": "unknown"
    },
    {
      "code": "config = config",
      "language": "unknown"
    },
    {
      "code": "config = config",
      "language": "unknown"
    },
    {
      "code": "current_layer = 0",
      "language": "unknown"
    },
    {
      "code": "current_layer = 0",
      "language": "unknown"
    },
    {
      "code": "enable_blending = enable_blending",
      "language": "unknown"
    },
    {
      "code": "enable_blending = enable_blending",
      "language": "unknown"
    },
    {
      "code": "force_skip_save = bool(\n    get(\"LMCACHE_FORCE_SKIP_SAVE\", False)\n)",
      "language": "unknown"
    },
    {
      "code": "force_skip_save = bool(\n    get(\"LMCACHE_FORCE_SKIP_SAVE\", False)\n)",
      "language": "unknown"
    },
    {
      "code": "kv_cache_manager: KVCacheManager | None = None",
      "language": "yaml"
    },
    {
      "code": "kv_cache_manager: KVCacheManager | None = None",
      "language": "yaml"
    },
    {
      "code": "kv_caches: dict[str, Tensor] = {}",
      "language": "yaml"
    },
    {
      "code": "kv_caches: dict[str, Tensor] = {}",
      "language": "yaml"
    },
    {
      "code": "kv_role = kv_role",
      "language": "unknown"
    },
    {
      "code": "kv_role = kv_role",
      "language": "unknown"
    },
    {
      "code": "layerwise_retrievers: list[\n    Generator[Tensor | None, None, None]\n] = []",
      "language": "yaml"
    },
    {
      "code": "layerwise_retrievers: list[\n    Generator[Tensor | None, None, None]\n] = []",
      "language": "yaml"
    },
    {
      "code": "lmcache_engine = None",
      "language": "rust"
    },
    {
      "code": "lmcache_engine = None",
      "language": "rust"
    },
    {
      "code": "load_specs: dict[str, LoadSpec] = {}",
      "language": "yaml"
    },
    {
      "code": "load_specs: dict[str, LoadSpec] = {}",
      "language": "yaml"
    },
    {
      "code": "lookup_client = create_lookup_client(vllm_config, config)",
      "language": "unknown"
    },
    {
      "code": "lookup_client = create_lookup_client(vllm_config, config)",
      "language": "unknown"
    },
    {
      "code": "lookup_server = create_lookup_server(\n    lmcache_engine, vllm_config\n)",
      "language": "unknown"
    },
    {
      "code": "lookup_server = create_lookup_server(\n    lmcache_engine, vllm_config\n)",
      "language": "unknown"
    },
    {
      "code": "num_layers = get_num_layers(parallel_config)",
      "language": "unknown"
    },
    {
      "code": "num_layers = get_num_layers(parallel_config)",
      "language": "unknown"
    },
    {
      "code": "offload_server = ZMQOffloadServer(\n    lmcache_engine,\n    vllm_config,\n    get_tensor_model_parallel_rank(),\n)",
      "language": "unknown"
    },
    {
      "code": "offload_server = ZMQOffloadServer(\n    lmcache_engine,\n    vllm_config,\n    get_tensor_model_parallel_rank(),\n)",
      "language": "unknown"
    },
    {
      "code": "plugin_launcher = PluginLauncher(\n    config,\n    role,\n    worker_count,\n    -1 if lmcache_engine is None else worker_id,\n)",
      "language": "rust"
    },
    {
      "code": "plugin_launcher = PluginLauncher(\n    config,\n    role,\n    worker_count,\n    -1 if lmcache_engine is None else worker_id,\n)",
      "language": "rust"
    },
    {
      "code": "skip_last_n_tokens = get_from_extra_config(\n    \"skip_last_n_tokens\", 0\n)",
      "language": "unknown"
    },
    {
      "code": "skip_last_n_tokens = get_from_extra_config(\n    \"skip_last_n_tokens\", 0\n)",
      "language": "unknown"
    },
    {
      "code": "use_layerwise = use_layerwise",
      "language": "unknown"
    },
    {
      "code": "use_layerwise = use_layerwise",
      "language": "unknown"
    },
    {
      "code": "worker_count = tensor_parallel_size",
      "language": "unknown"
    },
    {
      "code": "worker_count = tensor_parallel_size",
      "language": "unknown"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    parent: KVConnectorBase_V1,\n)",
      "language": "python"
    },
    {
      "code": "__init__(\n    vllm_config: VllmConfig,\n    role: KVConnectorRole,\n    parent: KVConnectorBase_V1,\n)",
      "language": "python"
    },
    {
      "code": "568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712",
      "language": "unknown"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: \"VllmConfig\",\n    role: KVConnectorRole,\n    parent: KVConnectorBase_V1,\n):\n    assert vllm_config.kv_transfer_config is not None\n    self._parent = parent\n    self._vllm_config = vllm_config\n    self.kv_role = vllm_config.kv_transfer_config.kv_role\n    self.worker_count = vllm_config.parallel_config.tensor_parallel_size\n    config = lmcache_get_or_create_config()\n    assert isinstance(config, LMCacheEngineConfig), (\n        \"LMCache v1 configuration is should be passed for vLLM v1.\"\n    )\n    # Put the leading with \"lmcache.\" and matched configs from\n    # vllm extra_config to the config\n    kv_connector_extra_config = (\n        vllm_config.kv_transfer_config.kv_connector_extra_config\n    )\n    if kv_connector_extra_config:\n        for key, value in kv_connector_extra_config.items():\n            if key.startswith(\"lmcache.\"):\n                config_key = key[8:]  # Remove \"lmcache.\" prefix\n                if _validate_and_set_config_value(config, config_key, value):\n                    logger.info(\n                        \"Updated config %s from vLLM extra config: %s\",\n                        config_key,\n                        value,\n                    )\n\n    self.config = config\n\n    self.async_loading = config.enable_async_loading\n    self.layerwise_retrievers: list[Generator[torch.Tensor | None, None, None]] = []\n    self._stats_monitor = LMCStatsMonitor.GetOrCreate()\n    if role == KVConnectorRole.SCHEDULER:\n        # Create lookup client using factory\n        self.lookup_client = LookupClientFactory.create_lookup_client(\n            vllm_config, config\n        )\n        self._unfinished_requests: dict[str, Request] = {}\n        self._lookup_requests_in_step: list[str] = []\n        self.lmcache_engine = None\n    else:\n        self.lmcache_engine = _init_lmcache_engine(\n            config,\n            vllm_config,\n        )\n\n        self.use_layerwise = config.use_layerwise\n        self.enable_blending = config.enable_blending\n\n        if self.enable_blending:\n            self.blender = LMCBlenderBuilder.get_or_create(\n                ENGINE_NAME,\n                self.lmcache_engine,\n                self.lmcache_engine.gpu_connector,\n                config,\n            )\n\n        # Create lookup server using factory\n        assert self.lmcache_engine is not None\n        self.lookup_server = LookupClientFactory.create_lookup_server(\n            self.lmcache_engine, vllm_config\n        )\n\n        self.offload_server = ZMQOffloadServer(\n            self.lmcache_engine,\n            vllm_config,\n            get_tensor_model_parallel_rank(),\n        )\n\n        # In case of MLA, the lookup server is only created on worker 0\n        if self.async_loading and self.lookup_server is not None:\n            assert isinstance(self.lookup_server, LMCacheAsyncLookupServer)\n            self.lmcache_engine.post_init(async_lookup_server=self.lookup_server)\n\n    self.kv_caches: dict[str, torch.Tensor] = {}\n\n    self._block_size = vllm_config.cache_config.block_size\n\n    # request_id -> (vllm cached tokens, lmcache cached tokens)\n    self.load_specs: dict[str, LoadSpec] = {}\n\n    self.kv_cache_manager: KVCacheManager | None = None\n\n    # request_id -> full_token_ids\n    self._request_trackers: dict[str, RequestTracker] = {}\n\n    # Whether to discard partial chunks\n    self._discard_partial_chunks = (\n        vllm_config.kv_transfer_config.get_from_extra_config(\n            \"discard_partial_chunks\", False\n        )\n        or not config.save_unfull_chunk\n    )\n\n    self._lmcache_chunk_size = config.chunk_size\n    self._save_decode_cache = config.save_decode_cache\n\n    self.skip_last_n_tokens = vllm_config.kv_transfer_config.get_from_extra_config(\n        \"skip_last_n_tokens\", 0\n    )\n\n    self.num_layers = vllm_config.model_config.get_num_layers(\n        vllm_config.parallel_config\n    )\n    self.current_layer = 0\n\n    self.force_skip_save = bool(os.environ.get(\"LMCACHE_FORCE_SKIP_SAVE\", False))\n\n    self._requests_priority: dict[str, int] = {}\n\n    # TODO(baoloongmao): Internal api server & plugin framework support\n    # dp > 1\n    if (\n        vllm_config.parallel_config.data_parallel_size_local == 1\n        or vllm_config.parallel_config.data_parallel_rank_local == 0\n    ):\n        # Start internal API server if enabled\n        # The enabled check is in the InternalAPIServer constructor\n        self.api_server = InternalAPIServer(self)\n        self.api_server.start()\n        # Launch plugins\n        self.plugin_launcher = RuntimePluginLauncher(\n            self.config,\n            role,\n            self.worker_count,\n            -1\n            if self.lmcache_engine is None  # scheduler side\n            else self.lmcache_engine.metadata.worker_id,\n        )\n        self.plugin_launcher.launch_plugins()\n    else:\n        self.api_server = None  # type: ignore[assignment]\n        self.plugin_launcher = None  # type: ignore[assignment]\n    logger.info(\n        \"LMCache initialized for role %s with version %s, \"\n        \"vllm version %s, lmcache cache_engine metadata: %s\",\n        role,\n        utils.get_version(),\n        VLLM_VERSION,\n        getattr(self.lmcache_engine, \"metadata\", None),\n    )",
      "language": "python"
    },
    {
      "code": "def __init__(\n    self,\n    vllm_config: \"VllmConfig\",\n    role: KVConnectorRole,\n    parent: KVConnectorBase_V1,\n):\n    assert vllm_config.kv_transfer_config is not None\n    self._parent = parent\n    self._vllm_config = vllm_config\n    self.kv_role = vllm_config.kv_transfer_config.kv_role\n    self.worker_count = vllm_config.parallel_config.tensor_parallel_size\n    config = lmcache_get_or_create_config()\n    assert isinstance(config, LMCacheEngineConfig), (\n        \"LMCache v1 configuration is should be passed for vLLM v1.\"\n    )\n    # Put the leading with \"lmcache.\" and matched configs from\n    # vllm extra_config to the config\n    kv_connector_extra_config = (\n        vllm_config.kv_transfer_config.kv_connector_extra_config\n    )\n    if kv_connector_extra_config:\n        for key, value in kv_connector_extra_config.items():\n            if key.startswith(\"lmcache.\"):\n                config_key = key[8:]  # Remove \"lmcache.\" prefix\n                if _validate_and_set_config_value(config, config_key, value):\n                    logger.info(\n                        \"Updated config %s from vLLM extra config: %s\",\n                        config_key,\n                        value,\n                    )\n\n    self.config = config\n\n    self.async_loading = config.enable_async_loading\n    self.layerwise_retrievers: list[Generator[torch.Tensor | None, None, None]] = []\n    self._stats_monitor = LMCStatsMonitor.GetOrCreate()\n    if role == KVConnectorRole.SCHEDULER:\n        # Create lookup client using factory\n        self.lookup_client = LookupClientFactory.create_lookup_client(\n            vllm_config, config\n        )\n        self._unfinished_requests: dict[str, Request] = {}\n        self._lookup_requests_in_step: list[str] = []\n        self.lmcache_engine = None\n    else:\n        self.lmcache_engine = _init_lmcache_engine(\n            config,\n            vllm_config,\n        )\n\n        self.use_layerwise = config.use_layerwise\n        self.enable_blending = config.enable_blending\n\n        if self.enable_blending:\n            self.blender = LMCBlenderBuilder.get_or_create(\n                ENGINE_NAME,\n                self.lmcache_engine,\n                self.lmcache_engine.gpu_connector,\n                config,\n            )\n\n        # Create lookup server using factory\n        assert self.lmcache_engine is not None\n        self.lookup_server = LookupClientFactory.create_lookup_server(\n            self.lmcache_engine, vllm_config\n        )\n\n        self.offload_server = ZMQOffloadServer(\n            self.lmcache_engine,\n            vllm_config,\n            get_tensor_model_parallel_rank(),\n        )\n\n        # In case of MLA, the lookup server is only created on worker 0\n        if self.async_loading and self.lookup_server is not None:\n            assert isinstance(self.lookup_server, LMCacheAsyncLookupServer)\n            self.lmcache_engine.post_init(async_lookup_server=self.lookup_server)\n\n    self.kv_caches: dict[str, torch.Tensor] = {}\n\n    self._block_size = vllm_config.cache_config.block_size\n\n    # request_id -> (vllm cached tokens, lmcache cached tokens)\n    self.load_specs: dict[str, LoadSpec] = {}\n\n    self.kv_cache_manager: KVCacheManager | None = None\n\n    # request_id -> full_token_ids\n    self._request_trackers: dict[str, RequestTracker] = {}\n\n    # Whether to discard partial chunks\n    self._discard_partial_chunks = (\n        vllm_config.kv_transfer_config.get_from_extra_config(\n            \"discard_partial_chunks\", False\n        )\n        or not config.save_unfull_chunk\n    )\n\n    self._lmcache_chunk_size = config.chunk_size\n    self._save_decode_cache = config.save_decode_cache\n\n    self.skip_last_n_tokens = vllm_config.kv_transfer_config.get_from_extra_config(\n        \"skip_last_n_tokens\", 0\n    )\n\n    self.num_layers = vllm_config.model_config.get_num_layers(\n        vllm_config.parallel_config\n    )\n    self.current_layer = 0\n\n    self.force_skip_save = bool(os.environ.get(\"LMCACHE_FORCE_SKIP_SAVE\", False))\n\n    self._requests_priority: dict[str, int] = {}\n\n    # TODO(baoloongmao): Internal api server & plugin framework support\n    # dp > 1\n    if (\n        vllm_config.parallel_config.data_parallel_size_local == 1\n        or vllm_config.parallel_config.data_parallel_rank_local == 0\n    ):\n        # Start internal API server if enabled\n        # The enabled check is in the InternalAPIServer constructor\n        self.api_server = InternalAPIServer(self)\n        self.api_server.start()\n        # Launch plugins\n        self.plugin_launcher = RuntimePluginLauncher(\n            self.config,\n            role,\n            self.worker_count,\n            -1\n            if self.lmcache_engine is None  # scheduler side\n            else self.lmcache_engine.metadata.worker_id,\n        )\n        self.plugin_launcher.launch_plugins()\n    else:\n        self.api_server = None  # type: ignore[assignment]\n        self.plugin_launcher = None  # type: ignore[assignment]\n    logger.info(\n        \"LMCache initialized for role %s with version %s, \"\n        \"vllm version %s, lmcache cache_engine metadata: %s\",\n        role,\n        utils.get_version(),\n        VLLM_VERSION,\n        getattr(self.lmcache_engine, \"metadata\", None),\n    )",
      "language": "python"
    },
    {
      "code": "_init_kv_caches_from_forward_context(\n    forward_context: ForwardContext,\n)",
      "language": "yaml"
    },
    {
      "code": "_init_kv_caches_from_forward_context(\n    forward_context: ForwardContext,\n)",
      "language": "yaml"
    },
    {
      "code": "769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef _init_kv_caches_from_forward_context(self, forward_context: \"ForwardContext\"):\n    for layer_name in forward_context.no_compile_layers:\n        attn_layer = forward_context.no_compile_layers[layer_name]\n        if not hasattr(attn_layer, \"kv_cache\"):\n            logger.debug(\"The layer %s does not have kv_cache, skip it\", layer_name)\n            continue\n\n        if layer_name not in self.kv_caches:\n            self.kv_caches[layer_name] = attn_layer.kv_cache[\n                forward_context.virtual_engine\n            ]",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef _init_kv_caches_from_forward_context(self, forward_context: \"ForwardContext\"):\n    for layer_name in forward_context.no_compile_layers:\n        attn_layer = forward_context.no_compile_layers[layer_name]\n        if not hasattr(attn_layer, \"kv_cache\"):\n            logger.debug(\"The layer %s does not have kv_cache, skip it\", layer_name)\n            continue\n\n        if layer_name not in self.kv_caches:\n            self.kv_caches[layer_name] = attn_layer.kv_cache[\n                forward_context.virtual_engine\n            ]",
      "language": "python"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "build_connector_meta(\n    scheduler_output: SchedulerOutput,\n) -> KVConnectorMetadata",
      "language": "php"
    },
    {
      "code": "1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311\n1312\n1313\n1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321\n1322\n1323\n1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377\n1378\n1379\n1380\n1381\n1382\n1383\n1384\n1385\n1386\n1387\n1388\n1389\n1390\n1391\n1392\n1393\n1394\n1395\n1396",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef build_connector_meta(\n    self, scheduler_output: SchedulerOutput\n) -> KVConnectorMetadata:\n    \"\"\"Attach the connector metadata to the request object.\n\n    This function should NOT modify other fields in the scheduler_output\n    except the `kv_connector_metadata` field.\n    Also, calling this function will reset the state of the connector.\n\n    Args:\n        scheduler_output (SchedulerOutput): the scheduler output object.\n    \"\"\"\n\n    force_skip_save = self.kv_role == \"kv_consumer\" or self.force_skip_save\n\n    meta = LMCacheConnectorMetadata()\n\n    # set and update lookup requests for unpin\n    meta.lookup_requests_in_step = self._lookup_requests_in_step\n    self._lookup_requests_in_step = []\n\n    for finished_req_id in scheduler_output.finished_req_ids:\n        self._request_trackers.pop(finished_req_id, None)\n        self._unfinished_requests.pop(finished_req_id, None)\n\n    for request in scheduler_output.scheduled_new_reqs:\n        # Right now, we only load KV for new requests\n        load_spec = self.load_specs.pop(request.req_id, None)\n        num_tokens_to_compute = (\n            request.num_computed_tokens\n            + scheduler_output.num_scheduled_tokens[request.req_id]\n        )\n        lmcache_cached_tokens = 0\n        if load_spec is not None:\n            lmcache_cached_tokens = load_spec.lmcache_cached_tokens\n        request_priority = self._requests_priority.pop(request.req_id, 0)\n\n        skip_save = force_skip_save or (\n            self.config.priority_limit is not None\n            and request_priority > self.config.priority_limit\n        )\n\n        request_tracker = RequestTracker.from_new_request(\n            self.config,\n            request,\n            num_tokens_to_compute,\n            lmcache_cached_tokens,\n            skip_save,\n        )\n        self._request_trackers[request.req_id] = request_tracker\n\n        req_meta = ReqMeta.from_request_tracker(\n            request_tracker,\n            self._block_size,\n            self._lmcache_chunk_size,\n            load_spec=load_spec,\n            discard_partial_chunks=self._discard_partial_chunks,\n            save_decode_cache=self._save_decode_cache,\n        )\n        if req_meta is not None:\n            meta.add_request(req_meta)\n\n    cached_reqs = scheduler_output.scheduled_cached_reqs\n\n    # NOTE: For backward compatibility with vllm version < 0.9.2,\n    # In the latest vllm version, the type of scheduled_cached_reqs has\n    # changed from list to object `CachedRequestData`\n    if isinstance(cached_reqs, list):\n        for i, req in enumerate(cached_reqs):\n            request_tracker = self._request_trackers[req.req_id]\n            request_tracker.update(req.new_token_ids, req.new_block_ids)\n\n            req_meta = ReqMeta.from_request_tracker(\n                request_tracker,\n                self._block_size,\n                self._lmcache_chunk_size,\n                load_spec=None,\n                discard_partial_chunks=self._discard_partial_chunks,\n            )\n            if req_meta is not None:\n                meta.add_request(req_meta)\n        return meta\n\n    for i, req_id in enumerate(cached_reqs.req_ids):\n        request_tracker = self._request_trackers[req_id]\n        num_new_tokens = scheduler_output.num_scheduled_tokens[req_id]\n        if cached_request := self._unfinished_requests.get(req_id):\n            num_current_tokens = len(request_tracker.token_ids)\n            new_token_ids = cached_request.all_token_ids[\n                num_current_tokens : num_current_tokens + num_new_tokens\n            ]\n        else:\n            raise ValueError(\n                f\"Request {req_id} is not in _unfinished_requests, \"\n                f\"but it is scheduled to be cached\"\n            )\n        new_block_ids = cached_reqs.new_block_ids[i]\n\n        request_tracker.update(new_token_ids, new_block_ids)\n\n        req_meta = ReqMeta.from_request_tracker(\n            request_tracker,\n            self._block_size,\n            self._lmcache_chunk_size,\n            load_spec=None,\n            discard_partial_chunks=self._discard_partial_chunks,\n            save_decode_cache=self._save_decode_cache,\n        )\n        if req_meta is not None:\n            meta.add_request(req_meta)\n\n    return meta",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef build_connector_meta(\n    self, scheduler_output: SchedulerOutput\n) -> KVConnectorMetadata:\n    \"\"\"Attach the connector metadata to the request object.\n\n    This function should NOT modify other fields in the scheduler_output\n    except the `kv_connector_metadata` field.\n    Also, calling this function will reset the state of the connector.\n\n    Args:\n        scheduler_output (SchedulerOutput): the scheduler output object.\n    \"\"\"\n\n    force_skip_save = self.kv_role == \"kv_consumer\" or self.force_skip_save\n\n    meta = LMCacheConnectorMetadata()\n\n    # set and update lookup requests for unpin\n    meta.lookup_requests_in_step = self._lookup_requests_in_step\n    self._lookup_requests_in_step = []\n\n    for finished_req_id in scheduler_output.finished_req_ids:\n        self._request_trackers.pop(finished_req_id, None)\n        self._unfinished_requests.pop(finished_req_id, None)\n\n    for request in scheduler_output.scheduled_new_reqs:\n        # Right now, we only load KV for new requests\n        load_spec = self.load_specs.pop(request.req_id, None)\n        num_tokens_to_compute = (\n            request.num_computed_tokens\n            + scheduler_output.num_scheduled_tokens[request.req_id]\n        )\n        lmcache_cached_tokens = 0\n        if load_spec is not None:\n            lmcache_cached_tokens = load_spec.lmcache_cached_tokens\n        request_priority = self._requests_priority.pop(request.req_id, 0)\n\n        skip_save = force_skip_save or (\n            self.config.priority_limit is not None\n            and request_priority > self.config.priority_limit\n        )\n\n        request_tracker = RequestTracker.from_new_request(\n            self.config,\n            request,\n            num_tokens_to_compute,\n            lmcache_cached_tokens,\n            skip_save,\n        )\n        self._request_trackers[request.req_id] = request_tracker\n\n        req_meta = ReqMeta.from_request_tracker(\n            request_tracker,\n            self._block_size,\n            self._lmcache_chunk_size,\n            load_spec=load_spec,\n            discard_partial_chunks=self._discard_partial_chunks,\n            save_decode_cache=self._save_decode_cache,\n        )\n        if req_meta is not None:\n            meta.add_request(req_meta)\n\n    cached_reqs = scheduler_output.scheduled_cached_reqs\n\n    # NOTE: For backward compatibility with vllm version < 0.9.2,\n    # In the latest vllm version, the type of scheduled_cached_reqs has\n    # changed from list to object `CachedRequestData`\n    if isinstance(cached_reqs, list):\n        for i, req in enumerate(cached_reqs):\n            request_tracker = self._request_trackers[req.req_id]\n            request_tracker.update(req.new_token_ids, req.new_block_ids)\n\n            req_meta = ReqMeta.from_request_tracker(\n                request_tracker,\n                self._block_size,\n                self._lmcache_chunk_size,\n                load_spec=None,\n                discard_partial_chunks=self._discard_partial_chunks,\n            )\n            if req_meta is not None:\n                meta.add_request(req_meta)\n        return meta\n\n    for i, req_id in enumerate(cached_reqs.req_ids):\n        request_tracker = self._request_trackers[req_id]\n        num_new_tokens = scheduler_output.num_scheduled_tokens[req_id]\n        if cached_request := self._unfinished_requests.get(req_id):\n            num_current_tokens = len(request_tracker.token_ids)\n            new_token_ids = cached_request.all_token_ids[\n                num_current_tokens : num_current_tokens + num_new_tokens\n            ]\n        else:\n            raise ValueError(\n                f\"Request {req_id} is not in _unfinished_requests, \"\n                f\"but it is scheduled to be cached\"\n            )\n        new_block_ids = cached_reqs.new_block_ids[i]\n\n        request_tracker.update(new_token_ids, new_block_ids)\n\n        req_meta = ReqMeta.from_request_tracker(\n            request_tracker,\n            self._block_size,\n            self._lmcache_chunk_size,\n            load_spec=None,\n            discard_partial_chunks=self._discard_partial_chunks,\n            save_decode_cache=self._save_decode_cache,\n        )\n        if req_meta is not None:\n            meta.add_request(req_meta)\n\n    return meta",
      "language": "python"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str],\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "get_finished(\n    finished_req_ids: set[str],\n) -> tuple[set[str] | None, set[str] | None]",
      "language": "rust"
    },
    {
      "code": "1119\n1120\n1121\n1122\n1123",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef get_finished(\n    self, finished_req_ids: set[str]\n) -> tuple[set[str] | None, set[str] | None]:\n    return None, None",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef get_finished(\n    self, finished_req_ids: set[str]\n) -> tuple[set[str] | None, set[str] | None]:\n    return None, None",
      "language": "python"
    },
    {
      "code": "get_inference_info() -> dict",
      "language": "php"
    },
    {
      "code": "get_inference_info() -> dict",
      "language": "php"
    },
    {
      "code": "714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759",
      "language": "unknown"
    },
    {
      "code": "def get_inference_info(self) -> dict:\n    \"\"\"Get inference information including vLLM config and related details.\n\n    Returns:\n        dict: Dictionary containing inference information\n    \"\"\"\n    # Get vLLM config information\n    vllm_config = self._vllm_config\n\n    # Use vLLM config's string representation and add specific configs\n    inference_info = {\n        \"vllm_version\": VLLM_VERSION,\n        \"lmcache_version\": utils.get_version(),\n        \"vllm_config\": str(vllm_config),\n        \"model_config\": {\n            \"model\": getattr(vllm_config.model_config, \"model\", None),\n            \"dtype\": str(getattr(vllm_config.model_config, \"dtype\", None)),\n            \"max_model_len\": getattr(\n                vllm_config.model_config, \"max_model_len\", None\n            ),\n            \"vocab_size\": vllm_config.model_config.get_vocab_size(),\n            \"num_layers\": getattr(\n                vllm_config.model_config, \"get_num_layers\", lambda _: None\n            )(vllm_config.parallel_config),\n            \"num_attention_heads\": getattr(\n                vllm_config.model_config, \"get_num_attention_heads\", lambda _: None\n            )(vllm_config.parallel_config),\n            \"num_kv_heads\": getattr(\n                vllm_config.model_config, \"get_num_kv_heads\", lambda _: None\n            )(vllm_config.parallel_config),\n            \"head_size\": getattr(\n                vllm_config.model_config, \"get_head_size\", lambda: None\n            )(),\n        },\n        \"cache_config\": {\n            \"block_size\": getattr(vllm_config.cache_config, \"block_size\", None),\n            \"cache_dtype\": str(\n                getattr(vllm_config.cache_config, \"cache_dtype\", None)\n            ),\n            \"gpu_memory_utilization\": getattr(\n                vllm_config.cache_config, \"gpu_memory_utilization\", None\n            ),\n        },\n    }\n\n    return inference_info",
      "language": "python"
    },
    {
      "code": "def get_inference_info(self) -> dict:\n    \"\"\"Get inference information including vLLM config and related details.\n\n    Returns:\n        dict: Dictionary containing inference information\n    \"\"\"\n    # Get vLLM config information\n    vllm_config = self._vllm_config\n\n    # Use vLLM config's string representation and add specific configs\n    inference_info = {\n        \"vllm_version\": VLLM_VERSION,\n        \"lmcache_version\": utils.get_version(),\n        \"vllm_config\": str(vllm_config),\n        \"model_config\": {\n            \"model\": getattr(vllm_config.model_config, \"model\", None),\n            \"dtype\": str(getattr(vllm_config.model_config, \"dtype\", None)),\n            \"max_model_len\": getattr(\n                vllm_config.model_config, \"max_model_len\", None\n            ),\n            \"vocab_size\": vllm_config.model_config.get_vocab_size(),\n            \"num_layers\": getattr(\n                vllm_config.model_config, \"get_num_layers\", lambda _: None\n            )(vllm_config.parallel_config),\n            \"num_attention_heads\": getattr(\n                vllm_config.model_config, \"get_num_attention_heads\", lambda _: None\n            )(vllm_config.parallel_config),\n            \"num_kv_heads\": getattr(\n                vllm_config.model_config, \"get_num_kv_heads\", lambda _: None\n            )(vllm_config.parallel_config),\n            \"head_size\": getattr(\n                vllm_config.model_config, \"get_head_size\", lambda: None\n            )(),\n        },\n        \"cache_config\": {\n            \"block_size\": getattr(vllm_config.cache_config, \"block_size\", None),\n            \"cache_dtype\": str(\n                getattr(vllm_config.cache_config, \"cache_dtype\", None)\n            ),\n            \"gpu_memory_utilization\": getattr(\n                vllm_config.cache_config, \"gpu_memory_utilization\", None\n            ),\n        },\n    }\n\n    return inference_info",
      "language": "python"
    },
    {
      "code": "get_inference_version() -> str",
      "language": "php"
    },
    {
      "code": "get_inference_version() -> str",
      "language": "php"
    },
    {
      "code": "761\n762\n763\n764\n765\n766\n767",
      "language": "unknown"
    },
    {
      "code": "def get_inference_version(self) -> str:\n    \"\"\"Get vLLM version information.\n\n    Returns:\n        str: vLLM version string\n    \"\"\"\n    return VLLM_VERSION",
      "language": "python"
    },
    {
      "code": "def get_inference_version(self) -> str:\n    \"\"\"Get vLLM version information.\n\n    Returns:\n        str: vLLM version string\n    \"\"\"\n    return VLLM_VERSION",
      "language": "python"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> int | None",
      "language": "rust"
    },
    {
      "code": "get_num_new_matched_tokens(\n    request: Request, num_computed_tokens: int\n) -> int | None",
      "language": "rust"
    },
    {
      "code": "1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef get_num_new_matched_tokens(\n    self,\n    request: \"Request\",\n    num_computed_tokens: int,\n) -> int | None:\n    \"\"\"\n    Check for external KV cache hit.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n\n    Returns:\n        the number of tokens that can be loaded from the\n        external KV cache beyond what is already computed.\n    \"\"\"\n    if self.kv_role == \"kv_producer\" and not hasattr(\n        self.lookup_client, \"supports_producer_reuse\"\n    ):\n        return 0\n\n    self._requests_priority[request.request_id] = request.priority\n\n    token_ids = request.prompt_token_ids\n\n    # If the request has multimodal hashes, apply them to the token ids\n    mm_hashes, mm_positions = extract_mm_features(request)\n    if mm_hashes and mm_positions:\n        # TODO(Jiayi): Optimize this\n        token_ids_tensor = torch.tensor(request.prompt_token_ids)\n        apply_mm_hashes_to_token_ids(token_ids_tensor, mm_hashes, mm_positions)\n        token_ids = token_ids_tensor.tolist()\n\n    if request.sampling_params:\n        request_configs = extract_request_configs(request.sampling_params)\n    else:\n        request_configs = None\n\n    if self.skip_last_n_tokens > 0:\n        assert token_ids is not None\n        token_ids = token_ids[: -self.skip_last_n_tokens]\n    lookup_id = request.request_id if self.async_loading else str(uuid.uuid4())\n\n    self._lookup_requests_in_step.append(lookup_id)\n\n    num_external_hit_tokens = self.lookup_client.lookup(\n        token_ids,\n        lookup_id=lookup_id,\n        request_configs=request_configs,\n    )\n\n    if num_external_hit_tokens is None:\n        logger.info(\n            \"Reqid: %s, Total tokens %d, LMCache hit tokens: None.\",\n            request.request_id,\n            request.num_tokens,\n        )\n        return None\n\n    # When prompt length is divisible by the block size and all\n    # blocks are cached, we need to recompute the last token.\n    # This will be removed in the future if vLLM's scheduler provides\n    # a better support for this case.\n    need_to_allocate = num_external_hit_tokens - num_computed_tokens\n\n    # In, full-prompt-hit case, we need to recompute the last token\n    if num_external_hit_tokens == request.num_tokens:\n        need_to_allocate -= 1\n\n    logger.info(\n        \"Reqid: %s, Total tokens %d, LMCache hit tokens: %d, need to load: %d\",\n        request.request_id,\n        request.num_tokens,\n        num_external_hit_tokens,\n        need_to_allocate,\n    )\n\n    self.load_specs[request.request_id] = LoadSpec(\n        vllm_cached_tokens=num_computed_tokens,\n        lmcache_cached_tokens=num_external_hit_tokens,\n        can_load=False,\n    )\n\n    if need_to_allocate <= 0:\n        return 0\n\n    return need_to_allocate",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef get_num_new_matched_tokens(\n    self,\n    request: \"Request\",\n    num_computed_tokens: int,\n) -> int | None:\n    \"\"\"\n    Check for external KV cache hit.\n\n    Args:\n        request (Request): the request object.\n        num_computed_tokens (int): the number of locally\n            computed tokens for this request\n\n    Returns:\n        the number of tokens that can be loaded from the\n        external KV cache beyond what is already computed.\n    \"\"\"\n    if self.kv_role == \"kv_producer\" and not hasattr(\n        self.lookup_client, \"supports_producer_reuse\"\n    ):\n        return 0\n\n    self._requests_priority[request.request_id] = request.priority\n\n    token_ids = request.prompt_token_ids\n\n    # If the request has multimodal hashes, apply them to the token ids\n    mm_hashes, mm_positions = extract_mm_features(request)\n    if mm_hashes and mm_positions:\n        # TODO(Jiayi): Optimize this\n        token_ids_tensor = torch.tensor(request.prompt_token_ids)\n        apply_mm_hashes_to_token_ids(token_ids_tensor, mm_hashes, mm_positions)\n        token_ids = token_ids_tensor.tolist()\n\n    if request.sampling_params:\n        request_configs = extract_request_configs(request.sampling_params)\n    else:\n        request_configs = None\n\n    if self.skip_last_n_tokens > 0:\n        assert token_ids is not None\n        token_ids = token_ids[: -self.skip_last_n_tokens]\n    lookup_id = request.request_id if self.async_loading else str(uuid.uuid4())\n\n    self._lookup_requests_in_step.append(lookup_id)\n\n    num_external_hit_tokens = self.lookup_client.lookup(\n        token_ids,\n        lookup_id=lookup_id,\n        request_configs=request_configs,\n    )\n\n    if num_external_hit_tokens is None:\n        logger.info(\n            \"Reqid: %s, Total tokens %d, LMCache hit tokens: None.\",\n            request.request_id,\n            request.num_tokens,\n        )\n        return None\n\n    # When prompt length is divisible by the block size and all\n    # blocks are cached, we need to recompute the last token.\n    # This will be removed in the future if vLLM's scheduler provides\n    # a better support for this case.\n    need_to_allocate = num_external_hit_tokens - num_computed_tokens\n\n    # In, full-prompt-hit case, we need to recompute the last token\n    if num_external_hit_tokens == request.num_tokens:\n        need_to_allocate -= 1\n\n    logger.info(\n        \"Reqid: %s, Total tokens %d, LMCache hit tokens: %d, need to load: %d\",\n        request.request_id,\n        request.num_tokens,\n        num_external_hit_tokens,\n        need_to_allocate,\n    )\n\n    self.load_specs[request.request_id] = LoadSpec(\n        vllm_cached_tokens=num_computed_tokens,\n        lmcache_cached_tokens=num_external_hit_tokens,\n        can_load=False,\n    )\n\n    if need_to_allocate <= 0:\n        return 0\n\n    return need_to_allocate",
      "language": "python"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "request_finished(\n    request: Request, block_ids: list[int]\n) -> tuple[bool, dict[str, Any] | None]",
      "language": "rust"
    },
    {
      "code": "1398\n1399\n1400\n1401\n1402\n1403\n1404\n1405\n1406\n1407\n1408\n1409\n1410\n1411\n1412\n1413\n1414\n1415\n1416\n1417\n1418",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    params = (\n        request.kv_transfer_params\n        if hasattr(request, \"kv_transfer_params\")\n        else None\n    )\n    return_params = None\n\n    # NOTE: Used to stream back the first token\n    # for disagg prefill\n    if params is not None and \"ret_first_tok\" in params:\n        return_params = {\n            \"first_tok\": request._output_token_ids[0],\n        }\n\n    return False, return_params",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef request_finished(\n    self,\n    request: \"Request\",\n    block_ids: list[int],\n) -> tuple[bool, dict[str, Any] | None]:\n    params = (\n        request.kv_transfer_params\n        if hasattr(request, \"kv_transfer_params\")\n        else None\n    )\n    return_params = None\n\n    # NOTE: Used to stream back the first token\n    # for disagg prefill\n    if params is not None and \"ret_first_tok\" in params:\n        return_params = {\n            \"first_tok\": request._output_token_ids[0],\n        }\n\n    return False, return_params",
      "language": "python"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs,\n) -> None",
      "language": "rust"
    },
    {
      "code": "save_kv_layer(\n    layer_name: str,\n    kv_layer: Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs,\n) -> None",
      "language": "rust"
    },
    {
      "code": "920\n 921\n 922\n 923\n 924\n 925\n 926\n 927\n 928\n 929\n 930\n 931\n 932\n 933\n 934\n 935\n 936\n 937\n 938\n 939\n 940\n 941\n 942\n 943\n 944\n 945\n 946\n 947\n 948\n 949\n 950\n 951\n 952\n 953\n 954\n 955\n 956\n 957\n 958\n 959\n 960\n 961\n 962\n 963\n 964\n 965\n 966\n 967\n 968\n 969\n 970\n 971\n 972\n 973\n 974\n 975\n 976\n 977\n 978\n 979\n 980\n 981\n 982\n 983\n 984\n 985\n 986\n 987\n 988\n 989\n 990\n 991\n 992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs,\n) -> None:\n    \"\"\"Start saving the a layer of KV cache from vLLM's paged buffer\n    to the connector.\n\n    Args:\n        layer_name (str): the name of the layer.\n        kv_layer (torch.Tensor): the paged KV buffer of the current\n            layer in vLLM.\n        attn_metadata (AttentionMetadata): the attention metadata.\n    \"\"\"\n    assert self.lmcache_engine is not None\n\n    if not self.use_layerwise:\n        return\n\n    if self.kv_role == \"kv_consumer\":\n        # Don't do save if the role is kv_consumer\n        return\n    if self._parent._connector_metadata is None:\n        logger.warning(\n            \"In connector.save_kv_layer, but the connector metadata is None\"\n        )\n        return\n    connector_metadata = self._parent._get_connector_metadata()\n    assert isinstance(connector_metadata, LMCacheConnectorMetadata)\n\n    assert len(self.kv_caches) > 0\n\n    kvcaches = list(self.kv_caches.values())\n    if self.current_layer == 0:\n        self.layerwise_storers = []\n\n        is_first = True\n\n        for idx, request in enumerate(connector_metadata.requests):\n            save_spec = request.save_spec\n            if save_spec is None or not save_spec.can_save:\n                continue\n\n            token_ids = request.token_ids\n            assert isinstance(token_ids, list)\n\n            slot_mapping = request.slot_mapping\n            assert isinstance(slot_mapping, torch.Tensor)\n            assert len(slot_mapping) == len(token_ids)\n\n            # TODO: have a pre-allocated buffer to hold the slot_mappings\n            slot_mapping = slot_mapping.cuda()\n\n            if self.kv_role == \"kv_producer\":\n                skip_leading_tokens = 0\n            else:\n                skip_leading_tokens = save_spec.skip_leading_tokens\n\n                if skip_leading_tokens == len(token_ids):\n                    continue  # skip this request\n                # Align to lmcache chunk size\n                skip_leading_tokens = (\n                    skip_leading_tokens\n                    // self._lmcache_chunk_size\n                    * self._lmcache_chunk_size\n                )\n\n            store_mask = torch.ones(len(token_ids), dtype=torch.bool)\n            store_mask[:skip_leading_tokens] = False\n\n            logger.info(\n                \"Storing KV cache for %d out of %d tokens \"\n                \"(skip_leading_tokens=%d) for request %s\",\n                len(token_ids) - skip_leading_tokens,\n                len(token_ids),\n                skip_leading_tokens,\n                request.req_id,\n            )\n\n            # TODO (Jiayi): need to make layerwise storing\n            # compatible with disagg spec\n            layerwise_storer = self.lmcache_engine.store_layer(\n                token_ids,\n                mask=store_mask,\n                kvcaches=kvcaches,\n                slot_mapping=slot_mapping,\n                offset=skip_leading_tokens,\n                sync=is_first,\n            )\n            self.layerwise_storers.append(layerwise_storer)\n            if is_first:\n                is_first = False\n\n    for layerwise_storer in self.layerwise_storers:\n        next(layerwise_storer)\n\n    self.current_layer += 1",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef save_kv_layer(\n    self,\n    layer_name: str,\n    kv_layer: torch.Tensor,\n    attn_metadata: AttentionMetadata,\n    **kwargs,\n) -> None:\n    \"\"\"Start saving the a layer of KV cache from vLLM's paged buffer\n    to the connector.\n\n    Args:\n        layer_name (str): the name of the layer.\n        kv_layer (torch.Tensor): the paged KV buffer of the current\n            layer in vLLM.\n        attn_metadata (AttentionMetadata): the attention metadata.\n    \"\"\"\n    assert self.lmcache_engine is not None\n\n    if not self.use_layerwise:\n        return\n\n    if self.kv_role == \"kv_consumer\":\n        # Don't do save if the role is kv_consumer\n        return\n    if self._parent._connector_metadata is None:\n        logger.warning(\n            \"In connector.save_kv_layer, but the connector metadata is None\"\n        )\n        return\n    connector_metadata = self._parent._get_connector_metadata()\n    assert isinstance(connector_metadata, LMCacheConnectorMetadata)\n\n    assert len(self.kv_caches) > 0\n\n    kvcaches = list(self.kv_caches.values())\n    if self.current_layer == 0:\n        self.layerwise_storers = []\n\n        is_first = True\n\n        for idx, request in enumerate(connector_metadata.requests):\n            save_spec = request.save_spec\n            if save_spec is None or not save_spec.can_save:\n                continue\n\n            token_ids = request.token_ids\n            assert isinstance(token_ids, list)\n\n            slot_mapping = request.slot_mapping\n            assert isinstance(slot_mapping, torch.Tensor)\n            assert len(slot_mapping) == len(token_ids)\n\n            # TODO: have a pre-allocated buffer to hold the slot_mappings\n            slot_mapping = slot_mapping.cuda()\n\n            if self.kv_role == \"kv_producer\":\n                skip_leading_tokens = 0\n            else:\n                skip_leading_tokens = save_spec.skip_leading_tokens\n\n                if skip_leading_tokens == len(token_ids):\n                    continue  # skip this request\n                # Align to lmcache chunk size\n                skip_leading_tokens = (\n                    skip_leading_tokens\n                    // self._lmcache_chunk_size\n                    * self._lmcache_chunk_size\n                )\n\n            store_mask = torch.ones(len(token_ids), dtype=torch.bool)\n            store_mask[:skip_leading_tokens] = False\n\n            logger.info(\n                \"Storing KV cache for %d out of %d tokens \"\n                \"(skip_leading_tokens=%d) for request %s\",\n                len(token_ids) - skip_leading_tokens,\n                len(token_ids),\n                skip_leading_tokens,\n                request.req_id,\n            )\n\n            # TODO (Jiayi): need to make layerwise storing\n            # compatible with disagg spec\n            layerwise_storer = self.lmcache_engine.store_layer(\n                token_ids,\n                mask=store_mask,\n                kvcaches=kvcaches,\n                slot_mapping=slot_mapping,\n                offset=skip_leading_tokens,\n                sync=is_first,\n            )\n            self.layerwise_storers.append(layerwise_storer)\n            if is_first:\n                is_first = False\n\n    for layerwise_storer in self.layerwise_storers:\n        next(layerwise_storer)\n\n    self.current_layer += 1",
      "language": "python"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs\n) -> None",
      "language": "rust"
    },
    {
      "code": "start_load_kv(\n    forward_context: ForwardContext, **kwargs\n) -> None",
      "language": "rust"
    },
    {
      "code": "786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef start_load_kv(self, forward_context: \"ForwardContext\", **kwargs) -> None:\n    \"\"\"Start loading the KV cache from the connector buffer to vLLM's\n    paged KV buffer.\n\n    Args:\n        forward_context (ForwardContext): the forward context.\n\n    Note:\n        The number of elements in kv_caches and layer_names should be\n        the same.\n    \"\"\"\n    self.current_layer = 0\n\n    if len(self.kv_caches) == 0:\n        self._init_kv_caches_from_forward_context(forward_context)\n\n    metadata = self._parent._get_connector_metadata()\n    assert isinstance(metadata, LMCacheConnectorMetadata)\n\n    assert len(self.kv_caches) > 0\n    kvcaches = list(self.kv_caches.values())\n\n    attn_metadata = forward_context.attn_metadata\n    if attn_metadata is None:\n        logger.debug(\"In connector.start_load_kv, but the attn_metadata is None\")\n        return\n\n    assert self.lmcache_engine is not None\n\n    self.lmcache_engine.post_init(kvcaches=kvcaches)\n\n    self.layerwise_retrievers = []\n\n    for idx, request in enumerate(metadata.requests):\n        if request.load_spec is None:\n            continue\n        last_idx = idx\n\n    for idx, request in enumerate(metadata.requests):\n        if request.load_spec is None:\n            continue\n\n        tokens = request.token_ids\n        # TODO: have a pre-allocated buffer to hold the slot_mappings\n        slot_mapping = request.slot_mapping.cuda()\n        assert len(tokens) == len(slot_mapping)\n\n        self._stats_monitor.update_interval_vllm_hit_tokens(\n            request.load_spec.vllm_cached_tokens\n        )\n        token_mask = torch.ones(len(tokens), dtype=torch.bool)\n        masked_token_count = (\n            request.load_spec.vllm_cached_tokens\n            // self._lmcache_chunk_size\n            * self._lmcache_chunk_size\n        )\n        token_mask[:masked_token_count] = False\n\n        lmcache_cached_tokens = request.load_spec.lmcache_cached_tokens\n        if self.use_layerwise:\n            sync = idx == last_idx\n            # NOTE(Jiayi): Perform blending before layerwise prefix caching\n            if self.enable_blending:\n                # TODO(Jiayi): Need to make prefix caching and blending\n                # compatible\n                self.blender.blend(\n                    tokens[:lmcache_cached_tokens],\n                    token_mask[:lmcache_cached_tokens],\n                    kvcaches=kvcaches,\n                    slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                )\n            else:\n                layerwise_retriever = self.lmcache_engine.retrieve_layer(\n                    tokens[:lmcache_cached_tokens],\n                    token_mask[:lmcache_cached_tokens],\n                    kvcaches=kvcaches,\n                    slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                    sync=sync,\n                )\n                # NOTE: retrieve for two layers at the first layer\n                next(layerwise_retriever)\n                next(layerwise_retriever)\n                self.layerwise_retrievers.append(layerwise_retriever)\n        else:\n            ret_token_mask = self.lmcache_engine.retrieve(\n                tokens[:lmcache_cached_tokens],\n                token_mask[:lmcache_cached_tokens],\n                kvcaches=kvcaches,\n                slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                request_configs=request.request_configs,\n                req_id=request.req_id,\n            )\n\n            # Check the result\n            num_retrieved_tokens = ret_token_mask.sum().item()\n            num_expected_tokens = (\n                lmcache_cached_tokens - request.load_spec.vllm_cached_tokens\n            )\n            if num_retrieved_tokens < num_expected_tokens:\n                logger.error(\n                    \"The number of retrieved tokens is less than the \"\n                    \"expected number of tokens! This should not happen!\"\n                )\n                logger.error(\n                    \"Num retrieved tokens: %d, num expected tokens: %d\",\n                    num_retrieved_tokens,\n                    num_expected_tokens,\n                )",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef start_load_kv(self, forward_context: \"ForwardContext\", **kwargs) -> None:\n    \"\"\"Start loading the KV cache from the connector buffer to vLLM's\n    paged KV buffer.\n\n    Args:\n        forward_context (ForwardContext): the forward context.\n\n    Note:\n        The number of elements in kv_caches and layer_names should be\n        the same.\n    \"\"\"\n    self.current_layer = 0\n\n    if len(self.kv_caches) == 0:\n        self._init_kv_caches_from_forward_context(forward_context)\n\n    metadata = self._parent._get_connector_metadata()\n    assert isinstance(metadata, LMCacheConnectorMetadata)\n\n    assert len(self.kv_caches) > 0\n    kvcaches = list(self.kv_caches.values())\n\n    attn_metadata = forward_context.attn_metadata\n    if attn_metadata is None:\n        logger.debug(\"In connector.start_load_kv, but the attn_metadata is None\")\n        return\n\n    assert self.lmcache_engine is not None\n\n    self.lmcache_engine.post_init(kvcaches=kvcaches)\n\n    self.layerwise_retrievers = []\n\n    for idx, request in enumerate(metadata.requests):\n        if request.load_spec is None:\n            continue\n        last_idx = idx\n\n    for idx, request in enumerate(metadata.requests):\n        if request.load_spec is None:\n            continue\n\n        tokens = request.token_ids\n        # TODO: have a pre-allocated buffer to hold the slot_mappings\n        slot_mapping = request.slot_mapping.cuda()\n        assert len(tokens) == len(slot_mapping)\n\n        self._stats_monitor.update_interval_vllm_hit_tokens(\n            request.load_spec.vllm_cached_tokens\n        )\n        token_mask = torch.ones(len(tokens), dtype=torch.bool)\n        masked_token_count = (\n            request.load_spec.vllm_cached_tokens\n            // self._lmcache_chunk_size\n            * self._lmcache_chunk_size\n        )\n        token_mask[:masked_token_count] = False\n\n        lmcache_cached_tokens = request.load_spec.lmcache_cached_tokens\n        if self.use_layerwise:\n            sync = idx == last_idx\n            # NOTE(Jiayi): Perform blending before layerwise prefix caching\n            if self.enable_blending:\n                # TODO(Jiayi): Need to make prefix caching and blending\n                # compatible\n                self.blender.blend(\n                    tokens[:lmcache_cached_tokens],\n                    token_mask[:lmcache_cached_tokens],\n                    kvcaches=kvcaches,\n                    slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                )\n            else:\n                layerwise_retriever = self.lmcache_engine.retrieve_layer(\n                    tokens[:lmcache_cached_tokens],\n                    token_mask[:lmcache_cached_tokens],\n                    kvcaches=kvcaches,\n                    slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                    sync=sync,\n                )\n                # NOTE: retrieve for two layers at the first layer\n                next(layerwise_retriever)\n                next(layerwise_retriever)\n                self.layerwise_retrievers.append(layerwise_retriever)\n        else:\n            ret_token_mask = self.lmcache_engine.retrieve(\n                tokens[:lmcache_cached_tokens],\n                token_mask[:lmcache_cached_tokens],\n                kvcaches=kvcaches,\n                slot_mapping=slot_mapping[:lmcache_cached_tokens],\n                request_configs=request.request_configs,\n                req_id=request.req_id,\n            )\n\n            # Check the result\n            num_retrieved_tokens = ret_token_mask.sum().item()\n            num_expected_tokens = (\n                lmcache_cached_tokens - request.load_spec.vllm_cached_tokens\n            )\n            if num_retrieved_tokens < num_expected_tokens:\n                logger.error(\n                    \"The number of retrieved tokens is less than the \"\n                    \"expected number of tokens! This should not happen!\"\n                )\n                logger.error(\n                    \"Num retrieved tokens: %d, num expected tokens: %d\",\n                    num_retrieved_tokens,\n                    num_expected_tokens,\n                )",
      "language": "python"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request, num_external_tokens: int\n)",
      "language": "yaml"
    },
    {
      "code": "update_state_after_alloc(\n    request: Request, num_external_tokens: int\n)",
      "language": "yaml"
    },
    {
      "code": "1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230\n1231\n1232\n1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243\n1244\n1245\n1246\n1247\n1248\n1249\n1250\n1251\n1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267\n1268\n1269\n1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef update_state_after_alloc(self, request: \"Request\", num_external_tokens: int):\n    \"\"\"\n    Update KVConnector state after temporary buffer alloc.\n\n    For SharedStorageConnector, update _request_needs_load\n    if the CacheManager this allocated blocks for us.\n    \"\"\"\n\n    # Clear local status in lookup client when a new request is\n    # successfully scheduled.\n    self.lookup_client.clear_lookup_status(request.request_id)\n\n    kv_transfer_params = (\n        request.kv_transfer_params\n        if hasattr(request, \"kv_transfer_params\")\n        else None\n    )\n\n    if kv_transfer_params is not None and \"disagg_spec\" in kv_transfer_params:\n        req_disagg_spec = kv_transfer_params[\"disagg_spec\"]\n\n        receiver_id = req_disagg_spec[\"receiver_host\"] + str(\n            req_disagg_spec[\"receiver_init_port\"]\n        )\n\n        disagg_spec = DisaggSpec(\n            req_id=req_disagg_spec[\"req_id\"],\n            receiver_id=receiver_id,\n            receiver_host=req_disagg_spec[\"receiver_host\"],\n            receiver_init_port=req_disagg_spec[\"receiver_init_port\"],\n            receiver_alloc_port=req_disagg_spec[\"receiver_alloc_port\"],\n        )\n\n        tmp_disagg_tracker[request.request_id] = disagg_spec\n    self._unfinished_requests[request.request_id] = request\n\n    if request.request_id not in self.load_specs:\n        # No KV tokens from external KV cache, return\n        return\n\n    if num_external_tokens == 0:\n        # No need to load anything\n        self.load_specs[request.request_id].can_load = False\n        return\n\n    # Only check for non-prompt-hit case\n    if (\n        self.load_specs[request.request_id].lmcache_cached_tokens\n        != request.num_tokens\n    ):\n        assert (\n            num_external_tokens > 0\n            and num_external_tokens\n            == self.load_specs[request.request_id].lmcache_cached_tokens\n            - self.load_specs[request.request_id].vllm_cached_tokens\n        ), (\n            f\"Mismatch in number of tokens: {num_external_tokens} vs \"\n            f\"{self.load_specs[request.request_id].lmcache_cached_tokens} -\"\n            f\" {self.load_specs[request.request_id].vllm_cached_tokens}\"\n            f\" for request {request.request_id}\"\n        )\n\n    self.load_specs[request.request_id].can_load = True",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef update_state_after_alloc(self, request: \"Request\", num_external_tokens: int):\n    \"\"\"\n    Update KVConnector state after temporary buffer alloc.\n\n    For SharedStorageConnector, update _request_needs_load\n    if the CacheManager this allocated blocks for us.\n    \"\"\"\n\n    # Clear local status in lookup client when a new request is\n    # successfully scheduled.\n    self.lookup_client.clear_lookup_status(request.request_id)\n\n    kv_transfer_params = (\n        request.kv_transfer_params\n        if hasattr(request, \"kv_transfer_params\")\n        else None\n    )\n\n    if kv_transfer_params is not None and \"disagg_spec\" in kv_transfer_params:\n        req_disagg_spec = kv_transfer_params[\"disagg_spec\"]\n\n        receiver_id = req_disagg_spec[\"receiver_host\"] + str(\n            req_disagg_spec[\"receiver_init_port\"]\n        )\n\n        disagg_spec = DisaggSpec(\n            req_id=req_disagg_spec[\"req_id\"],\n            receiver_id=receiver_id,\n            receiver_host=req_disagg_spec[\"receiver_host\"],\n            receiver_init_port=req_disagg_spec[\"receiver_init_port\"],\n            receiver_alloc_port=req_disagg_spec[\"receiver_alloc_port\"],\n        )\n\n        tmp_disagg_tracker[request.request_id] = disagg_spec\n    self._unfinished_requests[request.request_id] = request\n\n    if request.request_id not in self.load_specs:\n        # No KV tokens from external KV cache, return\n        return\n\n    if num_external_tokens == 0:\n        # No need to load anything\n        self.load_specs[request.request_id].can_load = False\n        return\n\n    # Only check for non-prompt-hit case\n    if (\n        self.load_specs[request.request_id].lmcache_cached_tokens\n        != request.num_tokens\n    ):\n        assert (\n            num_external_tokens > 0\n            and num_external_tokens\n            == self.load_specs[request.request_id].lmcache_cached_tokens\n            - self.load_specs[request.request_id].vllm_cached_tokens\n        ), (\n            f\"Mismatch in number of tokens: {num_external_tokens} vs \"\n            f\"{self.load_specs[request.request_id].lmcache_cached_tokens} -\"\n            f\" {self.load_specs[request.request_id].vllm_cached_tokens}\"\n            f\" for request {request.request_id}\"\n        )\n\n    self.load_specs[request.request_id].can_load = True",
      "language": "python"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "wait_for_layer_load(layer_name: str) -> None",
      "language": "rust"
    },
    {
      "code": "896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n    paged buffer.\n\n    This interface will be useful for layer-by-layer pipelining.\n\n    Args:\n        layer_name: the name of that layer\n    \"\"\"\n    if self.layerwise_retrievers:\n        logger.debug(\"Waiting for layer %s to be loaded\", self.current_layer)\n\n    # Wait for the layer to be loaded\n    for layerwise_retriever in self.layerwise_retrievers:\n        ret_token_mask = next(layerwise_retriever)\n\n        if self.current_layer == self.num_layers - 1:\n            assert ret_token_mask is not None\n            num_retrieved_tokens = ret_token_mask.sum().item()\n            logger.info(\"Retrieved %s tokens\", num_retrieved_tokens)\n\n    return",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef wait_for_layer_load(self, layer_name: str) -> None:\n    \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n    paged buffer.\n\n    This interface will be useful for layer-by-layer pipelining.\n\n    Args:\n        layer_name: the name of that layer\n    \"\"\"\n    if self.layerwise_retrievers:\n        logger.debug(\"Waiting for layer %s to be loaded\", self.current_layer)\n\n    # Wait for the layer to be loaded\n    for layerwise_retriever in self.layerwise_retrievers:\n        ret_token_mask = next(layerwise_retriever)\n\n        if self.current_layer == self.num_layers - 1:\n            assert ret_token_mask is not None\n            num_retrieved_tokens = ret_token_mask.sum().item()\n            logger.info(\"Retrieved %s tokens\", num_retrieved_tokens)\n\n    return",
      "language": "python"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "wait_for_save()",
      "language": "unknown"
    },
    {
      "code": "1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef wait_for_save(self):\n    \"\"\"Blocking until the KV cache is saved to the connector buffer.\"\"\"\n\n    connector_metadata = self._parent._get_connector_metadata()\n    assert isinstance(connector_metadata, LMCacheConnectorMetadata)\n\n    self.lmcache_engine.lookup_unpin(  # type: ignore\n        connector_metadata.lookup_requests_in_step\n    )\n\n    if self.kv_role == \"kv_consumer\":\n        # Don't do save if the role is kv_consumer\n        return\n\n    if self.use_layerwise:\n        for layerwise_storer in self.layerwise_storers:\n            next(layerwise_storer)\n        return\n\n    assert len(self.kv_caches) > 0\n    kvcaches = list(self.kv_caches.values())\n\n    assert self.lmcache_engine is not None\n\n    for request in connector_metadata.requests:\n        save_spec = request.save_spec\n        if (\n            save_spec is None or not save_spec.can_save\n        ) and self.kv_role != \"kv_producer\":\n            continue\n\n        token_ids = request.token_ids\n\n        slot_mapping = request.slot_mapping\n        assert isinstance(slot_mapping, torch.Tensor)\n        assert len(slot_mapping) == len(token_ids)\n        assert save_spec is not None\n\n        # TODO: have a pre-allocated buffer to hold the slot_mappings\n        slot_mapping = slot_mapping.cuda()\n\n        skip_leading_tokens = save_spec.skip_leading_tokens\n        if self.kv_role == \"kv_producer\":\n            assert request.disagg_spec is not None\n            skip_leading_tokens = min(\n                skip_leading_tokens, request.disagg_spec.num_transferred_tokens\n            )\n\n        if skip_leading_tokens == len(token_ids):\n            continue  # skip this request\n        # Align to lmcache chunk size\n        skip_leading_tokens = (\n            skip_leading_tokens\n            // self._lmcache_chunk_size\n            * self._lmcache_chunk_size\n        )\n\n        store_mask = torch.ones(len(token_ids), dtype=torch.bool)\n        store_mask[:skip_leading_tokens] = False\n\n        logger.info(\n            \"Storing KV cache for %d out of %d tokens \"\n            \"(skip_leading_tokens=%d) for request %s\",\n            len(token_ids) - skip_leading_tokens,\n            len(token_ids),\n            skip_leading_tokens,\n            request.req_id,\n        )\n\n        is_last_prefill = request.is_last_prefill\n        if is_last_prefill:\n            if request.disagg_spec:\n                request.disagg_spec.is_last_prefill = True\n        else:\n            token_len = len(token_ids)\n            aligned_token_len = (\n                token_len // self._lmcache_chunk_size * self._lmcache_chunk_size\n            )\n            token_ids = token_ids[:aligned_token_len]\n            store_mask = store_mask[:aligned_token_len]\n            slot_mapping = slot_mapping[:aligned_token_len]\n\n        self.lmcache_engine.store(\n            token_ids,\n            mask=store_mask,\n            kvcaches=kvcaches,\n            slot_mapping=slot_mapping,\n            offset=skip_leading_tokens,\n            transfer_spec=request.disagg_spec,\n            request_configs=request.request_configs,\n        )\n\n        # NOTE(Jiayi): We assume all tokens are saved\n        save_spec.skip_leading_tokens = len(token_ids)\n        if request.disagg_spec:\n            request.disagg_spec.num_transferred_tokens = len(token_ids)",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\ndef wait_for_save(self):\n    \"\"\"Blocking until the KV cache is saved to the connector buffer.\"\"\"\n\n    connector_metadata = self._parent._get_connector_metadata()\n    assert isinstance(connector_metadata, LMCacheConnectorMetadata)\n\n    self.lmcache_engine.lookup_unpin(  # type: ignore\n        connector_metadata.lookup_requests_in_step\n    )\n\n    if self.kv_role == \"kv_consumer\":\n        # Don't do save if the role is kv_consumer\n        return\n\n    if self.use_layerwise:\n        for layerwise_storer in self.layerwise_storers:\n            next(layerwise_storer)\n        return\n\n    assert len(self.kv_caches) > 0\n    kvcaches = list(self.kv_caches.values())\n\n    assert self.lmcache_engine is not None\n\n    for request in connector_metadata.requests:\n        save_spec = request.save_spec\n        if (\n            save_spec is None or not save_spec.can_save\n        ) and self.kv_role != \"kv_producer\":\n            continue\n\n        token_ids = request.token_ids\n\n        slot_mapping = request.slot_mapping\n        assert isinstance(slot_mapping, torch.Tensor)\n        assert len(slot_mapping) == len(token_ids)\n        assert save_spec is not None\n\n        # TODO: have a pre-allocated buffer to hold the slot_mappings\n        slot_mapping = slot_mapping.cuda()\n\n        skip_leading_tokens = save_spec.skip_leading_tokens\n        if self.kv_role == \"kv_producer\":\n            assert request.disagg_spec is not None\n            skip_leading_tokens = min(\n                skip_leading_tokens, request.disagg_spec.num_transferred_tokens\n            )\n\n        if skip_leading_tokens == len(token_ids):\n            continue  # skip this request\n        # Align to lmcache chunk size\n        skip_leading_tokens = (\n            skip_leading_tokens\n            // self._lmcache_chunk_size\n            * self._lmcache_chunk_size\n        )\n\n        store_mask = torch.ones(len(token_ids), dtype=torch.bool)\n        store_mask[:skip_leading_tokens] = False\n\n        logger.info(\n            \"Storing KV cache for %d out of %d tokens \"\n            \"(skip_leading_tokens=%d) for request %s\",\n            len(token_ids) - skip_leading_tokens,\n            len(token_ids),\n            skip_leading_tokens,\n            request.req_id,\n        )\n\n        is_last_prefill = request.is_last_prefill\n        if is_last_prefill:\n            if request.disagg_spec:\n                request.disagg_spec.is_last_prefill = True\n        else:\n            token_len = len(token_ids)\n            aligned_token_len = (\n                token_len // self._lmcache_chunk_size * self._lmcache_chunk_size\n            )\n            token_ids = token_ids[:aligned_token_len]\n            store_mask = store_mask[:aligned_token_len]\n            slot_mapping = slot_mapping[:aligned_token_len]\n\n        self.lmcache_engine.store(\n            token_ids,\n            mask=store_mask,\n            kvcaches=kvcaches,\n            slot_mapping=slot_mapping,\n            offset=skip_leading_tokens,\n            transfer_spec=request.disagg_spec,\n            request_configs=request.request_configs,\n        )\n\n        # NOTE(Jiayi): We assume all tokens are saved\n        save_spec.skip_leading_tokens = len(token_ids)\n        if request.disagg_spec:\n            request.disagg_spec.num_transferred_tokens = len(token_ids)",
      "language": "python"
    },
    {
      "code": "70\n71\n72\n73\n74\n75\n76\n77",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass LoadSpec:\n    # Number of tokens cached in vLLM\n    vllm_cached_tokens: int\n    # Number of tokens that are cached in LMCache\n    lmcache_cached_tokens: int\n    # Whether the scheduler allow us to load the tokens\n    can_load: bool",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass LoadSpec:\n    # Number of tokens cached in vLLM\n    vllm_cached_tokens: int\n    # Number of tokens that are cached in LMCache\n    lmcache_cached_tokens: int\n    # Whether the scheduler allow us to load the tokens\n    can_load: bool",
      "language": "python"
    },
    {
      "code": "can_load: bool",
      "language": "yaml"
    },
    {
      "code": "can_load: bool",
      "language": "yaml"
    },
    {
      "code": "lmcache_cached_tokens: int",
      "language": "yaml"
    },
    {
      "code": "lmcache_cached_tokens: int",
      "language": "yaml"
    },
    {
      "code": "vllm_cached_tokens: int",
      "language": "yaml"
    },
    {
      "code": "vllm_cached_tokens: int",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    vllm_cached_tokens: int,\n    lmcache_cached_tokens: int,\n    can_load: bool,\n) -> None",
      "language": "python"
    },
    {
      "code": "__init__(\n    vllm_cached_tokens: int,\n    lmcache_cached_tokens: int,\n    can_load: bool,\n) -> None",
      "language": "python"
    },
    {
      "code": "245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass ReqMeta:\n    # Request id\n    req_id: str\n    # Request tokens\n    token_ids: list[int]  # torch.Tensor\n    # Slot mapping\n    slot_mapping: torch.Tensor\n\n    # Whether is last prefill or not\n    is_last_prefill: bool = False\n\n    # Skip save or not\n    save_spec: SaveSpec | None = None\n    # load_spec\n    load_spec: LoadSpec | None = None\n    # disagg spec\n    disagg_spec: DisaggSpec | None = None\n    # the configs of the request\n    request_configs: dict | None = None\n\n    @staticmethod\n    def from_request_tracker(\n        tracker: RequestTracker,\n        block_size: int,\n        lmcache_chunk_size: int = 256,\n        load_spec: LoadSpec | None = None,\n        discard_partial_chunks: bool = True,\n        save_decode_cache: bool = False,\n    ) -> Optional[\"ReqMeta\"]:\n        \"\"\"Create the request metadata from a request tracker.\n\n        Args:\n            tracker (RequestTracker): the request tracker.\n            block_size (int): the block size in vLLM.\n            lmcache_chunk_size (int): the chunk size for LMCache.\n            load_spec (Optional[LoadSpec]): the load spec for KV cache loading.\n            discard_partial_chunks (bool): whether to discard partial chunks.\n            save_decode_cache (bool): whether to save the cache in decode phase.\n\n        Returns:\n            the request metadata if we need to perform load/save\n            operations, None otherwise.\n        \"\"\"\n        input_token_ids = tracker.token_ids\n        input_token_len = len(input_token_ids)\n\n        is_last_prefill = False\n        if input_token_len == tracker.prompt_len:\n            is_last_prefill = True\n\n        # For save operation: do not save if the following condition is met\n        # 1. has already been saved before (num_saved_tokens > 0)\n        # 2. number of unsaved tokens is not reached the chunk boundary\n        # 3. if save_decode_cache is False and it is in decode phase\n\n        skip_leading_tokens = tracker.num_saved_tokens\n        chunk_boundary = (\n            cdiv(tracker.num_saved_tokens + 1, lmcache_chunk_size) * lmcache_chunk_size\n        )\n\n        # NOTE(vladnosiv): for disagg, you cannot skip saving, as saving is a\n        # trqansfer. Check if request_configs has lmcache.skip_save set to True\n        request_skip = (tracker.request_configs or {}).get(\"lmcache.skip_save\", False)\n\n        skip_save = tracker.disagg_spec is None and (\n            tracker.skip_save\n            or (tracker.num_saved_tokens > 0 and input_token_len < chunk_boundary)\n            or (tracker.is_decode_phase and not save_decode_cache)\n            or request_skip\n        )\n\n        if skip_save and load_spec is None:\n            return None\n\n        # Calculate number of tokens to save based on discard_partial_chunks\n        # setting\n\n        # NOTE(vladnosiv): for the input_token_len chunk prefill,\n        # we are required to discard partial chunks,\n        # as new tokens will be added in the next iteration.\n        num_tokens_to_save = (\n            (input_token_len // lmcache_chunk_size * lmcache_chunk_size)\n            if not is_last_prefill or discard_partial_chunks\n            else input_token_len\n        )\n\n        # If we need to save, update the number of saved tokens\n        if not skip_save:\n            tracker.num_saved_tokens = num_tokens_to_save\n        save_spec = SaveSpec(skip_leading_tokens, not skip_save)\n\n        # Calculate the token ids and slot mappings for load and save\n        token_ids = input_token_ids[:num_tokens_to_save]\n\n        # If the request has multimodal hashes, apply them to the token ids\n        if tracker.mm_hashes:\n            token_ids_tensor = torch.tensor(token_ids)\n            assert tracker.mm_positions is not None, (\n                \"tracker got mm_hashes but no mm_positions\"\n            )\n            apply_mm_hashes_to_token_ids(\n                token_ids_tensor, tracker.mm_hashes, tracker.mm_positions\n            )\n            token_ids = token_ids_tensor.tolist()\n\n        num_blocks = len(tracker.allocated_block_ids)\n\n        if len(token_ids) > num_blocks * block_size:\n            logger.error(\n                \"The number of tokens is more than the number of blocks.\"\n                \"Something might be wrong in scheduling logic!\"\n            )\n            logger.error(\n                \"Num tokens: %d, num blocks: %d, block size: %d\",\n                len(token_ids),\n                num_blocks,\n                block_size,\n            )\n\n        block_ids = torch.tensor(tracker.allocated_block_ids, dtype=torch.long)\n        block_offsets = torch.arange(0, block_size, dtype=torch.long)\n        slot_mapping = (\n            block_offsets.reshape((1, block_size))\n            + block_ids.reshape((num_blocks, 1)) * block_size\n        )\n\n        slot_mapping = slot_mapping.flatten()[: len(token_ids)]\n        assert slot_mapping.dtype == torch.long\n\n        # For load operation: check whether the request is scheduled to load\n        if load_spec is not None and load_spec.can_load:\n            logger.debug(\n                \"Scheduled to load %d tokens for request %s\",\n                load_spec.lmcache_cached_tokens,\n                tracker.req_id,\n            )\n        else:\n            # Do not load if not in `can_load` state\n            load_spec = None\n\n        return ReqMeta(\n            req_id=tracker.req_id,\n            token_ids=token_ids,\n            slot_mapping=slot_mapping,\n            is_last_prefill=is_last_prefill,\n            save_spec=save_spec,\n            load_spec=load_spec,\n            disagg_spec=tracker.disagg_spec,\n            request_configs=tracker.request_configs,\n        )",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass ReqMeta:\n    # Request id\n    req_id: str\n    # Request tokens\n    token_ids: list[int]  # torch.Tensor\n    # Slot mapping\n    slot_mapping: torch.Tensor\n\n    # Whether is last prefill or not\n    is_last_prefill: bool = False\n\n    # Skip save or not\n    save_spec: SaveSpec | None = None\n    # load_spec\n    load_spec: LoadSpec | None = None\n    # disagg spec\n    disagg_spec: DisaggSpec | None = None\n    # the configs of the request\n    request_configs: dict | None = None\n\n    @staticmethod\n    def from_request_tracker(\n        tracker: RequestTracker,\n        block_size: int,\n        lmcache_chunk_size: int = 256,\n        load_spec: LoadSpec | None = None,\n        discard_partial_chunks: bool = True,\n        save_decode_cache: bool = False,\n    ) -> Optional[\"ReqMeta\"]:\n        \"\"\"Create the request metadata from a request tracker.\n\n        Args:\n            tracker (RequestTracker): the request tracker.\n            block_size (int): the block size in vLLM.\n            lmcache_chunk_size (int): the chunk size for LMCache.\n            load_spec (Optional[LoadSpec]): the load spec for KV cache loading.\n            discard_partial_chunks (bool): whether to discard partial chunks.\n            save_decode_cache (bool): whether to save the cache in decode phase.\n\n        Returns:\n            the request metadata if we need to perform load/save\n            operations, None otherwise.\n        \"\"\"\n        input_token_ids = tracker.token_ids\n        input_token_len = len(input_token_ids)\n\n        is_last_prefill = False\n        if input_token_len == tracker.prompt_len:\n            is_last_prefill = True\n\n        # For save operation: do not save if the following condition is met\n        # 1. has already been saved before (num_saved_tokens > 0)\n        # 2. number of unsaved tokens is not reached the chunk boundary\n        # 3. if save_decode_cache is False and it is in decode phase\n\n        skip_leading_tokens = tracker.num_saved_tokens\n        chunk_boundary = (\n            cdiv(tracker.num_saved_tokens + 1, lmcache_chunk_size) * lmcache_chunk_size\n        )\n\n        # NOTE(vladnosiv): for disagg, you cannot skip saving, as saving is a\n        # trqansfer. Check if request_configs has lmcache.skip_save set to True\n        request_skip = (tracker.request_configs or {}).get(\"lmcache.skip_save\", False)\n\n        skip_save = tracker.disagg_spec is None and (\n            tracker.skip_save\n            or (tracker.num_saved_tokens > 0 and input_token_len < chunk_boundary)\n            or (tracker.is_decode_phase and not save_decode_cache)\n            or request_skip\n        )\n\n        if skip_save and load_spec is None:\n            return None\n\n        # Calculate number of tokens to save based on discard_partial_chunks\n        # setting\n\n        # NOTE(vladnosiv): for the input_token_len chunk prefill,\n        # we are required to discard partial chunks,\n        # as new tokens will be added in the next iteration.\n        num_tokens_to_save = (\n            (input_token_len // lmcache_chunk_size * lmcache_chunk_size)\n            if not is_last_prefill or discard_partial_chunks\n            else input_token_len\n        )\n\n        # If we need to save, update the number of saved tokens\n        if not skip_save:\n            tracker.num_saved_tokens = num_tokens_to_save\n        save_spec = SaveSpec(skip_leading_tokens, not skip_save)\n\n        # Calculate the token ids and slot mappings for load and save\n        token_ids = input_token_ids[:num_tokens_to_save]\n\n        # If the request has multimodal hashes, apply them to the token ids\n        if tracker.mm_hashes:\n            token_ids_tensor = torch.tensor(token_ids)\n            assert tracker.mm_positions is not None, (\n                \"tracker got mm_hashes but no mm_positions\"\n            )\n            apply_mm_hashes_to_token_ids(\n                token_ids_tensor, tracker.mm_hashes, tracker.mm_positions\n            )\n            token_ids = token_ids_tensor.tolist()\n\n        num_blocks = len(tracker.allocated_block_ids)\n\n        if len(token_ids) > num_blocks * block_size:\n            logger.error(\n                \"The number of tokens is more than the number of blocks.\"\n                \"Something might be wrong in scheduling logic!\"\n            )\n            logger.error(\n                \"Num tokens: %d, num blocks: %d, block size: %d\",\n                len(token_ids),\n                num_blocks,\n                block_size,\n            )\n\n        block_ids = torch.tensor(tracker.allocated_block_ids, dtype=torch.long)\n        block_offsets = torch.arange(0, block_size, dtype=torch.long)\n        slot_mapping = (\n            block_offsets.reshape((1, block_size))\n            + block_ids.reshape((num_blocks, 1)) * block_size\n        )\n\n        slot_mapping = slot_mapping.flatten()[: len(token_ids)]\n        assert slot_mapping.dtype == torch.long\n\n        # For load operation: check whether the request is scheduled to load\n        if load_spec is not None and load_spec.can_load:\n            logger.debug(\n                \"Scheduled to load %d tokens for request %s\",\n                load_spec.lmcache_cached_tokens,\n                tracker.req_id,\n            )\n        else:\n            # Do not load if not in `can_load` state\n            load_spec = None\n\n        return ReqMeta(\n            req_id=tracker.req_id,\n            token_ids=token_ids,\n            slot_mapping=slot_mapping,\n            is_last_prefill=is_last_prefill,\n            save_spec=save_spec,\n            load_spec=load_spec,\n            disagg_spec=tracker.disagg_spec,\n            request_configs=tracker.request_configs,\n        )",
      "language": "python"
    },
    {
      "code": "disagg_spec: DisaggSpec | None = None",
      "language": "yaml"
    },
    {
      "code": "disagg_spec: DisaggSpec | None = None",
      "language": "yaml"
    },
    {
      "code": "is_last_prefill: bool = False",
      "language": "typescript"
    },
    {
      "code": "is_last_prefill: bool = False",
      "language": "typescript"
    },
    {
      "code": "load_spec: LoadSpec | None = None",
      "language": "yaml"
    },
    {
      "code": "load_spec: LoadSpec | None = None",
      "language": "yaml"
    },
    {
      "code": "req_id: str",
      "language": "yaml"
    },
    {
      "code": "req_id: str",
      "language": "yaml"
    },
    {
      "code": "request_configs: dict | None = None",
      "language": "yaml"
    },
    {
      "code": "request_configs: dict | None = None",
      "language": "yaml"
    },
    {
      "code": "save_spec: SaveSpec | None = None",
      "language": "yaml"
    },
    {
      "code": "save_spec: SaveSpec | None = None",
      "language": "yaml"
    },
    {
      "code": "slot_mapping: Tensor",
      "language": "yaml"
    },
    {
      "code": "slot_mapping: Tensor",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    req_id: str,\n    token_ids: list[int],\n    slot_mapping: Tensor,\n    is_last_prefill: bool = False,\n    save_spec: SaveSpec | None = None,\n    load_spec: LoadSpec | None = None,\n    disagg_spec: DisaggSpec | None = None,\n    request_configs: dict | None = None,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    req_id: str,\n    token_ids: list[int],\n    slot_mapping: Tensor,\n    is_last_prefill: bool = False,\n    save_spec: SaveSpec | None = None,\n    load_spec: LoadSpec | None = None,\n    disagg_spec: DisaggSpec | None = None,\n    request_configs: dict | None = None,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "from_request_tracker(\n    tracker: RequestTracker,\n    block_size: int,\n    lmcache_chunk_size: int = 256,\n    load_spec: LoadSpec | None = None,\n    discard_partial_chunks: bool = True,\n    save_decode_cache: bool = False,\n) -> Optional[ReqMeta]",
      "language": "typescript"
    },
    {
      "code": "from_request_tracker(\n    tracker: RequestTracker,\n    block_size: int,\n    lmcache_chunk_size: int = 256,\n    load_spec: LoadSpec | None = None,\n    discard_partial_chunks: bool = True,\n    save_decode_cache: bool = False,\n) -> Optional[ReqMeta]",
      "language": "typescript"
    },
    {
      "code": "266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395",
      "language": "unknown"
    },
    {
      "code": "@staticmethod\ndef from_request_tracker(\n    tracker: RequestTracker,\n    block_size: int,\n    lmcache_chunk_size: int = 256,\n    load_spec: LoadSpec | None = None,\n    discard_partial_chunks: bool = True,\n    save_decode_cache: bool = False,\n) -> Optional[\"ReqMeta\"]:\n    \"\"\"Create the request metadata from a request tracker.\n\n    Args:\n        tracker (RequestTracker): the request tracker.\n        block_size (int): the block size in vLLM.\n        lmcache_chunk_size (int): the chunk size for LMCache.\n        load_spec (Optional[LoadSpec]): the load spec for KV cache loading.\n        discard_partial_chunks (bool): whether to discard partial chunks.\n        save_decode_cache (bool): whether to save the cache in decode phase.\n\n    Returns:\n        the request metadata if we need to perform load/save\n        operations, None otherwise.\n    \"\"\"\n    input_token_ids = tracker.token_ids\n    input_token_len = len(input_token_ids)\n\n    is_last_prefill = False\n    if input_token_len == tracker.prompt_len:\n        is_last_prefill = True\n\n    # For save operation: do not save if the following condition is met\n    # 1. has already been saved before (num_saved_tokens > 0)\n    # 2. number of unsaved tokens is not reached the chunk boundary\n    # 3. if save_decode_cache is False and it is in decode phase\n\n    skip_leading_tokens = tracker.num_saved_tokens\n    chunk_boundary = (\n        cdiv(tracker.num_saved_tokens + 1, lmcache_chunk_size) * lmcache_chunk_size\n    )\n\n    # NOTE(vladnosiv): for disagg, you cannot skip saving, as saving is a\n    # trqansfer. Check if request_configs has lmcache.skip_save set to True\n    request_skip = (tracker.request_configs or {}).get(\"lmcache.skip_save\", False)\n\n    skip_save = tracker.disagg_spec is None and (\n        tracker.skip_save\n        or (tracker.num_saved_tokens > 0 and input_token_len < chunk_boundary)\n        or (tracker.is_decode_phase and not save_decode_cache)\n        or request_skip\n    )\n\n    if skip_save and load_spec is None:\n        return None\n\n    # Calculate number of tokens to save based on discard_partial_chunks\n    # setting\n\n    # NOTE(vladnosiv): for the input_token_len chunk prefill,\n    # we are required to discard partial chunks,\n    # as new tokens will be added in the next iteration.\n    num_tokens_to_save = (\n        (input_token_len // lmcache_chunk_size * lmcache_chunk_size)\n        if not is_last_prefill or discard_partial_chunks\n        else input_token_len\n    )\n\n    # If we need to save, update the number of saved tokens\n    if not skip_save:\n        tracker.num_saved_tokens = num_tokens_to_save\n    save_spec = SaveSpec(skip_leading_tokens, not skip_save)\n\n    # Calculate the token ids and slot mappings for load and save\n    token_ids = input_token_ids[:num_tokens_to_save]\n\n    # If the request has multimodal hashes, apply them to the token ids\n    if tracker.mm_hashes:\n        token_ids_tensor = torch.tensor(token_ids)\n        assert tracker.mm_positions is not None, (\n            \"tracker got mm_hashes but no mm_positions\"\n        )\n        apply_mm_hashes_to_token_ids(\n            token_ids_tensor, tracker.mm_hashes, tracker.mm_positions\n        )\n        token_ids = token_ids_tensor.tolist()\n\n    num_blocks = len(tracker.allocated_block_ids)\n\n    if len(token_ids) > num_blocks * block_size:\n        logger.error(\n            \"The number of tokens is more than the number of blocks.\"\n            \"Something might be wrong in scheduling logic!\"\n        )\n        logger.error(\n            \"Num tokens: %d, num blocks: %d, block size: %d\",\n            len(token_ids),\n            num_blocks,\n            block_size,\n        )\n\n    block_ids = torch.tensor(tracker.allocated_block_ids, dtype=torch.long)\n    block_offsets = torch.arange(0, block_size, dtype=torch.long)\n    slot_mapping = (\n        block_offsets.reshape((1, block_size))\n        + block_ids.reshape((num_blocks, 1)) * block_size\n    )\n\n    slot_mapping = slot_mapping.flatten()[: len(token_ids)]\n    assert slot_mapping.dtype == torch.long\n\n    # For load operation: check whether the request is scheduled to load\n    if load_spec is not None and load_spec.can_load:\n        logger.debug(\n            \"Scheduled to load %d tokens for request %s\",\n            load_spec.lmcache_cached_tokens,\n            tracker.req_id,\n        )\n    else:\n        # Do not load if not in `can_load` state\n        load_spec = None\n\n    return ReqMeta(\n        req_id=tracker.req_id,\n        token_ids=token_ids,\n        slot_mapping=slot_mapping,\n        is_last_prefill=is_last_prefill,\n        save_spec=save_spec,\n        load_spec=load_spec,\n        disagg_spec=tracker.disagg_spec,\n        request_configs=tracker.request_configs,\n    )",
      "language": "python"
    },
    {
      "code": "@staticmethod\ndef from_request_tracker(\n    tracker: RequestTracker,\n    block_size: int,\n    lmcache_chunk_size: int = 256,\n    load_spec: LoadSpec | None = None,\n    discard_partial_chunks: bool = True,\n    save_decode_cache: bool = False,\n) -> Optional[\"ReqMeta\"]:\n    \"\"\"Create the request metadata from a request tracker.\n\n    Args:\n        tracker (RequestTracker): the request tracker.\n        block_size (int): the block size in vLLM.\n        lmcache_chunk_size (int): the chunk size for LMCache.\n        load_spec (Optional[LoadSpec]): the load spec for KV cache loading.\n        discard_partial_chunks (bool): whether to discard partial chunks.\n        save_decode_cache (bool): whether to save the cache in decode phase.\n\n    Returns:\n        the request metadata if we need to perform load/save\n        operations, None otherwise.\n    \"\"\"\n    input_token_ids = tracker.token_ids\n    input_token_len = len(input_token_ids)\n\n    is_last_prefill = False\n    if input_token_len == tracker.prompt_len:\n        is_last_prefill = True\n\n    # For save operation: do not save if the following condition is met\n    # 1. has already been saved before (num_saved_tokens > 0)\n    # 2. number of unsaved tokens is not reached the chunk boundary\n    # 3. if save_decode_cache is False and it is in decode phase\n\n    skip_leading_tokens = tracker.num_saved_tokens\n    chunk_boundary = (\n        cdiv(tracker.num_saved_tokens + 1, lmcache_chunk_size) * lmcache_chunk_size\n    )\n\n    # NOTE(vladnosiv): for disagg, you cannot skip saving, as saving is a\n    # trqansfer. Check if request_configs has lmcache.skip_save set to True\n    request_skip = (tracker.request_configs or {}).get(\"lmcache.skip_save\", False)\n\n    skip_save = tracker.disagg_spec is None and (\n        tracker.skip_save\n        or (tracker.num_saved_tokens > 0 and input_token_len < chunk_boundary)\n        or (tracker.is_decode_phase and not save_decode_cache)\n        or request_skip\n    )\n\n    if skip_save and load_spec is None:\n        return None\n\n    # Calculate number of tokens to save based on discard_partial_chunks\n    # setting\n\n    # NOTE(vladnosiv): for the input_token_len chunk prefill,\n    # we are required to discard partial chunks,\n    # as new tokens will be added in the next iteration.\n    num_tokens_to_save = (\n        (input_token_len // lmcache_chunk_size * lmcache_chunk_size)\n        if not is_last_prefill or discard_partial_chunks\n        else input_token_len\n    )\n\n    # If we need to save, update the number of saved tokens\n    if not skip_save:\n        tracker.num_saved_tokens = num_tokens_to_save\n    save_spec = SaveSpec(skip_leading_tokens, not skip_save)\n\n    # Calculate the token ids and slot mappings for load and save\n    token_ids = input_token_ids[:num_tokens_to_save]\n\n    # If the request has multimodal hashes, apply them to the token ids\n    if tracker.mm_hashes:\n        token_ids_tensor = torch.tensor(token_ids)\n        assert tracker.mm_positions is not None, (\n            \"tracker got mm_hashes but no mm_positions\"\n        )\n        apply_mm_hashes_to_token_ids(\n            token_ids_tensor, tracker.mm_hashes, tracker.mm_positions\n        )\n        token_ids = token_ids_tensor.tolist()\n\n    num_blocks = len(tracker.allocated_block_ids)\n\n    if len(token_ids) > num_blocks * block_size:\n        logger.error(\n            \"The number of tokens is more than the number of blocks.\"\n            \"Something might be wrong in scheduling logic!\"\n        )\n        logger.error(\n            \"Num tokens: %d, num blocks: %d, block size: %d\",\n            len(token_ids),\n            num_blocks,\n            block_size,\n        )\n\n    block_ids = torch.tensor(tracker.allocated_block_ids, dtype=torch.long)\n    block_offsets = torch.arange(0, block_size, dtype=torch.long)\n    slot_mapping = (\n        block_offsets.reshape((1, block_size))\n        + block_ids.reshape((num_blocks, 1)) * block_size\n    )\n\n    slot_mapping = slot_mapping.flatten()[: len(token_ids)]\n    assert slot_mapping.dtype == torch.long\n\n    # For load operation: check whether the request is scheduled to load\n    if load_spec is not None and load_spec.can_load:\n        logger.debug(\n            \"Scheduled to load %d tokens for request %s\",\n            load_spec.lmcache_cached_tokens,\n            tracker.req_id,\n        )\n    else:\n        # Do not load if not in `can_load` state\n        load_spec = None\n\n    return ReqMeta(\n        req_id=tracker.req_id,\n        token_ids=token_ids,\n        slot_mapping=slot_mapping,\n        is_last_prefill=is_last_prefill,\n        save_spec=save_spec,\n        load_spec=load_spec,\n        disagg_spec=tracker.disagg_spec,\n        request_configs=tracker.request_configs,\n    )",
      "language": "python"
    },
    {
      "code": "120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass RequestTracker:\n    # Request id\n    req_id: str\n\n    # Total prompt token length\n    prompt_len: int\n\n    # The token ids that has been scheduled so far\n    token_ids: list[int]\n\n    # The block ids that has been allocated so far\n    # NOTE: allocated blocks could be more than the number of tokens\n    allocated_block_ids: list[int]\n\n    # The number of tokens that has been saved\n    num_saved_tokens: int = 0\n\n    # Disagg spec for the request\n    disagg_spec: DisaggSpec | None = None\n\n    # Multimodal hashes and positions\n    mm_hashes: list[str] | None = None\n    mm_positions: list[\"PlaceholderRange\"] | None = None\n\n    # The configs of the request, includes tags and other configs\n    request_configs: dict | None = None\n\n    # Whether the request is in decode phase\n    is_decode_phase = False\n\n    # Whether the request cache should be saved\n    skip_save: bool = False\n\n    @_lmcache_nvtx_annotate\n    @staticmethod\n    def from_new_request(\n        lmcache_config: LMCacheEngineConfig,\n        new_request: \"NewRequestData\",\n        num_tokens_to_compute: int,\n        lmcache_cached_tokens: int,\n        skip_save: bool,\n    ) -> \"RequestTracker\":\n        \"\"\"Create the request tracker from a new request.\n\n        Args:\n            lmcache_config (LMCacheEngineConfig): the LMCache engine config.\n            new_request (NewRequestData): the new request data.\n            num_tokens_to_compute (int): the number of tokens that will\n                be 'computed', including the `num_computed_tokens` (vLLM's\n                local cache hit) and new tokens that will be scheduled.\n            lmcache_cached_tokens (int): the number of tokens that are\n                cached in LMCache.\n            skip_save (bool): whether the request cache should be saved\n        \"\"\"\n        # vLLM 0.9.0 update: request.block_ids changed from list[int] to\n        # list[list[int]]\n        # Need to check the type of request.block_ids\n\n        unfolded_block_ids = []\n\n        if not isinstance(new_request.block_ids[0], list):\n            unfolded_block_ids = new_request.block_ids.copy()\n        else:\n            # According to the vLLM code\n            # (https://github.com/vllm-project/vllm/blob/main/vllm/v1/core/\n            # sched/scheduler.py#L943),\n            # only one KVCacheGroup is supported in connector for now.\n            unfolded_block_ids = new_request.block_ids[0].copy()\n\n        # NOTE: Initialized in `update_state_after_alloc`\n        disagg_spec = tmp_disagg_tracker.pop(new_request.req_id, None)\n\n        if new_request.sampling_params:\n            request_configs = extract_request_configs(new_request.sampling_params)\n        else:\n            request_configs = None\n\n        mm_hashes, mm_positions = extract_mm_features(new_request, modify=True)\n\n        assert new_request.prompt_token_ids is not None\n        return RequestTracker(\n            req_id=new_request.req_id,\n            prompt_len=len(new_request.prompt_token_ids),\n            token_ids=new_request.prompt_token_ids[:num_tokens_to_compute].copy(),\n            allocated_block_ids=unfolded_block_ids,\n            num_saved_tokens=lmcache_cached_tokens,\n            disagg_spec=disagg_spec,\n            mm_hashes=mm_hashes,\n            mm_positions=mm_positions,\n            skip_save=skip_save,\n            request_configs=request_configs,\n        )\n\n    def update(\n        self,\n        new_token_ids: list[int],\n        new_block_ids: tuple[list[int], ...] | None | list[int],\n    ) -> None:\n        \"\"\"Update the request tracker when a running request is\n        scheduled again\n        \"\"\"\n\n        self.token_ids.extend(new_token_ids)\n\n        if new_block_ids is None:\n            # https://github.com/vllm-project/vllm/commit/\n            # b029de9902aa3ac58806c8c17776c7074175b6db\n            new_block_ids = []\n        elif len(new_block_ids) == 0:\n            new_block_ids = []\n        elif isinstance(new_block_ids, tuple):\n            new_block_ids = new_block_ids[0]\n        elif isinstance(new_block_ids, list):\n            pass\n        else:\n            raise ValueError(f\"Unsupported new_block_ids type {type(new_block_ids)}\")\n        self.allocated_block_ids.extend(new_block_ids)\n\n        # When a request is scheduled again, and the number of new tokens\n        # is 1 (excluding chunked prefill), the request is in decode phase.\n        if len(new_token_ids) == 1:\n            self.is_decode_phase = True",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass RequestTracker:\n    # Request id\n    req_id: str\n\n    # Total prompt token length\n    prompt_len: int\n\n    # The token ids that has been scheduled so far\n    token_ids: list[int]\n\n    # The block ids that has been allocated so far\n    # NOTE: allocated blocks could be more than the number of tokens\n    allocated_block_ids: list[int]\n\n    # The number of tokens that has been saved\n    num_saved_tokens: int = 0\n\n    # Disagg spec for the request\n    disagg_spec: DisaggSpec | None = None\n\n    # Multimodal hashes and positions\n    mm_hashes: list[str] | None = None\n    mm_positions: list[\"PlaceholderRange\"] | None = None\n\n    # The configs of the request, includes tags and other configs\n    request_configs: dict | None = None\n\n    # Whether the request is in decode phase\n    is_decode_phase = False\n\n    # Whether the request cache should be saved\n    skip_save: bool = False\n\n    @_lmcache_nvtx_annotate\n    @staticmethod\n    def from_new_request(\n        lmcache_config: LMCacheEngineConfig,\n        new_request: \"NewRequestData\",\n        num_tokens_to_compute: int,\n        lmcache_cached_tokens: int,\n        skip_save: bool,\n    ) -> \"RequestTracker\":\n        \"\"\"Create the request tracker from a new request.\n\n        Args:\n            lmcache_config (LMCacheEngineConfig): the LMCache engine config.\n            new_request (NewRequestData): the new request data.\n            num_tokens_to_compute (int): the number of tokens that will\n                be 'computed', including the `num_computed_tokens` (vLLM's\n                local cache hit) and new tokens that will be scheduled.\n            lmcache_cached_tokens (int): the number of tokens that are\n                cached in LMCache.\n            skip_save (bool): whether the request cache should be saved\n        \"\"\"\n        # vLLM 0.9.0 update: request.block_ids changed from list[int] to\n        # list[list[int]]\n        # Need to check the type of request.block_ids\n\n        unfolded_block_ids = []\n\n        if not isinstance(new_request.block_ids[0], list):\n            unfolded_block_ids = new_request.block_ids.copy()\n        else:\n            # According to the vLLM code\n            # (https://github.com/vllm-project/vllm/blob/main/vllm/v1/core/\n            # sched/scheduler.py#L943),\n            # only one KVCacheGroup is supported in connector for now.\n            unfolded_block_ids = new_request.block_ids[0].copy()\n\n        # NOTE: Initialized in `update_state_after_alloc`\n        disagg_spec = tmp_disagg_tracker.pop(new_request.req_id, None)\n\n        if new_request.sampling_params:\n            request_configs = extract_request_configs(new_request.sampling_params)\n        else:\n            request_configs = None\n\n        mm_hashes, mm_positions = extract_mm_features(new_request, modify=True)\n\n        assert new_request.prompt_token_ids is not None\n        return RequestTracker(\n            req_id=new_request.req_id,\n            prompt_len=len(new_request.prompt_token_ids),\n            token_ids=new_request.prompt_token_ids[:num_tokens_to_compute].copy(),\n            allocated_block_ids=unfolded_block_ids,\n            num_saved_tokens=lmcache_cached_tokens,\n            disagg_spec=disagg_spec,\n            mm_hashes=mm_hashes,\n            mm_positions=mm_positions,\n            skip_save=skip_save,\n            request_configs=request_configs,\n        )\n\n    def update(\n        self,\n        new_token_ids: list[int],\n        new_block_ids: tuple[list[int], ...] | None | list[int],\n    ) -> None:\n        \"\"\"Update the request tracker when a running request is\n        scheduled again\n        \"\"\"\n\n        self.token_ids.extend(new_token_ids)\n\n        if new_block_ids is None:\n            # https://github.com/vllm-project/vllm/commit/\n            # b029de9902aa3ac58806c8c17776c7074175b6db\n            new_block_ids = []\n        elif len(new_block_ids) == 0:\n            new_block_ids = []\n        elif isinstance(new_block_ids, tuple):\n            new_block_ids = new_block_ids[0]\n        elif isinstance(new_block_ids, list):\n            pass\n        else:\n            raise ValueError(f\"Unsupported new_block_ids type {type(new_block_ids)}\")\n        self.allocated_block_ids.extend(new_block_ids)\n\n        # When a request is scheduled again, and the number of new tokens\n        # is 1 (excluding chunked prefill), the request is in decode phase.\n        if len(new_token_ids) == 1:\n            self.is_decode_phase = True",
      "language": "python"
    },
    {
      "code": "allocated_block_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "allocated_block_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "disagg_spec: DisaggSpec | None = None",
      "language": "yaml"
    },
    {
      "code": "disagg_spec: DisaggSpec | None = None",
      "language": "yaml"
    },
    {
      "code": "is_decode_phase = False",
      "language": "unknown"
    },
    {
      "code": "is_decode_phase = False",
      "language": "unknown"
    },
    {
      "code": "mm_hashes: list[str] | None = None",
      "language": "yaml"
    },
    {
      "code": "mm_hashes: list[str] | None = None",
      "language": "yaml"
    },
    {
      "code": "mm_positions: list[PlaceholderRange] | None = None",
      "language": "yaml"
    },
    {
      "code": "mm_positions: list[PlaceholderRange] | None = None",
      "language": "yaml"
    },
    {
      "code": "num_saved_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "num_saved_tokens: int = 0",
      "language": "typescript"
    },
    {
      "code": "prompt_len: int",
      "language": "yaml"
    },
    {
      "code": "prompt_len: int",
      "language": "yaml"
    },
    {
      "code": "req_id: str",
      "language": "yaml"
    },
    {
      "code": "req_id: str",
      "language": "yaml"
    },
    {
      "code": "request_configs: dict | None = None",
      "language": "yaml"
    },
    {
      "code": "request_configs: dict | None = None",
      "language": "yaml"
    },
    {
      "code": "skip_save: bool = False",
      "language": "typescript"
    },
    {
      "code": "skip_save: bool = False",
      "language": "typescript"
    },
    {
      "code": "token_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "token_ids: list[int]",
      "language": "yaml"
    },
    {
      "code": "__init__(\n    req_id: str,\n    prompt_len: int,\n    token_ids: list[int],\n    allocated_block_ids: list[int],\n    num_saved_tokens: int = 0,\n    disagg_spec: DisaggSpec | None = None,\n    mm_hashes: list[str] | None = None,\n    mm_positions: list[PlaceholderRange] | None = None,\n    request_configs: dict | None = None,\n    skip_save: bool = False,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "__init__(\n    req_id: str,\n    prompt_len: int,\n    token_ids: list[int],\n    allocated_block_ids: list[int],\n    num_saved_tokens: int = 0,\n    disagg_spec: DisaggSpec | None = None,\n    mm_hashes: list[str] | None = None,\n    mm_positions: list[PlaceholderRange] | None = None,\n    request_configs: dict | None = None,\n    skip_save: bool = False,\n) -> None",
      "language": "typescript"
    },
    {
      "code": "from_new_request(\n    lmcache_config: LMCacheEngineConfig,\n    new_request: NewRequestData,\n    num_tokens_to_compute: int,\n    lmcache_cached_tokens: int,\n    skip_save: bool,\n) -> RequestTracker",
      "language": "php"
    },
    {
      "code": "from_new_request(\n    lmcache_config: LMCacheEngineConfig,\n    new_request: NewRequestData,\n    num_tokens_to_compute: int,\n    lmcache_cached_tokens: int,\n    skip_save: bool,\n) -> RequestTracker",
      "language": "php"
    },
    {
      "code": "154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212",
      "language": "unknown"
    },
    {
      "code": "@_lmcache_nvtx_annotate\n@staticmethod\ndef from_new_request(\n    lmcache_config: LMCacheEngineConfig,\n    new_request: \"NewRequestData\",\n    num_tokens_to_compute: int,\n    lmcache_cached_tokens: int,\n    skip_save: bool,\n) -> \"RequestTracker\":\n    \"\"\"Create the request tracker from a new request.\n\n    Args:\n        lmcache_config (LMCacheEngineConfig): the LMCache engine config.\n        new_request (NewRequestData): the new request data.\n        num_tokens_to_compute (int): the number of tokens that will\n            be 'computed', including the `num_computed_tokens` (vLLM's\n            local cache hit) and new tokens that will be scheduled.\n        lmcache_cached_tokens (int): the number of tokens that are\n            cached in LMCache.\n        skip_save (bool): whether the request cache should be saved\n    \"\"\"\n    # vLLM 0.9.0 update: request.block_ids changed from list[int] to\n    # list[list[int]]\n    # Need to check the type of request.block_ids\n\n    unfolded_block_ids = []\n\n    if not isinstance(new_request.block_ids[0], list):\n        unfolded_block_ids = new_request.block_ids.copy()\n    else:\n        # According to the vLLM code\n        # (https://github.com/vllm-project/vllm/blob/main/vllm/v1/core/\n        # sched/scheduler.py#L943),\n        # only one KVCacheGroup is supported in connector for now.\n        unfolded_block_ids = new_request.block_ids[0].copy()\n\n    # NOTE: Initialized in `update_state_after_alloc`\n    disagg_spec = tmp_disagg_tracker.pop(new_request.req_id, None)\n\n    if new_request.sampling_params:\n        request_configs = extract_request_configs(new_request.sampling_params)\n    else:\n        request_configs = None\n\n    mm_hashes, mm_positions = extract_mm_features(new_request, modify=True)\n\n    assert new_request.prompt_token_ids is not None\n    return RequestTracker(\n        req_id=new_request.req_id,\n        prompt_len=len(new_request.prompt_token_ids),\n        token_ids=new_request.prompt_token_ids[:num_tokens_to_compute].copy(),\n        allocated_block_ids=unfolded_block_ids,\n        num_saved_tokens=lmcache_cached_tokens,\n        disagg_spec=disagg_spec,\n        mm_hashes=mm_hashes,\n        mm_positions=mm_positions,\n        skip_save=skip_save,\n        request_configs=request_configs,\n    )",
      "language": "python"
    },
    {
      "code": "@_lmcache_nvtx_annotate\n@staticmethod\ndef from_new_request(\n    lmcache_config: LMCacheEngineConfig,\n    new_request: \"NewRequestData\",\n    num_tokens_to_compute: int,\n    lmcache_cached_tokens: int,\n    skip_save: bool,\n) -> \"RequestTracker\":\n    \"\"\"Create the request tracker from a new request.\n\n    Args:\n        lmcache_config (LMCacheEngineConfig): the LMCache engine config.\n        new_request (NewRequestData): the new request data.\n        num_tokens_to_compute (int): the number of tokens that will\n            be 'computed', including the `num_computed_tokens` (vLLM's\n            local cache hit) and new tokens that will be scheduled.\n        lmcache_cached_tokens (int): the number of tokens that are\n            cached in LMCache.\n        skip_save (bool): whether the request cache should be saved\n    \"\"\"\n    # vLLM 0.9.0 update: request.block_ids changed from list[int] to\n    # list[list[int]]\n    # Need to check the type of request.block_ids\n\n    unfolded_block_ids = []\n\n    if not isinstance(new_request.block_ids[0], list):\n        unfolded_block_ids = new_request.block_ids.copy()\n    else:\n        # According to the vLLM code\n        # (https://github.com/vllm-project/vllm/blob/main/vllm/v1/core/\n        # sched/scheduler.py#L943),\n        # only one KVCacheGroup is supported in connector for now.\n        unfolded_block_ids = new_request.block_ids[0].copy()\n\n    # NOTE: Initialized in `update_state_after_alloc`\n    disagg_spec = tmp_disagg_tracker.pop(new_request.req_id, None)\n\n    if new_request.sampling_params:\n        request_configs = extract_request_configs(new_request.sampling_params)\n    else:\n        request_configs = None\n\n    mm_hashes, mm_positions = extract_mm_features(new_request, modify=True)\n\n    assert new_request.prompt_token_ids is not None\n    return RequestTracker(\n        req_id=new_request.req_id,\n        prompt_len=len(new_request.prompt_token_ids),\n        token_ids=new_request.prompt_token_ids[:num_tokens_to_compute].copy(),\n        allocated_block_ids=unfolded_block_ids,\n        num_saved_tokens=lmcache_cached_tokens,\n        disagg_spec=disagg_spec,\n        mm_hashes=mm_hashes,\n        mm_positions=mm_positions,\n        skip_save=skip_save,\n        request_configs=request_configs,\n    )",
      "language": "python"
    },
    {
      "code": "update(\n    new_token_ids: list[int],\n    new_block_ids: tuple[list[int], ...] | None | list[int],\n) -> None",
      "language": "rust"
    },
    {
      "code": "update(\n    new_token_ids: list[int],\n    new_block_ids: tuple[list[int], ...] | None | list[int],\n) -> None",
      "language": "rust"
    },
    {
      "code": "214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242",
      "language": "unknown"
    },
    {
      "code": "def update(\n    self,\n    new_token_ids: list[int],\n    new_block_ids: tuple[list[int], ...] | None | list[int],\n) -> None:\n    \"\"\"Update the request tracker when a running request is\n    scheduled again\n    \"\"\"\n\n    self.token_ids.extend(new_token_ids)\n\n    if new_block_ids is None:\n        # https://github.com/vllm-project/vllm/commit/\n        # b029de9902aa3ac58806c8c17776c7074175b6db\n        new_block_ids = []\n    elif len(new_block_ids) == 0:\n        new_block_ids = []\n    elif isinstance(new_block_ids, tuple):\n        new_block_ids = new_block_ids[0]\n    elif isinstance(new_block_ids, list):\n        pass\n    else:\n        raise ValueError(f\"Unsupported new_block_ids type {type(new_block_ids)}\")\n    self.allocated_block_ids.extend(new_block_ids)\n\n    # When a request is scheduled again, and the number of new tokens\n    # is 1 (excluding chunked prefill), the request is in decode phase.\n    if len(new_token_ids) == 1:\n        self.is_decode_phase = True",
      "language": "python"
    },
    {
      "code": "def update(\n    self,\n    new_token_ids: list[int],\n    new_block_ids: tuple[list[int], ...] | None | list[int],\n) -> None:\n    \"\"\"Update the request tracker when a running request is\n    scheduled again\n    \"\"\"\n\n    self.token_ids.extend(new_token_ids)\n\n    if new_block_ids is None:\n        # https://github.com/vllm-project/vllm/commit/\n        # b029de9902aa3ac58806c8c17776c7074175b6db\n        new_block_ids = []\n    elif len(new_block_ids) == 0:\n        new_block_ids = []\n    elif isinstance(new_block_ids, tuple):\n        new_block_ids = new_block_ids[0]\n    elif isinstance(new_block_ids, list):\n        pass\n    else:\n        raise ValueError(f\"Unsupported new_block_ids type {type(new_block_ids)}\")\n    self.allocated_block_ids.extend(new_block_ids)\n\n    # When a request is scheduled again, and the number of new tokens\n    # is 1 (excluding chunked prefill), the request is in decode phase.\n    if len(new_token_ids) == 1:\n        self.is_decode_phase = True",
      "language": "python"
    },
    {
      "code": "80\n81\n82\n83\n84\n85",
      "language": "unknown"
    },
    {
      "code": "@dataclass\nclass SaveSpec:\n    # Skip already saved tokens\n    skip_leading_tokens: int\n    # Whether the scheduler allow us to save the tokens\n    can_save: bool",
      "language": "python"
    },
    {
      "code": "@dataclass\nclass SaveSpec:\n    # Skip already saved tokens\n    skip_leading_tokens: int\n    # Whether the scheduler allow us to save the tokens\n    can_save: bool",
      "language": "python"
    },
    {
      "code": "can_save: bool",
      "language": "yaml"
    },
    {
      "code": "can_save: bool",
      "language": "yaml"
    },
    {
      "code": "skip_leading_tokens: int",
      "language": "yaml"
    },
    {
      "code": "skip_leading_tokens: int",
      "language": "yaml"
    },
    {
      "code": "__init__(skip_leading_tokens: int, can_save: bool) -> None",
      "language": "python"
    },
    {
      "code": "__init__(skip_leading_tokens: int, can_save: bool) -> None",
      "language": "python"
    },
    {
      "code": "_calculate_mtp_layers(vllm_config, model_config)",
      "language": "unknown"
    },
    {
      "code": "_calculate_mtp_layers(vllm_config, model_config)",
      "language": "unknown"
    },
    {
      "code": "402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427",
      "language": "unknown"
    },
    {
      "code": "def _calculate_mtp_layers(vllm_config, model_config):\n    num_mtp_layers = 0\n    if vllm_config is not None and vllm_config.speculative_config is not None:\n        logger.info(\n            \"vllm_config.speculative_config: %s\", vllm_config.speculative_config\n        )\n        # TODO(baoloongmao): Support other MTP methods\n        if vllm_config.speculative_config.method == \"deepseek_mtp\":\n            num_mtp_layers = getattr(\n                model_config.hf_config, \"num_nextn_predict_layers\", 0\n            )\n\n        elif vllm_config.speculative_config.use_eagle():\n            try:\n                draft_model_config = vllm_config.speculative_config.draft_model_config\n                num_mtp_layers = draft_model_config.get_num_layers(\n                    vllm_config.parallel_config\n                )\n                logger.info(\"EAGLE detected %d extra layer(s)\", num_mtp_layers)\n            except Exception:\n                logger.info(\n                    \"EAGLE detected, but failed to get the number of extra layers\"\n                    \"falling back to 1\"\n                )\n                num_mtp_layers = 1\n    return num_mtp_layers",
      "language": "python"
    },
    {
      "code": "def _calculate_mtp_layers(vllm_config, model_config):\n    num_mtp_layers = 0\n    if vllm_config is not None and vllm_config.speculative_config is not None:\n        logger.info(\n            \"vllm_config.speculative_config: %s\", vllm_config.speculative_config\n        )\n        # TODO(baoloongmao): Support other MTP methods\n        if vllm_config.speculative_config.method == \"deepseek_mtp\":\n            num_mtp_layers = getattr(\n                model_config.hf_config, \"num_nextn_predict_layers\", 0\n            )\n\n        elif vllm_config.speculative_config.use_eagle():\n            try:\n                draft_model_config = vllm_config.speculative_config.draft_model_config\n                num_mtp_layers = draft_model_config.get_num_layers(\n                    vllm_config.parallel_config\n                )\n                logger.info(\"EAGLE detected %d extra layer(s)\", num_mtp_layers)\n            except Exception:\n                logger.info(\n                    \"EAGLE detected, but failed to get the number of extra layers\"\n                    \"falling back to 1\"\n                )\n                num_mtp_layers = 1\n    return num_mtp_layers",
      "language": "python"
    },
    {
      "code": "_init_lmcache_engine(\n    lmcache_config: LMCacheEngineConfig,\n    vllm_config: VllmConfig,\n) -> LMCacheEngine",
      "language": "php"
    },
    {
      "code": "_init_lmcache_engine(\n    lmcache_config: LMCacheEngineConfig,\n    vllm_config: VllmConfig,\n) -> LMCacheEngine",
      "language": "php"
    },
    {
      "code": "430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549",
      "language": "unknown"
    },
    {
      "code": "def _init_lmcache_engine(\n    lmcache_config: LMCacheEngineConfig,\n    vllm_config: \"VllmConfig\",\n) -> LMCacheEngine:\n    \"\"\"Initialize the LMCache engine by the given model config and parallel\n    config. This function will check the environment variable\n    `LMCACHE_CONFIG_FILE` to load the configuration file. If that environment\n    variable is not set, this function will return None.\n\n    :param lmcache_config: The LMCache configuration.\n    :type lmcache_config: LMCacheEngineConfig\n    :param vllm_config: The vLLM configuration.\n    :type vllm_config: VllmConfig\n\n    :return: The initialized LMCache engine\n    :rtype: LMCacheEngine\n    \"\"\"\n    if curr_engine := LMCacheEngineBuilder.get(ENGINE_NAME):\n        return curr_engine\n\n    model_config = vllm_config.model_config\n    parallel_config = vllm_config.parallel_config\n    cache_config = vllm_config.cache_config\n\n    assert isinstance(lmcache_config, LMCacheEngineConfig), (\n        \"LMCache v1 configuration is should be passed.\"\n    )\n\n    kv_dtype = get_kv_cache_torch_dtype(cache_config.cache_dtype, model_config.dtype)\n\n    use_mla = mla_enabled(model_config)\n    if use_mla and (\n        lmcache_config.remote_serde != \"naive\"\n        and lmcache_config.remote_serde is not None\n    ):\n        raise ValueError(\"MLA only works with naive serde mode..\")\n\n    # construct kv shape (for mem pool)\n    num_layer = model_config.get_num_layers(parallel_config)\n    num_mtp_layers = _calculate_mtp_layers(vllm_config, model_config)\n    num_layer += num_mtp_layers\n    chunk_size = lmcache_config.chunk_size\n    num_kv_head = model_config.get_num_kv_heads(parallel_config)\n    head_size = model_config.get_head_size()\n    kv_shape = (num_layer, 1 if use_mla else 2, chunk_size, num_kv_head, head_size)\n    logger.info(\n        \"use mla: %s, kv shape: %s, num_mtp_layers: %s\",\n        use_mla,\n        kv_shape,\n        num_mtp_layers,\n    )\n\n    # Change current device.\n    num_gpus = torch.cuda.device_count()\n    local_rank = parallel_config.rank % num_gpus\n    torch.cuda.set_device(local_rank)\n    device = torch.device(f\"cuda:{local_rank}\")\n    metadata = LMCacheEngineMetadata(\n        model_config.model,\n        parallel_config.world_size,\n        parallel_config.rank,\n        \"vllm\",\n        kv_dtype,\n        kv_shape,\n        use_mla,\n    )\n\n    use_gpu = need_gpu_interm_buffer(lmcache_config)\n    vllm_gpu_connector: (\n        VLLMBufferLayerwiseGPUConnector\n        | VLLMPagedMemGPUConnectorV2\n        | VLLMPagedMemLayerwiseGPUConnector\n    )\n\n    if use_mla and lmcache_config.use_layerwise:\n        raise ValueError(\"layerwise MLA connector is not supported yet\")\n\n    # When use_mla is True, num_kv_head is 1\n    hidden_dim_size = num_kv_head * head_size\n    if lmcache_config.use_layerwise:\n        if lmcache_config.enable_blending:\n            # Use layerwise connector for blending\n            vllm_gpu_connector = VLLMBufferLayerwiseGPUConnector(\n                hidden_dim_size,\n                num_layer,\n                use_gpu=use_gpu,\n                chunk_size=chunk_size,\n                dtype=kv_dtype,\n                device=device,\n            )\n        else:\n            vllm_gpu_connector = VLLMPagedMemLayerwiseGPUConnector(\n                hidden_dim_size,\n                num_layer,\n                use_gpu=use_gpu,\n                chunk_size=chunk_size,\n                dtype=kv_dtype,\n                device=device,\n            )\n    else:\n        vllm_gpu_connector = VLLMPagedMemGPUConnectorV2(\n            hidden_dim_size,\n            num_layer,\n            use_gpu=use_gpu,\n            chunk_size=chunk_size,\n            dtype=kv_dtype,\n            device=device,\n            use_mla=use_mla,\n        )\n    tpg = get_tp_group()\n    engine = LMCacheEngineBuilder.get_or_create(\n        ENGINE_NAME,\n        lmcache_config,\n        metadata,\n        vllm_gpu_connector,\n        tpg.broadcast,\n        tpg.broadcast_object,\n    )\n\n    return engine",
      "language": "python"
    },
    {
      "code": "def _init_lmcache_engine(\n    lmcache_config: LMCacheEngineConfig,\n    vllm_config: \"VllmConfig\",\n) -> LMCacheEngine:\n    \"\"\"Initialize the LMCache engine by the given model config and parallel\n    config. This function will check the environment variable\n    `LMCACHE_CONFIG_FILE` to load the configuration file. If that environment\n    variable is not set, this function will return None.\n\n    :param lmcache_config: The LMCache configuration.\n    :type lmcache_config: LMCacheEngineConfig\n    :param vllm_config: The vLLM configuration.\n    :type vllm_config: VllmConfig\n\n    :return: The initialized LMCache engine\n    :rtype: LMCacheEngine\n    \"\"\"\n    if curr_engine := LMCacheEngineBuilder.get(ENGINE_NAME):\n        return curr_engine\n\n    model_config = vllm_config.model_config\n    parallel_config = vllm_config.parallel_config\n    cache_config = vllm_config.cache_config\n\n    assert isinstance(lmcache_config, LMCacheEngineConfig), (\n        \"LMCache v1 configuration is should be passed.\"\n    )\n\n    kv_dtype = get_kv_cache_torch_dtype(cache_config.cache_dtype, model_config.dtype)\n\n    use_mla = mla_enabled(model_config)\n    if use_mla and (\n        lmcache_config.remote_serde != \"naive\"\n        and lmcache_config.remote_serde is not None\n    ):\n        raise ValueError(\"MLA only works with naive serde mode..\")\n\n    # construct kv shape (for mem pool)\n    num_layer = model_config.get_num_layers(parallel_config)\n    num_mtp_layers = _calculate_mtp_layers(vllm_config, model_config)\n    num_layer += num_mtp_layers\n    chunk_size = lmcache_config.chunk_size\n    num_kv_head = model_config.get_num_kv_heads(parallel_config)\n    head_size = model_config.get_head_size()\n    kv_shape = (num_layer, 1 if use_mla else 2, chunk_size, num_kv_head, head_size)\n    logger.info(\n        \"use mla: %s, kv shape: %s, num_mtp_layers: %s\",\n        use_mla,\n        kv_shape,\n        num_mtp_layers,\n    )\n\n    # Change current device.\n    num_gpus = torch.cuda.device_count()\n    local_rank = parallel_config.rank % num_gpus\n    torch.cuda.set_device(local_rank)\n    device = torch.device(f\"cuda:{local_rank}\")\n    metadata = LMCacheEngineMetadata(\n        model_config.model,\n        parallel_config.world_size,\n        parallel_config.rank,\n        \"vllm\",\n        kv_dtype,\n        kv_shape,\n        use_mla,\n    )\n\n    use_gpu = need_gpu_interm_buffer(lmcache_config)\n    vllm_gpu_connector: (\n        VLLMBufferLayerwiseGPUConnector\n        | VLLMPagedMemGPUConnectorV2\n        | VLLMPagedMemLayerwiseGPUConnector\n    )\n\n    if use_mla and lmcache_config.use_layerwise:\n        raise ValueError(\"layerwise MLA connector is not supported yet\")\n\n    # When use_mla is True, num_kv_head is 1\n    hidden_dim_size = num_kv_head * head_size\n    if lmcache_config.use_layerwise:\n        if lmcache_config.enable_blending:\n            # Use layerwise connector for blending\n            vllm_gpu_connector = VLLMBufferLayerwiseGPUConnector(\n                hidden_dim_size,\n                num_layer,\n                use_gpu=use_gpu,\n                chunk_size=chunk_size,\n                dtype=kv_dtype,\n                device=device,\n            )\n        else:\n            vllm_gpu_connector = VLLMPagedMemLayerwiseGPUConnector(\n                hidden_dim_size,\n                num_layer,\n                use_gpu=use_gpu,\n                chunk_size=chunk_size,\n                dtype=kv_dtype,\n                device=device,\n            )\n    else:\n        vllm_gpu_connector = VLLMPagedMemGPUConnectorV2(\n            hidden_dim_size,\n            num_layer,\n            use_gpu=use_gpu,\n            chunk_size=chunk_size,\n            dtype=kv_dtype,\n            device=device,\n            use_mla=use_mla,\n        )\n    tpg = get_tp_group()\n    engine = LMCacheEngineBuilder.get_or_create(\n        ENGINE_NAME,\n        lmcache_config,\n        metadata,\n        vllm_gpu_connector,\n        tpg.broadcast,\n        tpg.broadcast_object,\n    )\n\n    return engine",
      "language": "python"
    },
    {
      "code": "extract_request_configs(\n    sampling_params: SamplingParams,\n) -> dict | None",
      "language": "rust"
    },
    {
      "code": "extract_request_configs(\n    sampling_params: SamplingParams,\n) -> dict | None",
      "language": "rust"
    },
    {
      "code": "102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117",
      "language": "unknown"
    },
    {
      "code": "def extract_request_configs(sampling_params: SamplingParams) -> dict | None:\n    request_configs = None\n    if (\n        sampling_params.extra_args is not None\n        and \"kv_transfer_params\" in sampling_params.extra_args\n    ):\n        kv_transfer_params = sampling_params.extra_args.get(\"kv_transfer_params\")\n        if kv_transfer_params is None:\n            return None\n        assert isinstance(kv_transfer_params, dict)\n        for k, v in kv_transfer_params.items():\n            if k.startswith(\"lmcache.\"):\n                if request_configs is None:\n                    request_configs = {}\n                request_configs[k] = v\n    return request_configs",
      "language": "python"
    },
    {
      "code": "def extract_request_configs(sampling_params: SamplingParams) -> dict | None:\n    request_configs = None\n    if (\n        sampling_params.extra_args is not None\n        and \"kv_transfer_params\" in sampling_params.extra_args\n    ):\n        kv_transfer_params = sampling_params.extra_args.get(\"kv_transfer_params\")\n        if kv_transfer_params is None:\n            return None\n        assert isinstance(kv_transfer_params, dict)\n        for k, v in kv_transfer_params.items():\n            if k.startswith(\"lmcache.\"):\n                if request_configs is None:\n                    request_configs = {}\n                request_configs[k] = v\n    return request_configs",
      "language": "python"
    },
    {
      "code": "need_gpu_interm_buffer(lmcache_config: LMCacheEngineConfig)",
      "language": "unknown"
    },
    {
      "code": "need_gpu_interm_buffer(lmcache_config: LMCacheEngineConfig)",
      "language": "unknown"
    },
    {
      "code": "def need_gpu_interm_buffer(lmcache_config: LMCacheEngineConfig):\n    return not lmcache_config.enable_pd",
      "language": "python"
    },
    {
      "code": "def need_gpu_interm_buffer(lmcache_config: LMCacheEngineConfig):\n    return not lmcache_config.enable_pd",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}