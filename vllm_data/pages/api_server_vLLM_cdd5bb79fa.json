{
  "url": "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
  "title": "api_server - vLLM",
  "content": "Pure ASGI middleware that authenticates each request by checking if the Authorization Bearer token exists and equals anyof \"{api_key}\".\n\nThere are two cases in which authentication is skipped: 1. The HTTP method is OPTIONS. 2. The request path doesn't start with /v1 (e.g. /health).\n\nRobust Server-Sent Events decoder for streaming responses.\n\nAdd content to the buffer.\n\nDecode a chunk of SSE data and return parsed events.\n\nExtract content from event data.\n\nGet the complete buffered content.\n\nMiddleware the set's the X-Request-Id header for each response to a random uuid4 (hex) value if the header isn't already present in the request, otherwise use the provided request id.\n\nConvert the generator to a stream of events in SSE format\n\nExtract content from a streaming response chunk.\n\nLog non-streaming response.\n\nLog streaming response with robust SSE parsing.\n\nCreate EngineClient, either: - in-process using the AsyncLLMEngine Directly - multiprocess using AsyncLLMEngine RPC\n\nReturns the Client or None if the creation failed.\n\nRun a single-worker API server.\n\nRun a single API server worker.\n\nValidate API server args, set up signal handler, create socket ready to serve.",
  "headings": [
    {
      "level": "h1",
      "text": "vllm.entrypoints.openai.api_server ¶",
      "id": "vllm.entrypoints.openai.api_server"
    },
    {
      "level": "h2",
      "text": "ENDPOINT_LOAD_METRICS_FORMAT_HEADER_LABEL module-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.ENDPOINT_LOAD_METRICS_FORMAT_HEADER_LABEL"
    },
    {
      "level": "h2",
      "text": "_running_tasks module-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server._running_tasks"
    },
    {
      "level": "h2",
      "text": "args module-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.args"
    },
    {
      "level": "h2",
      "text": "logger module-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.logger"
    },
    {
      "level": "h2",
      "text": "parser module-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.parser"
    },
    {
      "level": "h2",
      "text": "prometheus_multiproc_dir module-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.prometheus_multiproc_dir"
    },
    {
      "level": "h2",
      "text": "router module-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.router"
    },
    {
      "level": "h2",
      "text": "AuthenticationMiddleware ¶",
      "id": "vllm.entrypoints.openai.api_server.AuthenticationMiddleware"
    },
    {
      "level": "h4",
      "text": "Notes¶",
      "id": "vllm.entrypoints.openai.api_server.AuthenticationMiddleware--notes"
    },
    {
      "level": "h3",
      "text": "api_tokens instance-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.AuthenticationMiddleware.api_tokens"
    },
    {
      "level": "h3",
      "text": "app instance-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.AuthenticationMiddleware.app"
    },
    {
      "level": "h3",
      "text": "__call__ ¶",
      "id": "vllm.entrypoints.openai.api_server.AuthenticationMiddleware.__call__"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.api_server.AuthenticationMiddleware.__init__"
    },
    {
      "level": "h3",
      "text": "verify_token ¶",
      "id": "vllm.entrypoints.openai.api_server.AuthenticationMiddleware.verify_token"
    },
    {
      "level": "h2",
      "text": "SSEDecoder ¶",
      "id": "vllm.entrypoints.openai.api_server.SSEDecoder"
    },
    {
      "level": "h3",
      "text": "buffer instance-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.SSEDecoder.buffer"
    },
    {
      "level": "h3",
      "text": "content_buffer instance-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.SSEDecoder.content_buffer"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.api_server.SSEDecoder.__init__"
    },
    {
      "level": "h3",
      "text": "add_content ¶",
      "id": "vllm.entrypoints.openai.api_server.SSEDecoder.add_content"
    },
    {
      "level": "h3",
      "text": "decode_chunk ¶",
      "id": "vllm.entrypoints.openai.api_server.SSEDecoder.decode_chunk"
    },
    {
      "level": "h3",
      "text": "extract_content ¶",
      "id": "vllm.entrypoints.openai.api_server.SSEDecoder.extract_content"
    },
    {
      "level": "h3",
      "text": "get_complete_content ¶",
      "id": "vllm.entrypoints.openai.api_server.SSEDecoder.get_complete_content"
    },
    {
      "level": "h2",
      "text": "XRequestIdMiddleware ¶",
      "id": "vllm.entrypoints.openai.api_server.XRequestIdMiddleware"
    },
    {
      "level": "h3",
      "text": "app instance-attribute ¶",
      "id": "vllm.entrypoints.openai.api_server.XRequestIdMiddleware.app"
    },
    {
      "level": "h3",
      "text": "__call__ ¶",
      "id": "vllm.entrypoints.openai.api_server.XRequestIdMiddleware.__call__"
    },
    {
      "level": "h3",
      "text": "__init__ ¶",
      "id": "vllm.entrypoints.openai.api_server.XRequestIdMiddleware.__init__"
    },
    {
      "level": "h2",
      "text": "_convert_stream_to_sse_events async ¶",
      "id": "vllm.entrypoints.openai.api_server._convert_stream_to_sse_events"
    },
    {
      "level": "h2",
      "text": "_extract_content_from_chunk ¶",
      "id": "vllm.entrypoints.openai.api_server._extract_content_from_chunk"
    },
    {
      "level": "h2",
      "text": "_log_non_streaming_response ¶",
      "id": "vllm.entrypoints.openai.api_server._log_non_streaming_response"
    },
    {
      "level": "h2",
      "text": "_log_streaming_response ¶",
      "id": "vllm.entrypoints.openai.api_server._log_streaming_response"
    },
    {
      "level": "h2",
      "text": "base ¶",
      "id": "vllm.entrypoints.openai.api_server.base"
    },
    {
      "level": "h2",
      "text": "build_app ¶",
      "id": "vllm.entrypoints.openai.api_server.build_app"
    },
    {
      "level": "h2",
      "text": "build_async_engine_client async ¶",
      "id": "vllm.entrypoints.openai.api_server.build_async_engine_client"
    },
    {
      "level": "h2",
      "text": "build_async_engine_client_from_engine_args async ¶",
      "id": "vllm.entrypoints.openai.api_server.build_async_engine_client_from_engine_args"
    },
    {
      "level": "h2",
      "text": "cancel_responses async ¶",
      "id": "vllm.entrypoints.openai.api_server.cancel_responses"
    },
    {
      "level": "h2",
      "text": "chat ¶",
      "id": "vllm.entrypoints.openai.api_server.chat"
    },
    {
      "level": "h2",
      "text": "completion ¶",
      "id": "vllm.entrypoints.openai.api_server.completion"
    },
    {
      "level": "h2",
      "text": "create_chat_completion async ¶",
      "id": "vllm.entrypoints.openai.api_server.create_chat_completion"
    },
    {
      "level": "h2",
      "text": "create_completion async ¶",
      "id": "vllm.entrypoints.openai.api_server.create_completion"
    },
    {
      "level": "h2",
      "text": "create_messages async ¶",
      "id": "vllm.entrypoints.openai.api_server.create_messages"
    },
    {
      "level": "h2",
      "text": "create_responses async ¶",
      "id": "vllm.entrypoints.openai.api_server.create_responses"
    },
    {
      "level": "h2",
      "text": "create_server_socket ¶",
      "id": "vllm.entrypoints.openai.api_server.create_server_socket"
    },
    {
      "level": "h2",
      "text": "create_server_unix_socket ¶",
      "id": "vllm.entrypoints.openai.api_server.create_server_unix_socket"
    },
    {
      "level": "h2",
      "text": "create_transcriptions async ¶",
      "id": "vllm.entrypoints.openai.api_server.create_transcriptions"
    },
    {
      "level": "h2",
      "text": "create_translations async ¶",
      "id": "vllm.entrypoints.openai.api_server.create_translations"
    },
    {
      "level": "h2",
      "text": "engine_client ¶",
      "id": "vllm.entrypoints.openai.api_server.engine_client"
    },
    {
      "level": "h2",
      "text": "generate_tokens ¶",
      "id": "vllm.entrypoints.openai.api_server.generate_tokens"
    },
    {
      "level": "h2",
      "text": "get_server_load_metrics async ¶",
      "id": "vllm.entrypoints.openai.api_server.get_server_load_metrics"
    },
    {
      "level": "h2",
      "text": "init_app_state async ¶",
      "id": "vllm.entrypoints.openai.api_server.init_app_state"
    },
    {
      "level": "h2",
      "text": "lifespan async ¶",
      "id": "vllm.entrypoints.openai.api_server.lifespan"
    },
    {
      "level": "h2",
      "text": "load_log_config ¶",
      "id": "vllm.entrypoints.openai.api_server.load_log_config"
    },
    {
      "level": "h2",
      "text": "messages ¶",
      "id": "vllm.entrypoints.openai.api_server.messages"
    },
    {
      "level": "h2",
      "text": "models ¶",
      "id": "vllm.entrypoints.openai.api_server.models"
    },
    {
      "level": "h2",
      "text": "responses ¶",
      "id": "vllm.entrypoints.openai.api_server.responses"
    },
    {
      "level": "h2",
      "text": "retrieve_responses async ¶",
      "id": "vllm.entrypoints.openai.api_server.retrieve_responses"
    },
    {
      "level": "h2",
      "text": "run_server async ¶",
      "id": "vllm.entrypoints.openai.api_server.run_server"
    },
    {
      "level": "h2",
      "text": "run_server_worker async ¶",
      "id": "vllm.entrypoints.openai.api_server.run_server_worker"
    },
    {
      "level": "h2",
      "text": "setup_server ¶",
      "id": "vllm.entrypoints.openai.api_server.setup_server"
    },
    {
      "level": "h2",
      "text": "show_available_models async ¶",
      "id": "vllm.entrypoints.openai.api_server.show_available_models"
    },
    {
      "level": "h2",
      "text": "show_version async ¶",
      "id": "vllm.entrypoints.openai.api_server.show_version"
    },
    {
      "level": "h2",
      "text": "tokenization ¶",
      "id": "vllm.entrypoints.openai.api_server.tokenization"
    },
    {
      "level": "h2",
      "text": "transcription ¶",
      "id": "vllm.entrypoints.openai.api_server.transcription"
    },
    {
      "level": "h2",
      "text": "translation ¶",
      "id": "vllm.entrypoints.openai.api_server.translation"
    },
    {
      "level": "h2",
      "text": "validate_api_server_args ¶",
      "id": "vllm.entrypoints.openai.api_server.validate_api_server_args"
    }
  ],
  "code_samples": [
    {
      "code": "ENDPOINT_LOAD_METRICS_FORMAT_HEADER_LABEL = (\n    \"endpoint-load-metrics-format\"\n)",
      "language": "unknown"
    },
    {
      "code": "ENDPOINT_LOAD_METRICS_FORMAT_HEADER_LABEL = (\n    \"endpoint-load-metrics-format\"\n)",
      "language": "unknown"
    },
    {
      "code": "_running_tasks: set[Task] = set()",
      "language": "yaml"
    },
    {
      "code": "_running_tasks: set[Task] = set()",
      "language": "yaml"
    },
    {
      "code": "args = parse_args()",
      "language": "unknown"
    },
    {
      "code": "args = parse_args()",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger('vllm.entrypoints.openai.api_server')",
      "language": "unknown"
    },
    {
      "code": "logger = init_logger('vllm.entrypoints.openai.api_server')",
      "language": "unknown"
    },
    {
      "code": "parser = FlexibleArgumentParser(\n    description=\"vLLM OpenAI-Compatible RESTful API server.\"\n)",
      "language": "unknown"
    },
    {
      "code": "parser = FlexibleArgumentParser(\n    description=\"vLLM OpenAI-Compatible RESTful API server.\"\n)",
      "language": "unknown"
    },
    {
      "code": "prometheus_multiproc_dir: TemporaryDirectory",
      "language": "yaml"
    },
    {
      "code": "prometheus_multiproc_dir: TemporaryDirectory",
      "language": "yaml"
    },
    {
      "code": "router = APIRouter()",
      "language": "unknown"
    },
    {
      "code": "router = APIRouter()",
      "language": "unknown"
    },
    {
      "code": "654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699",
      "language": "unknown"
    },
    {
      "code": "class AuthenticationMiddleware:\n    \"\"\"\n    Pure ASGI middleware that authenticates each request by checking\n    if the Authorization Bearer token exists and equals anyof \"{api_key}\".\n\n    Notes\n    -----\n    There are two cases in which authentication is skipped:\n        1. The HTTP method is OPTIONS.\n        2. The request path doesn't start with /v1 (e.g. /health).\n    \"\"\"\n\n    def __init__(self, app: ASGIApp, tokens: list[str]) -> None:\n        self.app = app\n        self.api_tokens = [hashlib.sha256(t.encode(\"utf-8\")).digest() for t in tokens]\n\n    def verify_token(self, headers: Headers) -> bool:\n        authorization_header_value = headers.get(\"Authorization\")\n        if not authorization_header_value:\n            return False\n\n        scheme, _, param = authorization_header_value.partition(\" \")\n        if scheme.lower() != \"bearer\":\n            return False\n\n        param_hash = hashlib.sha256(param.encode(\"utf-8\")).digest()\n\n        token_match = False\n        for token_hash in self.api_tokens:\n            token_match |= secrets.compare_digest(param_hash, token_hash)\n\n        return token_match\n\n    def __call__(self, scope: Scope, receive: Receive, send: Send) -> Awaitable[None]:\n        if scope[\"type\"] not in (\"http\", \"websocket\") or scope[\"method\"] == \"OPTIONS\":\n            # scope[\"type\"] can be \"lifespan\" or \"startup\" for example,\n            # in which case we don't need to do anything\n            return self.app(scope, receive, send)\n        root_path = scope.get(\"root_path\", \"\")\n        url_path = URL(scope=scope).path.removeprefix(root_path)\n        headers = Headers(scope=scope)\n        # Type narrow to satisfy mypy.\n        if url_path.startswith(\"/v1\") and not self.verify_token(headers):\n            response = JSONResponse(content={\"error\": \"Unauthorized\"}, status_code=401)\n            return response(scope, receive, send)\n        return self.app(scope, receive, send)",
      "language": "python"
    },
    {
      "code": "class AuthenticationMiddleware:\n    \"\"\"\n    Pure ASGI middleware that authenticates each request by checking\n    if the Authorization Bearer token exists and equals anyof \"{api_key}\".\n\n    Notes\n    -----\n    There are two cases in which authentication is skipped:\n        1. The HTTP method is OPTIONS.\n        2. The request path doesn't start with /v1 (e.g. /health).\n    \"\"\"\n\n    def __init__(self, app: ASGIApp, tokens: list[str]) -> None:\n        self.app = app\n        self.api_tokens = [hashlib.sha256(t.encode(\"utf-8\")).digest() for t in tokens]\n\n    def verify_token(self, headers: Headers) -> bool:\n        authorization_header_value = headers.get(\"Authorization\")\n        if not authorization_header_value:\n            return False\n\n        scheme, _, param = authorization_header_value.partition(\" \")\n        if scheme.lower() != \"bearer\":\n            return False\n\n        param_hash = hashlib.sha256(param.encode(\"utf-8\")).digest()\n\n        token_match = False\n        for token_hash in self.api_tokens:\n            token_match |= secrets.compare_digest(param_hash, token_hash)\n\n        return token_match\n\n    def __call__(self, scope: Scope, receive: Receive, send: Send) -> Awaitable[None]:\n        if scope[\"type\"] not in (\"http\", \"websocket\") or scope[\"method\"] == \"OPTIONS\":\n            # scope[\"type\"] can be \"lifespan\" or \"startup\" for example,\n            # in which case we don't need to do anything\n            return self.app(scope, receive, send)\n        root_path = scope.get(\"root_path\", \"\")\n        url_path = URL(scope=scope).path.removeprefix(root_path)\n        headers = Headers(scope=scope)\n        # Type narrow to satisfy mypy.\n        if url_path.startswith(\"/v1\") and not self.verify_token(headers):\n            response = JSONResponse(content={\"error\": \"Unauthorized\"}, status_code=401)\n            return response(scope, receive, send)\n        return self.app(scope, receive, send)",
      "language": "python"
    },
    {
      "code": "api_tokens = [(digest()) for t in tokens]",
      "language": "bash"
    },
    {
      "code": "api_tokens = [(digest()) for t in tokens]",
      "language": "bash"
    },
    {
      "code": "__call__(\n    scope: Scope, receive: Receive, send: Send\n) -> Awaitable[None]",
      "language": "rust"
    },
    {
      "code": "__call__(\n    scope: Scope, receive: Receive, send: Send\n) -> Awaitable[None]",
      "language": "rust"
    },
    {
      "code": "687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699",
      "language": "unknown"
    },
    {
      "code": "def __call__(self, scope: Scope, receive: Receive, send: Send) -> Awaitable[None]:\n    if scope[\"type\"] not in (\"http\", \"websocket\") or scope[\"method\"] == \"OPTIONS\":\n        # scope[\"type\"] can be \"lifespan\" or \"startup\" for example,\n        # in which case we don't need to do anything\n        return self.app(scope, receive, send)\n    root_path = scope.get(\"root_path\", \"\")\n    url_path = URL(scope=scope).path.removeprefix(root_path)\n    headers = Headers(scope=scope)\n    # Type narrow to satisfy mypy.\n    if url_path.startswith(\"/v1\") and not self.verify_token(headers):\n        response = JSONResponse(content={\"error\": \"Unauthorized\"}, status_code=401)\n        return response(scope, receive, send)\n    return self.app(scope, receive, send)",
      "language": "python"
    },
    {
      "code": "def __call__(self, scope: Scope, receive: Receive, send: Send) -> Awaitable[None]:\n    if scope[\"type\"] not in (\"http\", \"websocket\") or scope[\"method\"] == \"OPTIONS\":\n        # scope[\"type\"] can be \"lifespan\" or \"startup\" for example,\n        # in which case we don't need to do anything\n        return self.app(scope, receive, send)\n    root_path = scope.get(\"root_path\", \"\")\n    url_path = URL(scope=scope).path.removeprefix(root_path)\n    headers = Headers(scope=scope)\n    # Type narrow to satisfy mypy.\n    if url_path.startswith(\"/v1\") and not self.verify_token(headers):\n        response = JSONResponse(content={\"error\": \"Unauthorized\"}, status_code=401)\n        return response(scope, receive, send)\n    return self.app(scope, receive, send)",
      "language": "python"
    },
    {
      "code": "__init__(app: ASGIApp, tokens: list[str]) -> None",
      "language": "python"
    },
    {
      "code": "__init__(app: ASGIApp, tokens: list[str]) -> None",
      "language": "python"
    },
    {
      "code": "666\n667\n668",
      "language": "unknown"
    },
    {
      "code": "def __init__(self, app: ASGIApp, tokens: list[str]) -> None:\n    self.app = app\n    self.api_tokens = [hashlib.sha256(t.encode(\"utf-8\")).digest() for t in tokens]",
      "language": "python"
    },
    {
      "code": "def __init__(self, app: ASGIApp, tokens: list[str]) -> None:\n    self.app = app\n    self.api_tokens = [hashlib.sha256(t.encode(\"utf-8\")).digest() for t in tokens]",
      "language": "python"
    },
    {
      "code": "verify_token(headers: Headers) -> bool",
      "language": "php"
    },
    {
      "code": "verify_token(headers: Headers) -> bool",
      "language": "php"
    },
    {
      "code": "670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685",
      "language": "unknown"
    },
    {
      "code": "def verify_token(self, headers: Headers) -> bool:\n    authorization_header_value = headers.get(\"Authorization\")\n    if not authorization_header_value:\n        return False\n\n    scheme, _, param = authorization_header_value.partition(\" \")\n    if scheme.lower() != \"bearer\":\n        return False\n\n    param_hash = hashlib.sha256(param.encode(\"utf-8\")).digest()\n\n    token_match = False\n    for token_hash in self.api_tokens:\n        token_match |= secrets.compare_digest(param_hash, token_hash)\n\n    return token_match",
      "language": "python"
    },
    {
      "code": "def verify_token(self, headers: Headers) -> bool:\n    authorization_header_value = headers.get(\"Authorization\")\n    if not authorization_header_value:\n        return False\n\n    scheme, _, param = authorization_header_value.partition(\" \")\n    if scheme.lower() != \"bearer\":\n        return False\n\n    param_hash = hashlib.sha256(param.encode(\"utf-8\")).digest()\n\n    token_match = False\n    for token_hash in self.api_tokens:\n        token_match |= secrets.compare_digest(param_hash, token_hash)\n\n    return token_match",
      "language": "python"
    },
    {
      "code": "761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811",
      "language": "unknown"
    },
    {
      "code": "class SSEDecoder:\n    \"\"\"Robust Server-Sent Events decoder for streaming responses.\"\"\"\n\n    def __init__(self):\n        self.buffer = \"\"\n        self.content_buffer = []\n\n    def decode_chunk(self, chunk: bytes) -> list[dict]:\n        \"\"\"Decode a chunk of SSE data and return parsed events.\"\"\"\n        import json\n\n        try:\n            chunk_str = chunk.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # Skip malformed chunks\n            return []\n\n        self.buffer += chunk_str\n        events = []\n\n        # Process complete lines\n        while \"\\n\" in self.buffer:\n            line, self.buffer = self.buffer.split(\"\\n\", 1)\n            line = line.rstrip(\"\\r\")  # Handle CRLF\n\n            if line.startswith(\"data: \"):\n                data_str = line[6:].strip()\n                if data_str == \"[DONE]\":\n                    events.append({\"type\": \"done\"})\n                elif data_str:\n                    try:\n                        event_data = json.loads(data_str)\n                        events.append({\"type\": \"data\", \"data\": event_data})\n                    except json.JSONDecodeError:\n                        # Skip malformed JSON\n                        continue\n\n        return events\n\n    def extract_content(self, event_data: dict) -> str:\n        \"\"\"Extract content from event data.\"\"\"\n        return _extract_content_from_chunk(event_data)\n\n    def add_content(self, content: str) -> None:\n        \"\"\"Add content to the buffer.\"\"\"\n        if content:\n            self.content_buffer.append(content)\n\n    def get_complete_content(self) -> str:\n        \"\"\"Get the complete buffered content.\"\"\"\n        return \"\".join(self.content_buffer)",
      "language": "python"
    },
    {
      "code": "class SSEDecoder:\n    \"\"\"Robust Server-Sent Events decoder for streaming responses.\"\"\"\n\n    def __init__(self):\n        self.buffer = \"\"\n        self.content_buffer = []\n\n    def decode_chunk(self, chunk: bytes) -> list[dict]:\n        \"\"\"Decode a chunk of SSE data and return parsed events.\"\"\"\n        import json\n\n        try:\n            chunk_str = chunk.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # Skip malformed chunks\n            return []\n\n        self.buffer += chunk_str\n        events = []\n\n        # Process complete lines\n        while \"\\n\" in self.buffer:\n            line, self.buffer = self.buffer.split(\"\\n\", 1)\n            line = line.rstrip(\"\\r\")  # Handle CRLF\n\n            if line.startswith(\"data: \"):\n                data_str = line[6:].strip()\n                if data_str == \"[DONE]\":\n                    events.append({\"type\": \"done\"})\n                elif data_str:\n                    try:\n                        event_data = json.loads(data_str)\n                        events.append({\"type\": \"data\", \"data\": event_data})\n                    except json.JSONDecodeError:\n                        # Skip malformed JSON\n                        continue\n\n        return events\n\n    def extract_content(self, event_data: dict) -> str:\n        \"\"\"Extract content from event data.\"\"\"\n        return _extract_content_from_chunk(event_data)\n\n    def add_content(self, content: str) -> None:\n        \"\"\"Add content to the buffer.\"\"\"\n        if content:\n            self.content_buffer.append(content)\n\n    def get_complete_content(self) -> str:\n        \"\"\"Get the complete buffered content.\"\"\"\n        return \"\".join(self.content_buffer)",
      "language": "python"
    },
    {
      "code": "buffer = ''",
      "language": "unknown"
    },
    {
      "code": "buffer = ''",
      "language": "unknown"
    },
    {
      "code": "content_buffer = []",
      "language": "unknown"
    },
    {
      "code": "content_buffer = []",
      "language": "unknown"
    },
    {
      "code": "764\n765\n766",
      "language": "unknown"
    },
    {
      "code": "def __init__(self):\n    self.buffer = \"\"\n    self.content_buffer = []",
      "language": "python"
    },
    {
      "code": "def __init__(self):\n    self.buffer = \"\"\n    self.content_buffer = []",
      "language": "python"
    },
    {
      "code": "add_content(content: str) -> None",
      "language": "rust"
    },
    {
      "code": "add_content(content: str) -> None",
      "language": "rust"
    },
    {
      "code": "804\n805\n806\n807",
      "language": "unknown"
    },
    {
      "code": "def add_content(self, content: str) -> None:\n    \"\"\"Add content to the buffer.\"\"\"\n    if content:\n        self.content_buffer.append(content)",
      "language": "python"
    },
    {
      "code": "def add_content(self, content: str) -> None:\n    \"\"\"Add content to the buffer.\"\"\"\n    if content:\n        self.content_buffer.append(content)",
      "language": "python"
    },
    {
      "code": "decode_chunk(chunk: bytes) -> list[dict]",
      "language": "php"
    },
    {
      "code": "decode_chunk(chunk: bytes) -> list[dict]",
      "language": "php"
    },
    {
      "code": "768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798",
      "language": "unknown"
    },
    {
      "code": "def decode_chunk(self, chunk: bytes) -> list[dict]:\n    \"\"\"Decode a chunk of SSE data and return parsed events.\"\"\"\n    import json\n\n    try:\n        chunk_str = chunk.decode(\"utf-8\")\n    except UnicodeDecodeError:\n        # Skip malformed chunks\n        return []\n\n    self.buffer += chunk_str\n    events = []\n\n    # Process complete lines\n    while \"\\n\" in self.buffer:\n        line, self.buffer = self.buffer.split(\"\\n\", 1)\n        line = line.rstrip(\"\\r\")  # Handle CRLF\n\n        if line.startswith(\"data: \"):\n            data_str = line[6:].strip()\n            if data_str == \"[DONE]\":\n                events.append({\"type\": \"done\"})\n            elif data_str:\n                try:\n                    event_data = json.loads(data_str)\n                    events.append({\"type\": \"data\", \"data\": event_data})\n                except json.JSONDecodeError:\n                    # Skip malformed JSON\n                    continue\n\n    return events",
      "language": "python"
    },
    {
      "code": "def decode_chunk(self, chunk: bytes) -> list[dict]:\n    \"\"\"Decode a chunk of SSE data and return parsed events.\"\"\"\n    import json\n\n    try:\n        chunk_str = chunk.decode(\"utf-8\")\n    except UnicodeDecodeError:\n        # Skip malformed chunks\n        return []\n\n    self.buffer += chunk_str\n    events = []\n\n    # Process complete lines\n    while \"\\n\" in self.buffer:\n        line, self.buffer = self.buffer.split(\"\\n\", 1)\n        line = line.rstrip(\"\\r\")  # Handle CRLF\n\n        if line.startswith(\"data: \"):\n            data_str = line[6:].strip()\n            if data_str == \"[DONE]\":\n                events.append({\"type\": \"done\"})\n            elif data_str:\n                try:\n                    event_data = json.loads(data_str)\n                    events.append({\"type\": \"data\", \"data\": event_data})\n                except json.JSONDecodeError:\n                    # Skip malformed JSON\n                    continue\n\n    return events",
      "language": "python"
    },
    {
      "code": "extract_content(event_data: dict) -> str",
      "language": "php"
    },
    {
      "code": "extract_content(event_data: dict) -> str",
      "language": "php"
    },
    {
      "code": "800\n801\n802",
      "language": "unknown"
    },
    {
      "code": "def extract_content(self, event_data: dict) -> str:\n    \"\"\"Extract content from event data.\"\"\"\n    return _extract_content_from_chunk(event_data)",
      "language": "python"
    },
    {
      "code": "def extract_content(self, event_data: dict) -> str:\n    \"\"\"Extract content from event data.\"\"\"\n    return _extract_content_from_chunk(event_data)",
      "language": "python"
    },
    {
      "code": "get_complete_content() -> str",
      "language": "php"
    },
    {
      "code": "get_complete_content() -> str",
      "language": "php"
    },
    {
      "code": "809\n810\n811",
      "language": "unknown"
    },
    {
      "code": "def get_complete_content(self) -> str:\n    \"\"\"Get the complete buffered content.\"\"\"\n    return \"\".join(self.content_buffer)",
      "language": "python"
    },
    {
      "code": "def get_complete_content(self) -> str:\n    \"\"\"Get the complete buffered content.\"\"\"\n    return \"\".join(self.content_buffer)",
      "language": "python"
    },
    {
      "code": "702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730",
      "language": "unknown"
    },
    {
      "code": "class XRequestIdMiddleware:\n    \"\"\"\n    Middleware the set's the X-Request-Id header for each response\n    to a random uuid4 (hex) value if the header isn't already\n    present in the request, otherwise use the provided request id.\n    \"\"\"\n\n    def __init__(self, app: ASGIApp) -> None:\n        self.app = app\n\n    def __call__(self, scope: Scope, receive: Receive, send: Send) -> Awaitable[None]:\n        if scope[\"type\"] not in (\"http\", \"websocket\"):\n            return self.app(scope, receive, send)\n\n        # Extract the request headers.\n        request_headers = Headers(scope=scope)\n\n        async def send_with_request_id(message: Message) -> None:\n            \"\"\"\n            Custom send function to mutate the response headers\n            and append X-Request-Id to it.\n            \"\"\"\n            if message[\"type\"] == \"http.response.start\":\n                response_headers = MutableHeaders(raw=message[\"headers\"])\n                request_id = request_headers.get(\"X-Request-Id\", uuid.uuid4().hex)\n                response_headers.append(\"X-Request-Id\", request_id)\n            await send(message)\n\n        return self.app(scope, receive, send_with_request_id)",
      "language": "python"
    },
    {
      "code": "class XRequestIdMiddleware:\n    \"\"\"\n    Middleware the set's the X-Request-Id header for each response\n    to a random uuid4 (hex) value if the header isn't already\n    present in the request, otherwise use the provided request id.\n    \"\"\"\n\n    def __init__(self, app: ASGIApp) -> None:\n        self.app = app\n\n    def __call__(self, scope: Scope, receive: Receive, send: Send) -> Awaitable[None]:\n        if scope[\"type\"] not in (\"http\", \"websocket\"):\n            return self.app(scope, receive, send)\n\n        # Extract the request headers.\n        request_headers = Headers(scope=scope)\n\n        async def send_with_request_id(message: Message) -> None:\n            \"\"\"\n            Custom send function to mutate the response headers\n            and append X-Request-Id to it.\n            \"\"\"\n            if message[\"type\"] == \"http.response.start\":\n                response_headers = MutableHeaders(raw=message[\"headers\"])\n                request_id = request_headers.get(\"X-Request-Id\", uuid.uuid4().hex)\n                response_headers.append(\"X-Request-Id\", request_id)\n            await send(message)\n\n        return self.app(scope, receive, send_with_request_id)",
      "language": "python"
    },
    {
      "code": "__call__(\n    scope: Scope, receive: Receive, send: Send\n) -> Awaitable[None]",
      "language": "rust"
    },
    {
      "code": "__call__(\n    scope: Scope, receive: Receive, send: Send\n) -> Awaitable[None]",
      "language": "rust"
    },
    {
      "code": "712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730",
      "language": "unknown"
    },
    {
      "code": "def __call__(self, scope: Scope, receive: Receive, send: Send) -> Awaitable[None]:\n    if scope[\"type\"] not in (\"http\", \"websocket\"):\n        return self.app(scope, receive, send)\n\n    # Extract the request headers.\n    request_headers = Headers(scope=scope)\n\n    async def send_with_request_id(message: Message) -> None:\n        \"\"\"\n        Custom send function to mutate the response headers\n        and append X-Request-Id to it.\n        \"\"\"\n        if message[\"type\"] == \"http.response.start\":\n            response_headers = MutableHeaders(raw=message[\"headers\"])\n            request_id = request_headers.get(\"X-Request-Id\", uuid.uuid4().hex)\n            response_headers.append(\"X-Request-Id\", request_id)\n        await send(message)\n\n    return self.app(scope, receive, send_with_request_id)",
      "language": "python"
    },
    {
      "code": "def __call__(self, scope: Scope, receive: Receive, send: Send) -> Awaitable[None]:\n    if scope[\"type\"] not in (\"http\", \"websocket\"):\n        return self.app(scope, receive, send)\n\n    # Extract the request headers.\n    request_headers = Headers(scope=scope)\n\n    async def send_with_request_id(message: Message) -> None:\n        \"\"\"\n        Custom send function to mutate the response headers\n        and append X-Request-Id to it.\n        \"\"\"\n        if message[\"type\"] == \"http.response.start\":\n            response_headers = MutableHeaders(raw=message[\"headers\"])\n            request_id = request_headers.get(\"X-Request-Id\", uuid.uuid4().hex)\n            response_headers.append(\"X-Request-Id\", request_id)\n        await send(message)\n\n    return self.app(scope, receive, send_with_request_id)",
      "language": "python"
    },
    {
      "code": "__init__(app: ASGIApp) -> None",
      "language": "python"
    },
    {
      "code": "__init__(app: ASGIApp) -> None",
      "language": "python"
    },
    {
      "code": "def __init__(self, app: ASGIApp) -> None:\n    self.app = app",
      "language": "python"
    },
    {
      "code": "def __init__(self, app: ASGIApp) -> None:\n    self.app = app",
      "language": "python"
    },
    {
      "code": "_convert_stream_to_sse_events(\n    generator: AsyncGenerator[\n        StreamingResponsesResponse, None\n    ],\n) -> AsyncGenerator[str, None]",
      "language": "rust"
    },
    {
      "code": "_convert_stream_to_sse_events(\n    generator: AsyncGenerator[\n        StreamingResponsesResponse, None\n    ],\n) -> AsyncGenerator[str, None]",
      "language": "rust"
    },
    {
      "code": "314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324",
      "language": "unknown"
    },
    {
      "code": "async def _convert_stream_to_sse_events(\n    generator: AsyncGenerator[StreamingResponsesResponse, None],\n) -> AsyncGenerator[str, None]:\n    \"\"\"Convert the generator to a stream of events in SSE format\"\"\"\n    async for event in generator:\n        event_type = getattr(event, \"type\", \"unknown\")\n        # https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format\n        event_data = (\n            f\"event: {event_type}\\ndata: {event.model_dump_json(indent=None)}\\n\\n\"\n        )\n        yield event_data",
      "language": "python"
    },
    {
      "code": "async def _convert_stream_to_sse_events(\n    generator: AsyncGenerator[StreamingResponsesResponse, None],\n) -> AsyncGenerator[str, None]:\n    \"\"\"Convert the generator to a stream of events in SSE format\"\"\"\n    async for event in generator:\n        event_type = getattr(event, \"type\", \"unknown\")\n        # https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format\n        event_data = (\n            f\"event: {event_type}\\ndata: {event.model_dump_json(indent=None)}\\n\\n\"\n        )\n        yield event_data",
      "language": "python"
    },
    {
      "code": "_extract_content_from_chunk(chunk_data: dict) -> str",
      "language": "php"
    },
    {
      "code": "_extract_content_from_chunk(chunk_data: dict) -> str",
      "language": "php"
    },
    {
      "code": "733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758",
      "language": "unknown"
    },
    {
      "code": "def _extract_content_from_chunk(chunk_data: dict) -> str:\n    \"\"\"Extract content from a streaming response chunk.\"\"\"\n    try:\n        from vllm.entrypoints.openai.protocol import (\n            ChatCompletionStreamResponse,\n            CompletionStreamResponse,\n        )\n\n        # Try using Completion types for type-safe parsing\n        if chunk_data.get(\"object\") == \"chat.completion.chunk\":\n            chat_response = ChatCompletionStreamResponse.model_validate(chunk_data)\n            if chat_response.choices and chat_response.choices[0].delta.content:\n                return chat_response.choices[0].delta.content\n        elif chunk_data.get(\"object\") == \"text_completion\":\n            completion_response = CompletionStreamResponse.model_validate(chunk_data)\n            if completion_response.choices and completion_response.choices[0].text:\n                return completion_response.choices[0].text\n    except pydantic.ValidationError:\n        # Fallback to manual parsing\n        if \"choices\" in chunk_data and chunk_data[\"choices\"]:\n            choice = chunk_data[\"choices\"][0]\n            if \"delta\" in choice and choice[\"delta\"].get(\"content\"):\n                return choice[\"delta\"][\"content\"]\n            elif choice.get(\"text\"):\n                return choice[\"text\"]\n    return \"\"",
      "language": "json"
    },
    {
      "code": "def _extract_content_from_chunk(chunk_data: dict) -> str:\n    \"\"\"Extract content from a streaming response chunk.\"\"\"\n    try:\n        from vllm.entrypoints.openai.protocol import (\n            ChatCompletionStreamResponse,\n            CompletionStreamResponse,\n        )\n\n        # Try using Completion types for type-safe parsing\n        if chunk_data.get(\"object\") == \"chat.completion.chunk\":\n            chat_response = ChatCompletionStreamResponse.model_validate(chunk_data)\n            if chat_response.choices and chat_response.choices[0].delta.content:\n                return chat_response.choices[0].delta.content\n        elif chunk_data.get(\"object\") == \"text_completion\":\n            completion_response = CompletionStreamResponse.model_validate(chunk_data)\n            if completion_response.choices and completion_response.choices[0].text:\n                return completion_response.choices[0].text\n    except pydantic.ValidationError:\n        # Fallback to manual parsing\n        if \"choices\" in chunk_data and chunk_data[\"choices\"]:\n            choice = chunk_data[\"choices\"][0]\n            if \"delta\" in choice and choice[\"delta\"].get(\"content\"):\n                return choice[\"delta\"][\"content\"]\n            elif choice.get(\"text\"):\n                return choice[\"text\"]\n    return \"\"",
      "language": "json"
    },
    {
      "code": "_log_non_streaming_response(response_body: list) -> None",
      "language": "rust"
    },
    {
      "code": "_log_non_streaming_response(response_body: list) -> None",
      "language": "rust"
    },
    {
      "code": "859\n860\n861\n862\n863\n864\n865",
      "language": "unknown"
    },
    {
      "code": "def _log_non_streaming_response(response_body: list) -> None:\n    \"\"\"Log non-streaming response.\"\"\"\n    try:\n        decoded_body = response_body[0].decode()\n        logger.info(\"response_body={%s}\", decoded_body)\n    except UnicodeDecodeError:\n        logger.info(\"response_body={<binary_data>}\")",
      "language": "typescript"
    },
    {
      "code": "def _log_non_streaming_response(response_body: list) -> None:\n    \"\"\"Log non-streaming response.\"\"\"\n    try:\n        decoded_body = response_body[0].decode()\n        logger.info(\"response_body={%s}\", decoded_body)\n    except UnicodeDecodeError:\n        logger.info(\"response_body={<binary_data>}\")",
      "language": "typescript"
    },
    {
      "code": "_log_streaming_response(\n    response, response_body: list\n) -> None",
      "language": "rust"
    },
    {
      "code": "_log_streaming_response(\n    response, response_body: list\n) -> None",
      "language": "rust"
    },
    {
      "code": "814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856",
      "language": "unknown"
    },
    {
      "code": "def _log_streaming_response(response, response_body: list) -> None:\n    \"\"\"Log streaming response with robust SSE parsing.\"\"\"\n    from starlette.concurrency import iterate_in_threadpool\n\n    sse_decoder = SSEDecoder()\n    chunk_count = 0\n\n    def buffered_iterator():\n        nonlocal chunk_count\n\n        for chunk in response_body:\n            chunk_count += 1\n            yield chunk\n\n            # Parse SSE events from chunk\n            events = sse_decoder.decode_chunk(chunk)\n\n            for event in events:\n                if event[\"type\"] == \"data\":\n                    content = sse_decoder.extract_content(event[\"data\"])\n                    sse_decoder.add_content(content)\n                elif event[\"type\"] == \"done\":\n                    # Log complete content when done\n                    full_content = sse_decoder.get_complete_content()\n                    if full_content:\n                        # Truncate if too long\n                        if len(full_content) > 2048:\n                            full_content = full_content[:2048] + \"\"\n                            \"...[truncated]\"\n                        logger.info(\n                            \"response_body={streaming_complete: content=%r, chunks=%d}\",\n                            full_content,\n                            chunk_count,\n                        )\n                    else:\n                        logger.info(\n                            \"response_body={streaming_complete: no_content, chunks=%d}\",\n                            chunk_count,\n                        )\n                    return\n\n    response.body_iterator = iterate_in_threadpool(buffered_iterator())\n    logger.info(\"response_body={streaming_started: chunks=%d}\", len(response_body))",
      "language": "python"
    },
    {
      "code": "def _log_streaming_response(response, response_body: list) -> None:\n    \"\"\"Log streaming response with robust SSE parsing.\"\"\"\n    from starlette.concurrency import iterate_in_threadpool\n\n    sse_decoder = SSEDecoder()\n    chunk_count = 0\n\n    def buffered_iterator():\n        nonlocal chunk_count\n\n        for chunk in response_body:\n            chunk_count += 1\n            yield chunk\n\n            # Parse SSE events from chunk\n            events = sse_decoder.decode_chunk(chunk)\n\n            for event in events:\n                if event[\"type\"] == \"data\":\n                    content = sse_decoder.extract_content(event[\"data\"])\n                    sse_decoder.add_content(content)\n                elif event[\"type\"] == \"done\":\n                    # Log complete content when done\n                    full_content = sse_decoder.get_complete_content()\n                    if full_content:\n                        # Truncate if too long\n                        if len(full_content) > 2048:\n                            full_content = full_content[:2048] + \"\"\n                            \"...[truncated]\"\n                        logger.info(\n                            \"response_body={streaming_complete: content=%r, chunks=%d}\",\n                            full_content,\n                            chunk_count,\n                        )\n                    else:\n                        logger.info(\n                            \"response_body={streaming_complete: no_content, chunks=%d}\",\n                            chunk_count,\n                        )\n                    return\n\n    response.body_iterator = iterate_in_threadpool(buffered_iterator())\n    logger.info(\"response_body={streaming_started: chunks=%d}\", len(response_body))",
      "language": "python"
    },
    {
      "code": "base(request: Request) -> OpenAIServing",
      "language": "php"
    },
    {
      "code": "base(request: Request) -> OpenAIServing",
      "language": "php"
    },
    {
      "code": "236\n237\n238",
      "language": "unknown"
    },
    {
      "code": "def base(request: Request) -> OpenAIServing:\n    # Reuse the existing instance\n    return tokenization(request)",
      "language": "python"
    },
    {
      "code": "def base(request: Request) -> OpenAIServing:\n    # Reuse the existing instance\n    return tokenization(request)",
      "language": "python"
    },
    {
      "code": "build_app(args: Namespace) -> FastAPI",
      "language": "php"
    },
    {
      "code": "build_app(args: Namespace) -> FastAPI",
      "language": "php"
    },
    {
      "code": "868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935\n936\n937\n938\n939\n940\n941\n942\n943\n944\n945\n946\n947\n948\n949\n950\n951\n952\n953\n954\n955\n956\n957\n958\n959\n960\n961\n962\n963\n964\n965\n966\n967\n968\n969\n970\n971\n972\n973\n974\n975\n976\n977\n978\n979\n980\n981\n982\n983\n984\n985\n986\n987\n988\n989",
      "language": "unknown"
    },
    {
      "code": "def build_app(args: Namespace) -> FastAPI:\n    if args.disable_fastapi_docs:\n        app = FastAPI(\n            openapi_url=None, docs_url=None, redoc_url=None, lifespan=lifespan\n        )\n    else:\n        app = FastAPI(lifespan=lifespan)\n    app.state.args = args\n    from vllm.entrypoints.serve import register_vllm_serve_api_routers\n\n    register_vllm_serve_api_routers(app)\n\n    from vllm.entrypoints.sagemaker.routes import register_sagemaker_routes\n\n    register_sagemaker_routes(router)\n    app.include_router(router)\n\n    app.root_path = args.root_path\n\n    from vllm.entrypoints.pooling import register_pooling_api_routers\n\n    register_pooling_api_routers(app)\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n\n    @app.exception_handler(HTTPException)\n    async def http_exception_handler(_: Request, exc: HTTPException):\n        err = ErrorResponse(\n            error=ErrorInfo(\n                message=exc.detail,\n                type=HTTPStatus(exc.status_code).phrase,\n                code=exc.status_code,\n            )\n        )\n        return JSONResponse(err.model_dump(), status_code=exc.status_code)\n\n    @app.exception_handler(RequestValidationError)\n    async def validation_exception_handler(_: Request, exc: RequestValidationError):\n        from vllm.entrypoints.openai.protocol import VLLMValidationError\n\n        param = None\n        for error in exc.errors():\n            if \"ctx\" in error and \"error\" in error[\"ctx\"]:\n                ctx_error = error[\"ctx\"][\"error\"]\n                if isinstance(ctx_error, VLLMValidationError):\n                    param = ctx_error.parameter\n                    break\n\n        exc_str = str(exc)\n        errors_str = str(exc.errors())\n\n        if exc.errors() and errors_str and errors_str != exc_str:\n            message = f\"{exc_str} {errors_str}\"\n        else:\n            message = exc_str\n\n        err = ErrorResponse(\n            error=ErrorInfo(\n                message=message,\n                type=HTTPStatus.BAD_REQUEST.phrase,\n                code=HTTPStatus.BAD_REQUEST,\n                param=param,\n            )\n        )\n        return JSONResponse(err.model_dump(), status_code=HTTPStatus.BAD_REQUEST)\n\n    # Ensure --api-key option from CLI takes precedence over VLLM_API_KEY\n    if tokens := [key for key in (args.api_key or [envs.VLLM_API_KEY]) if key]:\n        app.add_middleware(AuthenticationMiddleware, tokens=tokens)\n\n    if args.enable_request_id_headers:\n        app.add_middleware(XRequestIdMiddleware)\n\n    # Add scaling middleware to check for scaling state\n    app.add_middleware(ScalingMiddleware)\n\n    if envs.VLLM_DEBUG_LOG_API_SERVER_RESPONSE:\n        logger.warning(\n            \"CAUTION: Enabling log response in the API Server. \"\n            \"This can include sensitive information and should be \"\n            \"avoided in production.\"\n        )\n\n        @app.middleware(\"http\")\n        async def log_response(request: Request, call_next):\n            response = await call_next(request)\n            response_body = [section async for section in response.body_iterator]\n            response.body_iterator = iterate_in_threadpool(iter(response_body))\n            # Check if this is a streaming response by looking at content-type\n            content_type = response.headers.get(\"content-type\", \"\")\n            is_streaming = content_type == \"text/event-stream; charset=utf-8\"\n\n            # Log response body based on type\n            if not response_body:\n                logger.info(\"response_body={<empty>}\")\n            elif is_streaming:\n                _log_streaming_response(response, response_body)\n            else:\n                _log_non_streaming_response(response_body)\n            return response\n\n    for middleware in args.middleware:\n        module_path, object_name = middleware.rsplit(\".\", 1)\n        imported = getattr(importlib.import_module(module_path), object_name)\n        if inspect.isclass(imported):\n            app.add_middleware(imported)  # type: ignore[arg-type]\n        elif inspect.iscoroutinefunction(imported):\n            app.middleware(\"http\")(imported)\n        else:\n            raise ValueError(\n                f\"Invalid middleware {middleware}. Must be a function or a class.\"\n            )\n\n    app = sagemaker_standards.bootstrap(app)\n\n    return app",
      "language": "python"
    },
    {
      "code": "def build_app(args: Namespace) -> FastAPI:\n    if args.disable_fastapi_docs:\n        app = FastAPI(\n            openapi_url=None, docs_url=None, redoc_url=None, lifespan=lifespan\n        )\n    else:\n        app = FastAPI(lifespan=lifespan)\n    app.state.args = args\n    from vllm.entrypoints.serve import register_vllm_serve_api_routers\n\n    register_vllm_serve_api_routers(app)\n\n    from vllm.entrypoints.sagemaker.routes import register_sagemaker_routes\n\n    register_sagemaker_routes(router)\n    app.include_router(router)\n\n    app.root_path = args.root_path\n\n    from vllm.entrypoints.pooling import register_pooling_api_routers\n\n    register_pooling_api_routers(app)\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n\n    @app.exception_handler(HTTPException)\n    async def http_exception_handler(_: Request, exc: HTTPException):\n        err = ErrorResponse(\n            error=ErrorInfo(\n                message=exc.detail,\n                type=HTTPStatus(exc.status_code).phrase,\n                code=exc.status_code,\n            )\n        )\n        return JSONResponse(err.model_dump(), status_code=exc.status_code)\n\n    @app.exception_handler(RequestValidationError)\n    async def validation_exception_handler(_: Request, exc: RequestValidationError):\n        from vllm.entrypoints.openai.protocol import VLLMValidationError\n\n        param = None\n        for error in exc.errors():\n            if \"ctx\" in error and \"error\" in error[\"ctx\"]:\n                ctx_error = error[\"ctx\"][\"error\"]\n                if isinstance(ctx_error, VLLMValidationError):\n                    param = ctx_error.parameter\n                    break\n\n        exc_str = str(exc)\n        errors_str = str(exc.errors())\n\n        if exc.errors() and errors_str and errors_str != exc_str:\n            message = f\"{exc_str} {errors_str}\"\n        else:\n            message = exc_str\n\n        err = ErrorResponse(\n            error=ErrorInfo(\n                message=message,\n                type=HTTPStatus.BAD_REQUEST.phrase,\n                code=HTTPStatus.BAD_REQUEST,\n                param=param,\n            )\n        )\n        return JSONResponse(err.model_dump(), status_code=HTTPStatus.BAD_REQUEST)\n\n    # Ensure --api-key option from CLI takes precedence over VLLM_API_KEY\n    if tokens := [key for key in (args.api_key or [envs.VLLM_API_KEY]) if key]:\n        app.add_middleware(AuthenticationMiddleware, tokens=tokens)\n\n    if args.enable_request_id_headers:\n        app.add_middleware(XRequestIdMiddleware)\n\n    # Add scaling middleware to check for scaling state\n    app.add_middleware(ScalingMiddleware)\n\n    if envs.VLLM_DEBUG_LOG_API_SERVER_RESPONSE:\n        logger.warning(\n            \"CAUTION: Enabling log response in the API Server. \"\n            \"This can include sensitive information and should be \"\n            \"avoided in production.\"\n        )\n\n        @app.middleware(\"http\")\n        async def log_response(request: Request, call_next):\n            response = await call_next(request)\n            response_body = [section async for section in response.body_iterator]\n            response.body_iterator = iterate_in_threadpool(iter(response_body))\n            # Check if this is a streaming response by looking at content-type\n            content_type = response.headers.get(\"content-type\", \"\")\n            is_streaming = content_type == \"text/event-stream; charset=utf-8\"\n\n            # Log response body based on type\n            if not response_body:\n                logger.info(\"response_body={<empty>}\")\n            elif is_streaming:\n                _log_streaming_response(response, response_body)\n            else:\n                _log_non_streaming_response(response_body)\n            return response\n\n    for middleware in args.middleware:\n        module_path, object_name = middleware.rsplit(\".\", 1)\n        imported = getattr(importlib.import_module(module_path), object_name)\n        if inspect.isclass(imported):\n            app.add_middleware(imported)  # type: ignore[arg-type]\n        elif inspect.iscoroutinefunction(imported):\n            app.middleware(\"http\")(imported)\n        else:\n            raise ValueError(\n                f\"Invalid middleware {middleware}. Must be a function or a class.\"\n            )\n\n    app = sagemaker_standards.bootstrap(app)\n\n    return app",
      "language": "python"
    },
    {
      "code": "build_async_engine_client(\n    args: Namespace,\n    *,\n    usage_context: UsageContext = OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool | None = None,\n    client_config: dict[str, Any] | None = None,\n) -> AsyncIterator[EngineClient]",
      "language": "typescript"
    },
    {
      "code": "build_async_engine_client(\n    args: Namespace,\n    *,\n    usage_context: UsageContext = OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool | None = None,\n    client_config: dict[str, Any] | None = None,\n) -> AsyncIterator[EngineClient]",
      "language": "typescript"
    },
    {
      "code": "144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177",
      "language": "unknown"
    },
    {
      "code": "@asynccontextmanager\nasync def build_async_engine_client(\n    args: Namespace,\n    *,\n    usage_context: UsageContext = UsageContext.OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool | None = None,\n    client_config: dict[str, Any] | None = None,\n) -> AsyncIterator[EngineClient]:\n    if os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\") == \"forkserver\":\n        # The executor is expected to be mp.\n        # Pre-import heavy modules in the forkserver process\n        logger.debug(\"Setup forkserver with pre-imports\")\n        multiprocessing.set_start_method(\"forkserver\")\n        multiprocessing.set_forkserver_preload([\"vllm.v1.engine.async_llm\"])\n        forkserver.ensure_running()\n        logger.debug(\"Forkserver setup complete!\")\n\n    # Context manager to handle engine_client lifecycle\n    # Ensures everything is shutdown and cleaned up on error/exit\n    engine_args = AsyncEngineArgs.from_cli_args(args)\n    if client_config:\n        engine_args._api_process_count = client_config.get(\"client_count\", 1)\n        engine_args._api_process_rank = client_config.get(\"client_index\", 0)\n\n    if disable_frontend_multiprocessing is None:\n        disable_frontend_multiprocessing = bool(args.disable_frontend_multiprocessing)\n\n    async with build_async_engine_client_from_engine_args(\n        engine_args,\n        usage_context=usage_context,\n        disable_frontend_multiprocessing=disable_frontend_multiprocessing,\n        client_config=client_config,\n    ) as engine:\n        yield engine",
      "language": "python"
    },
    {
      "code": "@asynccontextmanager\nasync def build_async_engine_client(\n    args: Namespace,\n    *,\n    usage_context: UsageContext = UsageContext.OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool | None = None,\n    client_config: dict[str, Any] | None = None,\n) -> AsyncIterator[EngineClient]:\n    if os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\") == \"forkserver\":\n        # The executor is expected to be mp.\n        # Pre-import heavy modules in the forkserver process\n        logger.debug(\"Setup forkserver with pre-imports\")\n        multiprocessing.set_start_method(\"forkserver\")\n        multiprocessing.set_forkserver_preload([\"vllm.v1.engine.async_llm\"])\n        forkserver.ensure_running()\n        logger.debug(\"Forkserver setup complete!\")\n\n    # Context manager to handle engine_client lifecycle\n    # Ensures everything is shutdown and cleaned up on error/exit\n    engine_args = AsyncEngineArgs.from_cli_args(args)\n    if client_config:\n        engine_args._api_process_count = client_config.get(\"client_count\", 1)\n        engine_args._api_process_rank = client_config.get(\"client_index\", 0)\n\n    if disable_frontend_multiprocessing is None:\n        disable_frontend_multiprocessing = bool(args.disable_frontend_multiprocessing)\n\n    async with build_async_engine_client_from_engine_args(\n        engine_args,\n        usage_context=usage_context,\n        disable_frontend_multiprocessing=disable_frontend_multiprocessing,\n        client_config=client_config,\n    ) as engine:\n        yield engine",
      "language": "python"
    },
    {
      "code": "build_async_engine_client_from_engine_args(\n    engine_args: AsyncEngineArgs,\n    *,\n    usage_context: UsageContext = OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool = False,\n    client_config: dict[str, Any] | None = None,\n) -> AsyncIterator[EngineClient]",
      "language": "typescript"
    },
    {
      "code": "build_async_engine_client_from_engine_args(\n    engine_args: AsyncEngineArgs,\n    *,\n    usage_context: UsageContext = OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool = False,\n    client_config: dict[str, Any] | None = None,\n) -> AsyncIterator[EngineClient]",
      "language": "typescript"
    },
    {
      "code": "180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230",
      "language": "unknown"
    },
    {
      "code": "@asynccontextmanager\nasync def build_async_engine_client_from_engine_args(\n    engine_args: AsyncEngineArgs,\n    *,\n    usage_context: UsageContext = UsageContext.OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool = False,\n    client_config: dict[str, Any] | None = None,\n) -> AsyncIterator[EngineClient]:\n    \"\"\"\n    Create EngineClient, either:\n        - in-process using the AsyncLLMEngine Directly\n        - multiprocess using AsyncLLMEngine RPC\n\n    Returns the Client or None if the creation failed.\n    \"\"\"\n\n    # Create the EngineConfig (determines if we can use V1).\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\n    if disable_frontend_multiprocessing:\n        logger.warning(\"V1 is enabled, but got --disable-frontend-multiprocessing.\")\n\n    from vllm.v1.engine.async_llm import AsyncLLM\n\n    async_llm: AsyncLLM | None = None\n\n    # Don't mutate the input client_config\n    client_config = dict(client_config) if client_config else {}\n    client_count = client_config.pop(\"client_count\", 1)\n    client_index = client_config.pop(\"client_index\", 0)\n\n    try:\n        async_llm = AsyncLLM.from_vllm_config(\n            vllm_config=vllm_config,\n            usage_context=usage_context,\n            enable_log_requests=engine_args.enable_log_requests,\n            aggregate_engine_logging=engine_args.aggregate_engine_logging,\n            disable_log_stats=engine_args.disable_log_stats,\n            client_addresses=client_config,\n            client_count=client_count,\n            client_index=client_index,\n        )\n\n        # Don't keep the dummy data in memory\n        assert async_llm is not None\n        await async_llm.reset_mm_cache()\n\n        yield async_llm\n    finally:\n        if async_llm:\n            async_llm.shutdown()",
      "language": "python"
    },
    {
      "code": "@asynccontextmanager\nasync def build_async_engine_client_from_engine_args(\n    engine_args: AsyncEngineArgs,\n    *,\n    usage_context: UsageContext = UsageContext.OPENAI_API_SERVER,\n    disable_frontend_multiprocessing: bool = False,\n    client_config: dict[str, Any] | None = None,\n) -> AsyncIterator[EngineClient]:\n    \"\"\"\n    Create EngineClient, either:\n        - in-process using the AsyncLLMEngine Directly\n        - multiprocess using AsyncLLMEngine RPC\n\n    Returns the Client or None if the creation failed.\n    \"\"\"\n\n    # Create the EngineConfig (determines if we can use V1).\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n\n    if disable_frontend_multiprocessing:\n        logger.warning(\"V1 is enabled, but got --disable-frontend-multiprocessing.\")\n\n    from vllm.v1.engine.async_llm import AsyncLLM\n\n    async_llm: AsyncLLM | None = None\n\n    # Don't mutate the input client_config\n    client_config = dict(client_config) if client_config else {}\n    client_count = client_config.pop(\"client_count\", 1)\n    client_index = client_config.pop(\"client_index\", 0)\n\n    try:\n        async_llm = AsyncLLM.from_vllm_config(\n            vllm_config=vllm_config,\n            usage_context=usage_context,\n            enable_log_requests=engine_args.enable_log_requests,\n            aggregate_engine_logging=engine_args.aggregate_engine_logging,\n            disable_log_stats=engine_args.disable_log_stats,\n            client_addresses=client_config,\n            client_count=client_count,\n            client_index=client_index,\n        )\n\n        # Don't keep the dummy data in memory\n        assert async_llm is not None\n        await async_llm.reset_mm_cache()\n\n        yield async_llm\n    finally:\n        if async_llm:\n            async_llm.shutdown()",
      "language": "python"
    },
    {
      "code": "cancel_responses(response_id: str, raw_request: Request)",
      "language": "unknown"
    },
    {
      "code": "cancel_responses(response_id: str, raw_request: Request)",
      "language": "unknown"
    },
    {
      "code": "398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417",
      "language": "unknown"
    },
    {
      "code": "@router.post(\"/v1/responses/{response_id}/cancel\")\nasync def cancel_responses(response_id: str, raw_request: Request):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\"\n        )\n\n    try:\n        response = await handler.cancel_responses(response_id)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(response, ErrorResponse):\n        return JSONResponse(\n            content=response.model_dump(), status_code=response.error.code\n        )\n    return JSONResponse(content=response.model_dump())",
      "language": "python"
    },
    {
      "code": "@router.post(\"/v1/responses/{response_id}/cancel\")\nasync def cancel_responses(response_id: str, raw_request: Request):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\"\n        )\n\n    try:\n        response = await handler.cancel_responses(response_id)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(response, ErrorResponse):\n        return JSONResponse(\n            content=response.model_dump(), status_code=response.error.code\n        )\n    return JSONResponse(content=response.model_dump())",
      "language": "python"
    },
    {
      "code": "chat(request: Request) -> OpenAIServingChat | None",
      "language": "rust"
    },
    {
      "code": "chat(request: Request) -> OpenAIServingChat | None",
      "language": "rust"
    },
    {
      "code": "def chat(request: Request) -> OpenAIServingChat | None:\n    return request.app.state.openai_serving_chat",
      "language": "python"
    },
    {
      "code": "def chat(request: Request) -> OpenAIServingChat | None:\n    return request.app.state.openai_serving_chat",
      "language": "python"
    },
    {
      "code": "completion(\n    request: Request,\n) -> OpenAIServingCompletion | None",
      "language": "rust"
    },
    {
      "code": "completion(\n    request: Request,\n) -> OpenAIServingCompletion | None",
      "language": "rust"
    },
    {
      "code": "def completion(request: Request) -> OpenAIServingCompletion | None:\n    return request.app.state.openai_serving_completion",
      "language": "python"
    },
    {
      "code": "def completion(request: Request) -> OpenAIServingCompletion | None:\n    return request.app.state.openai_serving_completion",
      "language": "python"
    },
    {
      "code": "create_chat_completion(\n    request: ChatCompletionRequest, raw_request: Request\n)",
      "language": "yaml"
    },
    {
      "code": "create_chat_completion(\n    request: ChatCompletionRequest, raw_request: Request\n)",
      "language": "yaml"
    },
    {
      "code": "476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514",
      "language": "unknown"
    },
    {
      "code": "@router.post(\n    \"/v1/chat/completions\",\n    dependencies=[Depends(validate_json_request)],\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.NOT_FOUND.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_chat_completion(request: ChatCompletionRequest, raw_request: Request):\n    metrics_header_format = raw_request.headers.get(\n        ENDPOINT_LOAD_METRICS_FORMAT_HEADER_LABEL, \"\"\n    )\n    handler = chat(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Chat Completions API\"\n        )\n    try:\n        generator = await handler.create_chat_completion(request, raw_request)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n\n    elif isinstance(generator, ChatCompletionResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            headers=metrics_header(metrics_header_format),\n        )\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "@router.post(\n    \"/v1/chat/completions\",\n    dependencies=[Depends(validate_json_request)],\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.NOT_FOUND.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_chat_completion(request: ChatCompletionRequest, raw_request: Request):\n    metrics_header_format = raw_request.headers.get(\n        ENDPOINT_LOAD_METRICS_FORMAT_HEADER_LABEL, \"\"\n    )\n    handler = chat(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Chat Completions API\"\n        )\n    try:\n        generator = await handler.create_chat_completion(request, raw_request)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n\n    elif isinstance(generator, ChatCompletionResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            headers=metrics_header(metrics_header_format),\n        )\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "create_completion(\n    request: CompletionRequest, raw_request: Request\n)",
      "language": "yaml"
    },
    {
      "code": "create_completion(\n    request: CompletionRequest, raw_request: Request\n)",
      "language": "yaml"
    },
    {
      "code": "517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560",
      "language": "unknown"
    },
    {
      "code": "@router.post(\n    \"/v1/completions\",\n    dependencies=[Depends(validate_json_request)],\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.NOT_FOUND.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_completion(request: CompletionRequest, raw_request: Request):\n    metrics_header_format = raw_request.headers.get(\n        ENDPOINT_LOAD_METRICS_FORMAT_HEADER_LABEL, \"\"\n    )\n    handler = completion(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Completions API\"\n        )\n\n    try:\n        generator = await handler.create_completion(request, raw_request)\n    except OverflowError as e:\n        raise HTTPException(\n            status_code=HTTPStatus.BAD_REQUEST.value, detail=str(e)\n        ) from e\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n    elif isinstance(generator, CompletionResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            headers=metrics_header(metrics_header_format),\n        )\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "@router.post(\n    \"/v1/completions\",\n    dependencies=[Depends(validate_json_request)],\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.NOT_FOUND.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_completion(request: CompletionRequest, raw_request: Request):\n    metrics_header_format = raw_request.headers.get(\n        ENDPOINT_LOAD_METRICS_FORMAT_HEADER_LABEL, \"\"\n    )\n    handler = completion(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Completions API\"\n        )\n\n    try:\n        generator = await handler.create_completion(request, raw_request)\n    except OverflowError as e:\n        raise HTTPException(\n            status_code=HTTPStatus.BAD_REQUEST.value, detail=str(e)\n        ) from e\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n    elif isinstance(generator, CompletionResponse):\n        return JSONResponse(\n            content=generator.model_dump(),\n            headers=metrics_header(metrics_header_format),\n        )\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "create_messages(\n    request: AnthropicMessagesRequest, raw_request: Request\n)",
      "language": "yaml"
    },
    {
      "code": "create_messages(\n    request: AnthropicMessagesRequest, raw_request: Request\n)",
      "language": "yaml"
    },
    {
      "code": "420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473",
      "language": "unknown"
    },
    {
      "code": "@router.post(\n    \"/v1/messages\",\n    dependencies=[Depends(validate_json_request)],\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": AnthropicErrorResponse},\n        HTTPStatus.NOT_FOUND.value: {\"model\": AnthropicErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": AnthropicErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_messages(request: AnthropicMessagesRequest, raw_request: Request):\n    def translate_error_response(response: ErrorResponse) -> JSONResponse:\n        anthropic_error = AnthropicErrorResponse(\n            error=AnthropicError(\n                type=response.error.type,\n                message=response.error.message,\n            )\n        )\n        return JSONResponse(\n            status_code=response.error.code, content=anthropic_error.model_dump()\n        )\n\n    handler = messages(raw_request)\n    if handler is None:\n        error = base(raw_request).create_error_response(\n            message=\"The model does not support Messages API\"\n        )\n        return translate_error_response(error)\n\n    try:\n        generator = await handler.create_messages(request, raw_request)\n    except Exception as e:\n        logger.exception(\"Error in create_messages: %s\", e)\n        return JSONResponse(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value,\n            content=AnthropicErrorResponse(\n                error=AnthropicError(\n                    type=\"internal_error\",\n                    message=str(e),\n                )\n            ).model_dump(),\n        )\n\n    if isinstance(generator, ErrorResponse):\n        return translate_error_response(generator)\n\n    elif isinstance(generator, AnthropicMessagesResponse):\n        resp = generator.model_dump(exclude_none=True)\n        logger.debug(\"Anthropic Messages Response: %s\", resp)\n        return JSONResponse(content=resp)\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "@router.post(\n    \"/v1/messages\",\n    dependencies=[Depends(validate_json_request)],\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": AnthropicErrorResponse},\n        HTTPStatus.NOT_FOUND.value: {\"model\": AnthropicErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": AnthropicErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_messages(request: AnthropicMessagesRequest, raw_request: Request):\n    def translate_error_response(response: ErrorResponse) -> JSONResponse:\n        anthropic_error = AnthropicErrorResponse(\n            error=AnthropicError(\n                type=response.error.type,\n                message=response.error.message,\n            )\n        )\n        return JSONResponse(\n            status_code=response.error.code, content=anthropic_error.model_dump()\n        )\n\n    handler = messages(raw_request)\n    if handler is None:\n        error = base(raw_request).create_error_response(\n            message=\"The model does not support Messages API\"\n        )\n        return translate_error_response(error)\n\n    try:\n        generator = await handler.create_messages(request, raw_request)\n    except Exception as e:\n        logger.exception(\"Error in create_messages: %s\", e)\n        return JSONResponse(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value,\n            content=AnthropicErrorResponse(\n                error=AnthropicError(\n                    type=\"internal_error\",\n                    message=str(e),\n                )\n            ).model_dump(),\n        )\n\n    if isinstance(generator, ErrorResponse):\n        return translate_error_response(generator)\n\n    elif isinstance(generator, AnthropicMessagesResponse):\n        resp = generator.model_dump(exclude_none=True)\n        logger.debug(\"Anthropic Messages Response: %s\", resp)\n        return JSONResponse(content=resp)\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "create_responses(\n    request: ResponsesRequest, raw_request: Request\n)",
      "language": "yaml"
    },
    {
      "code": "create_responses(\n    request: ResponsesRequest, raw_request: Request\n)",
      "language": "yaml"
    },
    {
      "code": "327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360",
      "language": "unknown"
    },
    {
      "code": "@router.post(\n    \"/v1/responses\",\n    dependencies=[Depends(validate_json_request)],\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.NOT_FOUND.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\nasync def create_responses(request: ResponsesRequest, raw_request: Request):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\"\n        )\n    try:\n        generator = await handler.create_responses(request, raw_request)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n    elif isinstance(generator, ResponsesResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(\n        content=_convert_stream_to_sse_events(generator), media_type=\"text/event-stream\"\n    )",
      "language": "python"
    },
    {
      "code": "@router.post(\n    \"/v1/responses\",\n    dependencies=[Depends(validate_json_request)],\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.NOT_FOUND.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\nasync def create_responses(request: ResponsesRequest, raw_request: Request):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\"\n        )\n    try:\n        generator = await handler.create_responses(request, raw_request)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n    elif isinstance(generator, ResponsesResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(\n        content=_convert_stream_to_sse_events(generator), media_type=\"text/event-stream\"\n    )",
      "language": "python"
    },
    {
      "code": "create_server_socket(addr: tuple[str, int]) -> socket",
      "language": "php"
    },
    {
      "code": "create_server_socket(addr: tuple[str, int]) -> socket",
      "language": "php"
    },
    {
      "code": "1233\n1234\n1235\n1236\n1237\n1238\n1239\n1240\n1241\n1242\n1243",
      "language": "unknown"
    },
    {
      "code": "def create_server_socket(addr: tuple[str, int]) -> socket.socket:\n    family = socket.AF_INET\n    if is_valid_ipv6_address(addr[0]):\n        family = socket.AF_INET6\n\n    sock = socket.socket(family=family, type=socket.SOCK_STREAM)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n    sock.bind(addr)\n\n    return sock",
      "language": "python"
    },
    {
      "code": "def create_server_socket(addr: tuple[str, int]) -> socket.socket:\n    family = socket.AF_INET\n    if is_valid_ipv6_address(addr[0]):\n        family = socket.AF_INET6\n\n    sock = socket.socket(family=family, type=socket.SOCK_STREAM)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n    sock.bind(addr)\n\n    return sock",
      "language": "python"
    },
    {
      "code": "create_server_unix_socket(path: str) -> socket",
      "language": "php"
    },
    {
      "code": "create_server_unix_socket(path: str) -> socket",
      "language": "php"
    },
    {
      "code": "1246\n1247\n1248\n1249",
      "language": "unknown"
    },
    {
      "code": "def create_server_unix_socket(path: str) -> socket.socket:\n    sock = socket.socket(family=socket.AF_UNIX, type=socket.SOCK_STREAM)\n    sock.bind(path)\n    return sock",
      "language": "python"
    },
    {
      "code": "def create_server_unix_socket(path: str) -> socket.socket:\n    sock = socket.socket(family=socket.AF_UNIX, type=socket.SOCK_STREAM)\n    sock.bind(path)\n    return sock",
      "language": "python"
    },
    {
      "code": "create_transcriptions(\n    raw_request: Request,\n    request: Annotated[TranscriptionRequest, Form()],\n)",
      "language": "yaml"
    },
    {
      "code": "create_transcriptions(\n    raw_request: Request,\n    request: Annotated[TranscriptionRequest, Form()],\n)",
      "language": "yaml"
    },
    {
      "code": "563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599",
      "language": "unknown"
    },
    {
      "code": "@router.post(\n    \"/v1/audio/transcriptions\",\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.UNPROCESSABLE_ENTITY.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_transcriptions(\n    raw_request: Request, request: Annotated[TranscriptionRequest, Form()]\n):\n    handler = transcription(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Transcriptions API\"\n        )\n\n    audio_data = await request.file.read()\n    try:\n        generator = await handler.create_transcription(audio_data, request, raw_request)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n\n    elif isinstance(generator, TranscriptionResponseVariant):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "@router.post(\n    \"/v1/audio/transcriptions\",\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.UNPROCESSABLE_ENTITY.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_transcriptions(\n    raw_request: Request, request: Annotated[TranscriptionRequest, Form()]\n):\n    handler = transcription(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Transcriptions API\"\n        )\n\n    audio_data = await request.file.read()\n    try:\n        generator = await handler.create_transcription(audio_data, request, raw_request)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n\n    elif isinstance(generator, TranscriptionResponseVariant):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "create_translations(\n    request: Annotated[TranslationRequest, Form()],\n    raw_request: Request,\n)",
      "language": "yaml"
    },
    {
      "code": "create_translations(\n    request: Annotated[TranslationRequest, Form()],\n    raw_request: Request,\n)",
      "language": "yaml"
    },
    {
      "code": "602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638",
      "language": "unknown"
    },
    {
      "code": "@router.post(\n    \"/v1/audio/translations\",\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.UNPROCESSABLE_ENTITY.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_translations(\n    request: Annotated[TranslationRequest, Form()], raw_request: Request\n):\n    handler = translation(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Translations API\"\n        )\n\n    audio_data = await request.file.read()\n    try:\n        generator = await handler.create_translation(audio_data, request, raw_request)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n\n    elif isinstance(generator, TranslationResponseVariant):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "@router.post(\n    \"/v1/audio/translations\",\n    responses={\n        HTTPStatus.OK.value: {\"content\": {\"text/event-stream\": {}}},\n        HTTPStatus.BAD_REQUEST.value: {\"model\": ErrorResponse},\n        HTTPStatus.UNPROCESSABLE_ENTITY.value: {\"model\": ErrorResponse},\n        HTTPStatus.INTERNAL_SERVER_ERROR.value: {\"model\": ErrorResponse},\n    },\n)\n@with_cancellation\n@load_aware_call\nasync def create_translations(\n    request: Annotated[TranslationRequest, Form()], raw_request: Request\n):\n    handler = translation(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Translations API\"\n        )\n\n    audio_data = await request.file.read()\n    try:\n        generator = await handler.create_translation(audio_data, request, raw_request)\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(\n            content=generator.model_dump(), status_code=generator.error.code\n        )\n\n    elif isinstance(generator, TranslationResponseVariant):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")",
      "language": "python"
    },
    {
      "code": "engine_client(request: Request) -> EngineClient",
      "language": "php"
    },
    {
      "code": "engine_client(request: Request) -> EngineClient",
      "language": "php"
    },
    {
      "code": "def engine_client(request: Request) -> EngineClient:\n    return request.app.state.engine_client",
      "language": "python"
    },
    {
      "code": "def engine_client(request: Request) -> EngineClient:\n    return request.app.state.engine_client",
      "language": "python"
    },
    {
      "code": "generate_tokens(request: Request) -> ServingTokens | None",
      "language": "rust"
    },
    {
      "code": "generate_tokens(request: Request) -> ServingTokens | None",
      "language": "rust"
    },
    {
      "code": "def generate_tokens(request: Request) -> ServingTokens | None:\n    return request.app.state.serving_tokens",
      "language": "python"
    },
    {
      "code": "def generate_tokens(request: Request) -> ServingTokens | None:\n    return request.app.state.serving_tokens",
      "language": "python"
    },
    {
      "code": "get_server_load_metrics(request: Request)",
      "language": "unknown"
    },
    {
      "code": "get_server_load_metrics(request: Request)",
      "language": "unknown"
    },
    {
      "code": "281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297",
      "language": "unknown"
    },
    {
      "code": "@router.get(\"/load\")\nasync def get_server_load_metrics(request: Request):\n    # This endpoint returns the current server load metrics.\n    # It tracks requests utilizing the GPU from the following routes:\n    # - /v1/chat/completions\n    # - /v1/completions\n    # - /v1/audio/transcriptions\n    # - /v1/audio/translations\n    # - /v1/embeddings\n    # - /pooling\n    # - /classify\n    # - /score\n    # - /v1/score\n    # - /rerank\n    # - /v1/rerank\n    # - /v2/rerank\n    return JSONResponse(content={\"server_load\": request.app.state.server_load_metrics})",
      "language": "python"
    },
    {
      "code": "@router.get(\"/load\")\nasync def get_server_load_metrics(request: Request):\n    # This endpoint returns the current server load metrics.\n    # It tracks requests utilizing the GPU from the following routes:\n    # - /v1/chat/completions\n    # - /v1/completions\n    # - /v1/audio/transcriptions\n    # - /v1/audio/translations\n    # - /v1/embeddings\n    # - /pooling\n    # - /classify\n    # - /score\n    # - /v1/score\n    # - /rerank\n    # - /v1/rerank\n    # - /v2/rerank\n    return JSONResponse(content={\"server_load\": request.app.state.server_load_metrics})",
      "language": "python"
    },
    {
      "code": "init_app_state(\n    engine_client: EngineClient,\n    state: State,\n    args: Namespace,\n) -> None",
      "language": "rust"
    },
    {
      "code": "init_app_state(\n    engine_client: EngineClient,\n    state: State,\n    args: Namespace,\n) -> None",
      "language": "rust"
    },
    {
      "code": "992\n 993\n 994\n 995\n 996\n 997\n 998\n 999\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\n1200\n1201\n1202\n1203\n1204\n1205\n1206\n1207\n1208\n1209\n1210\n1211\n1212\n1213\n1214\n1215\n1216\n1217\n1218\n1219\n1220\n1221\n1222\n1223\n1224\n1225\n1226\n1227\n1228\n1229\n1230",
      "language": "unknown"
    },
    {
      "code": "async def init_app_state(\n    engine_client: EngineClient,\n    state: State,\n    args: Namespace,\n) -> None:\n    vllm_config = engine_client.vllm_config\n\n    if args.served_model_name is not None:\n        served_model_names = args.served_model_name\n    else:\n        served_model_names = [args.model]\n\n    if args.enable_log_requests:\n        request_logger = RequestLogger(max_log_len=args.max_log_len)\n    else:\n        request_logger = None\n\n    base_model_paths = [\n        BaseModelPath(name=name, model_path=args.model) for name in served_model_names\n    ]\n\n    state.engine_client = engine_client\n    state.log_stats = not args.disable_log_stats\n    state.vllm_config = vllm_config\n    state.args = args\n    supported_tasks = await engine_client.get_supported_tasks()\n    logger.info(\"Supported tasks: %s\", supported_tasks)\n\n    resolved_chat_template = await process_chat_template(\n        args.chat_template, engine_client, vllm_config.model_config\n    )\n\n    if args.tool_server == \"demo\":\n        tool_server: ToolServer | None = DemoToolServer()\n        assert isinstance(tool_server, DemoToolServer)\n        await tool_server.init_and_validate()\n    elif args.tool_server:\n        tool_server = MCPToolServer()\n        await tool_server.add_tool_server(args.tool_server)\n    else:\n        tool_server = None\n\n    # Merge default_mm_loras into the static lora_modules\n    default_mm_loras = (\n        vllm_config.lora_config.default_mm_loras\n        if vllm_config.lora_config is not None\n        else {}\n    )\n\n    default_mm_loras = (\n        vllm_config.lora_config.default_mm_loras\n        if vllm_config.lora_config is not None\n        else {}\n    )\n    lora_modules = process_lora_modules(args.lora_modules, default_mm_loras)\n\n    state.openai_serving_models = OpenAIServingModels(\n        engine_client=engine_client,\n        base_model_paths=base_model_paths,\n        lora_modules=lora_modules,\n    )\n    await state.openai_serving_models.init_static_loras()\n    state.openai_serving_responses = (\n        OpenAIServingResponses(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            enable_auto_tools=args.enable_auto_tool_choice,\n            tool_parser=args.tool_call_parser,\n            tool_server=tool_server,\n            reasoning_parser=args.structured_outputs_config.reasoning_parser,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_force_include_usage=args.enable_force_include_usage,\n            enable_log_outputs=args.enable_log_outputs,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n    state.openai_serving_chat = (\n        OpenAIServingChat(\n            engine_client,\n            state.openai_serving_models,\n            args.response_role,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            trust_request_chat_template=args.trust_request_chat_template,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            enable_auto_tools=args.enable_auto_tool_choice,\n            exclude_tools_when_tool_choice_none=args.exclude_tools_when_tool_choice_none,\n            tool_parser=args.tool_call_parser,\n            reasoning_parser=args.structured_outputs_config.reasoning_parser,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_force_include_usage=args.enable_force_include_usage,\n            enable_log_outputs=args.enable_log_outputs,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n    # Warm up chat template processing to avoid first-request latency\n    if state.openai_serving_chat is not None:\n        await state.openai_serving_chat.warmup()\n    state.openai_serving_completion = (\n        OpenAIServingCompletion(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_force_include_usage=args.enable_force_include_usage,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n    state.openai_serving_pooling = (\n        (\n            OpenAIServingPooling(\n                engine_client,\n                state.openai_serving_models,\n                supported_tasks=supported_tasks,\n                request_logger=request_logger,\n                chat_template=resolved_chat_template,\n                chat_template_content_format=args.chat_template_content_format,\n                trust_request_chat_template=args.trust_request_chat_template,\n                log_error_stack=args.log_error_stack,\n            )\n        )\n        if any(task in POOLING_TASKS for task in supported_tasks)\n        else None\n    )\n    state.openai_serving_embedding = (\n        OpenAIServingEmbedding(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            trust_request_chat_template=args.trust_request_chat_template,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"embed\" in supported_tasks\n        else None\n    )\n    state.openai_serving_classification = (\n        ServingClassification(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            trust_request_chat_template=args.trust_request_chat_template,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"classify\" in supported_tasks\n        else None\n    )\n    state.openai_serving_scores = (\n        ServingScores(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            score_template=resolved_chat_template,\n            log_error_stack=args.log_error_stack,\n        )\n        if (\"embed\" in supported_tasks or \"score\" in supported_tasks)\n        else None\n    )\n    state.openai_serving_tokenization = OpenAIServingTokenization(\n        engine_client,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n        trust_request_chat_template=args.trust_request_chat_template,\n        log_error_stack=args.log_error_stack,\n    )\n    state.openai_serving_transcription = (\n        OpenAIServingTranscription(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            log_error_stack=args.log_error_stack,\n            enable_force_include_usage=args.enable_force_include_usage,\n        )\n        if \"transcription\" in supported_tasks\n        else None\n    )\n    state.openai_serving_translation = (\n        OpenAIServingTranslation(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            log_error_stack=args.log_error_stack,\n            enable_force_include_usage=args.enable_force_include_usage,\n        )\n        if \"transcription\" in supported_tasks\n        else None\n    )\n    state.anthropic_serving_messages = (\n        AnthropicServingMessages(\n            engine_client,\n            state.openai_serving_models,\n            args.response_role,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            enable_auto_tools=args.enable_auto_tool_choice,\n            tool_parser=args.tool_call_parser,\n            reasoning_parser=args.structured_outputs_config.reasoning_parser,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_force_include_usage=args.enable_force_include_usage,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n    state.serving_tokens = (\n        ServingTokens(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            log_error_stack=args.log_error_stack,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_log_outputs=args.enable_log_outputs,\n            force_no_detokenize=args.tokens_only,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n\n    state.enable_server_load_tracking = args.enable_server_load_tracking\n    state.server_load_metrics = 0",
      "language": "python"
    },
    {
      "code": "async def init_app_state(\n    engine_client: EngineClient,\n    state: State,\n    args: Namespace,\n) -> None:\n    vllm_config = engine_client.vllm_config\n\n    if args.served_model_name is not None:\n        served_model_names = args.served_model_name\n    else:\n        served_model_names = [args.model]\n\n    if args.enable_log_requests:\n        request_logger = RequestLogger(max_log_len=args.max_log_len)\n    else:\n        request_logger = None\n\n    base_model_paths = [\n        BaseModelPath(name=name, model_path=args.model) for name in served_model_names\n    ]\n\n    state.engine_client = engine_client\n    state.log_stats = not args.disable_log_stats\n    state.vllm_config = vllm_config\n    state.args = args\n    supported_tasks = await engine_client.get_supported_tasks()\n    logger.info(\"Supported tasks: %s\", supported_tasks)\n\n    resolved_chat_template = await process_chat_template(\n        args.chat_template, engine_client, vllm_config.model_config\n    )\n\n    if args.tool_server == \"demo\":\n        tool_server: ToolServer | None = DemoToolServer()\n        assert isinstance(tool_server, DemoToolServer)\n        await tool_server.init_and_validate()\n    elif args.tool_server:\n        tool_server = MCPToolServer()\n        await tool_server.add_tool_server(args.tool_server)\n    else:\n        tool_server = None\n\n    # Merge default_mm_loras into the static lora_modules\n    default_mm_loras = (\n        vllm_config.lora_config.default_mm_loras\n        if vllm_config.lora_config is not None\n        else {}\n    )\n\n    default_mm_loras = (\n        vllm_config.lora_config.default_mm_loras\n        if vllm_config.lora_config is not None\n        else {}\n    )\n    lora_modules = process_lora_modules(args.lora_modules, default_mm_loras)\n\n    state.openai_serving_models = OpenAIServingModels(\n        engine_client=engine_client,\n        base_model_paths=base_model_paths,\n        lora_modules=lora_modules,\n    )\n    await state.openai_serving_models.init_static_loras()\n    state.openai_serving_responses = (\n        OpenAIServingResponses(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            enable_auto_tools=args.enable_auto_tool_choice,\n            tool_parser=args.tool_call_parser,\n            tool_server=tool_server,\n            reasoning_parser=args.structured_outputs_config.reasoning_parser,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_force_include_usage=args.enable_force_include_usage,\n            enable_log_outputs=args.enable_log_outputs,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n    state.openai_serving_chat = (\n        OpenAIServingChat(\n            engine_client,\n            state.openai_serving_models,\n            args.response_role,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            trust_request_chat_template=args.trust_request_chat_template,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            enable_auto_tools=args.enable_auto_tool_choice,\n            exclude_tools_when_tool_choice_none=args.exclude_tools_when_tool_choice_none,\n            tool_parser=args.tool_call_parser,\n            reasoning_parser=args.structured_outputs_config.reasoning_parser,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_force_include_usage=args.enable_force_include_usage,\n            enable_log_outputs=args.enable_log_outputs,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n    # Warm up chat template processing to avoid first-request latency\n    if state.openai_serving_chat is not None:\n        await state.openai_serving_chat.warmup()\n    state.openai_serving_completion = (\n        OpenAIServingCompletion(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_force_include_usage=args.enable_force_include_usage,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n    state.openai_serving_pooling = (\n        (\n            OpenAIServingPooling(\n                engine_client,\n                state.openai_serving_models,\n                supported_tasks=supported_tasks,\n                request_logger=request_logger,\n                chat_template=resolved_chat_template,\n                chat_template_content_format=args.chat_template_content_format,\n                trust_request_chat_template=args.trust_request_chat_template,\n                log_error_stack=args.log_error_stack,\n            )\n        )\n        if any(task in POOLING_TASKS for task in supported_tasks)\n        else None\n    )\n    state.openai_serving_embedding = (\n        OpenAIServingEmbedding(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            trust_request_chat_template=args.trust_request_chat_template,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"embed\" in supported_tasks\n        else None\n    )\n    state.openai_serving_classification = (\n        ServingClassification(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            trust_request_chat_template=args.trust_request_chat_template,\n            log_error_stack=args.log_error_stack,\n        )\n        if \"classify\" in supported_tasks\n        else None\n    )\n    state.openai_serving_scores = (\n        ServingScores(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            score_template=resolved_chat_template,\n            log_error_stack=args.log_error_stack,\n        )\n        if (\"embed\" in supported_tasks or \"score\" in supported_tasks)\n        else None\n    )\n    state.openai_serving_tokenization = OpenAIServingTokenization(\n        engine_client,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n        trust_request_chat_template=args.trust_request_chat_template,\n        log_error_stack=args.log_error_stack,\n    )\n    state.openai_serving_transcription = (\n        OpenAIServingTranscription(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            log_error_stack=args.log_error_stack,\n            enable_force_include_usage=args.enable_force_include_usage,\n        )\n        if \"transcription\" in supported_tasks\n        else None\n    )\n    state.openai_serving_translation = (\n        OpenAIServingTranslation(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            log_error_stack=args.log_error_stack,\n            enable_force_include_usage=args.enable_force_include_usage,\n        )\n        if \"transcription\" in supported_tasks\n        else None\n    )\n    state.anthropic_serving_messages = (\n        AnthropicServingMessages(\n            engine_client,\n            state.openai_serving_models,\n            args.response_role,\n            request_logger=request_logger,\n            chat_template=resolved_chat_template,\n            chat_template_content_format=args.chat_template_content_format,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            enable_auto_tools=args.enable_auto_tool_choice,\n            tool_parser=args.tool_call_parser,\n            reasoning_parser=args.structured_outputs_config.reasoning_parser,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_force_include_usage=args.enable_force_include_usage,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n    state.serving_tokens = (\n        ServingTokens(\n            engine_client,\n            state.openai_serving_models,\n            request_logger=request_logger,\n            return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n            log_error_stack=args.log_error_stack,\n            enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n            enable_log_outputs=args.enable_log_outputs,\n            force_no_detokenize=args.tokens_only,\n        )\n        if \"generate\" in supported_tasks\n        else None\n    )\n\n    state.enable_server_load_tracking = args.enable_server_load_tracking\n    state.server_load_metrics = 0",
      "language": "python"
    },
    {
      "code": "lifespan(app: FastAPI)",
      "language": "unknown"
    },
    {
      "code": "lifespan(app: FastAPI)",
      "language": "unknown"
    },
    {
      "code": "114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141",
      "language": "unknown"
    },
    {
      "code": "@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    try:\n        if app.state.log_stats:\n            engine_client: EngineClient = app.state.engine_client\n\n            async def _force_log():\n                while True:\n                    await asyncio.sleep(envs.VLLM_LOG_STATS_INTERVAL)\n                    await engine_client.do_log_stats()\n\n            task = asyncio.create_task(_force_log())\n            _running_tasks.add(task)\n            task.add_done_callback(_running_tasks.remove)\n        else:\n            task = None\n\n        # Mark the startup heap as static so that it's ignored by GC.\n        # Reduces pause times of oldest generation collections.\n        freeze_gc_heap()\n        try:\n            yield\n        finally:\n            if task is not None:\n                task.cancel()\n    finally:\n        # Ensure app state including engine ref is gc'd\n        del app.state",
      "language": "python"
    },
    {
      "code": "@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    try:\n        if app.state.log_stats:\n            engine_client: EngineClient = app.state.engine_client\n\n            async def _force_log():\n                while True:\n                    await asyncio.sleep(envs.VLLM_LOG_STATS_INTERVAL)\n                    await engine_client.do_log_stats()\n\n            task = asyncio.create_task(_force_log())\n            _running_tasks.add(task)\n            task.add_done_callback(_running_tasks.remove)\n        else:\n            task = None\n\n        # Mark the startup heap as static so that it's ignored by GC.\n        # Reduces pause times of oldest generation collections.\n        freeze_gc_heap()\n        try:\n            yield\n        finally:\n            if task is not None:\n                task.cancel()\n    finally:\n        # Ensure app state including engine ref is gc'd\n        del app.state",
      "language": "python"
    },
    {
      "code": "load_log_config(log_config_file: str | None) -> dict | None",
      "language": "rust"
    },
    {
      "code": "load_log_config(log_config_file: str | None) -> dict | None",
      "language": "rust"
    },
    {
      "code": "641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651",
      "language": "unknown"
    },
    {
      "code": "def load_log_config(log_config_file: str | None) -> dict | None:\n    if not log_config_file:\n        return None\n    try:\n        with open(log_config_file) as f:\n            return json.load(f)\n    except Exception as e:\n        logger.warning(\n            \"Failed to load log config from file %s: error %s\", log_config_file, e\n        )\n        return None",
      "language": "python"
    },
    {
      "code": "def load_log_config(log_config_file: str | None) -> dict | None:\n    if not log_config_file:\n        return None\n    try:\n        with open(log_config_file) as f:\n            return json.load(f)\n    except Exception as e:\n        logger.warning(\n            \"Failed to load log config from file %s: error %s\", log_config_file, e\n        )\n        return None",
      "language": "python"
    },
    {
      "code": "messages(request: Request) -> AnthropicServingMessages",
      "language": "php"
    },
    {
      "code": "messages(request: Request) -> AnthropicServingMessages",
      "language": "php"
    },
    {
      "code": "def messages(request: Request) -> AnthropicServingMessages:\n    return request.app.state.anthropic_serving_messages",
      "language": "python"
    },
    {
      "code": "def messages(request: Request) -> AnthropicServingMessages:\n    return request.app.state.anthropic_serving_messages",
      "language": "python"
    },
    {
      "code": "models(request: Request) -> OpenAIServingModels",
      "language": "php"
    },
    {
      "code": "models(request: Request) -> OpenAIServingModels",
      "language": "php"
    },
    {
      "code": "def models(request: Request) -> OpenAIServingModels:\n    return request.app.state.openai_serving_models",
      "language": "python"
    },
    {
      "code": "def models(request: Request) -> OpenAIServingModels:\n    return request.app.state.openai_serving_models",
      "language": "python"
    },
    {
      "code": "responses(\n    request: Request,\n) -> OpenAIServingResponses | None",
      "language": "rust"
    },
    {
      "code": "responses(\n    request: Request,\n) -> OpenAIServingResponses | None",
      "language": "rust"
    },
    {
      "code": "def responses(request: Request) -> OpenAIServingResponses | None:\n    return request.app.state.openai_serving_responses",
      "language": "python"
    },
    {
      "code": "def responses(request: Request) -> OpenAIServingResponses | None:\n    return request.app.state.openai_serving_responses",
      "language": "python"
    },
    {
      "code": "retrieve_responses(\n    response_id: str,\n    raw_request: Request,\n    starting_after: int | None = None,\n    stream: bool | None = False,\n)",
      "language": "rust"
    },
    {
      "code": "retrieve_responses(\n    response_id: str,\n    raw_request: Request,\n    starting_after: int | None = None,\n    stream: bool | None = False,\n)",
      "language": "rust"
    },
    {
      "code": "363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395",
      "language": "unknown"
    },
    {
      "code": "@router.get(\"/v1/responses/{response_id}\")\nasync def retrieve_responses(\n    response_id: str,\n    raw_request: Request,\n    starting_after: int | None = None,\n    stream: bool | None = False,\n):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\"\n        )\n\n    try:\n        response = await handler.retrieve_responses(\n            response_id,\n            starting_after=starting_after,\n            stream=stream,\n        )\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(response, ErrorResponse):\n        return JSONResponse(\n            content=response.model_dump(), status_code=response.error.code\n        )\n    elif isinstance(response, ResponsesResponse):\n        return JSONResponse(content=response.model_dump())\n    return StreamingResponse(\n        content=_convert_stream_to_sse_events(response), media_type=\"text/event-stream\"\n    )",
      "language": "python"
    },
    {
      "code": "@router.get(\"/v1/responses/{response_id}\")\nasync def retrieve_responses(\n    response_id: str,\n    raw_request: Request,\n    starting_after: int | None = None,\n    stream: bool | None = False,\n):\n    handler = responses(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Responses API\"\n        )\n\n    try:\n        response = await handler.retrieve_responses(\n            response_id,\n            starting_after=starting_after,\n            stream=stream,\n        )\n    except Exception as e:\n        raise HTTPException(\n            status_code=HTTPStatus.INTERNAL_SERVER_ERROR.value, detail=str(e)\n        ) from e\n\n    if isinstance(response, ErrorResponse):\n        return JSONResponse(\n            content=response.model_dump(), status_code=response.error.code\n        )\n    elif isinstance(response, ResponsesResponse):\n        return JSONResponse(content=response.model_dump())\n    return StreamingResponse(\n        content=_convert_stream_to_sse_events(response), media_type=\"text/event-stream\"\n    )",
      "language": "python"
    },
    {
      "code": "run_server(args, **uvicorn_kwargs) -> None",
      "language": "rust"
    },
    {
      "code": "run_server(args, **uvicorn_kwargs) -> None",
      "language": "rust"
    },
    {
      "code": "1314\n1315\n1316\n1317\n1318\n1319\n1320\n1321",
      "language": "unknown"
    },
    {
      "code": "async def run_server(args, **uvicorn_kwargs) -> None:\n    \"\"\"Run a single-worker API server.\"\"\"\n\n    # Add process-specific prefix to stdout and stderr.\n    decorate_logs(\"APIServer\")\n\n    listen_address, sock = setup_server(args)\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)",
      "language": "python"
    },
    {
      "code": "async def run_server(args, **uvicorn_kwargs) -> None:\n    \"\"\"Run a single-worker API server.\"\"\"\n\n    # Add process-specific prefix to stdout and stderr.\n    decorate_logs(\"APIServer\")\n\n    listen_address, sock = setup_server(args)\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)",
      "language": "python"
    },
    {
      "code": "run_server_worker(\n    listen_address,\n    sock,\n    args,\n    client_config=None,\n    **uvicorn_kwargs,\n) -> None",
      "language": "rust"
    },
    {
      "code": "run_server_worker(\n    listen_address,\n    sock,\n    args,\n    client_config=None,\n    **uvicorn_kwargs,\n) -> None",
      "language": "rust"
    },
    {
      "code": "1324\n1325\n1326\n1327\n1328\n1329\n1330\n1331\n1332\n1333\n1334\n1335\n1336\n1337\n1338\n1339\n1340\n1341\n1342\n1343\n1344\n1345\n1346\n1347\n1348\n1349\n1350\n1351\n1352\n1353\n1354\n1355\n1356\n1357\n1358\n1359\n1360\n1361\n1362\n1363\n1364\n1365\n1366\n1367\n1368\n1369\n1370\n1371\n1372\n1373\n1374\n1375\n1376\n1377",
      "language": "unknown"
    },
    {
      "code": "async def run_server_worker(\n    listen_address, sock, args, client_config=None, **uvicorn_kwargs\n) -> None:\n    \"\"\"Run a single API server worker.\"\"\"\n\n    if args.tool_parser_plugin and len(args.tool_parser_plugin) > 3:\n        ToolParserManager.import_tool_parser(args.tool_parser_plugin)\n\n    if args.reasoning_parser_plugin and len(args.reasoning_parser_plugin) > 3:\n        ReasoningParserManager.import_reasoning_parser(args.reasoning_parser_plugin)\n\n    # Load logging config for uvicorn if specified\n    log_config = load_log_config(args.log_config_file)\n    if log_config is not None:\n        uvicorn_kwargs[\"log_config\"] = log_config\n\n    async with build_async_engine_client(\n        args,\n        client_config=client_config,\n    ) as engine_client:\n        app = build_app(args)\n\n        await init_app_state(engine_client, app.state, args)\n\n        logger.info(\n            \"Starting vLLM API server %d on %s\",\n            engine_client.vllm_config.parallel_config._api_process_rank,\n            listen_address,\n        )\n        shutdown_task = await serve_http(\n            app,\n            sock=sock,\n            enable_ssl_refresh=args.enable_ssl_refresh,\n            host=args.host,\n            port=args.port,\n            log_level=args.uvicorn_log_level,\n            # NOTE: When the 'disable_uvicorn_access_log' value is True,\n            # no access log will be output.\n            access_log=not args.disable_uvicorn_access_log,\n            timeout_keep_alive=envs.VLLM_HTTP_TIMEOUT_KEEP_ALIVE,\n            ssl_keyfile=args.ssl_keyfile,\n            ssl_certfile=args.ssl_certfile,\n            ssl_ca_certs=args.ssl_ca_certs,\n            ssl_cert_reqs=args.ssl_cert_reqs,\n            h11_max_incomplete_event_size=args.h11_max_incomplete_event_size,\n            h11_max_header_count=args.h11_max_header_count,\n            **uvicorn_kwargs,\n        )\n\n    # NB: Await server shutdown only after the backend context is exited\n    try:\n        await shutdown_task\n    finally:\n        sock.close()",
      "language": "python"
    },
    {
      "code": "async def run_server_worker(\n    listen_address, sock, args, client_config=None, **uvicorn_kwargs\n) -> None:\n    \"\"\"Run a single API server worker.\"\"\"\n\n    if args.tool_parser_plugin and len(args.tool_parser_plugin) > 3:\n        ToolParserManager.import_tool_parser(args.tool_parser_plugin)\n\n    if args.reasoning_parser_plugin and len(args.reasoning_parser_plugin) > 3:\n        ReasoningParserManager.import_reasoning_parser(args.reasoning_parser_plugin)\n\n    # Load logging config for uvicorn if specified\n    log_config = load_log_config(args.log_config_file)\n    if log_config is not None:\n        uvicorn_kwargs[\"log_config\"] = log_config\n\n    async with build_async_engine_client(\n        args,\n        client_config=client_config,\n    ) as engine_client:\n        app = build_app(args)\n\n        await init_app_state(engine_client, app.state, args)\n\n        logger.info(\n            \"Starting vLLM API server %d on %s\",\n            engine_client.vllm_config.parallel_config._api_process_rank,\n            listen_address,\n        )\n        shutdown_task = await serve_http(\n            app,\n            sock=sock,\n            enable_ssl_refresh=args.enable_ssl_refresh,\n            host=args.host,\n            port=args.port,\n            log_level=args.uvicorn_log_level,\n            # NOTE: When the 'disable_uvicorn_access_log' value is True,\n            # no access log will be output.\n            access_log=not args.disable_uvicorn_access_log,\n            timeout_keep_alive=envs.VLLM_HTTP_TIMEOUT_KEEP_ALIVE,\n            ssl_keyfile=args.ssl_keyfile,\n            ssl_certfile=args.ssl_certfile,\n            ssl_ca_certs=args.ssl_ca_certs,\n            ssl_cert_reqs=args.ssl_cert_reqs,\n            h11_max_incomplete_event_size=args.h11_max_incomplete_event_size,\n            h11_max_header_count=args.h11_max_header_count,\n            **uvicorn_kwargs,\n        )\n\n    # NB: Await server shutdown only after the backend context is exited\n    try:\n        await shutdown_task\n    finally:\n        sock.close()",
      "language": "python"
    },
    {
      "code": "setup_server(args)",
      "language": "unknown"
    },
    {
      "code": "setup_server(args)",
      "language": "unknown"
    },
    {
      "code": "1270\n1271\n1272\n1273\n1274\n1275\n1276\n1277\n1278\n1279\n1280\n1281\n1282\n1283\n1284\n1285\n1286\n1287\n1288\n1289\n1290\n1291\n1292\n1293\n1294\n1295\n1296\n1297\n1298\n1299\n1300\n1301\n1302\n1303\n1304\n1305\n1306\n1307\n1308\n1309\n1310\n1311",
      "language": "unknown"
    },
    {
      "code": "def setup_server(args):\n    \"\"\"Validate API server args, set up signal handler, create socket\n    ready to serve.\"\"\"\n\n    logger.info(\"vLLM API server version %s\", VLLM_VERSION)\n    log_non_default_args(args)\n\n    if args.tool_parser_plugin and len(args.tool_parser_plugin) > 3:\n        ToolParserManager.import_tool_parser(args.tool_parser_plugin)\n\n    if args.reasoning_parser_plugin and len(args.reasoning_parser_plugin) > 3:\n        ReasoningParserManager.import_reasoning_parser(args.reasoning_parser_plugin)\n\n    validate_api_server_args(args)\n\n    # workaround to make sure that we bind the port before the engine is set up.\n    # This avoids race conditions with ray.\n    # see https://github.com/vllm-project/vllm/issues/8204\n    if args.uds:\n        sock = create_server_unix_socket(args.uds)\n    else:\n        sock_addr = (args.host or \"\", args.port)\n        sock = create_server_socket(sock_addr)\n\n    # workaround to avoid footguns where uvicorn drops requests with too\n    # many concurrent requests active\n    set_ulimit()\n\n    def signal_handler(*_) -> None:\n        # Interrupt server on sigterm while initializing\n        raise KeyboardInterrupt(\"terminated\")\n\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    if args.uds:\n        listen_address = f\"unix:{args.uds}\"\n    else:\n        addr, port = sock_addr\n        is_ssl = args.ssl_keyfile and args.ssl_certfile\n        host_part = f\"[{addr}]\" if is_valid_ipv6_address(addr) else addr or \"0.0.0.0\"\n        listen_address = f\"http{'s' if is_ssl else ''}://{host_part}:{port}\"\n    return listen_address, sock",
      "language": "python"
    },
    {
      "code": "def setup_server(args):\n    \"\"\"Validate API server args, set up signal handler, create socket\n    ready to serve.\"\"\"\n\n    logger.info(\"vLLM API server version %s\", VLLM_VERSION)\n    log_non_default_args(args)\n\n    if args.tool_parser_plugin and len(args.tool_parser_plugin) > 3:\n        ToolParserManager.import_tool_parser(args.tool_parser_plugin)\n\n    if args.reasoning_parser_plugin and len(args.reasoning_parser_plugin) > 3:\n        ReasoningParserManager.import_reasoning_parser(args.reasoning_parser_plugin)\n\n    validate_api_server_args(args)\n\n    # workaround to make sure that we bind the port before the engine is set up.\n    # This avoids race conditions with ray.\n    # see https://github.com/vllm-project/vllm/issues/8204\n    if args.uds:\n        sock = create_server_unix_socket(args.uds)\n    else:\n        sock_addr = (args.host or \"\", args.port)\n        sock = create_server_socket(sock_addr)\n\n    # workaround to avoid footguns where uvicorn drops requests with too\n    # many concurrent requests active\n    set_ulimit()\n\n    def signal_handler(*_) -> None:\n        # Interrupt server on sigterm while initializing\n        raise KeyboardInterrupt(\"terminated\")\n\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    if args.uds:\n        listen_address = f\"unix:{args.uds}\"\n    else:\n        addr, port = sock_addr\n        is_ssl = args.ssl_keyfile and args.ssl_certfile\n        host_part = f\"[{addr}]\" if is_valid_ipv6_address(addr) else addr or \"0.0.0.0\"\n        listen_address = f\"http{'s' if is_ssl else ''}://{host_part}:{port}\"\n    return listen_address, sock",
      "language": "python"
    },
    {
      "code": "show_available_models(raw_request: Request)",
      "language": "unknown"
    },
    {
      "code": "show_available_models(raw_request: Request)",
      "language": "unknown"
    },
    {
      "code": "300\n301\n302\n303\n304\n305",
      "language": "unknown"
    },
    {
      "code": "@router.get(\"/v1/models\")\nasync def show_available_models(raw_request: Request):\n    handler = models(raw_request)\n\n    models_ = await handler.show_available_models()\n    return JSONResponse(content=models_.model_dump())",
      "language": "python"
    },
    {
      "code": "@router.get(\"/v1/models\")\nasync def show_available_models(raw_request: Request):\n    handler = models(raw_request)\n\n    models_ = await handler.show_available_models()\n    return JSONResponse(content=models_.model_dump())",
      "language": "python"
    },
    {
      "code": "show_version()",
      "language": "unknown"
    },
    {
      "code": "show_version()",
      "language": "unknown"
    },
    {
      "code": "308\n309\n310\n311",
      "language": "unknown"
    },
    {
      "code": "@router.get(\"/version\")\nasync def show_version():\n    ver = {\"version\": VLLM_VERSION}\n    return JSONResponse(content=ver)",
      "language": "python"
    },
    {
      "code": "@router.get(\"/version\")\nasync def show_version():\n    ver = {\"version\": VLLM_VERSION}\n    return JSONResponse(content=ver)",
      "language": "python"
    },
    {
      "code": "tokenization(request: Request) -> OpenAIServingTokenization",
      "language": "php"
    },
    {
      "code": "tokenization(request: Request) -> OpenAIServingTokenization",
      "language": "php"
    },
    {
      "code": "def tokenization(request: Request) -> OpenAIServingTokenization:\n    return request.app.state.openai_serving_tokenization",
      "language": "python"
    },
    {
      "code": "def tokenization(request: Request) -> OpenAIServingTokenization:\n    return request.app.state.openai_serving_tokenization",
      "language": "python"
    },
    {
      "code": "transcription(\n    request: Request,\n) -> OpenAIServingTranscription",
      "language": "php"
    },
    {
      "code": "transcription(\n    request: Request,\n) -> OpenAIServingTranscription",
      "language": "php"
    },
    {
      "code": "def transcription(request: Request) -> OpenAIServingTranscription:\n    return request.app.state.openai_serving_transcription",
      "language": "python"
    },
    {
      "code": "def transcription(request: Request) -> OpenAIServingTranscription:\n    return request.app.state.openai_serving_transcription",
      "language": "python"
    },
    {
      "code": "translation(request: Request) -> OpenAIServingTranslation",
      "language": "php"
    },
    {
      "code": "translation(request: Request) -> OpenAIServingTranslation",
      "language": "php"
    },
    {
      "code": "def translation(request: Request) -> OpenAIServingTranslation:\n    return request.app.state.openai_serving_translation",
      "language": "python"
    },
    {
      "code": "def translation(request: Request) -> OpenAIServingTranslation:\n    return request.app.state.openai_serving_translation",
      "language": "python"
    },
    {
      "code": "validate_api_server_args(args)",
      "language": "unknown"
    },
    {
      "code": "validate_api_server_args(args)",
      "language": "unknown"
    },
    {
      "code": "1252\n1253\n1254\n1255\n1256\n1257\n1258\n1259\n1260\n1261\n1262\n1263\n1264\n1265\n1266\n1267",
      "language": "unknown"
    },
    {
      "code": "def validate_api_server_args(args):\n    valid_tool_parses = ToolParserManager.list_registered()\n    if args.enable_auto_tool_choice and args.tool_call_parser not in valid_tool_parses:\n        raise KeyError(\n            f\"invalid tool call parser: {args.tool_call_parser} \"\n            f\"(chose from {{ {','.join(valid_tool_parses)} }})\"\n        )\n\n    valid_reasoning_parsers = ReasoningParserManager.list_registered()\n    if (\n        reasoning_parser := args.structured_outputs_config.reasoning_parser\n    ) and reasoning_parser not in valid_reasoning_parsers:\n        raise KeyError(\n            f\"invalid reasoning parser: {reasoning_parser} \"\n            f\"(chose from {{ {','.join(valid_reasoning_parsers)} }})\"\n        )",
      "language": "python"
    },
    {
      "code": "def validate_api_server_args(args):\n    valid_tool_parses = ToolParserManager.list_registered()\n    if args.enable_auto_tool_choice and args.tool_call_parser not in valid_tool_parses:\n        raise KeyError(\n            f\"invalid tool call parser: {args.tool_call_parser} \"\n            f\"(chose from {{ {','.join(valid_tool_parses)} }})\"\n        )\n\n    valid_reasoning_parsers = ReasoningParserManager.list_registered()\n    if (\n        reasoning_parser := args.structured_outputs_config.reasoning_parser\n    ) and reasoning_parser not in valid_reasoning_parsers:\n        raise KeyError(\n            f\"invalid reasoning parser: {reasoning_parser} \"\n            f\"(chose from {{ {','.join(valid_reasoning_parsers)} }})\"\n        )",
      "language": "python"
    }
  ],
  "patterns": [],
  "links": [
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/api_server/",
    "https://docs.vllm.ai/en/latest/",
    "https://docs.vllm.ai/en/latest/usage/",
    "https://docs.vllm.ai/en/latest/contributing/",
    "https://docs.vllm.ai/en/latest/benchmarking/",
    "https://docs.vllm.ai/en/latest/api/",
    "https://docs.vllm.ai/en/latest/cli/",
    "https://docs.vllm.ai/en/latest/community/contact_us/",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/",
    "https://docs.vllm.ai/en/latest/getting_started/installation/cpu/",
    "https://docs.vllm.ai/en/latest/examples/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/async_llm_streaming/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/audio_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/basic/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/batch_llm_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/chat_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/context_extension/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated-prefill-v1/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/encoder_decoder_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/kv_load_failure_recovery/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/llm_engine_reset_kv/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/load_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/logits_processor/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/lora_with_quantization_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/metrics/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mistral-small/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/mlpspeculator/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/multilora_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/openai_batch/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prefix_caching/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/prompt_embed_inference/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen2_5_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen3_omni/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/qwen_1m/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/reproducibility/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_colocate/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_online_quant/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/rlhf_utils/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/save_sharded_state/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/simple_profiling/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/skip_loading_weights_in_engine_init/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/spec_decode/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_dp_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/torchrun_example/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language/",
    "https://docs.vllm.ai/en/latest/examples/offline_inference/vision_language_multi_image/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/api_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/chart-helm/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/dashboards/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_encoder/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_prefill/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/elastic_ep/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/gradio_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/kv_events_subscriber/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi-node-serving/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/multi_instance_data_parallel/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_for_multimodal/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_required/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_tool_calls_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_chat_completion_with_reasoning_streaming/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_completion_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_mcp_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_responses_client_with_tools/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_transcription_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/openai_translation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/opentelemetry/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prometheus_grafana/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/prompt_embed_inference_with_openai_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/ray_serve_deepseek/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_langchain/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/retrieval_augmented_generation_with_llamaindex/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/run_cluster/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/sagemaker-entrypoint/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/streamlit_openai_chatbot_webserver/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/structured_outputs/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/token_generation_client/",
    "https://docs.vllm.ai/en/latest/examples/online_serving/utils/",
    "https://docs.vllm.ai/en/latest/examples/others/lmcache/",
    "https://docs.vllm.ai/en/latest/examples/others/logging_configuration/",
    "https://docs.vllm.ai/en/latest/examples/others/tensorize_vllm_model/",
    "https://docs.vllm.ai/en/latest/examples/pooling/classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/embed/",
    "https://docs.vllm.ai/en/latest/examples/pooling/plugin/",
    "https://docs.vllm.ai/en/latest/examples/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/examples/pooling/score/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_classify/",
    "https://docs.vllm.ai/en/latest/examples/pooling/token_embed/",
    "https://docs.vllm.ai/en/latest/usage/v1_guide/",
    "https://docs.vllm.ai/en/latest/usage/faq/",
    "https://docs.vllm.ai/en/latest/usage/metrics/",
    "https://docs.vllm.ai/en/latest/usage/reproducibility/",
    "https://docs.vllm.ai/en/latest/usage/security/",
    "https://docs.vllm.ai/en/latest/usage/troubleshooting/",
    "https://docs.vllm.ai/en/latest/usage/usage_stats/",
    "https://docs.vllm.ai/en/latest/serving/offline_inference/",
    "https://docs.vllm.ai/en/latest/serving/openai_compatible_server/",
    "https://docs.vllm.ai/en/latest/serving/context_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/data_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/distributed_troubleshooting/",
    "https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/",
    "https://docs.vllm.ai/en/latest/serving/parallelism_scaling/",
    "https://docs.vllm.ai/en/latest/serving/integrations/langchain/",
    "https://docs.vllm.ai/en/latest/serving/integrations/llamaindex/",
    "https://docs.vllm.ai/en/latest/deployment/docker/",
    "https://docs.vllm.ai/en/latest/deployment/k8s/",
    "https://docs.vllm.ai/en/latest/deployment/nginx/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anyscale/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/anything-llm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/autogen/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/bentoml/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/cerebrium/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/chatbox/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dify/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/dstack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/haystack/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/helm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/hf_inference_endpoints/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/litellm/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lobe-chat/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/lws/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/modal/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/open-webui/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/retrieval_augmented_generation/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/skypilot/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/streamlit/",
    "https://docs.vllm.ai/en/latest/deployment/frameworks/triton/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kaito/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kserve/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kthena/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kubeai/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/kuberay/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llamastack/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llm-d/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/llmaz/",
    "https://docs.vllm.ai/en/latest/deployment/integrations/production-stack/",
    "https://docs.vllm.ai/en/latest/training/rlhf/",
    "https://docs.vllm.ai/en/latest/training/trl/",
    "https://docs.vllm.ai/en/latest/configuration/",
    "https://docs.vllm.ai/en/latest/configuration/conserving_memory/",
    "https://docs.vllm.ai/en/latest/configuration/engine_args/",
    "https://docs.vllm.ai/en/latest/configuration/env_vars/",
    "https://docs.vllm.ai/en/latest/configuration/model_resolution/",
    "https://docs.vllm.ai/en/latest/configuration/optimization/",
    "https://docs.vllm.ai/en/latest/configuration/serve_args/",
    "https://docs.vllm.ai/en/latest/models/supported_models/",
    "https://docs.vllm.ai/en/latest/models/generative_models/",
    "https://docs.vllm.ai/en/latest/models/pooling_models/",
    "https://docs.vllm.ai/en/latest/models/extensions/fastsafetensor/",
    "https://docs.vllm.ai/en/latest/models/extensions/runai_model_streamer/",
    "https://docs.vllm.ai/en/latest/models/extensions/tensorizer/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/cpu/",
    "https://docs.vllm.ai/en/latest/models/hardware_supported_models/xpu/",
    "https://docs.vllm.ai/en/latest/features/",
    "https://docs.vllm.ai/en/latest/features/automatic_prefix_caching/",
    "https://docs.vllm.ai/en/latest/features/batch_invariance/",
    "https://docs.vllm.ai/en/latest/features/custom_arguments/",
    "https://docs.vllm.ai/en/latest/features/custom_logitsprocs/",
    "https://docs.vllm.ai/en/latest/features/disagg_encoder/",
    "https://docs.vllm.ai/en/latest/features/disagg_prefill/",
    "https://docs.vllm.ai/en/latest/features/interleaved_thinking/",
    "https://docs.vllm.ai/en/latest/features/lora/",
    "https://docs.vllm.ai/en/latest/features/mooncake_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/multimodal_inputs/",
    "https://docs.vllm.ai/en/latest/features/nixl_connector_usage/",
    "https://docs.vllm.ai/en/latest/features/prompt_embeds/",
    "https://docs.vllm.ai/en/latest/features/reasoning_outputs/",
    "https://docs.vllm.ai/en/latest/features/sleep_mode/",
    "https://docs.vllm.ai/en/latest/features/spec_decode/",
    "https://docs.vllm.ai/en/latest/features/structured_outputs/",
    "https://docs.vllm.ai/en/latest/features/tool_calling/",
    "https://docs.vllm.ai/en/latest/features/quantization/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_awq/",
    "https://docs.vllm.ai/en/latest/features/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/features/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/features/quantization/bnb/",
    "https://docs.vllm.ai/en/latest/features/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/features/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/features/quantization/gptqmodel/",
    "https://docs.vllm.ai/en/latest/features/quantization/inc/",
    "https://docs.vllm.ai/en/latest/features/quantization/int4/",
    "https://docs.vllm.ai/en/latest/features/quantization/int8/",
    "https://docs.vllm.ai/en/latest/features/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/",
    "https://docs.vllm.ai/en/latest/features/quantization/quark/",
    "https://docs.vllm.ai/en/latest/features/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/contributing/deprecation_policy/",
    "https://docs.vllm.ai/en/latest/contributing/dockerfile/dockerfile/",
    "https://docs.vllm.ai/en/latest/contributing/incremental_build/",
    "https://docs.vllm.ai/en/latest/contributing/profiling/",
    "https://docs.vllm.ai/en/latest/contributing/vulnerability_management/",
    "https://docs.vllm.ai/en/latest/contributing/model/",
    "https://docs.vllm.ai/en/latest/contributing/model/basic/",
    "https://docs.vllm.ai/en/latest/contributing/model/registration/",
    "https://docs.vllm.ai/en/latest/contributing/model/tests/",
    "https://docs.vllm.ai/en/latest/contributing/model/multimodal/",
    "https://docs.vllm.ai/en/latest/contributing/model/transcription/",
    "https://docs.vllm.ai/en/latest/contributing/ci/failures/",
    "https://docs.vllm.ai/en/latest/contributing/ci/nightly_builds/",
    "https://docs.vllm.ai/en/latest/contributing/ci/update_pytorch_version/",
    "https://docs.vllm.ai/en/latest/design/io_processor_plugins/",
    "https://docs.vllm.ai/en/latest/design/lora_resolver_plugins/",
    "https://docs.vllm.ai/en/latest/design/plugin_system/",
    "https://docs.vllm.ai/en/latest/design/arch_overview/",
    "https://docs.vllm.ai/en/latest/design/cuda_graphs/",
    "https://docs.vllm.ai/en/latest/design/dbo/",
    "https://docs.vllm.ai/en/latest/design/debug_vllm_compile/",
    "https://docs.vllm.ai/en/latest/design/fused_moe_modular_kernel/",
    "https://docs.vllm.ai/en/latest/design/huggingface_integration/",
    "https://docs.vllm.ai/en/latest/design/hybrid_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/design/logits_processors/",
    "https://docs.vllm.ai/en/latest/design/metrics/",
    "https://docs.vllm.ai/en/latest/design/mm_processing/",
    "https://docs.vllm.ai/en/latest/design/moe_kernel_features/",
    "https://docs.vllm.ai/en/latest/design/multiprocessing/",
    "https://docs.vllm.ai/en/latest/design/optimization_levels/",
    "https://docs.vllm.ai/en/latest/design/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/design/paged_attention/",
    "https://docs.vllm.ai/en/latest/design/prefix_caching/",
    "https://docs.vllm.ai/en/latest/design/torch_compile/",
    "https://docs.vllm.ai/en/latest/benchmarking/cli/",
    "https://docs.vllm.ai/en/latest/benchmarking/sweeps/",
    "https://docs.vllm.ai/en/latest/benchmarking/dashboard/",
    "https://docs.vllm.ai/en/latest/api/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/beam_search/",
    "https://docs.vllm.ai/en/latest/api/vllm/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/connections/",
    "https://docs.vllm.ai/en/latest/api/vllm/env_override/",
    "https://docs.vllm.ai/en/latest/api/vllm/envs/",
    "https://docs.vllm.ai/en/latest/api/vllm/forward_context/",
    "https://docs.vllm.ai/en/latest/api/vllm/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/logits_process/",
    "https://docs.vllm.ai/en/latest/api/vllm/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/pooling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/sampling_params/",
    "https://docs.vllm.ai/en/latest/api/vllm/scalar_type/",
    "https://docs.vllm.ai/en/latest/api/vllm/scripts/",
    "https://docs.vllm.ai/en/latest/api/vllm/sequence/",
    "https://docs.vllm.ai/en/latest/api/vllm/tasks/",
    "https://docs.vllm.ai/en/latest/api/vllm/tracing/",
    "https://docs.vllm.ai/en/latest/api/vllm/version/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/assets/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/chunked_local_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/cross_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/encoder_only_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/layers/mm_encoder_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/chunked_prefill_paged_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/paged_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/pallas_kv_cache_update/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/prefix_prefill/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_decode_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_merge_attn_states/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_reshape_and_cache_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/triton_unified_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/ops/vit_attn_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/fa_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_sharing_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/attention/utils/kv_transfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/datasets/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/endpoint_request_func/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/ready_checker/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/lib/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/param_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/server/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/sla_sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/benchmarks/sweep/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/activation_quant_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/base_static_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/caching/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/collective_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/compiler_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/cuda_graph/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/decorators/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fix_functionalization/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fusion_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/fx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/matcher_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/monitor/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/noop_elimination/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/partition_rules/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/pass_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/piecewise_backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/post_cleanup/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/qk_norm_rope_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/rocm_aiter_fusion/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/sequence_parallelism/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/torch25_custom_graph_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/vllm_inductor_pass/",
    "https://docs.vllm.ai/en/latest/api/vllm/compilation/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/compilation/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/device/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/load/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/model/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/observability/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/parallel/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speculative/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/config/vllm/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/device_allocator/cumem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/communication_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_events/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/parallel_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/tpu_distributed_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all2all/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/all_reduce_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/base_device_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/cuda_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/custom_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/mnnvl_compat/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_allocator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/pynccl_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/quick_all_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/ray_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_broadcast/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/shm_object_storage/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/symm_mem/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/tpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/device_communicators/xpu_communicator/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/ec_transfer/ec_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/async_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/eplb_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/rebalance_execute/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/eplb/policy/default/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_transfer_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/example_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/multi_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/arg_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/async_llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/engine/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/api_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/chat_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/context/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/launcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/logger/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/renderer/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/responses_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/score_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/ssl/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/tool_server/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/anthropic/serving_messages/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/collect_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/types/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/latency/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/main/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/startup/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/sweep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/cli/benchmark/throughput/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/cli_args/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/orca_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/run_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_chat_stream_harmony/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_completion/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_models/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_responses/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/serving_transcription/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/speech_to_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/harmony_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/openai/parser/responses_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/classify/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/conftest/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/embed/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/pooling/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/pooling/score/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/sagemaker/routes/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/cache/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/disagg/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/elastic_ep/middleware/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/health/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/instrumentator/server_info/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/lora/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/profile/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rlhf/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/rpc/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/sleep/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/api_router/",
    "https://docs.vllm.ai/en/latest/api/vllm/entrypoints/serve/tokenize/serving/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/data/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/inputs/preprocess/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/dump_input/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/formatter/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/lazy/",
    "https://docs.vllm.ai/en/latest/api/vllm/logging_utils/log_time/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/lora_weights/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/model_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/peft_helper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/worker_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/base_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/column_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/replicated_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/row_parallel_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/layers/vocal_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/ipex_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/torch_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/fused_moe_lora_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/kernel_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_expand_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_kernel_metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/lora_shrink_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/triton_ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/ops/xla_ops/lora_ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_selector/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/punica_xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/lora/punica_wrapper/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/custom_op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/parameter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/activation/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/attention_layer_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/batch_invariant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/layernorm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/lightning_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/pooler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/resampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/vocab_parallel_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_delta_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_o/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/cumsum/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/fused_recurrent/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/index_py/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/kda/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/l2norm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/layernorm_guard/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/op/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/solve_tril/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fla/ops/wy_fast/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/all2all_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cpu_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deep_gemm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_batched_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_marlin_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_method_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/fused_moe_modular_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/layer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/modular_kernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_align_block_size/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_permute_unpermute/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/moe_torch_iterative/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/pplx_prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/prepare_finalize/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/routing_simulator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/shared_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/trtllm_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/fused_moe/zero_expert_fused_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_mixer2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/mamba_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/short_conv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/causal_conv1d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/layernorm_gated/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/mamba_ssm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_bmm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_chunk_state/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_combined/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/mamba/ops/ssd_state_passing/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/auto_round/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/awq_triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/base_config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/bitsandbytes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/cpu_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/deepspeedfp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/experts_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fbgemm_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/fp_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gguf/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/gptq_marlin_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/hqq_marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/inc/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/input_quant_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ipex_quant/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kv_cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/modelopt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/moe_wna16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/mxfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/petit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/ptpc_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/qutlass_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/rtn/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/torchao/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/tpu_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/module/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/conch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/machete/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/triton/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/kernels/scaled_mm/xla/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/quark_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_scheme/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/allspark_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/bitblas_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/flashinfer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/fp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/gptq_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/int8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/layer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/machete_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/marlin_utils_test_24/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp4_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp6_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/mxfp8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/nvfp4_moe_support/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/ocp_mx_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/petit_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/quant_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/quantization/utils/w8a8_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dual_chunk_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/linear_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama3_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/llama4_vision_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/mrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/xdrope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/base_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/bitsandbytes_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/default_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/dummy_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/gguf_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/online_quantization/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/runai_streamer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/sharded_state_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tensorizer_loader/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/model_loader/weight_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/adapters/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aimv2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/apertus/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arcee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aria/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/audioflamingo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/aya_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/baichuan/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bailing_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bee/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bert_with_rope/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/blip2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/bloom/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chameleon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/clip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/cohere2_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/commandr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dbrx/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepencoder/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_v2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/dots_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie45_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ernie_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/exaone4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fairseq2_llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/falcon_h1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/fuyu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gemma3n_mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4_moe_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/glm4v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_bigcode/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_j/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_neox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gpt_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granite_speech/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoehybrid/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/granitemoeshared/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/gritlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/grok1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/h2ovl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hunyuan_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/hyperclovax_vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics2_vision_model/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/idefics3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interfaces_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/intern_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internlm2_ve/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/interns1_vit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/internvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jais2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/jina_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/keye_vl1_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/lightonocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama4_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llama_eagle3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_next_video/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/llava_onevision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/longcat_flash_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mimo_v2_flash/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpm_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minicpmv/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_m2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_text_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/minimax_vl_01/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mistral_large_3_eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mllama4/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/modernbert/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/module_mapping/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/molmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/mpt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nano_nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_nas/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nemotron_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/nvlm_d/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/olmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opencua/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/openpangu_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/opt/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/orion/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ouro/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paddleocr_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/paligemma/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/persimmon/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi3v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phi4mm_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/phimoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/pixtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/plamo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_omni_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_5_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_rm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen2_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_next_mtp/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_omni_moe_thinker/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen3_vl_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/qwen_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/roberta/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/rvl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/seed_oss/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/siglip2navit/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/skyworkr1v/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/smolvlm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/solar/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/stablelm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/starcoder2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_text/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/swin/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/tarsier/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/telechat2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/teleflm/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/terratorch/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/vision/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/voxtral_streaming/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/whisper_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/zamba2/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/causal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/legacy/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/pooling/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/models/transformers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/deep_gemm_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/model_executor/warmup/kernel_warmup/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/audio/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/evs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/hasher/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/image/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/inputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/parse/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/processing/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/multimodal/video/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/cuda/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/rocm/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/platforms/xpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/io_processors/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/",
    "https://docs.vllm.ai/en/latest/api/vllm/plugins/lora_resolvers/filesystem_resolver/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/layerwise_profile/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/profiler/wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/lazy_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/ray/ray_env/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/abs_reasoning_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/basic_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_r1_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/deepseek_v3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/ernie45_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/glm4_moe_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/gptoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/granite_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/holo2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/hunyuan_a13b_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/identity_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/minimax_m2_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/mistral_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/olmo3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/qwen3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/seedoss_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/reasoning/step3_reasoning_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/deepseek_v32_encoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/detokenizer_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/hf/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/protocol/",
    "https://docs.vllm.ai/en/latest/api/vllm/tokenizers/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/abstract_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv31_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/deepseekv32_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/ernie45_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/functiongemma_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/gigachat3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm4_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/glm47_moe_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_20b_fc_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/granite_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hermes_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/hunyuan_a13b_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/internlm2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/jamba_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/kimi_k2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama4_pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/llama_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/longcat_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_m2_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/minimax_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/mistral_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/olmo3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/openai_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/phi4mini_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/pythonic_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3coder_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/qwen3xml_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/seed_oss_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/step3_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/tool_parsers/xlam_tool_parser/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/config_parser_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/dynamic_module/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/gguf_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/repo_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/runai_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/s3_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/tokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/chat_templates/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/afmoe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/arctic/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/chatglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/dotsocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/falcon/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/flex_olmo/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/jais/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_linear/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/kimi_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/lfm2_moe/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/midashenglm/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mistral/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/mlp_speculator/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/moonvit/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/nemotron_h/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/olmo3/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/qwen3_next/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/radio/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/step3_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/tarsier2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/ultravox/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/algos/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/configs/speculators/base/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/bagel/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_ocr/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/deepseek_vl2/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/hunyuan_vl_image/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis/",
    "https://docs.vllm.ai/en/latest/api/vllm/transformers_utils/processors/ovis2_5/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/triton_utils/importing/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/",
    "https://docs.vllm.ai/en/latest/api/vllm/usage/usage_lib/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/argparse_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/cache/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/collection_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/counter/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/deep_gemm/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/func_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/gc_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/hashing/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/import_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/jsontree/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/math_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_constants/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/mem_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nccl/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/network_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/nvtx_pytorch_hooks/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/platform_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/profiling/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/registry/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/system_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/tensor_schema/",
    "https://docs.vllm.ai/en/latest/api/vllm/utils/torch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/cudagraph_dispatcher/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_cache_interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/serial_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/cpu_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flash_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flashinfer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/flex_attention/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/gdn_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/linear_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba1_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba2_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mamba_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/pallas/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_fa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_aiter_unified_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/rocm_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/short_conv_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/tree_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/triton_attn/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/aiter_triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/common/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/cutlass_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashattn_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashinfer_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/flashmla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/indexer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/attention/backends/mla/triton_mla/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/block_pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/encoder_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/kv_cache_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/single_type_kv_cache_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/async_scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/request_queue/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/scheduler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/core/sched/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/async_llm/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/coordinator/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/core_client/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/detokenizer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/exceptions/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/input_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/llm_engine/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/output_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/parallel_sampling/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/engine/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/multiproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_distributed_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/ray_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/executor/uniproc_executor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/abstract/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/arc_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backend/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/factory/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/lru_manager/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/mediums/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/spec/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/backends/cpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/cpu_gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/kv_offload/worker/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/loggers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/perf/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/prometheus/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/ray_wrappers/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/reader/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/metrics/stats/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/pool/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/rejection_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/builtin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/interface/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/logits_processor/state/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/bad_words/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/logprobs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/ops/topk_topp_sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/sample/tpu/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/medusa/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/ngram_proposer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/suffix_decoding/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/spec_decode/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_guidance/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_lm_format_enforcer/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_outlines/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_types/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/backend_xgrammar/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/request/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/structured_output/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/cpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ec_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_ubatch_wrapper/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/kv_connector_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/lora_model_runner_mixin/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/tpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatch_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/ubatching/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/worker_base/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/workspace/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/xpu_worker/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/async_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/attn_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/block_table/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/cudagraph_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/dp_utils/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/input_batch/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/model_runner/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/states/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/structured_outputs/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/metrics/logits/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/gumbel/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/logprob/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/metadata/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/min_p/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/output/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/penalties/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/sample/sampler/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/eagle_cudagraph/",
    "https://docs.vllm.ai/en/latest/api/vllm/v1/worker/gpu/spec_decode/rejection_sample/",
    "https://docs.vllm.ai/en/latest/cli/serve/",
    "https://docs.vllm.ai/en/latest/cli/chat/",
    "https://docs.vllm.ai/en/latest/cli/complete/",
    "https://docs.vllm.ai/en/latest/cli/run-batch/",
    "https://docs.vllm.ai/en/latest/cli/bench/latency/",
    "https://docs.vllm.ai/en/latest/cli/bench/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/plot_pareto/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve/",
    "https://docs.vllm.ai/en/latest/cli/bench/sweep/serve_sla/",
    "https://docs.vllm.ai/en/latest/cli/bench/throughput/",
    "https://docs.vllm.ai/en/latest/community/meetups/",
    "https://docs.vllm.ai/en/latest/community/sponsors/",
    "https://docs.vllm.ai/en/latest/governance/collaboration/",
    "https://docs.vllm.ai/en/latest/governance/committers/",
    "https://docs.vllm.ai/en/latest/governance/process/"
  ]
}